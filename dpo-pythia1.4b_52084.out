no FSDP port specified; using open port for FSDP: 37585
seed: 0
exp_name: pythia1.4b_dpo_seed0
batch_size: 32
eval_batch_size: 16
debug: false
fsdp_port: 37585
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699
lr: 1.0e-06
gradient_accumulation_steps: 2
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 12000
minimum_log_interval_secs: 1.0
model:
  name_or_path: lomahony/pythia-1.4b-helpful-sft
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-222-35:.cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240112_004137-cn14yuod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia1.4b_dpo_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/cn14yuod
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-1.4b-helpful-sft
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2849/5103 [00:00<00:00, 28483.71it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 28289.70it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   0%|          | 361/96256 [00:00<00:36, 2632.59it/s]Processing HH static:   2%|‚ñè         | 1550/96256 [00:00<00:21, 4411.59it/s]Processing HH static:   5%|‚ñç         | 4372/96256 [00:00<00:07, 11652.61it/s]Processing HH static:   7%|‚ñã         | 7169/96256 [00:00<00:05, 16593.54it/s]Processing HH static:  10%|‚ñà         | 10060/96256 [00:00<00:04, 20311.47it/s]Processing HH static:  13%|‚ñà‚ñé        | 12938/96256 [00:00<00:03, 22863.09it/s]Processing HH static:  16%|‚ñà‚ñã        | 15819/96256 [00:00<00:03, 24651.18it/s]Processing HH static:  19%|‚ñà‚ñâ        | 18694/96256 [00:00<00:02, 25881.04it/s]Processing HH static:  22%|‚ñà‚ñà‚ñè       | 21562/96256 [00:01<00:02, 26720.49it/s]Processing HH static:  25%|‚ñà‚ñà‚ñå       | 24432/96256 [00:01<00:02, 27312.63it/s]Processing HH static:  28%|‚ñà‚ñà‚ñä       | 27303/96256 [00:01<00:02, 27730.78it/s]Processing HH static:  31%|‚ñà‚ñà‚ñà‚ñè      | 30144/96256 [00:01<00:02, 27934.04it/s]Processing HH static:  34%|‚ñà‚ñà‚ñà‚ñç      | 33011/96256 [00:01<00:02, 28151.97it/s]Processing HH static:  37%|‚ñà‚ñà‚ñà‚ñã      | 35849/96256 [00:01<00:03, 18822.88it/s]Processing HH static:  40%|‚ñà‚ñà‚ñà‚ñà      | 38673/96256 [00:01<00:02, 20915.29it/s]Processing HH static:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 41503/96256 [00:01<00:02, 22693.38it/s]Processing HH static:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 44264/96256 [00:02<00:02, 23951.00it/s]Processing HH static:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 47009/96256 [00:02<00:01, 24887.39it/s]Processing HH static:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 49734/96256 [00:02<00:01, 25540.94it/s]Processing HH static:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 52416/96256 [00:02<00:03, 12049.33it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 54452/96256 [00:03<00:05, 7297.67it/s] Processing HH static:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 57137/96256 [00:03<00:04, 9448.17it/s]Processing HH static:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 59866/96256 [00:03<00:03, 11870.61it/s]Processing HH static:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 62581/96256 [00:03<00:02, 14352.34it/s]Processing HH static:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 65081/96256 [00:03<00:01, 16356.37it/s]Processing HH static:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 67705/96256 [00:03<00:01, 18451.29it/s]Processing HH static:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 70319/96256 [00:03<00:01, 20237.42it/s]Processing HH static:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 73019/96256 [00:04<00:01, 21922.34it/s]Processing HH static:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 75574/96256 [00:04<00:02, 7946.33it/s] Processing HH static:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 77457/96256 [00:05<00:03, 6071.91it/s]Processing HH static:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 78878/96256 [00:05<00:03, 5238.01it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 79975/96256 [00:06<00:03, 4344.52it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80817/96256 [00:06<00:04, 3675.06it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81469/96256 [00:06<00:04, 3595.15it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82022/96256 [00:07<00:04, 3533.04it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82506/96256 [00:07<00:04, 3132.62it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82905/96256 [00:07<00:04, 2867.99it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83245/96256 [00:07<00:04, 2806.26it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83570/96256 [00:07<00:04, 2867.66it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83897/96256 [00:07<00:04, 2937.99it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84215/96256 [00:07<00:04, 2755.34it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84506/96256 [00:08<00:04, 2704.97it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84954/96256 [00:08<00:03, 3110.57it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85283/96256 [00:08<00:03, 2763.19it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85577/96256 [00:08<00:03, 2681.81it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85857/96256 [00:08<00:03, 2634.49it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86128/96256 [00:08<00:03, 2558.96it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86481/96256 [00:08<00:03, 2810.25it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86846/96256 [00:08<00:03, 3033.02it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87157/96256 [00:09<00:03, 2905.74it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87454/96256 [00:09<00:03, 2638.55it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87726/96256 [00:09<00:03, 2473.65it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88082/96256 [00:09<00:02, 2752.90it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88366/96256 [00:09<00:03, 2555.89it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88630/96256 [00:09<00:02, 2551.60it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88891/96256 [00:09<00:02, 2483.37it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89143/96256 [00:09<00:02, 2384.87it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89385/96256 [00:09<00:02, 2366.17it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89624/96256 [00:10<00:02, 2330.96it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89888/96256 [00:10<00:02, 2413.11it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90131/96256 [00:10<00:02, 2310.92it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90364/96256 [00:10<00:02, 2216.96it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90588/96256 [00:10<00:02, 2179.87it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90914/96256 [00:10<00:02, 2481.38it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91202/96256 [00:10<00:01, 2590.59it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91464/96256 [00:10<00:01, 2481.83it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91715/96256 [00:10<00:01, 2485.67it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92049/96256 [00:11<00:01, 2731.13it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92325/96256 [00:11<00:01, 2505.68it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92581/96256 [00:11<00:01, 2378.27it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 92823/96256 [00:11<00:01, 2320.34it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93058/96256 [00:11<00:01, 2222.05it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93283/96256 [00:11<00:01, 2211.55it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93512/96256 [00:11<00:01, 2232.16it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93739/96256 [00:11<00:01, 2236.70it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94020/96256 [00:11<00:00, 2399.87it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94262/96256 [00:12<00:00, 2373.24it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94512/96256 [00:12<00:00, 2405.69it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94754/96256 [00:12<00:00, 2396.90it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94995/96256 [00:12<00:00, 2371.45it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95233/96256 [00:12<00:00, 2229.69it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95458/96256 [00:12<00:00, 2216.60it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95681/96256 [00:12<00:00, 2168.43it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96038/96256 [00:12<00:00, 2566.26it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:12<00:00, 7490.18it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:02<00:33,  2.26s/it]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:02<00:14,  1.02s/it]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:02<00:08,  1.61it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:02<00:05,  2.32it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:02<00:03,  3.05it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:02<00:02,  3.88it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:03<00:01,  4.53it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:03<00:01,  5.10it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:03<00:01,  5.64it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:03<00:01,  5.94it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:03<00:00,  6.21it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:00,  6.44it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  6.58it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:04<00:00,  6.59it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:04<00:00,  6.72it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  6.74it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:04<00:00,  3.63it/s]
eval after 0: {'rewards_eval/chosen': '0.00083064', 'rewards_eval/rejected': '0.00067042', 'rewards_eval/accuracies': '0.51953', 'rewards_eval/margins': '0.00016022', 'logps_eval/rejected': '-118.27', 'logps_eval/chosen': '-136.44', 'loss/eval': '0.69332'}
train stats after 32 examples: {'rewards_train/chosen': '-0.0035604', 'rewards_train/rejected': '0.008398', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.011958', 'logps_train/rejected': '-108.11', 'logps_train/chosen': '-124.22', 'loss/train': '0.69928', 'examples_per_second': '27.056', 'grad_norm': '20.358', 'counters/examples': 32, 'counters/updates': 1}
skipping logging after 64 examples to avoid logging too frequently
train stats after 96 examples: {'rewards_train/chosen': '0.0063898', 'rewards_train/rejected': '0.004411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0019788', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-112', 'loss/train': '0.6923', 'examples_per_second': '44.829', 'grad_norm': '20.75', 'counters/examples': 96, 'counters/updates': 3}
skipping logging after 128 examples to avoid logging too frequently
train stats after 160 examples: {'rewards_train/chosen': '0.0095278', 'rewards_train/rejected': '0.0024927', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.007035', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-148.89', 'loss/train': '0.6899', 'examples_per_second': '45.589', 'grad_norm': '21.091', 'counters/examples': 160, 'counters/updates': 5}
skipping logging after 192 examples to avoid logging too frequently
train stats after 224 examples: {'rewards_train/chosen': '0.0013895', 'rewards_train/rejected': '-0.0094034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010793', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-124.8', 'loss/train': '0.68788', 'examples_per_second': '44.934', 'grad_norm': '20.418', 'counters/examples': 224, 'counters/updates': 7}
skipping logging after 256 examples to avoid logging too frequently
train stats after 288 examples: {'rewards_train/chosen': '0.014984', 'rewards_train/rejected': '0.00049511', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.014489', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-122.25', 'loss/train': '0.68605', 'examples_per_second': '45.849', 'grad_norm': '21.592', 'counters/examples': 288, 'counters/updates': 9}
skipping logging after 320 examples to avoid logging too frequently
train stats after 352 examples: {'rewards_train/chosen': '0.00034163', 'rewards_train/rejected': '-0.0033016', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0036432', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-123.61', 'loss/train': '0.69156', 'examples_per_second': '46.082', 'grad_norm': '22.211', 'counters/examples': 352, 'counters/updates': 11}
skipping logging after 384 examples to avoid logging too frequently
train stats after 416 examples: {'rewards_train/chosen': '0.0049202', 'rewards_train/rejected': '0.0036285', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0012916', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-151.71', 'loss/train': '0.69263', 'examples_per_second': '44.821', 'grad_norm': '20.837', 'counters/examples': 416, 'counters/updates': 13}
skipping logging after 448 examples to avoid logging too frequently
train stats after 480 examples: {'rewards_train/chosen': '0.006516', 'rewards_train/rejected': '-0.0013313', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0078473', 'logps_train/rejected': '-100.16', 'logps_train/chosen': '-133.52', 'loss/train': '0.68946', 'examples_per_second': '49.497', 'grad_norm': '20.115', 'counters/examples': 480, 'counters/updates': 15}
skipping logging after 512 examples to avoid logging too frequently
train stats after 544 examples: {'rewards_train/chosen': '0.022127', 'rewards_train/rejected': '0.012428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.009699', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-147.52', 'loss/train': '0.68867', 'examples_per_second': '45.541', 'grad_norm': '21.552', 'counters/examples': 544, 'counters/updates': 17}
skipping logging after 576 examples to avoid logging too frequently
train stats after 608 examples: {'rewards_train/chosen': '0.012712', 'rewards_train/rejected': '0.013873', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0011607', 'logps_train/rejected': '-116.55', 'logps_train/chosen': '-128.4', 'loss/train': '0.69386', 'examples_per_second': '45.86', 'grad_norm': '20.952', 'counters/examples': 608, 'counters/updates': 19}
skipping logging after 640 examples to avoid logging too frequently
train stats after 672 examples: {'rewards_train/chosen': '0.0091903', 'rewards_train/rejected': '-0.0012834', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010474', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-154.7', 'loss/train': '0.68827', 'examples_per_second': '47.072', 'grad_norm': '21.812', 'counters/examples': 672, 'counters/updates': 21}
skipping logging after 704 examples to avoid logging too frequently
train stats after 736 examples: {'rewards_train/chosen': '0.0087974', 'rewards_train/rejected': '0.0071303', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0016671', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-118.37', 'loss/train': '0.69268', 'examples_per_second': '46.931', 'grad_norm': '20.22', 'counters/examples': 736, 'counters/updates': 23}
skipping logging after 768 examples to avoid logging too frequently
train stats after 800 examples: {'rewards_train/chosen': '0.03951', 'rewards_train/rejected': '0.004431', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.035079', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-183.41', 'loss/train': '0.67615', 'examples_per_second': '44.614', 'grad_norm': '24.068', 'counters/examples': 800, 'counters/updates': 25}
skipping logging after 832 examples to avoid logging too frequently
train stats after 864 examples: {'rewards_train/chosen': '0.031758', 'rewards_train/rejected': '0.015144', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.016613', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-111.06', 'loss/train': '0.68538', 'examples_per_second': '45.527', 'grad_norm': '19.869', 'counters/examples': 864, 'counters/updates': 27}
skipping logging after 896 examples to avoid logging too frequently
train stats after 928 examples: {'rewards_train/chosen': '0.055017', 'rewards_train/rejected': '0.026959', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028058', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-169.6', 'loss/train': '0.67968', 'examples_per_second': '45.616', 'grad_norm': '23.448', 'counters/examples': 928, 'counters/updates': 29}
skipping logging after 960 examples to avoid logging too frequently
train stats after 992 examples: {'rewards_train/chosen': '0.036524', 'rewards_train/rejected': '-0.001558', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.038082', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-153.36', 'loss/train': '0.67469', 'examples_per_second': '45.117', 'grad_norm': '21.749', 'counters/examples': 992, 'counters/updates': 31}
skipping logging after 1024 examples to avoid logging too frequently
train stats after 1056 examples: {'rewards_train/chosen': '0.038724', 'rewards_train/rejected': '0.016016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022708', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-129.24', 'loss/train': '0.68258', 'examples_per_second': '48.498', 'grad_norm': '20.397', 'counters/examples': 1056, 'counters/updates': 33}
skipping logging after 1088 examples to avoid logging too frequently
train stats after 1120 examples: {'rewards_train/chosen': '0.011023', 'rewards_train/rejected': '-0.00029338', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011316', 'logps_train/rejected': '-111.34', 'logps_train/chosen': '-92.854', 'loss/train': '0.68826', 'examples_per_second': '47.231', 'grad_norm': '19.134', 'counters/examples': 1120, 'counters/updates': 35}
skipping logging after 1152 examples to avoid logging too frequently
train stats after 1184 examples: {'rewards_train/chosen': '0.078352', 'rewards_train/rejected': '0.029201', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049151', 'logps_train/rejected': '-156.43', 'logps_train/chosen': '-152.93', 'loss/train': '0.66992', 'examples_per_second': '44.68', 'grad_norm': '21.922', 'counters/examples': 1184, 'counters/updates': 37}
skipping logging after 1216 examples to avoid logging too frequently
train stats after 1248 examples: {'rewards_train/chosen': '0.083787', 'rewards_train/rejected': '0.045309', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038478', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-151.07', 'loss/train': '0.67588', 'examples_per_second': '45.627', 'grad_norm': '22.448', 'counters/examples': 1248, 'counters/updates': 39}
skipping logging after 1280 examples to avoid logging too frequently
train stats after 1312 examples: {'rewards_train/chosen': '0.059804', 'rewards_train/rejected': '0.018466', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.041338', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-141.71', 'loss/train': '0.67497', 'examples_per_second': '44.717', 'grad_norm': '22.096', 'counters/examples': 1312, 'counters/updates': 41}
skipping logging after 1344 examples to avoid logging too frequently
train stats after 1376 examples: {'rewards_train/chosen': '0.083702', 'rewards_train/rejected': '0.057008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026694', 'logps_train/rejected': '-162.49', 'logps_train/chosen': '-170.63', 'loss/train': '0.68285', 'examples_per_second': '45.552', 'grad_norm': '25.624', 'counters/examples': 1376, 'counters/updates': 43}
skipping logging after 1408 examples to avoid logging too frequently
train stats after 1440 examples: {'rewards_train/chosen': '0.061377', 'rewards_train/rejected': '-0.025188', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.086565', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-136.59', 'loss/train': '0.65237', 'examples_per_second': '45.062', 'grad_norm': '21.841', 'counters/examples': 1440, 'counters/updates': 45}
skipping logging after 1472 examples to avoid logging too frequently
train stats after 1504 examples: {'rewards_train/chosen': '0.055261', 'rewards_train/rejected': '-0.0050516', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060312', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-124.13', 'loss/train': '0.66666', 'examples_per_second': '46.213', 'grad_norm': '20.082', 'counters/examples': 1504, 'counters/updates': 47}
skipping logging after 1536 examples to avoid logging too frequently
train stats after 1568 examples: {'rewards_train/chosen': '0.072797', 'rewards_train/rejected': '0.040551', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032246', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-112.72', 'loss/train': '0.67929', 'examples_per_second': '45.242', 'grad_norm': '20.432', 'counters/examples': 1568, 'counters/updates': 49}
skipping logging after 1600 examples to avoid logging too frequently
train stats after 1632 examples: {'rewards_train/chosen': '0.068873', 'rewards_train/rejected': '-0.022202', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091075', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-125.32', 'loss/train': '0.6529', 'examples_per_second': '53.083', 'grad_norm': '19.968', 'counters/examples': 1632, 'counters/updates': 51}
skipping logging after 1664 examples to avoid logging too frequently
train stats after 1696 examples: {'rewards_train/chosen': '0.029914', 'rewards_train/rejected': '-0.04473', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074644', 'logps_train/rejected': '-102.13', 'logps_train/chosen': '-111.09', 'loss/train': '0.66107', 'examples_per_second': '59.662', 'grad_norm': '20.083', 'counters/examples': 1696, 'counters/updates': 53}
skipping logging after 1728 examples to avoid logging too frequently
train stats after 1760 examples: {'rewards_train/chosen': '0.091484', 'rewards_train/rejected': '-0.023285', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11477', 'logps_train/rejected': '-100.65', 'logps_train/chosen': '-168.25', 'loss/train': '0.64224', 'examples_per_second': '45.061', 'grad_norm': '22.019', 'counters/examples': 1760, 'counters/updates': 55}
skipping logging after 1792 examples to avoid logging too frequently
train stats after 1824 examples: {'rewards_train/chosen': '0.023186', 'rewards_train/rejected': '-0.013775', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036961', 'logps_train/rejected': '-147.64', 'logps_train/chosen': '-152.86', 'loss/train': '0.67963', 'examples_per_second': '45.634', 'grad_norm': '23.66', 'counters/examples': 1824, 'counters/updates': 57}
skipping logging after 1856 examples to avoid logging too frequently
train stats after 1888 examples: {'rewards_train/chosen': '0.044146', 'rewards_train/rejected': '-0.033078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077224', 'logps_train/rejected': '-99.491', 'logps_train/chosen': '-109.84', 'loss/train': '0.65957', 'examples_per_second': '44.564', 'grad_norm': '18.864', 'counters/examples': 1888, 'counters/updates': 59}
skipping logging after 1920 examples to avoid logging too frequently
train stats after 1952 examples: {'rewards_train/chosen': '0.081492', 'rewards_train/rejected': '-0.077826', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15932', 'logps_train/rejected': '-109.89', 'logps_train/chosen': '-104.8', 'loss/train': '0.62882', 'examples_per_second': '48.88', 'grad_norm': '19.376', 'counters/examples': 1952, 'counters/updates': 61}
skipping logging after 1984 examples to avoid logging too frequently
train stats after 2016 examples: {'rewards_train/chosen': '0.070949', 'rewards_train/rejected': '-0.042996', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11395', 'logps_train/rejected': '-160.27', 'logps_train/chosen': '-109.97', 'loss/train': '0.64896', 'examples_per_second': '45.221', 'grad_norm': '21.583', 'counters/examples': 2016, 'counters/updates': 63}
skipping logging after 2048 examples to avoid logging too frequently
train stats after 2080 examples: {'rewards_train/chosen': '0.038996', 'rewards_train/rejected': '-0.12918', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.16818', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-121.5', 'loss/train': '0.61713', 'examples_per_second': '44.666', 'grad_norm': '21.42', 'counters/examples': 2080, 'counters/updates': 65}
skipping logging after 2112 examples to avoid logging too frequently
train stats after 2144 examples: {'rewards_train/chosen': '0.029526', 'rewards_train/rejected': '-0.14197', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17149', 'logps_train/rejected': '-121.9', 'logps_train/chosen': '-110.45', 'loss/train': '0.6289', 'examples_per_second': '45.942', 'grad_norm': '19.745', 'counters/examples': 2144, 'counters/updates': 67}
skipping logging after 2176 examples to avoid logging too frequently
train stats after 2208 examples: {'rewards_train/chosen': '-0.0068146', 'rewards_train/rejected': '-0.18177', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17496', 'logps_train/rejected': '-135.68', 'logps_train/chosen': '-133.45', 'loss/train': '0.6225', 'examples_per_second': '45.271', 'grad_norm': '21.476', 'counters/examples': 2208, 'counters/updates': 69}
skipping logging after 2240 examples to avoid logging too frequently
train stats after 2272 examples: {'rewards_train/chosen': '-0.074724', 'rewards_train/rejected': '-0.29222', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.2175', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-129.96', 'loss/train': '0.60564', 'examples_per_second': '45.852', 'grad_norm': '21.464', 'counters/examples': 2272, 'counters/updates': 71}
skipping logging after 2304 examples to avoid logging too frequently
train stats after 2336 examples: {'rewards_train/chosen': '-0.11971', 'rewards_train/rejected': '-0.23068', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11097', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-136.14', 'loss/train': '0.65319', 'examples_per_second': '45.165', 'grad_norm': '23.226', 'counters/examples': 2336, 'counters/updates': 73}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2400 examples: {'rewards_train/chosen': '-0.10749', 'rewards_train/rejected': '-0.26369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1562', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-154.39', 'loss/train': '0.64297', 'examples_per_second': '44.532', 'grad_norm': '24.099', 'counters/examples': 2400, 'counters/updates': 75}
skipping logging after 2432 examples to avoid logging too frequently
train stats after 2464 examples: {'rewards_train/chosen': '-0.051582', 'rewards_train/rejected': '-0.29153', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23995', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-123.96', 'loss/train': '0.60835', 'examples_per_second': '46.881', 'grad_norm': '21.104', 'counters/examples': 2464, 'counters/updates': 77}
skipping logging after 2496 examples to avoid logging too frequently
train stats after 2528 examples: {'rewards_train/chosen': '-0.076494', 'rewards_train/rejected': '-0.1252', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048709', 'logps_train/rejected': '-119.66', 'logps_train/chosen': '-155.36', 'loss/train': '0.6975', 'examples_per_second': '44.865', 'grad_norm': '25.988', 'counters/examples': 2528, 'counters/updates': 79}
skipping logging after 2560 examples to avoid logging too frequently
train stats after 2592 examples: {'rewards_train/chosen': '-0.14678', 'rewards_train/rejected': '-0.2645', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11772', 'logps_train/rejected': '-90.884', 'logps_train/chosen': '-142.38', 'loss/train': '0.65401', 'examples_per_second': '49.675', 'grad_norm': '22.717', 'counters/examples': 2592, 'counters/updates': 81}
skipping logging after 2624 examples to avoid logging too frequently
train stats after 2656 examples: {'rewards_train/chosen': '-0.11453', 'rewards_train/rejected': '-0.055816', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.058709', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-136.1', 'loss/train': '0.73888', 'examples_per_second': '44.833', 'grad_norm': '25.81', 'counters/examples': 2656, 'counters/updates': 83}
skipping logging after 2688 examples to avoid logging too frequently
train stats after 2720 examples: {'rewards_train/chosen': '-0.019953', 'rewards_train/rejected': '-0.27145', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25149', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-151.39', 'loss/train': '0.59601', 'examples_per_second': '46.044', 'grad_norm': '22.885', 'counters/examples': 2720, 'counters/updates': 85}
skipping logging after 2752 examples to avoid logging too frequently
train stats after 2784 examples: {'rewards_train/chosen': '-0.019218', 'rewards_train/rejected': '-0.32992', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.31071', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-131.92', 'loss/train': '0.58687', 'examples_per_second': '43.814', 'grad_norm': '21.08', 'counters/examples': 2784, 'counters/updates': 87}
skipping logging after 2816 examples to avoid logging too frequently
train stats after 2848 examples: {'rewards_train/chosen': '-0.18031', 'rewards_train/rejected': '-0.27285', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09254', 'logps_train/rejected': '-101.27', 'logps_train/chosen': '-155.72', 'loss/train': '0.68031', 'examples_per_second': '45.477', 'grad_norm': '25.382', 'counters/examples': 2848, 'counters/updates': 89}
skipping logging after 2880 examples to avoid logging too frequently
train stats after 2912 examples: {'rewards_train/chosen': '-0.010972', 'rewards_train/rejected': '-0.23467', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2237', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-137.94', 'loss/train': '0.60379', 'examples_per_second': '47.379', 'grad_norm': '20.896', 'counters/examples': 2912, 'counters/updates': 91}
skipping logging after 2944 examples to avoid logging too frequently
train stats after 2976 examples: {'rewards_train/chosen': '0.00063381', 'rewards_train/rejected': '-0.20769', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20832', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-133.34', 'loss/train': '0.60719', 'examples_per_second': '46.106', 'grad_norm': '21.771', 'counters/examples': 2976, 'counters/updates': 93}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3040 examples: {'rewards_train/chosen': '-0.033292', 'rewards_train/rejected': '-0.23918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20589', 'logps_train/rejected': '-108.64', 'logps_train/chosen': '-116.5', 'loss/train': '0.63415', 'examples_per_second': '46.307', 'grad_norm': '19.567', 'counters/examples': 3040, 'counters/updates': 95}
skipping logging after 3072 examples to avoid logging too frequently
train stats after 3104 examples: {'rewards_train/chosen': '-0.062969', 'rewards_train/rejected': '-0.2452', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18223', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-127.38', 'loss/train': '0.6607', 'examples_per_second': '46.992', 'grad_norm': '23.614', 'counters/examples': 3104, 'counters/updates': 97}
skipping logging after 3136 examples to avoid logging too frequently
train stats after 3168 examples: {'rewards_train/chosen': '-0.090268', 'rewards_train/rejected': '-0.3406', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.25033', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-111.39', 'loss/train': '0.62577', 'examples_per_second': '45.607', 'grad_norm': '24.063', 'counters/examples': 3168, 'counters/updates': 99}
skipping logging after 3200 examples to avoid logging too frequently
train stats after 3232 examples: {'rewards_train/chosen': '0.14059', 'rewards_train/rejected': '-0.16323', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.30381', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-154.4', 'loss/train': '0.6105', 'examples_per_second': '44.173', 'grad_norm': '22.141', 'counters/examples': 3232, 'counters/updates': 101}
skipping logging after 3264 examples to avoid logging too frequently
train stats after 3296 examples: {'rewards_train/chosen': '-0.050279', 'rewards_train/rejected': '-0.33585', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28557', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-144.49', 'loss/train': '0.60595', 'examples_per_second': '45.288', 'grad_norm': '24.559', 'counters/examples': 3296, 'counters/updates': 103}
skipping logging after 3328 examples to avoid logging too frequently
train stats after 3360 examples: {'rewards_train/chosen': '0.1224', 'rewards_train/rejected': '-0.37565', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.49805', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-121.7', 'loss/train': '0.58814', 'examples_per_second': '46.943', 'grad_norm': '18.543', 'counters/examples': 3360, 'counters/updates': 105}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3424 examples: {'rewards_train/chosen': '-0.0022277', 'rewards_train/rejected': '-0.33739', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33517', 'logps_train/rejected': '-118.47', 'logps_train/chosen': '-157', 'loss/train': '0.59506', 'examples_per_second': '48.313', 'grad_norm': '21.075', 'counters/examples': 3424, 'counters/updates': 107}
skipping logging after 3456 examples to avoid logging too frequently
train stats after 3488 examples: {'rewards_train/chosen': '0.043904', 'rewards_train/rejected': '-0.33008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37399', 'logps_train/rejected': '-161.71', 'logps_train/chosen': '-152.76', 'loss/train': '0.62472', 'examples_per_second': '44.34', 'grad_norm': '26.662', 'counters/examples': 3488, 'counters/updates': 109}
skipping logging after 3520 examples to avoid logging too frequently
train stats after 3552 examples: {'rewards_train/chosen': '0.30587', 'rewards_train/rejected': '-0.025137', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.331', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-138.9', 'loss/train': '0.59207', 'examples_per_second': '45.695', 'grad_norm': '22.687', 'counters/examples': 3552, 'counters/updates': 111}
skipping logging after 3584 examples to avoid logging too frequently
train stats after 3616 examples: {'rewards_train/chosen': '0.016702', 'rewards_train/rejected': '-0.19503', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21173', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-142.81', 'loss/train': '0.64615', 'examples_per_second': '46.444', 'grad_norm': '23.223', 'counters/examples': 3616, 'counters/updates': 113}
skipping logging after 3648 examples to avoid logging too frequently
train stats after 3680 examples: {'rewards_train/chosen': '0.09726', 'rewards_train/rejected': '0.015876', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081383', 'logps_train/rejected': '-112.04', 'logps_train/chosen': '-158.85', 'loss/train': '0.69991', 'examples_per_second': '44.614', 'grad_norm': '27.028', 'counters/examples': 3680, 'counters/updates': 115}
skipping logging after 3712 examples to avoid logging too frequently
train stats after 3744 examples: {'rewards_train/chosen': '0.37113', 'rewards_train/rejected': '-0.0064835', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37762', 'logps_train/rejected': '-143.84', 'logps_train/chosen': '-126.09', 'loss/train': '0.594', 'examples_per_second': '45.852', 'grad_norm': '21.126', 'counters/examples': 3744, 'counters/updates': 117}
skipping logging after 3776 examples to avoid logging too frequently
train stats after 3808 examples: {'rewards_train/chosen': '0.18241', 'rewards_train/rejected': '-0.23172', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.41413', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-120.8', 'loss/train': '0.56398', 'examples_per_second': '45.734', 'grad_norm': '21.899', 'counters/examples': 3808, 'counters/updates': 119}
skipping logging after 3840 examples to avoid logging too frequently
train stats after 3872 examples: {'rewards_train/chosen': '0.12759', 'rewards_train/rejected': '0.026827', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10076', 'logps_train/rejected': '-120.63', 'logps_train/chosen': '-127.87', 'loss/train': '0.72222', 'examples_per_second': '44.468', 'grad_norm': '28.202', 'counters/examples': 3872, 'counters/updates': 121}
skipping logging after 3904 examples to avoid logging too frequently
train stats after 3936 examples: {'rewards_train/chosen': '0.10842', 'rewards_train/rejected': '-0.26862', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.37704', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-143.37', 'loss/train': '0.59178', 'examples_per_second': '44.743', 'grad_norm': '20.554', 'counters/examples': 3936, 'counters/updates': 123}
skipping logging after 3968 examples to avoid logging too frequently
train stats after 4000 examples: {'rewards_train/chosen': '0.060491', 'rewards_train/rejected': '-0.26079', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.32129', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-127.97', 'loss/train': '0.59656', 'examples_per_second': '48.415', 'grad_norm': '21.8', 'counters/examples': 4000, 'counters/updates': 125}
skipping logging after 4032 examples to avoid logging too frequently
train stats after 4064 examples: {'rewards_train/chosen': '-0.055251', 'rewards_train/rejected': '-0.36566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31041', 'logps_train/rejected': '-113.02', 'logps_train/chosen': '-133.94', 'loss/train': '0.63205', 'examples_per_second': '51.685', 'grad_norm': '23.173', 'counters/examples': 4064, 'counters/updates': 127}
skipping logging after 4096 examples to avoid logging too frequently
train stats after 4128 examples: {'rewards_train/chosen': '0.1033', 'rewards_train/rejected': '-0.17103', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27433', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-167.13', 'loss/train': '0.61232', 'examples_per_second': '45.605', 'grad_norm': '23.856', 'counters/examples': 4128, 'counters/updates': 129}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4192 examples: {'rewards_train/chosen': '0.054168', 'rewards_train/rejected': '-0.19745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25162', 'logps_train/rejected': '-146.66', 'logps_train/chosen': '-109.58', 'loss/train': '0.61276', 'examples_per_second': '45.682', 'grad_norm': '22.64', 'counters/examples': 4192, 'counters/updates': 131}
skipping logging after 4224 examples to avoid logging too frequently
train stats after 4256 examples: {'rewards_train/chosen': '-0.0086326', 'rewards_train/rejected': '-0.37502', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.36639', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-128.71', 'loss/train': '0.56829', 'examples_per_second': '44.441', 'grad_norm': '21.345', 'counters/examples': 4256, 'counters/updates': 133}
skipping logging after 4288 examples to avoid logging too frequently
train stats after 4320 examples: {'rewards_train/chosen': '-0.13524', 'rewards_train/rejected': '-0.30672', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17148', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-153.89', 'loss/train': '0.65803', 'examples_per_second': '52.097', 'grad_norm': '26.841', 'counters/examples': 4320, 'counters/updates': 135}
skipping logging after 4352 examples to avoid logging too frequently
train stats after 4384 examples: {'rewards_train/chosen': '-0.011724', 'rewards_train/rejected': '-0.1874', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17567', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-135.06', 'loss/train': '0.65167', 'examples_per_second': '52.244', 'grad_norm': '25.346', 'counters/examples': 4384, 'counters/updates': 137}
skipping logging after 4416 examples to avoid logging too frequently
train stats after 4448 examples: {'rewards_train/chosen': '-0.14024', 'rewards_train/rejected': '-0.38027', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24003', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-148.26', 'loss/train': '0.64136', 'examples_per_second': '46.363', 'grad_norm': '26.303', 'counters/examples': 4448, 'counters/updates': 139}
skipping logging after 4480 examples to avoid logging too frequently
train stats after 4512 examples: {'rewards_train/chosen': '0.17965', 'rewards_train/rejected': '-0.35543', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.53508', 'logps_train/rejected': '-103.81', 'logps_train/chosen': '-147.53', 'loss/train': '0.54109', 'examples_per_second': '46.657', 'grad_norm': '22.379', 'counters/examples': 4512, 'counters/updates': 141}
skipping logging after 4544 examples to avoid logging too frequently
train stats after 4576 examples: {'rewards_train/chosen': '0.19898', 'rewards_train/rejected': '-0.19848', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.39746', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-139.07', 'loss/train': '0.61561', 'examples_per_second': '45.995', 'grad_norm': '24.905', 'counters/examples': 4576, 'counters/updates': 143}
skipping logging after 4608 examples to avoid logging too frequently
train stats after 4640 examples: {'rewards_train/chosen': '-0.025012', 'rewards_train/rejected': '-0.20603', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18102', 'logps_train/rejected': '-155.84', 'logps_train/chosen': '-166.32', 'loss/train': '0.70263', 'examples_per_second': '44.681', 'grad_norm': '29.797', 'counters/examples': 4640, 'counters/updates': 145}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4704 examples: {'rewards_train/chosen': '0.2992', 'rewards_train/rejected': '-0.16107', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.46027', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-155.81', 'loss/train': '0.55144', 'examples_per_second': '45.767', 'grad_norm': '25.101', 'counters/examples': 4704, 'counters/updates': 147}
skipping logging after 4736 examples to avoid logging too frequently
train stats after 4768 examples: {'rewards_train/chosen': '0.23986', 'rewards_train/rejected': '-0.14619', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38605', 'logps_train/rejected': '-105.08', 'logps_train/chosen': '-121.64', 'loss/train': '0.57308', 'examples_per_second': '47.383', 'grad_norm': '19.874', 'counters/examples': 4768, 'counters/updates': 149}
skipping logging after 4800 examples to avoid logging too frequently
train stats after 4832 examples: {'rewards_train/chosen': '0.01551', 'rewards_train/rejected': '-0.501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.51651', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-161.02', 'loss/train': '0.55882', 'examples_per_second': '46.784', 'grad_norm': '22.978', 'counters/examples': 4832, 'counters/updates': 151}
skipping logging after 4864 examples to avoid logging too frequently
train stats after 4896 examples: {'rewards_train/chosen': '-0.33486', 'rewards_train/rejected': '-0.43023', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095376', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-139', 'loss/train': '0.69016', 'examples_per_second': '45.578', 'grad_norm': '24.588', 'counters/examples': 4896, 'counters/updates': 153}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4960 examples: {'rewards_train/chosen': '0.28273', 'rewards_train/rejected': '-0.085973', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.3687', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-154.51', 'loss/train': '0.60573', 'examples_per_second': '45.787', 'grad_norm': '22.798', 'counters/examples': 4960, 'counters/updates': 155}
skipping logging after 4992 examples to avoid logging too frequently
train stats after 5024 examples: {'rewards_train/chosen': '-0.020137', 'rewards_train/rejected': '-0.31657', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29644', 'logps_train/rejected': '-147.11', 'logps_train/chosen': '-116.95', 'loss/train': '0.63154', 'examples_per_second': '45.785', 'grad_norm': '21.695', 'counters/examples': 5024, 'counters/updates': 157}
skipping logging after 5056 examples to avoid logging too frequently
train stats after 5088 examples: {'rewards_train/chosen': '0.31352', 'rewards_train/rejected': '-0.36492', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.67844', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-113.94', 'loss/train': '0.51095', 'examples_per_second': '47.578', 'grad_norm': '18.809', 'counters/examples': 5088, 'counters/updates': 159}
skipping logging after 5120 examples to avoid logging too frequently
train stats after 5152 examples: {'rewards_train/chosen': '0.36077', 'rewards_train/rejected': '0.002352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35842', 'logps_train/rejected': '-122', 'logps_train/chosen': '-175.77', 'loss/train': '0.6014', 'examples_per_second': '44.739', 'grad_norm': '22.705', 'counters/examples': 5152, 'counters/updates': 161}
skipping logging after 5184 examples to avoid logging too frequently
train stats after 5216 examples: {'rewards_train/chosen': '0.12028', 'rewards_train/rejected': '0.085169', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035113', 'logps_train/rejected': '-110.17', 'logps_train/chosen': '-140.2', 'loss/train': '0.73214', 'examples_per_second': '45.425', 'grad_norm': '26.346', 'counters/examples': 5216, 'counters/updates': 163}
skipping logging after 5248 examples to avoid logging too frequently
train stats after 5280 examples: {'rewards_train/chosen': '0.25289', 'rewards_train/rejected': '-0.14292', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.39581', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-141.53', 'loss/train': '0.57462', 'examples_per_second': '45.634', 'grad_norm': '23.861', 'counters/examples': 5280, 'counters/updates': 165}
skipping logging after 5312 examples to avoid logging too frequently
train stats after 5344 examples: {'rewards_train/chosen': '0.14315', 'rewards_train/rejected': '-0.12924', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27239', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-174.49', 'loss/train': '0.63345', 'examples_per_second': '45.659', 'grad_norm': '27.752', 'counters/examples': 5344, 'counters/updates': 167}
skipping logging after 5376 examples to avoid logging too frequently
train stats after 5408 examples: {'rewards_train/chosen': '0.23609', 'rewards_train/rejected': '0.084949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15114', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-118.92', 'loss/train': '0.67109', 'examples_per_second': '45.839', 'grad_norm': '24.252', 'counters/examples': 5408, 'counters/updates': 169}
skipping logging after 5440 examples to avoid logging too frequently
train stats after 5472 examples: {'rewards_train/chosen': '0.25761', 'rewards_train/rejected': '-0.1158', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.37342', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-154.05', 'loss/train': '0.55888', 'examples_per_second': '47.088', 'grad_norm': '22.576', 'counters/examples': 5472, 'counters/updates': 171}
skipping logging after 5504 examples to avoid logging too frequently
train stats after 5536 examples: {'rewards_train/chosen': '-0.01997', 'rewards_train/rejected': '-0.25068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23071', 'logps_train/rejected': '-99.066', 'logps_train/chosen': '-91.816', 'loss/train': '0.66336', 'examples_per_second': '45.906', 'grad_norm': '21.379', 'counters/examples': 5536, 'counters/updates': 173}
skipping logging after 5568 examples to avoid logging too frequently
train stats after 5600 examples: {'rewards_train/chosen': '-0.039727', 'rewards_train/rejected': '-0.51092', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.4712', 'logps_train/rejected': '-100.84', 'logps_train/chosen': '-121.76', 'loss/train': '0.56581', 'examples_per_second': '50.465', 'grad_norm': '20.209', 'counters/examples': 5600, 'counters/updates': 175}
skipping logging after 5632 examples to avoid logging too frequently
train stats after 5664 examples: {'rewards_train/chosen': '-0.021346', 'rewards_train/rejected': '-0.50144', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.4801', 'logps_train/rejected': '-169.47', 'logps_train/chosen': '-124.45', 'loss/train': '0.56361', 'examples_per_second': '45.126', 'grad_norm': '23.217', 'counters/examples': 5664, 'counters/updates': 177}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5728 examples: {'rewards_train/chosen': '-0.0019642', 'rewards_train/rejected': '-0.41585', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41389', 'logps_train/rejected': '-98.053', 'logps_train/chosen': '-154.48', 'loss/train': '0.57937', 'examples_per_second': '46.404', 'grad_norm': '21.987', 'counters/examples': 5728, 'counters/updates': 179}
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5792 examples: {'rewards_train/chosen': '0.2228', 'rewards_train/rejected': '-0.27636', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49916', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-167.67', 'loss/train': '0.56106', 'examples_per_second': '45.648', 'grad_norm': '23.449', 'counters/examples': 5792, 'counters/updates': 181}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5856 examples: {'rewards_train/chosen': '-0.20875', 'rewards_train/rejected': '-0.56262', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.35386', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-130.62', 'loss/train': '0.60233', 'examples_per_second': '45.568', 'grad_norm': '25.505', 'counters/examples': 5856, 'counters/updates': 183}
skipping logging after 5888 examples to avoid logging too frequently
train stats after 5920 examples: {'rewards_train/chosen': '-0.094571', 'rewards_train/rejected': '-0.35345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25888', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-137.75', 'loss/train': '0.62857', 'examples_per_second': '49.766', 'grad_norm': '23.637', 'counters/examples': 5920, 'counters/updates': 185}
skipping logging after 5952 examples to avoid logging too frequently
train stats after 5984 examples: {'rewards_train/chosen': '-0.31167', 'rewards_train/rejected': '-0.36013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048463', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-102.32', 'loss/train': '0.75254', 'examples_per_second': '45.009', 'grad_norm': '26.434', 'counters/examples': 5984, 'counters/updates': 187}
skipping logging after 6016 examples to avoid logging too frequently
train stats after 6048 examples: {'rewards_train/chosen': '-0.45689', 'rewards_train/rejected': '-0.30383', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.15306', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-162.53', 'loss/train': '0.83058', 'examples_per_second': '45.788', 'grad_norm': '29.756', 'counters/examples': 6048, 'counters/updates': 189}
skipping logging after 6080 examples to avoid logging too frequently
train stats after 6112 examples: {'rewards_train/chosen': '-0.14543', 'rewards_train/rejected': '-0.56995', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42452', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-146.62', 'loss/train': '0.57565', 'examples_per_second': '53.633', 'grad_norm': '22.555', 'counters/examples': 6112, 'counters/updates': 191}
skipping logging after 6144 examples to avoid logging too frequently
train stats after 6176 examples: {'rewards_train/chosen': '-0.11599', 'rewards_train/rejected': '-0.42258', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3066', 'logps_train/rejected': '-124.05', 'logps_train/chosen': '-129.92', 'loss/train': '0.58639', 'examples_per_second': '47.578', 'grad_norm': '22.769', 'counters/examples': 6176, 'counters/updates': 193}
skipping logging after 6208 examples to avoid logging too frequently
train stats after 6240 examples: {'rewards_train/chosen': '-0.087554', 'rewards_train/rejected': '-0.4647', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.37715', 'logps_train/rejected': '-112.72', 'logps_train/chosen': '-160.3', 'loss/train': '0.56811', 'examples_per_second': '47.686', 'grad_norm': '22.474', 'counters/examples': 6240, 'counters/updates': 195}
skipping logging after 6272 examples to avoid logging too frequently
train stats after 6304 examples: {'rewards_train/chosen': '-0.12228', 'rewards_train/rejected': '-0.47564', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35337', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-156.53', 'loss/train': '0.60797', 'examples_per_second': '45.467', 'grad_norm': '24.756', 'counters/examples': 6304, 'counters/updates': 197}
skipping logging after 6336 examples to avoid logging too frequently
train stats after 6368 examples: {'rewards_train/chosen': '-0.05043', 'rewards_train/rejected': '-0.37747', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.32704', 'logps_train/rejected': '-120.7', 'logps_train/chosen': '-150.97', 'loss/train': '0.62202', 'examples_per_second': '45.434', 'grad_norm': '24.915', 'counters/examples': 6368, 'counters/updates': 199}
skipping logging after 6400 examples to avoid logging too frequently
train stats after 6432 examples: {'rewards_train/chosen': '-0.15629', 'rewards_train/rejected': '-0.53167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37539', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-150.58', 'loss/train': '0.59247', 'examples_per_second': '45.545', 'grad_norm': '21.48', 'counters/examples': 6432, 'counters/updates': 201}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6496 examples: {'rewards_train/chosen': '-0.055451', 'rewards_train/rejected': '-0.17345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.118', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-130.13', 'loss/train': '0.7092', 'examples_per_second': '44.432', 'grad_norm': '24.378', 'counters/examples': 6496, 'counters/updates': 203}
skipping logging after 6528 examples to avoid logging too frequently
train stats after 6560 examples: {'rewards_train/chosen': '-0.062153', 'rewards_train/rejected': '-0.48372', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42156', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-111.23', 'loss/train': '0.58478', 'examples_per_second': '46.785', 'grad_norm': '21.307', 'counters/examples': 6560, 'counters/updates': 205}
skipping logging after 6592 examples to avoid logging too frequently
train stats after 6624 examples: {'rewards_train/chosen': '-0.052057', 'rewards_train/rejected': '-0.34747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29541', 'logps_train/rejected': '-90.939', 'logps_train/chosen': '-122.03', 'loss/train': '0.63251', 'examples_per_second': '52.527', 'grad_norm': '21.863', 'counters/examples': 6624, 'counters/updates': 207}
skipping logging after 6656 examples to avoid logging too frequently
train stats after 6688 examples: {'rewards_train/chosen': '-0.23205', 'rewards_train/rejected': '-0.26432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032271', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-155.07', 'loss/train': '0.76513', 'examples_per_second': '45.611', 'grad_norm': '28.248', 'counters/examples': 6688, 'counters/updates': 209}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6752 examples: {'rewards_train/chosen': '-0.18764', 'rewards_train/rejected': '-0.53434', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.3467', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-138.25', 'loss/train': '0.60042', 'examples_per_second': '44.96', 'grad_norm': '23.027', 'counters/examples': 6752, 'counters/updates': 211}
skipping logging after 6784 examples to avoid logging too frequently
train stats after 6816 examples: {'rewards_train/chosen': '0.022159', 'rewards_train/rejected': '-0.43698', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45914', 'logps_train/rejected': '-119.87', 'logps_train/chosen': '-138.02', 'loss/train': '0.58683', 'examples_per_second': '45.646', 'grad_norm': '23.705', 'counters/examples': 6816, 'counters/updates': 213}
skipping logging after 6848 examples to avoid logging too frequently
train stats after 6880 examples: {'rewards_train/chosen': '-0.06461', 'rewards_train/rejected': '-0.039775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024835', 'logps_train/rejected': '-146.39', 'logps_train/chosen': '-165.08', 'loss/train': '0.76292', 'examples_per_second': '44.404', 'grad_norm': '28.418', 'counters/examples': 6880, 'counters/updates': 215}
skipping logging after 6912 examples to avoid logging too frequently
train stats after 6944 examples: {'rewards_train/chosen': '-0.21569', 'rewards_train/rejected': '-0.46554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24985', 'logps_train/rejected': '-134.95', 'logps_train/chosen': '-168.65', 'loss/train': '0.66374', 'examples_per_second': '47.984', 'grad_norm': '28.022', 'counters/examples': 6944, 'counters/updates': 217}
skipping logging after 6976 examples to avoid logging too frequently
train stats after 7008 examples: {'rewards_train/chosen': '-0.29073', 'rewards_train/rejected': '-0.48195', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19122', 'logps_train/rejected': '-126.9', 'logps_train/chosen': '-151.44', 'loss/train': '0.66462', 'examples_per_second': '45.734', 'grad_norm': '26.328', 'counters/examples': 7008, 'counters/updates': 219}
skipping logging after 7040 examples to avoid logging too frequently
train stats after 7072 examples: {'rewards_train/chosen': '-0.35903', 'rewards_train/rejected': '-0.67381', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31477', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-139.47', 'loss/train': '0.60878', 'examples_per_second': '44.948', 'grad_norm': '22.531', 'counters/examples': 7072, 'counters/updates': 221}
skipping logging after 7104 examples to avoid logging too frequently
train stats after 7136 examples: {'rewards_train/chosen': '-0.19047', 'rewards_train/rejected': '-0.39535', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20488', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-169.13', 'loss/train': '0.64433', 'examples_per_second': '45.763', 'grad_norm': '25.92', 'counters/examples': 7136, 'counters/updates': 223}
skipping logging after 7168 examples to avoid logging too frequently
train stats after 7200 examples: {'rewards_train/chosen': '-0.011726', 'rewards_train/rejected': '-0.43885', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42712', 'logps_train/rejected': '-95.393', 'logps_train/chosen': '-139.35', 'loss/train': '0.58144', 'examples_per_second': '47.081', 'grad_norm': '19.797', 'counters/examples': 7200, 'counters/updates': 225}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7264 examples: {'rewards_train/chosen': '0.0019617', 'rewards_train/rejected': '-0.20782', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20978', 'logps_train/rejected': '-116.16', 'logps_train/chosen': '-148.28', 'loss/train': '0.65052', 'examples_per_second': '45.867', 'grad_norm': '23.456', 'counters/examples': 7264, 'counters/updates': 227}
skipping logging after 7296 examples to avoid logging too frequently
train stats after 7328 examples: {'rewards_train/chosen': '0.013141', 'rewards_train/rejected': '-0.56826', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5814', 'logps_train/rejected': '-108.52', 'logps_train/chosen': '-162.09', 'loss/train': '0.53211', 'examples_per_second': '47.857', 'grad_norm': '20.719', 'counters/examples': 7328, 'counters/updates': 229}
skipping logging after 7360 examples to avoid logging too frequently
train stats after 7392 examples: {'rewards_train/chosen': '-0.092981', 'rewards_train/rejected': '-0.39771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30473', 'logps_train/rejected': '-88.379', 'logps_train/chosen': '-126.06', 'loss/train': '0.59874', 'examples_per_second': '52.892', 'grad_norm': '21.788', 'counters/examples': 7392, 'counters/updates': 231}
skipping logging after 7424 examples to avoid logging too frequently
train stats after 7456 examples: {'rewards_train/chosen': '-0.12185', 'rewards_train/rejected': '-0.2611', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13925', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-146.04', 'loss/train': '0.69378', 'examples_per_second': '46.916', 'grad_norm': '28.044', 'counters/examples': 7456, 'counters/updates': 233}
skipping logging after 7488 examples to avoid logging too frequently
train stats after 7520 examples: {'rewards_train/chosen': '0.19959', 'rewards_train/rejected': '-0.21488', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41447', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-99.45', 'loss/train': '0.56712', 'examples_per_second': '44.873', 'grad_norm': '20.882', 'counters/examples': 7520, 'counters/updates': 235}
skipping logging after 7552 examples to avoid logging too frequently
train stats after 7584 examples: {'rewards_train/chosen': '0.13081', 'rewards_train/rejected': '-0.080522', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21133', 'logps_train/rejected': '-146.68', 'logps_train/chosen': '-125.48', 'loss/train': '0.67239', 'examples_per_second': '45.572', 'grad_norm': '24.994', 'counters/examples': 7584, 'counters/updates': 237}
skipping logging after 7616 examples to avoid logging too frequently
train stats after 7648 examples: {'rewards_train/chosen': '0.1332', 'rewards_train/rejected': '-0.034373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16757', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-139.85', 'loss/train': '0.67851', 'examples_per_second': '47.044', 'grad_norm': '24.962', 'counters/examples': 7648, 'counters/updates': 239}
skipping logging after 7680 examples to avoid logging too frequently
train stats after 7712 examples: {'rewards_train/chosen': '0.036458', 'rewards_train/rejected': '-0.21043', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24689', 'logps_train/rejected': '-91.278', 'logps_train/chosen': '-133.78', 'loss/train': '0.62032', 'examples_per_second': '48.538', 'grad_norm': '22.273', 'counters/examples': 7712, 'counters/updates': 241}
skipping logging after 7744 examples to avoid logging too frequently
train stats after 7776 examples: {'rewards_train/chosen': '0.12171', 'rewards_train/rejected': '-0.43085', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55256', 'logps_train/rejected': '-104.1', 'logps_train/chosen': '-156.39', 'loss/train': '0.55729', 'examples_per_second': '48.945', 'grad_norm': '21.922', 'counters/examples': 7776, 'counters/updates': 243}
skipping logging after 7808 examples to avoid logging too frequently
train stats after 7840 examples: {'rewards_train/chosen': '-0.039452', 'rewards_train/rejected': '-0.43128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39183', 'logps_train/rejected': '-156.9', 'logps_train/chosen': '-149.04', 'loss/train': '0.63032', 'examples_per_second': '45.818', 'grad_norm': '25.53', 'counters/examples': 7840, 'counters/updates': 245}
skipping logging after 7872 examples to avoid logging too frequently
train stats after 7904 examples: {'rewards_train/chosen': '-0.054816', 'rewards_train/rejected': '-0.13148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076661', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-98.633', 'loss/train': '0.74964', 'examples_per_second': '44.668', 'grad_norm': '23.636', 'counters/examples': 7904, 'counters/updates': 247}
skipping logging after 7936 examples to avoid logging too frequently
train stats after 7968 examples: {'rewards_train/chosen': '0.40518', 'rewards_train/rejected': '0.08595', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31923', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-131.48', 'loss/train': '0.58707', 'examples_per_second': '47.219', 'grad_norm': '21.878', 'counters/examples': 7968, 'counters/updates': 249}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8032 examples: {'rewards_train/chosen': '0.33806', 'rewards_train/rejected': '-0.17898', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.51704', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-138.5', 'loss/train': '0.50558', 'examples_per_second': '45.163', 'grad_norm': '20.822', 'counters/examples': 8032, 'counters/updates': 251}
skipping logging after 8064 examples to avoid logging too frequently
train stats after 8096 examples: {'rewards_train/chosen': '0.092538', 'rewards_train/rejected': '-0.35507', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44761', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-148.72', 'loss/train': '0.57655', 'examples_per_second': '47.993', 'grad_norm': '28.646', 'counters/examples': 8096, 'counters/updates': 253}
skipping logging after 8128 examples to avoid logging too frequently
train stats after 8160 examples: {'rewards_train/chosen': '0.1335', 'rewards_train/rejected': '-0.11344', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24694', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-146.07', 'loss/train': '0.64682', 'examples_per_second': '44.346', 'grad_norm': '26.24', 'counters/examples': 8160, 'counters/updates': 255}
skipping logging after 8192 examples to avoid logging too frequently
train stats after 8224 examples: {'rewards_train/chosen': '0.092838', 'rewards_train/rejected': '-0.3338', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42664', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-130.6', 'loss/train': '0.55885', 'examples_per_second': '46.613', 'grad_norm': '20.713', 'counters/examples': 8224, 'counters/updates': 257}
skipping logging after 8256 examples to avoid logging too frequently
train stats after 8288 examples: {'rewards_train/chosen': '-0.13416', 'rewards_train/rejected': '-0.36929', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23513', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-131.29', 'loss/train': '0.65279', 'examples_per_second': '46.168', 'grad_norm': '21.195', 'counters/examples': 8288, 'counters/updates': 259}
skipping logging after 8320 examples to avoid logging too frequently
train stats after 8352 examples: {'rewards_train/chosen': '-0.13012', 'rewards_train/rejected': '-0.66411', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.534', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-139.37', 'loss/train': '0.54402', 'examples_per_second': '49.838', 'grad_norm': '20.31', 'counters/examples': 8352, 'counters/updates': 261}
skipping logging after 8384 examples to avoid logging too frequently
train stats after 8416 examples: {'rewards_train/chosen': '-0.069564', 'rewards_train/rejected': '-0.57165', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50208', 'logps_train/rejected': '-131.27', 'logps_train/chosen': '-114.93', 'loss/train': '0.56006', 'examples_per_second': '45.589', 'grad_norm': '20.55', 'counters/examples': 8416, 'counters/updates': 263}
skipping logging after 8448 examples to avoid logging too frequently
train stats after 8480 examples: {'rewards_train/chosen': '-0.39877', 'rewards_train/rejected': '-0.49687', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098098', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-147.76', 'loss/train': '0.73896', 'examples_per_second': '45.772', 'grad_norm': '27.716', 'counters/examples': 8480, 'counters/updates': 265}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8544 examples: {'rewards_train/chosen': '-0.036632', 'rewards_train/rejected': '-0.26703', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.2304', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-154.6', 'loss/train': '0.65272', 'examples_per_second': '44.738', 'grad_norm': '24.913', 'counters/examples': 8544, 'counters/updates': 267}
skipping logging after 8576 examples to avoid logging too frequently
train stats after 8608 examples: {'rewards_train/chosen': '-0.31742', 'rewards_train/rejected': '-0.66147', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34404', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-128.67', 'loss/train': '0.59158', 'examples_per_second': '46.515', 'grad_norm': '21.109', 'counters/examples': 8608, 'counters/updates': 269}
skipping logging after 8640 examples to avoid logging too frequently
train stats after 8672 examples: {'rewards_train/chosen': '-0.098658', 'rewards_train/rejected': '-0.65399', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.55533', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-139.84', 'loss/train': '0.49346', 'examples_per_second': '45.756', 'grad_norm': '19.218', 'counters/examples': 8672, 'counters/updates': 271}
skipping logging after 8704 examples to avoid logging too frequently
train stats after 8736 examples: {'rewards_train/chosen': '-0.083214', 'rewards_train/rejected': '-0.60405', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.52083', 'logps_train/rejected': '-99.845', 'logps_train/chosen': '-139.29', 'loss/train': '0.54226', 'examples_per_second': '49.561', 'grad_norm': '18.744', 'counters/examples': 8736, 'counters/updates': 273}
skipping logging after 8768 examples to avoid logging too frequently
train stats after 8800 examples: {'rewards_train/chosen': '-0.38911', 'rewards_train/rejected': '-0.92673', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.53762', 'logps_train/rejected': '-148.91', 'logps_train/chosen': '-138.56', 'loss/train': '0.50316', 'examples_per_second': '45.577', 'grad_norm': '20.037', 'counters/examples': 8800, 'counters/updates': 275}
skipping logging after 8832 examples to avoid logging too frequently
train stats after 8864 examples: {'rewards_train/chosen': '-0.32779', 'rewards_train/rejected': '-0.51166', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18388', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-130.56', 'loss/train': '0.66707', 'examples_per_second': '47.85', 'grad_norm': '26.199', 'counters/examples': 8864, 'counters/updates': 277}
skipping logging after 8896 examples to avoid logging too frequently
train stats after 8928 examples: {'rewards_train/chosen': '-0.26955', 'rewards_train/rejected': '-0.60687', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33731', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-134.01', 'loss/train': '0.61396', 'examples_per_second': '46.529', 'grad_norm': '23.762', 'counters/examples': 8928, 'counters/updates': 279}
skipping logging after 8960 examples to avoid logging too frequently
train stats after 8992 examples: {'rewards_train/chosen': '-0.33021', 'rewards_train/rejected': '-0.85654', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.52633', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-137.43', 'loss/train': '0.49851', 'examples_per_second': '44.406', 'grad_norm': '20.327', 'counters/examples': 8992, 'counters/updates': 281}
skipping logging after 9024 examples to avoid logging too frequently
train stats after 9056 examples: {'rewards_train/chosen': '-0.18314', 'rewards_train/rejected': '-0.71324', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.53009', 'logps_train/rejected': '-115.05', 'logps_train/chosen': '-141.57', 'loss/train': '0.53265', 'examples_per_second': '53.468', 'grad_norm': '21.629', 'counters/examples': 9056, 'counters/updates': 283}
skipping logging after 9088 examples to avoid logging too frequently
train stats after 9120 examples: {'rewards_train/chosen': '-0.19543', 'rewards_train/rejected': '-0.45608', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26065', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-141.77', 'loss/train': '0.66934', 'examples_per_second': '45.584', 'grad_norm': '25.596', 'counters/examples': 9120, 'counters/updates': 285}
skipping logging after 9152 examples to avoid logging too frequently
train stats after 9184 examples: {'rewards_train/chosen': '-0.24653', 'rewards_train/rejected': '-0.50537', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25884', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-145.48', 'loss/train': '0.63346', 'examples_per_second': '45.019', 'grad_norm': '25.666', 'counters/examples': 9184, 'counters/updates': 287}
skipping logging after 9216 examples to avoid logging too frequently
train stats after 9248 examples: {'rewards_train/chosen': '-0.37366', 'rewards_train/rejected': '-0.77572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40206', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-178.24', 'loss/train': '0.58053', 'examples_per_second': '45.643', 'grad_norm': '26.871', 'counters/examples': 9248, 'counters/updates': 289}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9312 examples: {'rewards_train/chosen': '0.023799', 'rewards_train/rejected': '-0.37262', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39642', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-130.56', 'loss/train': '0.56499', 'examples_per_second': '45.289', 'grad_norm': '21.954', 'counters/examples': 9312, 'counters/updates': 291}
skipping logging after 9344 examples to avoid logging too frequently
train stats after 9376 examples: {'rewards_train/chosen': '0.018489', 'rewards_train/rejected': '-0.64441', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.6629', 'logps_train/rejected': '-105.07', 'logps_train/chosen': '-108.03', 'loss/train': '0.50072', 'examples_per_second': '54.3', 'grad_norm': '18.603', 'counters/examples': 9376, 'counters/updates': 293}
train stats after 9408 examples: {'rewards_train/chosen': '0.069819', 'rewards_train/rejected': '-0.27121', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.34103', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-119.9', 'loss/train': '0.64102', 'examples_per_second': '32.407', 'grad_norm': '26.119', 'counters/examples': 9408, 'counters/updates': 294}
skipping logging after 9440 examples to avoid logging too frequently
train stats after 9472 examples: {'rewards_train/chosen': '-0.12065', 'rewards_train/rejected': '-0.71828', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.59763', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-104.49', 'loss/train': '0.55092', 'examples_per_second': '45.576', 'grad_norm': '19.743', 'counters/examples': 9472, 'counters/updates': 296}
skipping logging after 9504 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': '-0.25161', 'rewards_train/rejected': '-0.50705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25544', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-128.5', 'loss/train': '0.67614', 'examples_per_second': '45.607', 'grad_norm': '26.068', 'counters/examples': 9536, 'counters/updates': 298}
skipping logging after 9568 examples to avoid logging too frequently
train stats after 9600 examples: {'rewards_train/chosen': '0.072295', 'rewards_train/rejected': '-0.1791', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25139', 'logps_train/rejected': '-109.08', 'logps_train/chosen': '-162.71', 'loss/train': '0.69482', 'examples_per_second': '45.286', 'grad_norm': '30.507', 'counters/examples': 9600, 'counters/updates': 300}
skipping logging after 9632 examples to avoid logging too frequently
train stats after 9664 examples: {'rewards_train/chosen': '-0.12926', 'rewards_train/rejected': '-0.536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40674', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-164.45', 'loss/train': '0.60526', 'examples_per_second': '45.17', 'grad_norm': '27.129', 'counters/examples': 9664, 'counters/updates': 302}
skipping logging after 9696 examples to avoid logging too frequently
train stats after 9728 examples: {'rewards_train/chosen': '0.15351', 'rewards_train/rejected': '-0.43925', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59276', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-137.79', 'loss/train': '0.51057', 'examples_per_second': '45.752', 'grad_norm': '21.306', 'counters/examples': 9728, 'counters/updates': 304}
skipping logging after 9760 examples to avoid logging too frequently
train stats after 9792 examples: {'rewards_train/chosen': '0.062674', 'rewards_train/rejected': '-0.24647', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.30915', 'logps_train/rejected': '-126.7', 'logps_train/chosen': '-153.92', 'loss/train': '0.68428', 'examples_per_second': '44.862', 'grad_norm': '28.199', 'counters/examples': 9792, 'counters/updates': 306}
skipping logging after 9824 examples to avoid logging too frequently
train stats after 9856 examples: {'rewards_train/chosen': '-0.21785', 'rewards_train/rejected': '-0.6808', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46295', 'logps_train/rejected': '-99.278', 'logps_train/chosen': '-103.31', 'loss/train': '0.54923', 'examples_per_second': '46.284', 'grad_norm': '18.382', 'counters/examples': 9856, 'counters/updates': 308}
skipping logging after 9888 examples to avoid logging too frequently
train stats after 9920 examples: {'rewards_train/chosen': '0.18624', 'rewards_train/rejected': '-0.45891', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.64515', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-96.798', 'loss/train': '0.49847', 'examples_per_second': '46.481', 'grad_norm': '20.134', 'counters/examples': 9920, 'counters/updates': 310}
skipping logging after 9952 examples to avoid logging too frequently
train stats after 9984 examples: {'rewards_train/chosen': '0.086108', 'rewards_train/rejected': '-0.47893', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.56503', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-109.67', 'loss/train': '0.55192', 'examples_per_second': '45.011', 'grad_norm': '23.755', 'counters/examples': 9984, 'counters/updates': 312}
skipping logging after 10016 examples to avoid logging too frequently
train stats after 10048 examples: {'rewards_train/chosen': '0.26926', 'rewards_train/rejected': '-0.33335', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.60261', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-129.18', 'loss/train': '0.49189', 'examples_per_second': '44.457', 'grad_norm': '19.486', 'counters/examples': 10048, 'counters/updates': 314}
skipping logging after 10080 examples to avoid logging too frequently
train stats after 10112 examples: {'rewards_train/chosen': '-0.14889', 'rewards_train/rejected': '-0.6695', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.52061', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-168.78', 'loss/train': '0.55484', 'examples_per_second': '45.587', 'grad_norm': '26.413', 'counters/examples': 10112, 'counters/updates': 316}
skipping logging after 10144 examples to avoid logging too frequently
train stats after 10176 examples: {'rewards_train/chosen': '-0.5046', 'rewards_train/rejected': '-0.64629', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1417', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-158.51', 'loss/train': '0.69804', 'examples_per_second': '49.303', 'grad_norm': '27.995', 'counters/examples': 10176, 'counters/updates': 318}
skipping logging after 10208 examples to avoid logging too frequently
train stats after 10240 examples: {'rewards_train/chosen': '-0.55993', 'rewards_train/rejected': '-0.94144', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38151', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-155.84', 'loss/train': '0.62347', 'examples_per_second': '45.42', 'grad_norm': '24.192', 'counters/examples': 10240, 'counters/updates': 320}
skipping logging after 10272 examples to avoid logging too frequently
train stats after 10304 examples: {'rewards_train/chosen': '-0.40405', 'rewards_train/rejected': '-0.72765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3236', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-122.66', 'loss/train': '0.61123', 'examples_per_second': '45.394', 'grad_norm': '21.564', 'counters/examples': 10304, 'counters/updates': 322}
skipping logging after 10336 examples to avoid logging too frequently
train stats after 10368 examples: {'rewards_train/chosen': '-0.4212', 'rewards_train/rejected': '-0.91597', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49477', 'logps_train/rejected': '-103.72', 'logps_train/chosen': '-124.3', 'loss/train': '0.58615', 'examples_per_second': '46.776', 'grad_norm': '20.918', 'counters/examples': 10368, 'counters/updates': 324}
skipping logging after 10400 examples to avoid logging too frequently
train stats after 10432 examples: {'rewards_train/chosen': '-0.23287', 'rewards_train/rejected': '-0.56947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3366', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-155.35', 'loss/train': '0.62377', 'examples_per_second': '45.622', 'grad_norm': '26.332', 'counters/examples': 10432, 'counters/updates': 326}
skipping logging after 10464 examples to avoid logging too frequently
train stats after 10496 examples: {'rewards_train/chosen': '-0.49607', 'rewards_train/rejected': '-0.93393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43786', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-149.98', 'loss/train': '0.58716', 'examples_per_second': '48.688', 'grad_norm': '23.125', 'counters/examples': 10496, 'counters/updates': 328}
skipping logging after 10528 examples to avoid logging too frequently
train stats after 10560 examples: {'rewards_train/chosen': '-0.49117', 'rewards_train/rejected': '-0.83042', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33925', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-128.09', 'loss/train': '0.64247', 'examples_per_second': '48.788', 'grad_norm': '25.07', 'counters/examples': 10560, 'counters/updates': 330}
skipping logging after 10592 examples to avoid logging too frequently
train stats after 10624 examples: {'rewards_train/chosen': '-0.21426', 'rewards_train/rejected': '-0.79527', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58101', 'logps_train/rejected': '-108.76', 'logps_train/chosen': '-110.24', 'loss/train': '0.52118', 'examples_per_second': '45.253', 'grad_norm': '24.206', 'counters/examples': 10624, 'counters/updates': 332}
skipping logging after 10656 examples to avoid logging too frequently
train stats after 10688 examples: {'rewards_train/chosen': '-0.15798', 'rewards_train/rejected': '-0.52868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3707', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-138.31', 'loss/train': '0.649', 'examples_per_second': '45.38', 'grad_norm': '25.377', 'counters/examples': 10688, 'counters/updates': 334}
skipping logging after 10720 examples to avoid logging too frequently
train stats after 10752 examples: {'rewards_train/chosen': '-0.24323', 'rewards_train/rejected': '-0.86234', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61911', 'logps_train/rejected': '-128.03', 'logps_train/chosen': '-144.05', 'loss/train': '0.52359', 'examples_per_second': '51.917', 'grad_norm': '20.927', 'counters/examples': 10752, 'counters/updates': 336}
skipping logging after 10784 examples to avoid logging too frequently
train stats after 10816 examples: {'rewards_train/chosen': '-0.32686', 'rewards_train/rejected': '-0.73282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40596', 'logps_train/rejected': '-196.1', 'logps_train/chosen': '-148.83', 'loss/train': '0.67885', 'examples_per_second': '45.193', 'grad_norm': '25.136', 'counters/examples': 10816, 'counters/updates': 338}
skipping logging after 10848 examples to avoid logging too frequently
train stats after 10880 examples: {'rewards_train/chosen': '-0.30542', 'rewards_train/rejected': '-0.73549', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43007', 'logps_train/rejected': '-175.14', 'logps_train/chosen': '-180.98', 'loss/train': '0.64062', 'examples_per_second': '44.921', 'grad_norm': '28.991', 'counters/examples': 10880, 'counters/updates': 340}
skipping logging after 10912 examples to avoid logging too frequently
train stats after 10944 examples: {'rewards_train/chosen': '-0.30391', 'rewards_train/rejected': '-0.70759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40368', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-132.98', 'loss/train': '0.64611', 'examples_per_second': '45.539', 'grad_norm': '24.241', 'counters/examples': 10944, 'counters/updates': 342}
skipping logging after 10976 examples to avoid logging too frequently
train stats after 11008 examples: {'rewards_train/chosen': '-0.1306', 'rewards_train/rejected': '-0.63916', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.50856', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-134.99', 'loss/train': '0.59809', 'examples_per_second': '48.637', 'grad_norm': '21.483', 'counters/examples': 11008, 'counters/updates': 344}
skipping logging after 11040 examples to avoid logging too frequently
train stats after 11072 examples: {'rewards_train/chosen': '-0.4349', 'rewards_train/rejected': '-0.85961', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42471', 'logps_train/rejected': '-160.59', 'logps_train/chosen': '-136.24', 'loss/train': '0.62352', 'examples_per_second': '45.348', 'grad_norm': '22.959', 'counters/examples': 11072, 'counters/updates': 346}
skipping logging after 11104 examples to avoid logging too frequently
train stats after 11136 examples: {'rewards_train/chosen': '-0.12464', 'rewards_train/rejected': '-0.36512', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24047', 'logps_train/rejected': '-158.94', 'logps_train/chosen': '-174.74', 'loss/train': '0.67724', 'examples_per_second': '44.969', 'grad_norm': '29.674', 'counters/examples': 11136, 'counters/updates': 348}
skipping logging after 11168 examples to avoid logging too frequently
train stats after 11200 examples: {'rewards_train/chosen': '0.00077915', 'rewards_train/rejected': '-0.31004', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31082', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-152.83', 'loss/train': '0.63626', 'examples_per_second': '46.746', 'grad_norm': '24.945', 'counters/examples': 11200, 'counters/updates': 350}
skipping logging after 11232 examples to avoid logging too frequently
train stats after 11264 examples: {'rewards_train/chosen': '-0.41413', 'rewards_train/rejected': '-0.57431', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16018', 'logps_train/rejected': '-127.5', 'logps_train/chosen': '-141.34', 'loss/train': '0.70602', 'examples_per_second': '44.403', 'grad_norm': '28.17', 'counters/examples': 11264, 'counters/updates': 352}
skipping logging after 11296 examples to avoid logging too frequently
train stats after 11328 examples: {'rewards_train/chosen': '-0.45833', 'rewards_train/rejected': '-0.96204', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50371', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-148.93', 'loss/train': '0.56265', 'examples_per_second': '44.621', 'grad_norm': '23.953', 'counters/examples': 11328, 'counters/updates': 354}
skipping logging after 11360 examples to avoid logging too frequently
train stats after 11392 examples: {'rewards_train/chosen': '-0.041349', 'rewards_train/rejected': '-0.62858', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.58723', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-158.75', 'loss/train': '0.55202', 'examples_per_second': '46.029', 'grad_norm': '23.748', 'counters/examples': 11392, 'counters/updates': 356}
skipping logging after 11424 examples to avoid logging too frequently
train stats after 11456 examples: {'rewards_train/chosen': '-0.41295', 'rewards_train/rejected': '-0.81553', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.40258', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-128.64', 'loss/train': '0.58543', 'examples_per_second': '45.569', 'grad_norm': '23.261', 'counters/examples': 11456, 'counters/updates': 358}
skipping logging after 11488 examples to avoid logging too frequently
train stats after 11520 examples: {'rewards_train/chosen': '-0.25893', 'rewards_train/rejected': '-0.64858', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.38966', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-170.22', 'loss/train': '0.61839', 'examples_per_second': '45.479', 'grad_norm': '26.476', 'counters/examples': 11520, 'counters/updates': 360}
skipping logging after 11552 examples to avoid logging too frequently
train stats after 11584 examples: {'rewards_train/chosen': '-0.45727', 'rewards_train/rejected': '-0.95247', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.4952', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-135.01', 'loss/train': '0.54855', 'examples_per_second': '45.623', 'grad_norm': '23.651', 'counters/examples': 11584, 'counters/updates': 362}
skipping logging after 11616 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '-0.3973', 'rewards_train/rejected': '-0.87758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.48028', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-122.36', 'loss/train': '0.60308', 'examples_per_second': '44.596', 'grad_norm': '23.05', 'counters/examples': 11648, 'counters/updates': 364}
skipping logging after 11680 examples to avoid logging too frequently
train stats after 11712 examples: {'rewards_train/chosen': '-0.43092', 'rewards_train/rejected': '-0.50796', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.077042', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-127.44', 'loss/train': '0.75708', 'examples_per_second': '45.529', 'grad_norm': '28.449', 'counters/examples': 11712, 'counters/updates': 366}
skipping logging after 11744 examples to avoid logging too frequently
train stats after 11776 examples: {'rewards_train/chosen': '-0.30268', 'rewards_train/rejected': '-0.92789', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.62521', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-154.97', 'loss/train': '0.52317', 'examples_per_second': '45.701', 'grad_norm': '22.409', 'counters/examples': 11776, 'counters/updates': 368}
skipping logging after 11808 examples to avoid logging too frequently
train stats after 11840 examples: {'rewards_train/chosen': '-0.45337', 'rewards_train/rejected': '-1.0034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55005', 'logps_train/rejected': '-115.62', 'logps_train/chosen': '-146.72', 'loss/train': '0.57291', 'examples_per_second': '45.35', 'grad_norm': '24.708', 'counters/examples': 11840, 'counters/updates': 370}
skipping logging after 11872 examples to avoid logging too frequently
train stats after 11904 examples: {'rewards_train/chosen': '-0.53736', 'rewards_train/rejected': '-1.0024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46507', 'logps_train/rejected': '-153.24', 'logps_train/chosen': '-148.12', 'loss/train': '0.56067', 'examples_per_second': '44.239', 'grad_norm': '25.959', 'counters/examples': 11904, 'counters/updates': 372}
skipping logging after 11936 examples to avoid logging too frequently
train stats after 11968 examples: {'rewards_train/chosen': '-0.73014', 'rewards_train/rejected': '-1.2401', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50992', 'logps_train/rejected': '-156.21', 'logps_train/chosen': '-155.45', 'loss/train': '0.66944', 'examples_per_second': '45.436', 'grad_norm': '33.936', 'counters/examples': 11968, 'counters/updates': 374}
skipping logging after 12000 examples to avoid logging too frequently
Running evaluation after 12000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.17it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.84it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.94it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.95it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.87it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.30it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.15it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.09it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.13it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.00it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  6.98it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  6.99it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  6.98it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:02<00:00,  6.88it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.95it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.85it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.98it/s]
eval after 12000: {'rewards_eval/chosen': '-0.57835', 'rewards_eval/rejected': '-1.0181', 'rewards_eval/accuracies': '0.63281', 'rewards_eval/margins': '0.43974', 'logps_eval/rejected': '-128.46', 'logps_eval/chosen': '-142.23', 'loss/eval': '0.61696'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-12000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-12000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-12000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-12000/scheduler.pt...
train stats after 12032 examples: {'rewards_train/chosen': '-0.57913', 'rewards_train/rejected': '-0.93644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35731', 'logps_train/rejected': '-118', 'logps_train/chosen': '-166.75', 'loss/train': '0.62047', 'examples_per_second': '34.806', 'grad_norm': '28.68', 'counters/examples': 12032, 'counters/updates': 376}
skipping logging after 12064 examples to avoid logging too frequently
train stats after 12096 examples: {'rewards_train/chosen': '-0.5407', 'rewards_train/rejected': '-0.81869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27799', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-122.61', 'loss/train': '0.6817', 'examples_per_second': '44.349', 'grad_norm': '25.572', 'counters/examples': 12096, 'counters/updates': 378}
skipping logging after 12128 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.87709', 'rewards_train/rejected': '-1.4239', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54681', 'logps_train/rejected': '-143.92', 'logps_train/chosen': '-165.81', 'loss/train': '0.57363', 'examples_per_second': '46.339', 'grad_norm': '23.331', 'counters/examples': 12160, 'counters/updates': 380}
skipping logging after 12192 examples to avoid logging too frequently
train stats after 12224 examples: {'rewards_train/chosen': '-0.25615', 'rewards_train/rejected': '-0.71488', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45872', 'logps_train/rejected': '-106.98', 'logps_train/chosen': '-111.27', 'loss/train': '0.56146', 'examples_per_second': '47.021', 'grad_norm': '21.051', 'counters/examples': 12224, 'counters/updates': 382}
skipping logging after 12256 examples to avoid logging too frequently
train stats after 12288 examples: {'rewards_train/chosen': '-0.38996', 'rewards_train/rejected': '-1.1885', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.79853', 'logps_train/rejected': '-153.66', 'logps_train/chosen': '-150.01', 'loss/train': '0.46033', 'examples_per_second': '45.107', 'grad_norm': '22.055', 'counters/examples': 12288, 'counters/updates': 384}
skipping logging after 12320 examples to avoid logging too frequently
train stats after 12352 examples: {'rewards_train/chosen': '-0.77287', 'rewards_train/rejected': '-1.2125', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43962', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-136.39', 'loss/train': '0.5826', 'examples_per_second': '49.181', 'grad_norm': '23.877', 'counters/examples': 12352, 'counters/updates': 386}
skipping logging after 12384 examples to avoid logging too frequently
train stats after 12416 examples: {'rewards_train/chosen': '-0.7609', 'rewards_train/rejected': '-1.203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.44211', 'logps_train/rejected': '-142.03', 'logps_train/chosen': '-113.45', 'loss/train': '0.59893', 'examples_per_second': '45.359', 'grad_norm': '22.411', 'counters/examples': 12416, 'counters/updates': 388}
skipping logging after 12448 examples to avoid logging too frequently
train stats after 12480 examples: {'rewards_train/chosen': '-0.78393', 'rewards_train/rejected': '-1.1325', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34858', 'logps_train/rejected': '-135.75', 'logps_train/chosen': '-173.05', 'loss/train': '0.6567', 'examples_per_second': '44.883', 'grad_norm': '30.069', 'counters/examples': 12480, 'counters/updates': 390}
skipping logging after 12512 examples to avoid logging too frequently
train stats after 12544 examples: {'rewards_train/chosen': '-0.43773', 'rewards_train/rejected': '-1.0453', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.60761', 'logps_train/rejected': '-100.43', 'logps_train/chosen': '-157.21', 'loss/train': '0.51728', 'examples_per_second': '45.66', 'grad_norm': '20.29', 'counters/examples': 12544, 'counters/updates': 392}
skipping logging after 12576 examples to avoid logging too frequently
train stats after 12608 examples: {'rewards_train/chosen': '-0.97282', 'rewards_train/rejected': '-1.2074', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23455', 'logps_train/rejected': '-123.27', 'logps_train/chosen': '-151.1', 'loss/train': '0.67845', 'examples_per_second': '48.124', 'grad_norm': '30.404', 'counters/examples': 12608, 'counters/updates': 394}
skipping logging after 12640 examples to avoid logging too frequently
train stats after 12672 examples: {'rewards_train/chosen': '-0.38972', 'rewards_train/rejected': '-0.57624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18652', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-150.19', 'loss/train': '0.68643', 'examples_per_second': '44.669', 'grad_norm': '28.381', 'counters/examples': 12672, 'counters/updates': 396}
skipping logging after 12704 examples to avoid logging too frequently
train stats after 12736 examples: {'rewards_train/chosen': '-0.60979', 'rewards_train/rejected': '-0.84603', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.23624', 'logps_train/rejected': '-139.26', 'logps_train/chosen': '-142.19', 'loss/train': '0.74419', 'examples_per_second': '46.436', 'grad_norm': '29.504', 'counters/examples': 12736, 'counters/updates': 398}
skipping logging after 12768 examples to avoid logging too frequently
train stats after 12800 examples: {'rewards_train/chosen': '-0.17524', 'rewards_train/rejected': '-0.76321', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.58797', 'logps_train/rejected': '-117.73', 'logps_train/chosen': '-130.62', 'loss/train': '0.55052', 'examples_per_second': '45.697', 'grad_norm': '21.55', 'counters/examples': 12800, 'counters/updates': 400}
skipping logging after 12832 examples to avoid logging too frequently
train stats after 12864 examples: {'rewards_train/chosen': '-0.34914', 'rewards_train/rejected': '-0.63007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28093', 'logps_train/rejected': '-138.29', 'logps_train/chosen': '-146.8', 'loss/train': '0.64448', 'examples_per_second': '45.629', 'grad_norm': '24.783', 'counters/examples': 12864, 'counters/updates': 402}
skipping logging after 12896 examples to avoid logging too frequently
train stats after 12928 examples: {'rewards_train/chosen': '-0.31914', 'rewards_train/rejected': '-0.62428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30514', 'logps_train/rejected': '-188.69', 'logps_train/chosen': '-167.8', 'loss/train': '0.68375', 'examples_per_second': '49.341', 'grad_norm': '31.635', 'counters/examples': 12928, 'counters/updates': 404}
skipping logging after 12960 examples to avoid logging too frequently
train stats after 12992 examples: {'rewards_train/chosen': '-0.29399', 'rewards_train/rejected': '-0.73314', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.43915', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-128.99', 'loss/train': '0.56878', 'examples_per_second': '45.259', 'grad_norm': '24.25', 'counters/examples': 12992, 'counters/updates': 406}
skipping logging after 13024 examples to avoid logging too frequently
train stats after 13056 examples: {'rewards_train/chosen': '-0.35518', 'rewards_train/rejected': '-0.91721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56203', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-112.71', 'loss/train': '0.56066', 'examples_per_second': '46.736', 'grad_norm': '21.551', 'counters/examples': 13056, 'counters/updates': 408}
skipping logging after 13088 examples to avoid logging too frequently
train stats after 13120 examples: {'rewards_train/chosen': '-0.11201', 'rewards_train/rejected': '-0.7166', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60459', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-169.47', 'loss/train': '0.54164', 'examples_per_second': '45.595', 'grad_norm': '24.171', 'counters/examples': 13120, 'counters/updates': 410}
skipping logging after 13152 examples to avoid logging too frequently
train stats after 13184 examples: {'rewards_train/chosen': '-0.16323', 'rewards_train/rejected': '-0.65864', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.49541', 'logps_train/rejected': '-134.23', 'logps_train/chosen': '-135.84', 'loss/train': '0.54024', 'examples_per_second': '45.09', 'grad_norm': '20.847', 'counters/examples': 13184, 'counters/updates': 412}
skipping logging after 13216 examples to avoid logging too frequently
train stats after 13248 examples: {'rewards_train/chosen': '-0.37932', 'rewards_train/rejected': '-0.86329', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48397', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-150.9', 'loss/train': '0.58309', 'examples_per_second': '32.246', 'grad_norm': '25.177', 'counters/examples': 13248, 'counters/updates': 414}
skipping logging after 13280 examples to avoid logging too frequently
train stats after 13312 examples: {'rewards_train/chosen': '-0.31996', 'rewards_train/rejected': '-0.75573', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43577', 'logps_train/rejected': '-107.78', 'logps_train/chosen': '-184.47', 'loss/train': '0.58375', 'examples_per_second': '47.665', 'grad_norm': '23.679', 'counters/examples': 13312, 'counters/updates': 416}
skipping logging after 13344 examples to avoid logging too frequently
train stats after 13376 examples: {'rewards_train/chosen': '-0.28857', 'rewards_train/rejected': '-0.72334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43477', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-134.62', 'loss/train': '0.62257', 'examples_per_second': '44.255', 'grad_norm': '25.375', 'counters/examples': 13376, 'counters/updates': 418}
skipping logging after 13408 examples to avoid logging too frequently
train stats after 13440 examples: {'rewards_train/chosen': '-0.56457', 'rewards_train/rejected': '-0.86945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30488', 'logps_train/rejected': '-137.63', 'logps_train/chosen': '-134.63', 'loss/train': '0.64689', 'examples_per_second': '45.743', 'grad_norm': '26.874', 'counters/examples': 13440, 'counters/updates': 420}
skipping logging after 13472 examples to avoid logging too frequently
train stats after 13504 examples: {'rewards_train/chosen': '-0.14935', 'rewards_train/rejected': '-0.43133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28198', 'logps_train/rejected': '-150.84', 'logps_train/chosen': '-158.89', 'loss/train': '0.6189', 'examples_per_second': '45.803', 'grad_norm': '26.542', 'counters/examples': 13504, 'counters/updates': 422}
skipping logging after 13536 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '-0.20042', 'rewards_train/rejected': '-0.593', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.39258', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-135.2', 'loss/train': '0.58255', 'examples_per_second': '45.712', 'grad_norm': '23.367', 'counters/examples': 13568, 'counters/updates': 424}
skipping logging after 13600 examples to avoid logging too frequently
train stats after 13632 examples: {'rewards_train/chosen': '-0.51346', 'rewards_train/rejected': '-0.62923', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11577', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-123.72', 'loss/train': '0.74186', 'examples_per_second': '45.105', 'grad_norm': '26.812', 'counters/examples': 13632, 'counters/updates': 426}
skipping logging after 13664 examples to avoid logging too frequently
train stats after 13696 examples: {'rewards_train/chosen': '-0.28477', 'rewards_train/rejected': '-0.4092', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12443', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-153.56', 'loss/train': '0.73873', 'examples_per_second': '46.355', 'grad_norm': '30.771', 'counters/examples': 13696, 'counters/updates': 428}
skipping logging after 13728 examples to avoid logging too frequently
train stats after 13760 examples: {'rewards_train/chosen': '-0.40385', 'rewards_train/rejected': '-0.77603', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37217', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-162.09', 'loss/train': '0.58652', 'examples_per_second': '44.605', 'grad_norm': '25.654', 'counters/examples': 13760, 'counters/updates': 430}
skipping logging after 13792 examples to avoid logging too frequently
train stats after 13824 examples: {'rewards_train/chosen': '-0.26184', 'rewards_train/rejected': '-0.63966', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37782', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-125.71', 'loss/train': '0.64084', 'examples_per_second': '45.712', 'grad_norm': '26.421', 'counters/examples': 13824, 'counters/updates': 432}
skipping logging after 13856 examples to avoid logging too frequently
train stats after 13888 examples: {'rewards_train/chosen': '-0.0096915', 'rewards_train/rejected': '-0.45334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.44364', 'logps_train/rejected': '-156.81', 'logps_train/chosen': '-124.52', 'loss/train': '0.57517', 'examples_per_second': '45.746', 'grad_norm': '23.889', 'counters/examples': 13888, 'counters/updates': 434}
skipping logging after 13920 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '0.20896', 'rewards_train/rejected': '-0.356', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56496', 'logps_train/rejected': '-104.72', 'logps_train/chosen': '-120.82', 'loss/train': '0.55328', 'examples_per_second': '47.133', 'grad_norm': '21.627', 'counters/examples': 13952, 'counters/updates': 436}
skipping logging after 13984 examples to avoid logging too frequently
train stats after 14016 examples: {'rewards_train/chosen': '-0.033563', 'rewards_train/rejected': '-0.43473', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40117', 'logps_train/rejected': '-139.68', 'logps_train/chosen': '-153.03', 'loss/train': '0.59525', 'examples_per_second': '46.915', 'grad_norm': '25.137', 'counters/examples': 14016, 'counters/updates': 438}
skipping logging after 14048 examples to avoid logging too frequently
train stats after 14080 examples: {'rewards_train/chosen': '0.14984', 'rewards_train/rejected': '-0.26625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41609', 'logps_train/rejected': '-135.82', 'logps_train/chosen': '-121.57', 'loss/train': '0.66203', 'examples_per_second': '45.793', 'grad_norm': '25.734', 'counters/examples': 14080, 'counters/updates': 440}
skipping logging after 14112 examples to avoid logging too frequently
train stats after 14144 examples: {'rewards_train/chosen': '-0.015007', 'rewards_train/rejected': '-0.35873', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.34373', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-121.42', 'loss/train': '0.58303', 'examples_per_second': '45.67', 'grad_norm': '23.052', 'counters/examples': 14144, 'counters/updates': 442}
skipping logging after 14176 examples to avoid logging too frequently
train stats after 14208 examples: {'rewards_train/chosen': '-0.083605', 'rewards_train/rejected': '-0.37647', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29287', 'logps_train/rejected': '-105.68', 'logps_train/chosen': '-130.37', 'loss/train': '0.61365', 'examples_per_second': '47.923', 'grad_norm': '22.655', 'counters/examples': 14208, 'counters/updates': 444}
skipping logging after 14240 examples to avoid logging too frequently
train stats after 14272 examples: {'rewards_train/chosen': '-0.1352', 'rewards_train/rejected': '-0.61265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.47745', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-110.38', 'loss/train': '0.60214', 'examples_per_second': '46.15', 'grad_norm': '21.252', 'counters/examples': 14272, 'counters/updates': 446}
skipping logging after 14304 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.093722', 'rewards_train/rejected': '-0.39368', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.4874', 'logps_train/rejected': '-146.25', 'logps_train/chosen': '-157.53', 'loss/train': '0.54314', 'examples_per_second': '44.855', 'grad_norm': '23.149', 'counters/examples': 14336, 'counters/updates': 448}
skipping logging after 14368 examples to avoid logging too frequently
train stats after 14400 examples: {'rewards_train/chosen': '-0.30738', 'rewards_train/rejected': '-0.66903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36166', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-130.21', 'loss/train': '0.62755', 'examples_per_second': '52.295', 'grad_norm': '23.424', 'counters/examples': 14400, 'counters/updates': 450}
skipping logging after 14432 examples to avoid logging too frequently
train stats after 14464 examples: {'rewards_train/chosen': '-0.142', 'rewards_train/rejected': '-0.554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.412', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-131.7', 'loss/train': '0.60775', 'examples_per_second': '45.764', 'grad_norm': '25.699', 'counters/examples': 14464, 'counters/updates': 452}
skipping logging after 14496 examples to avoid logging too frequently
train stats after 14528 examples: {'rewards_train/chosen': '0.053714', 'rewards_train/rejected': '-0.55869', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.61241', 'logps_train/rejected': '-137.68', 'logps_train/chosen': '-107.67', 'loss/train': '0.4775', 'examples_per_second': '46.667', 'grad_norm': '20.612', 'counters/examples': 14528, 'counters/updates': 454}
skipping logging after 14560 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '-0.0019028', 'rewards_train/rejected': '-0.46802', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46612', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-128.49', 'loss/train': '0.58705', 'examples_per_second': '45.714', 'grad_norm': '23.273', 'counters/examples': 14592, 'counters/updates': 456}
skipping logging after 14624 examples to avoid logging too frequently
train stats after 14656 examples: {'rewards_train/chosen': '-0.28812', 'rewards_train/rejected': '-0.75583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.4677', 'logps_train/rejected': '-118.15', 'logps_train/chosen': '-127.65', 'loss/train': '0.57818', 'examples_per_second': '45.698', 'grad_norm': '22.267', 'counters/examples': 14656, 'counters/updates': 458}
skipping logging after 14688 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '-0.072187', 'rewards_train/rejected': '-0.27526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20307', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-170.28', 'loss/train': '0.65545', 'examples_per_second': '45.709', 'grad_norm': '29.513', 'counters/examples': 14720, 'counters/updates': 460}
skipping logging after 14752 examples to avoid logging too frequently
train stats after 14784 examples: {'rewards_train/chosen': '-0.34379', 'rewards_train/rejected': '-0.84919', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5054', 'logps_train/rejected': '-92.599', 'logps_train/chosen': '-136.39', 'loss/train': '0.52748', 'examples_per_second': '53.898', 'grad_norm': '21.176', 'counters/examples': 14784, 'counters/updates': 462}
skipping logging after 14816 examples to avoid logging too frequently
train stats after 14848 examples: {'rewards_train/chosen': '-0.088461', 'rewards_train/rejected': '-0.34574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25728', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-140.05', 'loss/train': '0.65478', 'examples_per_second': '49.66', 'grad_norm': '23.216', 'counters/examples': 14848, 'counters/updates': 464}
skipping logging after 14880 examples to avoid logging too frequently
train stats after 14912 examples: {'rewards_train/chosen': '0.013979', 'rewards_train/rejected': '-0.27618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29016', 'logps_train/rejected': '-132.44', 'logps_train/chosen': '-144.63', 'loss/train': '0.64097', 'examples_per_second': '48.743', 'grad_norm': '26.806', 'counters/examples': 14912, 'counters/updates': 466}
skipping logging after 14944 examples to avoid logging too frequently
train stats after 14976 examples: {'rewards_train/chosen': '0.19692', 'rewards_train/rejected': '-0.33911', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.53603', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-111.92', 'loss/train': '0.53956', 'examples_per_second': '46.336', 'grad_norm': '20.627', 'counters/examples': 14976, 'counters/updates': 468}
skipping logging after 15008 examples to avoid logging too frequently
train stats after 15040 examples: {'rewards_train/chosen': '-0.12594', 'rewards_train/rejected': '-0.33818', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21225', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-136.7', 'loss/train': '0.68911', 'examples_per_second': '48.347', 'grad_norm': '26.163', 'counters/examples': 15040, 'counters/updates': 470}
skipping logging after 15072 examples to avoid logging too frequently
train stats after 15104 examples: {'rewards_train/chosen': '-0.096748', 'rewards_train/rejected': '-0.53484', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4381', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-130.59', 'loss/train': '0.57939', 'examples_per_second': '45.014', 'grad_norm': '22.353', 'counters/examples': 15104, 'counters/updates': 472}
skipping logging after 15136 examples to avoid logging too frequently
train stats after 15168 examples: {'rewards_train/chosen': '0.0040385', 'rewards_train/rejected': '-0.23116', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2352', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-117.08', 'loss/train': '0.64865', 'examples_per_second': '45.653', 'grad_norm': '27.22', 'counters/examples': 15168, 'counters/updates': 474}
skipping logging after 15200 examples to avoid logging too frequently
train stats after 15232 examples: {'rewards_train/chosen': '-0.1511', 'rewards_train/rejected': '-0.38936', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.23827', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-114.58', 'loss/train': '0.64648', 'examples_per_second': '45.306', 'grad_norm': '23.597', 'counters/examples': 15232, 'counters/updates': 476}
skipping logging after 15264 examples to avoid logging too frequently
train stats after 15296 examples: {'rewards_train/chosen': '0.23175', 'rewards_train/rejected': '-0.23539', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.46713', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-152.02', 'loss/train': '0.57059', 'examples_per_second': '44.19', 'grad_norm': '23.225', 'counters/examples': 15296, 'counters/updates': 478}
skipping logging after 15328 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '0.024083', 'rewards_train/rejected': '-0.44148', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.46557', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-149.62', 'loss/train': '0.53128', 'examples_per_second': '44.304', 'grad_norm': '21.39', 'counters/examples': 15360, 'counters/updates': 480}
skipping logging after 15392 examples to avoid logging too frequently
train stats after 15424 examples: {'rewards_train/chosen': '-0.076298', 'rewards_train/rejected': '-0.89411', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.81781', 'logps_train/rejected': '-110.9', 'logps_train/chosen': '-131.54', 'loss/train': '0.49126', 'examples_per_second': '49.778', 'grad_norm': '21.563', 'counters/examples': 15424, 'counters/updates': 482}
skipping logging after 15456 examples to avoid logging too frequently
train stats after 15488 examples: {'rewards_train/chosen': '-0.20214', 'rewards_train/rejected': '-0.40463', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20249', 'logps_train/rejected': '-101.52', 'logps_train/chosen': '-115.98', 'loss/train': '0.64876', 'examples_per_second': '45.81', 'grad_norm': '21.789', 'counters/examples': 15488, 'counters/updates': 484}
skipping logging after 15520 examples to avoid logging too frequently
train stats after 15552 examples: {'rewards_train/chosen': '-0.029214', 'rewards_train/rejected': '-0.41218', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38297', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-132.42', 'loss/train': '0.63037', 'examples_per_second': '46.773', 'grad_norm': '25.329', 'counters/examples': 15552, 'counters/updates': 486}
skipping logging after 15584 examples to avoid logging too frequently
train stats after 15616 examples: {'rewards_train/chosen': '-0.22813', 'rewards_train/rejected': '-0.67105', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.44292', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-141.02', 'loss/train': '0.57998', 'examples_per_second': '45.718', 'grad_norm': '24.303', 'counters/examples': 15616, 'counters/updates': 488}
skipping logging after 15648 examples to avoid logging too frequently
train stats after 15680 examples: {'rewards_train/chosen': '-0.15029', 'rewards_train/rejected': '-0.29569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1454', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-111.05', 'loss/train': '0.73683', 'examples_per_second': '46.629', 'grad_norm': '26.094', 'counters/examples': 15680, 'counters/updates': 490}
skipping logging after 15712 examples to avoid logging too frequently
train stats after 15744 examples: {'rewards_train/chosen': '0.22593', 'rewards_train/rejected': '-0.3105', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53644', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-110.09', 'loss/train': '0.52053', 'examples_per_second': '45.466', 'grad_norm': '18.539', 'counters/examples': 15744, 'counters/updates': 492}
skipping logging after 15776 examples to avoid logging too frequently
train stats after 15808 examples: {'rewards_train/chosen': '-0.37072', 'rewards_train/rejected': '-0.80268', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43196', 'logps_train/rejected': '-153.45', 'logps_train/chosen': '-120.24', 'loss/train': '0.59421', 'examples_per_second': '45.493', 'grad_norm': '26.11', 'counters/examples': 15808, 'counters/updates': 494}
skipping logging after 15840 examples to avoid logging too frequently
train stats after 15872 examples: {'rewards_train/chosen': '0.038167', 'rewards_train/rejected': '-0.52739', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.56555', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-137.48', 'loss/train': '0.51502', 'examples_per_second': '45.627', 'grad_norm': '22.223', 'counters/examples': 15872, 'counters/updates': 496}
skipping logging after 15904 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': '-0.023541', 'rewards_train/rejected': '-0.67258', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64904', 'logps_train/rejected': '-169.89', 'logps_train/chosen': '-112.93', 'loss/train': '0.51183', 'examples_per_second': '51.323', 'grad_norm': '23.358', 'counters/examples': 15936, 'counters/updates': 498}
skipping logging after 15968 examples to avoid logging too frequently
train stats after 16000 examples: {'rewards_train/chosen': '-0.36233', 'rewards_train/rejected': '-0.80211', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43978', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-150.16', 'loss/train': '0.59557', 'examples_per_second': '45.69', 'grad_norm': '25.482', 'counters/examples': 16000, 'counters/updates': 500}
skipping logging after 16032 examples to avoid logging too frequently
train stats after 16064 examples: {'rewards_train/chosen': '-0.4282', 'rewards_train/rejected': '-0.83698', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40878', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-144.82', 'loss/train': '0.59203', 'examples_per_second': '49.34', 'grad_norm': '21.896', 'counters/examples': 16064, 'counters/updates': 502}
skipping logging after 16096 examples to avoid logging too frequently
train stats after 16128 examples: {'rewards_train/chosen': '-0.31953', 'rewards_train/rejected': '-0.73056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41103', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-147.83', 'loss/train': '0.61966', 'examples_per_second': '45.532', 'grad_norm': '25.9', 'counters/examples': 16128, 'counters/updates': 504}
skipping logging after 16160 examples to avoid logging too frequently
train stats after 16192 examples: {'rewards_train/chosen': '-0.57177', 'rewards_train/rejected': '-0.93233', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36057', 'logps_train/rejected': '-109.65', 'logps_train/chosen': '-158.09', 'loss/train': '0.61427', 'examples_per_second': '52.503', 'grad_norm': '23.049', 'counters/examples': 16192, 'counters/updates': 506}
skipping logging after 16224 examples to avoid logging too frequently
train stats after 16256 examples: {'rewards_train/chosen': '-0.2957', 'rewards_train/rejected': '-0.68111', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38541', 'logps_train/rejected': '-139.01', 'logps_train/chosen': '-130.47', 'loss/train': '0.58263', 'examples_per_second': '54.288', 'grad_norm': '23.072', 'counters/examples': 16256, 'counters/updates': 508}
skipping logging after 16288 examples to avoid logging too frequently
train stats after 16320 examples: {'rewards_train/chosen': '-0.66721', 'rewards_train/rejected': '-0.83419', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16698', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-158.82', 'loss/train': '0.72191', 'examples_per_second': '47.25', 'grad_norm': '29.423', 'counters/examples': 16320, 'counters/updates': 510}
skipping logging after 16352 examples to avoid logging too frequently
train stats after 16384 examples: {'rewards_train/chosen': '-0.36443', 'rewards_train/rejected': '-0.59913', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2347', 'logps_train/rejected': '-129.56', 'logps_train/chosen': '-155.97', 'loss/train': '0.64395', 'examples_per_second': '45.576', 'grad_norm': '26.697', 'counters/examples': 16384, 'counters/updates': 512}
skipping logging after 16416 examples to avoid logging too frequently
train stats after 16448 examples: {'rewards_train/chosen': '-0.46376', 'rewards_train/rejected': '-0.98419', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52043', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-153.38', 'loss/train': '0.61863', 'examples_per_second': '45.576', 'grad_norm': '28.968', 'counters/examples': 16448, 'counters/updates': 514}
skipping logging after 16480 examples to avoid logging too frequently
train stats after 16512 examples: {'rewards_train/chosen': '-0.46107', 'rewards_train/rejected': '-0.61842', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15735', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-125.91', 'loss/train': '0.69797', 'examples_per_second': '52.86', 'grad_norm': '25.036', 'counters/examples': 16512, 'counters/updates': 516}
skipping logging after 16544 examples to avoid logging too frequently
train stats after 16576 examples: {'rewards_train/chosen': '-0.35015', 'rewards_train/rejected': '-0.72008', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.36993', 'logps_train/rejected': '-125.16', 'logps_train/chosen': '-107.64', 'loss/train': '0.59', 'examples_per_second': '44.896', 'grad_norm': '22.36', 'counters/examples': 16576, 'counters/updates': 518}
skipping logging after 16608 examples to avoid logging too frequently
train stats after 16640 examples: {'rewards_train/chosen': '-0.41662', 'rewards_train/rejected': '-0.68702', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2704', 'logps_train/rejected': '-152.27', 'logps_train/chosen': '-152.23', 'loss/train': '0.65246', 'examples_per_second': '44.341', 'grad_norm': '28.919', 'counters/examples': 16640, 'counters/updates': 520}
skipping logging after 16672 examples to avoid logging too frequently
train stats after 16704 examples: {'rewards_train/chosen': '-0.36844', 'rewards_train/rejected': '-0.66217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29374', 'logps_train/rejected': '-139.02', 'logps_train/chosen': '-122.33', 'loss/train': '0.60714', 'examples_per_second': '44.92', 'grad_norm': '25.268', 'counters/examples': 16704, 'counters/updates': 522}
skipping logging after 16736 examples to avoid logging too frequently
train stats after 16768 examples: {'rewards_train/chosen': '-0.56083', 'rewards_train/rejected': '-0.70166', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.14083', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-144.54', 'loss/train': '0.7065', 'examples_per_second': '45.013', 'grad_norm': '27.14', 'counters/examples': 16768, 'counters/updates': 524}
skipping logging after 16800 examples to avoid logging too frequently
train stats after 16832 examples: {'rewards_train/chosen': '-0.31567', 'rewards_train/rejected': '-0.83541', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.51974', 'logps_train/rejected': '-162.48', 'logps_train/chosen': '-122.45', 'loss/train': '0.62144', 'examples_per_second': '44.551', 'grad_norm': '27.88', 'counters/examples': 16832, 'counters/updates': 526}
skipping logging after 16864 examples to avoid logging too frequently
train stats after 16896 examples: {'rewards_train/chosen': '-0.12', 'rewards_train/rejected': '-0.6835', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.56351', 'logps_train/rejected': '-144.97', 'logps_train/chosen': '-163.47', 'loss/train': '0.5324', 'examples_per_second': '45.485', 'grad_norm': '24.133', 'counters/examples': 16896, 'counters/updates': 528}
skipping logging after 16928 examples to avoid logging too frequently
train stats after 16960 examples: {'rewards_train/chosen': '-0.36385', 'rewards_train/rejected': '-0.96741', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.60356', 'logps_train/rejected': '-88.083', 'logps_train/chosen': '-115.76', 'loss/train': '0.50376', 'examples_per_second': '44.757', 'grad_norm': '17.702', 'counters/examples': 16960, 'counters/updates': 530}
skipping logging after 16992 examples to avoid logging too frequently
train stats after 17024 examples: {'rewards_train/chosen': '-0.76648', 'rewards_train/rejected': '-1.0681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30166', 'logps_train/rejected': '-144.68', 'logps_train/chosen': '-124.58', 'loss/train': '0.62754', 'examples_per_second': '45.657', 'grad_norm': '22.965', 'counters/examples': 17024, 'counters/updates': 532}
skipping logging after 17056 examples to avoid logging too frequently
train stats after 17088 examples: {'rewards_train/chosen': '-0.3887', 'rewards_train/rejected': '-0.95443', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.56573', 'logps_train/rejected': '-119.8', 'logps_train/chosen': '-160.45', 'loss/train': '0.5142', 'examples_per_second': '44.291', 'grad_norm': '22.259', 'counters/examples': 17088, 'counters/updates': 534}
skipping logging after 17120 examples to avoid logging too frequently
train stats after 17152 examples: {'rewards_train/chosen': '-0.32563', 'rewards_train/rejected': '-0.74674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4211', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-147.04', 'loss/train': '0.6164', 'examples_per_second': '44.254', 'grad_norm': '24.088', 'counters/examples': 17152, 'counters/updates': 536}
skipping logging after 17184 examples to avoid logging too frequently
train stats after 17216 examples: {'rewards_train/chosen': '-0.70184', 'rewards_train/rejected': '-1.0911', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.38923', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-130.95', 'loss/train': '0.65211', 'examples_per_second': '44.561', 'grad_norm': '22.608', 'counters/examples': 17216, 'counters/updates': 538}
skipping logging after 17248 examples to avoid logging too frequently
train stats after 17280 examples: {'rewards_train/chosen': '-1.0257', 'rewards_train/rejected': '-1.1932', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16746', 'logps_train/rejected': '-146.55', 'logps_train/chosen': '-126.01', 'loss/train': '0.72574', 'examples_per_second': '45.349', 'grad_norm': '29.678', 'counters/examples': 17280, 'counters/updates': 540}
skipping logging after 17312 examples to avoid logging too frequently
train stats after 17344 examples: {'rewards_train/chosen': '-0.65739', 'rewards_train/rejected': '-1.2281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57076', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-127.96', 'loss/train': '0.5776', 'examples_per_second': '47.037', 'grad_norm': '22.487', 'counters/examples': 17344, 'counters/updates': 542}
skipping logging after 17376 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.64727', 'rewards_train/rejected': '-1.04', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.39276', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-150.1', 'loss/train': '0.62406', 'examples_per_second': '44.422', 'grad_norm': '23.803', 'counters/examples': 17408, 'counters/updates': 544}
skipping logging after 17440 examples to avoid logging too frequently
train stats after 17472 examples: {'rewards_train/chosen': '-0.68439', 'rewards_train/rejected': '-1.0596', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3752', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-122.94', 'loss/train': '0.60394', 'examples_per_second': '44.495', 'grad_norm': '22.444', 'counters/examples': 17472, 'counters/updates': 546}
skipping logging after 17504 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '-0.71226', 'rewards_train/rejected': '-1.0421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32986', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-137.6', 'loss/train': '0.63102', 'examples_per_second': '46.404', 'grad_norm': '24.212', 'counters/examples': 17536, 'counters/updates': 548}
skipping logging after 17568 examples to avoid logging too frequently
train stats after 17600 examples: {'rewards_train/chosen': '-0.51455', 'rewards_train/rejected': '-0.90599', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39144', 'logps_train/rejected': '-160.27', 'logps_train/chosen': '-160.06', 'loss/train': '0.58087', 'examples_per_second': '48.554', 'grad_norm': '25.811', 'counters/examples': 17600, 'counters/updates': 550}
skipping logging after 17632 examples to avoid logging too frequently
train stats after 17664 examples: {'rewards_train/chosen': '-0.57775', 'rewards_train/rejected': '-1.2214', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.64361', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-137.73', 'loss/train': '0.52871', 'examples_per_second': '44.513', 'grad_norm': '21.243', 'counters/examples': 17664, 'counters/updates': 552}
skipping logging after 17696 examples to avoid logging too frequently
train stats after 17728 examples: {'rewards_train/chosen': '-0.51816', 'rewards_train/rejected': '-0.91394', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39578', 'logps_train/rejected': '-147.52', 'logps_train/chosen': '-144.85', 'loss/train': '0.65555', 'examples_per_second': '46.324', 'grad_norm': '25.987', 'counters/examples': 17728, 'counters/updates': 554}
skipping logging after 17760 examples to avoid logging too frequently
train stats after 17792 examples: {'rewards_train/chosen': '-0.81761', 'rewards_train/rejected': '-1.3005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48288', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-105.81', 'loss/train': '0.58805', 'examples_per_second': '47.124', 'grad_norm': '20.427', 'counters/examples': 17792, 'counters/updates': 556}
skipping logging after 17824 examples to avoid logging too frequently
train stats after 17856 examples: {'rewards_train/chosen': '-0.52982', 'rewards_train/rejected': '-1.1282', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.59841', 'logps_train/rejected': '-150.48', 'logps_train/chosen': '-188.77', 'loss/train': '0.49894', 'examples_per_second': '45.484', 'grad_norm': '23.907', 'counters/examples': 17856, 'counters/updates': 558}
skipping logging after 17888 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '-0.57477', 'rewards_train/rejected': '-1.0582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.48348', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-117.65', 'loss/train': '0.62468', 'examples_per_second': '48.01', 'grad_norm': '23.945', 'counters/examples': 17920, 'counters/updates': 560}
skipping logging after 17952 examples to avoid logging too frequently
train stats after 17984 examples: {'rewards_train/chosen': '-0.38227', 'rewards_train/rejected': '-0.81928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.437', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-135.42', 'loss/train': '0.60376', 'examples_per_second': '47.525', 'grad_norm': '26.129', 'counters/examples': 17984, 'counters/updates': 562}
skipping logging after 18016 examples to avoid logging too frequently
train stats after 18048 examples: {'rewards_train/chosen': '-0.39949', 'rewards_train/rejected': '-0.84184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44235', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-144.38', 'loss/train': '0.59155', 'examples_per_second': '45.497', 'grad_norm': '26.924', 'counters/examples': 18048, 'counters/updates': 564}
skipping logging after 18080 examples to avoid logging too frequently
train stats after 18112 examples: {'rewards_train/chosen': '-0.43182', 'rewards_train/rejected': '-1.1483', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.71652', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-112.36', 'loss/train': '0.49482', 'examples_per_second': '47.193', 'grad_norm': '19.262', 'counters/examples': 18112, 'counters/updates': 566}
skipping logging after 18144 examples to avoid logging too frequently
train stats after 18176 examples: {'rewards_train/chosen': '-0.59434', 'rewards_train/rejected': '-1.0816', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.48723', 'logps_train/rejected': '-120.8', 'logps_train/chosen': '-169.67', 'loss/train': '0.59527', 'examples_per_second': '47.738', 'grad_norm': '27.241', 'counters/examples': 18176, 'counters/updates': 568}
skipping logging after 18208 examples to avoid logging too frequently
train stats after 18240 examples: {'rewards_train/chosen': '-0.67886', 'rewards_train/rejected': '-1.1217', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.44284', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-157.01', 'loss/train': '0.60603', 'examples_per_second': '52.783', 'grad_norm': '26.439', 'counters/examples': 18240, 'counters/updates': 570}
skipping logging after 18272 examples to avoid logging too frequently
train stats after 18304 examples: {'rewards_train/chosen': '-0.33951', 'rewards_train/rejected': '-0.92436', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58485', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-166.31', 'loss/train': '0.49765', 'examples_per_second': '52.858', 'grad_norm': '23.17', 'counters/examples': 18304, 'counters/updates': 572}
skipping logging after 18336 examples to avoid logging too frequently
train stats after 18368 examples: {'rewards_train/chosen': '-0.49108', 'rewards_train/rejected': '-1.2328', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.74174', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-133.29', 'loss/train': '0.46509', 'examples_per_second': '45.449', 'grad_norm': '20.577', 'counters/examples': 18368, 'counters/updates': 574}
skipping logging after 18400 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '-0.063508', 'rewards_train/rejected': '-0.62074', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.55724', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-140.41', 'loss/train': '0.53287', 'examples_per_second': '44.419', 'grad_norm': '24.035', 'counters/examples': 18432, 'counters/updates': 576}
skipping logging after 18464 examples to avoid logging too frequently
train stats after 18496 examples: {'rewards_train/chosen': '-0.39463', 'rewards_train/rejected': '-0.932', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53737', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-124.44', 'loss/train': '0.54862', 'examples_per_second': '45.865', 'grad_norm': '23.34', 'counters/examples': 18496, 'counters/updates': 578}
skipping logging after 18528 examples to avoid logging too frequently
train stats after 18560 examples: {'rewards_train/chosen': '-0.26791', 'rewards_train/rejected': '-0.99824', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.73033', 'logps_train/rejected': '-123.33', 'logps_train/chosen': '-120.87', 'loss/train': '0.49789', 'examples_per_second': '32.107', 'grad_norm': '19.795', 'counters/examples': 18560, 'counters/updates': 580}
skipping logging after 18592 examples to avoid logging too frequently
train stats after 18624 examples: {'rewards_train/chosen': '-0.053651', 'rewards_train/rejected': '-0.83176', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.77811', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-166.14', 'loss/train': '0.49886', 'examples_per_second': '45.454', 'grad_norm': '24.033', 'counters/examples': 18624, 'counters/updates': 582}
skipping logging after 18656 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.37489', 'rewards_train/rejected': '-0.75304', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37815', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-146.15', 'loss/train': '0.67274', 'examples_per_second': '45.511', 'grad_norm': '24.097', 'counters/examples': 18688, 'counters/updates': 584}
skipping logging after 18720 examples to avoid logging too frequently
train stats after 18752 examples: {'rewards_train/chosen': '-0.34922', 'rewards_train/rejected': '-0.79249', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.44327', 'logps_train/rejected': '-148.94', 'logps_train/chosen': '-151.4', 'loss/train': '0.58397', 'examples_per_second': '44.648', 'grad_norm': '24.995', 'counters/examples': 18752, 'counters/updates': 586}
skipping logging after 18784 examples to avoid logging too frequently
train stats after 18816 examples: {'rewards_train/chosen': '-0.28796', 'rewards_train/rejected': '-0.4996', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21164', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-143.49', 'loss/train': '0.66806', 'examples_per_second': '44.164', 'grad_norm': '28.372', 'counters/examples': 18816, 'counters/updates': 588}
skipping logging after 18848 examples to avoid logging too frequently
train stats after 18880 examples: {'rewards_train/chosen': '-0.13021', 'rewards_train/rejected': '-0.68138', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55118', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-171.47', 'loss/train': '0.55255', 'examples_per_second': '46.485', 'grad_norm': '25.905', 'counters/examples': 18880, 'counters/updates': 590}
skipping logging after 18912 examples to avoid logging too frequently
train stats after 18944 examples: {'rewards_train/chosen': '-0.58308', 'rewards_train/rejected': '-0.61669', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033601', 'logps_train/rejected': '-149.01', 'logps_train/chosen': '-139.75', 'loss/train': '0.78396', 'examples_per_second': '45.37', 'grad_norm': '29.699', 'counters/examples': 18944, 'counters/updates': 592}
skipping logging after 18976 examples to avoid logging too frequently
train stats after 19008 examples: {'rewards_train/chosen': '-0.4587', 'rewards_train/rejected': '-0.85973', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.40104', 'logps_train/rejected': '-143.06', 'logps_train/chosen': '-159.22', 'loss/train': '0.62008', 'examples_per_second': '45.515', 'grad_norm': '27.261', 'counters/examples': 19008, 'counters/updates': 594}
skipping logging after 19040 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '-0.43778', 'rewards_train/rejected': '-0.68965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25187', 'logps_train/rejected': '-144.6', 'logps_train/chosen': '-165.78', 'loss/train': '0.6715', 'examples_per_second': '44.249', 'grad_norm': '27.968', 'counters/examples': 19072, 'counters/updates': 596}
skipping logging after 19104 examples to avoid logging too frequently
train stats after 19136 examples: {'rewards_train/chosen': '-0.093554', 'rewards_train/rejected': '-0.65458', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56102', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-127.62', 'loss/train': '0.61087', 'examples_per_second': '45.835', 'grad_norm': '25.421', 'counters/examples': 19136, 'counters/updates': 598}
skipping logging after 19168 examples to avoid logging too frequently
train stats after 19200 examples: {'rewards_train/chosen': '-0.29496', 'rewards_train/rejected': '-0.68976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3948', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-168.89', 'loss/train': '0.59699', 'examples_per_second': '44.151', 'grad_norm': '24.338', 'counters/examples': 19200, 'counters/updates': 600}
skipping logging after 19232 examples to avoid logging too frequently
train stats after 19264 examples: {'rewards_train/chosen': '-0.34513', 'rewards_train/rejected': '-0.85466', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50953', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-143.47', 'loss/train': '0.60208', 'examples_per_second': '46.551', 'grad_norm': '26.053', 'counters/examples': 19264, 'counters/updates': 602}
skipping logging after 19296 examples to avoid logging too frequently
train stats after 19328 examples: {'rewards_train/chosen': '-0.31093', 'rewards_train/rejected': '-0.37755', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066621', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-143.27', 'loss/train': '0.76394', 'examples_per_second': '45.823', 'grad_norm': '28.8', 'counters/examples': 19328, 'counters/updates': 604}
skipping logging after 19360 examples to avoid logging too frequently
train stats after 19392 examples: {'rewards_train/chosen': '-0.05211', 'rewards_train/rejected': '-0.36349', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.31138', 'logps_train/rejected': '-152.98', 'logps_train/chosen': '-132.82', 'loss/train': '0.64512', 'examples_per_second': '45.529', 'grad_norm': '24.796', 'counters/examples': 19392, 'counters/updates': 606}
skipping logging after 19424 examples to avoid logging too frequently
train stats after 19456 examples: {'rewards_train/chosen': '-0.46837', 'rewards_train/rejected': '-0.94678', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47841', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-194.12', 'loss/train': '0.56424', 'examples_per_second': '51.909', 'grad_norm': '25.033', 'counters/examples': 19456, 'counters/updates': 608}
skipping logging after 19488 examples to avoid logging too frequently
train stats after 19520 examples: {'rewards_train/chosen': '-0.73184', 'rewards_train/rejected': '-1.081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.34918', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-134.21', 'loss/train': '0.60815', 'examples_per_second': '45.044', 'grad_norm': '22.57', 'counters/examples': 19520, 'counters/updates': 610}
skipping logging after 19552 examples to avoid logging too frequently
train stats after 19584 examples: {'rewards_train/chosen': '-0.69612', 'rewards_train/rejected': '-1.0932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39706', 'logps_train/rejected': '-170.08', 'logps_train/chosen': '-156.37', 'loss/train': '0.58582', 'examples_per_second': '45.718', 'grad_norm': '24.021', 'counters/examples': 19584, 'counters/updates': 612}
skipping logging after 19616 examples to avoid logging too frequently
train stats after 19648 examples: {'rewards_train/chosen': '-0.39924', 'rewards_train/rejected': '-0.77776', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37852', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-148.27', 'loss/train': '0.60485', 'examples_per_second': '44.564', 'grad_norm': '23.98', 'counters/examples': 19648, 'counters/updates': 614}
skipping logging after 19680 examples to avoid logging too frequently
train stats after 19712 examples: {'rewards_train/chosen': '-0.42851', 'rewards_train/rejected': '-0.72451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.296', 'logps_train/rejected': '-123.79', 'logps_train/chosen': '-117.66', 'loss/train': '0.68361', 'examples_per_second': '45.414', 'grad_norm': '27.35', 'counters/examples': 19712, 'counters/updates': 616}
skipping logging after 19744 examples to avoid logging too frequently
train stats after 19776 examples: {'rewards_train/chosen': '-0.24292', 'rewards_train/rejected': '-0.67494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43202', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-134.59', 'loss/train': '0.57989', 'examples_per_second': '44.633', 'grad_norm': '23.555', 'counters/examples': 19776, 'counters/updates': 618}
skipping logging after 19808 examples to avoid logging too frequently
train stats after 19840 examples: {'rewards_train/chosen': '-0.54743', 'rewards_train/rejected': '-0.93131', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38388', 'logps_train/rejected': '-143.69', 'logps_train/chosen': '-164.54', 'loss/train': '0.56758', 'examples_per_second': '46.323', 'grad_norm': '23.76', 'counters/examples': 19840, 'counters/updates': 620}
skipping logging after 19872 examples to avoid logging too frequently
train stats after 19904 examples: {'rewards_train/chosen': '-0.42147', 'rewards_train/rejected': '-0.92015', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49868', 'logps_train/rejected': '-164.91', 'logps_train/chosen': '-159.51', 'loss/train': '0.59381', 'examples_per_second': '44.141', 'grad_norm': '25.029', 'counters/examples': 19904, 'counters/updates': 622}
skipping logging after 19936 examples to avoid logging too frequently
train stats after 19968 examples: {'rewards_train/chosen': '-0.41514', 'rewards_train/rejected': '-1.0167', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.60158', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-151.67', 'loss/train': '0.50027', 'examples_per_second': '47.87', 'grad_norm': '22.101', 'counters/examples': 19968, 'counters/updates': 624}
skipping logging after 20000 examples to avoid logging too frequently
train stats after 20032 examples: {'rewards_train/chosen': '-0.27006', 'rewards_train/rejected': '-0.67731', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.40725', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-143.72', 'loss/train': '0.58168', 'examples_per_second': '44.259', 'grad_norm': '20.418', 'counters/examples': 20032, 'counters/updates': 626}
skipping logging after 20064 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '-0.21555', 'rewards_train/rejected': '-0.59882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38328', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-163.57', 'loss/train': '0.61553', 'examples_per_second': '48.771', 'grad_norm': '25.847', 'counters/examples': 20096, 'counters/updates': 628}
skipping logging after 20128 examples to avoid logging too frequently
train stats after 20160 examples: {'rewards_train/chosen': '-0.12157', 'rewards_train/rejected': '-0.54161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42004', 'logps_train/rejected': '-136.07', 'logps_train/chosen': '-150.29', 'loss/train': '0.57402', 'examples_per_second': '45.627', 'grad_norm': '24.121', 'counters/examples': 20160, 'counters/updates': 630}
skipping logging after 20192 examples to avoid logging too frequently
train stats after 20224 examples: {'rewards_train/chosen': '-0.37387', 'rewards_train/rejected': '-1.216', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.84211', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-194.98', 'loss/train': '0.48729', 'examples_per_second': '45.396', 'grad_norm': '22.177', 'counters/examples': 20224, 'counters/updates': 632}
skipping logging after 20256 examples to avoid logging too frequently
train stats after 20288 examples: {'rewards_train/chosen': '-0.39955', 'rewards_train/rejected': '-0.92136', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52181', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-130.99', 'loss/train': '0.53606', 'examples_per_second': '44.278', 'grad_norm': '22.535', 'counters/examples': 20288, 'counters/updates': 634}
skipping logging after 20320 examples to avoid logging too frequently
train stats after 20352 examples: {'rewards_train/chosen': '-0.54282', 'rewards_train/rejected': '-1.2162', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.6734', 'logps_train/rejected': '-146.75', 'logps_train/chosen': '-154.54', 'loss/train': '0.51769', 'examples_per_second': '45.402', 'grad_norm': '25.193', 'counters/examples': 20352, 'counters/updates': 636}
skipping logging after 20384 examples to avoid logging too frequently
train stats after 20416 examples: {'rewards_train/chosen': '-0.1173', 'rewards_train/rejected': '-0.56723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.44993', 'logps_train/rejected': '-125.2', 'logps_train/chosen': '-147.23', 'loss/train': '0.55767', 'examples_per_second': '44.982', 'grad_norm': '23.519', 'counters/examples': 20416, 'counters/updates': 638}
skipping logging after 20448 examples to avoid logging too frequently
train stats after 20480 examples: {'rewards_train/chosen': '-0.37136', 'rewards_train/rejected': '-0.88729', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.51593', 'logps_train/rejected': '-190.01', 'logps_train/chosen': '-151.52', 'loss/train': '0.58483', 'examples_per_second': '45.574', 'grad_norm': '27.51', 'counters/examples': 20480, 'counters/updates': 640}
skipping logging after 20512 examples to avoid logging too frequently
train stats after 20544 examples: {'rewards_train/chosen': '-0.57517', 'rewards_train/rejected': '-0.89841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32324', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-141.46', 'loss/train': '0.66114', 'examples_per_second': '44.997', 'grad_norm': '24.448', 'counters/examples': 20544, 'counters/updates': 642}
skipping logging after 20576 examples to avoid logging too frequently
train stats after 20608 examples: {'rewards_train/chosen': '-0.29021', 'rewards_train/rejected': '-1.0059', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71569', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-173.98', 'loss/train': '0.49517', 'examples_per_second': '46.797', 'grad_norm': '24.456', 'counters/examples': 20608, 'counters/updates': 644}
skipping logging after 20640 examples to avoid logging too frequently
train stats after 20672 examples: {'rewards_train/chosen': '-0.1263', 'rewards_train/rejected': '-0.54369', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41739', 'logps_train/rejected': '-102.33', 'logps_train/chosen': '-129.7', 'loss/train': '0.57428', 'examples_per_second': '44.365', 'grad_norm': '21.342', 'counters/examples': 20672, 'counters/updates': 646}
skipping logging after 20704 examples to avoid logging too frequently
train stats after 20736 examples: {'rewards_train/chosen': '-0.19545', 'rewards_train/rejected': '-0.82986', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.63441', 'logps_train/rejected': '-145.05', 'logps_train/chosen': '-190.9', 'loss/train': '0.56077', 'examples_per_second': '44.247', 'grad_norm': '26.161', 'counters/examples': 20736, 'counters/updates': 648}
skipping logging after 20768 examples to avoid logging too frequently
train stats after 20800 examples: {'rewards_train/chosen': '-0.55769', 'rewards_train/rejected': '-0.83426', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.27657', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-116.59', 'loss/train': '0.6458', 'examples_per_second': '44.909', 'grad_norm': '25.061', 'counters/examples': 20800, 'counters/updates': 650}
skipping logging after 20832 examples to avoid logging too frequently
train stats after 20864 examples: {'rewards_train/chosen': '-0.58459', 'rewards_train/rejected': '-0.98212', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39752', 'logps_train/rejected': '-132.98', 'logps_train/chosen': '-160.19', 'loss/train': '0.58801', 'examples_per_second': '46.814', 'grad_norm': '25.154', 'counters/examples': 20864, 'counters/updates': 652}
skipping logging after 20896 examples to avoid logging too frequently
train stats after 20928 examples: {'rewards_train/chosen': '-0.53752', 'rewards_train/rejected': '-1.1846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.64704', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-135.41', 'loss/train': '0.55704', 'examples_per_second': '45.367', 'grad_norm': '24.065', 'counters/examples': 20928, 'counters/updates': 654}
skipping logging after 20960 examples to avoid logging too frequently
train stats after 20992 examples: {'rewards_train/chosen': '-0.45021', 'rewards_train/rejected': '-0.89108', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.44087', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-154.03', 'loss/train': '0.5799', 'examples_per_second': '45.417', 'grad_norm': '25.647', 'counters/examples': 20992, 'counters/updates': 656}
skipping logging after 21024 examples to avoid logging too frequently
train stats after 21056 examples: {'rewards_train/chosen': '-0.45081', 'rewards_train/rejected': '-0.80308', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.35227', 'logps_train/rejected': '-141.04', 'logps_train/chosen': '-169.38', 'loss/train': '0.63869', 'examples_per_second': '45.325', 'grad_norm': '27.962', 'counters/examples': 21056, 'counters/updates': 658}
skipping logging after 21088 examples to avoid logging too frequently
train stats after 21120 examples: {'rewards_train/chosen': '-0.58053', 'rewards_train/rejected': '-1.0231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44259', 'logps_train/rejected': '-144.07', 'logps_train/chosen': '-142.62', 'loss/train': '0.62607', 'examples_per_second': '45.567', 'grad_norm': '25.37', 'counters/examples': 21120, 'counters/updates': 660}
skipping logging after 21152 examples to avoid logging too frequently
train stats after 21184 examples: {'rewards_train/chosen': '-0.18333', 'rewards_train/rejected': '-0.43534', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25201', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-132.44', 'loss/train': '0.64305', 'examples_per_second': '44.725', 'grad_norm': '23.977', 'counters/examples': 21184, 'counters/updates': 662}
skipping logging after 21216 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '-0.26634', 'rewards_train/rejected': '-0.65368', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38734', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-115.49', 'loss/train': '0.5932', 'examples_per_second': '46.144', 'grad_norm': '23.042', 'counters/examples': 21248, 'counters/updates': 664}
skipping logging after 21280 examples to avoid logging too frequently
train stats after 21312 examples: {'rewards_train/chosen': '-0.18382', 'rewards_train/rejected': '-0.8828', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.69898', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-137.24', 'loss/train': '0.49048', 'examples_per_second': '46.455', 'grad_norm': '22.099', 'counters/examples': 21312, 'counters/updates': 666}
skipping logging after 21344 examples to avoid logging too frequently
train stats after 21376 examples: {'rewards_train/chosen': '0.022943', 'rewards_train/rejected': '-0.75993', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.78288', 'logps_train/rejected': '-168.45', 'logps_train/chosen': '-126.26', 'loss/train': '0.42464', 'examples_per_second': '45.639', 'grad_norm': '18.35', 'counters/examples': 21376, 'counters/updates': 668}
skipping logging after 21408 examples to avoid logging too frequently
train stats after 21440 examples: {'rewards_train/chosen': '-0.091087', 'rewards_train/rejected': '-0.67865', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.58756', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-161.62', 'loss/train': '0.6527', 'examples_per_second': '45.246', 'grad_norm': '27.844', 'counters/examples': 21440, 'counters/updates': 670}
skipping logging after 21472 examples to avoid logging too frequently
train stats after 21504 examples: {'rewards_train/chosen': '0.17794', 'rewards_train/rejected': '-0.8471', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.025', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-153.12', 'loss/train': '0.44457', 'examples_per_second': '45.399', 'grad_norm': '22.543', 'counters/examples': 21504, 'counters/updates': 672}
skipping logging after 21536 examples to avoid logging too frequently
train stats after 21568 examples: {'rewards_train/chosen': '-0.23354', 'rewards_train/rejected': '-0.53828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30475', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-142.28', 'loss/train': '0.67489', 'examples_per_second': '44.308', 'grad_norm': '24.017', 'counters/examples': 21568, 'counters/updates': 674}
skipping logging after 21600 examples to avoid logging too frequently
train stats after 21632 examples: {'rewards_train/chosen': '-0.1065', 'rewards_train/rejected': '-0.67418', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.56767', 'logps_train/rejected': '-103.58', 'logps_train/chosen': '-109.54', 'loss/train': '0.48976', 'examples_per_second': '45.315', 'grad_norm': '18.697', 'counters/examples': 21632, 'counters/updates': 676}
skipping logging after 21664 examples to avoid logging too frequently
train stats after 21696 examples: {'rewards_train/chosen': '-0.050707', 'rewards_train/rejected': '-0.52653', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47582', 'logps_train/rejected': '-122.86', 'logps_train/chosen': '-134.23', 'loss/train': '0.57524', 'examples_per_second': '44.485', 'grad_norm': '22.178', 'counters/examples': 21696, 'counters/updates': 678}
skipping logging after 21728 examples to avoid logging too frequently
train stats after 21760 examples: {'rewards_train/chosen': '-0.045066', 'rewards_train/rejected': '-0.81447', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.7694', 'logps_train/rejected': '-105.48', 'logps_train/chosen': '-165.27', 'loss/train': '0.46637', 'examples_per_second': '52.62', 'grad_norm': '20.197', 'counters/examples': 21760, 'counters/updates': 680}
skipping logging after 21792 examples to avoid logging too frequently
train stats after 21824 examples: {'rewards_train/chosen': '-0.70177', 'rewards_train/rejected': '-1.1915', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48976', 'logps_train/rejected': '-128.43', 'logps_train/chosen': '-145.79', 'loss/train': '0.61232', 'examples_per_second': '44.804', 'grad_norm': '26.523', 'counters/examples': 21824, 'counters/updates': 682}
skipping logging after 21856 examples to avoid logging too frequently
train stats after 21888 examples: {'rewards_train/chosen': '-0.40198', 'rewards_train/rejected': '-0.99116', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58917', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-136.64', 'loss/train': '0.53105', 'examples_per_second': '47.202', 'grad_norm': '19.76', 'counters/examples': 21888, 'counters/updates': 684}
skipping logging after 21920 examples to avoid logging too frequently
train stats after 21952 examples: {'rewards_train/chosen': '-0.65236', 'rewards_train/rejected': '-1.271', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.6186', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-150.11', 'loss/train': '0.57141', 'examples_per_second': '44.896', 'grad_norm': '23.69', 'counters/examples': 21952, 'counters/updates': 686}
skipping logging after 21984 examples to avoid logging too frequently
train stats after 22016 examples: {'rewards_train/chosen': '-0.82778', 'rewards_train/rejected': '-1.1702', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.34245', 'logps_train/rejected': '-152.05', 'logps_train/chosen': '-112.58', 'loss/train': '0.68596', 'examples_per_second': '45.032', 'grad_norm': '27.746', 'counters/examples': 22016, 'counters/updates': 688}
skipping logging after 22048 examples to avoid logging too frequently
train stats after 22080 examples: {'rewards_train/chosen': '-0.40747', 'rewards_train/rejected': '-0.57138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16391', 'logps_train/rejected': '-177.25', 'logps_train/chosen': '-170.89', 'loss/train': '0.74511', 'examples_per_second': '45.478', 'grad_norm': '30.306', 'counters/examples': 22080, 'counters/updates': 690}
skipping logging after 22112 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.52978', 'rewards_train/rejected': '-0.94342', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41364', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-116.82', 'loss/train': '0.60393', 'examples_per_second': '44.45', 'grad_norm': '21.982', 'counters/examples': 22144, 'counters/updates': 692}
skipping logging after 22176 examples to avoid logging too frequently
train stats after 22208 examples: {'rewards_train/chosen': '-0.49406', 'rewards_train/rejected': '-0.98313', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48907', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-151.71', 'loss/train': '0.53918', 'examples_per_second': '45.793', 'grad_norm': '22.814', 'counters/examples': 22208, 'counters/updates': 694}
skipping logging after 22240 examples to avoid logging too frequently
train stats after 22272 examples: {'rewards_train/chosen': '-0.61237', 'rewards_train/rejected': '-1.1541', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54174', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-110.12', 'loss/train': '0.55651', 'examples_per_second': '44.058', 'grad_norm': '19.742', 'counters/examples': 22272, 'counters/updates': 696}
skipping logging after 22304 examples to avoid logging too frequently
train stats after 22336 examples: {'rewards_train/chosen': '-0.69417', 'rewards_train/rejected': '-1.2236', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52944', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-165.87', 'loss/train': '0.53701', 'examples_per_second': '45.4', 'grad_norm': '22.589', 'counters/examples': 22336, 'counters/updates': 698}
skipping logging after 22368 examples to avoid logging too frequently
train stats after 22400 examples: {'rewards_train/chosen': '-0.71978', 'rewards_train/rejected': '-1.3311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61134', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-139.39', 'loss/train': '0.52537', 'examples_per_second': '45.332', 'grad_norm': '23.657', 'counters/examples': 22400, 'counters/updates': 700}
skipping logging after 22432 examples to avoid logging too frequently
train stats after 22464 examples: {'rewards_train/chosen': '-0.65341', 'rewards_train/rejected': '-1.0005', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34713', 'logps_train/rejected': '-110.01', 'logps_train/chosen': '-147.45', 'loss/train': '0.61869', 'examples_per_second': '49.802', 'grad_norm': '25.295', 'counters/examples': 22464, 'counters/updates': 702}
skipping logging after 22496 examples to avoid logging too frequently
train stats after 22528 examples: {'rewards_train/chosen': '-0.74768', 'rewards_train/rejected': '-1.1564', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.40877', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-139.09', 'loss/train': '0.60819', 'examples_per_second': '45.287', 'grad_norm': '24.432', 'counters/examples': 22528, 'counters/updates': 704}
skipping logging after 22560 examples to avoid logging too frequently
train stats after 22592 examples: {'rewards_train/chosen': '-0.8057', 'rewards_train/rejected': '-1.0947', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.28899', 'logps_train/rejected': '-158.42', 'logps_train/chosen': '-149.61', 'loss/train': '0.63254', 'examples_per_second': '44.396', 'grad_norm': '27.404', 'counters/examples': 22592, 'counters/updates': 706}
skipping logging after 22624 examples to avoid logging too frequently
train stats after 22656 examples: {'rewards_train/chosen': '-0.73196', 'rewards_train/rejected': '-1.1863', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45436', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-133.89', 'loss/train': '0.58695', 'examples_per_second': '45.533', 'grad_norm': '23.732', 'counters/examples': 22656, 'counters/updates': 708}
skipping logging after 22688 examples to avoid logging too frequently
train stats after 22720 examples: {'rewards_train/chosen': '-0.48726', 'rewards_train/rejected': '-0.98157', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49431', 'logps_train/rejected': '-148.79', 'logps_train/chosen': '-122.82', 'loss/train': '0.57394', 'examples_per_second': '45.369', 'grad_norm': '23.866', 'counters/examples': 22720, 'counters/updates': 710}
skipping logging after 22752 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '-0.3216', 'rewards_train/rejected': '-0.78678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46518', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-152.51', 'loss/train': '0.5803', 'examples_per_second': '46.37', 'grad_norm': '26.642', 'counters/examples': 22784, 'counters/updates': 712}
skipping logging after 22816 examples to avoid logging too frequently
train stats after 22848 examples: {'rewards_train/chosen': '-0.24926', 'rewards_train/rejected': '-0.66402', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41476', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-127.82', 'loss/train': '0.62041', 'examples_per_second': '44.18', 'grad_norm': '27.841', 'counters/examples': 22848, 'counters/updates': 714}
skipping logging after 22880 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.61136', 'rewards_train/rejected': '-1.1672', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55589', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-152.76', 'loss/train': '0.66647', 'examples_per_second': '47.988', 'grad_norm': '30.887', 'counters/examples': 22912, 'counters/updates': 716}
skipping logging after 22944 examples to avoid logging too frequently
train stats after 22976 examples: {'rewards_train/chosen': '-0.44273', 'rewards_train/rejected': '-0.92802', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48529', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-127.72', 'loss/train': '0.62183', 'examples_per_second': '47.099', 'grad_norm': '24.999', 'counters/examples': 22976, 'counters/updates': 718}
skipping logging after 23008 examples to avoid logging too frequently
train stats after 23040 examples: {'rewards_train/chosen': '-0.40805', 'rewards_train/rejected': '-0.89221', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.48417', 'logps_train/rejected': '-116.55', 'logps_train/chosen': '-136.52', 'loss/train': '0.54658', 'examples_per_second': '44.32', 'grad_norm': '22.427', 'counters/examples': 23040, 'counters/updates': 720}
skipping logging after 23072 examples to avoid logging too frequently
train stats after 23104 examples: {'rewards_train/chosen': '-0.46239', 'rewards_train/rejected': '-0.80798', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34559', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-141.05', 'loss/train': '0.6577', 'examples_per_second': '47.62', 'grad_norm': '31.355', 'counters/examples': 23104, 'counters/updates': 722}
skipping logging after 23136 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '-0.52954', 'rewards_train/rejected': '-1.1257', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.59613', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-179.69', 'loss/train': '0.56862', 'examples_per_second': '45.61', 'grad_norm': '30.483', 'counters/examples': 23168, 'counters/updates': 724}
skipping logging after 23200 examples to avoid logging too frequently
train stats after 23232 examples: {'rewards_train/chosen': '-0.58492', 'rewards_train/rejected': '-1.5564', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.97148', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-124.34', 'loss/train': '0.41684', 'examples_per_second': '45.35', 'grad_norm': '19.475', 'counters/examples': 23232, 'counters/updates': 726}
skipping logging after 23264 examples to avoid logging too frequently
train stats after 23296 examples: {'rewards_train/chosen': '-0.72551', 'rewards_train/rejected': '-1.0306', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30507', 'logps_train/rejected': '-134.67', 'logps_train/chosen': '-168.16', 'loss/train': '0.65364', 'examples_per_second': '44.799', 'grad_norm': '28.124', 'counters/examples': 23296, 'counters/updates': 728}
skipping logging after 23328 examples to avoid logging too frequently
train stats after 23360 examples: {'rewards_train/chosen': '-0.96129', 'rewards_train/rejected': '-1.0897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12841', 'logps_train/rejected': '-152.61', 'logps_train/chosen': '-141.49', 'loss/train': '0.74273', 'examples_per_second': '45.766', 'grad_norm': '30.443', 'counters/examples': 23360, 'counters/updates': 730}
skipping logging after 23392 examples to avoid logging too frequently
train stats after 23424 examples: {'rewards_train/chosen': '-0.93782', 'rewards_train/rejected': '-1.1761', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.23824', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-139.37', 'loss/train': '0.68026', 'examples_per_second': '45.82', 'grad_norm': '27.282', 'counters/examples': 23424, 'counters/updates': 732}
skipping logging after 23456 examples to avoid logging too frequently
train stats after 23488 examples: {'rewards_train/chosen': '-0.5129', 'rewards_train/rejected': '-1.2662', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.75331', 'logps_train/rejected': '-172.69', 'logps_train/chosen': '-140.35', 'loss/train': '0.46615', 'examples_per_second': '45.435', 'grad_norm': '22.269', 'counters/examples': 23488, 'counters/updates': 734}
skipping logging after 23520 examples to avoid logging too frequently
train stats after 23552 examples: {'rewards_train/chosen': '-0.31435', 'rewards_train/rejected': '-1.0372', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.72286', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-126.54', 'loss/train': '0.52752', 'examples_per_second': '49.074', 'grad_norm': '24.21', 'counters/examples': 23552, 'counters/updates': 736}
skipping logging after 23584 examples to avoid logging too frequently
train stats after 23616 examples: {'rewards_train/chosen': '-0.30952', 'rewards_train/rejected': '-0.85842', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54891', 'logps_train/rejected': '-172.13', 'logps_train/chosen': '-153.48', 'loss/train': '0.58774', 'examples_per_second': '44.054', 'grad_norm': '27.164', 'counters/examples': 23616, 'counters/updates': 738}
skipping logging after 23648 examples to avoid logging too frequently
train stats after 23680 examples: {'rewards_train/chosen': '-0.45731', 'rewards_train/rejected': '-1.1235', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.66623', 'logps_train/rejected': '-107.95', 'logps_train/chosen': '-131.33', 'loss/train': '0.55615', 'examples_per_second': '44.157', 'grad_norm': '27.427', 'counters/examples': 23680, 'counters/updates': 740}
skipping logging after 23712 examples to avoid logging too frequently
train stats after 23744 examples: {'rewards_train/chosen': '-0.73167', 'rewards_train/rejected': '-1.382', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.65037', 'logps_train/rejected': '-94.781', 'logps_train/chosen': '-103.29', 'loss/train': '0.50034', 'examples_per_second': '52.584', 'grad_norm': '18.391', 'counters/examples': 23744, 'counters/updates': 742}
skipping logging after 23776 examples to avoid logging too frequently
train stats after 23808 examples: {'rewards_train/chosen': '-0.43789', 'rewards_train/rejected': '-1.0134', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.57546', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-147.86', 'loss/train': '0.51279', 'examples_per_second': '46.564', 'grad_norm': '24.029', 'counters/examples': 23808, 'counters/updates': 744}
skipping logging after 23840 examples to avoid logging too frequently
train stats after 23872 examples: {'rewards_train/chosen': '-0.53906', 'rewards_train/rejected': '-1.0735', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53441', 'logps_train/rejected': '-105.61', 'logps_train/chosen': '-124.33', 'loss/train': '0.59385', 'examples_per_second': '44.796', 'grad_norm': '21.305', 'counters/examples': 23872, 'counters/updates': 746}
skipping logging after 23904 examples to avoid logging too frequently
train stats after 23936 examples: {'rewards_train/chosen': '-0.51706', 'rewards_train/rejected': '-0.80791', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.29085', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-117.83', 'loss/train': '0.64641', 'examples_per_second': '49.541', 'grad_norm': '25.485', 'counters/examples': 23936, 'counters/updates': 748}
skipping logging after 23968 examples to avoid logging too frequently
train stats after 24000 examples: {'rewards_train/chosen': '-0.43486', 'rewards_train/rejected': '-0.8322', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39733', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-146.47', 'loss/train': '0.63663', 'examples_per_second': '45.531', 'grad_norm': '23.504', 'counters/examples': 24000, 'counters/updates': 750}
Running evaluation after 24000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.85it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.96it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.97it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.89it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.33it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.19it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.12it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.14it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.03it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.04it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  7.03it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.05it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.93it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.99it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.01it/s]
eval after 24000: {'rewards_eval/chosen': '-0.32255', 'rewards_eval/rejected': '-0.74842', 'rewards_eval/accuracies': '0.63672', 'rewards_eval/margins': '0.42588', 'logps_eval/rejected': '-125.77', 'logps_eval/chosen': '-139.68', 'loss/eval': '0.63328'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-24000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-24000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-24000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-24000/scheduler.pt...
train stats after 24032 examples: {'rewards_train/chosen': '-0.37592', 'rewards_train/rejected': '-0.77825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40233', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-124.25', 'loss/train': '0.66738', 'examples_per_second': '30.562', 'grad_norm': '26.58', 'counters/examples': 24032, 'counters/updates': 751}
skipping logging after 24064 examples to avoid logging too frequently
train stats after 24096 examples: {'rewards_train/chosen': '-0.58164', 'rewards_train/rejected': '-1.1076', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52598', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-158.57', 'loss/train': '0.60423', 'examples_per_second': '45.847', 'grad_norm': '26.849', 'counters/examples': 24096, 'counters/updates': 753}
skipping logging after 24128 examples to avoid logging too frequently
train stats after 24160 examples: {'rewards_train/chosen': '-0.37022', 'rewards_train/rejected': '-1.3794', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '1.0091', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-129.7', 'loss/train': '0.39511', 'examples_per_second': '46.26', 'grad_norm': '15.899', 'counters/examples': 24160, 'counters/updates': 755}
skipping logging after 24192 examples to avoid logging too frequently
train stats after 24224 examples: {'rewards_train/chosen': '-0.97622', 'rewards_train/rejected': '-1.315', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33881', 'logps_train/rejected': '-114.81', 'logps_train/chosen': '-181.24', 'loss/train': '0.67376', 'examples_per_second': '44.971', 'grad_norm': '29.192', 'counters/examples': 24224, 'counters/updates': 757}
skipping logging after 24256 examples to avoid logging too frequently
train stats after 24288 examples: {'rewards_train/chosen': '-1.0777', 'rewards_train/rejected': '-1.4636', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38591', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-144.86', 'loss/train': '0.59619', 'examples_per_second': '46.429', 'grad_norm': '23.843', 'counters/examples': 24288, 'counters/updates': 759}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24352 examples: {'rewards_train/chosen': '-1.0565', 'rewards_train/rejected': '-1.6377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58127', 'logps_train/rejected': '-145.14', 'logps_train/chosen': '-142.54', 'loss/train': '0.6049', 'examples_per_second': '45.149', 'grad_norm': '24.982', 'counters/examples': 24352, 'counters/updates': 761}
skipping logging after 24384 examples to avoid logging too frequently
train stats after 24416 examples: {'rewards_train/chosen': '-0.82753', 'rewards_train/rejected': '-1.5317', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.70417', 'logps_train/rejected': '-142.57', 'logps_train/chosen': '-131.88', 'loss/train': '0.50065', 'examples_per_second': '44.315', 'grad_norm': '23.978', 'counters/examples': 24416, 'counters/updates': 763}
skipping logging after 24448 examples to avoid logging too frequently
train stats after 24480 examples: {'rewards_train/chosen': '-0.35245', 'rewards_train/rejected': '-1.0026', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.65018', 'logps_train/rejected': '-113.22', 'logps_train/chosen': '-144.22', 'loss/train': '0.54407', 'examples_per_second': '42.684', 'grad_norm': '22.027', 'counters/examples': 24480, 'counters/updates': 765}
skipping logging after 24512 examples to avoid logging too frequently
train stats after 24544 examples: {'rewards_train/chosen': '-0.27146', 'rewards_train/rejected': '-1.0581', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.78662', 'logps_train/rejected': '-155.93', 'logps_train/chosen': '-155.71', 'loss/train': '0.48768', 'examples_per_second': '45.449', 'grad_norm': '24.418', 'counters/examples': 24544, 'counters/updates': 767}
skipping logging after 24576 examples to avoid logging too frequently
train stats after 24608 examples: {'rewards_train/chosen': '-0.22877', 'rewards_train/rejected': '-0.91321', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.68444', 'logps_train/rejected': '-146.62', 'logps_train/chosen': '-131.98', 'loss/train': '0.50138', 'examples_per_second': '46.202', 'grad_norm': '21.741', 'counters/examples': 24608, 'counters/updates': 769}
skipping logging after 24640 examples to avoid logging too frequently
train stats after 24672 examples: {'rewards_train/chosen': '-0.45842', 'rewards_train/rejected': '-1.0701', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61166', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-133.06', 'loss/train': '0.53906', 'examples_per_second': '52.55', 'grad_norm': '23.966', 'counters/examples': 24672, 'counters/updates': 771}
skipping logging after 24704 examples to avoid logging too frequently
train stats after 24736 examples: {'rewards_train/chosen': '-0.72798', 'rewards_train/rejected': '-1.3663', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63829', 'logps_train/rejected': '-167.84', 'logps_train/chosen': '-137.96', 'loss/train': '0.50763', 'examples_per_second': '45.923', 'grad_norm': '24.661', 'counters/examples': 24736, 'counters/updates': 773}
skipping logging after 24768 examples to avoid logging too frequently
train stats after 24800 examples: {'rewards_train/chosen': '-0.3919', 'rewards_train/rejected': '-1.263', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.87114', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-144.41', 'loss/train': '0.47534', 'examples_per_second': '45.516', 'grad_norm': '22.665', 'counters/examples': 24800, 'counters/updates': 775}
skipping logging after 24832 examples to avoid logging too frequently
train stats after 24864 examples: {'rewards_train/chosen': '-0.70899', 'rewards_train/rejected': '-1.4285', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.71946', 'logps_train/rejected': '-155.32', 'logps_train/chosen': '-156.12', 'loss/train': '0.58046', 'examples_per_second': '44.909', 'grad_norm': '27.779', 'counters/examples': 24864, 'counters/updates': 777}
skipping logging after 24896 examples to avoid logging too frequently
train stats after 24928 examples: {'rewards_train/chosen': '-0.38159', 'rewards_train/rejected': '-0.92365', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54206', 'logps_train/rejected': '-110.39', 'logps_train/chosen': '-142.96', 'loss/train': '0.56078', 'examples_per_second': '47.46', 'grad_norm': '25.166', 'counters/examples': 24928, 'counters/updates': 779}
skipping logging after 24960 examples to avoid logging too frequently
train stats after 24992 examples: {'rewards_train/chosen': '-0.5717', 'rewards_train/rejected': '-1.1167', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54502', 'logps_train/rejected': '-159.24', 'logps_train/chosen': '-160.5', 'loss/train': '0.55044', 'examples_per_second': '44.704', 'grad_norm': '27.369', 'counters/examples': 24992, 'counters/updates': 781}
skipping logging after 25024 examples to avoid logging too frequently
train stats after 25056 examples: {'rewards_train/chosen': '-0.40697', 'rewards_train/rejected': '-0.65856', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2516', 'logps_train/rejected': '-158.82', 'logps_train/chosen': '-146.07', 'loss/train': '0.64462', 'examples_per_second': '45.742', 'grad_norm': '27.88', 'counters/examples': 25056, 'counters/updates': 783}
skipping logging after 25088 examples to avoid logging too frequently
train stats after 25120 examples: {'rewards_train/chosen': '-0.31694', 'rewards_train/rejected': '-0.52545', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20851', 'logps_train/rejected': '-139.68', 'logps_train/chosen': '-135.19', 'loss/train': '0.67072', 'examples_per_second': '44.694', 'grad_norm': '27.562', 'counters/examples': 25120, 'counters/updates': 785}
skipping logging after 25152 examples to avoid logging too frequently
train stats after 25184 examples: {'rewards_train/chosen': '-0.36689', 'rewards_train/rejected': '-0.81036', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.44347', 'logps_train/rejected': '-133.25', 'logps_train/chosen': '-142.7', 'loss/train': '0.61096', 'examples_per_second': '47.552', 'grad_norm': '25.869', 'counters/examples': 25184, 'counters/updates': 787}
skipping logging after 25216 examples to avoid logging too frequently
train stats after 25248 examples: {'rewards_train/chosen': '-0.23582', 'rewards_train/rejected': '-0.73565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.49984', 'logps_train/rejected': '-144.22', 'logps_train/chosen': '-118.3', 'loss/train': '0.6121', 'examples_per_second': '45.792', 'grad_norm': '23.675', 'counters/examples': 25248, 'counters/updates': 789}
skipping logging after 25280 examples to avoid logging too frequently
train stats after 25312 examples: {'rewards_train/chosen': '-0.32826', 'rewards_train/rejected': '-0.81824', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48999', 'logps_train/rejected': '-131.33', 'logps_train/chosen': '-137.83', 'loss/train': '0.59981', 'examples_per_second': '50.111', 'grad_norm': '26.239', 'counters/examples': 25312, 'counters/updates': 791}
skipping logging after 25344 examples to avoid logging too frequently
train stats after 25376 examples: {'rewards_train/chosen': '-0.53503', 'rewards_train/rejected': '-0.86065', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.32562', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-115.23', 'loss/train': '0.70087', 'examples_per_second': '52.5', 'grad_norm': '26.433', 'counters/examples': 25376, 'counters/updates': 793}
train stats after 25408 examples: {'rewards_train/chosen': '-0.64964', 'rewards_train/rejected': '-0.82398', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17434', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-145.73', 'loss/train': '0.70609', 'examples_per_second': '29.406', 'grad_norm': '30.079', 'counters/examples': 25408, 'counters/updates': 794}
skipping logging after 25440 examples to avoid logging too frequently
train stats after 25472 examples: {'rewards_train/chosen': '-0.87064', 'rewards_train/rejected': '-1.0367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16601', 'logps_train/rejected': '-130', 'logps_train/chosen': '-109.23', 'loss/train': '0.75244', 'examples_per_second': '46.081', 'grad_norm': '27.227', 'counters/examples': 25472, 'counters/updates': 796}
skipping logging after 25504 examples to avoid logging too frequently
train stats after 25536 examples: {'rewards_train/chosen': '-1.0383', 'rewards_train/rejected': '-1.6514', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61315', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-129.66', 'loss/train': '0.56051', 'examples_per_second': '45.439', 'grad_norm': '22.843', 'counters/examples': 25536, 'counters/updates': 798}
skipping logging after 25568 examples to avoid logging too frequently
train stats after 25600 examples: {'rewards_train/chosen': '-0.83401', 'rewards_train/rejected': '-1.4897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.65566', 'logps_train/rejected': '-146.23', 'logps_train/chosen': '-148.17', 'loss/train': '0.53626', 'examples_per_second': '45.473', 'grad_norm': '25.394', 'counters/examples': 25600, 'counters/updates': 800}
skipping logging after 25632 examples to avoid logging too frequently
train stats after 25664 examples: {'rewards_train/chosen': '-0.47718', 'rewards_train/rejected': '-1.4709', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.99369', 'logps_train/rejected': '-129.82', 'logps_train/chosen': '-133.81', 'loss/train': '0.41181', 'examples_per_second': '44.526', 'grad_norm': '20.217', 'counters/examples': 25664, 'counters/updates': 802}
skipping logging after 25696 examples to avoid logging too frequently
train stats after 25728 examples: {'rewards_train/chosen': '-0.53321', 'rewards_train/rejected': '-1.0402', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50695', 'logps_train/rejected': '-140.76', 'logps_train/chosen': '-160.97', 'loss/train': '0.57945', 'examples_per_second': '45.981', 'grad_norm': '27.002', 'counters/examples': 25728, 'counters/updates': 804}
skipping logging after 25760 examples to avoid logging too frequently
train stats after 25792 examples: {'rewards_train/chosen': '-0.42946', 'rewards_train/rejected': '-1.3812', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.95169', 'logps_train/rejected': '-134.53', 'logps_train/chosen': '-184.47', 'loss/train': '0.44941', 'examples_per_second': '47.771', 'grad_norm': '24.32', 'counters/examples': 25792, 'counters/updates': 806}
skipping logging after 25824 examples to avoid logging too frequently
train stats after 25856 examples: {'rewards_train/chosen': '-0.61831', 'rewards_train/rejected': '-1.0524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.43409', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-159.3', 'loss/train': '0.57883', 'examples_per_second': '45.513', 'grad_norm': '24.799', 'counters/examples': 25856, 'counters/updates': 808}
skipping logging after 25888 examples to avoid logging too frequently
train stats after 25920 examples: {'rewards_train/chosen': '-0.99745', 'rewards_train/rejected': '-1.3202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32273', 'logps_train/rejected': '-151.43', 'logps_train/chosen': '-172.75', 'loss/train': '0.689', 'examples_per_second': '45.565', 'grad_norm': '30.222', 'counters/examples': 25920, 'counters/updates': 810}
skipping logging after 25952 examples to avoid logging too frequently
train stats after 25984 examples: {'rewards_train/chosen': '-0.93972', 'rewards_train/rejected': '-1.4282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.48846', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-131.07', 'loss/train': '0.61584', 'examples_per_second': '46.371', 'grad_norm': '26.537', 'counters/examples': 25984, 'counters/updates': 812}
skipping logging after 26016 examples to avoid logging too frequently
train stats after 26048 examples: {'rewards_train/chosen': '-0.91763', 'rewards_train/rejected': '-1.2978', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38016', 'logps_train/rejected': '-131.5', 'logps_train/chosen': '-184.47', 'loss/train': '0.66779', 'examples_per_second': '44.633', 'grad_norm': '30.284', 'counters/examples': 26048, 'counters/updates': 814}
skipping logging after 26080 examples to avoid logging too frequently
train stats after 26112 examples: {'rewards_train/chosen': '-0.7171', 'rewards_train/rejected': '-1.088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37091', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-135.04', 'loss/train': '0.63255', 'examples_per_second': '52.546', 'grad_norm': '29.141', 'counters/examples': 26112, 'counters/updates': 816}
skipping logging after 26144 examples to avoid logging too frequently
train stats after 26176 examples: {'rewards_train/chosen': '-0.61917', 'rewards_train/rejected': '-1.3283', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70909', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-115.29', 'loss/train': '0.52985', 'examples_per_second': '44.445', 'grad_norm': '22.072', 'counters/examples': 26176, 'counters/updates': 818}
skipping logging after 26208 examples to avoid logging too frequently
train stats after 26240 examples: {'rewards_train/chosen': '-0.31894', 'rewards_train/rejected': '-0.73396', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41502', 'logps_train/rejected': '-95.349', 'logps_train/chosen': '-120.84', 'loss/train': '0.62295', 'examples_per_second': '44.63', 'grad_norm': '26.33', 'counters/examples': 26240, 'counters/updates': 820}
skipping logging after 26272 examples to avoid logging too frequently
train stats after 26304 examples: {'rewards_train/chosen': '-0.49593', 'rewards_train/rejected': '-0.73803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2421', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-155.9', 'loss/train': '0.7579', 'examples_per_second': '46.641', 'grad_norm': '29.923', 'counters/examples': 26304, 'counters/updates': 822}
skipping logging after 26336 examples to avoid logging too frequently
train stats after 26368 examples: {'rewards_train/chosen': '-0.56821', 'rewards_train/rejected': '-0.84001', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.2718', 'logps_train/rejected': '-142.59', 'logps_train/chosen': '-141.78', 'loss/train': '0.64131', 'examples_per_second': '44.814', 'grad_norm': '24.687', 'counters/examples': 26368, 'counters/updates': 824}
skipping logging after 26400 examples to avoid logging too frequently
train stats after 26432 examples: {'rewards_train/chosen': '-0.78815', 'rewards_train/rejected': '-1.0955', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30737', 'logps_train/rejected': '-161.66', 'logps_train/chosen': '-128.78', 'loss/train': '0.64411', 'examples_per_second': '51.397', 'grad_norm': '25.899', 'counters/examples': 26432, 'counters/updates': 826}
skipping logging after 26464 examples to avoid logging too frequently
train stats after 26496 examples: {'rewards_train/chosen': '-0.65229', 'rewards_train/rejected': '-0.95148', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.29919', 'logps_train/rejected': '-106.38', 'logps_train/chosen': '-127.88', 'loss/train': '0.64786', 'examples_per_second': '48.722', 'grad_norm': '22.518', 'counters/examples': 26496, 'counters/updates': 828}
skipping logging after 26528 examples to avoid logging too frequently
train stats after 26560 examples: {'rewards_train/chosen': '-0.71308', 'rewards_train/rejected': '-0.87216', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15907', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-120.93', 'loss/train': '0.67884', 'examples_per_second': '46.071', 'grad_norm': '24.619', 'counters/examples': 26560, 'counters/updates': 830}
skipping logging after 26592 examples to avoid logging too frequently
train stats after 26624 examples: {'rewards_train/chosen': '-0.50383', 'rewards_train/rejected': '-0.91003', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4062', 'logps_train/rejected': '-112.32', 'logps_train/chosen': '-133.7', 'loss/train': '0.63329', 'examples_per_second': '46.027', 'grad_norm': '26.334', 'counters/examples': 26624, 'counters/updates': 832}
skipping logging after 26656 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '-0.27958', 'rewards_train/rejected': '-1.0561', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.77654', 'logps_train/rejected': '-170.28', 'logps_train/chosen': '-171.12', 'loss/train': '0.51095', 'examples_per_second': '45.343', 'grad_norm': '26.944', 'counters/examples': 26688, 'counters/updates': 834}
skipping logging after 26720 examples to avoid logging too frequently
train stats after 26752 examples: {'rewards_train/chosen': '-0.52729', 'rewards_train/rejected': '-1.1111', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58378', 'logps_train/rejected': '-109.33', 'logps_train/chosen': '-121.61', 'loss/train': '0.57405', 'examples_per_second': '44.856', 'grad_norm': '21.954', 'counters/examples': 26752, 'counters/updates': 836}
skipping logging after 26784 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.59517', 'rewards_train/rejected': '-0.95756', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.36239', 'logps_train/rejected': '-121.31', 'logps_train/chosen': '-171.9', 'loss/train': '0.57113', 'examples_per_second': '45.538', 'grad_norm': '26.988', 'counters/examples': 26816, 'counters/updates': 838}
skipping logging after 26848 examples to avoid logging too frequently
train stats after 26880 examples: {'rewards_train/chosen': '-0.44214', 'rewards_train/rejected': '-1.491', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.0489', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-160', 'loss/train': '0.46828', 'examples_per_second': '45.69', 'grad_norm': '21.975', 'counters/examples': 26880, 'counters/updates': 840}
skipping logging after 26912 examples to avoid logging too frequently
train stats after 26944 examples: {'rewards_train/chosen': '-0.83233', 'rewards_train/rejected': '-1.4217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.58942', 'logps_train/rejected': '-178.38', 'logps_train/chosen': '-149.07', 'loss/train': '0.60911', 'examples_per_second': '47.426', 'grad_norm': '29.79', 'counters/examples': 26944, 'counters/updates': 842}
skipping logging after 26976 examples to avoid logging too frequently
train stats after 27008 examples: {'rewards_train/chosen': '-0.67802', 'rewards_train/rejected': '-1.0872', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.40915', 'logps_train/rejected': '-160.16', 'logps_train/chosen': '-137.67', 'loss/train': '0.63068', 'examples_per_second': '47.51', 'grad_norm': '23.963', 'counters/examples': 27008, 'counters/updates': 844}
skipping logging after 27040 examples to avoid logging too frequently
train stats after 27072 examples: {'rewards_train/chosen': '-0.82291', 'rewards_train/rejected': '-1.4908', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.6679', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-155.64', 'loss/train': '0.49674', 'examples_per_second': '44.924', 'grad_norm': '21.176', 'counters/examples': 27072, 'counters/updates': 846}
skipping logging after 27104 examples to avoid logging too frequently
train stats after 27136 examples: {'rewards_train/chosen': '-1.0244', 'rewards_train/rejected': '-1.2654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24096', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-125.36', 'loss/train': '0.68773', 'examples_per_second': '45.357', 'grad_norm': '23.989', 'counters/examples': 27136, 'counters/updates': 848}
skipping logging after 27168 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '-0.69567', 'rewards_train/rejected': '-1.1549', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45927', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-116.46', 'loss/train': '0.61348', 'examples_per_second': '44.175', 'grad_norm': '24.854', 'counters/examples': 27200, 'counters/updates': 850}
skipping logging after 27232 examples to avoid logging too frequently
train stats after 27264 examples: {'rewards_train/chosen': '-0.95289', 'rewards_train/rejected': '-1.0265', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073637', 'logps_train/rejected': '-125', 'logps_train/chosen': '-118.42', 'loss/train': '0.74191', 'examples_per_second': '48.043', 'grad_norm': '26.17', 'counters/examples': 27264, 'counters/updates': 852}
skipping logging after 27296 examples to avoid logging too frequently
train stats after 27328 examples: {'rewards_train/chosen': '-0.38302', 'rewards_train/rejected': '-1.1253', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.74225', 'logps_train/rejected': '-180.59', 'logps_train/chosen': '-151.59', 'loss/train': '0.48916', 'examples_per_second': '45.168', 'grad_norm': '24.322', 'counters/examples': 27328, 'counters/updates': 854}
skipping logging after 27360 examples to avoid logging too frequently
train stats after 27392 examples: {'rewards_train/chosen': '-0.43298', 'rewards_train/rejected': '-0.67607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24309', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-145.16', 'loss/train': '0.64657', 'examples_per_second': '45.144', 'grad_norm': '26.917', 'counters/examples': 27392, 'counters/updates': 856}
skipping logging after 27424 examples to avoid logging too frequently
train stats after 27456 examples: {'rewards_train/chosen': '-0.7237', 'rewards_train/rejected': '-0.75736', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033661', 'logps_train/rejected': '-150.81', 'logps_train/chosen': '-176.2', 'loss/train': '0.81269', 'examples_per_second': '45.549', 'grad_norm': '34.459', 'counters/examples': 27456, 'counters/updates': 858}
skipping logging after 27488 examples to avoid logging too frequently
train stats after 27520 examples: {'rewards_train/chosen': '-0.53284', 'rewards_train/rejected': '-1.2525', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.7197', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-152.6', 'loss/train': '0.47178', 'examples_per_second': '52.991', 'grad_norm': '19.845', 'counters/examples': 27520, 'counters/updates': 860}
skipping logging after 27552 examples to avoid logging too frequently
train stats after 27584 examples: {'rewards_train/chosen': '-0.6357', 'rewards_train/rejected': '-1.129', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.49335', 'logps_train/rejected': '-135.35', 'logps_train/chosen': '-134.73', 'loss/train': '0.56741', 'examples_per_second': '44.211', 'grad_norm': '22.489', 'counters/examples': 27584, 'counters/updates': 862}
skipping logging after 27616 examples to avoid logging too frequently
train stats after 27648 examples: {'rewards_train/chosen': '-0.52443', 'rewards_train/rejected': '-0.93856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41414', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-168.39', 'loss/train': '0.61549', 'examples_per_second': '48.184', 'grad_norm': '26.461', 'counters/examples': 27648, 'counters/updates': 864}
skipping logging after 27680 examples to avoid logging too frequently
train stats after 27712 examples: {'rewards_train/chosen': '-0.70142', 'rewards_train/rejected': '-0.99121', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28979', 'logps_train/rejected': '-161.5', 'logps_train/chosen': '-149.81', 'loss/train': '0.65591', 'examples_per_second': '31.953', 'grad_norm': '29.678', 'counters/examples': 27712, 'counters/updates': 866}
skipping logging after 27744 examples to avoid logging too frequently
train stats after 27776 examples: {'rewards_train/chosen': '-0.59027', 'rewards_train/rejected': '-1.0623', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.47203', 'logps_train/rejected': '-142.47', 'logps_train/chosen': '-143.1', 'loss/train': '0.59895', 'examples_per_second': '52.188', 'grad_norm': '22.75', 'counters/examples': 27776, 'counters/updates': 868}
skipping logging after 27808 examples to avoid logging too frequently
train stats after 27840 examples: {'rewards_train/chosen': '-0.80218', 'rewards_train/rejected': '-1.4955', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.69333', 'logps_train/rejected': '-130.39', 'logps_train/chosen': '-143.76', 'loss/train': '0.49553', 'examples_per_second': '45.853', 'grad_norm': '23.022', 'counters/examples': 27840, 'counters/updates': 870}
skipping logging after 27872 examples to avoid logging too frequently
train stats after 27904 examples: {'rewards_train/chosen': '-0.70364', 'rewards_train/rejected': '-0.98965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28601', 'logps_train/rejected': '-153.9', 'logps_train/chosen': '-153.29', 'loss/train': '0.65333', 'examples_per_second': '45.184', 'grad_norm': '26.252', 'counters/examples': 27904, 'counters/updates': 872}
skipping logging after 27936 examples to avoid logging too frequently
train stats after 27968 examples: {'rewards_train/chosen': '-0.53094', 'rewards_train/rejected': '-0.75275', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.22181', 'logps_train/rejected': '-135.2', 'logps_train/chosen': '-135.45', 'loss/train': '0.66612', 'examples_per_second': '44.648', 'grad_norm': '25.358', 'counters/examples': 27968, 'counters/updates': 874}
skipping logging after 28000 examples to avoid logging too frequently
train stats after 28032 examples: {'rewards_train/chosen': '-0.66175', 'rewards_train/rejected': '-0.6991', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037352', 'logps_train/rejected': '-186.61', 'logps_train/chosen': '-141.33', 'loss/train': '0.74991', 'examples_per_second': '44.692', 'grad_norm': '30.717', 'counters/examples': 28032, 'counters/updates': 876}
skipping logging after 28064 examples to avoid logging too frequently
train stats after 28096 examples: {'rewards_train/chosen': '-0.35283', 'rewards_train/rejected': '-0.94306', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59023', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-145.59', 'loss/train': '0.56335', 'examples_per_second': '45.529', 'grad_norm': '24.298', 'counters/examples': 28096, 'counters/updates': 878}
skipping logging after 28128 examples to avoid logging too frequently
train stats after 28160 examples: {'rewards_train/chosen': '-0.34866', 'rewards_train/rejected': '-0.5819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23324', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-134.3', 'loss/train': '0.68857', 'examples_per_second': '44.137', 'grad_norm': '27.474', 'counters/examples': 28160, 'counters/updates': 880}
skipping logging after 28192 examples to avoid logging too frequently
train stats after 28224 examples: {'rewards_train/chosen': '-0.65701', 'rewards_train/rejected': '-1.1845', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52753', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-133.77', 'loss/train': '0.5616', 'examples_per_second': '47.45', 'grad_norm': '24.343', 'counters/examples': 28224, 'counters/updates': 882}
skipping logging after 28256 examples to avoid logging too frequently
train stats after 28288 examples: {'rewards_train/chosen': '-0.42085', 'rewards_train/rejected': '-0.83445', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4136', 'logps_train/rejected': '-108.58', 'logps_train/chosen': '-126.02', 'loss/train': '0.60876', 'examples_per_second': '45.224', 'grad_norm': '22.639', 'counters/examples': 28288, 'counters/updates': 884}
skipping logging after 28320 examples to avoid logging too frequently
train stats after 28352 examples: {'rewards_train/chosen': '-0.40724', 'rewards_train/rejected': '-0.85826', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45101', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-117.24', 'loss/train': '0.57595', 'examples_per_second': '46.409', 'grad_norm': '21.551', 'counters/examples': 28352, 'counters/updates': 886}
skipping logging after 28384 examples to avoid logging too frequently
train stats after 28416 examples: {'rewards_train/chosen': '-0.044819', 'rewards_train/rejected': '-0.62057', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57575', 'logps_train/rejected': '-134.31', 'logps_train/chosen': '-140.9', 'loss/train': '0.58635', 'examples_per_second': '44.974', 'grad_norm': '22.916', 'counters/examples': 28416, 'counters/updates': 888}
skipping logging after 28448 examples to avoid logging too frequently
train stats after 28480 examples: {'rewards_train/chosen': '-0.16289', 'rewards_train/rejected': '-0.54781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.38492', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-116.33', 'loss/train': '0.62342', 'examples_per_second': '44.381', 'grad_norm': '22.113', 'counters/examples': 28480, 'counters/updates': 890}
skipping logging after 28512 examples to avoid logging too frequently
train stats after 28544 examples: {'rewards_train/chosen': '-0.58486', 'rewards_train/rejected': '-1.1781', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.59323', 'logps_train/rejected': '-139.79', 'logps_train/chosen': '-158.72', 'loss/train': '0.58036', 'examples_per_second': '44.065', 'grad_norm': '24.868', 'counters/examples': 28544, 'counters/updates': 892}
skipping logging after 28576 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.64896', 'rewards_train/rejected': '-1.1455', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.49659', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-165.42', 'loss/train': '0.63641', 'examples_per_second': '46.535', 'grad_norm': '27.881', 'counters/examples': 28608, 'counters/updates': 894}
skipping logging after 28640 examples to avoid logging too frequently
train stats after 28672 examples: {'rewards_train/chosen': '-0.44169', 'rewards_train/rejected': '-0.92087', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47917', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-142.53', 'loss/train': '0.6027', 'examples_per_second': '53.471', 'grad_norm': '23.275', 'counters/examples': 28672, 'counters/updates': 896}
skipping logging after 28704 examples to avoid logging too frequently
train stats after 28736 examples: {'rewards_train/chosen': '-0.67974', 'rewards_train/rejected': '-1.3112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.63142', 'logps_train/rejected': '-146.17', 'logps_train/chosen': '-148.41', 'loss/train': '0.58079', 'examples_per_second': '44.423', 'grad_norm': '25.038', 'counters/examples': 28736, 'counters/updates': 898}
skipping logging after 28768 examples to avoid logging too frequently
train stats after 28800 examples: {'rewards_train/chosen': '-0.40873', 'rewards_train/rejected': '-0.89404', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.48532', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-128.75', 'loss/train': '0.6144', 'examples_per_second': '45.458', 'grad_norm': '22.964', 'counters/examples': 28800, 'counters/updates': 900}
skipping logging after 28832 examples to avoid logging too frequently
train stats after 28864 examples: {'rewards_train/chosen': '-0.35119', 'rewards_train/rejected': '-0.89826', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54707', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-119.8', 'loss/train': '0.57243', 'examples_per_second': '46.25', 'grad_norm': '22.287', 'counters/examples': 28864, 'counters/updates': 902}
skipping logging after 28896 examples to avoid logging too frequently
train stats after 28928 examples: {'rewards_train/chosen': '-0.24235', 'rewards_train/rejected': '-0.97174', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.72939', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-130.93', 'loss/train': '0.50161', 'examples_per_second': '44.408', 'grad_norm': '21.464', 'counters/examples': 28928, 'counters/updates': 904}
skipping logging after 28960 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.50533', 'rewards_train/rejected': '-0.93988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.43455', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-117.13', 'loss/train': '0.63275', 'examples_per_second': '45.981', 'grad_norm': '24.204', 'counters/examples': 28992, 'counters/updates': 906}
skipping logging after 29024 examples to avoid logging too frequently
train stats after 29056 examples: {'rewards_train/chosen': '-0.5109', 'rewards_train/rejected': '-1.1752', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.6643', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-145.52', 'loss/train': '0.51579', 'examples_per_second': '45.282', 'grad_norm': '20.276', 'counters/examples': 29056, 'counters/updates': 908}
skipping logging after 29088 examples to avoid logging too frequently
train stats after 29120 examples: {'rewards_train/chosen': '-0.37149', 'rewards_train/rejected': '-0.77453', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.40304', 'logps_train/rejected': '-123.9', 'logps_train/chosen': '-159.16', 'loss/train': '0.60142', 'examples_per_second': '44.127', 'grad_norm': '23.667', 'counters/examples': 29120, 'counters/updates': 910}
skipping logging after 29152 examples to avoid logging too frequently
train stats after 29184 examples: {'rewards_train/chosen': '-0.62747', 'rewards_train/rejected': '-0.98142', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35395', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-157.5', 'loss/train': '0.61096', 'examples_per_second': '47.8', 'grad_norm': '23.931', 'counters/examples': 29184, 'counters/updates': 912}
skipping logging after 29216 examples to avoid logging too frequently
train stats after 29248 examples: {'rewards_train/chosen': '-0.78446', 'rewards_train/rejected': '-1.4847', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.70029', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-158.3', 'loss/train': '0.4921', 'examples_per_second': '45.397', 'grad_norm': '21.557', 'counters/examples': 29248, 'counters/updates': 914}
skipping logging after 29280 examples to avoid logging too frequently
train stats after 29312 examples: {'rewards_train/chosen': '-0.33584', 'rewards_train/rejected': '-1.4548', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.119', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-142.82', 'loss/train': '0.36715', 'examples_per_second': '44.492', 'grad_norm': '17.511', 'counters/examples': 29312, 'counters/updates': 916}
skipping logging after 29344 examples to avoid logging too frequently
train stats after 29376 examples: {'rewards_train/chosen': '-1.0157', 'rewards_train/rejected': '-1.3133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29756', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-142.15', 'loss/train': '0.626', 'examples_per_second': '44.355', 'grad_norm': '24.391', 'counters/examples': 29376, 'counters/updates': 918}
skipping logging after 29408 examples to avoid logging too frequently
train stats after 29440 examples: {'rewards_train/chosen': '-0.77957', 'rewards_train/rejected': '-1.2175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43793', 'logps_train/rejected': '-151.59', 'logps_train/chosen': '-161.3', 'loss/train': '0.71623', 'examples_per_second': '48.621', 'grad_norm': '25.961', 'counters/examples': 29440, 'counters/updates': 920}
skipping logging after 29472 examples to avoid logging too frequently
train stats after 29504 examples: {'rewards_train/chosen': '-1.3192', 'rewards_train/rejected': '-1.6683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.34902', 'logps_train/rejected': '-105.39', 'logps_train/chosen': '-114.47', 'loss/train': '0.66942', 'examples_per_second': '45.065', 'grad_norm': '20.793', 'counters/examples': 29504, 'counters/updates': 922}
skipping logging after 29536 examples to avoid logging too frequently
train stats after 29568 examples: {'rewards_train/chosen': '-0.95657', 'rewards_train/rejected': '-1.2267', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.2701', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-135.24', 'loss/train': '0.72963', 'examples_per_second': '45.362', 'grad_norm': '25.705', 'counters/examples': 29568, 'counters/updates': 924}
skipping logging after 29600 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '-0.89434', 'rewards_train/rejected': '-1.5461', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.65176', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-127.08', 'loss/train': '0.54128', 'examples_per_second': '45.628', 'grad_norm': '22.614', 'counters/examples': 29632, 'counters/updates': 926}
skipping logging after 29664 examples to avoid logging too frequently
train stats after 29696 examples: {'rewards_train/chosen': '-0.56957', 'rewards_train/rejected': '-0.82673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25716', 'logps_train/rejected': '-150.11', 'logps_train/chosen': '-114.72', 'loss/train': '0.66673', 'examples_per_second': '47.904', 'grad_norm': '27.332', 'counters/examples': 29696, 'counters/updates': 928}
skipping logging after 29728 examples to avoid logging too frequently
train stats after 29760 examples: {'rewards_train/chosen': '-0.66258', 'rewards_train/rejected': '-0.93352', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.27094', 'logps_train/rejected': '-147.82', 'logps_train/chosen': '-131.51', 'loss/train': '0.7051', 'examples_per_second': '44.712', 'grad_norm': '25.377', 'counters/examples': 29760, 'counters/updates': 930}
skipping logging after 29792 examples to avoid logging too frequently
train stats after 29824 examples: {'rewards_train/chosen': '-0.66407', 'rewards_train/rejected': '-1.0634', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3993', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-142.25', 'loss/train': '0.62186', 'examples_per_second': '47.809', 'grad_norm': '26.614', 'counters/examples': 29824, 'counters/updates': 932}
skipping logging after 29856 examples to avoid logging too frequently
train stats after 29888 examples: {'rewards_train/chosen': '-0.77474', 'rewards_train/rejected': '-1.2809', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50616', 'logps_train/rejected': '-103.23', 'logps_train/chosen': '-125.32', 'loss/train': '0.58462', 'examples_per_second': '48.106', 'grad_norm': '22.425', 'counters/examples': 29888, 'counters/updates': 934}
skipping logging after 29920 examples to avoid logging too frequently
train stats after 29952 examples: {'rewards_train/chosen': '-0.95044', 'rewards_train/rejected': '-1.253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30258', 'logps_train/rejected': '-141.25', 'logps_train/chosen': '-113.79', 'loss/train': '0.66668', 'examples_per_second': '47.266', 'grad_norm': '25.305', 'counters/examples': 29952, 'counters/updates': 936}
skipping logging after 29984 examples to avoid logging too frequently
train stats after 30016 examples: {'rewards_train/chosen': '-0.65923', 'rewards_train/rejected': '-1.1345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.47531', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-139.14', 'loss/train': '0.58893', 'examples_per_second': '47.491', 'grad_norm': '22.361', 'counters/examples': 30016, 'counters/updates': 938}
skipping logging after 30048 examples to avoid logging too frequently
train stats after 30080 examples: {'rewards_train/chosen': '-0.96145', 'rewards_train/rejected': '-1.3655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.40408', 'logps_train/rejected': '-138.4', 'logps_train/chosen': '-153.9', 'loss/train': '0.59908', 'examples_per_second': '44.339', 'grad_norm': '23.443', 'counters/examples': 30080, 'counters/updates': 940}
skipping logging after 30112 examples to avoid logging too frequently
train stats after 30144 examples: {'rewards_train/chosen': '-0.65773', 'rewards_train/rejected': '-1.235', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.57724', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-133.68', 'loss/train': '0.51172', 'examples_per_second': '45.677', 'grad_norm': '20.273', 'counters/examples': 30144, 'counters/updates': 942}
skipping logging after 30176 examples to avoid logging too frequently
train stats after 30208 examples: {'rewards_train/chosen': '-0.77032', 'rewards_train/rejected': '-1.1773', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40703', 'logps_train/rejected': '-114', 'logps_train/chosen': '-136.55', 'loss/train': '0.69285', 'examples_per_second': '53.213', 'grad_norm': '26.221', 'counters/examples': 30208, 'counters/updates': 944}
skipping logging after 30240 examples to avoid logging too frequently
train stats after 30272 examples: {'rewards_train/chosen': '-0.65312', 'rewards_train/rejected': '-1.2024', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.54927', 'logps_train/rejected': '-127.92', 'logps_train/chosen': '-140.85', 'loss/train': '0.62789', 'examples_per_second': '45.529', 'grad_norm': '26.055', 'counters/examples': 30272, 'counters/updates': 946}
skipping logging after 30304 examples to avoid logging too frequently
train stats after 30336 examples: {'rewards_train/chosen': '-0.32983', 'rewards_train/rejected': '-1.0908', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.76095', 'logps_train/rejected': '-153.51', 'logps_train/chosen': '-150.05', 'loss/train': '0.53008', 'examples_per_second': '45.507', 'grad_norm': '23.77', 'counters/examples': 30336, 'counters/updates': 948}
skipping logging after 30368 examples to avoid logging too frequently
train stats after 30400 examples: {'rewards_train/chosen': '-0.70075', 'rewards_train/rejected': '-1.1623', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46157', 'logps_train/rejected': '-161.11', 'logps_train/chosen': '-145.66', 'loss/train': '0.60121', 'examples_per_second': '45.478', 'grad_norm': '26.264', 'counters/examples': 30400, 'counters/updates': 950}
skipping logging after 30432 examples to avoid logging too frequently
train stats after 30464 examples: {'rewards_train/chosen': '-0.64193', 'rewards_train/rejected': '-1.13', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48811', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-144.1', 'loss/train': '0.55306', 'examples_per_second': '43.669', 'grad_norm': '25.415', 'counters/examples': 30464, 'counters/updates': 952}
skipping logging after 30496 examples to avoid logging too frequently
train stats after 30528 examples: {'rewards_train/chosen': '-0.31674', 'rewards_train/rejected': '-0.99175', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.675', 'logps_train/rejected': '-108.31', 'logps_train/chosen': '-124.29', 'loss/train': '0.49984', 'examples_per_second': '48.953', 'grad_norm': '20.756', 'counters/examples': 30528, 'counters/updates': 954}
skipping logging after 30560 examples to avoid logging too frequently
train stats after 30592 examples: {'rewards_train/chosen': '-0.48996', 'rewards_train/rejected': '-0.89958', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40962', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-150.27', 'loss/train': '0.58615', 'examples_per_second': '44.869', 'grad_norm': '22.033', 'counters/examples': 30592, 'counters/updates': 956}
skipping logging after 30624 examples to avoid logging too frequently
train stats after 30656 examples: {'rewards_train/chosen': '-0.44237', 'rewards_train/rejected': '-1.2816', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.83927', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-136.07', 'loss/train': '0.44392', 'examples_per_second': '44.278', 'grad_norm': '19.497', 'counters/examples': 30656, 'counters/updates': 958}
skipping logging after 30688 examples to avoid logging too frequently
train stats after 30720 examples: {'rewards_train/chosen': '-0.6932', 'rewards_train/rejected': '-1.197', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.50379', 'logps_train/rejected': '-128.45', 'logps_train/chosen': '-127.06', 'loss/train': '0.57548', 'examples_per_second': '49.594', 'grad_norm': '23.142', 'counters/examples': 30720, 'counters/updates': 960}
skipping logging after 30752 examples to avoid logging too frequently
train stats after 30784 examples: {'rewards_train/chosen': '-0.759', 'rewards_train/rejected': '-1.4389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.67987', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-119.95', 'loss/train': '0.54999', 'examples_per_second': '45.325', 'grad_norm': '22.549', 'counters/examples': 30784, 'counters/updates': 962}
skipping logging after 30816 examples to avoid logging too frequently
train stats after 30848 examples: {'rewards_train/chosen': '-0.71003', 'rewards_train/rejected': '-0.91157', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.20154', 'logps_train/rejected': '-146.11', 'logps_train/chosen': '-166.02', 'loss/train': '0.71994', 'examples_per_second': '45.339', 'grad_norm': '29.295', 'counters/examples': 30848, 'counters/updates': 964}
skipping logging after 30880 examples to avoid logging too frequently
train stats after 30912 examples: {'rewards_train/chosen': '-0.90151', 'rewards_train/rejected': '-1.2191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31756', 'logps_train/rejected': '-149.17', 'logps_train/chosen': '-167.55', 'loss/train': '0.63069', 'examples_per_second': '44.219', 'grad_norm': '26.824', 'counters/examples': 30912, 'counters/updates': 966}
skipping logging after 30944 examples to avoid logging too frequently
train stats after 30976 examples: {'rewards_train/chosen': '-0.92533', 'rewards_train/rejected': '-1.5349', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60962', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-138.28', 'loss/train': '0.54979', 'examples_per_second': '48.635', 'grad_norm': '24.783', 'counters/examples': 30976, 'counters/updates': 968}
skipping logging after 31008 examples to avoid logging too frequently
train stats after 31040 examples: {'rewards_train/chosen': '-0.90025', 'rewards_train/rejected': '-1.3573', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45704', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-106.22', 'loss/train': '0.60812', 'examples_per_second': '45.21', 'grad_norm': '23.682', 'counters/examples': 31040, 'counters/updates': 970}
skipping logging after 31072 examples to avoid logging too frequently
train stats after 31104 examples: {'rewards_train/chosen': '-0.77773', 'rewards_train/rejected': '-1.4425', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.66479', 'logps_train/rejected': '-170.39', 'logps_train/chosen': '-125.27', 'loss/train': '0.52021', 'examples_per_second': '46.949', 'grad_norm': '21.36', 'counters/examples': 31104, 'counters/updates': 972}
skipping logging after 31136 examples to avoid logging too frequently
train stats after 31168 examples: {'rewards_train/chosen': '-0.66972', 'rewards_train/rejected': '-1.1172', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44748', 'logps_train/rejected': '-177.06', 'logps_train/chosen': '-142.96', 'loss/train': '0.59752', 'examples_per_second': '45.382', 'grad_norm': '28.57', 'counters/examples': 31168, 'counters/updates': 974}
skipping logging after 31200 examples to avoid logging too frequently
train stats after 31232 examples: {'rewards_train/chosen': '-0.35276', 'rewards_train/rejected': '-1.2328', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.88002', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-150.66', 'loss/train': '0.55103', 'examples_per_second': '51.862', 'grad_norm': '25.385', 'counters/examples': 31232, 'counters/updates': 976}
skipping logging after 31264 examples to avoid logging too frequently
train stats after 31296 examples: {'rewards_train/chosen': '-0.70655', 'rewards_train/rejected': '-1.4434', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.7369', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-130.2', 'loss/train': '0.47501', 'examples_per_second': '44.608', 'grad_norm': '24.036', 'counters/examples': 31296, 'counters/updates': 978}
skipping logging after 31328 examples to avoid logging too frequently
train stats after 31360 examples: {'rewards_train/chosen': '-0.56121', 'rewards_train/rejected': '-1.0124', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.45119', 'logps_train/rejected': '-173.95', 'logps_train/chosen': '-162.94', 'loss/train': '0.63886', 'examples_per_second': '45.516', 'grad_norm': '27.412', 'counters/examples': 31360, 'counters/updates': 980}
skipping logging after 31392 examples to avoid logging too frequently
train stats after 31424 examples: {'rewards_train/chosen': '-0.65131', 'rewards_train/rejected': '-1.6359', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.98454', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-139.63', 'loss/train': '0.47232', 'examples_per_second': '44.032', 'grad_norm': '22.804', 'counters/examples': 31424, 'counters/updates': 982}
skipping logging after 31456 examples to avoid logging too frequently
train stats after 31488 examples: {'rewards_train/chosen': '-0.64213', 'rewards_train/rejected': '-1.2857', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.64353', 'logps_train/rejected': '-144.28', 'logps_train/chosen': '-155.73', 'loss/train': '0.52949', 'examples_per_second': '45.47', 'grad_norm': '26.56', 'counters/examples': 31488, 'counters/updates': 984}
skipping logging after 31520 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '-0.58981', 'rewards_train/rejected': '-1.3684', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.77862', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-149.15', 'loss/train': '0.51251', 'examples_per_second': '45.395', 'grad_norm': '23.724', 'counters/examples': 31552, 'counters/updates': 986}
skipping logging after 31584 examples to avoid logging too frequently
train stats after 31616 examples: {'rewards_train/chosen': '-0.87263', 'rewards_train/rejected': '-1.0922', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.21956', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-163.56', 'loss/train': '0.69642', 'examples_per_second': '44.835', 'grad_norm': '28.17', 'counters/examples': 31616, 'counters/updates': 988}
skipping logging after 31648 examples to avoid logging too frequently
train stats after 31680 examples: {'rewards_train/chosen': '-0.80018', 'rewards_train/rejected': '-1.0805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.28028', 'logps_train/rejected': '-127.43', 'logps_train/chosen': '-163.14', 'loss/train': '0.68252', 'examples_per_second': '45.489', 'grad_norm': '26.902', 'counters/examples': 31680, 'counters/updates': 990}
skipping logging after 31712 examples to avoid logging too frequently
train stats after 31744 examples: {'rewards_train/chosen': '-0.54432', 'rewards_train/rejected': '-1.3741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.82982', 'logps_train/rejected': '-163.71', 'logps_train/chosen': '-165.55', 'loss/train': '0.49175', 'examples_per_second': '44.952', 'grad_norm': '24.854', 'counters/examples': 31744, 'counters/updates': 992}
skipping logging after 31776 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '-0.69306', 'rewards_train/rejected': '-0.98665', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2936', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-176', 'loss/train': '0.6408', 'examples_per_second': '45.33', 'grad_norm': '28.762', 'counters/examples': 31808, 'counters/updates': 994}
skipping logging after 31840 examples to avoid logging too frequently
train stats after 31872 examples: {'rewards_train/chosen': '-0.75297', 'rewards_train/rejected': '-1.1959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44298', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-141.27', 'loss/train': '0.6078', 'examples_per_second': '46', 'grad_norm': '22.491', 'counters/examples': 31872, 'counters/updates': 996}
skipping logging after 31904 examples to avoid logging too frequently
train stats after 31936 examples: {'rewards_train/chosen': '-0.74223', 'rewards_train/rejected': '-1.2502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50795', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-147.16', 'loss/train': '0.62488', 'examples_per_second': '47.468', 'grad_norm': '27.965', 'counters/examples': 31936, 'counters/updates': 998}
skipping logging after 31968 examples to avoid logging too frequently
train stats after 32000 examples: {'rewards_train/chosen': '-0.70256', 'rewards_train/rejected': '-1.0285', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3259', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-149.64', 'loss/train': '0.6449', 'examples_per_second': '45.096', 'grad_norm': '26.483', 'counters/examples': 32000, 'counters/updates': 1000}
skipping logging after 32032 examples to avoid logging too frequently
train stats after 32064 examples: {'rewards_train/chosen': '-0.66932', 'rewards_train/rejected': '-1.1874', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51807', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-136.62', 'loss/train': '0.56584', 'examples_per_second': '45.368', 'grad_norm': '24', 'counters/examples': 32064, 'counters/updates': 1002}
skipping logging after 32096 examples to avoid logging too frequently
train stats after 32128 examples: {'rewards_train/chosen': '-0.46023', 'rewards_train/rejected': '-1.0395', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57924', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-121.16', 'loss/train': '0.58808', 'examples_per_second': '45.708', 'grad_norm': '22.667', 'counters/examples': 32128, 'counters/updates': 1004}
skipping logging after 32160 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '-0.28758', 'rewards_train/rejected': '-0.8703', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58272', 'logps_train/rejected': '-156.61', 'logps_train/chosen': '-150.7', 'loss/train': '0.52749', 'examples_per_second': '44.316', 'grad_norm': '23.648', 'counters/examples': 32192, 'counters/updates': 1006}
skipping logging after 32224 examples to avoid logging too frequently
train stats after 32256 examples: {'rewards_train/chosen': '-0.51266', 'rewards_train/rejected': '-0.79413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28147', 'logps_train/rejected': '-106.75', 'logps_train/chosen': '-126.39', 'loss/train': '0.70897', 'examples_per_second': '44.507', 'grad_norm': '27.925', 'counters/examples': 32256, 'counters/updates': 1008}
skipping logging after 32288 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.26621', 'rewards_train/rejected': '-1.0286', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.76238', 'logps_train/rejected': '-111.66', 'logps_train/chosen': '-146.36', 'loss/train': '0.48848', 'examples_per_second': '47.01', 'grad_norm': '20.981', 'counters/examples': 32320, 'counters/updates': 1010}
skipping logging after 32352 examples to avoid logging too frequently
train stats after 32384 examples: {'rewards_train/chosen': '-0.29109', 'rewards_train/rejected': '-0.51518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22409', 'logps_train/rejected': '-99.364', 'logps_train/chosen': '-150.61', 'loss/train': '0.68241', 'examples_per_second': '46.488', 'grad_norm': '24.379', 'counters/examples': 32384, 'counters/updates': 1012}
skipping logging after 32416 examples to avoid logging too frequently
train stats after 32448 examples: {'rewards_train/chosen': '-0.75052', 'rewards_train/rejected': '-1.3023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.55175', 'logps_train/rejected': '-156.38', 'logps_train/chosen': '-168.02', 'loss/train': '0.57597', 'examples_per_second': '45.489', 'grad_norm': '24.504', 'counters/examples': 32448, 'counters/updates': 1014}
skipping logging after 32480 examples to avoid logging too frequently
train stats after 32512 examples: {'rewards_train/chosen': '-0.46007', 'rewards_train/rejected': '-1.3194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.85929', 'logps_train/rejected': '-141.7', 'logps_train/chosen': '-159.59', 'loss/train': '0.50699', 'examples_per_second': '47.397', 'grad_norm': '24.401', 'counters/examples': 32512, 'counters/updates': 1016}
skipping logging after 32544 examples to avoid logging too frequently
train stats after 32576 examples: {'rewards_train/chosen': '-0.46833', 'rewards_train/rejected': '-1.4219', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.95359', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-135.7', 'loss/train': '0.45446', 'examples_per_second': '45.573', 'grad_norm': '21.023', 'counters/examples': 32576, 'counters/updates': 1018}
skipping logging after 32608 examples to avoid logging too frequently
train stats after 32640 examples: {'rewards_train/chosen': '-0.93519', 'rewards_train/rejected': '-1.3086', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.37343', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-116.23', 'loss/train': '0.63886', 'examples_per_second': '47.324', 'grad_norm': '23.718', 'counters/examples': 32640, 'counters/updates': 1020}
skipping logging after 32672 examples to avoid logging too frequently
train stats after 32704 examples: {'rewards_train/chosen': '-0.68925', 'rewards_train/rejected': '-0.93236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24311', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-168.58', 'loss/train': '0.69863', 'examples_per_second': '46.827', 'grad_norm': '30.299', 'counters/examples': 32704, 'counters/updates': 1022}
skipping logging after 32736 examples to avoid logging too frequently
train stats after 32768 examples: {'rewards_train/chosen': '-0.75206', 'rewards_train/rejected': '-1.3823', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63023', 'logps_train/rejected': '-178.55', 'logps_train/chosen': '-159.77', 'loss/train': '0.5786', 'examples_per_second': '45.422', 'grad_norm': '27.666', 'counters/examples': 32768, 'counters/updates': 1024}
skipping logging after 32800 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '-0.54612', 'rewards_train/rejected': '-1.3121', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.76598', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-123.28', 'loss/train': '0.51202', 'examples_per_second': '45.381', 'grad_norm': '20.042', 'counters/examples': 32832, 'counters/updates': 1026}
skipping logging after 32864 examples to avoid logging too frequently
train stats after 32896 examples: {'rewards_train/chosen': '-0.8433', 'rewards_train/rejected': '-1.1675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.32417', 'logps_train/rejected': '-142.57', 'logps_train/chosen': '-133.87', 'loss/train': '0.68169', 'examples_per_second': '45.681', 'grad_norm': '27.875', 'counters/examples': 32896, 'counters/updates': 1028}
skipping logging after 32928 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.41146', 'rewards_train/rejected': '-1.12', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.70858', 'logps_train/rejected': '-163.5', 'logps_train/chosen': '-143.02', 'loss/train': '0.4734', 'examples_per_second': '44.392', 'grad_norm': '22.64', 'counters/examples': 32960, 'counters/updates': 1030}
skipping logging after 32992 examples to avoid logging too frequently
train stats after 33024 examples: {'rewards_train/chosen': '-0.81591', 'rewards_train/rejected': '-1.2071', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.39115', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-112.94', 'loss/train': '0.62985', 'examples_per_second': '45.575', 'grad_norm': '23.147', 'counters/examples': 33024, 'counters/updates': 1032}
skipping logging after 33056 examples to avoid logging too frequently
train stats after 33088 examples: {'rewards_train/chosen': '-1.0888', 'rewards_train/rejected': '-1.6038', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.51498', 'logps_train/rejected': '-200.43', 'logps_train/chosen': '-174.26', 'loss/train': '0.60732', 'examples_per_second': '45.36', 'grad_norm': '26.756', 'counters/examples': 33088, 'counters/updates': 1034}
skipping logging after 33120 examples to avoid logging too frequently
train stats after 33152 examples: {'rewards_train/chosen': '-0.70015', 'rewards_train/rejected': '-1.1523', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.45211', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-139.78', 'loss/train': '0.6143', 'examples_per_second': '44.478', 'grad_norm': '27.41', 'counters/examples': 33152, 'counters/updates': 1036}
skipping logging after 33184 examples to avoid logging too frequently
train stats after 33216 examples: {'rewards_train/chosen': '-0.79531', 'rewards_train/rejected': '-1.0365', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24118', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-139.65', 'loss/train': '0.70776', 'examples_per_second': '44.507', 'grad_norm': '28.003', 'counters/examples': 33216, 'counters/updates': 1038}
skipping logging after 33248 examples to avoid logging too frequently
train stats after 33280 examples: {'rewards_train/chosen': '-0.35114', 'rewards_train/rejected': '-0.91671', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56558', 'logps_train/rejected': '-160.59', 'logps_train/chosen': '-131.44', 'loss/train': '0.55956', 'examples_per_second': '46.181', 'grad_norm': '25.287', 'counters/examples': 33280, 'counters/updates': 1040}
skipping logging after 33312 examples to avoid logging too frequently
train stats after 33344 examples: {'rewards_train/chosen': '-0.55339', 'rewards_train/rejected': '-1.1246', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57125', 'logps_train/rejected': '-168.29', 'logps_train/chosen': '-149.36', 'loss/train': '0.56853', 'examples_per_second': '46.432', 'grad_norm': '26.444', 'counters/examples': 33344, 'counters/updates': 1042}
skipping logging after 33376 examples to avoid logging too frequently
train stats after 33408 examples: {'rewards_train/chosen': '-0.54186', 'rewards_train/rejected': '-1.0359', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49404', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-125.54', 'loss/train': '0.60066', 'examples_per_second': '48.286', 'grad_norm': '24.122', 'counters/examples': 33408, 'counters/updates': 1044}
skipping logging after 33440 examples to avoid logging too frequently
train stats after 33472 examples: {'rewards_train/chosen': '-0.67952', 'rewards_train/rejected': '-1.2225', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54301', 'logps_train/rejected': '-147.52', 'logps_train/chosen': '-148.2', 'loss/train': '0.52334', 'examples_per_second': '45.409', 'grad_norm': '23.58', 'counters/examples': 33472, 'counters/updates': 1046}
skipping logging after 33504 examples to avoid logging too frequently
train stats after 33536 examples: {'rewards_train/chosen': '-0.986', 'rewards_train/rejected': '-1.5653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57929', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-155.72', 'loss/train': '0.65954', 'examples_per_second': '46.518', 'grad_norm': '26.584', 'counters/examples': 33536, 'counters/updates': 1048}
skipping logging after 33568 examples to avoid logging too frequently
train stats after 33600 examples: {'rewards_train/chosen': '-0.7863', 'rewards_train/rejected': '-1.1305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3442', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-181.41', 'loss/train': '0.63972', 'examples_per_second': '45.109', 'grad_norm': '32.929', 'counters/examples': 33600, 'counters/updates': 1050}
skipping logging after 33632 examples to avoid logging too frequently
train stats after 33664 examples: {'rewards_train/chosen': '-0.51352', 'rewards_train/rejected': '-0.9816', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46808', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-126.72', 'loss/train': '0.60978', 'examples_per_second': '44.171', 'grad_norm': '24.456', 'counters/examples': 33664, 'counters/updates': 1052}
skipping logging after 33696 examples to avoid logging too frequently
train stats after 33728 examples: {'rewards_train/chosen': '-0.57739', 'rewards_train/rejected': '-1.0685', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.4911', 'logps_train/rejected': '-152.25', 'logps_train/chosen': '-135.49', 'loss/train': '0.51423', 'examples_per_second': '44.312', 'grad_norm': '20.165', 'counters/examples': 33728, 'counters/updates': 1054}
skipping logging after 33760 examples to avoid logging too frequently
train stats after 33792 examples: {'rewards_train/chosen': '-0.80967', 'rewards_train/rejected': '-1.326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.51629', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-151.71', 'loss/train': '0.59466', 'examples_per_second': '45.747', 'grad_norm': '23.241', 'counters/examples': 33792, 'counters/updates': 1056}
skipping logging after 33824 examples to avoid logging too frequently
train stats after 33856 examples: {'rewards_train/chosen': '-0.56268', 'rewards_train/rejected': '-0.99097', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42829', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-125.01', 'loss/train': '0.58448', 'examples_per_second': '45.498', 'grad_norm': '24.243', 'counters/examples': 33856, 'counters/updates': 1058}
skipping logging after 33888 examples to avoid logging too frequently
train stats after 33920 examples: {'rewards_train/chosen': '-0.68937', 'rewards_train/rejected': '-1.2342', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5448', 'logps_train/rejected': '-123.31', 'logps_train/chosen': '-148.56', 'loss/train': '0.61071', 'examples_per_second': '45.698', 'grad_norm': '27.676', 'counters/examples': 33920, 'counters/updates': 1060}
skipping logging after 33952 examples to avoid logging too frequently
train stats after 33984 examples: {'rewards_train/chosen': '-0.43168', 'rewards_train/rejected': '-0.8422', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41052', 'logps_train/rejected': '-104.44', 'logps_train/chosen': '-159.85', 'loss/train': '0.62449', 'examples_per_second': '45.212', 'grad_norm': '29.255', 'counters/examples': 33984, 'counters/updates': 1062}
skipping logging after 34016 examples to avoid logging too frequently
train stats after 34048 examples: {'rewards_train/chosen': '-0.54629', 'rewards_train/rejected': '-1.1899', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.64363', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-138.84', 'loss/train': '0.51513', 'examples_per_second': '44.353', 'grad_norm': '21.135', 'counters/examples': 34048, 'counters/updates': 1064}
skipping logging after 34080 examples to avoid logging too frequently
train stats after 34112 examples: {'rewards_train/chosen': '-0.61653', 'rewards_train/rejected': '-1.3488', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.73231', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-128.03', 'loss/train': '0.48628', 'examples_per_second': '47.57', 'grad_norm': '19.876', 'counters/examples': 34112, 'counters/updates': 1066}
skipping logging after 34144 examples to avoid logging too frequently
train stats after 34176 examples: {'rewards_train/chosen': '-0.67466', 'rewards_train/rejected': '-1.4288', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.75413', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-163.08', 'loss/train': '0.51912', 'examples_per_second': '45.267', 'grad_norm': '23.85', 'counters/examples': 34176, 'counters/updates': 1068}
skipping logging after 34208 examples to avoid logging too frequently
train stats after 34240 examples: {'rewards_train/chosen': '-0.68412', 'rewards_train/rejected': '-1.5048', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.82066', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-150.89', 'loss/train': '0.52899', 'examples_per_second': '45.346', 'grad_norm': '22.689', 'counters/examples': 34240, 'counters/updates': 1070}
skipping logging after 34272 examples to avoid logging too frequently
train stats after 34304 examples: {'rewards_train/chosen': '-0.60969', 'rewards_train/rejected': '-1.1613', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55158', 'logps_train/rejected': '-147.85', 'logps_train/chosen': '-148.06', 'loss/train': '0.58505', 'examples_per_second': '45.823', 'grad_norm': '23.641', 'counters/examples': 34304, 'counters/updates': 1072}
skipping logging after 34336 examples to avoid logging too frequently
train stats after 34368 examples: {'rewards_train/chosen': '-0.90996', 'rewards_train/rejected': '-1.4496', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.53962', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-168.76', 'loss/train': '0.56667', 'examples_per_second': '52.427', 'grad_norm': '23.958', 'counters/examples': 34368, 'counters/updates': 1074}
skipping logging after 34400 examples to avoid logging too frequently
train stats after 34432 examples: {'rewards_train/chosen': '-0.96711', 'rewards_train/rejected': '-1.4262', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45914', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-146.64', 'loss/train': '0.68431', 'examples_per_second': '44.815', 'grad_norm': '27.143', 'counters/examples': 34432, 'counters/updates': 1076}
skipping logging after 34464 examples to avoid logging too frequently
train stats after 34496 examples: {'rewards_train/chosen': '-0.88006', 'rewards_train/rejected': '-1.2826', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40255', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-141.52', 'loss/train': '0.61703', 'examples_per_second': '45.254', 'grad_norm': '24.699', 'counters/examples': 34496, 'counters/updates': 1078}
skipping logging after 34528 examples to avoid logging too frequently
train stats after 34560 examples: {'rewards_train/chosen': '-0.6742', 'rewards_train/rejected': '-1.0997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42553', 'logps_train/rejected': '-131.07', 'logps_train/chosen': '-145.94', 'loss/train': '0.65464', 'examples_per_second': '44.355', 'grad_norm': '27.526', 'counters/examples': 34560, 'counters/updates': 1080}
skipping logging after 34592 examples to avoid logging too frequently
train stats after 34624 examples: {'rewards_train/chosen': '-0.71515', 'rewards_train/rejected': '-1.1987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48358', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-129.94', 'loss/train': '0.60054', 'examples_per_second': '48.649', 'grad_norm': '26.067', 'counters/examples': 34624, 'counters/updates': 1082}
skipping logging after 34656 examples to avoid logging too frequently
train stats after 34688 examples: {'rewards_train/chosen': '-0.93681', 'rewards_train/rejected': '-1.369', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43222', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-121.35', 'loss/train': '0.59353', 'examples_per_second': '44.544', 'grad_norm': '21.844', 'counters/examples': 34688, 'counters/updates': 1084}
skipping logging after 34720 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '-0.73301', 'rewards_train/rejected': '-1.0612', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32822', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-123.42', 'loss/train': '0.61744', 'examples_per_second': '45.967', 'grad_norm': '25.212', 'counters/examples': 34752, 'counters/updates': 1086}
skipping logging after 34784 examples to avoid logging too frequently
train stats after 34816 examples: {'rewards_train/chosen': '-0.85862', 'rewards_train/rejected': '-1.163', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.30433', 'logps_train/rejected': '-165.53', 'logps_train/chosen': '-113.02', 'loss/train': '0.70824', 'examples_per_second': '44.947', 'grad_norm': '25.488', 'counters/examples': 34816, 'counters/updates': 1088}
skipping logging after 34848 examples to avoid logging too frequently
train stats after 34880 examples: {'rewards_train/chosen': '-0.60992', 'rewards_train/rejected': '-1.0396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.4297', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-125.38', 'loss/train': '0.59576', 'examples_per_second': '46.504', 'grad_norm': '21.942', 'counters/examples': 34880, 'counters/updates': 1090}
skipping logging after 34912 examples to avoid logging too frequently
train stats after 34944 examples: {'rewards_train/chosen': '-0.70285', 'rewards_train/rejected': '-1.2976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.59479', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-149.51', 'loss/train': '0.57533', 'examples_per_second': '44.353', 'grad_norm': '26.112', 'counters/examples': 34944, 'counters/updates': 1092}
skipping logging after 34976 examples to avoid logging too frequently
train stats after 35008 examples: {'rewards_train/chosen': '-0.53485', 'rewards_train/rejected': '-0.83114', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29629', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-130.43', 'loss/train': '0.66709', 'examples_per_second': '45.797', 'grad_norm': '25.535', 'counters/examples': 35008, 'counters/updates': 1094}
skipping logging after 35040 examples to avoid logging too frequently
train stats after 35072 examples: {'rewards_train/chosen': '-0.63029', 'rewards_train/rejected': '-1.0523', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.422', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-152.88', 'loss/train': '0.63281', 'examples_per_second': '48.596', 'grad_norm': '25.621', 'counters/examples': 35072, 'counters/updates': 1096}
skipping logging after 35104 examples to avoid logging too frequently
train stats after 35136 examples: {'rewards_train/chosen': '-0.57244', 'rewards_train/rejected': '-0.89666', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32422', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-120.63', 'loss/train': '0.72413', 'examples_per_second': '46.575', 'grad_norm': '27.41', 'counters/examples': 35136, 'counters/updates': 1098}
skipping logging after 35168 examples to avoid logging too frequently
train stats after 35200 examples: {'rewards_train/chosen': '-0.27316', 'rewards_train/rejected': '-0.51843', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24526', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-156.56', 'loss/train': '0.65279', 'examples_per_second': '45.286', 'grad_norm': '27.251', 'counters/examples': 35200, 'counters/updates': 1100}
skipping logging after 35232 examples to avoid logging too frequently
train stats after 35264 examples: {'rewards_train/chosen': '-0.60409', 'rewards_train/rejected': '-1.0507', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44658', 'logps_train/rejected': '-113.62', 'logps_train/chosen': '-110.88', 'loss/train': '0.55357', 'examples_per_second': '45.261', 'grad_norm': '22.432', 'counters/examples': 35264, 'counters/updates': 1102}
skipping logging after 35296 examples to avoid logging too frequently
train stats after 35328 examples: {'rewards_train/chosen': '-0.2184', 'rewards_train/rejected': '-0.48524', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26684', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-134.51', 'loss/train': '0.66454', 'examples_per_second': '44.265', 'grad_norm': '22.91', 'counters/examples': 35328, 'counters/updates': 1104}
skipping logging after 35360 examples to avoid logging too frequently
train stats after 35392 examples: {'rewards_train/chosen': '-0.25838', 'rewards_train/rejected': '-1.0056', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.74725', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-150.65', 'loss/train': '0.51774', 'examples_per_second': '44.168', 'grad_norm': '24.027', 'counters/examples': 35392, 'counters/updates': 1106}
skipping logging after 35424 examples to avoid logging too frequently
train stats after 35456 examples: {'rewards_train/chosen': '-0.31409', 'rewards_train/rejected': '-0.76014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.44605', 'logps_train/rejected': '-150.58', 'logps_train/chosen': '-132.47', 'loss/train': '0.59804', 'examples_per_second': '45.068', 'grad_norm': '24.408', 'counters/examples': 35456, 'counters/updates': 1108}
skipping logging after 35488 examples to avoid logging too frequently
train stats after 35520 examples: {'rewards_train/chosen': '-0.096344', 'rewards_train/rejected': '-0.95244', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.8561', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-166.97', 'loss/train': '0.46227', 'examples_per_second': '45.498', 'grad_norm': '23.554', 'counters/examples': 35520, 'counters/updates': 1110}
skipping logging after 35552 examples to avoid logging too frequently
train stats after 35584 examples: {'rewards_train/chosen': '-0.2494', 'rewards_train/rejected': '-0.47184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.22244', 'logps_train/rejected': '-117.9', 'logps_train/chosen': '-123.9', 'loss/train': '0.66909', 'examples_per_second': '45.392', 'grad_norm': '24.898', 'counters/examples': 35584, 'counters/updates': 1112}
skipping logging after 35616 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '-0.23584', 'rewards_train/rejected': '-0.81314', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5773', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-150.29', 'loss/train': '0.54776', 'examples_per_second': '45.349', 'grad_norm': '27.103', 'counters/examples': 35648, 'counters/updates': 1114}
skipping logging after 35680 examples to avoid logging too frequently
train stats after 35712 examples: {'rewards_train/chosen': '-0.21514', 'rewards_train/rejected': '-0.46731', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25218', 'logps_train/rejected': '-152.05', 'logps_train/chosen': '-121.86', 'loss/train': '0.62982', 'examples_per_second': '44.708', 'grad_norm': '28.222', 'counters/examples': 35712, 'counters/updates': 1116}
skipping logging after 35744 examples to avoid logging too frequently
train stats after 35776 examples: {'rewards_train/chosen': '-0.27038', 'rewards_train/rejected': '-0.76682', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.49644', 'logps_train/rejected': '-146.85', 'logps_train/chosen': '-140.9', 'loss/train': '0.54855', 'examples_per_second': '44.35', 'grad_norm': '25.283', 'counters/examples': 35776, 'counters/updates': 1118}
skipping logging after 35808 examples to avoid logging too frequently
train stats after 35840 examples: {'rewards_train/chosen': '-0.56103', 'rewards_train/rejected': '-1.1765', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.61551', 'logps_train/rejected': '-152.31', 'logps_train/chosen': '-126.21', 'loss/train': '0.62672', 'examples_per_second': '45.423', 'grad_norm': '24.19', 'counters/examples': 35840, 'counters/updates': 1120}
skipping logging after 35872 examples to avoid logging too frequently
train stats after 35904 examples: {'rewards_train/chosen': '-0.2846', 'rewards_train/rejected': '-0.49365', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20905', 'logps_train/rejected': '-109.92', 'logps_train/chosen': '-145.11', 'loss/train': '0.68863', 'examples_per_second': '46.182', 'grad_norm': '27.444', 'counters/examples': 35904, 'counters/updates': 1122}
skipping logging after 35936 examples to avoid logging too frequently
train stats after 35968 examples: {'rewards_train/chosen': '-0.28244', 'rewards_train/rejected': '-0.80172', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.51928', 'logps_train/rejected': '-111.1', 'logps_train/chosen': '-117.61', 'loss/train': '0.56085', 'examples_per_second': '44.915', 'grad_norm': '22.844', 'counters/examples': 35968, 'counters/updates': 1124}
skipping logging after 36000 examples to avoid logging too frequently
Running evaluation after 36000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.79it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.90it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.93it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.86it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.30it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.15it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.10it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.13it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.00it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  6.98it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  6.98it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.00it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:02<00:00,  6.89it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.95it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.85it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.98it/s]
eval after 36000: {'rewards_eval/chosen': '-0.25473', 'rewards_eval/rejected': '-0.70632', 'rewards_eval/accuracies': '0.64062', 'rewards_eval/margins': '0.4516', 'logps_eval/rejected': '-125.34', 'logps_eval/chosen': '-139', 'loss/eval': '0.61654'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-36000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-36000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-36000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-36000/scheduler.pt...
train stats after 36032 examples: {'rewards_train/chosen': '-0.25287', 'rewards_train/rejected': '-0.67176', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41888', 'logps_train/rejected': '-106.57', 'logps_train/chosen': '-160.72', 'loss/train': '0.58844', 'examples_per_second': '15.022', 'grad_norm': '25.026', 'counters/examples': 36032, 'counters/updates': 1126}
skipping logging after 36064 examples to avoid logging too frequently
train stats after 36096 examples: {'rewards_train/chosen': '-0.32439', 'rewards_train/rejected': '-0.63846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31407', 'logps_train/rejected': '-134.23', 'logps_train/chosen': '-120.76', 'loss/train': '0.65304', 'examples_per_second': '45.204', 'grad_norm': '24.846', 'counters/examples': 36096, 'counters/updates': 1128}
skipping logging after 36128 examples to avoid logging too frequently
train stats after 36160 examples: {'rewards_train/chosen': '-0.50148', 'rewards_train/rejected': '-1.2741', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.77264', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-135.48', 'loss/train': '0.51568', 'examples_per_second': '44.95', 'grad_norm': '24.432', 'counters/examples': 36160, 'counters/updates': 1130}
skipping logging after 36192 examples to avoid logging too frequently
train stats after 36224 examples: {'rewards_train/chosen': '-0.23863', 'rewards_train/rejected': '-0.54383', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3052', 'logps_train/rejected': '-148.05', 'logps_train/chosen': '-144.86', 'loss/train': '0.64346', 'examples_per_second': '45.401', 'grad_norm': '26.541', 'counters/examples': 36224, 'counters/updates': 1132}
skipping logging after 36256 examples to avoid logging too frequently
train stats after 36288 examples: {'rewards_train/chosen': '-0.21521', 'rewards_train/rejected': '-0.86589', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.65067', 'logps_train/rejected': '-153.47', 'logps_train/chosen': '-135.79', 'loss/train': '0.51925', 'examples_per_second': '47.553', 'grad_norm': '24.319', 'counters/examples': 36288, 'counters/updates': 1134}
skipping logging after 36320 examples to avoid logging too frequently
train stats after 36352 examples: {'rewards_train/chosen': '-0.56608', 'rewards_train/rejected': '-1.1241', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55798', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-134.79', 'loss/train': '0.60651', 'examples_per_second': '46.675', 'grad_norm': '23.566', 'counters/examples': 36352, 'counters/updates': 1136}
skipping logging after 36384 examples to avoid logging too frequently
train stats after 36416 examples: {'rewards_train/chosen': '-0.667', 'rewards_train/rejected': '-0.99775', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33075', 'logps_train/rejected': '-111.25', 'logps_train/chosen': '-140.05', 'loss/train': '0.66194', 'examples_per_second': '44.392', 'grad_norm': '25.595', 'counters/examples': 36416, 'counters/updates': 1138}
skipping logging after 36448 examples to avoid logging too frequently
train stats after 36480 examples: {'rewards_train/chosen': '-0.38429', 'rewards_train/rejected': '-0.94481', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56052', 'logps_train/rejected': '-103.41', 'logps_train/chosen': '-127.86', 'loss/train': '0.561', 'examples_per_second': '46.589', 'grad_norm': '22.873', 'counters/examples': 36480, 'counters/updates': 1140}
skipping logging after 36512 examples to avoid logging too frequently
train stats after 36544 examples: {'rewards_train/chosen': '-0.50757', 'rewards_train/rejected': '-1.2502', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.7426', 'logps_train/rejected': '-154.23', 'logps_train/chosen': '-166.73', 'loss/train': '0.54449', 'examples_per_second': '44.907', 'grad_norm': '24.992', 'counters/examples': 36544, 'counters/updates': 1142}
skipping logging after 36576 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '-0.63759', 'rewards_train/rejected': '-1.3281', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.6905', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-120.66', 'loss/train': '0.51189', 'examples_per_second': '44.922', 'grad_norm': '21.993', 'counters/examples': 36608, 'counters/updates': 1144}
skipping logging after 36640 examples to avoid logging too frequently
train stats after 36672 examples: {'rewards_train/chosen': '-0.50308', 'rewards_train/rejected': '-1.0886', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58553', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-161.64', 'loss/train': '0.52707', 'examples_per_second': '45.78', 'grad_norm': '22.836', 'counters/examples': 36672, 'counters/updates': 1146}
skipping logging after 36704 examples to avoid logging too frequently
train stats after 36736 examples: {'rewards_train/chosen': '-0.54337', 'rewards_train/rejected': '-1.031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4876', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-143.07', 'loss/train': '0.57427', 'examples_per_second': '45.464', 'grad_norm': '23.862', 'counters/examples': 36736, 'counters/updates': 1148}
skipping logging after 36768 examples to avoid logging too frequently
train stats after 36800 examples: {'rewards_train/chosen': '-0.46836', 'rewards_train/rejected': '-0.98203', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.51367', 'logps_train/rejected': '-100.63', 'logps_train/chosen': '-153.67', 'loss/train': '0.60047', 'examples_per_second': '45.31', 'grad_norm': '22.046', 'counters/examples': 36800, 'counters/updates': 1150}
skipping logging after 36832 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '-0.39671', 'rewards_train/rejected': '-1.2622', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.86549', 'logps_train/rejected': '-152.6', 'logps_train/chosen': '-158.41', 'loss/train': '0.44956', 'examples_per_second': '44.565', 'grad_norm': '22', 'counters/examples': 36864, 'counters/updates': 1152}
skipping logging after 36896 examples to avoid logging too frequently
train stats after 36928 examples: {'rewards_train/chosen': '-0.47679', 'rewards_train/rejected': '-1.2731', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.79632', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-138.87', 'loss/train': '0.46425', 'examples_per_second': '45.629', 'grad_norm': '22.547', 'counters/examples': 36928, 'counters/updates': 1154}
skipping logging after 36960 examples to avoid logging too frequently
train stats after 36992 examples: {'rewards_train/chosen': '-0.66773', 'rewards_train/rejected': '-1.2777', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60993', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-157.92', 'loss/train': '0.58479', 'examples_per_second': '32.785', 'grad_norm': '28.893', 'counters/examples': 36992, 'counters/updates': 1156}
skipping logging after 37024 examples to avoid logging too frequently
train stats after 37056 examples: {'rewards_train/chosen': '-0.55832', 'rewards_train/rejected': '-1.3995', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.84119', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-139.64', 'loss/train': '0.4573', 'examples_per_second': '45.359', 'grad_norm': '21.508', 'counters/examples': 37056, 'counters/updates': 1158}
skipping logging after 37088 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '-1.1639', 'rewards_train/rejected': '-1.8695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.70563', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-165.09', 'loss/train': '0.56853', 'examples_per_second': '52.418', 'grad_norm': '26.072', 'counters/examples': 37120, 'counters/updates': 1160}
skipping logging after 37152 examples to avoid logging too frequently
train stats after 37184 examples: {'rewards_train/chosen': '-0.74514', 'rewards_train/rejected': '-1.668', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.92282', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-172.74', 'loss/train': '0.46494', 'examples_per_second': '45.478', 'grad_norm': '20.923', 'counters/examples': 37184, 'counters/updates': 1162}
skipping logging after 37216 examples to avoid logging too frequently
train stats after 37248 examples: {'rewards_train/chosen': '-0.86408', 'rewards_train/rejected': '-1.4915', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.62741', 'logps_train/rejected': '-151.75', 'logps_train/chosen': '-158.59', 'loss/train': '0.60709', 'examples_per_second': '47.574', 'grad_norm': '25.078', 'counters/examples': 37248, 'counters/updates': 1164}
skipping logging after 37280 examples to avoid logging too frequently
train stats after 37312 examples: {'rewards_train/chosen': '-1.2757', 'rewards_train/rejected': '-1.3164', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0407', 'logps_train/rejected': '-182.05', 'logps_train/chosen': '-172.26', 'loss/train': '0.84476', 'examples_per_second': '45.866', 'grad_norm': '37.704', 'counters/examples': 37312, 'counters/updates': 1166}
skipping logging after 37344 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.58123', 'rewards_train/rejected': '-1.245', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.6638', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-151.66', 'loss/train': '0.57626', 'examples_per_second': '45.326', 'grad_norm': '24.676', 'counters/examples': 37376, 'counters/updates': 1168}
skipping logging after 37408 examples to avoid logging too frequently
train stats after 37440 examples: {'rewards_train/chosen': '-0.79036', 'rewards_train/rejected': '-1.3057', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51531', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-173.59', 'loss/train': '0.65423', 'examples_per_second': '45.45', 'grad_norm': '27.723', 'counters/examples': 37440, 'counters/updates': 1170}
skipping logging after 37472 examples to avoid logging too frequently
train stats after 37504 examples: {'rewards_train/chosen': '-0.77993', 'rewards_train/rejected': '-1.3653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.58534', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-131.44', 'loss/train': '0.52183', 'examples_per_second': '47.444', 'grad_norm': '22.162', 'counters/examples': 37504, 'counters/updates': 1172}
skipping logging after 37536 examples to avoid logging too frequently
train stats after 37568 examples: {'rewards_train/chosen': '-0.56008', 'rewards_train/rejected': '-0.93321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37313', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-106.34', 'loss/train': '0.62956', 'examples_per_second': '31.109', 'grad_norm': '24.536', 'counters/examples': 37568, 'counters/updates': 1174}
skipping logging after 37600 examples to avoid logging too frequently
train stats after 37632 examples: {'rewards_train/chosen': '-0.64491', 'rewards_train/rejected': '-0.91794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27302', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-118.91', 'loss/train': '0.73052', 'examples_per_second': '44.738', 'grad_norm': '25.425', 'counters/examples': 37632, 'counters/updates': 1176}
skipping logging after 37664 examples to avoid logging too frequently
train stats after 37696 examples: {'rewards_train/chosen': '-0.64151', 'rewards_train/rejected': '-1.0593', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41781', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-165.87', 'loss/train': '0.6176', 'examples_per_second': '48.255', 'grad_norm': '27.222', 'counters/examples': 37696, 'counters/updates': 1178}
skipping logging after 37728 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '-0.88503', 'rewards_train/rejected': '-1.3806', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49553', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-190.34', 'loss/train': '0.56945', 'examples_per_second': '45.844', 'grad_norm': '26.436', 'counters/examples': 37760, 'counters/updates': 1180}
skipping logging after 37792 examples to avoid logging too frequently
train stats after 37824 examples: {'rewards_train/chosen': '-0.62583', 'rewards_train/rejected': '-1.4037', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.77785', 'logps_train/rejected': '-172.81', 'logps_train/chosen': '-163.86', 'loss/train': '0.52062', 'examples_per_second': '45.692', 'grad_norm': '24.204', 'counters/examples': 37824, 'counters/updates': 1182}
skipping logging after 37856 examples to avoid logging too frequently
train stats after 37888 examples: {'rewards_train/chosen': '-0.6117', 'rewards_train/rejected': '-1.1804', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56874', 'logps_train/rejected': '-98.642', 'logps_train/chosen': '-117.53', 'loss/train': '0.55465', 'examples_per_second': '50.07', 'grad_norm': '20.996', 'counters/examples': 37888, 'counters/updates': 1184}
skipping logging after 37920 examples to avoid logging too frequently
train stats after 37952 examples: {'rewards_train/chosen': '-0.53704', 'rewards_train/rejected': '-0.82022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28318', 'logps_train/rejected': '-90.261', 'logps_train/chosen': '-121.13', 'loss/train': '0.63762', 'examples_per_second': '48.327', 'grad_norm': '23.224', 'counters/examples': 37952, 'counters/updates': 1186}
skipping logging after 37984 examples to avoid logging too frequently
train stats after 38016 examples: {'rewards_train/chosen': '-0.53027', 'rewards_train/rejected': '-0.87584', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.34557', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-167.27', 'loss/train': '0.63883', 'examples_per_second': '46.353', 'grad_norm': '26.442', 'counters/examples': 38016, 'counters/updates': 1188}
skipping logging after 38048 examples to avoid logging too frequently
train stats after 38080 examples: {'rewards_train/chosen': '-0.36825', 'rewards_train/rejected': '-0.93677', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.56852', 'logps_train/rejected': '-97.895', 'logps_train/chosen': '-150.54', 'loss/train': '0.51537', 'examples_per_second': '44.542', 'grad_norm': '21.17', 'counters/examples': 38080, 'counters/updates': 1190}
skipping logging after 38112 examples to avoid logging too frequently
train stats after 38144 examples: {'rewards_train/chosen': '-0.99555', 'rewards_train/rejected': '-1.2163', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.2207', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-129.5', 'loss/train': '0.70573', 'examples_per_second': '45.639', 'grad_norm': '28.035', 'counters/examples': 38144, 'counters/updates': 1192}
skipping logging after 38176 examples to avoid logging too frequently
train stats after 38208 examples: {'rewards_train/chosen': '-0.25982', 'rewards_train/rejected': '-0.79784', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53802', 'logps_train/rejected': '-111.97', 'logps_train/chosen': '-172.36', 'loss/train': '0.54056', 'examples_per_second': '46.162', 'grad_norm': '23.117', 'counters/examples': 38208, 'counters/updates': 1194}
skipping logging after 38240 examples to avoid logging too frequently
train stats after 38272 examples: {'rewards_train/chosen': '-0.83249', 'rewards_train/rejected': '-1.4468', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.6143', 'logps_train/rejected': '-147.69', 'logps_train/chosen': '-155.67', 'loss/train': '0.51564', 'examples_per_second': '44.403', 'grad_norm': '22.503', 'counters/examples': 38272, 'counters/updates': 1196}
skipping logging after 38304 examples to avoid logging too frequently
train stats after 38336 examples: {'rewards_train/chosen': '-0.80885', 'rewards_train/rejected': '-1.2297', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.42085', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-103.23', 'loss/train': '0.59712', 'examples_per_second': '52.433', 'grad_norm': '23.685', 'counters/examples': 38336, 'counters/updates': 1198}
skipping logging after 38368 examples to avoid logging too frequently
train stats after 38400 examples: {'rewards_train/chosen': '-1.1994', 'rewards_train/rejected': '-1.4727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.27321', 'logps_train/rejected': '-143.85', 'logps_train/chosen': '-153', 'loss/train': '0.68068', 'examples_per_second': '46.477', 'grad_norm': '27.374', 'counters/examples': 38400, 'counters/updates': 1200}
skipping logging after 38432 examples to avoid logging too frequently
train stats after 38464 examples: {'rewards_train/chosen': '-0.91472', 'rewards_train/rejected': '-1.6317', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.71699', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-173.4', 'loss/train': '0.49791', 'examples_per_second': '45.638', 'grad_norm': '21.742', 'counters/examples': 38464, 'counters/updates': 1202}
skipping logging after 38496 examples to avoid logging too frequently
train stats after 38528 examples: {'rewards_train/chosen': '-0.52022', 'rewards_train/rejected': '-1.1965', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.67626', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-138.84', 'loss/train': '0.50155', 'examples_per_second': '45.374', 'grad_norm': '21.888', 'counters/examples': 38528, 'counters/updates': 1204}
skipping logging after 38560 examples to avoid logging too frequently
train stats after 38592 examples: {'rewards_train/chosen': '-0.89526', 'rewards_train/rejected': '-1.4937', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59848', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-125.91', 'loss/train': '0.57501', 'examples_per_second': '49.484', 'grad_norm': '21.795', 'counters/examples': 38592, 'counters/updates': 1206}
skipping logging after 38624 examples to avoid logging too frequently
train stats after 38656 examples: {'rewards_train/chosen': '-0.77895', 'rewards_train/rejected': '-1.3801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60117', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-156.76', 'loss/train': '0.57094', 'examples_per_second': '46.184', 'grad_norm': '24.45', 'counters/examples': 38656, 'counters/updates': 1208}
skipping logging after 38688 examples to avoid logging too frequently
train stats after 38720 examples: {'rewards_train/chosen': '-0.61778', 'rewards_train/rejected': '-1.2502', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63238', 'logps_train/rejected': '-156.05', 'logps_train/chosen': '-159.18', 'loss/train': '0.52372', 'examples_per_second': '44.865', 'grad_norm': '25.121', 'counters/examples': 38720, 'counters/updates': 1210}
skipping logging after 38752 examples to avoid logging too frequently
train stats after 38784 examples: {'rewards_train/chosen': '-0.40511', 'rewards_train/rejected': '-1.2924', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.88733', 'logps_train/rejected': '-160.05', 'logps_train/chosen': '-149.08', 'loss/train': '0.44202', 'examples_per_second': '44.193', 'grad_norm': '20.65', 'counters/examples': 38784, 'counters/updates': 1212}
skipping logging after 38816 examples to avoid logging too frequently
train stats after 38848 examples: {'rewards_train/chosen': '-0.62906', 'rewards_train/rejected': '-0.92381', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29475', 'logps_train/rejected': '-161.37', 'logps_train/chosen': '-178.34', 'loss/train': '0.64097', 'examples_per_second': '44.611', 'grad_norm': '29.503', 'counters/examples': 38848, 'counters/updates': 1214}
skipping logging after 38880 examples to avoid logging too frequently
train stats after 38912 examples: {'rewards_train/chosen': '-0.69551', 'rewards_train/rejected': '-1.333', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63746', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-123.9', 'loss/train': '0.59738', 'examples_per_second': '45.65', 'grad_norm': '22.508', 'counters/examples': 38912, 'counters/updates': 1216}
skipping logging after 38944 examples to avoid logging too frequently
train stats after 38976 examples: {'rewards_train/chosen': '-0.54661', 'rewards_train/rejected': '-1.1661', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61947', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-179.36', 'loss/train': '0.60085', 'examples_per_second': '45.665', 'grad_norm': '28.241', 'counters/examples': 38976, 'counters/updates': 1218}
skipping logging after 39008 examples to avoid logging too frequently
train stats after 39040 examples: {'rewards_train/chosen': '-0.66243', 'rewards_train/rejected': '-1.4899', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.82751', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-129.03', 'loss/train': '0.43991', 'examples_per_second': '45.704', 'grad_norm': '19.402', 'counters/examples': 39040, 'counters/updates': 1220}
skipping logging after 39072 examples to avoid logging too frequently
train stats after 39104 examples: {'rewards_train/chosen': '-0.52683', 'rewards_train/rejected': '-1.0214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49456', 'logps_train/rejected': '-104.74', 'logps_train/chosen': '-195.87', 'loss/train': '0.58795', 'examples_per_second': '46.875', 'grad_norm': '29.179', 'counters/examples': 39104, 'counters/updates': 1222}
skipping logging after 39136 examples to avoid logging too frequently
train stats after 39168 examples: {'rewards_train/chosen': '-1.0398', 'rewards_train/rejected': '-1.7011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.66125', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-145.85', 'loss/train': '0.60211', 'examples_per_second': '52.925', 'grad_norm': '24.609', 'counters/examples': 39168, 'counters/updates': 1224}
skipping logging after 39200 examples to avoid logging too frequently
train stats after 39232 examples: {'rewards_train/chosen': '-0.98853', 'rewards_train/rejected': '-1.7945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.80597', 'logps_train/rejected': '-131.17', 'logps_train/chosen': '-138.44', 'loss/train': '0.5208', 'examples_per_second': '53.753', 'grad_norm': '22.39', 'counters/examples': 39232, 'counters/updates': 1226}
skipping logging after 39264 examples to avoid logging too frequently
train stats after 39296 examples: {'rewards_train/chosen': '-0.86306', 'rewards_train/rejected': '-1.4945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63144', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-140.09', 'loss/train': '0.59007', 'examples_per_second': '49.004', 'grad_norm': '24.424', 'counters/examples': 39296, 'counters/updates': 1228}
skipping logging after 39328 examples to avoid logging too frequently
train stats after 39360 examples: {'rewards_train/chosen': '-0.69254', 'rewards_train/rejected': '-1.1355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.44295', 'logps_train/rejected': '-151.11', 'logps_train/chosen': '-128.04', 'loss/train': '0.61254', 'examples_per_second': '46.904', 'grad_norm': '25.923', 'counters/examples': 39360, 'counters/updates': 1230}
skipping logging after 39392 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '-0.85592', 'rewards_train/rejected': '-1.3627', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.50679', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-157.91', 'loss/train': '0.55394', 'examples_per_second': '45.639', 'grad_norm': '24.635', 'counters/examples': 39424, 'counters/updates': 1232}
skipping logging after 39456 examples to avoid logging too frequently
train stats after 39488 examples: {'rewards_train/chosen': '-1.2046', 'rewards_train/rejected': '-1.4032', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.19856', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-120.28', 'loss/train': '0.70506', 'examples_per_second': '44.471', 'grad_norm': '27.623', 'counters/examples': 39488, 'counters/updates': 1234}
skipping logging after 39520 examples to avoid logging too frequently
train stats after 39552 examples: {'rewards_train/chosen': '-1.2255', 'rewards_train/rejected': '-1.6313', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.40587', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-119.67', 'loss/train': '0.65478', 'examples_per_second': '45.012', 'grad_norm': '25.623', 'counters/examples': 39552, 'counters/updates': 1236}
skipping logging after 39584 examples to avoid logging too frequently
train stats after 39616 examples: {'rewards_train/chosen': '-0.96752', 'rewards_train/rejected': '-1.1937', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2262', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-144.14', 'loss/train': '0.69544', 'examples_per_second': '46.796', 'grad_norm': '25.165', 'counters/examples': 39616, 'counters/updates': 1238}
skipping logging after 39648 examples to avoid logging too frequently
train stats after 39680 examples: {'rewards_train/chosen': '-0.6276', 'rewards_train/rejected': '-1.1161', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.48852', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-153.14', 'loss/train': '0.58739', 'examples_per_second': '46.973', 'grad_norm': '25.816', 'counters/examples': 39680, 'counters/updates': 1240}
skipping logging after 39712 examples to avoid logging too frequently
train stats after 39744 examples: {'rewards_train/chosen': '-0.53018', 'rewards_train/rejected': '-1.2052', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.67504', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-166.49', 'loss/train': '0.49382', 'examples_per_second': '46.306', 'grad_norm': '20.366', 'counters/examples': 39744, 'counters/updates': 1242}
skipping logging after 39776 examples to avoid logging too frequently
train stats after 39808 examples: {'rewards_train/chosen': '-0.54237', 'rewards_train/rejected': '-0.74594', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.20358', 'logps_train/rejected': '-107.25', 'logps_train/chosen': '-122.96', 'loss/train': '0.68263', 'examples_per_second': '46.809', 'grad_norm': '24.467', 'counters/examples': 39808, 'counters/updates': 1244}
skipping logging after 39840 examples to avoid logging too frequently
train stats after 39872 examples: {'rewards_train/chosen': '-0.77144', 'rewards_train/rejected': '-1.3355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.56405', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-117.43', 'loss/train': '0.55556', 'examples_per_second': '44.67', 'grad_norm': '22.543', 'counters/examples': 39872, 'counters/updates': 1246}
skipping logging after 39904 examples to avoid logging too frequently
train stats after 39936 examples: {'rewards_train/chosen': '-0.73063', 'rewards_train/rejected': '-1.2557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52511', 'logps_train/rejected': '-108.64', 'logps_train/chosen': '-145.1', 'loss/train': '0.56149', 'examples_per_second': '44.736', 'grad_norm': '21.858', 'counters/examples': 39936, 'counters/updates': 1248}
skipping logging after 39968 examples to avoid logging too frequently
train stats after 40000 examples: {'rewards_train/chosen': '-0.7091', 'rewards_train/rejected': '-1.1291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42005', 'logps_train/rejected': '-152.81', 'logps_train/chosen': '-129.04', 'loss/train': '0.58012', 'examples_per_second': '44.47', 'grad_norm': '27.084', 'counters/examples': 40000, 'counters/updates': 1250}
skipping logging after 40032 examples to avoid logging too frequently
train stats after 40064 examples: {'rewards_train/chosen': '-0.48622', 'rewards_train/rejected': '-1.1203', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.63411', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-144.98', 'loss/train': '0.53252', 'examples_per_second': '44.527', 'grad_norm': '27.552', 'counters/examples': 40064, 'counters/updates': 1252}
skipping logging after 40096 examples to avoid logging too frequently
train stats after 40128 examples: {'rewards_train/chosen': '-0.72979', 'rewards_train/rejected': '-1.2077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47792', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-159.62', 'loss/train': '0.60361', 'examples_per_second': '45.71', 'grad_norm': '22.588', 'counters/examples': 40128, 'counters/updates': 1254}
skipping logging after 40160 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '-0.76122', 'rewards_train/rejected': '-1.4767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.71546', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-157.5', 'loss/train': '0.5467', 'examples_per_second': '46.348', 'grad_norm': '23.23', 'counters/examples': 40192, 'counters/updates': 1256}
skipping logging after 40224 examples to avoid logging too frequently
train stats after 40256 examples: {'rewards_train/chosen': '-0.71938', 'rewards_train/rejected': '-1.357', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.63758', 'logps_train/rejected': '-150.88', 'logps_train/chosen': '-159', 'loss/train': '0.58359', 'examples_per_second': '44.978', 'grad_norm': '27.395', 'counters/examples': 40256, 'counters/updates': 1258}
skipping logging after 40288 examples to avoid logging too frequently
train stats after 40320 examples: {'rewards_train/chosen': '-0.71541', 'rewards_train/rejected': '-1.3458', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.63039', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-130.04', 'loss/train': '0.6116', 'examples_per_second': '45.761', 'grad_norm': '23.121', 'counters/examples': 40320, 'counters/updates': 1260}
skipping logging after 40352 examples to avoid logging too frequently
train stats after 40384 examples: {'rewards_train/chosen': '-0.9574', 'rewards_train/rejected': '-1.6185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.66105', 'logps_train/rejected': '-123.33', 'logps_train/chosen': '-133.6', 'loss/train': '0.54636', 'examples_per_second': '47.114', 'grad_norm': '22.158', 'counters/examples': 40384, 'counters/updates': 1262}
skipping logging after 40416 examples to avoid logging too frequently
train stats after 40448 examples: {'rewards_train/chosen': '-0.69876', 'rewards_train/rejected': '-1.2552', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55643', 'logps_train/rejected': '-176.49', 'logps_train/chosen': '-134.85', 'loss/train': '0.57878', 'examples_per_second': '46.229', 'grad_norm': '24.172', 'counters/examples': 40448, 'counters/updates': 1264}
skipping logging after 40480 examples to avoid logging too frequently
train stats after 40512 examples: {'rewards_train/chosen': '-0.55224', 'rewards_train/rejected': '-0.92411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37187', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-134.42', 'loss/train': '0.65313', 'examples_per_second': '46.468', 'grad_norm': '25.713', 'counters/examples': 40512, 'counters/updates': 1266}
skipping logging after 40544 examples to avoid logging too frequently
train stats after 40576 examples: {'rewards_train/chosen': '-0.61066', 'rewards_train/rejected': '-0.97449', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36383', 'logps_train/rejected': '-124.45', 'logps_train/chosen': '-140.83', 'loss/train': '0.67002', 'examples_per_second': '44.613', 'grad_norm': '27.247', 'counters/examples': 40576, 'counters/updates': 1268}
skipping logging after 40608 examples to avoid logging too frequently
train stats after 40640 examples: {'rewards_train/chosen': '-0.82028', 'rewards_train/rejected': '-1.1937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37338', 'logps_train/rejected': '-111.09', 'logps_train/chosen': '-114.27', 'loss/train': '0.66669', 'examples_per_second': '46.555', 'grad_norm': '23.924', 'counters/examples': 40640, 'counters/updates': 1270}
skipping logging after 40672 examples to avoid logging too frequently
train stats after 40704 examples: {'rewards_train/chosen': '-0.74981', 'rewards_train/rejected': '-0.87514', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12533', 'logps_train/rejected': '-105.98', 'logps_train/chosen': '-132.2', 'loss/train': '0.79341', 'examples_per_second': '46.669', 'grad_norm': '30.076', 'counters/examples': 40704, 'counters/updates': 1272}
skipping logging after 40736 examples to avoid logging too frequently
train stats after 40768 examples: {'rewards_train/chosen': '-0.54711', 'rewards_train/rejected': '-1.4089', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.86182', 'logps_train/rejected': '-134.97', 'logps_train/chosen': '-154.14', 'loss/train': '0.49388', 'examples_per_second': '47.208', 'grad_norm': '23.751', 'counters/examples': 40768, 'counters/updates': 1274}
skipping logging after 40800 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.92458', 'rewards_train/rejected': '-1.5707', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64613', 'logps_train/rejected': '-104.2', 'logps_train/chosen': '-123.41', 'loss/train': '0.52309', 'examples_per_second': '51.337', 'grad_norm': '20.527', 'counters/examples': 40832, 'counters/updates': 1276}
skipping logging after 40864 examples to avoid logging too frequently
train stats after 40896 examples: {'rewards_train/chosen': '-0.83342', 'rewards_train/rejected': '-1.5848', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.7514', 'logps_train/rejected': '-153.58', 'logps_train/chosen': '-161.28', 'loss/train': '0.47501', 'examples_per_second': '45.931', 'grad_norm': '22.287', 'counters/examples': 40896, 'counters/updates': 1278}
skipping logging after 40928 examples to avoid logging too frequently
train stats after 40960 examples: {'rewards_train/chosen': '-0.75693', 'rewards_train/rejected': '-1.4933', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.73639', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-122.04', 'loss/train': '0.5474', 'examples_per_second': '48.65', 'grad_norm': '21.36', 'counters/examples': 40960, 'counters/updates': 1280}
skipping logging after 40992 examples to avoid logging too frequently
train stats after 41024 examples: {'rewards_train/chosen': '-0.51421', 'rewards_train/rejected': '-0.95001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4358', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-135.84', 'loss/train': '0.63542', 'examples_per_second': '45.71', 'grad_norm': '24.645', 'counters/examples': 41024, 'counters/updates': 1282}
skipping logging after 41056 examples to avoid logging too frequently
train stats after 41088 examples: {'rewards_train/chosen': '-1.1484', 'rewards_train/rejected': '-1.3032', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.15482', 'logps_train/rejected': '-151.13', 'logps_train/chosen': '-148.55', 'loss/train': '0.77999', 'examples_per_second': '44.868', 'grad_norm': '29.184', 'counters/examples': 41088, 'counters/updates': 1284}
skipping logging after 41120 examples to avoid logging too frequently
train stats after 41152 examples: {'rewards_train/chosen': '-0.84706', 'rewards_train/rejected': '-1.4967', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.64961', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-145', 'loss/train': '0.49591', 'examples_per_second': '48.769', 'grad_norm': '21.657', 'counters/examples': 41152, 'counters/updates': 1286}
skipping logging after 41184 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '-0.8941', 'rewards_train/rejected': '-1.4604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.56633', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-126.67', 'loss/train': '0.54759', 'examples_per_second': '52.604', 'grad_norm': '23.661', 'counters/examples': 41216, 'counters/updates': 1288}
skipping logging after 41248 examples to avoid logging too frequently
train stats after 41280 examples: {'rewards_train/chosen': '-1.0526', 'rewards_train/rejected': '-1.2975', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.24487', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-147.24', 'loss/train': '0.70656', 'examples_per_second': '46.847', 'grad_norm': '26.462', 'counters/examples': 41280, 'counters/updates': 1290}
skipping logging after 41312 examples to avoid logging too frequently
train stats after 41344 examples: {'rewards_train/chosen': '-1.5093', 'rewards_train/rejected': '-1.7722', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.26291', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-119.03', 'loss/train': '0.78771', 'examples_per_second': '48.912', 'grad_norm': '23.792', 'counters/examples': 41344, 'counters/updates': 1292}
skipping logging after 41376 examples to avoid logging too frequently
train stats after 41408 examples: {'rewards_train/chosen': '-0.85222', 'rewards_train/rejected': '-1.55', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.69773', 'logps_train/rejected': '-124.48', 'logps_train/chosen': '-134.85', 'loss/train': '0.51221', 'examples_per_second': '46.407', 'grad_norm': '22.62', 'counters/examples': 41408, 'counters/updates': 1294}
skipping logging after 41440 examples to avoid logging too frequently
train stats after 41472 examples: {'rewards_train/chosen': '-0.931', 'rewards_train/rejected': '-1.0754', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.14436', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-179.26', 'loss/train': '0.76112', 'examples_per_second': '45.174', 'grad_norm': '29.631', 'counters/examples': 41472, 'counters/updates': 1296}
skipping logging after 41504 examples to avoid logging too frequently
train stats after 41536 examples: {'rewards_train/chosen': '-0.63209', 'rewards_train/rejected': '-1.1493', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51717', 'logps_train/rejected': '-163.59', 'logps_train/chosen': '-145.84', 'loss/train': '0.57576', 'examples_per_second': '47.172', 'grad_norm': '25.836', 'counters/examples': 41536, 'counters/updates': 1298}
skipping logging after 41568 examples to avoid logging too frequently
train stats after 41600 examples: {'rewards_train/chosen': '-0.92611', 'rewards_train/rejected': '-1.3396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41346', 'logps_train/rejected': '-156.03', 'logps_train/chosen': '-135.27', 'loss/train': '0.63254', 'examples_per_second': '45.342', 'grad_norm': '28.68', 'counters/examples': 41600, 'counters/updates': 1300}
skipping logging after 41632 examples to avoid logging too frequently
train stats after 41664 examples: {'rewards_train/chosen': '-0.78416', 'rewards_train/rejected': '-1.2337', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44954', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-128.36', 'loss/train': '0.61707', 'examples_per_second': '45.691', 'grad_norm': '24.765', 'counters/examples': 41664, 'counters/updates': 1302}
skipping logging after 41696 examples to avoid logging too frequently
train stats after 41728 examples: {'rewards_train/chosen': '-0.78516', 'rewards_train/rejected': '-0.62495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.16021', 'logps_train/rejected': '-112.45', 'logps_train/chosen': '-134.81', 'loss/train': '0.90759', 'examples_per_second': '44.581', 'grad_norm': '29.668', 'counters/examples': 41728, 'counters/updates': 1304}
skipping logging after 41760 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': '-0.63361', 'rewards_train/rejected': '-0.85603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22242', 'logps_train/rejected': '-165.02', 'logps_train/chosen': '-141', 'loss/train': '0.72338', 'examples_per_second': '45.561', 'grad_norm': '30.207', 'counters/examples': 41792, 'counters/updates': 1306}
skipping logging after 41824 examples to avoid logging too frequently
train stats after 41856 examples: {'rewards_train/chosen': '-0.62772', 'rewards_train/rejected': '-0.98448', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35675', 'logps_train/rejected': '-135.36', 'logps_train/chosen': '-154.74', 'loss/train': '0.66954', 'examples_per_second': '45.794', 'grad_norm': '27.238', 'counters/examples': 41856, 'counters/updates': 1308}
skipping logging after 41888 examples to avoid logging too frequently
train stats after 41920 examples: {'rewards_train/chosen': '0.13089', 'rewards_train/rejected': '-0.8514', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.9823', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-128.34', 'loss/train': '0.41254', 'examples_per_second': '45.07', 'grad_norm': '19.822', 'counters/examples': 41920, 'counters/updates': 1310}
skipping logging after 41952 examples to avoid logging too frequently
train stats after 41984 examples: {'rewards_train/chosen': '-0.40519', 'rewards_train/rejected': '-0.9107', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50551', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-118.82', 'loss/train': '0.57864', 'examples_per_second': '47.744', 'grad_norm': '26.509', 'counters/examples': 41984, 'counters/updates': 1312}
skipping logging after 42016 examples to avoid logging too frequently
train stats after 42048 examples: {'rewards_train/chosen': '-0.68254', 'rewards_train/rejected': '-1.4114', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.7289', 'logps_train/rejected': '-150.36', 'logps_train/chosen': '-143.01', 'loss/train': '0.537', 'examples_per_second': '46.671', 'grad_norm': '24.832', 'counters/examples': 42048, 'counters/updates': 1314}
skipping logging after 42080 examples to avoid logging too frequently
train stats after 42112 examples: {'rewards_train/chosen': '-0.56294', 'rewards_train/rejected': '-1.0943', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.53137', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-126.25', 'loss/train': '0.53922', 'examples_per_second': '45.757', 'grad_norm': '21.61', 'counters/examples': 42112, 'counters/updates': 1316}
skipping logging after 42144 examples to avoid logging too frequently
train stats after 42176 examples: {'rewards_train/chosen': '-1.0835', 'rewards_train/rejected': '-1.9454', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.86186', 'logps_train/rejected': '-189.87', 'logps_train/chosen': '-168.37', 'loss/train': '0.55917', 'examples_per_second': '45.271', 'grad_norm': '27.494', 'counters/examples': 42176, 'counters/updates': 1318}
skipping logging after 42208 examples to avoid logging too frequently
train stats after 42240 examples: {'rewards_train/chosen': '-0.98935', 'rewards_train/rejected': '-1.8272', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.83789', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-151.57', 'loss/train': '0.5327', 'examples_per_second': '44.834', 'grad_norm': '25.686', 'counters/examples': 42240, 'counters/updates': 1320}
skipping logging after 42272 examples to avoid logging too frequently
train stats after 42304 examples: {'rewards_train/chosen': '-1.1081', 'rewards_train/rejected': '-1.5189', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41083', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-134.35', 'loss/train': '0.62007', 'examples_per_second': '46.478', 'grad_norm': '24.342', 'counters/examples': 42304, 'counters/updates': 1322}
skipping logging after 42336 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '-0.94121', 'rewards_train/rejected': '-1.6273', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.68614', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-133.45', 'loss/train': '0.56096', 'examples_per_second': '52.353', 'grad_norm': '27.578', 'counters/examples': 42368, 'counters/updates': 1324}
skipping logging after 42400 examples to avoid logging too frequently
train stats after 42432 examples: {'rewards_train/chosen': '-0.4138', 'rewards_train/rejected': '-0.95102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53721', 'logps_train/rejected': '-99.879', 'logps_train/chosen': '-115.83', 'loss/train': '0.56058', 'examples_per_second': '46.902', 'grad_norm': '24.598', 'counters/examples': 42432, 'counters/updates': 1326}
skipping logging after 42464 examples to avoid logging too frequently
train stats after 42496 examples: {'rewards_train/chosen': '-0.57847', 'rewards_train/rejected': '-1.1257', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.54723', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-128.46', 'loss/train': '0.55458', 'examples_per_second': '45.663', 'grad_norm': '22.573', 'counters/examples': 42496, 'counters/updates': 1328}
skipping logging after 42528 examples to avoid logging too frequently
train stats after 42560 examples: {'rewards_train/chosen': '-0.77178', 'rewards_train/rejected': '-1.5057', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.73394', 'logps_train/rejected': '-154.36', 'logps_train/chosen': '-134.16', 'loss/train': '0.49391', 'examples_per_second': '45.403', 'grad_norm': '23.8', 'counters/examples': 42560, 'counters/updates': 1330}
skipping logging after 42592 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '-0.80717', 'rewards_train/rejected': '-1.0997', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.29252', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-138.11', 'loss/train': '0.71461', 'examples_per_second': '44.966', 'grad_norm': '26.198', 'counters/examples': 42624, 'counters/updates': 1332}
skipping logging after 42656 examples to avoid logging too frequently
train stats after 42688 examples: {'rewards_train/chosen': '-0.65407', 'rewards_train/rejected': '-1.1151', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.46106', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-144.2', 'loss/train': '0.56873', 'examples_per_second': '47.037', 'grad_norm': '25.774', 'counters/examples': 42688, 'counters/updates': 1334}
skipping logging after 42720 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '-0.65746', 'rewards_train/rejected': '-1.1561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49862', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-164.2', 'loss/train': '0.58586', 'examples_per_second': '45.761', 'grad_norm': '25.035', 'counters/examples': 42752, 'counters/updates': 1336}
skipping logging after 42784 examples to avoid logging too frequently
train stats after 42816 examples: {'rewards_train/chosen': '-0.8186', 'rewards_train/rejected': '-1.1495', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33088', 'logps_train/rejected': '-159.73', 'logps_train/chosen': '-166.29', 'loss/train': '0.65044', 'examples_per_second': '46.44', 'grad_norm': '30.066', 'counters/examples': 42816, 'counters/updates': 1338}
skipping logging after 42848 examples to avoid logging too frequently
train stats after 42880 examples: {'rewards_train/chosen': '-0.6644', 'rewards_train/rejected': '-1.4431', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.77875', 'logps_train/rejected': '-150.24', 'logps_train/chosen': '-130.72', 'loss/train': '0.49128', 'examples_per_second': '48.078', 'grad_norm': '21.687', 'counters/examples': 42880, 'counters/updates': 1340}
skipping logging after 42912 examples to avoid logging too frequently
train stats after 42944 examples: {'rewards_train/chosen': '-0.89498', 'rewards_train/rejected': '-1.2385', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3435', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-141.48', 'loss/train': '0.7156', 'examples_per_second': '45.626', 'grad_norm': '26.316', 'counters/examples': 42944, 'counters/updates': 1342}
skipping logging after 42976 examples to avoid logging too frequently
train stats after 43008 examples: {'rewards_train/chosen': '-0.44572', 'rewards_train/rejected': '-1.2758', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.83006', 'logps_train/rejected': '-145.51', 'logps_train/chosen': '-169.06', 'loss/train': '0.55956', 'examples_per_second': '45.644', 'grad_norm': '26.654', 'counters/examples': 43008, 'counters/updates': 1344}
skipping logging after 43040 examples to avoid logging too frequently
train stats after 43072 examples: {'rewards_train/chosen': '-0.59644', 'rewards_train/rejected': '-1.4632', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.86679', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-153.89', 'loss/train': '0.4927', 'examples_per_second': '46.393', 'grad_norm': '23.166', 'counters/examples': 43072, 'counters/updates': 1346}
skipping logging after 43104 examples to avoid logging too frequently
train stats after 43136 examples: {'rewards_train/chosen': '-0.69902', 'rewards_train/rejected': '-1.3528', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.65374', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-155.14', 'loss/train': '0.59329', 'examples_per_second': '46.297', 'grad_norm': '23.927', 'counters/examples': 43136, 'counters/updates': 1348}
skipping logging after 43168 examples to avoid logging too frequently
train stats after 43200 examples: {'rewards_train/chosen': '-0.40578', 'rewards_train/rejected': '-0.85429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.44851', 'logps_train/rejected': '-130.88', 'logps_train/chosen': '-143.84', 'loss/train': '0.64129', 'examples_per_second': '45.577', 'grad_norm': '26.692', 'counters/examples': 43200, 'counters/updates': 1350}
skipping logging after 43232 examples to avoid logging too frequently
train stats after 43264 examples: {'rewards_train/chosen': '-0.66732', 'rewards_train/rejected': '-1.2115', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54415', 'logps_train/rejected': '-162.06', 'logps_train/chosen': '-159.6', 'loss/train': '0.64727', 'examples_per_second': '44.741', 'grad_norm': '28.46', 'counters/examples': 43264, 'counters/updates': 1352}
skipping logging after 43296 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': '-0.16372', 'rewards_train/rejected': '-1.1699', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.0061', 'logps_train/rejected': '-147.9', 'logps_train/chosen': '-161.63', 'loss/train': '0.40956', 'examples_per_second': '44.785', 'grad_norm': '21.408', 'counters/examples': 43328, 'counters/updates': 1354}
skipping logging after 43360 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '-0.18913', 'rewards_train/rejected': '-0.83959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.65045', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-152.52', 'loss/train': '0.5548', 'examples_per_second': '44.634', 'grad_norm': '26.968', 'counters/examples': 43392, 'counters/updates': 1356}
skipping logging after 43424 examples to avoid logging too frequently
train stats after 43456 examples: {'rewards_train/chosen': '-0.30947', 'rewards_train/rejected': '-1.0787', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.76924', 'logps_train/rejected': '-156.16', 'logps_train/chosen': '-127.13', 'loss/train': '0.49519', 'examples_per_second': '45.514', 'grad_norm': '21.666', 'counters/examples': 43456, 'counters/updates': 1358}
skipping logging after 43488 examples to avoid logging too frequently
train stats after 43520 examples: {'rewards_train/chosen': '-0.54107', 'rewards_train/rejected': '-1.5814', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.0403', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-153.14', 'loss/train': '0.44915', 'examples_per_second': '45.543', 'grad_norm': '21.925', 'counters/examples': 43520, 'counters/updates': 1360}
skipping logging after 43552 examples to avoid logging too frequently
train stats after 43584 examples: {'rewards_train/chosen': '-0.43734', 'rewards_train/rejected': '-1.0457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60839', 'logps_train/rejected': '-133.75', 'logps_train/chosen': '-146.56', 'loss/train': '0.58455', 'examples_per_second': '45.616', 'grad_norm': '29.495', 'counters/examples': 43584, 'counters/updates': 1362}
skipping logging after 43616 examples to avoid logging too frequently
train stats after 43648 examples: {'rewards_train/chosen': '-0.55053', 'rewards_train/rejected': '-1.3577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.80717', 'logps_train/rejected': '-105.12', 'logps_train/chosen': '-155.11', 'loss/train': '0.49328', 'examples_per_second': '47.74', 'grad_norm': '23.434', 'counters/examples': 43648, 'counters/updates': 1364}
skipping logging after 43680 examples to avoid logging too frequently
train stats after 43712 examples: {'rewards_train/chosen': '-0.9546', 'rewards_train/rejected': '-1.1991', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24453', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-173.93', 'loss/train': '0.65896', 'examples_per_second': '45.583', 'grad_norm': '27.425', 'counters/examples': 43712, 'counters/updates': 1366}
skipping logging after 43744 examples to avoid logging too frequently
train stats after 43776 examples: {'rewards_train/chosen': '-1.064', 'rewards_train/rejected': '-1.7728', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.70879', 'logps_train/rejected': '-139.54', 'logps_train/chosen': '-154.27', 'loss/train': '0.53217', 'examples_per_second': '46.708', 'grad_norm': '22.683', 'counters/examples': 43776, 'counters/updates': 1368}
skipping logging after 43808 examples to avoid logging too frequently
train stats after 43840 examples: {'rewards_train/chosen': '-1.1498', 'rewards_train/rejected': '-2.0366', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.88672', 'logps_train/rejected': '-176.06', 'logps_train/chosen': '-161.06', 'loss/train': '0.5015', 'examples_per_second': '45.26', 'grad_norm': '26.155', 'counters/examples': 43840, 'counters/updates': 1370}
skipping logging after 43872 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '-1.0175', 'rewards_train/rejected': '-1.5774', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55989', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-121.5', 'loss/train': '0.6279', 'examples_per_second': '48.445', 'grad_norm': '23.942', 'counters/examples': 43904, 'counters/updates': 1372}
skipping logging after 43936 examples to avoid logging too frequently
train stats after 43968 examples: {'rewards_train/chosen': '-0.60151', 'rewards_train/rejected': '-1.4052', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.80369', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-126.75', 'loss/train': '0.48639', 'examples_per_second': '46.647', 'grad_norm': '21.194', 'counters/examples': 43968, 'counters/updates': 1374}
skipping logging after 44000 examples to avoid logging too frequently
train stats after 44032 examples: {'rewards_train/chosen': '-0.68081', 'rewards_train/rejected': '-1.3335', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.65272', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-137.42', 'loss/train': '0.60507', 'examples_per_second': '45.791', 'grad_norm': '25.553', 'counters/examples': 44032, 'counters/updates': 1376}
skipping logging after 44064 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': '-0.69949', 'rewards_train/rejected': '-1.3887', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.68924', 'logps_train/rejected': '-111.06', 'logps_train/chosen': '-110.9', 'loss/train': '0.5024', 'examples_per_second': '45.041', 'grad_norm': '21.396', 'counters/examples': 44096, 'counters/updates': 1378}
skipping logging after 44128 examples to avoid logging too frequently
train stats after 44160 examples: {'rewards_train/chosen': '-0.913', 'rewards_train/rejected': '-1.5171', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.60409', 'logps_train/rejected': '-122.07', 'logps_train/chosen': '-127.32', 'loss/train': '0.56545', 'examples_per_second': '45.809', 'grad_norm': '22.891', 'counters/examples': 44160, 'counters/updates': 1380}
skipping logging after 44192 examples to avoid logging too frequently
train stats after 44224 examples: {'rewards_train/chosen': '-0.93315', 'rewards_train/rejected': '-1.5976', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.66447', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-172.75', 'loss/train': '0.51956', 'examples_per_second': '45.564', 'grad_norm': '22.357', 'counters/examples': 44224, 'counters/updates': 1382}
skipping logging after 44256 examples to avoid logging too frequently
train stats after 44288 examples: {'rewards_train/chosen': '-0.7205', 'rewards_train/rejected': '-1.1647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.44418', 'logps_train/rejected': '-141.49', 'logps_train/chosen': '-123.86', 'loss/train': '0.6154', 'examples_per_second': '45.961', 'grad_norm': '26.59', 'counters/examples': 44288, 'counters/updates': 1384}
skipping logging after 44320 examples to avoid logging too frequently
train stats after 44352 examples: {'rewards_train/chosen': '-0.79048', 'rewards_train/rejected': '-1.5546', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.76416', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-129.91', 'loss/train': '0.48397', 'examples_per_second': '44.309', 'grad_norm': '20.765', 'counters/examples': 44352, 'counters/updates': 1386}
skipping logging after 44384 examples to avoid logging too frequently
train stats after 44416 examples: {'rewards_train/chosen': '-0.67011', 'rewards_train/rejected': '-1.0572', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.38705', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-151.26', 'loss/train': '0.72298', 'examples_per_second': '43.902', 'grad_norm': '30.164', 'counters/examples': 44416, 'counters/updates': 1388}
skipping logging after 44448 examples to avoid logging too frequently
train stats after 44480 examples: {'rewards_train/chosen': '-0.97539', 'rewards_train/rejected': '-1.682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.70656', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-146.13', 'loss/train': '0.51251', 'examples_per_second': '46.741', 'grad_norm': '23.068', 'counters/examples': 44480, 'counters/updates': 1390}
skipping logging after 44512 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '-0.67421', 'rewards_train/rejected': '-1.4217', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.74751', 'logps_train/rejected': '-146.88', 'logps_train/chosen': '-121.23', 'loss/train': '0.52071', 'examples_per_second': '45.754', 'grad_norm': '23.036', 'counters/examples': 44544, 'counters/updates': 1392}
skipping logging after 44576 examples to avoid logging too frequently
train stats after 44608 examples: {'rewards_train/chosen': '-0.69565', 'rewards_train/rejected': '-1.3987', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.70307', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-173.08', 'loss/train': '0.50604', 'examples_per_second': '45.052', 'grad_norm': '23.258', 'counters/examples': 44608, 'counters/updates': 1394}
skipping logging after 44640 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '-0.7078', 'rewards_train/rejected': '-0.9481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2403', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-144.78', 'loss/train': '0.725', 'examples_per_second': '45.51', 'grad_norm': '30.466', 'counters/examples': 44672, 'counters/updates': 1396}
skipping logging after 44704 examples to avoid logging too frequently
train stats after 44736 examples: {'rewards_train/chosen': '-0.26424', 'rewards_train/rejected': '-0.8807', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61646', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-137.9', 'loss/train': '0.55635', 'examples_per_second': '45.096', 'grad_norm': '26.667', 'counters/examples': 44736, 'counters/updates': 1398}
skipping logging after 44768 examples to avoid logging too frequently
train stats after 44800 examples: {'rewards_train/chosen': '-0.84942', 'rewards_train/rejected': '-1.464', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.61459', 'logps_train/rejected': '-156.07', 'logps_train/chosen': '-192.12', 'loss/train': '0.58835', 'examples_per_second': '45.392', 'grad_norm': '28.864', 'counters/examples': 44800, 'counters/updates': 1400}
skipping logging after 44832 examples to avoid logging too frequently
train stats after 44864 examples: {'rewards_train/chosen': '-0.68195', 'rewards_train/rejected': '-1.2117', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.52975', 'logps_train/rejected': '-160.32', 'logps_train/chosen': '-145.58', 'loss/train': '0.58213', 'examples_per_second': '45.114', 'grad_norm': '27.036', 'counters/examples': 44864, 'counters/updates': 1402}
skipping logging after 44896 examples to avoid logging too frequently
train stats after 44928 examples: {'rewards_train/chosen': '-0.84277', 'rewards_train/rejected': '-1.3001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45737', 'logps_train/rejected': '-191.5', 'logps_train/chosen': '-159.18', 'loss/train': '0.60923', 'examples_per_second': '42.376', 'grad_norm': '27.514', 'counters/examples': 44928, 'counters/updates': 1404}
skipping logging after 44960 examples to avoid logging too frequently
train stats after 44992 examples: {'rewards_train/chosen': '-0.62592', 'rewards_train/rejected': '-1.1309', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50499', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-153.21', 'loss/train': '0.63798', 'examples_per_second': '48.579', 'grad_norm': '30.021', 'counters/examples': 44992, 'counters/updates': 1406}
skipping logging after 45024 examples to avoid logging too frequently
train stats after 45056 examples: {'rewards_train/chosen': '-0.55911', 'rewards_train/rejected': '-1.1169', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55779', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-129.67', 'loss/train': '0.54703', 'examples_per_second': '46.052', 'grad_norm': '23.161', 'counters/examples': 45056, 'counters/updates': 1408}
skipping logging after 45088 examples to avoid logging too frequently
train stats after 45120 examples: {'rewards_train/chosen': '-0.65254', 'rewards_train/rejected': '-1.2358', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58326', 'logps_train/rejected': '-111.74', 'logps_train/chosen': '-119.52', 'loss/train': '0.56981', 'examples_per_second': '46.713', 'grad_norm': '20.82', 'counters/examples': 45120, 'counters/updates': 1410}
skipping logging after 45152 examples to avoid logging too frequently
train stats after 45184 examples: {'rewards_train/chosen': '-0.97381', 'rewards_train/rejected': '-1.3687', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39486', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-141.62', 'loss/train': '0.65988', 'examples_per_second': '45.015', 'grad_norm': '28.224', 'counters/examples': 45184, 'counters/updates': 1412}
skipping logging after 45216 examples to avoid logging too frequently
train stats after 45248 examples: {'rewards_train/chosen': '-0.88426', 'rewards_train/rejected': '-1.1228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.23859', 'logps_train/rejected': '-146.11', 'logps_train/chosen': '-121.1', 'loss/train': '0.69312', 'examples_per_second': '46.744', 'grad_norm': '28.726', 'counters/examples': 45248, 'counters/updates': 1414}
skipping logging after 45280 examples to avoid logging too frequently
train stats after 45312 examples: {'rewards_train/chosen': '-0.79743', 'rewards_train/rejected': '-1.622', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.82459', 'logps_train/rejected': '-160.3', 'logps_train/chosen': '-162.7', 'loss/train': '0.50355', 'examples_per_second': '46.4', 'grad_norm': '23.591', 'counters/examples': 45312, 'counters/updates': 1416}
skipping logging after 45344 examples to avoid logging too frequently
train stats after 45376 examples: {'rewards_train/chosen': '-1.2102', 'rewards_train/rejected': '-1.7448', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.53453', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-139.36', 'loss/train': '0.65574', 'examples_per_second': '45.735', 'grad_norm': '29.155', 'counters/examples': 45376, 'counters/updates': 1418}
skipping logging after 45408 examples to avoid logging too frequently
train stats after 45440 examples: {'rewards_train/chosen': '-0.59985', 'rewards_train/rejected': '-0.70183', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10198', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-116.53', 'loss/train': '0.73052', 'examples_per_second': '45.492', 'grad_norm': '26.509', 'counters/examples': 45440, 'counters/updates': 1420}
skipping logging after 45472 examples to avoid logging too frequently
train stats after 45504 examples: {'rewards_train/chosen': '-0.8839', 'rewards_train/rejected': '-1.5202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63627', 'logps_train/rejected': '-146.69', 'logps_train/chosen': '-191.35', 'loss/train': '0.58951', 'examples_per_second': '47.173', 'grad_norm': '28.249', 'counters/examples': 45504, 'counters/updates': 1422}
skipping logging after 45536 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '-1.0775', 'rewards_train/rejected': '-1.4119', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33442', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-121.52', 'loss/train': '0.61056', 'examples_per_second': '46.674', 'grad_norm': '23.776', 'counters/examples': 45568, 'counters/updates': 1424}
skipping logging after 45600 examples to avoid logging too frequently
train stats after 45632 examples: {'rewards_train/chosen': '-1.0851', 'rewards_train/rejected': '-1.7103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.62516', 'logps_train/rejected': '-106', 'logps_train/chosen': '-128.38', 'loss/train': '0.54048', 'examples_per_second': '49.231', 'grad_norm': '22.606', 'counters/examples': 45632, 'counters/updates': 1426}
skipping logging after 45664 examples to avoid logging too frequently
train stats after 45696 examples: {'rewards_train/chosen': '-0.92675', 'rewards_train/rejected': '-1.6881', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.76138', 'logps_train/rejected': '-137.67', 'logps_train/chosen': '-126.91', 'loss/train': '0.50359', 'examples_per_second': '46.429', 'grad_norm': '22.78', 'counters/examples': 45696, 'counters/updates': 1428}
skipping logging after 45728 examples to avoid logging too frequently
train stats after 45760 examples: {'rewards_train/chosen': '-0.78991', 'rewards_train/rejected': '-1.4845', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.69458', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-161.24', 'loss/train': '0.50658', 'examples_per_second': '44.624', 'grad_norm': '23.636', 'counters/examples': 45760, 'counters/updates': 1430}
skipping logging after 45792 examples to avoid logging too frequently
train stats after 45824 examples: {'rewards_train/chosen': '-1.0504', 'rewards_train/rejected': '-1.5523', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.50194', 'logps_train/rejected': '-139.8', 'logps_train/chosen': '-142.96', 'loss/train': '0.62783', 'examples_per_second': '45.734', 'grad_norm': '24.696', 'counters/examples': 45824, 'counters/updates': 1432}
skipping logging after 45856 examples to avoid logging too frequently
train stats after 45888 examples: {'rewards_train/chosen': '-1.1332', 'rewards_train/rejected': '-2.0328', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.89958', 'logps_train/rejected': '-152.12', 'logps_train/chosen': '-154.75', 'loss/train': '0.55617', 'examples_per_second': '44.4', 'grad_norm': '28.65', 'counters/examples': 45888, 'counters/updates': 1434}
skipping logging after 45920 examples to avoid logging too frequently
train stats after 45952 examples: {'rewards_train/chosen': '-0.95458', 'rewards_train/rejected': '-1.6483', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6937', 'logps_train/rejected': '-108.59', 'logps_train/chosen': '-113.76', 'loss/train': '0.51226', 'examples_per_second': '47.131', 'grad_norm': '19.779', 'counters/examples': 45952, 'counters/updates': 1436}
skipping logging after 45984 examples to avoid logging too frequently
train stats after 46016 examples: {'rewards_train/chosen': '-0.97364', 'rewards_train/rejected': '-1.3141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34046', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-157.42', 'loss/train': '0.62366', 'examples_per_second': '45.99', 'grad_norm': '27.337', 'counters/examples': 46016, 'counters/updates': 1438}
skipping logging after 46048 examples to avoid logging too frequently
train stats after 46080 examples: {'rewards_train/chosen': '-1.2335', 'rewards_train/rejected': '-1.3848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15124', 'logps_train/rejected': '-154.9', 'logps_train/chosen': '-148.76', 'loss/train': '0.75453', 'examples_per_second': '46.903', 'grad_norm': '27.208', 'counters/examples': 46080, 'counters/updates': 1440}
skipping logging after 46112 examples to avoid logging too frequently
train stats after 46144 examples: {'rewards_train/chosen': '-1.2563', 'rewards_train/rejected': '-1.4752', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21881', 'logps_train/rejected': '-149.3', 'logps_train/chosen': '-185.63', 'loss/train': '0.77047', 'examples_per_second': '45.717', 'grad_norm': '35.854', 'counters/examples': 46144, 'counters/updates': 1442}
skipping logging after 46176 examples to avoid logging too frequently
train stats after 46208 examples: {'rewards_train/chosen': '-1.1214', 'rewards_train/rejected': '-1.7937', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.6723', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-159.79', 'loss/train': '0.52089', 'examples_per_second': '32.688', 'grad_norm': '22.934', 'counters/examples': 46208, 'counters/updates': 1444}
skipping logging after 46240 examples to avoid logging too frequently
train stats after 46272 examples: {'rewards_train/chosen': '-0.93272', 'rewards_train/rejected': '-1.3684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43567', 'logps_train/rejected': '-153.97', 'logps_train/chosen': '-155.89', 'loss/train': '0.63402', 'examples_per_second': '45.517', 'grad_norm': '32.572', 'counters/examples': 46272, 'counters/updates': 1446}
skipping logging after 46304 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '-0.9921', 'rewards_train/rejected': '-1.1947', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20255', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-152.98', 'loss/train': '0.71476', 'examples_per_second': '45.739', 'grad_norm': '28.083', 'counters/examples': 46336, 'counters/updates': 1448}
skipping logging after 46368 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': '-0.77105', 'rewards_train/rejected': '-1.6843', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.9132', 'logps_train/rejected': '-199.91', 'logps_train/chosen': '-152.96', 'loss/train': '0.50252', 'examples_per_second': '46.809', 'grad_norm': '24.254', 'counters/examples': 46400, 'counters/updates': 1450}
skipping logging after 46432 examples to avoid logging too frequently
train stats after 46464 examples: {'rewards_train/chosen': '-0.90992', 'rewards_train/rejected': '-1.3517', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44178', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-115.92', 'loss/train': '0.59266', 'examples_per_second': '46.401', 'grad_norm': '24.794', 'counters/examples': 46464, 'counters/updates': 1452}
skipping logging after 46496 examples to avoid logging too frequently
train stats after 46528 examples: {'rewards_train/chosen': '-1.1801', 'rewards_train/rejected': '-1.76', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57989', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-149.92', 'loss/train': '0.56181', 'examples_per_second': '52.73', 'grad_norm': '23.619', 'counters/examples': 46528, 'counters/updates': 1454}
skipping logging after 46560 examples to avoid logging too frequently
train stats after 46592 examples: {'rewards_train/chosen': '-1.1462', 'rewards_train/rejected': '-1.7295', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58334', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-153.55', 'loss/train': '0.55224', 'examples_per_second': '45.634', 'grad_norm': '24.596', 'counters/examples': 46592, 'counters/updates': 1456}
skipping logging after 46624 examples to avoid logging too frequently
train stats after 46656 examples: {'rewards_train/chosen': '-1.0829', 'rewards_train/rejected': '-1.7533', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.67042', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-124.39', 'loss/train': '0.5125', 'examples_per_second': '45.262', 'grad_norm': '18.825', 'counters/examples': 46656, 'counters/updates': 1458}
skipping logging after 46688 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '-0.66928', 'rewards_train/rejected': '-1.1745', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50524', 'logps_train/rejected': '-147.3', 'logps_train/chosen': '-186.68', 'loss/train': '0.56115', 'examples_per_second': '44.708', 'grad_norm': '26.884', 'counters/examples': 46720, 'counters/updates': 1460}
skipping logging after 46752 examples to avoid logging too frequently
train stats after 46784 examples: {'rewards_train/chosen': '-1.2143', 'rewards_train/rejected': '-1.953', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.73877', 'logps_train/rejected': '-180.12', 'logps_train/chosen': '-155.15', 'loss/train': '0.49568', 'examples_per_second': '46.801', 'grad_norm': '25.735', 'counters/examples': 46784, 'counters/updates': 1462}
skipping logging after 46816 examples to avoid logging too frequently
train stats after 46848 examples: {'rewards_train/chosen': '-1.2657', 'rewards_train/rejected': '-1.8357', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56995', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-149.1', 'loss/train': '0.61876', 'examples_per_second': '52.134', 'grad_norm': '24.628', 'counters/examples': 46848, 'counters/updates': 1464}
skipping logging after 46880 examples to avoid logging too frequently
train stats after 46912 examples: {'rewards_train/chosen': '-1.2171', 'rewards_train/rejected': '-1.6842', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46711', 'logps_train/rejected': '-154.69', 'logps_train/chosen': '-187.31', 'loss/train': '0.62351', 'examples_per_second': '45.567', 'grad_norm': '30.281', 'counters/examples': 46912, 'counters/updates': 1466}
skipping logging after 46944 examples to avoid logging too frequently
train stats after 46976 examples: {'rewards_train/chosen': '-0.8735', 'rewards_train/rejected': '-1.4202', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54671', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-152.79', 'loss/train': '0.59789', 'examples_per_second': '46.455', 'grad_norm': '28.675', 'counters/examples': 46976, 'counters/updates': 1468}
skipping logging after 47008 examples to avoid logging too frequently
train stats after 47040 examples: {'rewards_train/chosen': '-1.0281', 'rewards_train/rejected': '-1.3559', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32778', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-140.18', 'loss/train': '0.68241', 'examples_per_second': '44.281', 'grad_norm': '26.62', 'counters/examples': 47040, 'counters/updates': 1470}
skipping logging after 47072 examples to avoid logging too frequently
train stats after 47104 examples: {'rewards_train/chosen': '-0.87241', 'rewards_train/rejected': '-1.5259', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.65353', 'logps_train/rejected': '-144.45', 'logps_train/chosen': '-160.34', 'loss/train': '0.58451', 'examples_per_second': '45.853', 'grad_norm': '24.716', 'counters/examples': 47104, 'counters/updates': 1472}
skipping logging after 47136 examples to avoid logging too frequently
train stats after 47168 examples: {'rewards_train/chosen': '-0.81157', 'rewards_train/rejected': '-1.5205', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70896', 'logps_train/rejected': '-145.98', 'logps_train/chosen': '-136.9', 'loss/train': '0.5307', 'examples_per_second': '46.949', 'grad_norm': '21.975', 'counters/examples': 47168, 'counters/updates': 1474}
skipping logging after 47200 examples to avoid logging too frequently
train stats after 47232 examples: {'rewards_train/chosen': '-0.66556', 'rewards_train/rejected': '-1.3997', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.73414', 'logps_train/rejected': '-130.35', 'logps_train/chosen': '-130.88', 'loss/train': '0.51253', 'examples_per_second': '48.116', 'grad_norm': '20.586', 'counters/examples': 47232, 'counters/updates': 1476}
skipping logging after 47264 examples to avoid logging too frequently
train stats after 47296 examples: {'rewards_train/chosen': '-0.80665', 'rewards_train/rejected': '-1.4145', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60783', 'logps_train/rejected': '-146.18', 'logps_train/chosen': '-176.41', 'loss/train': '0.63882', 'examples_per_second': '47.838', 'grad_norm': '29.059', 'counters/examples': 47296, 'counters/updates': 1478}
skipping logging after 47328 examples to avoid logging too frequently
train stats after 47360 examples: {'rewards_train/chosen': '-0.75236', 'rewards_train/rejected': '-1.3526', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60028', 'logps_train/rejected': '-121.99', 'logps_train/chosen': '-148.21', 'loss/train': '0.57164', 'examples_per_second': '47.688', 'grad_norm': '22.47', 'counters/examples': 47360, 'counters/updates': 1480}
skipping logging after 47392 examples to avoid logging too frequently
train stats after 47424 examples: {'rewards_train/chosen': '-0.83145', 'rewards_train/rejected': '-1.235', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.40352', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-145.3', 'loss/train': '0.5875', 'examples_per_second': '47.219', 'grad_norm': '25.148', 'counters/examples': 47424, 'counters/updates': 1482}
skipping logging after 47456 examples to avoid logging too frequently
train stats after 47488 examples: {'rewards_train/chosen': '-0.78533', 'rewards_train/rejected': '-1.0404', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25505', 'logps_train/rejected': '-150.6', 'logps_train/chosen': '-167.95', 'loss/train': '0.71434', 'examples_per_second': '45.453', 'grad_norm': '29.998', 'counters/examples': 47488, 'counters/updates': 1484}
skipping logging after 47520 examples to avoid logging too frequently
train stats after 47552 examples: {'rewards_train/chosen': '-0.70139', 'rewards_train/rejected': '-1.2512', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54979', 'logps_train/rejected': '-133.01', 'logps_train/chosen': '-145.78', 'loss/train': '0.62271', 'examples_per_second': '45.561', 'grad_norm': '24.634', 'counters/examples': 47552, 'counters/updates': 1486}
skipping logging after 47584 examples to avoid logging too frequently
train stats after 47616 examples: {'rewards_train/chosen': '-0.8251', 'rewards_train/rejected': '-1.4233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.59823', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-156.71', 'loss/train': '0.53217', 'examples_per_second': '47.132', 'grad_norm': '23.836', 'counters/examples': 47616, 'counters/updates': 1488}
skipping logging after 47648 examples to avoid logging too frequently
train stats after 47680 examples: {'rewards_train/chosen': '-0.86756', 'rewards_train/rejected': '-1.3604', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.49284', 'logps_train/rejected': '-132.15', 'logps_train/chosen': '-153.31', 'loss/train': '0.58293', 'examples_per_second': '47.543', 'grad_norm': '22.957', 'counters/examples': 47680, 'counters/updates': 1490}
skipping logging after 47712 examples to avoid logging too frequently
train stats after 47744 examples: {'rewards_train/chosen': '-0.72083', 'rewards_train/rejected': '-1.3779', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.65703', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-124.76', 'loss/train': '0.48187', 'examples_per_second': '45.762', 'grad_norm': '19.345', 'counters/examples': 47744, 'counters/updates': 1492}
skipping logging after 47776 examples to avoid logging too frequently
train stats after 47808 examples: {'rewards_train/chosen': '-0.8161', 'rewards_train/rejected': '-1.3359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51982', 'logps_train/rejected': '-179.25', 'logps_train/chosen': '-148.74', 'loss/train': '0.63606', 'examples_per_second': '44.413', 'grad_norm': '27.178', 'counters/examples': 47808, 'counters/updates': 1494}
skipping logging after 47840 examples to avoid logging too frequently
train stats after 47872 examples: {'rewards_train/chosen': '-1.0327', 'rewards_train/rejected': '-1.4896', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45688', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-125.01', 'loss/train': '0.56581', 'examples_per_second': '47.966', 'grad_norm': '23.183', 'counters/examples': 47872, 'counters/updates': 1496}
skipping logging after 47904 examples to avoid logging too frequently
train stats after 47936 examples: {'rewards_train/chosen': '-0.7001', 'rewards_train/rejected': '-1.2667', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.56665', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-136.43', 'loss/train': '0.55517', 'examples_per_second': '45.52', 'grad_norm': '20.78', 'counters/examples': 47936, 'counters/updates': 1498}
skipping logging after 47968 examples to avoid logging too frequently
train stats after 48000 examples: {'rewards_train/chosen': '-0.70618', 'rewards_train/rejected': '-1.6093', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.9031', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-157.15', 'loss/train': '0.47994', 'examples_per_second': '48.622', 'grad_norm': '20.268', 'counters/examples': 48000, 'counters/updates': 1500}
Running evaluation after 48000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.22it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.86it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  7.00it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  7.01it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.91it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.35it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.20it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.12it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.14it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.03it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.04it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  7.04it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.03it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.93it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.99it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.90it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.02it/s]
eval after 48000: {'rewards_eval/chosen': '-0.97466', 'rewards_eval/rejected': '-1.5797', 'rewards_eval/accuracies': '0.70312', 'rewards_eval/margins': '0.60502', 'logps_eval/rejected': '-134.08', 'logps_eval/chosen': '-146.2', 'loss/eval': '0.59259'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-48000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-48000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-48000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-48000/scheduler.pt...
train stats after 48032 examples: {'rewards_train/chosen': '-1.0416', 'rewards_train/rejected': '-1.7487', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.70708', 'logps_train/rejected': '-146.2', 'logps_train/chosen': '-133.05', 'loss/train': '0.55323', 'examples_per_second': '34.893', 'grad_norm': '25.267', 'counters/examples': 48032, 'counters/updates': 1501}
skipping logging after 48064 examples to avoid logging too frequently
train stats after 48096 examples: {'rewards_train/chosen': '-1.0279', 'rewards_train/rejected': '-1.4229', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39499', 'logps_train/rejected': '-134.63', 'logps_train/chosen': '-127.45', 'loss/train': '0.68047', 'examples_per_second': '53.369', 'grad_norm': '25.452', 'counters/examples': 48096, 'counters/updates': 1503}
skipping logging after 48128 examples to avoid logging too frequently
train stats after 48160 examples: {'rewards_train/chosen': '-1.0798', 'rewards_train/rejected': '-1.6927', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.61298', 'logps_train/rejected': '-118.28', 'logps_train/chosen': '-140.91', 'loss/train': '0.52649', 'examples_per_second': '44.875', 'grad_norm': '21.706', 'counters/examples': 48160, 'counters/updates': 1505}
skipping logging after 48192 examples to avoid logging too frequently
train stats after 48224 examples: {'rewards_train/chosen': '-1.1113', 'rewards_train/rejected': '-1.758', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.64673', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-118.67', 'loss/train': '0.50987', 'examples_per_second': '45.282', 'grad_norm': '19.051', 'counters/examples': 48224, 'counters/updates': 1507}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48288 examples: {'rewards_train/chosen': '-1.284', 'rewards_train/rejected': '-1.5297', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.2457', 'logps_train/rejected': '-143.51', 'logps_train/chosen': '-136.01', 'loss/train': '0.7501', 'examples_per_second': '45.348', 'grad_norm': '30.274', 'counters/examples': 48288, 'counters/updates': 1509}
skipping logging after 48320 examples to avoid logging too frequently
train stats after 48352 examples: {'rewards_train/chosen': '-0.81482', 'rewards_train/rejected': '-1.8646', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.0498', 'logps_train/rejected': '-92.808', 'logps_train/chosen': '-137.88', 'loss/train': '0.4364', 'examples_per_second': '45.228', 'grad_norm': '18.959', 'counters/examples': 48352, 'counters/updates': 1511}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48416 examples: {'rewards_train/chosen': '-0.7758', 'rewards_train/rejected': '-1.1363', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36048', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-127.52', 'loss/train': '0.64286', 'examples_per_second': '46.455', 'grad_norm': '26.163', 'counters/examples': 48416, 'counters/updates': 1513}
skipping logging after 48448 examples to avoid logging too frequently
train stats after 48480 examples: {'rewards_train/chosen': '-0.54913', 'rewards_train/rejected': '-0.92255', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37342', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-165.71', 'loss/train': '0.65143', 'examples_per_second': '45.614', 'grad_norm': '29.292', 'counters/examples': 48480, 'counters/updates': 1515}
skipping logging after 48512 examples to avoid logging too frequently
train stats after 48544 examples: {'rewards_train/chosen': '-0.80242', 'rewards_train/rejected': '-1.7469', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.94453', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-138.86', 'loss/train': '0.50315', 'examples_per_second': '47.048', 'grad_norm': '22.287', 'counters/examples': 48544, 'counters/updates': 1517}
skipping logging after 48576 examples to avoid logging too frequently
train stats after 48608 examples: {'rewards_train/chosen': '-0.57205', 'rewards_train/rejected': '-1.4192', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.84712', 'logps_train/rejected': '-161.3', 'logps_train/chosen': '-145.25', 'loss/train': '0.47275', 'examples_per_second': '49.77', 'grad_norm': '22.961', 'counters/examples': 48608, 'counters/updates': 1519}
skipping logging after 48640 examples to avoid logging too frequently
train stats after 48672 examples: {'rewards_train/chosen': '-0.40775', 'rewards_train/rejected': '-1.0788', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.67104', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-134.18', 'loss/train': '0.55265', 'examples_per_second': '45.489', 'grad_norm': '26.975', 'counters/examples': 48672, 'counters/updates': 1521}
skipping logging after 48704 examples to avoid logging too frequently
train stats after 48736 examples: {'rewards_train/chosen': '-0.6954', 'rewards_train/rejected': '-1.6018', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.90644', 'logps_train/rejected': '-107.62', 'logps_train/chosen': '-131.86', 'loss/train': '0.42263', 'examples_per_second': '47.494', 'grad_norm': '18.171', 'counters/examples': 48736, 'counters/updates': 1523}
skipping logging after 48768 examples to avoid logging too frequently
train stats after 48800 examples: {'rewards_train/chosen': '-1.019', 'rewards_train/rejected': '-1.6437', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.62471', 'logps_train/rejected': '-158.14', 'logps_train/chosen': '-171.9', 'loss/train': '0.66525', 'examples_per_second': '45.598', 'grad_norm': '32.817', 'counters/examples': 48800, 'counters/updates': 1525}
skipping logging after 48832 examples to avoid logging too frequently
train stats after 48864 examples: {'rewards_train/chosen': '-0.55634', 'rewards_train/rejected': '-1.059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50265', 'logps_train/rejected': '-145.7', 'logps_train/chosen': '-137.71', 'loss/train': '0.5969', 'examples_per_second': '46.035', 'grad_norm': '24.673', 'counters/examples': 48864, 'counters/updates': 1527}
skipping logging after 48896 examples to avoid logging too frequently
train stats after 48928 examples: {'rewards_train/chosen': '-0.44673', 'rewards_train/rejected': '-0.84111', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39438', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-174.47', 'loss/train': '0.59124', 'examples_per_second': '45.369', 'grad_norm': '30.055', 'counters/examples': 48928, 'counters/updates': 1529}
skipping logging after 48960 examples to avoid logging too frequently
train stats after 48992 examples: {'rewards_train/chosen': '-1.0397', 'rewards_train/rejected': '-1.0408', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0010475', 'logps_train/rejected': '-147.02', 'logps_train/chosen': '-114.71', 'loss/train': '0.83519', 'examples_per_second': '45.663', 'grad_norm': '30.527', 'counters/examples': 48992, 'counters/updates': 1531}
skipping logging after 49024 examples to avoid logging too frequently
train stats after 49056 examples: {'rewards_train/chosen': '-0.56142', 'rewards_train/rejected': '-1.1776', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.61623', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-117.75', 'loss/train': '0.49356', 'examples_per_second': '48.519', 'grad_norm': '20.014', 'counters/examples': 49056, 'counters/updates': 1533}
skipping logging after 49088 examples to avoid logging too frequently
train stats after 49120 examples: {'rewards_train/chosen': '-0.65128', 'rewards_train/rejected': '-1.2101', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.55885', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-146.33', 'loss/train': '0.54832', 'examples_per_second': '46.261', 'grad_norm': '24.031', 'counters/examples': 49120, 'counters/updates': 1535}
skipping logging after 49152 examples to avoid logging too frequently
train stats after 49184 examples: {'rewards_train/chosen': '-0.69553', 'rewards_train/rejected': '-1.1117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41615', 'logps_train/rejected': '-167.53', 'logps_train/chosen': '-177.29', 'loss/train': '0.65898', 'examples_per_second': '45.526', 'grad_norm': '27.595', 'counters/examples': 49184, 'counters/updates': 1537}
skipping logging after 49216 examples to avoid logging too frequently
train stats after 49248 examples: {'rewards_train/chosen': '-0.80157', 'rewards_train/rejected': '-1.421', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.61941', 'logps_train/rejected': '-107', 'logps_train/chosen': '-160.13', 'loss/train': '0.53694', 'examples_per_second': '46.714', 'grad_norm': '23.747', 'counters/examples': 49248, 'counters/updates': 1539}
skipping logging after 49280 examples to avoid logging too frequently
train stats after 49312 examples: {'rewards_train/chosen': '-1.0864', 'rewards_train/rejected': '-1.443', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35661', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-146.91', 'loss/train': '0.69222', 'examples_per_second': '45.554', 'grad_norm': '26.73', 'counters/examples': 49312, 'counters/updates': 1541}
skipping logging after 49344 examples to avoid logging too frequently
train stats after 49376 examples: {'rewards_train/chosen': '-0.87326', 'rewards_train/rejected': '-0.96469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091435', 'logps_train/rejected': '-157.3', 'logps_train/chosen': '-167.44', 'loss/train': '0.7724', 'examples_per_second': '46.501', 'grad_norm': '33.287', 'counters/examples': 49376, 'counters/updates': 1543}
skipping logging after 49408 examples to avoid logging too frequently
train stats after 49440 examples: {'rewards_train/chosen': '-1.3075', 'rewards_train/rejected': '-1.7405', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43295', 'logps_train/rejected': '-120.56', 'logps_train/chosen': '-146.47', 'loss/train': '0.5804', 'examples_per_second': '46.746', 'grad_norm': '23.893', 'counters/examples': 49440, 'counters/updates': 1545}
skipping logging after 49472 examples to avoid logging too frequently
train stats after 49504 examples: {'rewards_train/chosen': '-1.2043', 'rewards_train/rejected': '-1.2625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058238', 'logps_train/rejected': '-186.72', 'logps_train/chosen': '-136.06', 'loss/train': '0.76493', 'examples_per_second': '45.57', 'grad_norm': '33.08', 'counters/examples': 49504, 'counters/updates': 1547}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49568 examples: {'rewards_train/chosen': '-0.75706', 'rewards_train/rejected': '-1.3692', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61213', 'logps_train/rejected': '-158.92', 'logps_train/chosen': '-157.65', 'loss/train': '0.50947', 'examples_per_second': '45.634', 'grad_norm': '23.122', 'counters/examples': 49568, 'counters/updates': 1549}
skipping logging after 49600 examples to avoid logging too frequently
train stats after 49632 examples: {'rewards_train/chosen': '-0.44808', 'rewards_train/rejected': '-1.3535', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.90542', 'logps_train/rejected': '-137.12', 'logps_train/chosen': '-154.69', 'loss/train': '0.43721', 'examples_per_second': '52.393', 'grad_norm': '19.944', 'counters/examples': 49632, 'counters/updates': 1551}
skipping logging after 49664 examples to avoid logging too frequently
train stats after 49696 examples: {'rewards_train/chosen': '-0.89893', 'rewards_train/rejected': '-1.2819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38294', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-143.53', 'loss/train': '0.63754', 'examples_per_second': '52.5', 'grad_norm': '28.398', 'counters/examples': 49696, 'counters/updates': 1553}
train stats after 49728 examples: {'rewards_train/chosen': '-0.99367', 'rewards_train/rejected': '-1.433', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43931', 'logps_train/rejected': '-150.35', 'logps_train/chosen': '-130.41', 'loss/train': '0.65316', 'examples_per_second': '28.761', 'grad_norm': '28.307', 'counters/examples': 49728, 'counters/updates': 1554}
skipping logging after 49760 examples to avoid logging too frequently
train stats after 49792 examples: {'rewards_train/chosen': '-0.69247', 'rewards_train/rejected': '-1.3901', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.69762', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-167.57', 'loss/train': '0.53214', 'examples_per_second': '45.672', 'grad_norm': '22.799', 'counters/examples': 49792, 'counters/updates': 1556}
skipping logging after 49824 examples to avoid logging too frequently
train stats after 49856 examples: {'rewards_train/chosen': '-0.88562', 'rewards_train/rejected': '-1.0738', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.18814', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-132.58', 'loss/train': '0.75048', 'examples_per_second': '47.688', 'grad_norm': '26.639', 'counters/examples': 49856, 'counters/updates': 1558}
skipping logging after 49888 examples to avoid logging too frequently
train stats after 49920 examples: {'rewards_train/chosen': '-0.5009', 'rewards_train/rejected': '-1.1687', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.66779', 'logps_train/rejected': '-116.79', 'logps_train/chosen': '-153.7', 'loss/train': '0.48673', 'examples_per_second': '44.281', 'grad_norm': '22.561', 'counters/examples': 49920, 'counters/updates': 1560}
skipping logging after 49952 examples to avoid logging too frequently
train stats after 49984 examples: {'rewards_train/chosen': '-0.73328', 'rewards_train/rejected': '-1.3408', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60753', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-136.96', 'loss/train': '0.57556', 'examples_per_second': '45.617', 'grad_norm': '22.775', 'counters/examples': 49984, 'counters/updates': 1562}
skipping logging after 50016 examples to avoid logging too frequently
train stats after 50048 examples: {'rewards_train/chosen': '-0.82193', 'rewards_train/rejected': '-1.0541', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23214', 'logps_train/rejected': '-144.83', 'logps_train/chosen': '-147.75', 'loss/train': '0.67417', 'examples_per_second': '45.529', 'grad_norm': '25.767', 'counters/examples': 50048, 'counters/updates': 1564}
skipping logging after 50080 examples to avoid logging too frequently
train stats after 50112 examples: {'rewards_train/chosen': '-1.0506', 'rewards_train/rejected': '-1.2286', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17799', 'logps_train/rejected': '-150.15', 'logps_train/chosen': '-169.75', 'loss/train': '0.78905', 'examples_per_second': '44.257', 'grad_norm': '35.275', 'counters/examples': 50112, 'counters/updates': 1566}
skipping logging after 50144 examples to avoid logging too frequently
train stats after 50176 examples: {'rewards_train/chosen': '-0.53765', 'rewards_train/rejected': '-0.9838', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.44615', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-126.95', 'loss/train': '0.55902', 'examples_per_second': '46.96', 'grad_norm': '20.977', 'counters/examples': 50176, 'counters/updates': 1568}
skipping logging after 50208 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '-0.97893', 'rewards_train/rejected': '-1.4919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.51297', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-130.98', 'loss/train': '0.59452', 'examples_per_second': '46.198', 'grad_norm': '25.203', 'counters/examples': 50240, 'counters/updates': 1570}
skipping logging after 50272 examples to avoid logging too frequently
train stats after 50304 examples: {'rewards_train/chosen': '-0.70784', 'rewards_train/rejected': '-1.2403', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.53243', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-155.25', 'loss/train': '0.53408', 'examples_per_second': '44.812', 'grad_norm': '22.628', 'counters/examples': 50304, 'counters/updates': 1572}
skipping logging after 50336 examples to avoid logging too frequently
train stats after 50368 examples: {'rewards_train/chosen': '-1.0688', 'rewards_train/rejected': '-1.7682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69941', 'logps_train/rejected': '-105.46', 'logps_train/chosen': '-125.85', 'loss/train': '0.55633', 'examples_per_second': '53.668', 'grad_norm': '21.911', 'counters/examples': 50368, 'counters/updates': 1574}
skipping logging after 50400 examples to avoid logging too frequently
train stats after 50432 examples: {'rewards_train/chosen': '-0.48962', 'rewards_train/rejected': '-1.2212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.73155', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-149.87', 'loss/train': '0.55465', 'examples_per_second': '45.522', 'grad_norm': '21.251', 'counters/examples': 50432, 'counters/updates': 1576}
skipping logging after 50464 examples to avoid logging too frequently
train stats after 50496 examples: {'rewards_train/chosen': '-0.75335', 'rewards_train/rejected': '-1.3969', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.6435', 'logps_train/rejected': '-175.73', 'logps_train/chosen': '-171.02', 'loss/train': '0.53969', 'examples_per_second': '45.754', 'grad_norm': '23.738', 'counters/examples': 50496, 'counters/updates': 1578}
skipping logging after 50528 examples to avoid logging too frequently
train stats after 50560 examples: {'rewards_train/chosen': '-0.6449', 'rewards_train/rejected': '-1.3459', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70098', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-106.44', 'loss/train': '0.50962', 'examples_per_second': '46.426', 'grad_norm': '21.251', 'counters/examples': 50560, 'counters/updates': 1580}
skipping logging after 50592 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '-0.82904', 'rewards_train/rejected': '-1.5513', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.72222', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-122.94', 'loss/train': '0.55765', 'examples_per_second': '47.077', 'grad_norm': '23.124', 'counters/examples': 50624, 'counters/updates': 1582}
skipping logging after 50656 examples to avoid logging too frequently
train stats after 50688 examples: {'rewards_train/chosen': '-0.43902', 'rewards_train/rejected': '-1.223', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.78394', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-139.1', 'loss/train': '0.45737', 'examples_per_second': '45.731', 'grad_norm': '19.697', 'counters/examples': 50688, 'counters/updates': 1584}
skipping logging after 50720 examples to avoid logging too frequently
train stats after 50752 examples: {'rewards_train/chosen': '-0.5478', 'rewards_train/rejected': '-1.0078', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46003', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-124.97', 'loss/train': '0.60381', 'examples_per_second': '45.6', 'grad_norm': '24.38', 'counters/examples': 50752, 'counters/updates': 1586}
skipping logging after 50784 examples to avoid logging too frequently
train stats after 50816 examples: {'rewards_train/chosen': '-0.50317', 'rewards_train/rejected': '-1.0672', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.564', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-154.39', 'loss/train': '0.55712', 'examples_per_second': '45.1', 'grad_norm': '24.995', 'counters/examples': 50816, 'counters/updates': 1588}
skipping logging after 50848 examples to avoid logging too frequently
train stats after 50880 examples: {'rewards_train/chosen': '-1.0149', 'rewards_train/rejected': '-1.3262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31131', 'logps_train/rejected': '-144.94', 'logps_train/chosen': '-144.56', 'loss/train': '0.67595', 'examples_per_second': '44.969', 'grad_norm': '28.051', 'counters/examples': 50880, 'counters/updates': 1590}
skipping logging after 50912 examples to avoid logging too frequently
train stats after 50944 examples: {'rewards_train/chosen': '-0.46993', 'rewards_train/rejected': '-1.1125', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.64253', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-133.44', 'loss/train': '0.53636', 'examples_per_second': '47.214', 'grad_norm': '22.677', 'counters/examples': 50944, 'counters/updates': 1592}
skipping logging after 50976 examples to avoid logging too frequently
train stats after 51008 examples: {'rewards_train/chosen': '-0.98377', 'rewards_train/rejected': '-1.5423', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55849', 'logps_train/rejected': '-164.62', 'logps_train/chosen': '-116.72', 'loss/train': '0.56112', 'examples_per_second': '53.718', 'grad_norm': '22.538', 'counters/examples': 51008, 'counters/updates': 1594}
skipping logging after 51040 examples to avoid logging too frequently
train stats after 51072 examples: {'rewards_train/chosen': '-0.79263', 'rewards_train/rejected': '-1.2565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4639', 'logps_train/rejected': '-153.07', 'logps_train/chosen': '-148.47', 'loss/train': '0.6806', 'examples_per_second': '45.219', 'grad_norm': '29.732', 'counters/examples': 51072, 'counters/updates': 1596}
skipping logging after 51104 examples to avoid logging too frequently
train stats after 51136 examples: {'rewards_train/chosen': '-0.81167', 'rewards_train/rejected': '-1.3872', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57553', 'logps_train/rejected': '-103.34', 'logps_train/chosen': '-117.18', 'loss/train': '0.60907', 'examples_per_second': '45.586', 'grad_norm': '22.821', 'counters/examples': 51136, 'counters/updates': 1598}
skipping logging after 51168 examples to avoid logging too frequently
train stats after 51200 examples: {'rewards_train/chosen': '-0.6901', 'rewards_train/rejected': '-1.3261', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63604', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-141.5', 'loss/train': '0.52977', 'examples_per_second': '44.568', 'grad_norm': '23.256', 'counters/examples': 51200, 'counters/updates': 1600}
skipping logging after 51232 examples to avoid logging too frequently
train stats after 51264 examples: {'rewards_train/chosen': '-0.54469', 'rewards_train/rejected': '-1.3474', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.80275', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-175.24', 'loss/train': '0.5076', 'examples_per_second': '52.201', 'grad_norm': '24.145', 'counters/examples': 51264, 'counters/updates': 1602}
skipping logging after 51296 examples to avoid logging too frequently
train stats after 51328 examples: {'rewards_train/chosen': '-1.0892', 'rewards_train/rejected': '-1.4329', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.34369', 'logps_train/rejected': '-155.2', 'logps_train/chosen': '-145.55', 'loss/train': '0.76128', 'examples_per_second': '44.284', 'grad_norm': '30.648', 'counters/examples': 51328, 'counters/updates': 1604}
skipping logging after 51360 examples to avoid logging too frequently
train stats after 51392 examples: {'rewards_train/chosen': '-0.87614', 'rewards_train/rejected': '-1.2474', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37125', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-134.66', 'loss/train': '0.67732', 'examples_per_second': '44.434', 'grad_norm': '24.823', 'counters/examples': 51392, 'counters/updates': 1606}
skipping logging after 51424 examples to avoid logging too frequently
train stats after 51456 examples: {'rewards_train/chosen': '-0.58789', 'rewards_train/rejected': '-1.2059', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61798', 'logps_train/rejected': '-136.24', 'logps_train/chosen': '-121.96', 'loss/train': '0.56748', 'examples_per_second': '47.518', 'grad_norm': '25.424', 'counters/examples': 51456, 'counters/updates': 1608}
skipping logging after 51488 examples to avoid logging too frequently
train stats after 51520 examples: {'rewards_train/chosen': '-0.87216', 'rewards_train/rejected': '-1.2312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35904', 'logps_train/rejected': '-163.98', 'logps_train/chosen': '-137.41', 'loss/train': '0.61276', 'examples_per_second': '45.492', 'grad_norm': '26.969', 'counters/examples': 51520, 'counters/updates': 1610}
skipping logging after 51552 examples to avoid logging too frequently
train stats after 51584 examples: {'rewards_train/chosen': '-0.77721', 'rewards_train/rejected': '-1.3323', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55513', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-138.69', 'loss/train': '0.56148', 'examples_per_second': '45.994', 'grad_norm': '24.718', 'counters/examples': 51584, 'counters/updates': 1612}
skipping logging after 51616 examples to avoid logging too frequently
train stats after 51648 examples: {'rewards_train/chosen': '-0.50815', 'rewards_train/rejected': '-1.1245', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61633', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-148.04', 'loss/train': '0.54091', 'examples_per_second': '45.466', 'grad_norm': '23.635', 'counters/examples': 51648, 'counters/updates': 1614}
skipping logging after 51680 examples to avoid logging too frequently
train stats after 51712 examples: {'rewards_train/chosen': '-0.7861', 'rewards_train/rejected': '-1.2274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.44128', 'logps_train/rejected': '-128.19', 'logps_train/chosen': '-175.62', 'loss/train': '0.59653', 'examples_per_second': '45.031', 'grad_norm': '26.862', 'counters/examples': 51712, 'counters/updates': 1616}
skipping logging after 51744 examples to avoid logging too frequently
train stats after 51776 examples: {'rewards_train/chosen': '-0.71837', 'rewards_train/rejected': '-1.0568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.33839', 'logps_train/rejected': '-147.86', 'logps_train/chosen': '-125.97', 'loss/train': '0.6889', 'examples_per_second': '44.898', 'grad_norm': '26.846', 'counters/examples': 51776, 'counters/updates': 1618}
skipping logging after 51808 examples to avoid logging too frequently
train stats after 51840 examples: {'rewards_train/chosen': '-0.94942', 'rewards_train/rejected': '-1.5472', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59774', 'logps_train/rejected': '-114', 'logps_train/chosen': '-143.59', 'loss/train': '0.52977', 'examples_per_second': '45.521', 'grad_norm': '21.235', 'counters/examples': 51840, 'counters/updates': 1620}
skipping logging after 51872 examples to avoid logging too frequently
train stats after 51904 examples: {'rewards_train/chosen': '-0.81574', 'rewards_train/rejected': '-1.1241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30834', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-129.33', 'loss/train': '0.67584', 'examples_per_second': '48.014', 'grad_norm': '27.695', 'counters/examples': 51904, 'counters/updates': 1622}
skipping logging after 51936 examples to avoid logging too frequently
train stats after 51968 examples: {'rewards_train/chosen': '-0.85625', 'rewards_train/rejected': '-1.1792', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.32291', 'logps_train/rejected': '-148.9', 'logps_train/chosen': '-154.36', 'loss/train': '0.65756', 'examples_per_second': '44.155', 'grad_norm': '27.098', 'counters/examples': 51968, 'counters/updates': 1624}
skipping logging after 52000 examples to avoid logging too frequently
train stats after 52032 examples: {'rewards_train/chosen': '-0.78896', 'rewards_train/rejected': '-1.271', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48207', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-163.98', 'loss/train': '0.63592', 'examples_per_second': '49.184', 'grad_norm': '27.59', 'counters/examples': 52032, 'counters/updates': 1626}
skipping logging after 52064 examples to avoid logging too frequently
train stats after 52096 examples: {'rewards_train/chosen': '-0.69553', 'rewards_train/rejected': '-1.2267', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.53113', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-175.86', 'loss/train': '0.59931', 'examples_per_second': '45.437', 'grad_norm': '26.407', 'counters/examples': 52096, 'counters/updates': 1628}
skipping logging after 52128 examples to avoid logging too frequently
train stats after 52160 examples: {'rewards_train/chosen': '-0.38682', 'rewards_train/rejected': '-0.87762', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4908', 'logps_train/rejected': '-147.9', 'logps_train/chosen': '-135.21', 'loss/train': '0.60508', 'examples_per_second': '46.816', 'grad_norm': '25.443', 'counters/examples': 52160, 'counters/updates': 1630}
skipping logging after 52192 examples to avoid logging too frequently
train stats after 52224 examples: {'rewards_train/chosen': '-0.48873', 'rewards_train/rejected': '-1.1714', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.6827', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-132.54', 'loss/train': '0.50521', 'examples_per_second': '48.739', 'grad_norm': '21.402', 'counters/examples': 52224, 'counters/updates': 1632}
skipping logging after 52256 examples to avoid logging too frequently
train stats after 52288 examples: {'rewards_train/chosen': '-0.74617', 'rewards_train/rejected': '-1.2665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52035', 'logps_train/rejected': '-177', 'logps_train/chosen': '-174.45', 'loss/train': '0.60878', 'examples_per_second': '45.216', 'grad_norm': '28.549', 'counters/examples': 52288, 'counters/updates': 1634}
skipping logging after 52320 examples to avoid logging too frequently
train stats after 52352 examples: {'rewards_train/chosen': '-0.65851', 'rewards_train/rejected': '-1.0035', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34498', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-116.65', 'loss/train': '0.6931', 'examples_per_second': '45.573', 'grad_norm': '25.813', 'counters/examples': 52352, 'counters/updates': 1636}
skipping logging after 52384 examples to avoid logging too frequently
train stats after 52416 examples: {'rewards_train/chosen': '-0.62787', 'rewards_train/rejected': '-1.4068', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.77891', 'logps_train/rejected': '-152.81', 'logps_train/chosen': '-169.55', 'loss/train': '0.5167', 'examples_per_second': '48.228', 'grad_norm': '22.215', 'counters/examples': 52416, 'counters/updates': 1638}
skipping logging after 52448 examples to avoid logging too frequently
train stats after 52480 examples: {'rewards_train/chosen': '-0.53654', 'rewards_train/rejected': '-1.5448', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '1.0083', 'logps_train/rejected': '-154.46', 'logps_train/chosen': '-200.58', 'loss/train': '0.47793', 'examples_per_second': '45.784', 'grad_norm': '23.472', 'counters/examples': 52480, 'counters/updates': 1640}
skipping logging after 52512 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '-0.79385', 'rewards_train/rejected': '-1.6762', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.88238', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-94.683', 'loss/train': '0.51873', 'examples_per_second': '44.516', 'grad_norm': '19.808', 'counters/examples': 52544, 'counters/updates': 1642}
skipping logging after 52576 examples to avoid logging too frequently
train stats after 52608 examples: {'rewards_train/chosen': '-0.93908', 'rewards_train/rejected': '-1.6717', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.7326', 'logps_train/rejected': '-173.96', 'logps_train/chosen': '-148.43', 'loss/train': '0.54577', 'examples_per_second': '45.712', 'grad_norm': '26.987', 'counters/examples': 52608, 'counters/updates': 1644}
skipping logging after 52640 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '-0.42861', 'rewards_train/rejected': '-1.0741', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.64549', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-140.55', 'loss/train': '0.56177', 'examples_per_second': '44.537', 'grad_norm': '23.72', 'counters/examples': 52672, 'counters/updates': 1646}
skipping logging after 52704 examples to avoid logging too frequently
train stats after 52736 examples: {'rewards_train/chosen': '-0.63422', 'rewards_train/rejected': '-1.2886', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.65442', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-126.46', 'loss/train': '0.52916', 'examples_per_second': '44.349', 'grad_norm': '21.47', 'counters/examples': 52736, 'counters/updates': 1648}
skipping logging after 52768 examples to avoid logging too frequently
train stats after 52800 examples: {'rewards_train/chosen': '-0.57939', 'rewards_train/rejected': '-0.81522', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23583', 'logps_train/rejected': '-154.66', 'logps_train/chosen': '-158.62', 'loss/train': '0.69322', 'examples_per_second': '45.59', 'grad_norm': '30.191', 'counters/examples': 52800, 'counters/updates': 1650}
skipping logging after 52832 examples to avoid logging too frequently
train stats after 52864 examples: {'rewards_train/chosen': '-0.91767', 'rewards_train/rejected': '-1.2944', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37669', 'logps_train/rejected': '-139.2', 'logps_train/chosen': '-122.7', 'loss/train': '0.7351', 'examples_per_second': '48.431', 'grad_norm': '27.941', 'counters/examples': 52864, 'counters/updates': 1652}
skipping logging after 52896 examples to avoid logging too frequently
train stats after 52928 examples: {'rewards_train/chosen': '-0.86396', 'rewards_train/rejected': '-1.3545', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49049', 'logps_train/rejected': '-148.56', 'logps_train/chosen': '-140.94', 'loss/train': '0.79099', 'examples_per_second': '44.401', 'grad_norm': '55.62', 'counters/examples': 52928, 'counters/updates': 1654}
skipping logging after 52960 examples to avoid logging too frequently
train stats after 52992 examples: {'rewards_train/chosen': '-0.8104', 'rewards_train/rejected': '-1.2726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4622', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-221.02', 'loss/train': '0.61122', 'examples_per_second': '45.729', 'grad_norm': '30.986', 'counters/examples': 52992, 'counters/updates': 1656}
skipping logging after 53024 examples to avoid logging too frequently
train stats after 53056 examples: {'rewards_train/chosen': '-0.81942', 'rewards_train/rejected': '-1.238', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4186', 'logps_train/rejected': '-108.62', 'logps_train/chosen': '-132.1', 'loss/train': '0.61518', 'examples_per_second': '46.092', 'grad_norm': '24.772', 'counters/examples': 53056, 'counters/updates': 1658}
skipping logging after 53088 examples to avoid logging too frequently
train stats after 53120 examples: {'rewards_train/chosen': '-0.74275', 'rewards_train/rejected': '-1.5178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.7751', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-117.77', 'loss/train': '0.56272', 'examples_per_second': '52.377', 'grad_norm': '22.076', 'counters/examples': 53120, 'counters/updates': 1660}
skipping logging after 53152 examples to avoid logging too frequently
train stats after 53184 examples: {'rewards_train/chosen': '-0.9261', 'rewards_train/rejected': '-1.3059', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37982', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-120.86', 'loss/train': '0.67711', 'examples_per_second': '45.33', 'grad_norm': '24.613', 'counters/examples': 53184, 'counters/updates': 1662}
skipping logging after 53216 examples to avoid logging too frequently
train stats after 53248 examples: {'rewards_train/chosen': '-0.73875', 'rewards_train/rejected': '-1.1166', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.37783', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-144.93', 'loss/train': '0.65701', 'examples_per_second': '45.597', 'grad_norm': '26.141', 'counters/examples': 53248, 'counters/updates': 1664}
skipping logging after 53280 examples to avoid logging too frequently
train stats after 53312 examples: {'rewards_train/chosen': '-1.0036', 'rewards_train/rejected': '-1.7604', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.75683', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-149.63', 'loss/train': '0.49565', 'examples_per_second': '51.774', 'grad_norm': '23.288', 'counters/examples': 53312, 'counters/updates': 1666}
skipping logging after 53344 examples to avoid logging too frequently
train stats after 53376 examples: {'rewards_train/chosen': '-0.92324', 'rewards_train/rejected': '-1.4153', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49207', 'logps_train/rejected': '-123.9', 'logps_train/chosen': '-129.15', 'loss/train': '0.55661', 'examples_per_second': '46.417', 'grad_norm': '23.166', 'counters/examples': 53376, 'counters/updates': 1668}
skipping logging after 53408 examples to avoid logging too frequently
train stats after 53440 examples: {'rewards_train/chosen': '-1.013', 'rewards_train/rejected': '-1.5469', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53391', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-135.43', 'loss/train': '0.56381', 'examples_per_second': '45.513', 'grad_norm': '26.561', 'counters/examples': 53440, 'counters/updates': 1670}
skipping logging after 53472 examples to avoid logging too frequently
train stats after 53504 examples: {'rewards_train/chosen': '-0.86664', 'rewards_train/rejected': '-1.376', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.50939', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-126.4', 'loss/train': '0.61273', 'examples_per_second': '44.779', 'grad_norm': '24.51', 'counters/examples': 53504, 'counters/updates': 1672}
skipping logging after 53536 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '-0.96549', 'rewards_train/rejected': '-1.7276', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.76209', 'logps_train/rejected': '-140.12', 'logps_train/chosen': '-131.19', 'loss/train': '0.53341', 'examples_per_second': '47.75', 'grad_norm': '23.186', 'counters/examples': 53568, 'counters/updates': 1674}
skipping logging after 53600 examples to avoid logging too frequently
train stats after 53632 examples: {'rewards_train/chosen': '-0.5631', 'rewards_train/rejected': '-1.1361', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57296', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-115.16', 'loss/train': '0.55904', 'examples_per_second': '46.366', 'grad_norm': '20.382', 'counters/examples': 53632, 'counters/updates': 1676}
skipping logging after 53664 examples to avoid logging too frequently
train stats after 53696 examples: {'rewards_train/chosen': '-1.0053', 'rewards_train/rejected': '-1.2714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.26608', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-138.74', 'loss/train': '0.70215', 'examples_per_second': '45.279', 'grad_norm': '31.034', 'counters/examples': 53696, 'counters/updates': 1678}
skipping logging after 53728 examples to avoid logging too frequently
train stats after 53760 examples: {'rewards_train/chosen': '-1.053', 'rewards_train/rejected': '-1.5197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46668', 'logps_train/rejected': '-111.84', 'logps_train/chosen': '-168.52', 'loss/train': '0.60759', 'examples_per_second': '45.772', 'grad_norm': '27.59', 'counters/examples': 53760, 'counters/updates': 1680}
skipping logging after 53792 examples to avoid logging too frequently
train stats after 53824 examples: {'rewards_train/chosen': '-1.0687', 'rewards_train/rejected': '-1.797', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.72832', 'logps_train/rejected': '-168.07', 'logps_train/chosen': '-193.71', 'loss/train': '0.59062', 'examples_per_second': '45.529', 'grad_norm': '29.888', 'counters/examples': 53824, 'counters/updates': 1682}
skipping logging after 53856 examples to avoid logging too frequently
train stats after 53888 examples: {'rewards_train/chosen': '-0.82803', 'rewards_train/rejected': '-1.2827', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45469', 'logps_train/rejected': '-146.81', 'logps_train/chosen': '-151.25', 'loss/train': '0.58008', 'examples_per_second': '46.156', 'grad_norm': '26.603', 'counters/examples': 53888, 'counters/updates': 1684}
skipping logging after 53920 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '-1.1102', 'rewards_train/rejected': '-1.8056', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69541', 'logps_train/rejected': '-146.39', 'logps_train/chosen': '-174.72', 'loss/train': '0.5224', 'examples_per_second': '45.821', 'grad_norm': '24.935', 'counters/examples': 53952, 'counters/updates': 1686}
skipping logging after 53984 examples to avoid logging too frequently
train stats after 54016 examples: {'rewards_train/chosen': '-0.63932', 'rewards_train/rejected': '-1.3777', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.7384', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-130.17', 'loss/train': '0.53873', 'examples_per_second': '46.783', 'grad_norm': '25.422', 'counters/examples': 54016, 'counters/updates': 1688}
skipping logging after 54048 examples to avoid logging too frequently
train stats after 54080 examples: {'rewards_train/chosen': '-0.68833', 'rewards_train/rejected': '-1.2594', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57104', 'logps_train/rejected': '-159.24', 'logps_train/chosen': '-157.81', 'loss/train': '0.60313', 'examples_per_second': '45.651', 'grad_norm': '27.236', 'counters/examples': 54080, 'counters/updates': 1690}
skipping logging after 54112 examples to avoid logging too frequently
train stats after 54144 examples: {'rewards_train/chosen': '-0.76121', 'rewards_train/rejected': '-1.0302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.26902', 'logps_train/rejected': '-106.68', 'logps_train/chosen': '-156.24', 'loss/train': '0.69157', 'examples_per_second': '44.67', 'grad_norm': '28.663', 'counters/examples': 54144, 'counters/updates': 1692}
skipping logging after 54176 examples to avoid logging too frequently
train stats after 54208 examples: {'rewards_train/chosen': '-0.57913', 'rewards_train/rejected': '-1.0797', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50052', 'logps_train/rejected': '-147.28', 'logps_train/chosen': '-127.83', 'loss/train': '0.66257', 'examples_per_second': '46.612', 'grad_norm': '25.22', 'counters/examples': 54208, 'counters/updates': 1694}
skipping logging after 54240 examples to avoid logging too frequently
train stats after 54272 examples: {'rewards_train/chosen': '-0.62101', 'rewards_train/rejected': '-1.696', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '1.075', 'logps_train/rejected': '-137.59', 'logps_train/chosen': '-164.07', 'loss/train': '0.39568', 'examples_per_second': '48.216', 'grad_norm': '20.953', 'counters/examples': 54272, 'counters/updates': 1696}
skipping logging after 54304 examples to avoid logging too frequently
train stats after 54336 examples: {'rewards_train/chosen': '-0.66105', 'rewards_train/rejected': '-1.3575', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.69643', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-151.64', 'loss/train': '0.52274', 'examples_per_second': '45.519', 'grad_norm': '22.656', 'counters/examples': 54336, 'counters/updates': 1698}
skipping logging after 54368 examples to avoid logging too frequently
train stats after 54400 examples: {'rewards_train/chosen': '-0.76633', 'rewards_train/rejected': '-1.2353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46893', 'logps_train/rejected': '-144.33', 'logps_train/chosen': '-140.79', 'loss/train': '0.62717', 'examples_per_second': '45.731', 'grad_norm': '24.836', 'counters/examples': 54400, 'counters/updates': 1700}
skipping logging after 54432 examples to avoid logging too frequently
train stats after 54464 examples: {'rewards_train/chosen': '-0.77071', 'rewards_train/rejected': '-1.4873', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71659', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-174.59', 'loss/train': '0.49705', 'examples_per_second': '45.46', 'grad_norm': '23.865', 'counters/examples': 54464, 'counters/updates': 1702}
skipping logging after 54496 examples to avoid logging too frequently
train stats after 54528 examples: {'rewards_train/chosen': '-1.1986', 'rewards_train/rejected': '-1.4452', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2466', 'logps_train/rejected': '-174.44', 'logps_train/chosen': '-134.44', 'loss/train': '0.64592', 'examples_per_second': '45.587', 'grad_norm': '27.22', 'counters/examples': 54528, 'counters/updates': 1704}
skipping logging after 54560 examples to avoid logging too frequently
train stats after 54592 examples: {'rewards_train/chosen': '-1.0881', 'rewards_train/rejected': '-1.5462', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.45812', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-166.28', 'loss/train': '0.71668', 'examples_per_second': '44.691', 'grad_norm': '33.604', 'counters/examples': 54592, 'counters/updates': 1706}
skipping logging after 54624 examples to avoid logging too frequently
train stats after 54656 examples: {'rewards_train/chosen': '-1.2268', 'rewards_train/rejected': '-1.624', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39715', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-141.64', 'loss/train': '0.583', 'examples_per_second': '45.624', 'grad_norm': '24.403', 'counters/examples': 54656, 'counters/updates': 1708}
skipping logging after 54688 examples to avoid logging too frequently
train stats after 54720 examples: {'rewards_train/chosen': '-1.025', 'rewards_train/rejected': '-1.5965', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57152', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-125.08', 'loss/train': '0.574', 'examples_per_second': '44.303', 'grad_norm': '23.378', 'counters/examples': 54720, 'counters/updates': 1710}
skipping logging after 54752 examples to avoid logging too frequently
train stats after 54784 examples: {'rewards_train/chosen': '-0.87364', 'rewards_train/rejected': '-1.3671', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4935', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-149.81', 'loss/train': '0.58398', 'examples_per_second': '46.22', 'grad_norm': '24.526', 'counters/examples': 54784, 'counters/updates': 1712}
skipping logging after 54816 examples to avoid logging too frequently
train stats after 54848 examples: {'rewards_train/chosen': '-1.0143', 'rewards_train/rejected': '-1.6973', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.68293', 'logps_train/rejected': '-142.38', 'logps_train/chosen': '-166.25', 'loss/train': '0.48649', 'examples_per_second': '44.992', 'grad_norm': '21.637', 'counters/examples': 54848, 'counters/updates': 1714}
skipping logging after 54880 examples to avoid logging too frequently
train stats after 54912 examples: {'rewards_train/chosen': '-0.9173', 'rewards_train/rejected': '-1.3954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.47813', 'logps_train/rejected': '-120', 'logps_train/chosen': '-160.73', 'loss/train': '0.59855', 'examples_per_second': '46.765', 'grad_norm': '26.112', 'counters/examples': 54912, 'counters/updates': 1716}
skipping logging after 54944 examples to avoid logging too frequently
train stats after 54976 examples: {'rewards_train/chosen': '-1.0459', 'rewards_train/rejected': '-1.4575', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41158', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-149.61', 'loss/train': '0.61847', 'examples_per_second': '45.697', 'grad_norm': '26.777', 'counters/examples': 54976, 'counters/updates': 1718}
skipping logging after 55008 examples to avoid logging too frequently
train stats after 55040 examples: {'rewards_train/chosen': '-0.93982', 'rewards_train/rejected': '-1.4081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46831', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-163.92', 'loss/train': '0.56507', 'examples_per_second': '47.115', 'grad_norm': '24.248', 'counters/examples': 55040, 'counters/updates': 1720}
skipping logging after 55072 examples to avoid logging too frequently
train stats after 55104 examples: {'rewards_train/chosen': '-1.2347', 'rewards_train/rejected': '-1.8172', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.58248', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-116.66', 'loss/train': '0.54162', 'examples_per_second': '47.31', 'grad_norm': '21.914', 'counters/examples': 55104, 'counters/updates': 1722}
skipping logging after 55136 examples to avoid logging too frequently
train stats after 55168 examples: {'rewards_train/chosen': '-1.0548', 'rewards_train/rejected': '-1.3843', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32953', 'logps_train/rejected': '-146.36', 'logps_train/chosen': '-160.12', 'loss/train': '0.66874', 'examples_per_second': '44.59', 'grad_norm': '27.352', 'counters/examples': 55168, 'counters/updates': 1724}
skipping logging after 55200 examples to avoid logging too frequently
train stats after 55232 examples: {'rewards_train/chosen': '-1.044', 'rewards_train/rejected': '-1.5033', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4593', 'logps_train/rejected': '-180.31', 'logps_train/chosen': '-167.03', 'loss/train': '0.58692', 'examples_per_second': '46.805', 'grad_norm': '27.099', 'counters/examples': 55232, 'counters/updates': 1726}
skipping logging after 55264 examples to avoid logging too frequently
train stats after 55296 examples: {'rewards_train/chosen': '-1.2613', 'rewards_train/rejected': '-1.7385', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47721', 'logps_train/rejected': '-114.93', 'logps_train/chosen': '-140.85', 'loss/train': '0.60467', 'examples_per_second': '46.215', 'grad_norm': '24.185', 'counters/examples': 55296, 'counters/updates': 1728}
skipping logging after 55328 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '-0.66727', 'rewards_train/rejected': '-1.4161', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.74884', 'logps_train/rejected': '-160.68', 'logps_train/chosen': '-145.94', 'loss/train': '0.51747', 'examples_per_second': '31.821', 'grad_norm': '23.99', 'counters/examples': 55360, 'counters/updates': 1730}
skipping logging after 55392 examples to avoid logging too frequently
train stats after 55424 examples: {'rewards_train/chosen': '-1.1787', 'rewards_train/rejected': '-1.4957', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.31703', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-140.42', 'loss/train': '0.73391', 'examples_per_second': '47.017', 'grad_norm': '25.971', 'counters/examples': 55424, 'counters/updates': 1732}
skipping logging after 55456 examples to avoid logging too frequently
train stats after 55488 examples: {'rewards_train/chosen': '-0.80304', 'rewards_train/rejected': '-1.2858', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.48272', 'logps_train/rejected': '-142.31', 'logps_train/chosen': '-154.73', 'loss/train': '0.60339', 'examples_per_second': '44.79', 'grad_norm': '28.577', 'counters/examples': 55488, 'counters/updates': 1734}
skipping logging after 55520 examples to avoid logging too frequently
train stats after 55552 examples: {'rewards_train/chosen': '-1.1578', 'rewards_train/rejected': '-1.6462', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.48846', 'logps_train/rejected': '-171.4', 'logps_train/chosen': '-179.93', 'loss/train': '0.62504', 'examples_per_second': '45.653', 'grad_norm': '26.985', 'counters/examples': 55552, 'counters/updates': 1736}
skipping logging after 55584 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '-0.84054', 'rewards_train/rejected': '-1.4307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59017', 'logps_train/rejected': '-141.32', 'logps_train/chosen': '-152.03', 'loss/train': '0.54174', 'examples_per_second': '44.099', 'grad_norm': '25.151', 'counters/examples': 55616, 'counters/updates': 1738}
skipping logging after 55648 examples to avoid logging too frequently
train stats after 55680 examples: {'rewards_train/chosen': '-1.2505', 'rewards_train/rejected': '-1.6179', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36738', 'logps_train/rejected': '-166', 'logps_train/chosen': '-137.37', 'loss/train': '0.69709', 'examples_per_second': '45.574', 'grad_norm': '30.094', 'counters/examples': 55680, 'counters/updates': 1740}
skipping logging after 55712 examples to avoid logging too frequently
train stats after 55744 examples: {'rewards_train/chosen': '-1.2493', 'rewards_train/rejected': '-1.6468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.39749', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-137.48', 'loss/train': '0.64259', 'examples_per_second': '45.593', 'grad_norm': '28.658', 'counters/examples': 55744, 'counters/updates': 1742}
skipping logging after 55776 examples to avoid logging too frequently
train stats after 55808 examples: {'rewards_train/chosen': '-0.70991', 'rewards_train/rejected': '-1.4363', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.72642', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-149.95', 'loss/train': '0.52005', 'examples_per_second': '45.833', 'grad_norm': '21.78', 'counters/examples': 55808, 'counters/updates': 1744}
skipping logging after 55840 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '-0.84208', 'rewards_train/rejected': '-1.4471', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60498', 'logps_train/rejected': '-158.8', 'logps_train/chosen': '-145.91', 'loss/train': '0.53801', 'examples_per_second': '44.382', 'grad_norm': '22.839', 'counters/examples': 55872, 'counters/updates': 1746}
skipping logging after 55904 examples to avoid logging too frequently
train stats after 55936 examples: {'rewards_train/chosen': '-0.97102', 'rewards_train/rejected': '-1.4952', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52423', 'logps_train/rejected': '-153.03', 'logps_train/chosen': '-128.77', 'loss/train': '0.55746', 'examples_per_second': '45.215', 'grad_norm': '26.257', 'counters/examples': 55936, 'counters/updates': 1748}
skipping logging after 55968 examples to avoid logging too frequently
train stats after 56000 examples: {'rewards_train/chosen': '-0.77403', 'rewards_train/rejected': '-1.4827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.70866', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-159.63', 'loss/train': '0.54897', 'examples_per_second': '45.672', 'grad_norm': '27.76', 'counters/examples': 56000, 'counters/updates': 1750}
skipping logging after 56032 examples to avoid logging too frequently
train stats after 56064 examples: {'rewards_train/chosen': '-0.8819', 'rewards_train/rejected': '-1.2902', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.40831', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-164.56', 'loss/train': '0.64092', 'examples_per_second': '44.468', 'grad_norm': '26.274', 'counters/examples': 56064, 'counters/updates': 1752}
skipping logging after 56096 examples to avoid logging too frequently
train stats after 56128 examples: {'rewards_train/chosen': '-0.619', 'rewards_train/rejected': '-1.1803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56128', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-145.75', 'loss/train': '0.60474', 'examples_per_second': '45.443', 'grad_norm': '26.747', 'counters/examples': 56128, 'counters/updates': 1754}
skipping logging after 56160 examples to avoid logging too frequently
train stats after 56192 examples: {'rewards_train/chosen': '-0.44364', 'rewards_train/rejected': '-1.0793', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63566', 'logps_train/rejected': '-147.07', 'logps_train/chosen': '-163.8', 'loss/train': '0.51622', 'examples_per_second': '45.474', 'grad_norm': '23.674', 'counters/examples': 56192, 'counters/updates': 1756}
skipping logging after 56224 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '-0.81407', 'rewards_train/rejected': '-1.1884', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37437', 'logps_train/rejected': '-180.95', 'logps_train/chosen': '-145.33', 'loss/train': '0.60625', 'examples_per_second': '45.135', 'grad_norm': '26.643', 'counters/examples': 56256, 'counters/updates': 1758}
skipping logging after 56288 examples to avoid logging too frequently
train stats after 56320 examples: {'rewards_train/chosen': '-0.35255', 'rewards_train/rejected': '-0.96076', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.6082', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-135.44', 'loss/train': '0.48971', 'examples_per_second': '45.386', 'grad_norm': '21.61', 'counters/examples': 56320, 'counters/updates': 1760}
skipping logging after 56352 examples to avoid logging too frequently
train stats after 56384 examples: {'rewards_train/chosen': '-0.8982', 'rewards_train/rejected': '-1.2704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37219', 'logps_train/rejected': '-141.25', 'logps_train/chosen': '-171.55', 'loss/train': '0.68271', 'examples_per_second': '45.521', 'grad_norm': '32.564', 'counters/examples': 56384, 'counters/updates': 1762}
skipping logging after 56416 examples to avoid logging too frequently
train stats after 56448 examples: {'rewards_train/chosen': '-0.53711', 'rewards_train/rejected': '-0.98186', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44475', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-135.33', 'loss/train': '0.59911', 'examples_per_second': '47.987', 'grad_norm': '22.894', 'counters/examples': 56448, 'counters/updates': 1764}
skipping logging after 56480 examples to avoid logging too frequently
train stats after 56512 examples: {'rewards_train/chosen': '-0.92328', 'rewards_train/rejected': '-1.3004', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.37716', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-117.91', 'loss/train': '0.64416', 'examples_per_second': '47.218', 'grad_norm': '25.439', 'counters/examples': 56512, 'counters/updates': 1766}
skipping logging after 56544 examples to avoid logging too frequently
train stats after 56576 examples: {'rewards_train/chosen': '-0.54097', 'rewards_train/rejected': '-1.1393', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59829', 'logps_train/rejected': '-110.1', 'logps_train/chosen': '-154.19', 'loss/train': '0.52992', 'examples_per_second': '47.947', 'grad_norm': '22.358', 'counters/examples': 56576, 'counters/updates': 1768}
skipping logging after 56608 examples to avoid logging too frequently
train stats after 56640 examples: {'rewards_train/chosen': '-0.82419', 'rewards_train/rejected': '-1.5304', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.70621', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-118', 'loss/train': '0.48483', 'examples_per_second': '48.045', 'grad_norm': '20.493', 'counters/examples': 56640, 'counters/updates': 1770}
skipping logging after 56672 examples to avoid logging too frequently
train stats after 56704 examples: {'rewards_train/chosen': '-0.65975', 'rewards_train/rejected': '-1.2261', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.5664', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-131.7', 'loss/train': '0.51584', 'examples_per_second': '46.901', 'grad_norm': '22.272', 'counters/examples': 56704, 'counters/updates': 1772}
skipping logging after 56736 examples to avoid logging too frequently
train stats after 56768 examples: {'rewards_train/chosen': '-0.75493', 'rewards_train/rejected': '-1.3741', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61917', 'logps_train/rejected': '-143.13', 'logps_train/chosen': '-160.29', 'loss/train': '0.56485', 'examples_per_second': '45.818', 'grad_norm': '22.451', 'counters/examples': 56768, 'counters/updates': 1774}
skipping logging after 56800 examples to avoid logging too frequently
train stats after 56832 examples: {'rewards_train/chosen': '-0.4476', 'rewards_train/rejected': '-1.2147', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.76712', 'logps_train/rejected': '-97.183', 'logps_train/chosen': '-111.51', 'loss/train': '0.44978', 'examples_per_second': '45.764', 'grad_norm': '17.348', 'counters/examples': 56832, 'counters/updates': 1776}
skipping logging after 56864 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '-0.86845', 'rewards_train/rejected': '-1.5842', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.7158', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-140.39', 'loss/train': '0.55083', 'examples_per_second': '48.381', 'grad_norm': '23.89', 'counters/examples': 56896, 'counters/updates': 1778}
skipping logging after 56928 examples to avoid logging too frequently
train stats after 56960 examples: {'rewards_train/chosen': '-0.69189', 'rewards_train/rejected': '-1.4122', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.72029', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-142.84', 'loss/train': '0.52389', 'examples_per_second': '45.776', 'grad_norm': '21.811', 'counters/examples': 56960, 'counters/updates': 1780}
skipping logging after 56992 examples to avoid logging too frequently
train stats after 57024 examples: {'rewards_train/chosen': '-0.6892', 'rewards_train/rejected': '-1.5718', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.88257', 'logps_train/rejected': '-147.57', 'logps_train/chosen': '-130.14', 'loss/train': '0.49696', 'examples_per_second': '46.688', 'grad_norm': '21.212', 'counters/examples': 57024, 'counters/updates': 1782}
skipping logging after 57056 examples to avoid logging too frequently
train stats after 57088 examples: {'rewards_train/chosen': '-0.88777', 'rewards_train/rejected': '-1.617', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.7292', 'logps_train/rejected': '-119.79', 'logps_train/chosen': '-173.98', 'loss/train': '0.57173', 'examples_per_second': '45.727', 'grad_norm': '26.608', 'counters/examples': 57088, 'counters/updates': 1784}
skipping logging after 57120 examples to avoid logging too frequently
train stats after 57152 examples: {'rewards_train/chosen': '-0.90749', 'rewards_train/rejected': '-1.4489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54145', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-110.69', 'loss/train': '0.61355', 'examples_per_second': '44.886', 'grad_norm': '23.934', 'counters/examples': 57152, 'counters/updates': 1786}
skipping logging after 57184 examples to avoid logging too frequently
train stats after 57216 examples: {'rewards_train/chosen': '-0.92096', 'rewards_train/rejected': '-1.5006', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57968', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-179.95', 'loss/train': '0.58365', 'examples_per_second': '45.403', 'grad_norm': '24.897', 'counters/examples': 57216, 'counters/updates': 1788}
skipping logging after 57248 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '-1.1267', 'rewards_train/rejected': '-1.4868', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36006', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-132.41', 'loss/train': '0.67855', 'examples_per_second': '45.741', 'grad_norm': '28.205', 'counters/examples': 57280, 'counters/updates': 1790}
skipping logging after 57312 examples to avoid logging too frequently
train stats after 57344 examples: {'rewards_train/chosen': '-0.92747', 'rewards_train/rejected': '-1.4308', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50337', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-132.34', 'loss/train': '0.54823', 'examples_per_second': '44.611', 'grad_norm': '21.109', 'counters/examples': 57344, 'counters/updates': 1792}
skipping logging after 57376 examples to avoid logging too frequently
train stats after 57408 examples: {'rewards_train/chosen': '-0.81049', 'rewards_train/rejected': '-1.6261', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.81565', 'logps_train/rejected': '-195.44', 'logps_train/chosen': '-164.69', 'loss/train': '0.54103', 'examples_per_second': '45.76', 'grad_norm': '26.061', 'counters/examples': 57408, 'counters/updates': 1794}
skipping logging after 57440 examples to avoid logging too frequently
train stats after 57472 examples: {'rewards_train/chosen': '-0.98134', 'rewards_train/rejected': '-1.5068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52546', 'logps_train/rejected': '-158.73', 'logps_train/chosen': '-123.22', 'loss/train': '0.59552', 'examples_per_second': '45.727', 'grad_norm': '26.485', 'counters/examples': 57472, 'counters/updates': 1796}
skipping logging after 57504 examples to avoid logging too frequently
train stats after 57536 examples: {'rewards_train/chosen': '-1.1345', 'rewards_train/rejected': '-1.545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41056', 'logps_train/rejected': '-119.9', 'logps_train/chosen': '-131.51', 'loss/train': '0.61515', 'examples_per_second': '46.044', 'grad_norm': '25.553', 'counters/examples': 57536, 'counters/updates': 1798}
skipping logging after 57568 examples to avoid logging too frequently
train stats after 57600 examples: {'rewards_train/chosen': '-0.98922', 'rewards_train/rejected': '-1.7646', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.77541', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-156.31', 'loss/train': '0.48974', 'examples_per_second': '46.893', 'grad_norm': '24.515', 'counters/examples': 57600, 'counters/updates': 1800}
skipping logging after 57632 examples to avoid logging too frequently
train stats after 57664 examples: {'rewards_train/chosen': '-0.94274', 'rewards_train/rejected': '-1.4358', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.49306', 'logps_train/rejected': '-151.6', 'logps_train/chosen': '-132.22', 'loss/train': '0.59413', 'examples_per_second': '44.814', 'grad_norm': '24.109', 'counters/examples': 57664, 'counters/updates': 1802}
skipping logging after 57696 examples to avoid logging too frequently
train stats after 57728 examples: {'rewards_train/chosen': '-0.99278', 'rewards_train/rejected': '-1.8878', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.89504', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-145.27', 'loss/train': '0.49079', 'examples_per_second': '45.505', 'grad_norm': '27.191', 'counters/examples': 57728, 'counters/updates': 1804}
skipping logging after 57760 examples to avoid logging too frequently
train stats after 57792 examples: {'rewards_train/chosen': '-0.67474', 'rewards_train/rejected': '-1.2295', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.55477', 'logps_train/rejected': '-166.03', 'logps_train/chosen': '-179.03', 'loss/train': '0.55454', 'examples_per_second': '45.658', 'grad_norm': '25.924', 'counters/examples': 57792, 'counters/updates': 1806}
skipping logging after 57824 examples to avoid logging too frequently
train stats after 57856 examples: {'rewards_train/chosen': '-1.1765', 'rewards_train/rejected': '-1.3514', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17491', 'logps_train/rejected': '-144.14', 'logps_train/chosen': '-148.58', 'loss/train': '0.76631', 'examples_per_second': '52.518', 'grad_norm': '32.841', 'counters/examples': 57856, 'counters/updates': 1808}
skipping logging after 57888 examples to avoid logging too frequently
train stats after 57920 examples: {'rewards_train/chosen': '-1.125', 'rewards_train/rejected': '-1.3148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18982', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-133.66', 'loss/train': '0.71718', 'examples_per_second': '52.437', 'grad_norm': '26.694', 'counters/examples': 57920, 'counters/updates': 1810}
skipping logging after 57952 examples to avoid logging too frequently
train stats after 57984 examples: {'rewards_train/chosen': '-1.3512', 'rewards_train/rejected': '-1.6759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32467', 'logps_train/rejected': '-137.03', 'logps_train/chosen': '-142.27', 'loss/train': '0.66191', 'examples_per_second': '45.422', 'grad_norm': '25.572', 'counters/examples': 57984, 'counters/updates': 1812}
skipping logging after 58016 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '-0.73231', 'rewards_train/rejected': '-1.477', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.74466', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-126.58', 'loss/train': '0.54493', 'examples_per_second': '52.721', 'grad_norm': '23.243', 'counters/examples': 58048, 'counters/updates': 1814}
skipping logging after 58080 examples to avoid logging too frequently
train stats after 58112 examples: {'rewards_train/chosen': '-1.2292', 'rewards_train/rejected': '-1.6451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41591', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-124.5', 'loss/train': '0.63235', 'examples_per_second': '45.883', 'grad_norm': '23.624', 'counters/examples': 58112, 'counters/updates': 1816}
skipping logging after 58144 examples to avoid logging too frequently
train stats after 58176 examples: {'rewards_train/chosen': '-1.0636', 'rewards_train/rejected': '-1.5049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.44135', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-158.02', 'loss/train': '0.65753', 'examples_per_second': '47.666', 'grad_norm': '25.717', 'counters/examples': 58176, 'counters/updates': 1818}
skipping logging after 58208 examples to avoid logging too frequently
train stats after 58240 examples: {'rewards_train/chosen': '-1.4467', 'rewards_train/rejected': '-2.215', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.76826', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-144.79', 'loss/train': '0.51937', 'examples_per_second': '45.764', 'grad_norm': '24.461', 'counters/examples': 58240, 'counters/updates': 1820}
skipping logging after 58272 examples to avoid logging too frequently
train stats after 58304 examples: {'rewards_train/chosen': '-1.2102', 'rewards_train/rejected': '-1.9804', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.7702', 'logps_train/rejected': '-158.34', 'logps_train/chosen': '-141.17', 'loss/train': '0.54825', 'examples_per_second': '46.508', 'grad_norm': '25.53', 'counters/examples': 58304, 'counters/updates': 1822}
skipping logging after 58336 examples to avoid logging too frequently
train stats after 58368 examples: {'rewards_train/chosen': '-1.2169', 'rewards_train/rejected': '-1.891', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.67409', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-146.85', 'loss/train': '0.59029', 'examples_per_second': '45.671', 'grad_norm': '27.447', 'counters/examples': 58368, 'counters/updates': 1824}
skipping logging after 58400 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '-1.1071', 'rewards_train/rejected': '-1.9023', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.79514', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-130.66', 'loss/train': '0.47102', 'examples_per_second': '46.945', 'grad_norm': '20.032', 'counters/examples': 58432, 'counters/updates': 1826}
skipping logging after 58464 examples to avoid logging too frequently
train stats after 58496 examples: {'rewards_train/chosen': '-0.9769', 'rewards_train/rejected': '-1.9477', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.9708', 'logps_train/rejected': '-128.47', 'logps_train/chosen': '-145.97', 'loss/train': '0.48852', 'examples_per_second': '45.049', 'grad_norm': '21.25', 'counters/examples': 58496, 'counters/updates': 1828}
skipping logging after 58528 examples to avoid logging too frequently
train stats after 58560 examples: {'rewards_train/chosen': '-1.0801', 'rewards_train/rejected': '-1.9156', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.83551', 'logps_train/rejected': '-150.12', 'logps_train/chosen': '-170.75', 'loss/train': '0.50581', 'examples_per_second': '46.032', 'grad_norm': '24.68', 'counters/examples': 58560, 'counters/updates': 1830}
skipping logging after 58592 examples to avoid logging too frequently
train stats after 58624 examples: {'rewards_train/chosen': '-0.85227', 'rewards_train/rejected': '-1.8287', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.97644', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-149.22', 'loss/train': '0.45829', 'examples_per_second': '47.709', 'grad_norm': '20.428', 'counters/examples': 58624, 'counters/updates': 1832}
skipping logging after 58656 examples to avoid logging too frequently
train stats after 58688 examples: {'rewards_train/chosen': '-1.2122', 'rewards_train/rejected': '-1.7203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50813', 'logps_train/rejected': '-157.52', 'logps_train/chosen': '-136.09', 'loss/train': '0.59397', 'examples_per_second': '44.52', 'grad_norm': '26.396', 'counters/examples': 58688, 'counters/updates': 1834}
skipping logging after 58720 examples to avoid logging too frequently
train stats after 58752 examples: {'rewards_train/chosen': '-0.58895', 'rewards_train/rejected': '-1.4679', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.87898', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-125.72', 'loss/train': '0.5148', 'examples_per_second': '44.473', 'grad_norm': '24.666', 'counters/examples': 58752, 'counters/updates': 1836}
skipping logging after 58784 examples to avoid logging too frequently
train stats after 58816 examples: {'rewards_train/chosen': '-0.59243', 'rewards_train/rejected': '-1.2017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.60927', 'logps_train/rejected': '-183.92', 'logps_train/chosen': '-174.31', 'loss/train': '0.56412', 'examples_per_second': '45.753', 'grad_norm': '27.137', 'counters/examples': 58816, 'counters/updates': 1838}
skipping logging after 58848 examples to avoid logging too frequently
train stats after 58880 examples: {'rewards_train/chosen': '-0.68376', 'rewards_train/rejected': '-1.4177', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.73392', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-146.07', 'loss/train': '0.53617', 'examples_per_second': '44.387', 'grad_norm': '24.384', 'counters/examples': 58880, 'counters/updates': 1840}
skipping logging after 58912 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '-0.59859', 'rewards_train/rejected': '-1.5038', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.90519', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-143.24', 'loss/train': '0.4872', 'examples_per_second': '46.163', 'grad_norm': '22.715', 'counters/examples': 58944, 'counters/updates': 1842}
skipping logging after 58976 examples to avoid logging too frequently
train stats after 59008 examples: {'rewards_train/chosen': '-0.88975', 'rewards_train/rejected': '-1.5539', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.6641', 'logps_train/rejected': '-163.18', 'logps_train/chosen': '-157.98', 'loss/train': '0.58945', 'examples_per_second': '48.814', 'grad_norm': '27.053', 'counters/examples': 59008, 'counters/updates': 1844}
skipping logging after 59040 examples to avoid logging too frequently
train stats after 59072 examples: {'rewards_train/chosen': '-0.57704', 'rewards_train/rejected': '-1.2476', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.67056', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-131.54', 'loss/train': '0.58723', 'examples_per_second': '44.99', 'grad_norm': '23.374', 'counters/examples': 59072, 'counters/updates': 1846}
skipping logging after 59104 examples to avoid logging too frequently
train stats after 59136 examples: {'rewards_train/chosen': '-0.7626', 'rewards_train/rejected': '-1.1415', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.37889', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-163.34', 'loss/train': '0.70676', 'examples_per_second': '46.206', 'grad_norm': '30.83', 'counters/examples': 59136, 'counters/updates': 1848}
skipping logging after 59168 examples to avoid logging too frequently
train stats after 59200 examples: {'rewards_train/chosen': '-0.76364', 'rewards_train/rejected': '-1.0487', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28506', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-131.72', 'loss/train': '0.65912', 'examples_per_second': '45.195', 'grad_norm': '26.504', 'counters/examples': 59200, 'counters/updates': 1850}
skipping logging after 59232 examples to avoid logging too frequently
train stats after 59264 examples: {'rewards_train/chosen': '-0.83845', 'rewards_train/rejected': '-0.82703', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011413', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-166.87', 'loss/train': '0.81306', 'examples_per_second': '44.879', 'grad_norm': '32.69', 'counters/examples': 59264, 'counters/updates': 1852}
skipping logging after 59296 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '-0.72069', 'rewards_train/rejected': '-1.2924', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.57174', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-130.33', 'loss/train': '0.53611', 'examples_per_second': '44.902', 'grad_norm': '25.381', 'counters/examples': 59328, 'counters/updates': 1854}
skipping logging after 59360 examples to avoid logging too frequently
train stats after 59392 examples: {'rewards_train/chosen': '-0.80676', 'rewards_train/rejected': '-1.3021', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.49537', 'logps_train/rejected': '-171.44', 'logps_train/chosen': '-128.63', 'loss/train': '0.57202', 'examples_per_second': '52.503', 'grad_norm': '27.075', 'counters/examples': 59392, 'counters/updates': 1856}
skipping logging after 59424 examples to avoid logging too frequently
train stats after 59456 examples: {'rewards_train/chosen': '-0.99967', 'rewards_train/rejected': '-1.3105', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3108', 'logps_train/rejected': '-177.85', 'logps_train/chosen': '-162.39', 'loss/train': '0.64723', 'examples_per_second': '44.892', 'grad_norm': '27.835', 'counters/examples': 59456, 'counters/updates': 1858}
skipping logging after 59488 examples to avoid logging too frequently
train stats after 59520 examples: {'rewards_train/chosen': '-0.57135', 'rewards_train/rejected': '-1.1149', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54359', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-134.45', 'loss/train': '0.54449', 'examples_per_second': '45.497', 'grad_norm': '23.512', 'counters/examples': 59520, 'counters/updates': 1860}
skipping logging after 59552 examples to avoid logging too frequently
train stats after 59584 examples: {'rewards_train/chosen': '-0.71344', 'rewards_train/rejected': '-0.75434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040893', 'logps_train/rejected': '-163.78', 'logps_train/chosen': '-157.09', 'loss/train': '0.76411', 'examples_per_second': '45.732', 'grad_norm': '32.895', 'counters/examples': 59584, 'counters/updates': 1862}
skipping logging after 59616 examples to avoid logging too frequently
train stats after 59648 examples: {'rewards_train/chosen': '-0.54321', 'rewards_train/rejected': '-1.3133', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.77005', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-126.22', 'loss/train': '0.49248', 'examples_per_second': '51.333', 'grad_norm': '20.622', 'counters/examples': 59648, 'counters/updates': 1864}
skipping logging after 59680 examples to avoid logging too frequently
train stats after 59712 examples: {'rewards_train/chosen': '-0.91933', 'rewards_train/rejected': '-1.7467', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.8274', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-155.93', 'loss/train': '0.43792', 'examples_per_second': '47.968', 'grad_norm': '21.491', 'counters/examples': 59712, 'counters/updates': 1866}
skipping logging after 59744 examples to avoid logging too frequently
train stats after 59776 examples: {'rewards_train/chosen': '-0.7898', 'rewards_train/rejected': '-1.1358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.34604', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-138.85', 'loss/train': '0.66455', 'examples_per_second': '48.437', 'grad_norm': '25.938', 'counters/examples': 59776, 'counters/updates': 1868}
skipping logging after 59808 examples to avoid logging too frequently
train stats after 59840 examples: {'rewards_train/chosen': '-1.0373', 'rewards_train/rejected': '-1.3051', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26784', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-122.81', 'loss/train': '0.74057', 'examples_per_second': '46.552', 'grad_norm': '26.473', 'counters/examples': 59840, 'counters/updates': 1870}
skipping logging after 59872 examples to avoid logging too frequently
train stats after 59904 examples: {'rewards_train/chosen': '-0.71901', 'rewards_train/rejected': '-1.4127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.69373', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-141.08', 'loss/train': '0.5342', 'examples_per_second': '44.387', 'grad_norm': '21.559', 'counters/examples': 59904, 'counters/updates': 1872}
skipping logging after 59936 examples to avoid logging too frequently
train stats after 59968 examples: {'rewards_train/chosen': '-0.85135', 'rewards_train/rejected': '-1.374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52265', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-134.35', 'loss/train': '0.59922', 'examples_per_second': '44.61', 'grad_norm': '25.396', 'counters/examples': 59968, 'counters/updates': 1874}
skipping logging after 60000 examples to avoid logging too frequently
Running evaluation after 60000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.19it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.86it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.99it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  7.01it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.92it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.36it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.23it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.17it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.19it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.07it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.08it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  7.09it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.09it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.97it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  7.03it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.94it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.05it/s]
eval after 60000: {'rewards_eval/chosen': '-0.74581', 'rewards_eval/rejected': '-1.3005', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.55467', 'logps_eval/rejected': '-131.29', 'logps_eval/chosen': '-143.91', 'loss/eval': '0.6041'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-60000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-60000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-60000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-60000/scheduler.pt...
train stats after 60032 examples: {'rewards_train/chosen': '-0.9531', 'rewards_train/rejected': '-1.434', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.48088', 'logps_train/rejected': '-159.01', 'logps_train/chosen': '-169.84', 'loss/train': '0.61711', 'examples_per_second': '32.135', 'grad_norm': '30.804', 'counters/examples': 60032, 'counters/updates': 1876}
skipping logging after 60064 examples to avoid logging too frequently
train stats after 60096 examples: {'rewards_train/chosen': '-0.78911', 'rewards_train/rejected': '-1.4119', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62278', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-145.06', 'loss/train': '0.55856', 'examples_per_second': '45.563', 'grad_norm': '26.712', 'counters/examples': 60096, 'counters/updates': 1878}
skipping logging after 60128 examples to avoid logging too frequently
train stats after 60160 examples: {'rewards_train/chosen': '-0.5834', 'rewards_train/rejected': '-0.95792', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37451', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-161.86', 'loss/train': '0.66141', 'examples_per_second': '47.108', 'grad_norm': '30.937', 'counters/examples': 60160, 'counters/updates': 1880}
skipping logging after 60192 examples to avoid logging too frequently
train stats after 60224 examples: {'rewards_train/chosen': '-0.67752', 'rewards_train/rejected': '-1.249', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57153', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-182.9', 'loss/train': '0.52575', 'examples_per_second': '46.389', 'grad_norm': '24.572', 'counters/examples': 60224, 'counters/updates': 1882}
skipping logging after 60256 examples to avoid logging too frequently
train stats after 60288 examples: {'rewards_train/chosen': '-0.46216', 'rewards_train/rejected': '-0.90362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44146', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-140.25', 'loss/train': '0.593', 'examples_per_second': '44.45', 'grad_norm': '24.484', 'counters/examples': 60288, 'counters/updates': 1884}
skipping logging after 60320 examples to avoid logging too frequently
train stats after 60352 examples: {'rewards_train/chosen': '-0.48056', 'rewards_train/rejected': '-1.2516', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.771', 'logps_train/rejected': '-120.39', 'logps_train/chosen': '-133.71', 'loss/train': '0.49301', 'examples_per_second': '45.109', 'grad_norm': '21.878', 'counters/examples': 60352, 'counters/updates': 1886}
skipping logging after 60384 examples to avoid logging too frequently
train stats after 60416 examples: {'rewards_train/chosen': '-0.56573', 'rewards_train/rejected': '-1.4477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.88192', 'logps_train/rejected': '-145.03', 'logps_train/chosen': '-148.97', 'loss/train': '0.52287', 'examples_per_second': '45.358', 'grad_norm': '24.308', 'counters/examples': 60416, 'counters/updates': 1888}
skipping logging after 60448 examples to avoid logging too frequently
train stats after 60480 examples: {'rewards_train/chosen': '-0.71516', 'rewards_train/rejected': '-1.4402', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.72508', 'logps_train/rejected': '-109.75', 'logps_train/chosen': '-113.15', 'loss/train': '0.62292', 'examples_per_second': '46.465', 'grad_norm': '22.539', 'counters/examples': 60480, 'counters/updates': 1890}
skipping logging after 60512 examples to avoid logging too frequently
train stats after 60544 examples: {'rewards_train/chosen': '-0.61262', 'rewards_train/rejected': '-0.89128', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27867', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-114.31', 'loss/train': '0.66459', 'examples_per_second': '46.549', 'grad_norm': '24.493', 'counters/examples': 60544, 'counters/updates': 1892}
skipping logging after 60576 examples to avoid logging too frequently
train stats after 60608 examples: {'rewards_train/chosen': '-0.59159', 'rewards_train/rejected': '-1.0851', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49349', 'logps_train/rejected': '-148.84', 'logps_train/chosen': '-139.92', 'loss/train': '0.57941', 'examples_per_second': '45.205', 'grad_norm': '26.286', 'counters/examples': 60608, 'counters/updates': 1894}
skipping logging after 60640 examples to avoid logging too frequently
train stats after 60672 examples: {'rewards_train/chosen': '-0.78091', 'rewards_train/rejected': '-1.2401', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.45917', 'logps_train/rejected': '-92.02', 'logps_train/chosen': '-152.43', 'loss/train': '0.56239', 'examples_per_second': '46.716', 'grad_norm': '22.966', 'counters/examples': 60672, 'counters/updates': 1896}
skipping logging after 60704 examples to avoid logging too frequently
train stats after 60736 examples: {'rewards_train/chosen': '-0.89748', 'rewards_train/rejected': '-1.3915', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.49407', 'logps_train/rejected': '-147.01', 'logps_train/chosen': '-125.43', 'loss/train': '0.56675', 'examples_per_second': '44.752', 'grad_norm': '22.707', 'counters/examples': 60736, 'counters/updates': 1898}
skipping logging after 60768 examples to avoid logging too frequently
train stats after 60800 examples: {'rewards_train/chosen': '-1.1032', 'rewards_train/rejected': '-1.7819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.67867', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-124.28', 'loss/train': '0.56893', 'examples_per_second': '46.716', 'grad_norm': '23.089', 'counters/examples': 60800, 'counters/updates': 1900}
skipping logging after 60832 examples to avoid logging too frequently
train stats after 60864 examples: {'rewards_train/chosen': '-0.82297', 'rewards_train/rejected': '-1.4334', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61044', 'logps_train/rejected': '-127.91', 'logps_train/chosen': '-149.08', 'loss/train': '0.53038', 'examples_per_second': '44.871', 'grad_norm': '23.401', 'counters/examples': 60864, 'counters/updates': 1902}
skipping logging after 60896 examples to avoid logging too frequently
train stats after 60928 examples: {'rewards_train/chosen': '-0.99974', 'rewards_train/rejected': '-1.4985', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.49876', 'logps_train/rejected': '-149.02', 'logps_train/chosen': '-154.54', 'loss/train': '0.62889', 'examples_per_second': '44.459', 'grad_norm': '27.649', 'counters/examples': 60928, 'counters/updates': 1904}
skipping logging after 60960 examples to avoid logging too frequently
train stats after 60992 examples: {'rewards_train/chosen': '-1.4189', 'rewards_train/rejected': '-1.9827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56379', 'logps_train/rejected': '-138.94', 'logps_train/chosen': '-130.57', 'loss/train': '0.56031', 'examples_per_second': '47.672', 'grad_norm': '23.49', 'counters/examples': 60992, 'counters/updates': 1906}
skipping logging after 61024 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': '-1.3672', 'rewards_train/rejected': '-1.7027', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3356', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-131.93', 'loss/train': '0.70755', 'examples_per_second': '48.843', 'grad_norm': '23.752', 'counters/examples': 61056, 'counters/updates': 1908}
skipping logging after 61088 examples to avoid logging too frequently
train stats after 61120 examples: {'rewards_train/chosen': '-1.1757', 'rewards_train/rejected': '-1.6322', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.45644', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-143.62', 'loss/train': '0.64093', 'examples_per_second': '46.626', 'grad_norm': '27.752', 'counters/examples': 61120, 'counters/updates': 1910}
skipping logging after 61152 examples to avoid logging too frequently
train stats after 61184 examples: {'rewards_train/chosen': '-0.99192', 'rewards_train/rejected': '-1.6919', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.69997', 'logps_train/rejected': '-164.87', 'logps_train/chosen': '-146.41', 'loss/train': '0.54464', 'examples_per_second': '44.39', 'grad_norm': '22.224', 'counters/examples': 61184, 'counters/updates': 1912}
skipping logging after 61216 examples to avoid logging too frequently
train stats after 61248 examples: {'rewards_train/chosen': '-0.89464', 'rewards_train/rejected': '-1.3869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49227', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-139.78', 'loss/train': '0.5901', 'examples_per_second': '46.39', 'grad_norm': '24.804', 'counters/examples': 61248, 'counters/updates': 1914}
skipping logging after 61280 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '-1.1239', 'rewards_train/rejected': '-1.9404', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.81654', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-157.63', 'loss/train': '0.46415', 'examples_per_second': '44.7', 'grad_norm': '23.01', 'counters/examples': 61312, 'counters/updates': 1916}
skipping logging after 61344 examples to avoid logging too frequently
train stats after 61376 examples: {'rewards_train/chosen': '-0.9811', 'rewards_train/rejected': '-1.4917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.51061', 'logps_train/rejected': '-147.66', 'logps_train/chosen': '-147.58', 'loss/train': '0.6281', 'examples_per_second': '44.287', 'grad_norm': '25.558', 'counters/examples': 61376, 'counters/updates': 1918}
skipping logging after 61408 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '-0.86827', 'rewards_train/rejected': '-1.4787', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.61042', 'logps_train/rejected': '-136.07', 'logps_train/chosen': '-133.61', 'loss/train': '0.52688', 'examples_per_second': '45.62', 'grad_norm': '23.862', 'counters/examples': 61440, 'counters/updates': 1920}
skipping logging after 61472 examples to avoid logging too frequently
train stats after 61504 examples: {'rewards_train/chosen': '-0.87066', 'rewards_train/rejected': '-1.1399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2692', 'logps_train/rejected': '-144.33', 'logps_train/chosen': '-115.08', 'loss/train': '0.69379', 'examples_per_second': '44.401', 'grad_norm': '27.063', 'counters/examples': 61504, 'counters/updates': 1922}
skipping logging after 61536 examples to avoid logging too frequently
train stats after 61568 examples: {'rewards_train/chosen': '-0.789', 'rewards_train/rejected': '-1.4796', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69061', 'logps_train/rejected': '-156.52', 'logps_train/chosen': '-160.39', 'loss/train': '0.54871', 'examples_per_second': '45.624', 'grad_norm': '25.638', 'counters/examples': 61568, 'counters/updates': 1924}
skipping logging after 61600 examples to avoid logging too frequently
train stats after 61632 examples: {'rewards_train/chosen': '-0.62391', 'rewards_train/rejected': '-1.3427', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.71883', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-132.52', 'loss/train': '0.52449', 'examples_per_second': '46.119', 'grad_norm': '21.994', 'counters/examples': 61632, 'counters/updates': 1926}
skipping logging after 61664 examples to avoid logging too frequently
train stats after 61696 examples: {'rewards_train/chosen': '-0.60624', 'rewards_train/rejected': '-1.2788', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.67257', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-107.49', 'loss/train': '0.55224', 'examples_per_second': '45.031', 'grad_norm': '23.219', 'counters/examples': 61696, 'counters/updates': 1928}
skipping logging after 61728 examples to avoid logging too frequently
train stats after 61760 examples: {'rewards_train/chosen': '-0.64336', 'rewards_train/rejected': '-1.5926', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.94927', 'logps_train/rejected': '-144.57', 'logps_train/chosen': '-121.19', 'loss/train': '0.42962', 'examples_per_second': '32.04', 'grad_norm': '18.967', 'counters/examples': 61760, 'counters/updates': 1930}
skipping logging after 61792 examples to avoid logging too frequently
train stats after 61824 examples: {'rewards_train/chosen': '-0.7468', 'rewards_train/rejected': '-1.3072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56043', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-158.58', 'loss/train': '0.61151', 'examples_per_second': '45.866', 'grad_norm': '28.111', 'counters/examples': 61824, 'counters/updates': 1932}
skipping logging after 61856 examples to avoid logging too frequently
train stats after 61888 examples: {'rewards_train/chosen': '-0.64443', 'rewards_train/rejected': '-1.4519', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.80744', 'logps_train/rejected': '-110.37', 'logps_train/chosen': '-114.99', 'loss/train': '0.44358', 'examples_per_second': '45.654', 'grad_norm': '18.143', 'counters/examples': 61888, 'counters/updates': 1934}
skipping logging after 61920 examples to avoid logging too frequently
train stats after 61952 examples: {'rewards_train/chosen': '-0.70612', 'rewards_train/rejected': '-1.099', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.39283', 'logps_train/rejected': '-150.76', 'logps_train/chosen': '-169.69', 'loss/train': '0.69285', 'examples_per_second': '45.192', 'grad_norm': '31.728', 'counters/examples': 61952, 'counters/updates': 1936}
skipping logging after 61984 examples to avoid logging too frequently
train stats after 62016 examples: {'rewards_train/chosen': '-0.46432', 'rewards_train/rejected': '-1.4991', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.0348', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-117.28', 'loss/train': '0.45457', 'examples_per_second': '45.705', 'grad_norm': '19.747', 'counters/examples': 62016, 'counters/updates': 1938}
skipping logging after 62048 examples to avoid logging too frequently
train stats after 62080 examples: {'rewards_train/chosen': '-0.60068', 'rewards_train/rejected': '-1.1186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.51794', 'logps_train/rejected': '-138.35', 'logps_train/chosen': '-147.68', 'loss/train': '0.61314', 'examples_per_second': '44.42', 'grad_norm': '26.908', 'counters/examples': 62080, 'counters/updates': 1940}
skipping logging after 62112 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '-0.61901', 'rewards_train/rejected': '-1.3629', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.74392', 'logps_train/rejected': '-109.79', 'logps_train/chosen': '-149.9', 'loss/train': '0.51656', 'examples_per_second': '45.691', 'grad_norm': '21.504', 'counters/examples': 62144, 'counters/updates': 1942}
skipping logging after 62176 examples to avoid logging too frequently
train stats after 62208 examples: {'rewards_train/chosen': '-1.4021', 'rewards_train/rejected': '-1.4119', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0097693', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-157.7', 'loss/train': '0.86306', 'examples_per_second': '45.507', 'grad_norm': '32.921', 'counters/examples': 62208, 'counters/updates': 1944}
skipping logging after 62240 examples to avoid logging too frequently
train stats after 62272 examples: {'rewards_train/chosen': '-0.7967', 'rewards_train/rejected': '-1.3025', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.50581', 'logps_train/rejected': '-131.02', 'logps_train/chosen': '-137.31', 'loss/train': '0.6659', 'examples_per_second': '46.007', 'grad_norm': '25.633', 'counters/examples': 62272, 'counters/updates': 1946}
skipping logging after 62304 examples to avoid logging too frequently
train stats after 62336 examples: {'rewards_train/chosen': '-0.84124', 'rewards_train/rejected': '-1.6226', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.78133', 'logps_train/rejected': '-157.62', 'logps_train/chosen': '-121.59', 'loss/train': '0.50617', 'examples_per_second': '45.433', 'grad_norm': '22', 'counters/examples': 62336, 'counters/updates': 1948}
skipping logging after 62368 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '-0.87763', 'rewards_train/rejected': '-1.7771', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.89945', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-150.67', 'loss/train': '0.44238', 'examples_per_second': '45.984', 'grad_norm': '21.609', 'counters/examples': 62400, 'counters/updates': 1950}
skipping logging after 62432 examples to avoid logging too frequently
train stats after 62464 examples: {'rewards_train/chosen': '-0.81326', 'rewards_train/rejected': '-1.6581', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.84485', 'logps_train/rejected': '-145.96', 'logps_train/chosen': '-166.16', 'loss/train': '0.46596', 'examples_per_second': '44.56', 'grad_norm': '21.721', 'counters/examples': 62464, 'counters/updates': 1952}
skipping logging after 62496 examples to avoid logging too frequently
train stats after 62528 examples: {'rewards_train/chosen': '-1.0165', 'rewards_train/rejected': '-1.2028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18629', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-153.68', 'loss/train': '0.78772', 'examples_per_second': '46.069', 'grad_norm': '30.937', 'counters/examples': 62528, 'counters/updates': 1954}
skipping logging after 62560 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '-1.3291', 'rewards_train/rejected': '-1.8444', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.51531', 'logps_train/rejected': '-154.52', 'logps_train/chosen': '-167.85', 'loss/train': '0.58049', 'examples_per_second': '45.69', 'grad_norm': '26.251', 'counters/examples': 62592, 'counters/updates': 1956}
skipping logging after 62624 examples to avoid logging too frequently
train stats after 62656 examples: {'rewards_train/chosen': '-1.1613', 'rewards_train/rejected': '-1.6454', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48412', 'logps_train/rejected': '-155.02', 'logps_train/chosen': '-161.67', 'loss/train': '0.63044', 'examples_per_second': '46.746', 'grad_norm': '26.887', 'counters/examples': 62656, 'counters/updates': 1958}
skipping logging after 62688 examples to avoid logging too frequently
train stats after 62720 examples: {'rewards_train/chosen': '-0.97979', 'rewards_train/rejected': '-1.7896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.8098', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-145.56', 'loss/train': '0.58672', 'examples_per_second': '47.184', 'grad_norm': '26.159', 'counters/examples': 62720, 'counters/updates': 1960}
skipping logging after 62752 examples to avoid logging too frequently
train stats after 62784 examples: {'rewards_train/chosen': '-1.2345', 'rewards_train/rejected': '-2.0104', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.77591', 'logps_train/rejected': '-133.84', 'logps_train/chosen': '-141.52', 'loss/train': '0.63517', 'examples_per_second': '46.171', 'grad_norm': '26.061', 'counters/examples': 62784, 'counters/updates': 1962}
skipping logging after 62816 examples to avoid logging too frequently
train stats after 62848 examples: {'rewards_train/chosen': '-0.90087', 'rewards_train/rejected': '-1.6914', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.79052', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-104.08', 'loss/train': '0.49126', 'examples_per_second': '44.415', 'grad_norm': '21.763', 'counters/examples': 62848, 'counters/updates': 1964}
skipping logging after 62880 examples to avoid logging too frequently
train stats after 62912 examples: {'rewards_train/chosen': '-0.84342', 'rewards_train/rejected': '-1.3989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.55546', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-137.34', 'loss/train': '0.57463', 'examples_per_second': '46.603', 'grad_norm': '24.526', 'counters/examples': 62912, 'counters/updates': 1966}
skipping logging after 62944 examples to avoid logging too frequently
train stats after 62976 examples: {'rewards_train/chosen': '-0.96113', 'rewards_train/rejected': '-1.5359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57481', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-170.82', 'loss/train': '0.61457', 'examples_per_second': '45.574', 'grad_norm': '27.364', 'counters/examples': 62976, 'counters/updates': 1968}
skipping logging after 63008 examples to avoid logging too frequently
train stats after 63040 examples: {'rewards_train/chosen': '-0.67622', 'rewards_train/rejected': '-1.2386', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56236', 'logps_train/rejected': '-161.42', 'logps_train/chosen': '-151.3', 'loss/train': '0.59297', 'examples_per_second': '46.685', 'grad_norm': '27.417', 'counters/examples': 63040, 'counters/updates': 1970}
skipping logging after 63072 examples to avoid logging too frequently
train stats after 63104 examples: {'rewards_train/chosen': '-0.74444', 'rewards_train/rejected': '-1.1034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35895', 'logps_train/rejected': '-148.09', 'logps_train/chosen': '-134.88', 'loss/train': '0.68197', 'examples_per_second': '45.152', 'grad_norm': '29.41', 'counters/examples': 63104, 'counters/updates': 1972}
skipping logging after 63136 examples to avoid logging too frequently
train stats after 63168 examples: {'rewards_train/chosen': '-1.083', 'rewards_train/rejected': '-1.5002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.41716', 'logps_train/rejected': '-127.48', 'logps_train/chosen': '-125.72', 'loss/train': '0.70489', 'examples_per_second': '45.314', 'grad_norm': '24.853', 'counters/examples': 63168, 'counters/updates': 1974}
skipping logging after 63200 examples to avoid logging too frequently
train stats after 63232 examples: {'rewards_train/chosen': '-0.80818', 'rewards_train/rejected': '-1.2555', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44735', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-138.14', 'loss/train': '0.60712', 'examples_per_second': '50.071', 'grad_norm': '26.161', 'counters/examples': 63232, 'counters/updates': 1976}
skipping logging after 63264 examples to avoid logging too frequently
train stats after 63296 examples: {'rewards_train/chosen': '-0.50043', 'rewards_train/rejected': '-1.131', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63062', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-151.18', 'loss/train': '0.56888', 'examples_per_second': '46.753', 'grad_norm': '23.578', 'counters/examples': 63296, 'counters/updates': 1978}
skipping logging after 63328 examples to avoid logging too frequently
train stats after 63360 examples: {'rewards_train/chosen': '-0.91711', 'rewards_train/rejected': '-1.3887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.47162', 'logps_train/rejected': '-147.89', 'logps_train/chosen': '-134.72', 'loss/train': '0.70761', 'examples_per_second': '53.562', 'grad_norm': '27.403', 'counters/examples': 63360, 'counters/updates': 1980}
skipping logging after 63392 examples to avoid logging too frequently
train stats after 63424 examples: {'rewards_train/chosen': '-0.45927', 'rewards_train/rejected': '-1.3482', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.88897', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-133.45', 'loss/train': '0.47755', 'examples_per_second': '46.572', 'grad_norm': '19.225', 'counters/examples': 63424, 'counters/updates': 1982}
skipping logging after 63456 examples to avoid logging too frequently
train stats after 63488 examples: {'rewards_train/chosen': '-0.60547', 'rewards_train/rejected': '-0.83046', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.22499', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-142.43', 'loss/train': '0.72394', 'examples_per_second': '44.258', 'grad_norm': '28.885', 'counters/examples': 63488, 'counters/updates': 1984}
skipping logging after 63520 examples to avoid logging too frequently
train stats after 63552 examples: {'rewards_train/chosen': '-0.65757', 'rewards_train/rejected': '-1.278', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.62043', 'logps_train/rejected': '-143.02', 'logps_train/chosen': '-133.6', 'loss/train': '0.53983', 'examples_per_second': '45.2', 'grad_norm': '23.862', 'counters/examples': 63552, 'counters/updates': 1986}
skipping logging after 63584 examples to avoid logging too frequently
train stats after 63616 examples: {'rewards_train/chosen': '-0.1488', 'rewards_train/rejected': '-0.56512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41632', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-180.2', 'loss/train': '0.62879', 'examples_per_second': '45.489', 'grad_norm': '28.195', 'counters/examples': 63616, 'counters/updates': 1988}
skipping logging after 63648 examples to avoid logging too frequently
train stats after 63680 examples: {'rewards_train/chosen': '-0.60472', 'rewards_train/rejected': '-0.98038', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.37566', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-132.81', 'loss/train': '0.61634', 'examples_per_second': '45.099', 'grad_norm': '27.992', 'counters/examples': 63680, 'counters/updates': 1990}
skipping logging after 63712 examples to avoid logging too frequently
train stats after 63744 examples: {'rewards_train/chosen': '-0.60704', 'rewards_train/rejected': '-1.187', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57999', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-191.38', 'loss/train': '0.55441', 'examples_per_second': '46.729', 'grad_norm': '28.542', 'counters/examples': 63744, 'counters/updates': 1992}
skipping logging after 63776 examples to avoid logging too frequently
train stats after 63808 examples: {'rewards_train/chosen': '-0.9288', 'rewards_train/rejected': '-1.2837', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35492', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-188.18', 'loss/train': '0.60652', 'examples_per_second': '47.66', 'grad_norm': '28.623', 'counters/examples': 63808, 'counters/updates': 1994}
skipping logging after 63840 examples to avoid logging too frequently
train stats after 63872 examples: {'rewards_train/chosen': '-1.0036', 'rewards_train/rejected': '-1.2318', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.22828', 'logps_train/rejected': '-150.3', 'logps_train/chosen': '-136.93', 'loss/train': '0.69333', 'examples_per_second': '47.102', 'grad_norm': '31.642', 'counters/examples': 63872, 'counters/updates': 1996}
skipping logging after 63904 examples to avoid logging too frequently
train stats after 63936 examples: {'rewards_train/chosen': '-0.88773', 'rewards_train/rejected': '-1.2144', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32667', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-155.35', 'loss/train': '0.62941', 'examples_per_second': '44.781', 'grad_norm': '27.316', 'counters/examples': 63936, 'counters/updates': 1998}
skipping logging after 63968 examples to avoid logging too frequently
train stats after 64000 examples: {'rewards_train/chosen': '-0.8965', 'rewards_train/rejected': '-1.4963', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59979', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-165.33', 'loss/train': '0.54538', 'examples_per_second': '46.173', 'grad_norm': '23.204', 'counters/examples': 64000, 'counters/updates': 2000}
skipping logging after 64032 examples to avoid logging too frequently
train stats after 64064 examples: {'rewards_train/chosen': '-1.0437', 'rewards_train/rejected': '-1.6126', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5689', 'logps_train/rejected': '-160.41', 'logps_train/chosen': '-164.71', 'loss/train': '0.60623', 'examples_per_second': '45.631', 'grad_norm': '25.101', 'counters/examples': 64064, 'counters/updates': 2002}
skipping logging after 64096 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '-1.4624', 'rewards_train/rejected': '-1.6658', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.20345', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-134.25', 'loss/train': '0.71694', 'examples_per_second': '45.517', 'grad_norm': '27.394', 'counters/examples': 64128, 'counters/updates': 2004}
skipping logging after 64160 examples to avoid logging too frequently
train stats after 64192 examples: {'rewards_train/chosen': '-1.2591', 'rewards_train/rejected': '-1.9763', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71716', 'logps_train/rejected': '-146.71', 'logps_train/chosen': '-150.65', 'loss/train': '0.54452', 'examples_per_second': '45.749', 'grad_norm': '23.232', 'counters/examples': 64192, 'counters/updates': 2006}
skipping logging after 64224 examples to avoid logging too frequently
train stats after 64256 examples: {'rewards_train/chosen': '-1.0003', 'rewards_train/rejected': '-1.745', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.74465', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-129.98', 'loss/train': '0.47041', 'examples_per_second': '48.756', 'grad_norm': '19.578', 'counters/examples': 64256, 'counters/updates': 2008}
skipping logging after 64288 examples to avoid logging too frequently
train stats after 64320 examples: {'rewards_train/chosen': '-1.0843', 'rewards_train/rejected': '-1.657', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57276', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-127.36', 'loss/train': '0.54112', 'examples_per_second': '47.191', 'grad_norm': '24.099', 'counters/examples': 64320, 'counters/updates': 2010}
skipping logging after 64352 examples to avoid logging too frequently
train stats after 64384 examples: {'rewards_train/chosen': '-0.92656', 'rewards_train/rejected': '-1.2183', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29175', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-147.54', 'loss/train': '0.7511', 'examples_per_second': '45.537', 'grad_norm': '30.73', 'counters/examples': 64384, 'counters/updates': 2012}
skipping logging after 64416 examples to avoid logging too frequently
train stats after 64448 examples: {'rewards_train/chosen': '-0.78186', 'rewards_train/rejected': '-1.3985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.61663', 'logps_train/rejected': '-190.62', 'logps_train/chosen': '-194.61', 'loss/train': '0.59496', 'examples_per_second': '46.311', 'grad_norm': '32.982', 'counters/examples': 64448, 'counters/updates': 2014}
skipping logging after 64480 examples to avoid logging too frequently
train stats after 64512 examples: {'rewards_train/chosen': '-0.85686', 'rewards_train/rejected': '-1.492', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63517', 'logps_train/rejected': '-102.54', 'logps_train/chosen': '-121.95', 'loss/train': '0.49605', 'examples_per_second': '33.96', 'grad_norm': '20.904', 'counters/examples': 64512, 'counters/updates': 2016}
skipping logging after 64544 examples to avoid logging too frequently
train stats after 64576 examples: {'rewards_train/chosen': '-0.54835', 'rewards_train/rejected': '-1.4768', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.92843', 'logps_train/rejected': '-149.4', 'logps_train/chosen': '-164.33', 'loss/train': '0.45829', 'examples_per_second': '45.575', 'grad_norm': '24.323', 'counters/examples': 64576, 'counters/updates': 2018}
skipping logging after 64608 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '-0.6701', 'rewards_train/rejected': '-0.93048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.26038', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-128.86', 'loss/train': '0.66144', 'examples_per_second': '45.261', 'grad_norm': '26.652', 'counters/examples': 64640, 'counters/updates': 2020}
skipping logging after 64672 examples to avoid logging too frequently
train stats after 64704 examples: {'rewards_train/chosen': '-0.5538', 'rewards_train/rejected': '-1.1871', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63327', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-105.15', 'loss/train': '0.53407', 'examples_per_second': '49.642', 'grad_norm': '20.629', 'counters/examples': 64704, 'counters/updates': 2022}
skipping logging after 64736 examples to avoid logging too frequently
train stats after 64768 examples: {'rewards_train/chosen': '-0.70729', 'rewards_train/rejected': '-1.2533', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54599', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-127.25', 'loss/train': '0.54904', 'examples_per_second': '48.673', 'grad_norm': '23.001', 'counters/examples': 64768, 'counters/updates': 2024}
skipping logging after 64800 examples to avoid logging too frequently
train stats after 64832 examples: {'rewards_train/chosen': '-0.42721', 'rewards_train/rejected': '-1.2366', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.80942', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-149.07', 'loss/train': '0.54411', 'examples_per_second': '44.686', 'grad_norm': '21.806', 'counters/examples': 64832, 'counters/updates': 2026}
skipping logging after 64864 examples to avoid logging too frequently
train stats after 64896 examples: {'rewards_train/chosen': '-0.99511', 'rewards_train/rejected': '-1.4456', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45048', 'logps_train/rejected': '-187.31', 'logps_train/chosen': '-183.75', 'loss/train': '0.57862', 'examples_per_second': '44.896', 'grad_norm': '27.037', 'counters/examples': 64896, 'counters/updates': 2028}
skipping logging after 64928 examples to avoid logging too frequently
train stats after 64960 examples: {'rewards_train/chosen': '-0.46422', 'rewards_train/rejected': '-0.92997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.46575', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-117.07', 'loss/train': '0.62335', 'examples_per_second': '45.373', 'grad_norm': '25.993', 'counters/examples': 64960, 'counters/updates': 2030}
skipping logging after 64992 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '-1.1333', 'rewards_train/rejected': '-1.3434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21012', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-131.29', 'loss/train': '0.68659', 'examples_per_second': '44.376', 'grad_norm': '25.513', 'counters/examples': 65024, 'counters/updates': 2032}
skipping logging after 65056 examples to avoid logging too frequently
train stats after 65088 examples: {'rewards_train/chosen': '-1.0156', 'rewards_train/rejected': '-1.2493', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2337', 'logps_train/rejected': '-146.26', 'logps_train/chosen': '-146.45', 'loss/train': '0.7623', 'examples_per_second': '44.38', 'grad_norm': '31.998', 'counters/examples': 65088, 'counters/updates': 2034}
skipping logging after 65120 examples to avoid logging too frequently
train stats after 65152 examples: {'rewards_train/chosen': '-0.65129', 'rewards_train/rejected': '-1.5918', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.94047', 'logps_train/rejected': '-114.24', 'logps_train/chosen': '-132.36', 'loss/train': '0.42422', 'examples_per_second': '45.41', 'grad_norm': '17.705', 'counters/examples': 65152, 'counters/updates': 2036}
skipping logging after 65184 examples to avoid logging too frequently
train stats after 65216 examples: {'rewards_train/chosen': '-0.6344', 'rewards_train/rejected': '-1.3219', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.68746', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-159.86', 'loss/train': '0.55151', 'examples_per_second': '44.558', 'grad_norm': '26.655', 'counters/examples': 65216, 'counters/updates': 2038}
skipping logging after 65248 examples to avoid logging too frequently
train stats after 65280 examples: {'rewards_train/chosen': '-1.1485', 'rewards_train/rejected': '-1.7897', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64129', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-132.65', 'loss/train': '0.62333', 'examples_per_second': '45.508', 'grad_norm': '26.279', 'counters/examples': 65280, 'counters/updates': 2040}
skipping logging after 65312 examples to avoid logging too frequently
train stats after 65344 examples: {'rewards_train/chosen': '-1.5782', 'rewards_train/rejected': '-1.9043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.326', 'logps_train/rejected': '-129.29', 'logps_train/chosen': '-175.27', 'loss/train': '0.6866', 'examples_per_second': '44.318', 'grad_norm': '30.466', 'counters/examples': 65344, 'counters/updates': 2042}
skipping logging after 65376 examples to avoid logging too frequently
train stats after 65408 examples: {'rewards_train/chosen': '-1.224', 'rewards_train/rejected': '-1.9208', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.69684', 'logps_train/rejected': '-160.42', 'logps_train/chosen': '-190.59', 'loss/train': '0.57746', 'examples_per_second': '45.59', 'grad_norm': '33.255', 'counters/examples': 65408, 'counters/updates': 2044}
skipping logging after 65440 examples to avoid logging too frequently
train stats after 65472 examples: {'rewards_train/chosen': '-0.76094', 'rewards_train/rejected': '-1.5307', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.76977', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-174.28', 'loss/train': '0.51977', 'examples_per_second': '44.534', 'grad_norm': '26.225', 'counters/examples': 65472, 'counters/updates': 2046}
skipping logging after 65504 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '-1.2715', 'rewards_train/rejected': '-1.7891', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.51765', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-131.29', 'loss/train': '0.57407', 'examples_per_second': '46.009', 'grad_norm': '23.251', 'counters/examples': 65536, 'counters/updates': 2048}
skipping logging after 65568 examples to avoid logging too frequently
train stats after 65600 examples: {'rewards_train/chosen': '-1.1591', 'rewards_train/rejected': '-1.9514', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.79228', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-150.49', 'loss/train': '0.58222', 'examples_per_second': '45.593', 'grad_norm': '28.354', 'counters/examples': 65600, 'counters/updates': 2050}
skipping logging after 65632 examples to avoid logging too frequently
train stats after 65664 examples: {'rewards_train/chosen': '-0.80353', 'rewards_train/rejected': '-1.6702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.86663', 'logps_train/rejected': '-143.77', 'logps_train/chosen': '-181.66', 'loss/train': '0.53916', 'examples_per_second': '45.838', 'grad_norm': '24.295', 'counters/examples': 65664, 'counters/updates': 2052}
skipping logging after 65696 examples to avoid logging too frequently
train stats after 65728 examples: {'rewards_train/chosen': '-1.0413', 'rewards_train/rejected': '-1.6157', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.57444', 'logps_train/rejected': '-149.4', 'logps_train/chosen': '-151.84', 'loss/train': '0.52551', 'examples_per_second': '46.954', 'grad_norm': '23.731', 'counters/examples': 65728, 'counters/updates': 2054}
skipping logging after 65760 examples to avoid logging too frequently
train stats after 65792 examples: {'rewards_train/chosen': '-0.92402', 'rewards_train/rejected': '-1.5043', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58026', 'logps_train/rejected': '-116', 'logps_train/chosen': '-127.55', 'loss/train': '0.57374', 'examples_per_second': '46.858', 'grad_norm': '23.091', 'counters/examples': 65792, 'counters/updates': 2056}
skipping logging after 65824 examples to avoid logging too frequently
train stats after 65856 examples: {'rewards_train/chosen': '-0.96563', 'rewards_train/rejected': '-1.4904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52473', 'logps_train/rejected': '-155.18', 'logps_train/chosen': '-147.59', 'loss/train': '0.61858', 'examples_per_second': '44.684', 'grad_norm': '26.538', 'counters/examples': 65856, 'counters/updates': 2058}
skipping logging after 65888 examples to avoid logging too frequently
train stats after 65920 examples: {'rewards_train/chosen': '-0.82297', 'rewards_train/rejected': '-1.7368', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.91379', 'logps_train/rejected': '-172.41', 'logps_train/chosen': '-158.57', 'loss/train': '0.53454', 'examples_per_second': '45.584', 'grad_norm': '25.702', 'counters/examples': 65920, 'counters/updates': 2060}
skipping logging after 65952 examples to avoid logging too frequently
train stats after 65984 examples: {'rewards_train/chosen': '-0.6627', 'rewards_train/rejected': '-1.364', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.70131', 'logps_train/rejected': '-145.62', 'logps_train/chosen': '-143.22', 'loss/train': '0.56526', 'examples_per_second': '44.381', 'grad_norm': '23.849', 'counters/examples': 65984, 'counters/updates': 2062}
skipping logging after 66016 examples to avoid logging too frequently
train stats after 66048 examples: {'rewards_train/chosen': '-1.1925', 'rewards_train/rejected': '-1.9962', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.80369', 'logps_train/rejected': '-157.97', 'logps_train/chosen': '-167.43', 'loss/train': '0.47939', 'examples_per_second': '45.618', 'grad_norm': '25.513', 'counters/examples': 66048, 'counters/updates': 2064}
skipping logging after 66080 examples to avoid logging too frequently
train stats after 66112 examples: {'rewards_train/chosen': '-1.6037', 'rewards_train/rejected': '-2.1852', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.58147', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-132.81', 'loss/train': '0.63701', 'examples_per_second': '45.439', 'grad_norm': '28.487', 'counters/examples': 66112, 'counters/updates': 2066}
skipping logging after 66144 examples to avoid logging too frequently
train stats after 66176 examples: {'rewards_train/chosen': '-1.1423', 'rewards_train/rejected': '-1.6665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52417', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-145.33', 'loss/train': '0.60598', 'examples_per_second': '52.41', 'grad_norm': '27.027', 'counters/examples': 66176, 'counters/updates': 2068}
skipping logging after 66208 examples to avoid logging too frequently
train stats after 66240 examples: {'rewards_train/chosen': '-0.98986', 'rewards_train/rejected': '-2.0292', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.0394', 'logps_train/rejected': '-157.76', 'logps_train/chosen': '-150.11', 'loss/train': '0.40779', 'examples_per_second': '45.781', 'grad_norm': '20.42', 'counters/examples': 66240, 'counters/updates': 2070}
skipping logging after 66272 examples to avoid logging too frequently
train stats after 66304 examples: {'rewards_train/chosen': '-1.0044', 'rewards_train/rejected': '-1.5865', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58215', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-133.66', 'loss/train': '0.64379', 'examples_per_second': '44.097', 'grad_norm': '26.262', 'counters/examples': 66304, 'counters/updates': 2072}
skipping logging after 66336 examples to avoid logging too frequently
train stats after 66368 examples: {'rewards_train/chosen': '-0.9461', 'rewards_train/rejected': '-1.6379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69184', 'logps_train/rejected': '-157.22', 'logps_train/chosen': '-186.11', 'loss/train': '0.5641', 'examples_per_second': '46.427', 'grad_norm': '24.749', 'counters/examples': 66368, 'counters/updates': 2074}
skipping logging after 66400 examples to avoid logging too frequently
train stats after 66432 examples: {'rewards_train/chosen': '-1.0508', 'rewards_train/rejected': '-1.36', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30918', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-137.25', 'loss/train': '0.77828', 'examples_per_second': '45.902', 'grad_norm': '28.856', 'counters/examples': 66432, 'counters/updates': 2076}
skipping logging after 66464 examples to avoid logging too frequently
train stats after 66496 examples: {'rewards_train/chosen': '-0.73622', 'rewards_train/rejected': '-1.5296', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.79339', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-136.27', 'loss/train': '0.49146', 'examples_per_second': '45.523', 'grad_norm': '23.299', 'counters/examples': 66496, 'counters/updates': 2078}
skipping logging after 66528 examples to avoid logging too frequently
train stats after 66560 examples: {'rewards_train/chosen': '-0.56552', 'rewards_train/rejected': '-1.6271', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.0615', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-131.22', 'loss/train': '0.44367', 'examples_per_second': '44.582', 'grad_norm': '19.121', 'counters/examples': 66560, 'counters/updates': 2080}
skipping logging after 66592 examples to avoid logging too frequently
train stats after 66624 examples: {'rewards_train/chosen': '-1.1697', 'rewards_train/rejected': '-1.5424', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37273', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-128.2', 'loss/train': '0.63792', 'examples_per_second': '49.915', 'grad_norm': '24.124', 'counters/examples': 66624, 'counters/updates': 2082}
skipping logging after 66656 examples to avoid logging too frequently
train stats after 66688 examples: {'rewards_train/chosen': '-0.97692', 'rewards_train/rejected': '-1.3304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.35346', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-176.88', 'loss/train': '0.75659', 'examples_per_second': '47.051', 'grad_norm': '33.409', 'counters/examples': 66688, 'counters/updates': 2084}
skipping logging after 66720 examples to avoid logging too frequently
train stats after 66752 examples: {'rewards_train/chosen': '-0.79911', 'rewards_train/rejected': '-1.3214', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5223', 'logps_train/rejected': '-174', 'logps_train/chosen': '-169.82', 'loss/train': '0.55077', 'examples_per_second': '45.387', 'grad_norm': '27.64', 'counters/examples': 66752, 'counters/updates': 2086}
skipping logging after 66784 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '-0.67695', 'rewards_train/rejected': '-1.6543', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.9773', 'logps_train/rejected': '-166.52', 'logps_train/chosen': '-156.1', 'loss/train': '0.41136', 'examples_per_second': '45.425', 'grad_norm': '23.153', 'counters/examples': 66816, 'counters/updates': 2088}
skipping logging after 66848 examples to avoid logging too frequently
train stats after 66880 examples: {'rewards_train/chosen': '-0.76722', 'rewards_train/rejected': '-1.175', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.40779', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-134.73', 'loss/train': '0.59345', 'examples_per_second': '45.329', 'grad_norm': '24.785', 'counters/examples': 66880, 'counters/updates': 2090}
skipping logging after 66912 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '-0.82738', 'rewards_train/rejected': '-1.4149', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58749', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-141.37', 'loss/train': '0.58491', 'examples_per_second': '46.565', 'grad_norm': '25.01', 'counters/examples': 66944, 'counters/updates': 2092}
skipping logging after 66976 examples to avoid logging too frequently
train stats after 67008 examples: {'rewards_train/chosen': '-0.61236', 'rewards_train/rejected': '-1.3066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69422', 'logps_train/rejected': '-139.01', 'logps_train/chosen': '-143.65', 'loss/train': '0.52347', 'examples_per_second': '48.511', 'grad_norm': '23.107', 'counters/examples': 67008, 'counters/updates': 2094}
skipping logging after 67040 examples to avoid logging too frequently
train stats after 67072 examples: {'rewards_train/chosen': '-0.68991', 'rewards_train/rejected': '-0.93872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24881', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-115.63', 'loss/train': '0.66555', 'examples_per_second': '45.271', 'grad_norm': '23.877', 'counters/examples': 67072, 'counters/updates': 2096}
skipping logging after 67104 examples to avoid logging too frequently
train stats after 67136 examples: {'rewards_train/chosen': '-0.42087', 'rewards_train/rejected': '-1.0552', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63432', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-146', 'loss/train': '0.56138', 'examples_per_second': '44.546', 'grad_norm': '25.681', 'counters/examples': 67136, 'counters/updates': 2098}
skipping logging after 67168 examples to avoid logging too frequently
train stats after 67200 examples: {'rewards_train/chosen': '-0.76594', 'rewards_train/rejected': '-1.396', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63006', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-114.27', 'loss/train': '0.54939', 'examples_per_second': '44.481', 'grad_norm': '22.42', 'counters/examples': 67200, 'counters/updates': 2100}
skipping logging after 67232 examples to avoid logging too frequently
train stats after 67264 examples: {'rewards_train/chosen': '-0.5529', 'rewards_train/rejected': '-0.99315', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44025', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-131.57', 'loss/train': '0.60218', 'examples_per_second': '45.269', 'grad_norm': '26.493', 'counters/examples': 67264, 'counters/updates': 2102}
skipping logging after 67296 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '-0.72034', 'rewards_train/rejected': '-0.98603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.26569', 'logps_train/rejected': '-148.13', 'logps_train/chosen': '-126.78', 'loss/train': '0.70056', 'examples_per_second': '45.837', 'grad_norm': '28.885', 'counters/examples': 67328, 'counters/updates': 2104}
skipping logging after 67360 examples to avoid logging too frequently
train stats after 67392 examples: {'rewards_train/chosen': '-0.67686', 'rewards_train/rejected': '-0.91146', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.2346', 'logps_train/rejected': '-107.18', 'logps_train/chosen': '-124.03', 'loss/train': '0.74733', 'examples_per_second': '51.653', 'grad_norm': '27.087', 'counters/examples': 67392, 'counters/updates': 2106}
skipping logging after 67424 examples to avoid logging too frequently
train stats after 67456 examples: {'rewards_train/chosen': '-0.47821', 'rewards_train/rejected': '-0.83066', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.35244', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-128.82', 'loss/train': '0.64484', 'examples_per_second': '48.734', 'grad_norm': '26.808', 'counters/examples': 67456, 'counters/updates': 2108}
skipping logging after 67488 examples to avoid logging too frequently
train stats after 67520 examples: {'rewards_train/chosen': '-0.99066', 'rewards_train/rejected': '-1.1649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17422', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-135.8', 'loss/train': '0.74147', 'examples_per_second': '45.396', 'grad_norm': '28.844', 'counters/examples': 67520, 'counters/updates': 2110}
skipping logging after 67552 examples to avoid logging too frequently
train stats after 67584 examples: {'rewards_train/chosen': '-1.0751', 'rewards_train/rejected': '-1.6453', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5702', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-129.48', 'loss/train': '0.56068', 'examples_per_second': '46.067', 'grad_norm': '24.568', 'counters/examples': 67584, 'counters/updates': 2112}
skipping logging after 67616 examples to avoid logging too frequently
train stats after 67648 examples: {'rewards_train/chosen': '-0.87772', 'rewards_train/rejected': '-1.2254', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3477', 'logps_train/rejected': '-100', 'logps_train/chosen': '-157.76', 'loss/train': '0.5992', 'examples_per_second': '45.895', 'grad_norm': '25.161', 'counters/examples': 67648, 'counters/updates': 2114}
skipping logging after 67680 examples to avoid logging too frequently
train stats after 67712 examples: {'rewards_train/chosen': '-0.83958', 'rewards_train/rejected': '-1.6921', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.85247', 'logps_train/rejected': '-173.27', 'logps_train/chosen': '-167.24', 'loss/train': '0.54128', 'examples_per_second': '45.544', 'grad_norm': '28.343', 'counters/examples': 67712, 'counters/updates': 2116}
skipping logging after 67744 examples to avoid logging too frequently
train stats after 67776 examples: {'rewards_train/chosen': '-0.79984', 'rewards_train/rejected': '-1.352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55212', 'logps_train/rejected': '-143.13', 'logps_train/chosen': '-130.57', 'loss/train': '0.60402', 'examples_per_second': '45.397', 'grad_norm': '25.52', 'counters/examples': 67776, 'counters/updates': 2118}
skipping logging after 67808 examples to avoid logging too frequently
train stats after 67840 examples: {'rewards_train/chosen': '-0.93396', 'rewards_train/rejected': '-1.1383', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20434', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-117.24', 'loss/train': '0.70647', 'examples_per_second': '45.85', 'grad_norm': '29.428', 'counters/examples': 67840, 'counters/updates': 2120}
skipping logging after 67872 examples to avoid logging too frequently
train stats after 67904 examples: {'rewards_train/chosen': '-0.62349', 'rewards_train/rejected': '-1.3449', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.72141', 'logps_train/rejected': '-100.37', 'logps_train/chosen': '-115.9', 'loss/train': '0.48371', 'examples_per_second': '48.451', 'grad_norm': '19.431', 'counters/examples': 67904, 'counters/updates': 2122}
skipping logging after 67936 examples to avoid logging too frequently
train stats after 67968 examples: {'rewards_train/chosen': '-0.92069', 'rewards_train/rejected': '-1.1433', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2226', 'logps_train/rejected': '-117.7', 'logps_train/chosen': '-217.38', 'loss/train': '0.73333', 'examples_per_second': '45.292', 'grad_norm': '31.506', 'counters/examples': 67968, 'counters/updates': 2124}
skipping logging after 68000 examples to avoid logging too frequently
train stats after 68032 examples: {'rewards_train/chosen': '-0.77519', 'rewards_train/rejected': '-1.4854', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71024', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-147.54', 'loss/train': '0.49858', 'examples_per_second': '45.462', 'grad_norm': '24.779', 'counters/examples': 68032, 'counters/updates': 2126}
skipping logging after 68064 examples to avoid logging too frequently
train stats after 68096 examples: {'rewards_train/chosen': '-0.9351', 'rewards_train/rejected': '-1.4543', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51921', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-149.94', 'loss/train': '0.60164', 'examples_per_second': '44.302', 'grad_norm': '26.872', 'counters/examples': 68096, 'counters/updates': 2128}
skipping logging after 68128 examples to avoid logging too frequently
train stats after 68160 examples: {'rewards_train/chosen': '-1.2983', 'rewards_train/rejected': '-2.1627', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.86441', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-165.68', 'loss/train': '0.45438', 'examples_per_second': '46.815', 'grad_norm': '20.021', 'counters/examples': 68160, 'counters/updates': 2130}
skipping logging after 68192 examples to avoid logging too frequently
train stats after 68224 examples: {'rewards_train/chosen': '-0.87328', 'rewards_train/rejected': '-1.6042', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.73095', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-136.68', 'loss/train': '0.47998', 'examples_per_second': '44.741', 'grad_norm': '21.58', 'counters/examples': 68224, 'counters/updates': 2132}
skipping logging after 68256 examples to avoid logging too frequently
train stats after 68288 examples: {'rewards_train/chosen': '-1.0914', 'rewards_train/rejected': '-1.6258', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53441', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-145.78', 'loss/train': '0.67655', 'examples_per_second': '46.204', 'grad_norm': '30.036', 'counters/examples': 68288, 'counters/updates': 2134}
skipping logging after 68320 examples to avoid logging too frequently
train stats after 68352 examples: {'rewards_train/chosen': '-0.77091', 'rewards_train/rejected': '-1.5844', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.81347', 'logps_train/rejected': '-154.57', 'logps_train/chosen': '-150.09', 'loss/train': '0.48904', 'examples_per_second': '45.225', 'grad_norm': '21.623', 'counters/examples': 68352, 'counters/updates': 2136}
skipping logging after 68384 examples to avoid logging too frequently
train stats after 68416 examples: {'rewards_train/chosen': '-1.0571', 'rewards_train/rejected': '-1.3281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.27104', 'logps_train/rejected': '-149.67', 'logps_train/chosen': '-182.14', 'loss/train': '0.65043', 'examples_per_second': '45.343', 'grad_norm': '28.893', 'counters/examples': 68416, 'counters/updates': 2138}
skipping logging after 68448 examples to avoid logging too frequently
train stats after 68480 examples: {'rewards_train/chosen': '-1.0844', 'rewards_train/rejected': '-1.5242', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43979', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-127.41', 'loss/train': '0.58628', 'examples_per_second': '47.569', 'grad_norm': '24.6', 'counters/examples': 68480, 'counters/updates': 2140}
skipping logging after 68512 examples to avoid logging too frequently
train stats after 68544 examples: {'rewards_train/chosen': '-0.87556', 'rewards_train/rejected': '-1.5666', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69105', 'logps_train/rejected': '-140.74', 'logps_train/chosen': '-143.67', 'loss/train': '0.53431', 'examples_per_second': '52.586', 'grad_norm': '22.244', 'counters/examples': 68544, 'counters/updates': 2142}
skipping logging after 68576 examples to avoid logging too frequently
train stats after 68608 examples: {'rewards_train/chosen': '-0.71559', 'rewards_train/rejected': '-1.4915', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.77587', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-159.89', 'loss/train': '0.53088', 'examples_per_second': '45.429', 'grad_norm': '23.28', 'counters/examples': 68608, 'counters/updates': 2144}
skipping logging after 68640 examples to avoid logging too frequently
train stats after 68672 examples: {'rewards_train/chosen': '-0.82633', 'rewards_train/rejected': '-1.7165', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.89018', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-125.27', 'loss/train': '0.41459', 'examples_per_second': '44.646', 'grad_norm': '18.647', 'counters/examples': 68672, 'counters/updates': 2146}
skipping logging after 68704 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '-0.74412', 'rewards_train/rejected': '-1.6583', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.91415', 'logps_train/rejected': '-151.75', 'logps_train/chosen': '-176.07', 'loss/train': '0.44026', 'examples_per_second': '45.588', 'grad_norm': '22.105', 'counters/examples': 68736, 'counters/updates': 2148}
skipping logging after 68768 examples to avoid logging too frequently
train stats after 68800 examples: {'rewards_train/chosen': '-0.96308', 'rewards_train/rejected': '-1.229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26594', 'logps_train/rejected': '-160.11', 'logps_train/chosen': '-142.12', 'loss/train': '0.72521', 'examples_per_second': '46.729', 'grad_norm': '31.172', 'counters/examples': 68800, 'counters/updates': 2150}
skipping logging after 68832 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '-0.78895', 'rewards_train/rejected': '-1.2954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.50646', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-148.95', 'loss/train': '0.59983', 'examples_per_second': '44.617', 'grad_norm': '27.255', 'counters/examples': 68864, 'counters/updates': 2152}
skipping logging after 68896 examples to avoid logging too frequently
train stats after 68928 examples: {'rewards_train/chosen': '-0.81665', 'rewards_train/rejected': '-1.2536', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43696', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-172.56', 'loss/train': '0.58472', 'examples_per_second': '45.395', 'grad_norm': '23.947', 'counters/examples': 68928, 'counters/updates': 2154}
skipping logging after 68960 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '-0.66864', 'rewards_train/rejected': '-0.86872', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20008', 'logps_train/rejected': '-158.89', 'logps_train/chosen': '-159.15', 'loss/train': '0.72205', 'examples_per_second': '45.636', 'grad_norm': '30.737', 'counters/examples': 68992, 'counters/updates': 2156}
skipping logging after 69024 examples to avoid logging too frequently
train stats after 69056 examples: {'rewards_train/chosen': '-0.51834', 'rewards_train/rejected': '-1.2377', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.71935', 'logps_train/rejected': '-119.66', 'logps_train/chosen': '-125.56', 'loss/train': '0.48982', 'examples_per_second': '46.239', 'grad_norm': '20.295', 'counters/examples': 69056, 'counters/updates': 2158}
skipping logging after 69088 examples to avoid logging too frequently
train stats after 69120 examples: {'rewards_train/chosen': '-0.81578', 'rewards_train/rejected': '-1.3814', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56558', 'logps_train/rejected': '-130.16', 'logps_train/chosen': '-127.01', 'loss/train': '0.65069', 'examples_per_second': '44.654', 'grad_norm': '27.341', 'counters/examples': 69120, 'counters/updates': 2160}
skipping logging after 69152 examples to avoid logging too frequently
train stats after 69184 examples: {'rewards_train/chosen': '-0.57119', 'rewards_train/rejected': '-1.3382', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.76698', 'logps_train/rejected': '-187.35', 'logps_train/chosen': '-194.81', 'loss/train': '0.46327', 'examples_per_second': '45.586', 'grad_norm': '24.54', 'counters/examples': 69184, 'counters/updates': 2162}
skipping logging after 69216 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '-0.64709', 'rewards_train/rejected': '-1.4418', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.79475', 'logps_train/rejected': '-147.4', 'logps_train/chosen': '-105.54', 'loss/train': '0.48877', 'examples_per_second': '44.688', 'grad_norm': '19.816', 'counters/examples': 69248, 'counters/updates': 2164}
skipping logging after 69280 examples to avoid logging too frequently
train stats after 69312 examples: {'rewards_train/chosen': '-0.97928', 'rewards_train/rejected': '-1.1093', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13003', 'logps_train/rejected': '-184.07', 'logps_train/chosen': '-186.4', 'loss/train': '0.73521', 'examples_per_second': '45.446', 'grad_norm': '30.954', 'counters/examples': 69312, 'counters/updates': 2166}
skipping logging after 69344 examples to avoid logging too frequently
train stats after 69376 examples: {'rewards_train/chosen': '-0.66829', 'rewards_train/rejected': '-1.2508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5825', 'logps_train/rejected': '-123.44', 'logps_train/chosen': '-113.35', 'loss/train': '0.58803', 'examples_per_second': '46.219', 'grad_norm': '21.808', 'counters/examples': 69376, 'counters/updates': 2168}
skipping logging after 69408 examples to avoid logging too frequently
train stats after 69440 examples: {'rewards_train/chosen': '-0.53219', 'rewards_train/rejected': '-0.88256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35037', 'logps_train/rejected': '-143.18', 'logps_train/chosen': '-131.72', 'loss/train': '0.61468', 'examples_per_second': '44.318', 'grad_norm': '26.425', 'counters/examples': 69440, 'counters/updates': 2170}
skipping logging after 69472 examples to avoid logging too frequently
train stats after 69504 examples: {'rewards_train/chosen': '-0.49387', 'rewards_train/rejected': '-0.96413', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.47026', 'logps_train/rejected': '-116.7', 'logps_train/chosen': '-127.55', 'loss/train': '0.60188', 'examples_per_second': '47.854', 'grad_norm': '21.819', 'counters/examples': 69504, 'counters/updates': 2172}
skipping logging after 69536 examples to avoid logging too frequently
train stats after 69568 examples: {'rewards_train/chosen': '-0.89368', 'rewards_train/rejected': '-1.5236', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.62996', 'logps_train/rejected': '-163.04', 'logps_train/chosen': '-151.99', 'loss/train': '0.56687', 'examples_per_second': '45.34', 'grad_norm': '27.254', 'counters/examples': 69568, 'counters/updates': 2174}
skipping logging after 69600 examples to avoid logging too frequently
train stats after 69632 examples: {'rewards_train/chosen': '-0.65816', 'rewards_train/rejected': '-1.366', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '0.70781', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-140.1', 'loss/train': '0.47429', 'examples_per_second': '52.206', 'grad_norm': '21.338', 'counters/examples': 69632, 'counters/updates': 2176}
skipping logging after 69664 examples to avoid logging too frequently
train stats after 69696 examples: {'rewards_train/chosen': '-0.76085', 'rewards_train/rejected': '-1.2845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.52366', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-133', 'loss/train': '0.58045', 'examples_per_second': '47.592', 'grad_norm': '23.523', 'counters/examples': 69696, 'counters/updates': 2178}
skipping logging after 69728 examples to avoid logging too frequently
train stats after 69760 examples: {'rewards_train/chosen': '-1.1011', 'rewards_train/rejected': '-1.642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54095', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-115.42', 'loss/train': '0.6', 'examples_per_second': '47.459', 'grad_norm': '25.669', 'counters/examples': 69760, 'counters/updates': 2180}
skipping logging after 69792 examples to avoid logging too frequently
train stats after 69824 examples: {'rewards_train/chosen': '-0.9083', 'rewards_train/rejected': '-1.3421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4338', 'logps_train/rejected': '-143.78', 'logps_train/chosen': '-153.61', 'loss/train': '0.55928', 'examples_per_second': '45.403', 'grad_norm': '25.674', 'counters/examples': 69824, 'counters/updates': 2182}
skipping logging after 69856 examples to avoid logging too frequently
train stats after 69888 examples: {'rewards_train/chosen': '-1.0499', 'rewards_train/rejected': '-1.7065', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.65664', 'logps_train/rejected': '-164.26', 'logps_train/chosen': '-141.59', 'loss/train': '0.54032', 'examples_per_second': '48.527', 'grad_norm': '24.62', 'counters/examples': 69888, 'counters/updates': 2184}
skipping logging after 69920 examples to avoid logging too frequently
train stats after 69952 examples: {'rewards_train/chosen': '-0.91465', 'rewards_train/rejected': '-1.7855', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.87086', 'logps_train/rejected': '-140.14', 'logps_train/chosen': '-153.03', 'loss/train': '0.50916', 'examples_per_second': '44.133', 'grad_norm': '25.619', 'counters/examples': 69952, 'counters/updates': 2186}
skipping logging after 69984 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '-1.0271', 'rewards_train/rejected': '-1.3276', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30052', 'logps_train/rejected': '-102.38', 'logps_train/chosen': '-118.8', 'loss/train': '0.6663', 'examples_per_second': '46.208', 'grad_norm': '22.987', 'counters/examples': 70016, 'counters/updates': 2188}
skipping logging after 70048 examples to avoid logging too frequently
train stats after 70080 examples: {'rewards_train/chosen': '-0.95667', 'rewards_train/rejected': '-1.7072', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.75053', 'logps_train/rejected': '-125.19', 'logps_train/chosen': '-160.59', 'loss/train': '0.52221', 'examples_per_second': '47.446', 'grad_norm': '22.625', 'counters/examples': 70080, 'counters/updates': 2190}
skipping logging after 70112 examples to avoid logging too frequently
train stats after 70144 examples: {'rewards_train/chosen': '-0.7287', 'rewards_train/rejected': '-1.5617', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.83296', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-186.61', 'loss/train': '0.42669', 'examples_per_second': '44.052', 'grad_norm': '21.441', 'counters/examples': 70144, 'counters/updates': 2192}
skipping logging after 70176 examples to avoid logging too frequently
train stats after 70208 examples: {'rewards_train/chosen': '-1.0923', 'rewards_train/rejected': '-1.8459', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.75363', 'logps_train/rejected': '-161.06', 'logps_train/chosen': '-132.07', 'loss/train': '0.48265', 'examples_per_second': '44.99', 'grad_norm': '22.078', 'counters/examples': 70208, 'counters/updates': 2194}
skipping logging after 70240 examples to avoid logging too frequently
train stats after 70272 examples: {'rewards_train/chosen': '-1.044', 'rewards_train/rejected': '-1.6613', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61728', 'logps_train/rejected': '-141.91', 'logps_train/chosen': '-133.89', 'loss/train': '0.55492', 'examples_per_second': '46.394', 'grad_norm': '22.729', 'counters/examples': 70272, 'counters/updates': 2196}
skipping logging after 70304 examples to avoid logging too frequently
train stats after 70336 examples: {'rewards_train/chosen': '-1.141', 'rewards_train/rejected': '-1.9912', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.8502', 'logps_train/rejected': '-146.68', 'logps_train/chosen': '-157.51', 'loss/train': '0.49943', 'examples_per_second': '49.456', 'grad_norm': '22.976', 'counters/examples': 70336, 'counters/updates': 2198}
skipping logging after 70368 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '-0.62717', 'rewards_train/rejected': '-1.5012', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.87407', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-149.16', 'loss/train': '0.49183', 'examples_per_second': '44.235', 'grad_norm': '22.208', 'counters/examples': 70400, 'counters/updates': 2200}
skipping logging after 70432 examples to avoid logging too frequently
train stats after 70464 examples: {'rewards_train/chosen': '-1.095', 'rewards_train/rejected': '-1.3958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30076', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-152.73', 'loss/train': '0.6551', 'examples_per_second': '44.145', 'grad_norm': '29.911', 'counters/examples': 70464, 'counters/updates': 2202}
skipping logging after 70496 examples to avoid logging too frequently
train stats after 70528 examples: {'rewards_train/chosen': '-0.79129', 'rewards_train/rejected': '-1.42', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.62873', 'logps_train/rejected': '-143.84', 'logps_train/chosen': '-151.75', 'loss/train': '0.61275', 'examples_per_second': '44.824', 'grad_norm': '24.863', 'counters/examples': 70528, 'counters/updates': 2204}
skipping logging after 70560 examples to avoid logging too frequently
train stats after 70592 examples: {'rewards_train/chosen': '-1.0178', 'rewards_train/rejected': '-1.6154', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.59752', 'logps_train/rejected': '-141.64', 'logps_train/chosen': '-143.13', 'loss/train': '0.59871', 'examples_per_second': '46.338', 'grad_norm': '26.942', 'counters/examples': 70592, 'counters/updates': 2206}
skipping logging after 70624 examples to avoid logging too frequently
train stats after 70656 examples: {'rewards_train/chosen': '-1.1413', 'rewards_train/rejected': '-1.8505', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70915', 'logps_train/rejected': '-144.43', 'logps_train/chosen': '-172.8', 'loss/train': '0.53196', 'examples_per_second': '45.051', 'grad_norm': '24.51', 'counters/examples': 70656, 'counters/updates': 2208}
skipping logging after 70688 examples to avoid logging too frequently
train stats after 70720 examples: {'rewards_train/chosen': '-1.0137', 'rewards_train/rejected': '-1.411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39736', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-152.41', 'loss/train': '0.67473', 'examples_per_second': '45.343', 'grad_norm': '29.237', 'counters/examples': 70720, 'counters/updates': 2210}
skipping logging after 70752 examples to avoid logging too frequently
train stats after 70784 examples: {'rewards_train/chosen': '-1.2331', 'rewards_train/rejected': '-1.9943', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.76115', 'logps_train/rejected': '-128.41', 'logps_train/chosen': '-168.57', 'loss/train': '0.51426', 'examples_per_second': '44.797', 'grad_norm': '26.704', 'counters/examples': 70784, 'counters/updates': 2212}
skipping logging after 70816 examples to avoid logging too frequently
train stats after 70848 examples: {'rewards_train/chosen': '-1.3882', 'rewards_train/rejected': '-1.5169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12873', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-155.13', 'loss/train': '0.74938', 'examples_per_second': '44.729', 'grad_norm': '29.533', 'counters/examples': 70848, 'counters/updates': 2214}
skipping logging after 70880 examples to avoid logging too frequently
train stats after 70912 examples: {'rewards_train/chosen': '-1.5417', 'rewards_train/rejected': '-1.993', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.45133', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-157.41', 'loss/train': '0.6015', 'examples_per_second': '46.276', 'grad_norm': '28.298', 'counters/examples': 70912, 'counters/updates': 2216}
skipping logging after 70944 examples to avoid logging too frequently
train stats after 70976 examples: {'rewards_train/chosen': '-1.8866', 'rewards_train/rejected': '-2.1156', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22897', 'logps_train/rejected': '-177.92', 'logps_train/chosen': '-154.03', 'loss/train': '0.71462', 'examples_per_second': '45.506', 'grad_norm': '28.563', 'counters/examples': 70976, 'counters/updates': 2218}
skipping logging after 71008 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '-1.207', 'rewards_train/rejected': '-2.0157', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.8087', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-148.91', 'loss/train': '0.4764', 'examples_per_second': '45.177', 'grad_norm': '23.975', 'counters/examples': 71040, 'counters/updates': 2220}
skipping logging after 71072 examples to avoid logging too frequently
train stats after 71104 examples: {'rewards_train/chosen': '-1.1108', 'rewards_train/rejected': '-1.8212', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.71048', 'logps_train/rejected': '-125.12', 'logps_train/chosen': '-127.2', 'loss/train': '0.52596', 'examples_per_second': '45.748', 'grad_norm': '22.583', 'counters/examples': 71104, 'counters/updates': 2222}
skipping logging after 71136 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '-1.6579', 'rewards_train/rejected': '-2.3197', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6618', 'logps_train/rejected': '-156.51', 'logps_train/chosen': '-145.06', 'loss/train': '0.53913', 'examples_per_second': '47.817', 'grad_norm': '24.685', 'counters/examples': 71168, 'counters/updates': 2224}
skipping logging after 71200 examples to avoid logging too frequently
train stats after 71232 examples: {'rewards_train/chosen': '-1.0755', 'rewards_train/rejected': '-2.0538', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.97822', 'logps_train/rejected': '-104', 'logps_train/chosen': '-147.25', 'loss/train': '0.40242', 'examples_per_second': '46.173', 'grad_norm': '20.971', 'counters/examples': 71232, 'counters/updates': 2226}
skipping logging after 71264 examples to avoid logging too frequently
train stats after 71296 examples: {'rewards_train/chosen': '-1.1682', 'rewards_train/rejected': '-1.6958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52762', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-154.18', 'loss/train': '0.59', 'examples_per_second': '44.399', 'grad_norm': '23.449', 'counters/examples': 71296, 'counters/updates': 2228}
skipping logging after 71328 examples to avoid logging too frequently
train stats after 71360 examples: {'rewards_train/chosen': '-1.3549', 'rewards_train/rejected': '-1.8351', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.48015', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-123.47', 'loss/train': '0.63986', 'examples_per_second': '56.134', 'grad_norm': '26.135', 'counters/examples': 71360, 'counters/updates': 2230}
skipping logging after 71392 examples to avoid logging too frequently
train stats after 71424 examples: {'rewards_train/chosen': '-1.1825', 'rewards_train/rejected': '-1.6373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.45474', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-114.21', 'loss/train': '0.66932', 'examples_per_second': '46.201', 'grad_norm': '26.172', 'counters/examples': 71424, 'counters/updates': 2232}
skipping logging after 71456 examples to avoid logging too frequently
train stats after 71488 examples: {'rewards_train/chosen': '-0.94969', 'rewards_train/rejected': '-1.7289', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.77922', 'logps_train/rejected': '-160.49', 'logps_train/chosen': '-181.08', 'loss/train': '0.50668', 'examples_per_second': '44.787', 'grad_norm': '26.36', 'counters/examples': 71488, 'counters/updates': 2234}
skipping logging after 71520 examples to avoid logging too frequently
train stats after 71552 examples: {'rewards_train/chosen': '-1.3342', 'rewards_train/rejected': '-1.8048', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.4706', 'logps_train/rejected': '-128', 'logps_train/chosen': '-132.96', 'loss/train': '0.77681', 'examples_per_second': '46.35', 'grad_norm': '26.381', 'counters/examples': 71552, 'counters/updates': 2236}
skipping logging after 71584 examples to avoid logging too frequently
train stats after 71616 examples: {'rewards_train/chosen': '-0.83031', 'rewards_train/rejected': '-1.5396', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.70925', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-150.67', 'loss/train': '0.46699', 'examples_per_second': '47.849', 'grad_norm': '22.342', 'counters/examples': 71616, 'counters/updates': 2238}
skipping logging after 71648 examples to avoid logging too frequently
train stats after 71680 examples: {'rewards_train/chosen': '-1.6008', 'rewards_train/rejected': '-2.1545', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.55372', 'logps_train/rejected': '-134.91', 'logps_train/chosen': '-153.54', 'loss/train': '0.62744', 'examples_per_second': '45.476', 'grad_norm': '26.477', 'counters/examples': 71680, 'counters/updates': 2240}
skipping logging after 71712 examples to avoid logging too frequently
train stats after 71744 examples: {'rewards_train/chosen': '-1.547', 'rewards_train/rejected': '-1.8631', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31613', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-190.77', 'loss/train': '0.6958', 'examples_per_second': '46.581', 'grad_norm': '30.716', 'counters/examples': 71744, 'counters/updates': 2242}
skipping logging after 71776 examples to avoid logging too frequently
train stats after 71808 examples: {'rewards_train/chosen': '-1.1651', 'rewards_train/rejected': '-1.5861', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42099', 'logps_train/rejected': '-112.77', 'logps_train/chosen': '-146.82', 'loss/train': '0.57899', 'examples_per_second': '47.7', 'grad_norm': '26.083', 'counters/examples': 71808, 'counters/updates': 2244}
skipping logging after 71840 examples to avoid logging too frequently
train stats after 71872 examples: {'rewards_train/chosen': '-1.3908', 'rewards_train/rejected': '-1.9601', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5693', 'logps_train/rejected': '-140.34', 'logps_train/chosen': '-164.79', 'loss/train': '0.55512', 'examples_per_second': '44.893', 'grad_norm': '24.271', 'counters/examples': 71872, 'counters/updates': 2246}
skipping logging after 71904 examples to avoid logging too frequently
train stats after 71936 examples: {'rewards_train/chosen': '-1.4574', 'rewards_train/rejected': '-2.3276', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.87018', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-169.91', 'loss/train': '0.48808', 'examples_per_second': '45.333', 'grad_norm': '24.159', 'counters/examples': 71936, 'counters/updates': 2248}
skipping logging after 71968 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '-1.1468', 'rewards_train/rejected': '-1.9378', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.79105', 'logps_train/rejected': '-161.75', 'logps_train/chosen': '-148.98', 'loss/train': '0.48606', 'examples_per_second': '46.32', 'grad_norm': '20.945', 'counters/examples': 72000, 'counters/updates': 2250}
Running evaluation after 72000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.21it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.87it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.97it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.97it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.90it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.34it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.17it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.11it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.14it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.03it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.02it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  7.01it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.01it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.92it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.98it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.89it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.00it/s]
eval after 72000: {'rewards_eval/chosen': '-1.1175', 'rewards_eval/rejected': '-1.7075', 'rewards_eval/accuracies': '0.67578', 'rewards_eval/margins': '0.59008', 'logps_eval/rejected': '-135.36', 'logps_eval/chosen': '-147.63', 'loss/eval': '0.60514'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-72000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-72000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-72000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-72000/scheduler.pt...
train stats after 72032 examples: {'rewards_train/chosen': '-1.1962', 'rewards_train/rejected': '-1.8576', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.66139', 'logps_train/rejected': '-181.66', 'logps_train/chosen': '-161.7', 'loss/train': '0.51724', 'examples_per_second': '29.538', 'grad_norm': '25.144', 'counters/examples': 72032, 'counters/updates': 2251}
skipping logging after 72064 examples to avoid logging too frequently
train stats after 72096 examples: {'rewards_train/chosen': '-1.3738', 'rewards_train/rejected': '-1.6309', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.25706', 'logps_train/rejected': '-153.21', 'logps_train/chosen': '-192.27', 'loss/train': '0.74753', 'examples_per_second': '45.745', 'grad_norm': '32.129', 'counters/examples': 72096, 'counters/updates': 2253}
skipping logging after 72128 examples to avoid logging too frequently
train stats after 72160 examples: {'rewards_train/chosen': '-1.6102', 'rewards_train/rejected': '-1.8749', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26475', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-128.84', 'loss/train': '0.70665', 'examples_per_second': '45.774', 'grad_norm': '26.12', 'counters/examples': 72160, 'counters/updates': 2255}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72224 examples: {'rewards_train/chosen': '-1.0824', 'rewards_train/rejected': '-1.9737', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.89137', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-169.32', 'loss/train': '0.41488', 'examples_per_second': '45.133', 'grad_norm': '19.284', 'counters/examples': 72224, 'counters/updates': 2257}
skipping logging after 72256 examples to avoid logging too frequently
train stats after 72288 examples: {'rewards_train/chosen': '-1.493', 'rewards_train/rejected': '-2.3138', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.82077', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-162.7', 'loss/train': '0.50466', 'examples_per_second': '45.613', 'grad_norm': '23.143', 'counters/examples': 72288, 'counters/updates': 2259}
skipping logging after 72320 examples to avoid logging too frequently
train stats after 72352 examples: {'rewards_train/chosen': '-1.4685', 'rewards_train/rejected': '-2.1294', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.66086', 'logps_train/rejected': '-156.84', 'logps_train/chosen': '-132.64', 'loss/train': '0.67505', 'examples_per_second': '49.713', 'grad_norm': '28.736', 'counters/examples': 72352, 'counters/updates': 2261}
skipping logging after 72384 examples to avoid logging too frequently
train stats after 72416 examples: {'rewards_train/chosen': '-0.97866', 'rewards_train/rejected': '-1.6329', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.65429', 'logps_train/rejected': '-125.38', 'logps_train/chosen': '-135.32', 'loss/train': '0.49945', 'examples_per_second': '45.585', 'grad_norm': '21.043', 'counters/examples': 72416, 'counters/updates': 2263}
skipping logging after 72448 examples to avoid logging too frequently
train stats after 72480 examples: {'rewards_train/chosen': '-1.5788', 'rewards_train/rejected': '-1.6521', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073259', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-150.49', 'loss/train': '0.77817', 'examples_per_second': '44.506', 'grad_norm': '28.102', 'counters/examples': 72480, 'counters/updates': 2265}
skipping logging after 72512 examples to avoid logging too frequently
train stats after 72544 examples: {'rewards_train/chosen': '-1.4351', 'rewards_train/rejected': '-2.0517', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.61659', 'logps_train/rejected': '-129.93', 'logps_train/chosen': '-113.63', 'loss/train': '0.58211', 'examples_per_second': '52.2', 'grad_norm': '23.323', 'counters/examples': 72544, 'counters/updates': 2267}
skipping logging after 72576 examples to avoid logging too frequently
train stats after 72608 examples: {'rewards_train/chosen': '-1.3245', 'rewards_train/rejected': '-1.898', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57351', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-136.88', 'loss/train': '0.55346', 'examples_per_second': '46.497', 'grad_norm': '24.824', 'counters/examples': 72608, 'counters/updates': 2269}
skipping logging after 72640 examples to avoid logging too frequently
train stats after 72672 examples: {'rewards_train/chosen': '-1.2951', 'rewards_train/rejected': '-1.576', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28081', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-137.61', 'loss/train': '0.68929', 'examples_per_second': '46.158', 'grad_norm': '27.895', 'counters/examples': 72672, 'counters/updates': 2271}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72736 examples: {'rewards_train/chosen': '-0.97304', 'rewards_train/rejected': '-1.4838', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51074', 'logps_train/rejected': '-148.79', 'logps_train/chosen': '-130.41', 'loss/train': '0.65671', 'examples_per_second': '46.608', 'grad_norm': '27.245', 'counters/examples': 72736, 'counters/updates': 2273}
skipping logging after 72768 examples to avoid logging too frequently
train stats after 72800 examples: {'rewards_train/chosen': '-1.0283', 'rewards_train/rejected': '-1.6067', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.57841', 'logps_train/rejected': '-96.424', 'logps_train/chosen': '-104.31', 'loss/train': '0.59353', 'examples_per_second': '47.853', 'grad_norm': '21.654', 'counters/examples': 72800, 'counters/updates': 2275}
skipping logging after 72832 examples to avoid logging too frequently
train stats after 72864 examples: {'rewards_train/chosen': '-1.1236', 'rewards_train/rejected': '-1.3813', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2577', 'logps_train/rejected': '-143.93', 'logps_train/chosen': '-178.72', 'loss/train': '0.69771', 'examples_per_second': '47.297', 'grad_norm': '29.142', 'counters/examples': 72864, 'counters/updates': 2277}
skipping logging after 72896 examples to avoid logging too frequently
train stats after 72928 examples: {'rewards_train/chosen': '-0.67896', 'rewards_train/rejected': '-1.7446', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.0657', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-131.99', 'loss/train': '0.44316', 'examples_per_second': '46.722', 'grad_norm': '20.001', 'counters/examples': 72928, 'counters/updates': 2279}
skipping logging after 72960 examples to avoid logging too frequently
train stats after 72992 examples: {'rewards_train/chosen': '-0.87198', 'rewards_train/rejected': '-0.96977', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.097792', 'logps_train/rejected': '-185.73', 'logps_train/chosen': '-152.85', 'loss/train': '0.77417', 'examples_per_second': '46.105', 'grad_norm': '34.164', 'counters/examples': 72992, 'counters/updates': 2281}
skipping logging after 73024 examples to avoid logging too frequently
train stats after 73056 examples: {'rewards_train/chosen': '-0.72114', 'rewards_train/rejected': '-1.1077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38658', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-145.57', 'loss/train': '0.67912', 'examples_per_second': '47.663', 'grad_norm': '26.445', 'counters/examples': 73056, 'counters/updates': 2283}
skipping logging after 73088 examples to avoid logging too frequently
train stats after 73120 examples: {'rewards_train/chosen': '-0.66266', 'rewards_train/rejected': '-1.5835', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.92085', 'logps_train/rejected': '-147.74', 'logps_train/chosen': '-127.72', 'loss/train': '0.46447', 'examples_per_second': '45.494', 'grad_norm': '21.11', 'counters/examples': 73120, 'counters/updates': 2285}
skipping logging after 73152 examples to avoid logging too frequently
train stats after 73184 examples: {'rewards_train/chosen': '-0.58404', 'rewards_train/rejected': '-1.1925', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60849', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-116.39', 'loss/train': '0.57052', 'examples_per_second': '46.869', 'grad_norm': '22.295', 'counters/examples': 73184, 'counters/updates': 2287}
skipping logging after 73216 examples to avoid logging too frequently
train stats after 73248 examples: {'rewards_train/chosen': '-0.77579', 'rewards_train/rejected': '-1.1846', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.40883', 'logps_train/rejected': '-148.99', 'logps_train/chosen': '-137.78', 'loss/train': '0.67344', 'examples_per_second': '45.44', 'grad_norm': '31.529', 'counters/examples': 73248, 'counters/updates': 2289}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73312 examples: {'rewards_train/chosen': '-0.64914', 'rewards_train/rejected': '-1.1998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.55065', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-149.66', 'loss/train': '0.6065', 'examples_per_second': '53.128', 'grad_norm': '32.007', 'counters/examples': 73312, 'counters/updates': 2291}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73376 examples: {'rewards_train/chosen': '-0.82742', 'rewards_train/rejected': '-1.7335', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.90609', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-138.27', 'loss/train': '0.43977', 'examples_per_second': '47.785', 'grad_norm': '21.586', 'counters/examples': 73376, 'counters/updates': 2293}
skipping logging after 73408 examples to avoid logging too frequently
train stats after 73440 examples: {'rewards_train/chosen': '-0.76733', 'rewards_train/rejected': '-1.62', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.8527', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-140.59', 'loss/train': '0.51243', 'examples_per_second': '47.271', 'grad_norm': '24.251', 'counters/examples': 73440, 'counters/updates': 2295}
skipping logging after 73472 examples to avoid logging too frequently
train stats after 73504 examples: {'rewards_train/chosen': '-0.74403', 'rewards_train/rejected': '-1.1298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38577', 'logps_train/rejected': '-155.21', 'logps_train/chosen': '-144.08', 'loss/train': '0.57504', 'examples_per_second': '44.535', 'grad_norm': '26.071', 'counters/examples': 73504, 'counters/updates': 2297}
skipping logging after 73536 examples to avoid logging too frequently
train stats after 73568 examples: {'rewards_train/chosen': '-0.9334', 'rewards_train/rejected': '-1.602', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.66858', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-163.64', 'loss/train': '0.49546', 'examples_per_second': '45.128', 'grad_norm': '23.631', 'counters/examples': 73568, 'counters/updates': 2299}
skipping logging after 73600 examples to avoid logging too frequently
train stats after 73632 examples: {'rewards_train/chosen': '-0.59763', 'rewards_train/rejected': '-1.5037', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.90611', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-138.69', 'loss/train': '0.46514', 'examples_per_second': '46.36', 'grad_norm': '19.323', 'counters/examples': 73632, 'counters/updates': 2301}
train stats after 73664 examples: {'rewards_train/chosen': '-0.9474', 'rewards_train/rejected': '-1.5398', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59235', 'logps_train/rejected': '-184.96', 'logps_train/chosen': '-159.47', 'loss/train': '0.62155', 'examples_per_second': '31.545', 'grad_norm': '31.51', 'counters/examples': 73664, 'counters/updates': 2302}
skipping logging after 73696 examples to avoid logging too frequently
train stats after 73728 examples: {'rewards_train/chosen': '-0.67834', 'rewards_train/rejected': '-1.2229', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54455', 'logps_train/rejected': '-153.27', 'logps_train/chosen': '-136.59', 'loss/train': '0.63669', 'examples_per_second': '44.87', 'grad_norm': '27.733', 'counters/examples': 73728, 'counters/updates': 2304}
skipping logging after 73760 examples to avoid logging too frequently
train stats after 73792 examples: {'rewards_train/chosen': '-0.49064', 'rewards_train/rejected': '-1.0903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.59965', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-126.07', 'loss/train': '0.59242', 'examples_per_second': '31.285', 'grad_norm': '24.395', 'counters/examples': 73792, 'counters/updates': 2306}
skipping logging after 73824 examples to avoid logging too frequently
train stats after 73856 examples: {'rewards_train/chosen': '-0.56664', 'rewards_train/rejected': '-1.4296', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.86298', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-133.45', 'loss/train': '0.45882', 'examples_per_second': '45.652', 'grad_norm': '20.985', 'counters/examples': 73856, 'counters/updates': 2308}
skipping logging after 73888 examples to avoid logging too frequently
train stats after 73920 examples: {'rewards_train/chosen': '-0.58271', 'rewards_train/rejected': '-1.1293', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54657', 'logps_train/rejected': '-126.47', 'logps_train/chosen': '-139.53', 'loss/train': '0.56457', 'examples_per_second': '46.268', 'grad_norm': '26.097', 'counters/examples': 73920, 'counters/updates': 2310}
skipping logging after 73952 examples to avoid logging too frequently
train stats after 73984 examples: {'rewards_train/chosen': '-0.5843', 'rewards_train/rejected': '-1.3281', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.74377', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-124.23', 'loss/train': '0.50709', 'examples_per_second': '44.582', 'grad_norm': '19.774', 'counters/examples': 73984, 'counters/updates': 2312}
skipping logging after 74016 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '-0.31402', 'rewards_train/rejected': '-0.90179', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58776', 'logps_train/rejected': '-132.93', 'logps_train/chosen': '-137.16', 'loss/train': '0.63467', 'examples_per_second': '48.654', 'grad_norm': '26.927', 'counters/examples': 74048, 'counters/updates': 2314}
skipping logging after 74080 examples to avoid logging too frequently
train stats after 74112 examples: {'rewards_train/chosen': '-0.56939', 'rewards_train/rejected': '-1.0631', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.49368', 'logps_train/rejected': '-150.43', 'logps_train/chosen': '-148.36', 'loss/train': '0.62156', 'examples_per_second': '43.702', 'grad_norm': '27.049', 'counters/examples': 74112, 'counters/updates': 2316}
skipping logging after 74144 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '-0.57944', 'rewards_train/rejected': '-1.1004', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52097', 'logps_train/rejected': '-98.088', 'logps_train/chosen': '-127.2', 'loss/train': '0.58452', 'examples_per_second': '44.778', 'grad_norm': '22.97', 'counters/examples': 74176, 'counters/updates': 2318}
skipping logging after 74208 examples to avoid logging too frequently
train stats after 74240 examples: {'rewards_train/chosen': '-0.7324', 'rewards_train/rejected': '-1.446', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.71359', 'logps_train/rejected': '-116.7', 'logps_train/chosen': '-158.54', 'loss/train': '0.54631', 'examples_per_second': '45.473', 'grad_norm': '22.307', 'counters/examples': 74240, 'counters/updates': 2320}
skipping logging after 74272 examples to avoid logging too frequently
train stats after 74304 examples: {'rewards_train/chosen': '-0.94564', 'rewards_train/rejected': '-1.697', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.75134', 'logps_train/rejected': '-144.21', 'logps_train/chosen': '-145.94', 'loss/train': '0.48023', 'examples_per_second': '48.338', 'grad_norm': '22.131', 'counters/examples': 74304, 'counters/updates': 2322}
skipping logging after 74336 examples to avoid logging too frequently
train stats after 74368 examples: {'rewards_train/chosen': '-0.87293', 'rewards_train/rejected': '-1.6736', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.80062', 'logps_train/rejected': '-138.74', 'logps_train/chosen': '-168.67', 'loss/train': '0.53391', 'examples_per_second': '46.201', 'grad_norm': '23.334', 'counters/examples': 74368, 'counters/updates': 2324}
skipping logging after 74400 examples to avoid logging too frequently
train stats after 74432 examples: {'rewards_train/chosen': '-1.0295', 'rewards_train/rejected': '-1.4756', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.4461', 'logps_train/rejected': '-152.27', 'logps_train/chosen': '-124.55', 'loss/train': '0.59838', 'examples_per_second': '45.177', 'grad_norm': '23.424', 'counters/examples': 74432, 'counters/updates': 2326}
skipping logging after 74464 examples to avoid logging too frequently
train stats after 74496 examples: {'rewards_train/chosen': '-1.058', 'rewards_train/rejected': '-1.461', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.40304', 'logps_train/rejected': '-152.94', 'logps_train/chosen': '-142', 'loss/train': '0.75472', 'examples_per_second': '45.749', 'grad_norm': '27.231', 'counters/examples': 74496, 'counters/updates': 2328}
skipping logging after 74528 examples to avoid logging too frequently
train stats after 74560 examples: {'rewards_train/chosen': '-0.83983', 'rewards_train/rejected': '-1.5705', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.73065', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-139.48', 'loss/train': '0.51592', 'examples_per_second': '45.611', 'grad_norm': '19.918', 'counters/examples': 74560, 'counters/updates': 2330}
skipping logging after 74592 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '-1.1054', 'rewards_train/rejected': '-1.597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.49155', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-132.21', 'loss/train': '0.62493', 'examples_per_second': '44.647', 'grad_norm': '23.772', 'counters/examples': 74624, 'counters/updates': 2332}
skipping logging after 74656 examples to avoid logging too frequently
train stats after 74688 examples: {'rewards_train/chosen': '-1.1542', 'rewards_train/rejected': '-1.9397', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.78545', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-135.16', 'loss/train': '0.55243', 'examples_per_second': '47.735', 'grad_norm': '22.22', 'counters/examples': 74688, 'counters/updates': 2334}
skipping logging after 74720 examples to avoid logging too frequently
train stats after 74752 examples: {'rewards_train/chosen': '-1.0299', 'rewards_train/rejected': '-2.3123', 'rewards_train/accuracies': '0.90625', 'rewards_train/margins': '1.2824', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-156.93', 'loss/train': '0.32764', 'examples_per_second': '45.3', 'grad_norm': '18.625', 'counters/examples': 74752, 'counters/updates': 2336}
skipping logging after 74784 examples to avoid logging too frequently
train stats after 74816 examples: {'rewards_train/chosen': '-1.008', 'rewards_train/rejected': '-1.6273', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61931', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-130.77', 'loss/train': '0.59712', 'examples_per_second': '45.332', 'grad_norm': '24.054', 'counters/examples': 74816, 'counters/updates': 2338}
skipping logging after 74848 examples to avoid logging too frequently
train stats after 74880 examples: {'rewards_train/chosen': '-1.1622', 'rewards_train/rejected': '-1.5892', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42704', 'logps_train/rejected': '-93.532', 'logps_train/chosen': '-132.78', 'loss/train': '0.60928', 'examples_per_second': '45.21', 'grad_norm': '21.168', 'counters/examples': 74880, 'counters/updates': 2340}
skipping logging after 74912 examples to avoid logging too frequently
train stats after 74944 examples: {'rewards_train/chosen': '-0.95558', 'rewards_train/rejected': '-1.5476', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59201', 'logps_train/rejected': '-169.47', 'logps_train/chosen': '-142.22', 'loss/train': '0.56967', 'examples_per_second': '46.617', 'grad_norm': '25.959', 'counters/examples': 74944, 'counters/updates': 2342}
skipping logging after 74976 examples to avoid logging too frequently
train stats after 75008 examples: {'rewards_train/chosen': '-0.98395', 'rewards_train/rejected': '-1.4456', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46163', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-141.31', 'loss/train': '0.60086', 'examples_per_second': '45.336', 'grad_norm': '25.148', 'counters/examples': 75008, 'counters/updates': 2344}
skipping logging after 75040 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '-1.1511', 'rewards_train/rejected': '-1.4807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.32967', 'logps_train/rejected': '-178.58', 'logps_train/chosen': '-166.71', 'loss/train': '0.69999', 'examples_per_second': '45.104', 'grad_norm': '32.411', 'counters/examples': 75072, 'counters/updates': 2346}
skipping logging after 75104 examples to avoid logging too frequently
train stats after 75136 examples: {'rewards_train/chosen': '-0.84492', 'rewards_train/rejected': '-1.3116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46664', 'logps_train/rejected': '-140.99', 'logps_train/chosen': '-133.84', 'loss/train': '0.61741', 'examples_per_second': '43.863', 'grad_norm': '23.957', 'counters/examples': 75136, 'counters/updates': 2348}
skipping logging after 75168 examples to avoid logging too frequently
train stats after 75200 examples: {'rewards_train/chosen': '-0.43166', 'rewards_train/rejected': '-1.0005', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56888', 'logps_train/rejected': '-128', 'logps_train/chosen': '-140.61', 'loss/train': '0.54206', 'examples_per_second': '48.253', 'grad_norm': '23.744', 'counters/examples': 75200, 'counters/updates': 2350}
skipping logging after 75232 examples to avoid logging too frequently
train stats after 75264 examples: {'rewards_train/chosen': '-0.81536', 'rewards_train/rejected': '-1.4442', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62884', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-144.92', 'loss/train': '0.54794', 'examples_per_second': '44.458', 'grad_norm': '22.643', 'counters/examples': 75264, 'counters/updates': 2352}
skipping logging after 75296 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '-1.1131', 'rewards_train/rejected': '-1.2489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13577', 'logps_train/rejected': '-144.27', 'logps_train/chosen': '-167.53', 'loss/train': '0.72322', 'examples_per_second': '44.247', 'grad_norm': '30.167', 'counters/examples': 75328, 'counters/updates': 2354}
skipping logging after 75360 examples to avoid logging too frequently
train stats after 75392 examples: {'rewards_train/chosen': '-1.0983', 'rewards_train/rejected': '-1.5654', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.46708', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-125.52', 'loss/train': '0.64043', 'examples_per_second': '43.527', 'grad_norm': '23.567', 'counters/examples': 75392, 'counters/updates': 2356}
skipping logging after 75424 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '-0.64442', 'rewards_train/rejected': '-1.3523', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.70788', 'logps_train/rejected': '-148', 'logps_train/chosen': '-156.42', 'loss/train': '0.53581', 'examples_per_second': '49.848', 'grad_norm': '25.811', 'counters/examples': 75456, 'counters/updates': 2358}
skipping logging after 75488 examples to avoid logging too frequently
train stats after 75520 examples: {'rewards_train/chosen': '-0.64808', 'rewards_train/rejected': '-1.4312', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.78309', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-179.7', 'loss/train': '0.49497', 'examples_per_second': '45.339', 'grad_norm': '23.772', 'counters/examples': 75520, 'counters/updates': 2360}
skipping logging after 75552 examples to avoid logging too frequently
train stats after 75584 examples: {'rewards_train/chosen': '-0.63649', 'rewards_train/rejected': '-1.298', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66152', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-118.31', 'loss/train': '0.55645', 'examples_per_second': '44.326', 'grad_norm': '24.409', 'counters/examples': 75584, 'counters/updates': 2362}
skipping logging after 75616 examples to avoid logging too frequently
train stats after 75648 examples: {'rewards_train/chosen': '-0.72914', 'rewards_train/rejected': '-1.3669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.63771', 'logps_train/rejected': '-162.37', 'logps_train/chosen': '-165.52', 'loss/train': '0.55304', 'examples_per_second': '45.592', 'grad_norm': '26.514', 'counters/examples': 75648, 'counters/updates': 2364}
skipping logging after 75680 examples to avoid logging too frequently
train stats after 75712 examples: {'rewards_train/chosen': '-0.92052', 'rewards_train/rejected': '-1.4627', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.54213', 'logps_train/rejected': '-119.66', 'logps_train/chosen': '-109.12', 'loss/train': '0.65793', 'examples_per_second': '54.44', 'grad_norm': '24.59', 'counters/examples': 75712, 'counters/updates': 2366}
skipping logging after 75744 examples to avoid logging too frequently
train stats after 75776 examples: {'rewards_train/chosen': '-0.87522', 'rewards_train/rejected': '-1.2113', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33603', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-151.6', 'loss/train': '0.6997', 'examples_per_second': '44.474', 'grad_norm': '28.182', 'counters/examples': 75776, 'counters/updates': 2368}
skipping logging after 75808 examples to avoid logging too frequently
train stats after 75840 examples: {'rewards_train/chosen': '-0.40842', 'rewards_train/rejected': '-0.96655', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55813', 'logps_train/rejected': '-162.14', 'logps_train/chosen': '-131.67', 'loss/train': '0.57851', 'examples_per_second': '45.008', 'grad_norm': '23.84', 'counters/examples': 75840, 'counters/updates': 2370}
skipping logging after 75872 examples to avoid logging too frequently
train stats after 75904 examples: {'rewards_train/chosen': '-0.89889', 'rewards_train/rejected': '-1.3438', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.44486', 'logps_train/rejected': '-131.44', 'logps_train/chosen': '-149.3', 'loss/train': '0.58034', 'examples_per_second': '45.115', 'grad_norm': '25.897', 'counters/examples': 75904, 'counters/updates': 2372}
skipping logging after 75936 examples to avoid logging too frequently
train stats after 75968 examples: {'rewards_train/chosen': '-0.97298', 'rewards_train/rejected': '-1.359', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38598', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-136.66', 'loss/train': '0.63882', 'examples_per_second': '45.359', 'grad_norm': '26.986', 'counters/examples': 75968, 'counters/updates': 2374}
skipping logging after 76000 examples to avoid logging too frequently
train stats after 76032 examples: {'rewards_train/chosen': '-0.84402', 'rewards_train/rejected': '-1.35', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50595', 'logps_train/rejected': '-167.71', 'logps_train/chosen': '-183.41', 'loss/train': '0.5688', 'examples_per_second': '46.611', 'grad_norm': '25.825', 'counters/examples': 76032, 'counters/updates': 2376}
skipping logging after 76064 examples to avoid logging too frequently
train stats after 76096 examples: {'rewards_train/chosen': '-0.94826', 'rewards_train/rejected': '-1.4489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50059', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-126.39', 'loss/train': '0.59633', 'examples_per_second': '45.475', 'grad_norm': '23.508', 'counters/examples': 76096, 'counters/updates': 2378}
skipping logging after 76128 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '-1.0743', 'rewards_train/rejected': '-1.5249', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45059', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-124.85', 'loss/train': '0.63452', 'examples_per_second': '48.697', 'grad_norm': '26.606', 'counters/examples': 76160, 'counters/updates': 2380}
skipping logging after 76192 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '-1.0613', 'rewards_train/rejected': '-1.6044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.54314', 'logps_train/rejected': '-182.81', 'logps_train/chosen': '-167.08', 'loss/train': '0.60367', 'examples_per_second': '45.341', 'grad_norm': '27.937', 'counters/examples': 76224, 'counters/updates': 2382}
skipping logging after 76256 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '-0.44213', 'rewards_train/rejected': '-1.1391', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.69697', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-120.42', 'loss/train': '0.49516', 'examples_per_second': '45.815', 'grad_norm': '20.983', 'counters/examples': 76288, 'counters/updates': 2384}
skipping logging after 76320 examples to avoid logging too frequently
train stats after 76352 examples: {'rewards_train/chosen': '-1.0411', 'rewards_train/rejected': '-1.5127', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47165', 'logps_train/rejected': '-155.61', 'logps_train/chosen': '-186.15', 'loss/train': '0.5725', 'examples_per_second': '44.452', 'grad_norm': '27.27', 'counters/examples': 76352, 'counters/updates': 2386}
skipping logging after 76384 examples to avoid logging too frequently
train stats after 76416 examples: {'rewards_train/chosen': '-1.1197', 'rewards_train/rejected': '-1.8642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.74443', 'logps_train/rejected': '-159.94', 'logps_train/chosen': '-163.24', 'loss/train': '0.53458', 'examples_per_second': '45.648', 'grad_norm': '26.165', 'counters/examples': 76416, 'counters/updates': 2388}
skipping logging after 76448 examples to avoid logging too frequently
train stats after 76480 examples: {'rewards_train/chosen': '-1.0159', 'rewards_train/rejected': '-1.6158', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59989', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-146.62', 'loss/train': '0.59544', 'examples_per_second': '44.233', 'grad_norm': '23.299', 'counters/examples': 76480, 'counters/updates': 2390}
skipping logging after 76512 examples to avoid logging too frequently
train stats after 76544 examples: {'rewards_train/chosen': '-1.3945', 'rewards_train/rejected': '-1.9077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.51326', 'logps_train/rejected': '-135.86', 'logps_train/chosen': '-143.91', 'loss/train': '0.63827', 'examples_per_second': '45.231', 'grad_norm': '26.349', 'counters/examples': 76544, 'counters/updates': 2392}
skipping logging after 76576 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '-1.0992', 'rewards_train/rejected': '-1.7907', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.69152', 'logps_train/rejected': '-158.91', 'logps_train/chosen': '-154.88', 'loss/train': '0.53793', 'examples_per_second': '51.795', 'grad_norm': '23.833', 'counters/examples': 76608, 'counters/updates': 2394}
skipping logging after 76640 examples to avoid logging too frequently
train stats after 76672 examples: {'rewards_train/chosen': '-1.1141', 'rewards_train/rejected': '-1.3933', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27922', 'logps_train/rejected': '-158.78', 'logps_train/chosen': '-169.47', 'loss/train': '0.76064', 'examples_per_second': '45.088', 'grad_norm': '30.254', 'counters/examples': 76672, 'counters/updates': 2396}
skipping logging after 76704 examples to avoid logging too frequently
train stats after 76736 examples: {'rewards_train/chosen': '-1.1502', 'rewards_train/rejected': '-1.7778', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62758', 'logps_train/rejected': '-120.63', 'logps_train/chosen': '-119.76', 'loss/train': '0.60848', 'examples_per_second': '46.294', 'grad_norm': '21.718', 'counters/examples': 76736, 'counters/updates': 2398}
skipping logging after 76768 examples to avoid logging too frequently
train stats after 76800 examples: {'rewards_train/chosen': '-1.5472', 'rewards_train/rejected': '-1.8444', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29721', 'logps_train/rejected': '-153.17', 'logps_train/chosen': '-149.43', 'loss/train': '0.68312', 'examples_per_second': '45.345', 'grad_norm': '27.508', 'counters/examples': 76800, 'counters/updates': 2400}
skipping logging after 76832 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '-1.2758', 'rewards_train/rejected': '-1.9197', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.64389', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-135.33', 'loss/train': '0.54816', 'examples_per_second': '45.167', 'grad_norm': '25.353', 'counters/examples': 76864, 'counters/updates': 2402}
skipping logging after 76896 examples to avoid logging too frequently
train stats after 76928 examples: {'rewards_train/chosen': '-1.2004', 'rewards_train/rejected': '-1.8375', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63712', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-140.92', 'loss/train': '0.56587', 'examples_per_second': '48.506', 'grad_norm': '23.826', 'counters/examples': 76928, 'counters/updates': 2404}
skipping logging after 76960 examples to avoid logging too frequently
train stats after 76992 examples: {'rewards_train/chosen': '-1.524', 'rewards_train/rejected': '-2.4767', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.9527', 'logps_train/rejected': '-122.76', 'logps_train/chosen': '-147.03', 'loss/train': '0.48527', 'examples_per_second': '46.646', 'grad_norm': '22.26', 'counters/examples': 76992, 'counters/updates': 2406}
skipping logging after 77024 examples to avoid logging too frequently
train stats after 77056 examples: {'rewards_train/chosen': '-1.3932', 'rewards_train/rejected': '-1.9063', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5131', 'logps_train/rejected': '-132.76', 'logps_train/chosen': '-133.48', 'loss/train': '0.56214', 'examples_per_second': '44.991', 'grad_norm': '24.947', 'counters/examples': 77056, 'counters/updates': 2408}
skipping logging after 77088 examples to avoid logging too frequently
train stats after 77120 examples: {'rewards_train/chosen': '-1.1564', 'rewards_train/rejected': '-1.9159', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.75953', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-154.46', 'loss/train': '0.52651', 'examples_per_second': '51.956', 'grad_norm': '21.655', 'counters/examples': 77120, 'counters/updates': 2410}
skipping logging after 77152 examples to avoid logging too frequently
train stats after 77184 examples: {'rewards_train/chosen': '-1.3175', 'rewards_train/rejected': '-1.9398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.62231', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-166.87', 'loss/train': '0.60781', 'examples_per_second': '44.863', 'grad_norm': '28.089', 'counters/examples': 77184, 'counters/updates': 2412}
skipping logging after 77216 examples to avoid logging too frequently
train stats after 77248 examples: {'rewards_train/chosen': '-0.76471', 'rewards_train/rejected': '-1.941', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '1.1763', 'logps_train/rejected': '-148.21', 'logps_train/chosen': '-166.55', 'loss/train': '0.44966', 'examples_per_second': '44.15', 'grad_norm': '25.053', 'counters/examples': 77248, 'counters/updates': 2414}
skipping logging after 77280 examples to avoid logging too frequently
train stats after 77312 examples: {'rewards_train/chosen': '-1.1917', 'rewards_train/rejected': '-1.9936', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.80181', 'logps_train/rejected': '-172.06', 'logps_train/chosen': '-157.11', 'loss/train': '0.52745', 'examples_per_second': '44.155', 'grad_norm': '26.066', 'counters/examples': 77312, 'counters/updates': 2416}
skipping logging after 77344 examples to avoid logging too frequently
train stats after 77376 examples: {'rewards_train/chosen': '-0.70358', 'rewards_train/rejected': '-2.0342', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '1.3307', 'logps_train/rejected': '-141.4', 'logps_train/chosen': '-127.01', 'loss/train': '0.4159', 'examples_per_second': '45.625', 'grad_norm': '19.107', 'counters/examples': 77376, 'counters/updates': 2418}
skipping logging after 77408 examples to avoid logging too frequently
train stats after 77440 examples: {'rewards_train/chosen': '-1.1209', 'rewards_train/rejected': '-1.9701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.8492', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-156.37', 'loss/train': '0.60628', 'examples_per_second': '46.529', 'grad_norm': '28.597', 'counters/examples': 77440, 'counters/updates': 2420}
skipping logging after 77472 examples to avoid logging too frequently
train stats after 77504 examples: {'rewards_train/chosen': '-1.1775', 'rewards_train/rejected': '-1.7348', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55727', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-126.1', 'loss/train': '0.61731', 'examples_per_second': '45.203', 'grad_norm': '21.523', 'counters/examples': 77504, 'counters/updates': 2422}
skipping logging after 77536 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '-0.88218', 'rewards_train/rejected': '-1.4215', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53936', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-130.09', 'loss/train': '0.59569', 'examples_per_second': '44.378', 'grad_norm': '28.594', 'counters/examples': 77568, 'counters/updates': 2424}
skipping logging after 77600 examples to avoid logging too frequently
train stats after 77632 examples: {'rewards_train/chosen': '-0.73767', 'rewards_train/rejected': '-1.2184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.48077', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-139.47', 'loss/train': '0.60926', 'examples_per_second': '44.356', 'grad_norm': '27.132', 'counters/examples': 77632, 'counters/updates': 2426}
skipping logging after 77664 examples to avoid logging too frequently
train stats after 77696 examples: {'rewards_train/chosen': '-0.82973', 'rewards_train/rejected': '-1.414', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.58424', 'logps_train/rejected': '-156.23', 'logps_train/chosen': '-155.93', 'loss/train': '0.55036', 'examples_per_second': '45.153', 'grad_norm': '24.983', 'counters/examples': 77696, 'counters/updates': 2428}
skipping logging after 77728 examples to avoid logging too frequently
train stats after 77760 examples: {'rewards_train/chosen': '-0.97407', 'rewards_train/rejected': '-1.2012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22716', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-163.79', 'loss/train': '0.75607', 'examples_per_second': '45.235', 'grad_norm': '32.181', 'counters/examples': 77760, 'counters/updates': 2430}
skipping logging after 77792 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '-0.93647', 'rewards_train/rejected': '-1.4422', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.50572', 'logps_train/rejected': '-151.6', 'logps_train/chosen': '-171.38', 'loss/train': '0.63051', 'examples_per_second': '44.03', 'grad_norm': '30.612', 'counters/examples': 77824, 'counters/updates': 2432}
skipping logging after 77856 examples to avoid logging too frequently
train stats after 77888 examples: {'rewards_train/chosen': '-1.1671', 'rewards_train/rejected': '-1.7177', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55054', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-135.54', 'loss/train': '0.59027', 'examples_per_second': '44.425', 'grad_norm': '23.83', 'counters/examples': 77888, 'counters/updates': 2434}
skipping logging after 77920 examples to avoid logging too frequently
train stats after 77952 examples: {'rewards_train/chosen': '-1.221', 'rewards_train/rejected': '-1.9701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.74904', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-146.32', 'loss/train': '0.6165', 'examples_per_second': '45.972', 'grad_norm': '26.36', 'counters/examples': 77952, 'counters/updates': 2436}
skipping logging after 77984 examples to avoid logging too frequently
train stats after 78016 examples: {'rewards_train/chosen': '-1.3368', 'rewards_train/rejected': '-1.9076', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57079', 'logps_train/rejected': '-155.99', 'logps_train/chosen': '-170.12', 'loss/train': '0.58448', 'examples_per_second': '44.884', 'grad_norm': '32.519', 'counters/examples': 78016, 'counters/updates': 2438}
skipping logging after 78048 examples to avoid logging too frequently
train stats after 78080 examples: {'rewards_train/chosen': '-0.94344', 'rewards_train/rejected': '-1.9882', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.0447', 'logps_train/rejected': '-178.14', 'logps_train/chosen': '-149.41', 'loss/train': '0.45557', 'examples_per_second': '45.389', 'grad_norm': '22.113', 'counters/examples': 78080, 'counters/updates': 2440}
skipping logging after 78112 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '-0.9301', 'rewards_train/rejected': '-1.6938', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.76372', 'logps_train/rejected': '-143.59', 'logps_train/chosen': '-166.97', 'loss/train': '0.5582', 'examples_per_second': '52.296', 'grad_norm': '24.884', 'counters/examples': 78144, 'counters/updates': 2442}
skipping logging after 78176 examples to avoid logging too frequently
train stats after 78208 examples: {'rewards_train/chosen': '-1.1975', 'rewards_train/rejected': '-1.8466', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64911', 'logps_train/rejected': '-160.29', 'logps_train/chosen': '-151.15', 'loss/train': '0.6366', 'examples_per_second': '45.193', 'grad_norm': '26.52', 'counters/examples': 78208, 'counters/updates': 2444}
skipping logging after 78240 examples to avoid logging too frequently
train stats after 78272 examples: {'rewards_train/chosen': '-1.152', 'rewards_train/rejected': '-1.9622', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.8102', 'logps_train/rejected': '-116.06', 'logps_train/chosen': '-157.81', 'loss/train': '0.55805', 'examples_per_second': '46.071', 'grad_norm': '25.476', 'counters/examples': 78272, 'counters/updates': 2446}
skipping logging after 78304 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '-1.1748', 'rewards_train/rejected': '-1.7082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53335', 'logps_train/rejected': '-150.24', 'logps_train/chosen': '-158.76', 'loss/train': '0.59146', 'examples_per_second': '45.373', 'grad_norm': '27.83', 'counters/examples': 78336, 'counters/updates': 2448}
skipping logging after 78368 examples to avoid logging too frequently
train stats after 78400 examples: {'rewards_train/chosen': '-1.1919', 'rewards_train/rejected': '-1.6497', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.45778', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-145.93', 'loss/train': '0.58426', 'examples_per_second': '51.605', 'grad_norm': '23.42', 'counters/examples': 78400, 'counters/updates': 2450}
skipping logging after 78432 examples to avoid logging too frequently
train stats after 78464 examples: {'rewards_train/chosen': '-1.111', 'rewards_train/rejected': '-1.7418', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.63088', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-120.25', 'loss/train': '0.52905', 'examples_per_second': '45.142', 'grad_norm': '19.636', 'counters/examples': 78464, 'counters/updates': 2452}
skipping logging after 78496 examples to avoid logging too frequently
train stats after 78528 examples: {'rewards_train/chosen': '-1.7614', 'rewards_train/rejected': '-2.3717', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.61035', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-145.92', 'loss/train': '0.58707', 'examples_per_second': '49.978', 'grad_norm': '24.094', 'counters/examples': 78528, 'counters/updates': 2454}
skipping logging after 78560 examples to avoid logging too frequently
train stats after 78592 examples: {'rewards_train/chosen': '-1.2907', 'rewards_train/rejected': '-1.9058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61512', 'logps_train/rejected': '-162.48', 'logps_train/chosen': '-129.88', 'loss/train': '0.53533', 'examples_per_second': '47.359', 'grad_norm': '26.358', 'counters/examples': 78592, 'counters/updates': 2456}
skipping logging after 78624 examples to avoid logging too frequently
train stats after 78656 examples: {'rewards_train/chosen': '-1.2442', 'rewards_train/rejected': '-2.1312', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.88707', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-189.09', 'loss/train': '0.47663', 'examples_per_second': '45.201', 'grad_norm': '24.448', 'counters/examples': 78656, 'counters/updates': 2458}
skipping logging after 78688 examples to avoid logging too frequently
train stats after 78720 examples: {'rewards_train/chosen': '-0.96465', 'rewards_train/rejected': '-1.2722', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30754', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-178.15', 'loss/train': '0.65208', 'examples_per_second': '44.497', 'grad_norm': '26.149', 'counters/examples': 78720, 'counters/updates': 2460}
skipping logging after 78752 examples to avoid logging too frequently
train stats after 78784 examples: {'rewards_train/chosen': '-1.0681', 'rewards_train/rejected': '-2.0022', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.93404', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-137.64', 'loss/train': '0.45109', 'examples_per_second': '45.374', 'grad_norm': '20.258', 'counters/examples': 78784, 'counters/updates': 2462}
skipping logging after 78816 examples to avoid logging too frequently
train stats after 78848 examples: {'rewards_train/chosen': '-1.0912', 'rewards_train/rejected': '-1.6289', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53766', 'logps_train/rejected': '-177.34', 'logps_train/chosen': '-147.15', 'loss/train': '0.57854', 'examples_per_second': '47.353', 'grad_norm': '28.3', 'counters/examples': 78848, 'counters/updates': 2464}
skipping logging after 78880 examples to avoid logging too frequently
train stats after 78912 examples: {'rewards_train/chosen': '-0.85414', 'rewards_train/rejected': '-2.047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '1.1929', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-153.93', 'loss/train': '0.48534', 'examples_per_second': '45.418', 'grad_norm': '24.084', 'counters/examples': 78912, 'counters/updates': 2466}
skipping logging after 78944 examples to avoid logging too frequently
train stats after 78976 examples: {'rewards_train/chosen': '-0.89179', 'rewards_train/rejected': '-1.7062', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.81439', 'logps_train/rejected': '-143.32', 'logps_train/chosen': '-137.35', 'loss/train': '0.56067', 'examples_per_second': '44.14', 'grad_norm': '25.023', 'counters/examples': 78976, 'counters/updates': 2468}
skipping logging after 79008 examples to avoid logging too frequently
train stats after 79040 examples: {'rewards_train/chosen': '-1.1264', 'rewards_train/rejected': '-1.323', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19659', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-155.38', 'loss/train': '0.71522', 'examples_per_second': '44.763', 'grad_norm': '28.852', 'counters/examples': 79040, 'counters/updates': 2470}
skipping logging after 79072 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '-0.6626', 'rewards_train/rejected': '-1.4177', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.75513', 'logps_train/rejected': '-148.26', 'logps_train/chosen': '-144.46', 'loss/train': '0.51245', 'examples_per_second': '48.622', 'grad_norm': '23.338', 'counters/examples': 79104, 'counters/updates': 2472}
skipping logging after 79136 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '-0.84073', 'rewards_train/rejected': '-1.5119', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.6712', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-162.48', 'loss/train': '0.55594', 'examples_per_second': '45.395', 'grad_norm': '23.941', 'counters/examples': 79168, 'counters/updates': 2474}
skipping logging after 79200 examples to avoid logging too frequently
train stats after 79232 examples: {'rewards_train/chosen': '-0.98187', 'rewards_train/rejected': '-1.3428', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36098', 'logps_train/rejected': '-119.4', 'logps_train/chosen': '-111.87', 'loss/train': '0.64182', 'examples_per_second': '47.544', 'grad_norm': '21.778', 'counters/examples': 79232, 'counters/updates': 2476}
skipping logging after 79264 examples to avoid logging too frequently
train stats after 79296 examples: {'rewards_train/chosen': '-1.266', 'rewards_train/rejected': '-1.5125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.24649', 'logps_train/rejected': '-164.49', 'logps_train/chosen': '-154.16', 'loss/train': '0.74069', 'examples_per_second': '44.265', 'grad_norm': '33.23', 'counters/examples': 79296, 'counters/updates': 2478}
skipping logging after 79328 examples to avoid logging too frequently
train stats after 79360 examples: {'rewards_train/chosen': '-1.1493', 'rewards_train/rejected': '-1.7538', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.60458', 'logps_train/rejected': '-125.32', 'logps_train/chosen': '-131.62', 'loss/train': '0.58966', 'examples_per_second': '52.607', 'grad_norm': '25.038', 'counters/examples': 79360, 'counters/updates': 2480}
skipping logging after 79392 examples to avoid logging too frequently
train stats after 79424 examples: {'rewards_train/chosen': '-0.76863', 'rewards_train/rejected': '-1.6584', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.88975', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-143.65', 'loss/train': '0.48265', 'examples_per_second': '45.662', 'grad_norm': '21.506', 'counters/examples': 79424, 'counters/updates': 2482}
skipping logging after 79456 examples to avoid logging too frequently
train stats after 79488 examples: {'rewards_train/chosen': '-0.89496', 'rewards_train/rejected': '-1.8601', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.96513', 'logps_train/rejected': '-130.29', 'logps_train/chosen': '-152.48', 'loss/train': '0.41983', 'examples_per_second': '44.903', 'grad_norm': '20.647', 'counters/examples': 79488, 'counters/updates': 2484}
skipping logging after 79520 examples to avoid logging too frequently
train stats after 79552 examples: {'rewards_train/chosen': '-1.11', 'rewards_train/rejected': '-1.4729', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36288', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-108.06', 'loss/train': '0.65422', 'examples_per_second': '46.984', 'grad_norm': '23.91', 'counters/examples': 79552, 'counters/updates': 2486}
skipping logging after 79584 examples to avoid logging too frequently
train stats after 79616 examples: {'rewards_train/chosen': '-1.0269', 'rewards_train/rejected': '-1.2528', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22593', 'logps_train/rejected': '-175.21', 'logps_train/chosen': '-171.72', 'loss/train': '0.70491', 'examples_per_second': '45.075', 'grad_norm': '31.292', 'counters/examples': 79616, 'counters/updates': 2488}
skipping logging after 79648 examples to avoid logging too frequently
train stats after 79680 examples: {'rewards_train/chosen': '-1.0377', 'rewards_train/rejected': '-1.3983', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36069', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-149', 'loss/train': '0.64749', 'examples_per_second': '45.155', 'grad_norm': '28.299', 'counters/examples': 79680, 'counters/updates': 2490}
skipping logging after 79712 examples to avoid logging too frequently
train stats after 79744 examples: {'rewards_train/chosen': '-1.2452', 'rewards_train/rejected': '-1.6502', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40501', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-186.37', 'loss/train': '0.69632', 'examples_per_second': '44.242', 'grad_norm': '31.757', 'counters/examples': 79744, 'counters/updates': 2492}
skipping logging after 79776 examples to avoid logging too frequently
train stats after 79808 examples: {'rewards_train/chosen': '-1.0165', 'rewards_train/rejected': '-1.2917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27524', 'logps_train/rejected': '-154.56', 'logps_train/chosen': '-169.18', 'loss/train': '0.71435', 'examples_per_second': '46.012', 'grad_norm': '31.91', 'counters/examples': 79808, 'counters/updates': 2494}
skipping logging after 79840 examples to avoid logging too frequently
train stats after 79872 examples: {'rewards_train/chosen': '-1.2599', 'rewards_train/rejected': '-1.5757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31582', 'logps_train/rejected': '-168.69', 'logps_train/chosen': '-125.64', 'loss/train': '0.66627', 'examples_per_second': '45.283', 'grad_norm': '25.367', 'counters/examples': 79872, 'counters/updates': 2496}
skipping logging after 79904 examples to avoid logging too frequently
train stats after 79936 examples: {'rewards_train/chosen': '-0.89161', 'rewards_train/rejected': '-1.7073', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.81569', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-160.23', 'loss/train': '0.55243', 'examples_per_second': '44.952', 'grad_norm': '23.871', 'counters/examples': 79936, 'counters/updates': 2498}
skipping logging after 79968 examples to avoid logging too frequently
train stats after 80000 examples: {'rewards_train/chosen': '-1.2517', 'rewards_train/rejected': '-1.9072', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.65549', 'logps_train/rejected': '-182.47', 'logps_train/chosen': '-153.66', 'loss/train': '0.61747', 'examples_per_second': '45.186', 'grad_norm': '27.783', 'counters/examples': 80000, 'counters/updates': 2500}
skipping logging after 80032 examples to avoid logging too frequently
train stats after 80064 examples: {'rewards_train/chosen': '-1.4218', 'rewards_train/rejected': '-1.6276', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2058', 'logps_train/rejected': '-142.33', 'logps_train/chosen': '-163.32', 'loss/train': '0.709', 'examples_per_second': '46.475', 'grad_norm': '27.96', 'counters/examples': 80064, 'counters/updates': 2502}
skipping logging after 80096 examples to avoid logging too frequently
train stats after 80128 examples: {'rewards_train/chosen': '-1.1337', 'rewards_train/rejected': '-1.5621', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42841', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-132.9', 'loss/train': '0.58928', 'examples_per_second': '47.827', 'grad_norm': '21.753', 'counters/examples': 80128, 'counters/updates': 2504}
skipping logging after 80160 examples to avoid logging too frequently
train stats after 80192 examples: {'rewards_train/chosen': '-0.93192', 'rewards_train/rejected': '-1.374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44212', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-123.48', 'loss/train': '0.57745', 'examples_per_second': '45.78', 'grad_norm': '23.54', 'counters/examples': 80192, 'counters/updates': 2506}
skipping logging after 80224 examples to avoid logging too frequently
train stats after 80256 examples: {'rewards_train/chosen': '-1.3561', 'rewards_train/rejected': '-1.5551', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.199', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-149.12', 'loss/train': '0.71278', 'examples_per_second': '45.204', 'grad_norm': '29.718', 'counters/examples': 80256, 'counters/updates': 2508}
skipping logging after 80288 examples to avoid logging too frequently
train stats after 80320 examples: {'rewards_train/chosen': '-1.1931', 'rewards_train/rejected': '-1.7678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57469', 'logps_train/rejected': '-180.94', 'logps_train/chosen': '-145.49', 'loss/train': '0.63348', 'examples_per_second': '44.355', 'grad_norm': '27.987', 'counters/examples': 80320, 'counters/updates': 2510}
skipping logging after 80352 examples to avoid logging too frequently
train stats after 80384 examples: {'rewards_train/chosen': '-1.2693', 'rewards_train/rejected': '-1.914', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.64474', 'logps_train/rejected': '-132.29', 'logps_train/chosen': '-161.58', 'loss/train': '0.53783', 'examples_per_second': '48.414', 'grad_norm': '25.578', 'counters/examples': 80384, 'counters/updates': 2512}
skipping logging after 80416 examples to avoid logging too frequently
train stats after 80448 examples: {'rewards_train/chosen': '-1.3288', 'rewards_train/rejected': '-1.4366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10787', 'logps_train/rejected': '-143.06', 'logps_train/chosen': '-132.68', 'loss/train': '0.8362', 'examples_per_second': '45.447', 'grad_norm': '31.428', 'counters/examples': 80448, 'counters/updates': 2514}
skipping logging after 80480 examples to avoid logging too frequently
train stats after 80512 examples: {'rewards_train/chosen': '-0.93721', 'rewards_train/rejected': '-1.7242', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.78696', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-136.98', 'loss/train': '0.53138', 'examples_per_second': '47.264', 'grad_norm': '25.726', 'counters/examples': 80512, 'counters/updates': 2516}
skipping logging after 80544 examples to avoid logging too frequently
train stats after 80576 examples: {'rewards_train/chosen': '-0.77472', 'rewards_train/rejected': '-1.559', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.78427', 'logps_train/rejected': '-107.37', 'logps_train/chosen': '-112.23', 'loss/train': '0.46396', 'examples_per_second': '44.311', 'grad_norm': '19.094', 'counters/examples': 80576, 'counters/updates': 2518}
skipping logging after 80608 examples to avoid logging too frequently
train stats after 80640 examples: {'rewards_train/chosen': '-1.1108', 'rewards_train/rejected': '-1.7418', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.631', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-134.23', 'loss/train': '0.52842', 'examples_per_second': '44.38', 'grad_norm': '23.312', 'counters/examples': 80640, 'counters/updates': 2520}
skipping logging after 80672 examples to avoid logging too frequently
train stats after 80704 examples: {'rewards_train/chosen': '-1.2913', 'rewards_train/rejected': '-1.5145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22325', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-174.37', 'loss/train': '0.77445', 'examples_per_second': '45.327', 'grad_norm': '33.654', 'counters/examples': 80704, 'counters/updates': 2522}
skipping logging after 80736 examples to avoid logging too frequently
train stats after 80768 examples: {'rewards_train/chosen': '-0.98665', 'rewards_train/rejected': '-1.8464', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.85972', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-133.34', 'loss/train': '0.46619', 'examples_per_second': '44.548', 'grad_norm': '21.083', 'counters/examples': 80768, 'counters/updates': 2524}
skipping logging after 80800 examples to avoid logging too frequently
train stats after 80832 examples: {'rewards_train/chosen': '-0.98878', 'rewards_train/rejected': '-2.0823', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.0935', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-152.66', 'loss/train': '0.4698', 'examples_per_second': '45.333', 'grad_norm': '23.371', 'counters/examples': 80832, 'counters/updates': 2526}
skipping logging after 80864 examples to avoid logging too frequently
train stats after 80896 examples: {'rewards_train/chosen': '-0.81507', 'rewards_train/rejected': '-1.0972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.28217', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-139.64', 'loss/train': '0.68453', 'examples_per_second': '45.609', 'grad_norm': '26.865', 'counters/examples': 80896, 'counters/updates': 2528}
skipping logging after 80928 examples to avoid logging too frequently
train stats after 80960 examples: {'rewards_train/chosen': '-0.65462', 'rewards_train/rejected': '-1.3886', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.73396', 'logps_train/rejected': '-119.8', 'logps_train/chosen': '-155.96', 'loss/train': '0.49578', 'examples_per_second': '45.082', 'grad_norm': '20.17', 'counters/examples': 80960, 'counters/updates': 2530}
skipping logging after 80992 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '-0.95962', 'rewards_train/rejected': '-1.5557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59611', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-154.69', 'loss/train': '0.57053', 'examples_per_second': '48.112', 'grad_norm': '26.932', 'counters/examples': 81024, 'counters/updates': 2532}
skipping logging after 81056 examples to avoid logging too frequently
train stats after 81088 examples: {'rewards_train/chosen': '-1.0723', 'rewards_train/rejected': '-1.561', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.48876', 'logps_train/rejected': '-155.94', 'logps_train/chosen': '-162.21', 'loss/train': '0.64314', 'examples_per_second': '45.23', 'grad_norm': '30.579', 'counters/examples': 81088, 'counters/updates': 2534}
skipping logging after 81120 examples to avoid logging too frequently
train stats after 81152 examples: {'rewards_train/chosen': '-0.90268', 'rewards_train/rejected': '-1.6427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.74002', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-151.69', 'loss/train': '0.5153', 'examples_per_second': '44.627', 'grad_norm': '22.168', 'counters/examples': 81152, 'counters/updates': 2536}
skipping logging after 81184 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '-0.76623', 'rewards_train/rejected': '-1.7843', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.0181', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-145.73', 'loss/train': '0.47101', 'examples_per_second': '46.07', 'grad_norm': '20.148', 'counters/examples': 81216, 'counters/updates': 2538}
skipping logging after 81248 examples to avoid logging too frequently
train stats after 81280 examples: {'rewards_train/chosen': '-1.3302', 'rewards_train/rejected': '-1.7502', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.42006', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-125.88', 'loss/train': '0.65668', 'examples_per_second': '45.267', 'grad_norm': '24.961', 'counters/examples': 81280, 'counters/updates': 2540}
skipping logging after 81312 examples to avoid logging too frequently
train stats after 81344 examples: {'rewards_train/chosen': '-0.94351', 'rewards_train/rejected': '-1.7319', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.78835', 'logps_train/rejected': '-129.59', 'logps_train/chosen': '-156.2', 'loss/train': '0.57008', 'examples_per_second': '47.716', 'grad_norm': '24.249', 'counters/examples': 81344, 'counters/updates': 2542}
skipping logging after 81376 examples to avoid logging too frequently
train stats after 81408 examples: {'rewards_train/chosen': '-0.62867', 'rewards_train/rejected': '-1.1256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49689', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-153.38', 'loss/train': '0.58406', 'examples_per_second': '45.194', 'grad_norm': '26.494', 'counters/examples': 81408, 'counters/updates': 2544}
skipping logging after 81440 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '-0.92632', 'rewards_train/rejected': '-1.4362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50992', 'logps_train/rejected': '-142.44', 'logps_train/chosen': '-176.89', 'loss/train': '0.56598', 'examples_per_second': '43.676', 'grad_norm': '27.282', 'counters/examples': 81472, 'counters/updates': 2546}
skipping logging after 81504 examples to avoid logging too frequently
train stats after 81536 examples: {'rewards_train/chosen': '-1.0131', 'rewards_train/rejected': '-1.7487', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.73565', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-152.26', 'loss/train': '0.48707', 'examples_per_second': '48.01', 'grad_norm': '23.394', 'counters/examples': 81536, 'counters/updates': 2548}
skipping logging after 81568 examples to avoid logging too frequently
train stats after 81600 examples: {'rewards_train/chosen': '-0.89438', 'rewards_train/rejected': '-1.1038', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.20941', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-123.52', 'loss/train': '0.73164', 'examples_per_second': '43.343', 'grad_norm': '28.203', 'counters/examples': 81600, 'counters/updates': 2550}
skipping logging after 81632 examples to avoid logging too frequently
train stats after 81664 examples: {'rewards_train/chosen': '-0.86989', 'rewards_train/rejected': '-1.3637', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.49377', 'logps_train/rejected': '-136.56', 'logps_train/chosen': '-130.31', 'loss/train': '0.5589', 'examples_per_second': '43.937', 'grad_norm': '22.669', 'counters/examples': 81664, 'counters/updates': 2552}
skipping logging after 81696 examples to avoid logging too frequently
train stats after 81728 examples: {'rewards_train/chosen': '-0.88757', 'rewards_train/rejected': '-1.6796', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.79205', 'logps_train/rejected': '-142.36', 'logps_train/chosen': '-173.23', 'loss/train': '0.49467', 'examples_per_second': '45.361', 'grad_norm': '24.869', 'counters/examples': 81728, 'counters/updates': 2554}
skipping logging after 81760 examples to avoid logging too frequently
train stats after 81792 examples: {'rewards_train/chosen': '-1.3141', 'rewards_train/rejected': '-1.3531', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038992', 'logps_train/rejected': '-155.38', 'logps_train/chosen': '-146.93', 'loss/train': '0.83515', 'examples_per_second': '45.155', 'grad_norm': '30.752', 'counters/examples': 81792, 'counters/updates': 2556}
skipping logging after 81824 examples to avoid logging too frequently
train stats after 81856 examples: {'rewards_train/chosen': '-0.78785', 'rewards_train/rejected': '-1.5539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.7661', 'logps_train/rejected': '-114.24', 'logps_train/chosen': '-99.465', 'loss/train': '0.58069', 'examples_per_second': '45.258', 'grad_norm': '21.603', 'counters/examples': 81856, 'counters/updates': 2558}
skipping logging after 81888 examples to avoid logging too frequently
train stats after 81920 examples: {'rewards_train/chosen': '-0.87755', 'rewards_train/rejected': '-1.5647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6871', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-135.6', 'loss/train': '0.54658', 'examples_per_second': '44.928', 'grad_norm': '24.208', 'counters/examples': 81920, 'counters/updates': 2560}
skipping logging after 81952 examples to avoid logging too frequently
train stats after 81984 examples: {'rewards_train/chosen': '-0.98423', 'rewards_train/rejected': '-1.8764', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.8922', 'logps_train/rejected': '-164.32', 'logps_train/chosen': '-185.82', 'loss/train': '0.56532', 'examples_per_second': '45.292', 'grad_norm': '27.875', 'counters/examples': 81984, 'counters/updates': 2562}
skipping logging after 82016 examples to avoid logging too frequently
train stats after 82048 examples: {'rewards_train/chosen': '-1.6488', 'rewards_train/rejected': '-1.6517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0028878', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-117.11', 'loss/train': '0.80763', 'examples_per_second': '46.969', 'grad_norm': '29.807', 'counters/examples': 82048, 'counters/updates': 2564}
skipping logging after 82080 examples to avoid logging too frequently
train stats after 82112 examples: {'rewards_train/chosen': '-1.1365', 'rewards_train/rejected': '-1.5293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.39279', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-140.61', 'loss/train': '0.63592', 'examples_per_second': '48.653', 'grad_norm': '26.957', 'counters/examples': 82112, 'counters/updates': 2566}
skipping logging after 82144 examples to avoid logging too frequently
train stats after 82176 examples: {'rewards_train/chosen': '-0.80085', 'rewards_train/rejected': '-1.3198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51891', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-151.97', 'loss/train': '0.59546', 'examples_per_second': '45.257', 'grad_norm': '27.905', 'counters/examples': 82176, 'counters/updates': 2568}
skipping logging after 82208 examples to avoid logging too frequently
train stats after 82240 examples: {'rewards_train/chosen': '-1.1824', 'rewards_train/rejected': '-1.6193', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43684', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-190.24', 'loss/train': '0.58931', 'examples_per_second': '46.545', 'grad_norm': '24.643', 'counters/examples': 82240, 'counters/updates': 2570}
skipping logging after 82272 examples to avoid logging too frequently
train stats after 82304 examples: {'rewards_train/chosen': '-0.79441', 'rewards_train/rejected': '-1.4611', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66666', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-152.06', 'loss/train': '0.48346', 'examples_per_second': '45.404', 'grad_norm': '22.772', 'counters/examples': 82304, 'counters/updates': 2572}
skipping logging after 82336 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '-0.93702', 'rewards_train/rejected': '-1.6709', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.73387', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-150.14', 'loss/train': '0.56235', 'examples_per_second': '47.028', 'grad_norm': '22.334', 'counters/examples': 82368, 'counters/updates': 2574}
skipping logging after 82400 examples to avoid logging too frequently
train stats after 82432 examples: {'rewards_train/chosen': '-1.2799', 'rewards_train/rejected': '-1.4593', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17938', 'logps_train/rejected': '-162.44', 'logps_train/chosen': '-158', 'loss/train': '0.79315', 'examples_per_second': '45.981', 'grad_norm': '34.958', 'counters/examples': 82432, 'counters/updates': 2576}
skipping logging after 82464 examples to avoid logging too frequently
train stats after 82496 examples: {'rewards_train/chosen': '-0.80862', 'rewards_train/rejected': '-1.3525', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54387', 'logps_train/rejected': '-175.55', 'logps_train/chosen': '-167.29', 'loss/train': '0.56894', 'examples_per_second': '45.394', 'grad_norm': '25.651', 'counters/examples': 82496, 'counters/updates': 2578}
skipping logging after 82528 examples to avoid logging too frequently
train stats after 82560 examples: {'rewards_train/chosen': '-0.98495', 'rewards_train/rejected': '-1.5568', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57183', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-159.6', 'loss/train': '0.55198', 'examples_per_second': '44.755', 'grad_norm': '25.172', 'counters/examples': 82560, 'counters/updates': 2580}
skipping logging after 82592 examples to avoid logging too frequently
train stats after 82624 examples: {'rewards_train/chosen': '-1.1333', 'rewards_train/rejected': '-1.8767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.74341', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-137.77', 'loss/train': '0.52818', 'examples_per_second': '51.293', 'grad_norm': '21.253', 'counters/examples': 82624, 'counters/updates': 2582}
skipping logging after 82656 examples to avoid logging too frequently
train stats after 82688 examples: {'rewards_train/chosen': '-0.70888', 'rewards_train/rejected': '-1.8726', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.1637', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-149.21', 'loss/train': '0.42237', 'examples_per_second': '43.856', 'grad_norm': '19.83', 'counters/examples': 82688, 'counters/updates': 2584}
skipping logging after 82720 examples to avoid logging too frequently
train stats after 82752 examples: {'rewards_train/chosen': '-1.4526', 'rewards_train/rejected': '-2.0711', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61844', 'logps_train/rejected': '-147.79', 'logps_train/chosen': '-155.89', 'loss/train': '0.60654', 'examples_per_second': '44.437', 'grad_norm': '27.707', 'counters/examples': 82752, 'counters/updates': 2586}
skipping logging after 82784 examples to avoid logging too frequently
train stats after 82816 examples: {'rewards_train/chosen': '-0.91566', 'rewards_train/rejected': '-1.7622', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.84655', 'logps_train/rejected': '-120.69', 'logps_train/chosen': '-132.18', 'loss/train': '0.4972', 'examples_per_second': '46.163', 'grad_norm': '20.816', 'counters/examples': 82816, 'counters/updates': 2588}
skipping logging after 82848 examples to avoid logging too frequently
train stats after 82880 examples: {'rewards_train/chosen': '-1.3947', 'rewards_train/rejected': '-1.8901', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49534', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-137.34', 'loss/train': '0.6401', 'examples_per_second': '29.707', 'grad_norm': '25.156', 'counters/examples': 82880, 'counters/updates': 2590}
skipping logging after 82912 examples to avoid logging too frequently
train stats after 82944 examples: {'rewards_train/chosen': '-1.1927', 'rewards_train/rejected': '-1.8803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.68761', 'logps_train/rejected': '-182.01', 'logps_train/chosen': '-174.47', 'loss/train': '0.54518', 'examples_per_second': '45.254', 'grad_norm': '25.479', 'counters/examples': 82944, 'counters/updates': 2592}
skipping logging after 82976 examples to avoid logging too frequently
train stats after 83008 examples: {'rewards_train/chosen': '-1.2613', 'rewards_train/rejected': '-1.8535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.59222', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-127.93', 'loss/train': '0.60123', 'examples_per_second': '52.279', 'grad_norm': '23.44', 'counters/examples': 83008, 'counters/updates': 2594}
skipping logging after 83040 examples to avoid logging too frequently
train stats after 83072 examples: {'rewards_train/chosen': '-1.0408', 'rewards_train/rejected': '-1.8795', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.83869', 'logps_train/rejected': '-153.74', 'logps_train/chosen': '-146.94', 'loss/train': '0.50404', 'examples_per_second': '44.273', 'grad_norm': '26.31', 'counters/examples': 83072, 'counters/updates': 2596}
skipping logging after 83104 examples to avoid logging too frequently
train stats after 83136 examples: {'rewards_train/chosen': '-1.1335', 'rewards_train/rejected': '-1.5016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36807', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-132.94', 'loss/train': '0.59083', 'examples_per_second': '44.759', 'grad_norm': '23.38', 'counters/examples': 83136, 'counters/updates': 2598}
skipping logging after 83168 examples to avoid logging too frequently
train stats after 83200 examples: {'rewards_train/chosen': '-1.2181', 'rewards_train/rejected': '-1.4904', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27232', 'logps_train/rejected': '-112.38', 'logps_train/chosen': '-144.3', 'loss/train': '0.69371', 'examples_per_second': '48.651', 'grad_norm': '25.547', 'counters/examples': 83200, 'counters/updates': 2600}
skipping logging after 83232 examples to avoid logging too frequently
train stats after 83264 examples: {'rewards_train/chosen': '-1.2361', 'rewards_train/rejected': '-1.3874', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.15123', 'logps_train/rejected': '-142.33', 'logps_train/chosen': '-149.16', 'loss/train': '0.75997', 'examples_per_second': '45.041', 'grad_norm': '28.575', 'counters/examples': 83264, 'counters/updates': 2602}
skipping logging after 83296 examples to avoid logging too frequently
train stats after 83328 examples: {'rewards_train/chosen': '-1.2185', 'rewards_train/rejected': '-1.757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53855', 'logps_train/rejected': '-181.16', 'logps_train/chosen': '-167.05', 'loss/train': '0.6675', 'examples_per_second': '47.039', 'grad_norm': '28.737', 'counters/examples': 83328, 'counters/updates': 2604}
skipping logging after 83360 examples to avoid logging too frequently
train stats after 83392 examples: {'rewards_train/chosen': '-1.0447', 'rewards_train/rejected': '-1.4672', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4225', 'logps_train/rejected': '-97.704', 'logps_train/chosen': '-121.83', 'loss/train': '0.64805', 'examples_per_second': '48.63', 'grad_norm': '23.004', 'counters/examples': 83392, 'counters/updates': 2606}
skipping logging after 83424 examples to avoid logging too frequently
train stats after 83456 examples: {'rewards_train/chosen': '-1.1496', 'rewards_train/rejected': '-1.5084', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35876', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-157.28', 'loss/train': '0.62678', 'examples_per_second': '45.192', 'grad_norm': '25.71', 'counters/examples': 83456, 'counters/updates': 2608}
skipping logging after 83488 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '-1.2016', 'rewards_train/rejected': '-2.1089', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.90727', 'logps_train/rejected': '-198.34', 'logps_train/chosen': '-163.6', 'loss/train': '0.49461', 'examples_per_second': '46.888', 'grad_norm': '25.462', 'counters/examples': 83520, 'counters/updates': 2610}
skipping logging after 83552 examples to avoid logging too frequently
train stats after 83584 examples: {'rewards_train/chosen': '-1.3553', 'rewards_train/rejected': '-1.6802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32491', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-133.99', 'loss/train': '0.72051', 'examples_per_second': '47.765', 'grad_norm': '25.678', 'counters/examples': 83584, 'counters/updates': 2612}
skipping logging after 83616 examples to avoid logging too frequently
train stats after 83648 examples: {'rewards_train/chosen': '-1.1143', 'rewards_train/rejected': '-1.9235', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.80922', 'logps_train/rejected': '-150.31', 'logps_train/chosen': '-151.76', 'loss/train': '0.46867', 'examples_per_second': '51.416', 'grad_norm': '21.937', 'counters/examples': 83648, 'counters/updates': 2614}
skipping logging after 83680 examples to avoid logging too frequently
train stats after 83712 examples: {'rewards_train/chosen': '-1.294', 'rewards_train/rejected': '-1.4449', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15092', 'logps_train/rejected': '-128.88', 'logps_train/chosen': '-149.62', 'loss/train': '0.77277', 'examples_per_second': '45.12', 'grad_norm': '27.072', 'counters/examples': 83712, 'counters/updates': 2616}
skipping logging after 83744 examples to avoid logging too frequently
train stats after 83776 examples: {'rewards_train/chosen': '-1.3087', 'rewards_train/rejected': '-1.7949', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48619', 'logps_train/rejected': '-136.98', 'logps_train/chosen': '-140.15', 'loss/train': '0.56754', 'examples_per_second': '47.153', 'grad_norm': '24.037', 'counters/examples': 83776, 'counters/updates': 2618}
skipping logging after 83808 examples to avoid logging too frequently
train stats after 83840 examples: {'rewards_train/chosen': '-1.3973', 'rewards_train/rejected': '-1.854', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.45666', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-132.68', 'loss/train': '0.61316', 'examples_per_second': '47.747', 'grad_norm': '24.694', 'counters/examples': 83840, 'counters/updates': 2620}
skipping logging after 83872 examples to avoid logging too frequently
train stats after 83904 examples: {'rewards_train/chosen': '-0.82228', 'rewards_train/rejected': '-1.5809', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.75864', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-144.96', 'loss/train': '0.51859', 'examples_per_second': '45.963', 'grad_norm': '20.986', 'counters/examples': 83904, 'counters/updates': 2622}
skipping logging after 83936 examples to avoid logging too frequently
train stats after 83968 examples: {'rewards_train/chosen': '-1.1647', 'rewards_train/rejected': '-1.7374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57277', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-157.1', 'loss/train': '0.61888', 'examples_per_second': '44.827', 'grad_norm': '27.141', 'counters/examples': 83968, 'counters/updates': 2624}
skipping logging after 84000 examples to avoid logging too frequently
Running evaluation after 84000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.18it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.86it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.97it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.98it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.90it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.34it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.18it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.11it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.14it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.02it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.00it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  7.01it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.02it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.91it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.97it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.88it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.00it/s]
eval after 84000: {'rewards_eval/chosen': '-1.0525', 'rewards_eval/rejected': '-1.5499', 'rewards_eval/accuracies': '0.63281', 'rewards_eval/margins': '0.49741', 'logps_eval/rejected': '-133.78', 'logps_eval/chosen': '-146.98', 'loss/eval': '0.6297'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-84000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-84000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-84000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-84000/scheduler.pt...
train stats after 84032 examples: {'rewards_train/chosen': '-1.1697', 'rewards_train/rejected': '-1.6358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46613', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-169.02', 'loss/train': '0.69708', 'examples_per_second': '34.289', 'grad_norm': '28.908', 'counters/examples': 84032, 'counters/updates': 2626}
skipping logging after 84064 examples to avoid logging too frequently
train stats after 84096 examples: {'rewards_train/chosen': '-1.1535', 'rewards_train/rejected': '-1.9551', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.80159', 'logps_train/rejected': '-171.5', 'logps_train/chosen': '-159.91', 'loss/train': '0.47197', 'examples_per_second': '46.344', 'grad_norm': '24.539', 'counters/examples': 84096, 'counters/updates': 2628}
skipping logging after 84128 examples to avoid logging too frequently
train stats after 84160 examples: {'rewards_train/chosen': '-0.99507', 'rewards_train/rejected': '-1.4494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45428', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-123.02', 'loss/train': '0.59754', 'examples_per_second': '48.06', 'grad_norm': '23.234', 'counters/examples': 84160, 'counters/updates': 2630}
skipping logging after 84192 examples to avoid logging too frequently
train stats after 84224 examples: {'rewards_train/chosen': '-1.0487', 'rewards_train/rejected': '-1.5835', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53485', 'logps_train/rejected': '-122.76', 'logps_train/chosen': '-169.47', 'loss/train': '0.55903', 'examples_per_second': '45.496', 'grad_norm': '22.158', 'counters/examples': 84224, 'counters/updates': 2632}
skipping logging after 84256 examples to avoid logging too frequently
train stats after 84288 examples: {'rewards_train/chosen': '-1.0931', 'rewards_train/rejected': '-1.3282', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23506', 'logps_train/rejected': '-130.75', 'logps_train/chosen': '-182.32', 'loss/train': '0.6975', 'examples_per_second': '49.694', 'grad_norm': '36.896', 'counters/examples': 84288, 'counters/updates': 2634}
skipping logging after 84320 examples to avoid logging too frequently
train stats after 84352 examples: {'rewards_train/chosen': '-0.6152', 'rewards_train/rejected': '-1.0985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48328', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-154.18', 'loss/train': '0.56859', 'examples_per_second': '44.247', 'grad_norm': '22.641', 'counters/examples': 84352, 'counters/updates': 2636}
skipping logging after 84384 examples to avoid logging too frequently
train stats after 84416 examples: {'rewards_train/chosen': '-0.53265', 'rewards_train/rejected': '-1.2146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.68196', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-147.2', 'loss/train': '0.51746', 'examples_per_second': '44.579', 'grad_norm': '26.278', 'counters/examples': 84416, 'counters/updates': 2638}
skipping logging after 84448 examples to avoid logging too frequently
train stats after 84480 examples: {'rewards_train/chosen': '-0.71914', 'rewards_train/rejected': '-1.5072', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.78802', 'logps_train/rejected': '-134.93', 'logps_train/chosen': '-138.59', 'loss/train': '0.52964', 'examples_per_second': '44.268', 'grad_norm': '24.487', 'counters/examples': 84480, 'counters/updates': 2640}
skipping logging after 84512 examples to avoid logging too frequently
train stats after 84544 examples: {'rewards_train/chosen': '-0.56467', 'rewards_train/rejected': '-1.2968', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.73217', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-166.15', 'loss/train': '0.51167', 'examples_per_second': '45.532', 'grad_norm': '22.864', 'counters/examples': 84544, 'counters/updates': 2642}
skipping logging after 84576 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '-0.75412', 'rewards_train/rejected': '-1.4717', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.71754', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-110.57', 'loss/train': '0.5822', 'examples_per_second': '45.627', 'grad_norm': '26.431', 'counters/examples': 84608, 'counters/updates': 2644}
skipping logging after 84640 examples to avoid logging too frequently
train stats after 84672 examples: {'rewards_train/chosen': '-0.29495', 'rewards_train/rejected': '-1.1253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.83036', 'logps_train/rejected': '-135.33', 'logps_train/chosen': '-124.31', 'loss/train': '0.56141', 'examples_per_second': '44.736', 'grad_norm': '23.391', 'counters/examples': 84672, 'counters/updates': 2646}
skipping logging after 84704 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '-0.92818', 'rewards_train/rejected': '-1.7805', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.85235', 'logps_train/rejected': '-143.9', 'logps_train/chosen': '-146.18', 'loss/train': '0.54789', 'examples_per_second': '45.596', 'grad_norm': '24.599', 'counters/examples': 84736, 'counters/updates': 2648}
skipping logging after 84768 examples to avoid logging too frequently
train stats after 84800 examples: {'rewards_train/chosen': '-0.732', 'rewards_train/rejected': '-1.1912', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.45924', 'logps_train/rejected': '-137.74', 'logps_train/chosen': '-136.6', 'loss/train': '0.64808', 'examples_per_second': '45.412', 'grad_norm': '27.295', 'counters/examples': 84800, 'counters/updates': 2650}
skipping logging after 84832 examples to avoid logging too frequently
train stats after 84864 examples: {'rewards_train/chosen': '-0.57415', 'rewards_train/rejected': '-1.5938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '1.0196', 'logps_train/rejected': '-154.87', 'logps_train/chosen': '-139.79', 'loss/train': '0.47018', 'examples_per_second': '44.394', 'grad_norm': '23.324', 'counters/examples': 84864, 'counters/updates': 2652}
skipping logging after 84896 examples to avoid logging too frequently
train stats after 84928 examples: {'rewards_train/chosen': '-1.0467', 'rewards_train/rejected': '-1.8947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.84806', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-142.96', 'loss/train': '0.66895', 'examples_per_second': '45.95', 'grad_norm': '26.543', 'counters/examples': 84928, 'counters/updates': 2654}
skipping logging after 84960 examples to avoid logging too frequently
train stats after 84992 examples: {'rewards_train/chosen': '-0.95621', 'rewards_train/rejected': '-1.5017', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54545', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-127.13', 'loss/train': '0.57084', 'examples_per_second': '48.388', 'grad_norm': '22.576', 'counters/examples': 84992, 'counters/updates': 2656}
skipping logging after 85024 examples to avoid logging too frequently
train stats after 85056 examples: {'rewards_train/chosen': '-1.0288', 'rewards_train/rejected': '-1.7637', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.73493', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-161.2', 'loss/train': '0.48216', 'examples_per_second': '45.486', 'grad_norm': '23.485', 'counters/examples': 85056, 'counters/updates': 2658}
skipping logging after 85088 examples to avoid logging too frequently
train stats after 85120 examples: {'rewards_train/chosen': '-0.86808', 'rewards_train/rejected': '-1.508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.63992', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-173.73', 'loss/train': '0.52616', 'examples_per_second': '45.533', 'grad_norm': '26.527', 'counters/examples': 85120, 'counters/updates': 2660}
skipping logging after 85152 examples to avoid logging too frequently
train stats after 85184 examples: {'rewards_train/chosen': '-1.1711', 'rewards_train/rejected': '-1.9101', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.73903', 'logps_train/rejected': '-156.24', 'logps_train/chosen': '-129.94', 'loss/train': '0.51475', 'examples_per_second': '45.664', 'grad_norm': '22.636', 'counters/examples': 85184, 'counters/updates': 2662}
skipping logging after 85216 examples to avoid logging too frequently
train stats after 85248 examples: {'rewards_train/chosen': '-0.73138', 'rewards_train/rejected': '-1.455', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.7236', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-144.83', 'loss/train': '0.5624', 'examples_per_second': '46.579', 'grad_norm': '23.971', 'counters/examples': 85248, 'counters/updates': 2664}
skipping logging after 85280 examples to avoid logging too frequently
train stats after 85312 examples: {'rewards_train/chosen': '-0.77747', 'rewards_train/rejected': '-1.2999', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.52246', 'logps_train/rejected': '-153.09', 'logps_train/chosen': '-160.03', 'loss/train': '0.64983', 'examples_per_second': '45.71', 'grad_norm': '26.891', 'counters/examples': 85312, 'counters/updates': 2666}
skipping logging after 85344 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '-0.81196', 'rewards_train/rejected': '-1.6574', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.84547', 'logps_train/rejected': '-179.96', 'logps_train/chosen': '-146.88', 'loss/train': '0.45137', 'examples_per_second': '46.397', 'grad_norm': '21.661', 'counters/examples': 85376, 'counters/updates': 2668}
skipping logging after 85408 examples to avoid logging too frequently
train stats after 85440 examples: {'rewards_train/chosen': '-0.55323', 'rewards_train/rejected': '-1.3771', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.82383', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-140.37', 'loss/train': '0.4603', 'examples_per_second': '46.636', 'grad_norm': '21.871', 'counters/examples': 85440, 'counters/updates': 2670}
skipping logging after 85472 examples to avoid logging too frequently
train stats after 85504 examples: {'rewards_train/chosen': '-1.0151', 'rewards_train/rejected': '-2.2109', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '1.1958', 'logps_train/rejected': '-154.63', 'logps_train/chosen': '-147.74', 'loss/train': '0.43177', 'examples_per_second': '47.828', 'grad_norm': '19.309', 'counters/examples': 85504, 'counters/updates': 2672}
skipping logging after 85536 examples to avoid logging too frequently
train stats after 85568 examples: {'rewards_train/chosen': '-1.059', 'rewards_train/rejected': '-1.9267', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.86767', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-114.53', 'loss/train': '0.48195', 'examples_per_second': '45.556', 'grad_norm': '19.641', 'counters/examples': 85568, 'counters/updates': 2674}
skipping logging after 85600 examples to avoid logging too frequently
train stats after 85632 examples: {'rewards_train/chosen': '-0.50633', 'rewards_train/rejected': '-1.3284', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.82205', 'logps_train/rejected': '-157.12', 'logps_train/chosen': '-152.57', 'loss/train': '0.47583', 'examples_per_second': '45.63', 'grad_norm': '20.589', 'counters/examples': 85632, 'counters/updates': 2676}
skipping logging after 85664 examples to avoid logging too frequently
train stats after 85696 examples: {'rewards_train/chosen': '-0.942', 'rewards_train/rejected': '-1.2421', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30013', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-147.84', 'loss/train': '0.6749', 'examples_per_second': '46.958', 'grad_norm': '26.76', 'counters/examples': 85696, 'counters/updates': 2678}
skipping logging after 85728 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '-0.96013', 'rewards_train/rejected': '-1.6134', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.65326', 'logps_train/rejected': '-149.2', 'logps_train/chosen': '-144.56', 'loss/train': '0.53803', 'examples_per_second': '45.528', 'grad_norm': '26.134', 'counters/examples': 85760, 'counters/updates': 2680}
skipping logging after 85792 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '-1.2024', 'rewards_train/rejected': '-1.9408', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.73843', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-162.88', 'loss/train': '0.47181', 'examples_per_second': '44.29', 'grad_norm': '21.583', 'counters/examples': 85824, 'counters/updates': 2682}
skipping logging after 85856 examples to avoid logging too frequently
train stats after 85888 examples: {'rewards_train/chosen': '-1.2785', 'rewards_train/rejected': '-2.0724', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.79388', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-162.78', 'loss/train': '0.58256', 'examples_per_second': '46.53', 'grad_norm': '24.144', 'counters/examples': 85888, 'counters/updates': 2684}
skipping logging after 85920 examples to avoid logging too frequently
train stats after 85952 examples: {'rewards_train/chosen': '-1.1499', 'rewards_train/rejected': '-1.844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.69409', 'logps_train/rejected': '-146.33', 'logps_train/chosen': '-155.36', 'loss/train': '0.576', 'examples_per_second': '31.406', 'grad_norm': '25.735', 'counters/examples': 85952, 'counters/updates': 2686}
skipping logging after 85984 examples to avoid logging too frequently
train stats after 86016 examples: {'rewards_train/chosen': '-1.3999', 'rewards_train/rejected': '-1.6491', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.24914', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-155.64', 'loss/train': '0.74005', 'examples_per_second': '45.265', 'grad_norm': '29.597', 'counters/examples': 86016, 'counters/updates': 2688}
skipping logging after 86048 examples to avoid logging too frequently
train stats after 86080 examples: {'rewards_train/chosen': '-1.4193', 'rewards_train/rejected': '-1.855', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.43576', 'logps_train/rejected': '-190.26', 'logps_train/chosen': '-193.99', 'loss/train': '0.60038', 'examples_per_second': '44.484', 'grad_norm': '28.081', 'counters/examples': 86080, 'counters/updates': 2690}
skipping logging after 86112 examples to avoid logging too frequently
train stats after 86144 examples: {'rewards_train/chosen': '-1.0127', 'rewards_train/rejected': '-1.7003', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.68765', 'logps_train/rejected': '-154.08', 'logps_train/chosen': '-155.19', 'loss/train': '0.59657', 'examples_per_second': '47.044', 'grad_norm': '26.68', 'counters/examples': 86144, 'counters/updates': 2692}
skipping logging after 86176 examples to avoid logging too frequently
train stats after 86208 examples: {'rewards_train/chosen': '-0.88112', 'rewards_train/rejected': '-1.8491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.968', 'logps_train/rejected': '-117.33', 'logps_train/chosen': '-126.12', 'loss/train': '0.45587', 'examples_per_second': '46.205', 'grad_norm': '21.024', 'counters/examples': 86208, 'counters/updates': 2694}
skipping logging after 86240 examples to avoid logging too frequently
train stats after 86272 examples: {'rewards_train/chosen': '-1.0013', 'rewards_train/rejected': '-1.5925', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5912', 'logps_train/rejected': '-159.41', 'logps_train/chosen': '-125.64', 'loss/train': '0.63004', 'examples_per_second': '47.297', 'grad_norm': '25.088', 'counters/examples': 86272, 'counters/updates': 2696}
skipping logging after 86304 examples to avoid logging too frequently
train stats after 86336 examples: {'rewards_train/chosen': '-1.1288', 'rewards_train/rejected': '-1.7426', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.61386', 'logps_train/rejected': '-165.41', 'logps_train/chosen': '-118.15', 'loss/train': '0.55518', 'examples_per_second': '47.041', 'grad_norm': '22.822', 'counters/examples': 86336, 'counters/updates': 2698}
skipping logging after 86368 examples to avoid logging too frequently
train stats after 86400 examples: {'rewards_train/chosen': '-0.92702', 'rewards_train/rejected': '-1.3481', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42108', 'logps_train/rejected': '-125.88', 'logps_train/chosen': '-129.87', 'loss/train': '0.56762', 'examples_per_second': '44.846', 'grad_norm': '22.429', 'counters/examples': 86400, 'counters/updates': 2700}
skipping logging after 86432 examples to avoid logging too frequently
train stats after 86464 examples: {'rewards_train/chosen': '-1.1596', 'rewards_train/rejected': '-1.3472', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18765', 'logps_train/rejected': '-113.26', 'logps_train/chosen': '-166.96', 'loss/train': '0.69387', 'examples_per_second': '45.579', 'grad_norm': '27.793', 'counters/examples': 86464, 'counters/updates': 2702}
skipping logging after 86496 examples to avoid logging too frequently
train stats after 86528 examples: {'rewards_train/chosen': '-0.81595', 'rewards_train/rejected': '-1.1162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30026', 'logps_train/rejected': '-129.07', 'logps_train/chosen': '-126.72', 'loss/train': '0.69567', 'examples_per_second': '45.549', 'grad_norm': '25.491', 'counters/examples': 86528, 'counters/updates': 2704}
skipping logging after 86560 examples to avoid logging too frequently
train stats after 86592 examples: {'rewards_train/chosen': '-0.95593', 'rewards_train/rejected': '-1.5334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.57743', 'logps_train/rejected': '-155.67', 'logps_train/chosen': '-130.53', 'loss/train': '0.55064', 'examples_per_second': '49.922', 'grad_norm': '24.949', 'counters/examples': 86592, 'counters/updates': 2706}
skipping logging after 86624 examples to avoid logging too frequently
train stats after 86656 examples: {'rewards_train/chosen': '-0.99641', 'rewards_train/rejected': '-1.4881', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.49172', 'logps_train/rejected': '-183.68', 'logps_train/chosen': '-175.45', 'loss/train': '0.75507', 'examples_per_second': '45.399', 'grad_norm': '32.871', 'counters/examples': 86656, 'counters/updates': 2708}
skipping logging after 86688 examples to avoid logging too frequently
train stats after 86720 examples: {'rewards_train/chosen': '-1.272', 'rewards_train/rejected': '-1.6142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3422', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-125.07', 'loss/train': '0.70232', 'examples_per_second': '44.096', 'grad_norm': '25.061', 'counters/examples': 86720, 'counters/updates': 2710}
skipping logging after 86752 examples to avoid logging too frequently
train stats after 86784 examples: {'rewards_train/chosen': '-0.96215', 'rewards_train/rejected': '-1.529', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.56683', 'logps_train/rejected': '-161.98', 'logps_train/chosen': '-121.7', 'loss/train': '0.55002', 'examples_per_second': '44.144', 'grad_norm': '23.683', 'counters/examples': 86784, 'counters/updates': 2712}
skipping logging after 86816 examples to avoid logging too frequently
train stats after 86848 examples: {'rewards_train/chosen': '-1.1148', 'rewards_train/rejected': '-1.675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56026', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-141.51', 'loss/train': '0.62919', 'examples_per_second': '46.348', 'grad_norm': '25.661', 'counters/examples': 86848, 'counters/updates': 2714}
skipping logging after 86880 examples to avoid logging too frequently
train stats after 86912 examples: {'rewards_train/chosen': '-0.75619', 'rewards_train/rejected': '-1.6386', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.88238', 'logps_train/rejected': '-152.11', 'logps_train/chosen': '-116.23', 'loss/train': '0.4727', 'examples_per_second': '47.025', 'grad_norm': '21.593', 'counters/examples': 86912, 'counters/updates': 2716}
skipping logging after 86944 examples to avoid logging too frequently
train stats after 86976 examples: {'rewards_train/chosen': '-1.0682', 'rewards_train/rejected': '-1.6792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61098', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-151.21', 'loss/train': '0.55666', 'examples_per_second': '48.5', 'grad_norm': '23.96', 'counters/examples': 86976, 'counters/updates': 2718}
skipping logging after 87008 examples to avoid logging too frequently
train stats after 87040 examples: {'rewards_train/chosen': '-1.0891', 'rewards_train/rejected': '-1.6372', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54816', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-147.41', 'loss/train': '0.59593', 'examples_per_second': '46.75', 'grad_norm': '28.414', 'counters/examples': 87040, 'counters/updates': 2720}
skipping logging after 87072 examples to avoid logging too frequently
train stats after 87104 examples: {'rewards_train/chosen': '-1.09', 'rewards_train/rejected': '-1.578', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48804', 'logps_train/rejected': '-136.29', 'logps_train/chosen': '-161.81', 'loss/train': '0.70697', 'examples_per_second': '45.032', 'grad_norm': '33.906', 'counters/examples': 87104, 'counters/updates': 2722}
skipping logging after 87136 examples to avoid logging too frequently
train stats after 87168 examples: {'rewards_train/chosen': '-1.0303', 'rewards_train/rejected': '-1.7164', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.68612', 'logps_train/rejected': '-164.38', 'logps_train/chosen': '-132.56', 'loss/train': '0.49334', 'examples_per_second': '44.779', 'grad_norm': '23.15', 'counters/examples': 87168, 'counters/updates': 2724}
skipping logging after 87200 examples to avoid logging too frequently
train stats after 87232 examples: {'rewards_train/chosen': '-1.3719', 'rewards_train/rejected': '-2.0597', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6878', 'logps_train/rejected': '-138.8', 'logps_train/chosen': '-125.59', 'loss/train': '0.53983', 'examples_per_second': '54.869', 'grad_norm': '23.954', 'counters/examples': 87232, 'counters/updates': 2726}
skipping logging after 87264 examples to avoid logging too frequently
train stats after 87296 examples: {'rewards_train/chosen': '-0.99163', 'rewards_train/rejected': '-1.5713', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57967', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-163.84', 'loss/train': '0.57506', 'examples_per_second': '49.146', 'grad_norm': '26.896', 'counters/examples': 87296, 'counters/updates': 2728}
skipping logging after 87328 examples to avoid logging too frequently
train stats after 87360 examples: {'rewards_train/chosen': '-0.9762', 'rewards_train/rejected': '-2.0876', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '1.1114', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-140.27', 'loss/train': '0.44732', 'examples_per_second': '44.186', 'grad_norm': '20.976', 'counters/examples': 87360, 'counters/updates': 2730}
skipping logging after 87392 examples to avoid logging too frequently
train stats after 87424 examples: {'rewards_train/chosen': '-1.3362', 'rewards_train/rejected': '-1.5988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26256', 'logps_train/rejected': '-157.88', 'logps_train/chosen': '-122.63', 'loss/train': '0.68905', 'examples_per_second': '45.385', 'grad_norm': '27.902', 'counters/examples': 87424, 'counters/updates': 2732}
skipping logging after 87456 examples to avoid logging too frequently
train stats after 87488 examples: {'rewards_train/chosen': '-0.52374', 'rewards_train/rejected': '-1.4188', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.89502', 'logps_train/rejected': '-151.18', 'logps_train/chosen': '-110.43', 'loss/train': '0.49201', 'examples_per_second': '48.352', 'grad_norm': '20.927', 'counters/examples': 87488, 'counters/updates': 2734}
skipping logging after 87520 examples to avoid logging too frequently
train stats after 87552 examples: {'rewards_train/chosen': '-0.77206', 'rewards_train/rejected': '-1.557', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.78494', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-143.18', 'loss/train': '0.52273', 'examples_per_second': '46.226', 'grad_norm': '24.693', 'counters/examples': 87552, 'counters/updates': 2736}
skipping logging after 87584 examples to avoid logging too frequently
train stats after 87616 examples: {'rewards_train/chosen': '-0.90063', 'rewards_train/rejected': '-1.6826', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.78195', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-129.89', 'loss/train': '0.46397', 'examples_per_second': '44.811', 'grad_norm': '22.608', 'counters/examples': 87616, 'counters/updates': 2738}
skipping logging after 87648 examples to avoid logging too frequently
train stats after 87680 examples: {'rewards_train/chosen': '-0.90256', 'rewards_train/rejected': '-1.4415', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53899', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-135.83', 'loss/train': '0.62661', 'examples_per_second': '45.285', 'grad_norm': '26.031', 'counters/examples': 87680, 'counters/updates': 2740}
skipping logging after 87712 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '-0.82828', 'rewards_train/rejected': '-1.5047', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.67639', 'logps_train/rejected': '-116.16', 'logps_train/chosen': '-171.11', 'loss/train': '0.56993', 'examples_per_second': '46.29', 'grad_norm': '29.669', 'counters/examples': 87744, 'counters/updates': 2742}
skipping logging after 87776 examples to avoid logging too frequently
train stats after 87808 examples: {'rewards_train/chosen': '-0.55076', 'rewards_train/rejected': '-1.694', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '1.1432', 'logps_train/rejected': '-161.31', 'logps_train/chosen': '-136.42', 'loss/train': '0.40004', 'examples_per_second': '48.212', 'grad_norm': '17.478', 'counters/examples': 87808, 'counters/updates': 2744}
skipping logging after 87840 examples to avoid logging too frequently
train stats after 87872 examples: {'rewards_train/chosen': '-0.87311', 'rewards_train/rejected': '-1.2864', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41333', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-141.54', 'loss/train': '0.65134', 'examples_per_second': '47.152', 'grad_norm': '26.036', 'counters/examples': 87872, 'counters/updates': 2746}
skipping logging after 87904 examples to avoid logging too frequently
train stats after 87936 examples: {'rewards_train/chosen': '-0.77996', 'rewards_train/rejected': '-1.2011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42117', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-134.29', 'loss/train': '0.62455', 'examples_per_second': '44.186', 'grad_norm': '25.823', 'counters/examples': 87936, 'counters/updates': 2748}
skipping logging after 87968 examples to avoid logging too frequently
train stats after 88000 examples: {'rewards_train/chosen': '-0.71294', 'rewards_train/rejected': '-1.0283', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31532', 'logps_train/rejected': '-104.15', 'logps_train/chosen': '-96.116', 'loss/train': '0.66291', 'examples_per_second': '47.602', 'grad_norm': '23.73', 'counters/examples': 88000, 'counters/updates': 2750}
skipping logging after 88032 examples to avoid logging too frequently
train stats after 88064 examples: {'rewards_train/chosen': '-0.91721', 'rewards_train/rejected': '-1.2951', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37792', 'logps_train/rejected': '-128.71', 'logps_train/chosen': '-163.36', 'loss/train': '0.68197', 'examples_per_second': '46.055', 'grad_norm': '30.619', 'counters/examples': 88064, 'counters/updates': 2752}
skipping logging after 88096 examples to avoid logging too frequently
train stats after 88128 examples: {'rewards_train/chosen': '-0.78971', 'rewards_train/rejected': '-1.3608', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57109', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-132.97', 'loss/train': '0.54619', 'examples_per_second': '45.664', 'grad_norm': '21.727', 'counters/examples': 88128, 'counters/updates': 2754}
skipping logging after 88160 examples to avoid logging too frequently
train stats after 88192 examples: {'rewards_train/chosen': '-1.0668', 'rewards_train/rejected': '-1.4985', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43161', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-117.05', 'loss/train': '0.57017', 'examples_per_second': '44.283', 'grad_norm': '24.767', 'counters/examples': 88192, 'counters/updates': 2756}
skipping logging after 88224 examples to avoid logging too frequently
train stats after 88256 examples: {'rewards_train/chosen': '-0.74889', 'rewards_train/rejected': '-1.5793', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.83043', 'logps_train/rejected': '-116.84', 'logps_train/chosen': '-141.64', 'loss/train': '0.52557', 'examples_per_second': '46.209', 'grad_norm': '21.99', 'counters/examples': 88256, 'counters/updates': 2758}
skipping logging after 88288 examples to avoid logging too frequently
train stats after 88320 examples: {'rewards_train/chosen': '-0.88491', 'rewards_train/rejected': '-1.1387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2538', 'logps_train/rejected': '-153.57', 'logps_train/chosen': '-130.54', 'loss/train': '0.74864', 'examples_per_second': '44.886', 'grad_norm': '28.439', 'counters/examples': 88320, 'counters/updates': 2760}
skipping logging after 88352 examples to avoid logging too frequently
train stats after 88384 examples: {'rewards_train/chosen': '-1.2091', 'rewards_train/rejected': '-1.7915', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58238', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-132.96', 'loss/train': '0.53226', 'examples_per_second': '44.185', 'grad_norm': '22.535', 'counters/examples': 88384, 'counters/updates': 2762}
skipping logging after 88416 examples to avoid logging too frequently
train stats after 88448 examples: {'rewards_train/chosen': '-0.95693', 'rewards_train/rejected': '-1.5878', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.63087', 'logps_train/rejected': '-131.26', 'logps_train/chosen': '-144.54', 'loss/train': '0.48148', 'examples_per_second': '46.405', 'grad_norm': '22.324', 'counters/examples': 88448, 'counters/updates': 2764}
skipping logging after 88480 examples to avoid logging too frequently
train stats after 88512 examples: {'rewards_train/chosen': '-1.0225', 'rewards_train/rejected': '-1.7604', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.73787', 'logps_train/rejected': '-165.49', 'logps_train/chosen': '-168.9', 'loss/train': '0.45605', 'examples_per_second': '44.242', 'grad_norm': '24.372', 'counters/examples': 88512, 'counters/updates': 2766}
skipping logging after 88544 examples to avoid logging too frequently
train stats after 88576 examples: {'rewards_train/chosen': '-1.268', 'rewards_train/rejected': '-1.7019', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43391', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-162.71', 'loss/train': '0.67639', 'examples_per_second': '45.742', 'grad_norm': '28.78', 'counters/examples': 88576, 'counters/updates': 2768}
skipping logging after 88608 examples to avoid logging too frequently
train stats after 88640 examples: {'rewards_train/chosen': '-0.60592', 'rewards_train/rejected': '-1.081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.47506', 'logps_train/rejected': '-126.74', 'logps_train/chosen': '-140.53', 'loss/train': '0.58142', 'examples_per_second': '44.445', 'grad_norm': '24.133', 'counters/examples': 88640, 'counters/updates': 2770}
skipping logging after 88672 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '-0.9444', 'rewards_train/rejected': '-1.4898', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.5454', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-134.03', 'loss/train': '0.59123', 'examples_per_second': '47.965', 'grad_norm': '24.519', 'counters/examples': 88704, 'counters/updates': 2772}
skipping logging after 88736 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '-0.94487', 'rewards_train/rejected': '-1.7925', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.84765', 'logps_train/rejected': '-150.07', 'logps_train/chosen': '-132.42', 'loss/train': '0.47557', 'examples_per_second': '45.192', 'grad_norm': '25.085', 'counters/examples': 88768, 'counters/updates': 2774}
skipping logging after 88800 examples to avoid logging too frequently
train stats after 88832 examples: {'rewards_train/chosen': '-0.9268', 'rewards_train/rejected': '-1.5503', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.62351', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-150.81', 'loss/train': '0.64025', 'examples_per_second': '46.915', 'grad_norm': '27.486', 'counters/examples': 88832, 'counters/updates': 2776}
skipping logging after 88864 examples to avoid logging too frequently
train stats after 88896 examples: {'rewards_train/chosen': '-0.60169', 'rewards_train/rejected': '-1.5922', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.99055', 'logps_train/rejected': '-134.1', 'logps_train/chosen': '-132.91', 'loss/train': '0.47342', 'examples_per_second': '44.847', 'grad_norm': '20.101', 'counters/examples': 88896, 'counters/updates': 2778}
skipping logging after 88928 examples to avoid logging too frequently
train stats after 88960 examples: {'rewards_train/chosen': '-1.2816', 'rewards_train/rejected': '-2.071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.78935', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-136.95', 'loss/train': '0.57853', 'examples_per_second': '48.445', 'grad_norm': '23.083', 'counters/examples': 88960, 'counters/updates': 2780}
skipping logging after 88992 examples to avoid logging too frequently
train stats after 89024 examples: {'rewards_train/chosen': '-1.1295', 'rewards_train/rejected': '-1.5686', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43914', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-129.36', 'loss/train': '0.64868', 'examples_per_second': '46.314', 'grad_norm': '25.059', 'counters/examples': 89024, 'counters/updates': 2782}
skipping logging after 89056 examples to avoid logging too frequently
train stats after 89088 examples: {'rewards_train/chosen': '-0.85582', 'rewards_train/rejected': '-1.5555', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.69965', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-166.09', 'loss/train': '0.54131', 'examples_per_second': '46.052', 'grad_norm': '26.337', 'counters/examples': 89088, 'counters/updates': 2784}
skipping logging after 89120 examples to avoid logging too frequently
train stats after 89152 examples: {'rewards_train/chosen': '-0.88478', 'rewards_train/rejected': '-1.0906', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20586', 'logps_train/rejected': '-146.52', 'logps_train/chosen': '-169.21', 'loss/train': '0.67117', 'examples_per_second': '44.342', 'grad_norm': '31.368', 'counters/examples': 89152, 'counters/updates': 2786}
skipping logging after 89184 examples to avoid logging too frequently
train stats after 89216 examples: {'rewards_train/chosen': '-1.0882', 'rewards_train/rejected': '-1.6272', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53903', 'logps_train/rejected': '-123.78', 'logps_train/chosen': '-155.9', 'loss/train': '0.63891', 'examples_per_second': '35.205', 'grad_norm': '26.901', 'counters/examples': 89216, 'counters/updates': 2788}
skipping logging after 89248 examples to avoid logging too frequently
train stats after 89280 examples: {'rewards_train/chosen': '-0.78535', 'rewards_train/rejected': '-1.4236', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.63822', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-147.28', 'loss/train': '0.52235', 'examples_per_second': '45.181', 'grad_norm': '23.486', 'counters/examples': 89280, 'counters/updates': 2790}
skipping logging after 89312 examples to avoid logging too frequently
train stats after 89344 examples: {'rewards_train/chosen': '-1.1087', 'rewards_train/rejected': '-1.4758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36713', 'logps_train/rejected': '-154.01', 'logps_train/chosen': '-186.69', 'loss/train': '0.63952', 'examples_per_second': '45.467', 'grad_norm': '27.089', 'counters/examples': 89344, 'counters/updates': 2792}
skipping logging after 89376 examples to avoid logging too frequently
train stats after 89408 examples: {'rewards_train/chosen': '-1.1514', 'rewards_train/rejected': '-1.6864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53494', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-136.08', 'loss/train': '0.58487', 'examples_per_second': '45.89', 'grad_norm': '25.455', 'counters/examples': 89408, 'counters/updates': 2794}
skipping logging after 89440 examples to avoid logging too frequently
train stats after 89472 examples: {'rewards_train/chosen': '-1.0958', 'rewards_train/rejected': '-1.7079', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.6121', 'logps_train/rejected': '-127.07', 'logps_train/chosen': '-137.9', 'loss/train': '0.59337', 'examples_per_second': '46.716', 'grad_norm': '26.266', 'counters/examples': 89472, 'counters/updates': 2796}
skipping logging after 89504 examples to avoid logging too frequently
train stats after 89536 examples: {'rewards_train/chosen': '-0.82967', 'rewards_train/rejected': '-1.2085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.37886', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-156.22', 'loss/train': '0.72622', 'examples_per_second': '45.408', 'grad_norm': '28.913', 'counters/examples': 89536, 'counters/updates': 2798}
skipping logging after 89568 examples to avoid logging too frequently
train stats after 89600 examples: {'rewards_train/chosen': '-1.0921', 'rewards_train/rejected': '-1.3902', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29814', 'logps_train/rejected': '-167.83', 'logps_train/chosen': '-132.88', 'loss/train': '0.67981', 'examples_per_second': '44.901', 'grad_norm': '28.997', 'counters/examples': 89600, 'counters/updates': 2800}
skipping logging after 89632 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '-0.64284', 'rewards_train/rejected': '-1.4555', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.81264', 'logps_train/rejected': '-143.72', 'logps_train/chosen': '-146.38', 'loss/train': '0.50931', 'examples_per_second': '46.234', 'grad_norm': '23.648', 'counters/examples': 89664, 'counters/updates': 2802}
skipping logging after 89696 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '-0.82212', 'rewards_train/rejected': '-1.2275', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.40538', 'logps_train/rejected': '-151.44', 'logps_train/chosen': '-168.62', 'loss/train': '0.63017', 'examples_per_second': '45.416', 'grad_norm': '27.857', 'counters/examples': 89728, 'counters/updates': 2804}
skipping logging after 89760 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '-1.0399', 'rewards_train/rejected': '-1.2849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24495', 'logps_train/rejected': '-179.07', 'logps_train/chosen': '-128.51', 'loss/train': '0.67619', 'examples_per_second': '45.435', 'grad_norm': '27.801', 'counters/examples': 89792, 'counters/updates': 2806}
skipping logging after 89824 examples to avoid logging too frequently
train stats after 89856 examples: {'rewards_train/chosen': '-1.1926', 'rewards_train/rejected': '-1.6059', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4133', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-160.64', 'loss/train': '0.63868', 'examples_per_second': '45.412', 'grad_norm': '26.406', 'counters/examples': 89856, 'counters/updates': 2808}
skipping logging after 89888 examples to avoid logging too frequently
train stats after 89920 examples: {'rewards_train/chosen': '-0.8621', 'rewards_train/rejected': '-1.3815', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.51938', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-151.89', 'loss/train': '0.58445', 'examples_per_second': '43.981', 'grad_norm': '23.704', 'counters/examples': 89920, 'counters/updates': 2810}
skipping logging after 89952 examples to avoid logging too frequently
train stats after 89984 examples: {'rewards_train/chosen': '-0.96871', 'rewards_train/rejected': '-1.5586', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5899', 'logps_train/rejected': '-145.76', 'logps_train/chosen': '-181.61', 'loss/train': '0.54306', 'examples_per_second': '45.915', 'grad_norm': '25.484', 'counters/examples': 89984, 'counters/updates': 2812}
skipping logging after 90016 examples to avoid logging too frequently
train stats after 90048 examples: {'rewards_train/chosen': '-1.0263', 'rewards_train/rejected': '-1.3947', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.36842', 'logps_train/rejected': '-172.65', 'logps_train/chosen': '-142.37', 'loss/train': '0.70639', 'examples_per_second': '44.34', 'grad_norm': '30.249', 'counters/examples': 90048, 'counters/updates': 2814}
skipping logging after 90080 examples to avoid logging too frequently
train stats after 90112 examples: {'rewards_train/chosen': '-0.73667', 'rewards_train/rejected': '-1.6121', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.87547', 'logps_train/rejected': '-135.35', 'logps_train/chosen': '-154.09', 'loss/train': '0.49763', 'examples_per_second': '45.595', 'grad_norm': '21.099', 'counters/examples': 90112, 'counters/updates': 2816}
skipping logging after 90144 examples to avoid logging too frequently
train stats after 90176 examples: {'rewards_train/chosen': '-0.85257', 'rewards_train/rejected': '-1.7297', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.87712', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-152.65', 'loss/train': '0.52221', 'examples_per_second': '44.719', 'grad_norm': '24.664', 'counters/examples': 90176, 'counters/updates': 2818}
skipping logging after 90208 examples to avoid logging too frequently
train stats after 90240 examples: {'rewards_train/chosen': '-1.0588', 'rewards_train/rejected': '-1.776', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.71723', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-184.45', 'loss/train': '0.6299', 'examples_per_second': '44.709', 'grad_norm': '25.571', 'counters/examples': 90240, 'counters/updates': 2820}
skipping logging after 90272 examples to avoid logging too frequently
train stats after 90304 examples: {'rewards_train/chosen': '-0.76855', 'rewards_train/rejected': '-1.3038', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.53521', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-144.35', 'loss/train': '0.62905', 'examples_per_second': '44.464', 'grad_norm': '26.58', 'counters/examples': 90304, 'counters/updates': 2822}
skipping logging after 90336 examples to avoid logging too frequently
train stats after 90368 examples: {'rewards_train/chosen': '-0.50525', 'rewards_train/rejected': '-1.1446', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.63936', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-118.19', 'loss/train': '0.4964', 'examples_per_second': '46.017', 'grad_norm': '20.989', 'counters/examples': 90368, 'counters/updates': 2824}
skipping logging after 90400 examples to avoid logging too frequently
train stats after 90432 examples: {'rewards_train/chosen': '-1.0776', 'rewards_train/rejected': '-1.5419', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46426', 'logps_train/rejected': '-88.275', 'logps_train/chosen': '-117.5', 'loss/train': '0.59042', 'examples_per_second': '48.567', 'grad_norm': '20.206', 'counters/examples': 90432, 'counters/updates': 2826}
skipping logging after 90464 examples to avoid logging too frequently
train stats after 90496 examples: {'rewards_train/chosen': '-1.1518', 'rewards_train/rejected': '-1.5672', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41536', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-126.97', 'loss/train': '0.60947', 'examples_per_second': '43.888', 'grad_norm': '25.619', 'counters/examples': 90496, 'counters/updates': 2828}
skipping logging after 90528 examples to avoid logging too frequently
train stats after 90560 examples: {'rewards_train/chosen': '-0.83317', 'rewards_train/rejected': '-1.6253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.79215', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-159.99', 'loss/train': '0.58003', 'examples_per_second': '44.148', 'grad_norm': '28.253', 'counters/examples': 90560, 'counters/updates': 2830}
skipping logging after 90592 examples to avoid logging too frequently
train stats after 90624 examples: {'rewards_train/chosen': '-0.81147', 'rewards_train/rejected': '-1.5648', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.75337', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-114.51', 'loss/train': '0.55858', 'examples_per_second': '46.656', 'grad_norm': '23.256', 'counters/examples': 90624, 'counters/updates': 2832}
skipping logging after 90656 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '-0.83298', 'rewards_train/rejected': '-1.807', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.97403', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-127.05', 'loss/train': '0.46026', 'examples_per_second': '48.126', 'grad_norm': '22.43', 'counters/examples': 90688, 'counters/updates': 2834}
skipping logging after 90720 examples to avoid logging too frequently
train stats after 90752 examples: {'rewards_train/chosen': '-1.1929', 'rewards_train/rejected': '-1.5556', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36269', 'logps_train/rejected': '-91.774', 'logps_train/chosen': '-118.15', 'loss/train': '0.6658', 'examples_per_second': '45.66', 'grad_norm': '26.064', 'counters/examples': 90752, 'counters/updates': 2836}
skipping logging after 90784 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '-0.71871', 'rewards_train/rejected': '-1.5294', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.81065', 'logps_train/rejected': '-179.23', 'logps_train/chosen': '-143.54', 'loss/train': '0.47705', 'examples_per_second': '45.215', 'grad_norm': '20.592', 'counters/examples': 90816, 'counters/updates': 2838}
skipping logging after 90848 examples to avoid logging too frequently
train stats after 90880 examples: {'rewards_train/chosen': '-0.90386', 'rewards_train/rejected': '-1.1486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24475', 'logps_train/rejected': '-194.85', 'logps_train/chosen': '-176.96', 'loss/train': '0.70278', 'examples_per_second': '45.295', 'grad_norm': '30.374', 'counters/examples': 90880, 'counters/updates': 2840}
skipping logging after 90912 examples to avoid logging too frequently
train stats after 90944 examples: {'rewards_train/chosen': '-1.0332', 'rewards_train/rejected': '-1.9715', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.93823', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-154.23', 'loss/train': '0.45231', 'examples_per_second': '46.622', 'grad_norm': '20.165', 'counters/examples': 90944, 'counters/updates': 2842}
skipping logging after 90976 examples to avoid logging too frequently
train stats after 91008 examples: {'rewards_train/chosen': '-1.1003', 'rewards_train/rejected': '-1.4787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37842', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-135.3', 'loss/train': '0.63586', 'examples_per_second': '44.966', 'grad_norm': '25.351', 'counters/examples': 91008, 'counters/updates': 2844}
skipping logging after 91040 examples to avoid logging too frequently
train stats after 91072 examples: {'rewards_train/chosen': '-1.0772', 'rewards_train/rejected': '-1.51', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43279', 'logps_train/rejected': '-144.96', 'logps_train/chosen': '-150.28', 'loss/train': '0.59194', 'examples_per_second': '46.185', 'grad_norm': '24.804', 'counters/examples': 91072, 'counters/updates': 2846}
skipping logging after 91104 examples to avoid logging too frequently
train stats after 91136 examples: {'rewards_train/chosen': '-0.78106', 'rewards_train/rejected': '-1.4122', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.63114', 'logps_train/rejected': '-109.5', 'logps_train/chosen': '-139.98', 'loss/train': '0.52789', 'examples_per_second': '45.216', 'grad_norm': '24.569', 'counters/examples': 91136, 'counters/updates': 2848}
skipping logging after 91168 examples to avoid logging too frequently
train stats after 91200 examples: {'rewards_train/chosen': '-1.0804', 'rewards_train/rejected': '-1.5453', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.46494', 'logps_train/rejected': '-96.348', 'logps_train/chosen': '-112.45', 'loss/train': '0.62811', 'examples_per_second': '47.818', 'grad_norm': '22.632', 'counters/examples': 91200, 'counters/updates': 2850}
skipping logging after 91232 examples to avoid logging too frequently
train stats after 91264 examples: {'rewards_train/chosen': '-1.2473', 'rewards_train/rejected': '-1.6496', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.40239', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-139.82', 'loss/train': '0.67903', 'examples_per_second': '44.954', 'grad_norm': '29.467', 'counters/examples': 91264, 'counters/updates': 2852}
skipping logging after 91296 examples to avoid logging too frequently
train stats after 91328 examples: {'rewards_train/chosen': '-1.0275', 'rewards_train/rejected': '-1.3999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37237', 'logps_train/rejected': '-118.19', 'logps_train/chosen': '-143.78', 'loss/train': '0.68952', 'examples_per_second': '45.532', 'grad_norm': '24.987', 'counters/examples': 91328, 'counters/updates': 2854}
skipping logging after 91360 examples to avoid logging too frequently
train stats after 91392 examples: {'rewards_train/chosen': '-1.2946', 'rewards_train/rejected': '-1.6491', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35444', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-154.11', 'loss/train': '0.58612', 'examples_per_second': '44.135', 'grad_norm': '26.748', 'counters/examples': 91392, 'counters/updates': 2856}
skipping logging after 91424 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '-1.1336', 'rewards_train/rejected': '-1.6444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.51078', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-164.96', 'loss/train': '0.5778', 'examples_per_second': '45.455', 'grad_norm': '24.218', 'counters/examples': 91456, 'counters/updates': 2858}
skipping logging after 91488 examples to avoid logging too frequently
train stats after 91520 examples: {'rewards_train/chosen': '-0.83524', 'rewards_train/rejected': '-1.6402', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.80492', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-128.63', 'loss/train': '0.45846', 'examples_per_second': '49.051', 'grad_norm': '21.22', 'counters/examples': 91520, 'counters/updates': 2860}
skipping logging after 91552 examples to avoid logging too frequently
train stats after 91584 examples: {'rewards_train/chosen': '-0.95335', 'rewards_train/rejected': '-1.2332', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27983', 'logps_train/rejected': '-170.32', 'logps_train/chosen': '-152.48', 'loss/train': '0.6822', 'examples_per_second': '45.33', 'grad_norm': '27.264', 'counters/examples': 91584, 'counters/updates': 2862}
skipping logging after 91616 examples to avoid logging too frequently
train stats after 91648 examples: {'rewards_train/chosen': '-0.65806', 'rewards_train/rejected': '-1.298', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.63997', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-131.82', 'loss/train': '0.62214', 'examples_per_second': '44.801', 'grad_norm': '26.534', 'counters/examples': 91648, 'counters/updates': 2864}
skipping logging after 91680 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '-0.55671', 'rewards_train/rejected': '-1.3314', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.77467', 'logps_train/rejected': '-116.58', 'logps_train/chosen': '-140.85', 'loss/train': '0.47803', 'examples_per_second': '45.233', 'grad_norm': '20.587', 'counters/examples': 91712, 'counters/updates': 2866}
skipping logging after 91744 examples to avoid logging too frequently
train stats after 91776 examples: {'rewards_train/chosen': '-0.79885', 'rewards_train/rejected': '-1.2559', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45701', 'logps_train/rejected': '-104.27', 'logps_train/chosen': '-140.75', 'loss/train': '0.65557', 'examples_per_second': '44.828', 'grad_norm': '24.22', 'counters/examples': 91776, 'counters/updates': 2868}
skipping logging after 91808 examples to avoid logging too frequently
train stats after 91840 examples: {'rewards_train/chosen': '-1.0279', 'rewards_train/rejected': '-1.2969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26903', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-163.12', 'loss/train': '0.75439', 'examples_per_second': '46.6', 'grad_norm': '30.366', 'counters/examples': 91840, 'counters/updates': 2870}
skipping logging after 91872 examples to avoid logging too frequently
train stats after 91904 examples: {'rewards_train/chosen': '-0.7337', 'rewards_train/rejected': '-1.1995', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46577', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-157.72', 'loss/train': '0.59663', 'examples_per_second': '46.424', 'grad_norm': '25.985', 'counters/examples': 91904, 'counters/updates': 2872}
skipping logging after 91936 examples to avoid logging too frequently
train stats after 91968 examples: {'rewards_train/chosen': '-0.88669', 'rewards_train/rejected': '-1.2978', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41109', 'logps_train/rejected': '-153.82', 'logps_train/chosen': '-166.41', 'loss/train': '0.60042', 'examples_per_second': '45.357', 'grad_norm': '26.1', 'counters/examples': 91968, 'counters/updates': 2874}
skipping logging after 92000 examples to avoid logging too frequently
train stats after 92032 examples: {'rewards_train/chosen': '-1.1124', 'rewards_train/rejected': '-1.9338', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.82144', 'logps_train/rejected': '-128.13', 'logps_train/chosen': '-145.55', 'loss/train': '0.50681', 'examples_per_second': '48.848', 'grad_norm': '23.107', 'counters/examples': 92032, 'counters/updates': 2876}
skipping logging after 92064 examples to avoid logging too frequently
train stats after 92096 examples: {'rewards_train/chosen': '-0.62646', 'rewards_train/rejected': '-1.3659', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.73947', 'logps_train/rejected': '-154.04', 'logps_train/chosen': '-137.05', 'loss/train': '0.48825', 'examples_per_second': '46.03', 'grad_norm': '21.902', 'counters/examples': 92096, 'counters/updates': 2878}
skipping logging after 92128 examples to avoid logging too frequently
train stats after 92160 examples: {'rewards_train/chosen': '-0.89906', 'rewards_train/rejected': '-1.638', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.73897', 'logps_train/rejected': '-150.25', 'logps_train/chosen': '-159.94', 'loss/train': '0.53248', 'examples_per_second': '33.885', 'grad_norm': '23.829', 'counters/examples': 92160, 'counters/updates': 2880}
skipping logging after 92192 examples to avoid logging too frequently
train stats after 92224 examples: {'rewards_train/chosen': '-0.78041', 'rewards_train/rejected': '-1.3602', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57983', 'logps_train/rejected': '-157.06', 'logps_train/chosen': '-147.28', 'loss/train': '0.53564', 'examples_per_second': '45.316', 'grad_norm': '23.789', 'counters/examples': 92224, 'counters/updates': 2882}
skipping logging after 92256 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '-0.9315', 'rewards_train/rejected': '-1.571', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63954', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-147.4', 'loss/train': '0.51923', 'examples_per_second': '45.184', 'grad_norm': '23.245', 'counters/examples': 92288, 'counters/updates': 2884}
skipping logging after 92320 examples to avoid logging too frequently
train stats after 92352 examples: {'rewards_train/chosen': '-1.4325', 'rewards_train/rejected': '-1.7524', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31986', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-187.69', 'loss/train': '0.71293', 'examples_per_second': '44.431', 'grad_norm': '33.082', 'counters/examples': 92352, 'counters/updates': 2886}
skipping logging after 92384 examples to avoid logging too frequently
train stats after 92416 examples: {'rewards_train/chosen': '-1.3037', 'rewards_train/rejected': '-2.1852', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.88155', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-124.31', 'loss/train': '0.46466', 'examples_per_second': '46.511', 'grad_norm': '18.658', 'counters/examples': 92416, 'counters/updates': 2888}
skipping logging after 92448 examples to avoid logging too frequently
train stats after 92480 examples: {'rewards_train/chosen': '-1.2177', 'rewards_train/rejected': '-1.7628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54504', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-162.98', 'loss/train': '0.58443', 'examples_per_second': '45.477', 'grad_norm': '29.38', 'counters/examples': 92480, 'counters/updates': 2890}
skipping logging after 92512 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '-1.2105', 'rewards_train/rejected': '-1.6797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.46916', 'logps_train/rejected': '-148.68', 'logps_train/chosen': '-149.78', 'loss/train': '0.62354', 'examples_per_second': '46.297', 'grad_norm': '29.465', 'counters/examples': 92544, 'counters/updates': 2892}
skipping logging after 92576 examples to avoid logging too frequently
train stats after 92608 examples: {'rewards_train/chosen': '-1.4455', 'rewards_train/rejected': '-2.1295', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.68406', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-174.42', 'loss/train': '0.54315', 'examples_per_second': '44.213', 'grad_norm': '25.872', 'counters/examples': 92608, 'counters/updates': 2894}
skipping logging after 92640 examples to avoid logging too frequently
train stats after 92672 examples: {'rewards_train/chosen': '-1.5363', 'rewards_train/rejected': '-2.0145', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.47822', 'logps_train/rejected': '-145.34', 'logps_train/chosen': '-175.9', 'loss/train': '0.57134', 'examples_per_second': '47.979', 'grad_norm': '27.492', 'counters/examples': 92672, 'counters/updates': 2896}
skipping logging after 92704 examples to avoid logging too frequently
train stats after 92736 examples: {'rewards_train/chosen': '-1.4925', 'rewards_train/rejected': '-1.8249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33243', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-134.24', 'loss/train': '0.70052', 'examples_per_second': '46.601', 'grad_norm': '27.833', 'counters/examples': 92736, 'counters/updates': 2898}
skipping logging after 92768 examples to avoid logging too frequently
train stats after 92800 examples: {'rewards_train/chosen': '-1.1244', 'rewards_train/rejected': '-1.8922', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.7678', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-120.92', 'loss/train': '0.51514', 'examples_per_second': '48.561', 'grad_norm': '21.921', 'counters/examples': 92800, 'counters/updates': 2900}
skipping logging after 92832 examples to avoid logging too frequently
train stats after 92864 examples: {'rewards_train/chosen': '-1.3142', 'rewards_train/rejected': '-1.9844', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.67025', 'logps_train/rejected': '-117.18', 'logps_train/chosen': '-159.53', 'loss/train': '0.51434', 'examples_per_second': '44.067', 'grad_norm': '23.751', 'counters/examples': 92864, 'counters/updates': 2902}
skipping logging after 92896 examples to avoid logging too frequently
train stats after 92928 examples: {'rewards_train/chosen': '-1.2632', 'rewards_train/rejected': '-1.6777', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41452', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-145.6', 'loss/train': '0.69299', 'examples_per_second': '44.775', 'grad_norm': '28.637', 'counters/examples': 92928, 'counters/updates': 2904}
skipping logging after 92960 examples to avoid logging too frequently
train stats after 92992 examples: {'rewards_train/chosen': '-1.2828', 'rewards_train/rejected': '-1.8961', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61327', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-167.61', 'loss/train': '0.55512', 'examples_per_second': '47.632', 'grad_norm': '24.851', 'counters/examples': 92992, 'counters/updates': 2906}
skipping logging after 93024 examples to avoid logging too frequently
train stats after 93056 examples: {'rewards_train/chosen': '-1.2698', 'rewards_train/rejected': '-2.0319', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.76208', 'logps_train/rejected': '-113.3', 'logps_train/chosen': '-145.33', 'loss/train': '0.52118', 'examples_per_second': '47.798', 'grad_norm': '20.941', 'counters/examples': 93056, 'counters/updates': 2908}
skipping logging after 93088 examples to avoid logging too frequently
train stats after 93120 examples: {'rewards_train/chosen': '-0.91515', 'rewards_train/rejected': '-1.0757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16058', 'logps_train/rejected': '-152.08', 'logps_train/chosen': '-152.39', 'loss/train': '0.6854', 'examples_per_second': '44.582', 'grad_norm': '29.183', 'counters/examples': 93120, 'counters/updates': 2910}
skipping logging after 93152 examples to avoid logging too frequently
train stats after 93184 examples: {'rewards_train/chosen': '-1.0219', 'rewards_train/rejected': '-1.7412', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.71934', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-134.13', 'loss/train': '0.57056', 'examples_per_second': '51.263', 'grad_norm': '23.729', 'counters/examples': 93184, 'counters/updates': 2912}
skipping logging after 93216 examples to avoid logging too frequently
train stats after 93248 examples: {'rewards_train/chosen': '-1.1478', 'rewards_train/rejected': '-1.5623', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.41443', 'logps_train/rejected': '-149.42', 'logps_train/chosen': '-129.09', 'loss/train': '0.68067', 'examples_per_second': '45.247', 'grad_norm': '26.758', 'counters/examples': 93248, 'counters/updates': 2914}
skipping logging after 93280 examples to avoid logging too frequently
train stats after 93312 examples: {'rewards_train/chosen': '-1.1228', 'rewards_train/rejected': '-1.5919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4691', 'logps_train/rejected': '-129.22', 'logps_train/chosen': '-108.08', 'loss/train': '0.616', 'examples_per_second': '45.346', 'grad_norm': '22.742', 'counters/examples': 93312, 'counters/updates': 2916}
skipping logging after 93344 examples to avoid logging too frequently
train stats after 93376 examples: {'rewards_train/chosen': '-1.413', 'rewards_train/rejected': '-1.7016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28857', 'logps_train/rejected': '-176.85', 'logps_train/chosen': '-155.34', 'loss/train': '0.67778', 'examples_per_second': '45.466', 'grad_norm': '29.149', 'counters/examples': 93376, 'counters/updates': 2918}
skipping logging after 93408 examples to avoid logging too frequently
train stats after 93440 examples: {'rewards_train/chosen': '-1.1738', 'rewards_train/rejected': '-1.6398', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46607', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-161.85', 'loss/train': '0.59999', 'examples_per_second': '45.361', 'grad_norm': '26.983', 'counters/examples': 93440, 'counters/updates': 2920}
skipping logging after 93472 examples to avoid logging too frequently
train stats after 93504 examples: {'rewards_train/chosen': '-1.0207', 'rewards_train/rejected': '-1.7843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.76359', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-157.73', 'loss/train': '0.52677', 'examples_per_second': '45.294', 'grad_norm': '24.177', 'counters/examples': 93504, 'counters/updates': 2922}
skipping logging after 93536 examples to avoid logging too frequently
train stats after 93568 examples: {'rewards_train/chosen': '-1.3359', 'rewards_train/rejected': '-2.0834', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.74746', 'logps_train/rejected': '-192.99', 'logps_train/chosen': '-179.1', 'loss/train': '0.48325', 'examples_per_second': '45.235', 'grad_norm': '25.973', 'counters/examples': 93568, 'counters/updates': 2924}
skipping logging after 93600 examples to avoid logging too frequently
train stats after 93632 examples: {'rewards_train/chosen': '-1.251', 'rewards_train/rejected': '-1.834', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.58304', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-157.1', 'loss/train': '0.57971', 'examples_per_second': '45.02', 'grad_norm': '28.195', 'counters/examples': 93632, 'counters/updates': 2926}
skipping logging after 93664 examples to avoid logging too frequently
train stats after 93696 examples: {'rewards_train/chosen': '-1.2122', 'rewards_train/rejected': '-1.4224', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21018', 'logps_train/rejected': '-145.82', 'logps_train/chosen': '-175.19', 'loss/train': '0.74048', 'examples_per_second': '45.271', 'grad_norm': '27.372', 'counters/examples': 93696, 'counters/updates': 2928}
skipping logging after 93728 examples to avoid logging too frequently
train stats after 93760 examples: {'rewards_train/chosen': '-1.2849', 'rewards_train/rejected': '-1.9067', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.6218', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-154.63', 'loss/train': '0.53831', 'examples_per_second': '51.456', 'grad_norm': '20.872', 'counters/examples': 93760, 'counters/updates': 2930}
skipping logging after 93792 examples to avoid logging too frequently
train stats after 93824 examples: {'rewards_train/chosen': '-1.2829', 'rewards_train/rejected': '-1.7703', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48741', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-135.3', 'loss/train': '0.60636', 'examples_per_second': '45.189', 'grad_norm': '24.448', 'counters/examples': 93824, 'counters/updates': 2932}
skipping logging after 93856 examples to avoid logging too frequently
train stats after 93888 examples: {'rewards_train/chosen': '-1.0858', 'rewards_train/rejected': '-1.6627', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57694', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-156.08', 'loss/train': '0.57591', 'examples_per_second': '47.128', 'grad_norm': '23.756', 'counters/examples': 93888, 'counters/updates': 2934}
skipping logging after 93920 examples to avoid logging too frequently
train stats after 93952 examples: {'rewards_train/chosen': '-1.3492', 'rewards_train/rejected': '-1.3729', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023749', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-143.4', 'loss/train': '0.87082', 'examples_per_second': '46.627', 'grad_norm': '31.359', 'counters/examples': 93952, 'counters/updates': 2936}
skipping logging after 93984 examples to avoid logging too frequently
train stats after 94016 examples: {'rewards_train/chosen': '-1.0432', 'rewards_train/rejected': '-1.5196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47633', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-145.91', 'loss/train': '0.58775', 'examples_per_second': '44.262', 'grad_norm': '25.32', 'counters/examples': 94016, 'counters/updates': 2938}
skipping logging after 94048 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '-0.82968', 'rewards_train/rejected': '-1.3289', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49919', 'logps_train/rejected': '-161.43', 'logps_train/chosen': '-161.63', 'loss/train': '0.57861', 'examples_per_second': '45.338', 'grad_norm': '25.91', 'counters/examples': 94080, 'counters/updates': 2940}
skipping logging after 94112 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '-1.2692', 'rewards_train/rejected': '-1.7215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.45222', 'logps_train/rejected': '-144.05', 'logps_train/chosen': '-184.43', 'loss/train': '0.62228', 'examples_per_second': '45.427', 'grad_norm': '30.018', 'counters/examples': 94144, 'counters/updates': 2942}
skipping logging after 94176 examples to avoid logging too frequently
train stats after 94208 examples: {'rewards_train/chosen': '-1.0735', 'rewards_train/rejected': '-1.6943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.62082', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-179.24', 'loss/train': '0.59557', 'examples_per_second': '44.529', 'grad_norm': '27.993', 'counters/examples': 94208, 'counters/updates': 2944}
skipping logging after 94240 examples to avoid logging too frequently
train stats after 94272 examples: {'rewards_train/chosen': '-0.89615', 'rewards_train/rejected': '-1.4031', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50699', 'logps_train/rejected': '-148.91', 'logps_train/chosen': '-160.25', 'loss/train': '0.63579', 'examples_per_second': '46.88', 'grad_norm': '31.145', 'counters/examples': 94272, 'counters/updates': 2946}
skipping logging after 94304 examples to avoid logging too frequently
train stats after 94336 examples: {'rewards_train/chosen': '-1.3512', 'rewards_train/rejected': '-1.7696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4184', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-129.38', 'loss/train': '0.59164', 'examples_per_second': '44.406', 'grad_norm': '23.429', 'counters/examples': 94336, 'counters/updates': 2948}
skipping logging after 94368 examples to avoid logging too frequently
train stats after 94400 examples: {'rewards_train/chosen': '-1.0561', 'rewards_train/rejected': '-1.6025', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54637', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-135.81', 'loss/train': '0.53292', 'examples_per_second': '47.909', 'grad_norm': '22.462', 'counters/examples': 94400, 'counters/updates': 2950}
skipping logging after 94432 examples to avoid logging too frequently
train stats after 94464 examples: {'rewards_train/chosen': '-1.3186', 'rewards_train/rejected': '-1.9321', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61343', 'logps_train/rejected': '-182.9', 'logps_train/chosen': '-158.44', 'loss/train': '0.54391', 'examples_per_second': '45.184', 'grad_norm': '28.091', 'counters/examples': 94464, 'counters/updates': 2952}
skipping logging after 94496 examples to avoid logging too frequently
train stats after 94528 examples: {'rewards_train/chosen': '-0.93153', 'rewards_train/rejected': '-1.61', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.67846', 'logps_train/rejected': '-119.93', 'logps_train/chosen': '-122.62', 'loss/train': '0.51235', 'examples_per_second': '44.35', 'grad_norm': '18.31', 'counters/examples': 94528, 'counters/updates': 2954}
skipping logging after 94560 examples to avoid logging too frequently
train stats after 94592 examples: {'rewards_train/chosen': '-0.92677', 'rewards_train/rejected': '-1.6262', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.69939', 'logps_train/rejected': '-157.46', 'logps_train/chosen': '-143.67', 'loss/train': '0.49343', 'examples_per_second': '45.345', 'grad_norm': '21.642', 'counters/examples': 94592, 'counters/updates': 2956}
skipping logging after 94624 examples to avoid logging too frequently
train stats after 94656 examples: {'rewards_train/chosen': '-1.2086', 'rewards_train/rejected': '-1.8759', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.66727', 'logps_train/rejected': '-162.6', 'logps_train/chosen': '-168.63', 'loss/train': '0.5651', 'examples_per_second': '47.491', 'grad_norm': '28.181', 'counters/examples': 94656, 'counters/updates': 2958}
skipping logging after 94688 examples to avoid logging too frequently
train stats after 94720 examples: {'rewards_train/chosen': '-1.5642', 'rewards_train/rejected': '-1.8617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.29751', 'logps_train/rejected': '-100.86', 'logps_train/chosen': '-144.25', 'loss/train': '0.68802', 'examples_per_second': '48.812', 'grad_norm': '25.67', 'counters/examples': 94720, 'counters/updates': 2960}
skipping logging after 94752 examples to avoid logging too frequently
train stats after 94784 examples: {'rewards_train/chosen': '-1.5737', 'rewards_train/rejected': '-1.9273', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35368', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-136.34', 'loss/train': '0.60913', 'examples_per_second': '45.002', 'grad_norm': '30.876', 'counters/examples': 94784, 'counters/updates': 2962}
skipping logging after 94816 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '-1.1535', 'rewards_train/rejected': '-1.6628', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5093', 'logps_train/rejected': '-142.02', 'logps_train/chosen': '-144.39', 'loss/train': '0.56074', 'examples_per_second': '45.655', 'grad_norm': '23.102', 'counters/examples': 94848, 'counters/updates': 2964}
skipping logging after 94880 examples to avoid logging too frequently
train stats after 94912 examples: {'rewards_train/chosen': '-1.1412', 'rewards_train/rejected': '-1.4544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31315', 'logps_train/rejected': '-149.25', 'logps_train/chosen': '-135.93', 'loss/train': '0.65077', 'examples_per_second': '46.294', 'grad_norm': '27.506', 'counters/examples': 94912, 'counters/updates': 2966}
skipping logging after 94944 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '-1.2861', 'rewards_train/rejected': '-1.6226', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33651', 'logps_train/rejected': '-171.74', 'logps_train/chosen': '-142', 'loss/train': '0.64061', 'examples_per_second': '46.38', 'grad_norm': '27.519', 'counters/examples': 94976, 'counters/updates': 2968}
skipping logging after 95008 examples to avoid logging too frequently
train stats after 95040 examples: {'rewards_train/chosen': '-0.99735', 'rewards_train/rejected': '-1.2636', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26628', 'logps_train/rejected': '-151.74', 'logps_train/chosen': '-130.67', 'loss/train': '0.66302', 'examples_per_second': '44.806', 'grad_norm': '28.073', 'counters/examples': 95040, 'counters/updates': 2970}
skipping logging after 95072 examples to avoid logging too frequently
train stats after 95104 examples: {'rewards_train/chosen': '-0.70359', 'rewards_train/rejected': '-1.1995', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.49592', 'logps_train/rejected': '-114.5', 'logps_train/chosen': '-154.53', 'loss/train': '0.5783', 'examples_per_second': '45.016', 'grad_norm': '24.696', 'counters/examples': 95104, 'counters/updates': 2972}
skipping logging after 95136 examples to avoid logging too frequently
train stats after 95168 examples: {'rewards_train/chosen': '-1.1357', 'rewards_train/rejected': '-1.7045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56876', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-154.09', 'loss/train': '0.55292', 'examples_per_second': '49.269', 'grad_norm': '25.914', 'counters/examples': 95168, 'counters/updates': 2974}
skipping logging after 95200 examples to avoid logging too frequently
train stats after 95232 examples: {'rewards_train/chosen': '-1.0094', 'rewards_train/rejected': '-1.3218', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31239', 'logps_train/rejected': '-146.35', 'logps_train/chosen': '-127.56', 'loss/train': '0.64613', 'examples_per_second': '46.419', 'grad_norm': '27.334', 'counters/examples': 95232, 'counters/updates': 2976}
skipping logging after 95264 examples to avoid logging too frequently
train stats after 95296 examples: {'rewards_train/chosen': '-0.88348', 'rewards_train/rejected': '-1.5666', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.68307', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-132.59', 'loss/train': '0.55308', 'examples_per_second': '45.37', 'grad_norm': '23.241', 'counters/examples': 95296, 'counters/updates': 2978}
skipping logging after 95328 examples to avoid logging too frequently
train stats after 95360 examples: {'rewards_train/chosen': '-0.92627', 'rewards_train/rejected': '-1.4705', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54424', 'logps_train/rejected': '-156.42', 'logps_train/chosen': '-123.1', 'loss/train': '0.57947', 'examples_per_second': '44.958', 'grad_norm': '24.46', 'counters/examples': 95360, 'counters/updates': 2980}
skipping logging after 95392 examples to avoid logging too frequently
train stats after 95424 examples: {'rewards_train/chosen': '-0.96836', 'rewards_train/rejected': '-1.6591', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.69076', 'logps_train/rejected': '-165.55', 'logps_train/chosen': '-161.48', 'loss/train': '0.52265', 'examples_per_second': '44.284', 'grad_norm': '24.435', 'counters/examples': 95424, 'counters/updates': 2982}
skipping logging after 95456 examples to avoid logging too frequently
train stats after 95488 examples: {'rewards_train/chosen': '-0.98805', 'rewards_train/rejected': '-1.4082', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42016', 'logps_train/rejected': '-173.69', 'logps_train/chosen': '-141.93', 'loss/train': '0.65215', 'examples_per_second': '45.4', 'grad_norm': '25.276', 'counters/examples': 95488, 'counters/updates': 2984}
skipping logging after 95520 examples to avoid logging too frequently
train stats after 95552 examples: {'rewards_train/chosen': '-1.0839', 'rewards_train/rejected': '-1.4708', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38685', 'logps_train/rejected': '-128.83', 'logps_train/chosen': '-159.68', 'loss/train': '0.64094', 'examples_per_second': '45.367', 'grad_norm': '27.054', 'counters/examples': 95552, 'counters/updates': 2986}
skipping logging after 95584 examples to avoid logging too frequently
train stats after 95616 examples: {'rewards_train/chosen': '-0.73188', 'rewards_train/rejected': '-1.4397', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.70779', 'logps_train/rejected': '-139.57', 'logps_train/chosen': '-183.35', 'loss/train': '0.54045', 'examples_per_second': '45.425', 'grad_norm': '26.852', 'counters/examples': 95616, 'counters/updates': 2988}
skipping logging after 95648 examples to avoid logging too frequently
train stats after 95680 examples: {'rewards_train/chosen': '-1.1555', 'rewards_train/rejected': '-1.8562', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.70074', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-176.56', 'loss/train': '0.5424', 'examples_per_second': '45.29', 'grad_norm': '25.78', 'counters/examples': 95680, 'counters/updates': 2990}
skipping logging after 95712 examples to avoid logging too frequently
train stats after 95744 examples: {'rewards_train/chosen': '-1.0453', 'rewards_train/rejected': '-1.6851', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63986', 'logps_train/rejected': '-168.36', 'logps_train/chosen': '-146.31', 'loss/train': '0.54915', 'examples_per_second': '44.234', 'grad_norm': '23.375', 'counters/examples': 95744, 'counters/updates': 2992}
skipping logging after 95776 examples to avoid logging too frequently
train stats after 95808 examples: {'rewards_train/chosen': '-1.458', 'rewards_train/rejected': '-1.7864', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3284', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-153.27', 'loss/train': '0.6155', 'examples_per_second': '44.306', 'grad_norm': '26.903', 'counters/examples': 95808, 'counters/updates': 2994}
skipping logging after 95840 examples to avoid logging too frequently
train stats after 95872 examples: {'rewards_train/chosen': '-0.51043', 'rewards_train/rejected': '-1.271', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.76057', 'logps_train/rejected': '-120.31', 'logps_train/chosen': '-138.38', 'loss/train': '0.47208', 'examples_per_second': '44.864', 'grad_norm': '19.518', 'counters/examples': 95872, 'counters/updates': 2996}
skipping logging after 95904 examples to avoid logging too frequently
train stats after 95936 examples: {'rewards_train/chosen': '-1.2558', 'rewards_train/rejected': '-1.3978', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14203', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-113.98', 'loss/train': '0.7294', 'examples_per_second': '44.153', 'grad_norm': '25.04', 'counters/examples': 95936, 'counters/updates': 2998}
skipping logging after 95968 examples to avoid logging too frequently
train stats after 96000 examples: {'rewards_train/chosen': '-0.98782', 'rewards_train/rejected': '-1.1287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1409', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-161.16', 'loss/train': '0.77064', 'examples_per_second': '45.376', 'grad_norm': '29.91', 'counters/examples': 96000, 'counters/updates': 3000}
Running evaluation after 96000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:02,  7.20it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:02,  6.88it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01,  6.98it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  6.99it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  6.90it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  7.34it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:01,  7.17it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:01<00:01,  7.09it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:01<00:00,  7.13it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  7.01it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00,  7.01it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00,  6.99it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00,  7.00it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00,  6.89it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:02<00:00,  6.96it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.87it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.00it/s]
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
eval after 96000: {'rewards_eval/chosen': '-0.87862', 'rewards_eval/rejected': '-1.3785', 'rewards_eval/accuracies': '0.64062', 'rewards_eval/margins': '0.49987', 'logps_eval/rejected': '-132.07', 'logps_eval/chosen': '-145.24', 'loss/eval': '0.62144'}
creating checkpoint to write to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-96000...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-96000/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-96000/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/step-96000/scheduler.pt...
train stats after 96032 examples: {'rewards_train/chosen': '-1.337', 'rewards_train/rejected': '-1.8429', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50582', 'logps_train/rejected': '-134.43', 'logps_train/chosen': '-187.04', 'loss/train': '0.62556', 'examples_per_second': '33.461', 'grad_norm': '26.458', 'counters/examples': 96032, 'counters/updates': 3001}
skipping logging after 96064 examples to avoid logging too frequently
train stats after 96096 examples: {'rewards_train/chosen': '-0.97228', 'rewards_train/rejected': '-1.911', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.93872', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-163', 'loss/train': '0.45385', 'examples_per_second': '45.05', 'grad_norm': '22.397', 'counters/examples': 96096, 'counters/updates': 3003}
skipping logging after 96128 examples to avoid logging too frequently
train stats after 96160 examples: {'rewards_train/chosen': '-1.2042', 'rewards_train/rejected': '-1.6583', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.45406', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-129.12', 'loss/train': '0.61153', 'examples_per_second': '44.45', 'grad_norm': '24.718', 'counters/examples': 96160, 'counters/updates': 3005}
skipping logging after 96192 examples to avoid logging too frequently
train stats after 96224 examples: {'rewards_train/chosen': '-0.8378', 'rewards_train/rejected': '-1.5822', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.74439', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-172.97', 'loss/train': '0.5463', 'examples_per_second': '44.78', 'grad_norm': '26.873', 'counters/examples': 96224, 'counters/updates': 3007}
skipping logging after 96256 examples to avoid logging too frequently
Finished generating 1 epochs on train split
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/LATEST/policy.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/LATEST/optimizer.pt...
writing checkpoint to .cache/laura/pythia1.4b_dpo_seed0_2024-01-12_00-39-54_703699/LATEST/scheduler.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.675 MB uploaded (0.000 MB deduped)wandb: / 0.673 MB of 0.675 MB uploaded (0.000 MB deduped)wandb: - 0.673 MB of 0.675 MB uploaded (0.000 MB deduped)wandb: \ 0.673 MB of 0.675 MB uploaded (0.000 MB deduped)wandb: | 0.673 MB of 0.675 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         counters/updates ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      examples_per_second ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ
wandb:                grad_norm ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñà‚ñá‚ñà
wandb:        logps_eval/chosen ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:      logps_eval/rejected ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb:       logps_train/chosen ‚ñÑ‚ñà‚ñà‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÖ‚ñá‚ñÑ‚ñá‚ñÑ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÉ
wandb:     logps_train/rejected ‚ñÖ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñá‚ñÖ
wandb:                loss/eval ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ
wandb:               loss/train ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÜ‚ñÜ‚ñá
wandb:  rewards_eval/accuracies ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñÖ‚ñÜ
wandb:      rewards_eval/chosen ‚ñà‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:     rewards_eval/margins ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá
wandb:    rewards_eval/rejected ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: rewards_train/accuracies ‚ñá‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÉ‚ñá‚ñà‚ñÑ‚ñÜ‚ñÉ
wandb:     rewards_train/chosen ‚ñá‚ñÜ‚ñà‚ñá‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ
wandb:    rewards_train/margins ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÇ‚ñÑ‚ñÇ
wandb:   rewards_train/rejected ‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ
wandb: 
wandb: Run summary:
wandb:        counters/examples 96224
wandb:         counters/updates 3007
wandb:      examples_per_second 44.77989
wandb:                grad_norm 26.87263
wandb:        logps_eval/chosen -145.23758
wandb:      logps_eval/rejected -132.06638
wandb:       logps_train/chosen -172.96861
wandb:     logps_train/rejected -133.04071
wandb:                loss/eval 0.62144
wandb:               loss/train 0.5463
wandb:  rewards_eval/accuracies 0.64062
wandb:      rewards_eval/chosen -0.87862
wandb:     rewards_eval/margins 0.49987
wandb:    rewards_eval/rejected -1.37848
wandb: rewards_train/accuracies 0.71875
wandb:     rewards_train/chosen -0.8378
wandb:    rewards_train/margins 0.74439
wandb:   rewards_train/rejected -1.58219
wandb: 
wandb: üöÄ View run pythia1.4b_dpo_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/cn14yuod
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240112_004137-cn14yuod/logs
