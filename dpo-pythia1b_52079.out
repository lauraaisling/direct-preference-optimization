WARNING: eval_every must be divisible by batch_size
Setting eval_every to 11968
no FSDP port specified; using open port for FSDP: 51751
seed: 0
exp_name: pythia1b_dpo_seed0
batch_size: 64
eval_batch_size: 16
debug: false
fsdp_port: 51751
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895
lr: 1.0e-06
gradient_accumulation_steps: 1
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 11968
minimum_log_interval_secs: 1.0
model:
  name_or_path: lomahony/pythia-1b-helpful-sft
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-252-71:.cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240112_001703-0mhjakjz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia1b_dpo_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/0mhjakjz
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-1b-helpful-sft
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 2837/5103 [00:00<00:00, 28357.52it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 28232.98it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   0%|          | 298/96256 [00:00<00:32, 2971.82it/s]Processing HH static:   1%|          | 627/96256 [00:00<00:30, 3158.55it/s]Processing HH static:   1%|          | 943/96256 [00:00<00:36, 2636.77it/s]Processing HH static:   4%|‚ñç         | 3818/96256 [00:00<00:07, 11837.76it/s]Processing HH static:   7%|‚ñã         | 6330/96256 [00:00<00:08, 10276.74it/s]Processing HH static:  10%|‚ñâ         | 9207/96256 [00:00<00:05, 14563.73it/s]Processing HH static:  13%|‚ñà‚ñé        | 12093/96256 [00:00<00:04, 18120.05it/s]Processing HH static:  15%|‚ñà‚ñå        | 14854/96256 [00:01<00:03, 20607.43it/s]Processing HH static:  18%|‚ñà‚ñä        | 17762/96256 [00:01<00:03, 22914.42it/s]Processing HH static:  21%|‚ñà‚ñà‚ñè       | 20645/96256 [00:01<00:03, 24571.51it/s]Processing HH static:  24%|‚ñà‚ñà‚ñç       | 23507/96256 [00:01<00:02, 25726.56it/s]Processing HH static:  27%|‚ñà‚ñà‚ñã       | 26364/96256 [00:01<00:02, 26550.07it/s]Processing HH static:  30%|‚ñà‚ñà‚ñà       | 29241/96256 [00:01<00:02, 27197.30it/s]Processing HH static:  33%|‚ñà‚ñà‚ñà‚ñé      | 32120/96256 [00:01<00:02, 27664.46it/s]Processing HH static:  36%|‚ñà‚ñà‚ñà‚ñã      | 34963/96256 [00:01<00:02, 27890.06it/s]Processing HH static:  39%|‚ñà‚ñà‚ñà‚ñâ      | 37813/96256 [00:01<00:02, 28069.06it/s]Processing HH static:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 40643/96256 [00:02<00:03, 18189.65it/s]Processing HH static:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 43489/96256 [00:02<00:02, 20406.07it/s]Processing HH static:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 46218/96256 [00:02<00:02, 22027.80it/s]Processing HH static:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 48939/96256 [00:02<00:02, 23331.98it/s]Processing HH static:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 51529/96256 [00:02<00:02, 19959.31it/s]Processing HH static:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 53782/96256 [00:02<00:03, 14157.48it/s]Processing HH static:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 56526/96256 [00:02<00:02, 16684.45it/s]Processing HH static:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 59257/96256 [00:03<00:01, 18955.02it/s]Processing HH static:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 62004/96256 [00:03<00:01, 20945.88it/s]Processing HH static:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 64750/96256 [00:03<00:01, 22553.66it/s]Processing HH static:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 67483/96256 [00:03<00:01, 23810.16it/s]Processing HH static:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 70209/96256 [00:03<00:01, 24750.75it/s]Processing HH static:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 72945/96256 [00:03<00:00, 25481.29it/s]Processing HH static:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 75603/96256 [00:04<00:02, 8700.76it/s] Processing HH static:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 77565/96256 [00:05<00:03, 5498.17it/s]Processing HH static:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 79014/96256 [00:05<00:04, 4224.32it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 80094/96256 [00:06<00:04, 3482.67it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80908/96256 [00:06<00:04, 3199.09it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81545/96256 [00:07<00:04, 3019.54it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82060/96256 [00:07<00:04, 2923.86it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82495/96256 [00:07<00:04, 2808.93it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82869/96256 [00:07<00:05, 2669.90it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83194/96256 [00:07<00:05, 2608.04it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83492/96256 [00:07<00:05, 2507.78it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83764/96256 [00:07<00:05, 2492.79it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84028/96256 [00:08<00:05, 2438.25it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84280/96256 [00:08<00:05, 2394.86it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84524/96256 [00:08<00:04, 2399.76it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85122/96256 [00:08<00:03, 3281.36it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85472/96256 [00:08<00:03, 3286.14it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85816/96256 [00:08<00:03, 2936.31it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86126/96256 [00:08<00:03, 2655.99it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86406/96256 [00:08<00:03, 2535.53it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86669/96256 [00:09<00:03, 2445.41it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86920/96256 [00:09<00:03, 2417.53it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87166/96256 [00:09<00:03, 2346.69it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87403/96256 [00:09<00:03, 2330.29it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87638/96256 [00:09<00:03, 2260.03it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 87865/96256 [00:09<00:03, 2207.04it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88087/96256 [00:09<00:03, 2201.61it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88308/96256 [00:09<00:03, 2161.29it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88525/96256 [00:09<00:03, 2128.41it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88891/96256 [00:10<00:02, 2559.23it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89171/96256 [00:10<00:02, 2622.97it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89435/96256 [00:10<00:02, 2491.36it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89687/96256 [00:10<00:02, 2492.99it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89938/96256 [00:10<00:02, 2419.24it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90182/96256 [00:10<00:02, 2327.49it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90417/96256 [00:10<00:02, 2290.40it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90647/96256 [00:10<00:02, 2275.13it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90876/96256 [00:10<00:02, 2275.62it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91104/96256 [00:10<00:02, 2235.38it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91328/96256 [00:11<00:02, 2199.39it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91561/96256 [00:11<00:02, 2231.29it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91797/96256 [00:11<00:01, 2267.38it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92025/96256 [00:11<00:01, 2232.31it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92249/96256 [00:11<00:01, 2216.61it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92485/96256 [00:11<00:01, 2256.48it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 92752/96256 [00:11<00:01, 2378.10it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93071/96256 [00:11<00:01, 2612.32it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93333/96256 [00:11<00:01, 2607.07it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93594/96256 [00:11<00:01, 2588.83it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 93854/96256 [00:12<00:00, 2551.06it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94110/96256 [00:12<00:00, 2459.25it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94371/96256 [00:12<00:00, 2502.40it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94622/96256 [00:12<00:00, 2491.27it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94872/96256 [00:12<00:00, 2415.21it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95115/96256 [00:12<00:00, 2353.63it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95357/96256 [00:12<00:00, 2372.56it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95595/96256 [00:12<00:00, 2347.69it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95831/96256 [00:12<00:00, 2346.91it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96069/96256 [00:13<00:00, 2354.40it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:13<00:00, 7337.42it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:02<00:36,  2.44s/it]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:02<00:09,  1.43it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:02<00:04,  2.56it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:02<00:02,  3.80it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:03<00:01,  5.04it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:03<00:00,  6.10it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:03<00:00,  7.06it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  7.92it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  4.15it/s]
eval after 0: {'rewards_eval/chosen': '-0.00041692', 'rewards_eval/rejected': '-0.00032938', 'rewards_eval/accuracies': '0.5', 'rewards_eval/margins': '-8.7544e-05', 'logps_eval/rejected': '-121.15', 'logps_eval/chosen': '-140.8', 'loss/eval': '0.69319'}
train stats after 64 examples: {'rewards_train/chosen': '-5.8676e-05', 'rewards_train/rejected': '0.00014151', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '-0.00020019', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-136.83', 'loss/train': '0.69325', 'examples_per_second': '53.471', 'grad_norm': '14.625', 'counters/examples': 64, 'counters/updates': 1}
train stats after 128 examples: {'rewards_train/chosen': '-0.0012245', 'rewards_train/rejected': '0.00035988', 'rewards_train/accuracies': '0.39062', 'rewards_train/margins': '-0.0015844', 'logps_train/rejected': '-134.09', 'logps_train/chosen': '-142.45', 'loss/train': '0.69394', 'examples_per_second': '51.312', 'grad_norm': '14.418', 'counters/examples': 128, 'counters/updates': 2}
skipping logging after 192 examples to avoid logging too frequently
train stats after 256 examples: {'rewards_train/chosen': '5.7116e-05', 'rewards_train/rejected': '-0.00021045', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00026757', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-159.84', 'loss/train': '0.69302', 'examples_per_second': '91.33', 'grad_norm': '15.658', 'counters/examples': 256, 'counters/updates': 4}
skipping logging after 320 examples to avoid logging too frequently
train stats after 384 examples: {'rewards_train/chosen': '-0.00028957', 'rewards_train/rejected': '-0.00050015', 'rewards_train/accuracies': '0.48438', 'rewards_train/margins': '0.00021058', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-131.36', 'loss/train': '0.69304', 'examples_per_second': '93.503', 'grad_norm': '14.019', 'counters/examples': 384, 'counters/updates': 6}
skipping logging after 448 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '-0.0001705', 'rewards_train/rejected': '-0.00078283', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.00061233', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-143.27', 'loss/train': '0.69284', 'examples_per_second': '101.4', 'grad_norm': '14.595', 'counters/examples': 512, 'counters/updates': 8}
skipping logging after 576 examples to avoid logging too frequently
train stats after 640 examples: {'rewards_train/chosen': '0.00054872', 'rewards_train/rejected': '-0.00048494', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.0010337', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-144.46', 'loss/train': '0.69264', 'examples_per_second': '89.123', 'grad_norm': '14.457', 'counters/examples': 640, 'counters/updates': 10}
skipping logging after 704 examples to avoid logging too frequently
train stats after 768 examples: {'rewards_train/chosen': '0.00081834', 'rewards_train/rejected': '0.00010485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00071349', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-122.01', 'loss/train': '0.6928', 'examples_per_second': '96.943', 'grad_norm': '13.426', 'counters/examples': 768, 'counters/updates': 12}
skipping logging after 832 examples to avoid logging too frequently
train stats after 896 examples: {'rewards_train/chosen': '7.2366e-05', 'rewards_train/rejected': '-0.0026449', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0027173', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-124.74', 'loss/train': '0.6918', 'examples_per_second': '91.507', 'grad_norm': '13.686', 'counters/examples': 896, 'counters/updates': 14}
skipping logging after 960 examples to avoid logging too frequently
train stats after 1024 examples: {'rewards_train/chosen': '0.00063604', 'rewards_train/rejected': '-0.0038383', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.0044743', 'logps_train/rejected': '-146.45', 'logps_train/chosen': '-141.01', 'loss/train': '0.69093', 'examples_per_second': '90.894', 'grad_norm': '14.523', 'counters/examples': 1024, 'counters/updates': 16}
skipping logging after 1088 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '-0.0013829', 'rewards_train/rejected': '-0.0077653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0063824', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-117.65', 'loss/train': '0.69', 'examples_per_second': '88.395', 'grad_norm': '13.595', 'counters/examples': 1152, 'counters/updates': 18}
skipping logging after 1216 examples to avoid logging too frequently
train stats after 1280 examples: {'rewards_train/chosen': '0.0042418', 'rewards_train/rejected': '-0.004894', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.0091358', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-140.91', 'loss/train': '0.68865', 'examples_per_second': '91.439', 'grad_norm': '14.203', 'counters/examples': 1280, 'counters/updates': 20}
skipping logging after 1344 examples to avoid logging too frequently
train stats after 1408 examples: {'rewards_train/chosen': '0.0078846', 'rewards_train/rejected': '5.2565e-05', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.007832', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-155.6', 'loss/train': '0.68934', 'examples_per_second': '91.143', 'grad_norm': '15.267', 'counters/examples': 1408, 'counters/updates': 22}
skipping logging after 1472 examples to avoid logging too frequently
train stats after 1536 examples: {'rewards_train/chosen': '0.0019749', 'rewards_train/rejected': '-0.015427', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.017402', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-139.16', 'loss/train': '0.68469', 'examples_per_second': '90.986', 'grad_norm': '14.55', 'counters/examples': 1536, 'counters/updates': 24}
skipping logging after 1600 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '0.0070058', 'rewards_train/rejected': '-0.011792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.018798', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-128.45', 'loss/train': '0.684', 'examples_per_second': '88.234', 'grad_norm': '13.75', 'counters/examples': 1664, 'counters/updates': 26}
skipping logging after 1728 examples to avoid logging too frequently
train stats after 1792 examples: {'rewards_train/chosen': '0.0058818', 'rewards_train/rejected': '-0.020927', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.026808', 'logps_train/rejected': '-131.35', 'logps_train/chosen': '-181.75', 'loss/train': '0.68011', 'examples_per_second': '90.444', 'grad_norm': '16.027', 'counters/examples': 1792, 'counters/updates': 28}
skipping logging after 1856 examples to avoid logging too frequently
train stats after 1920 examples: {'rewards_train/chosen': '-0.010387', 'rewards_train/rejected': '-0.02899', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018603', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-119.48', 'loss/train': '0.68429', 'examples_per_second': '89.344', 'grad_norm': '13.872', 'counters/examples': 1920, 'counters/updates': 30}
skipping logging after 1984 examples to avoid logging too frequently
train stats after 2048 examples: {'rewards_train/chosen': '0.0028783', 'rewards_train/rejected': '-0.013242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01612', 'logps_train/rejected': '-157.69', 'logps_train/chosen': '-149.98', 'loss/train': '0.68565', 'examples_per_second': '91.354', 'grad_norm': '15.176', 'counters/examples': 2048, 'counters/updates': 32}
skipping logging after 2112 examples to avoid logging too frequently
train stats after 2176 examples: {'rewards_train/chosen': '-0.0061843', 'rewards_train/rejected': '-0.033311', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.027126', 'logps_train/rejected': '-135.35', 'logps_train/chosen': '-122.38', 'loss/train': '0.68056', 'examples_per_second': '93.82', 'grad_norm': '13.946', 'counters/examples': 2176, 'counters/updates': 34}
skipping logging after 2240 examples to avoid logging too frequently
train stats after 2304 examples: {'rewards_train/chosen': '0.0056818', 'rewards_train/rejected': '-0.045262', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.050943', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-130.86', 'loss/train': '0.66912', 'examples_per_second': '92.43', 'grad_norm': '13.601', 'counters/examples': 2304, 'counters/updates': 36}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2432 examples: {'rewards_train/chosen': '-0.0028906', 'rewards_train/rejected': '-0.049108', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.046218', 'logps_train/rejected': '-118.05', 'logps_train/chosen': '-135.64', 'loss/train': '0.67174', 'examples_per_second': '87.892', 'grad_norm': '13.637', 'counters/examples': 2432, 'counters/updates': 38}
skipping logging after 2496 examples to avoid logging too frequently
train stats after 2560 examples: {'rewards_train/chosen': '-0.012872', 'rewards_train/rejected': '-0.055819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.042947', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-141.38', 'loss/train': '0.67382', 'examples_per_second': '89.401', 'grad_norm': '14.359', 'counters/examples': 2560, 'counters/updates': 40}
skipping logging after 2624 examples to avoid logging too frequently
train stats after 2688 examples: {'rewards_train/chosen': '-0.031527', 'rewards_train/rejected': '-0.029116', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024104', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-144.99', 'loss/train': '0.69575', 'examples_per_second': '88.989', 'grad_norm': '15.503', 'counters/examples': 2688, 'counters/updates': 42}
skipping logging after 2752 examples to avoid logging too frequently
train stats after 2816 examples: {'rewards_train/chosen': '-0.00088327', 'rewards_train/rejected': '-0.062159', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.061276', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-140.15', 'loss/train': '0.66583', 'examples_per_second': '89.786', 'grad_norm': '14.291', 'counters/examples': 2816, 'counters/updates': 44}
skipping logging after 2880 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '-0.00036467', 'rewards_train/rejected': '-0.05487', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.054505', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-151.66', 'loss/train': '0.66856', 'examples_per_second': '97.62', 'grad_norm': '14.565', 'counters/examples': 2944, 'counters/updates': 46}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3072 examples: {'rewards_train/chosen': '0.0054178', 'rewards_train/rejected': '-0.049946', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055364', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-138.11', 'loss/train': '0.66914', 'examples_per_second': '94.428', 'grad_norm': '13.865', 'counters/examples': 3072, 'counters/updates': 48}
skipping logging after 3136 examples to avoid logging too frequently
train stats after 3200 examples: {'rewards_train/chosen': '-0.010157', 'rewards_train/rejected': '-0.097736', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.087579', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-136.32', 'loss/train': '0.6548', 'examples_per_second': '91.195', 'grad_norm': '13.759', 'counters/examples': 3200, 'counters/updates': 50}
skipping logging after 3264 examples to avoid logging too frequently
train stats after 3328 examples: {'rewards_train/chosen': '-0.0034323', 'rewards_train/rejected': '-0.072235', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.068802', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-149.74', 'loss/train': '0.6639', 'examples_per_second': '89.263', 'grad_norm': '14.522', 'counters/examples': 3328, 'counters/updates': 52}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3456 examples: {'rewards_train/chosen': '-0.047543', 'rewards_train/rejected': '-0.10751', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.059963', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-162.82', 'loss/train': '0.67061', 'examples_per_second': '88.531', 'grad_norm': '14.848', 'counters/examples': 3456, 'counters/updates': 54}
skipping logging after 3520 examples to avoid logging too frequently
train stats after 3584 examples: {'rewards_train/chosen': '0.015035', 'rewards_train/rejected': '-0.083112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098147', 'logps_train/rejected': '-123.67', 'logps_train/chosen': '-166.72', 'loss/train': '0.65467', 'examples_per_second': '91.348', 'grad_norm': '15.163', 'counters/examples': 3584, 'counters/updates': 56}
skipping logging after 3648 examples to avoid logging too frequently
train stats after 3712 examples: {'rewards_train/chosen': '-0.037264', 'rewards_train/rejected': '-0.10187', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.064601', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-145.02', 'loss/train': '0.67215', 'examples_per_second': '91.129', 'grad_norm': '14.959', 'counters/examples': 3712, 'counters/updates': 58}
skipping logging after 3776 examples to avoid logging too frequently
train stats after 3840 examples: {'rewards_train/chosen': '-0.018208', 'rewards_train/rejected': '-0.10697', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088762', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-130.22', 'loss/train': '0.65845', 'examples_per_second': '91.314', 'grad_norm': '13.85', 'counters/examples': 3840, 'counters/updates': 60}
skipping logging after 3904 examples to avoid logging too frequently
train stats after 3968 examples: {'rewards_train/chosen': '0.018481', 'rewards_train/rejected': '-0.11393', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.13241', 'logps_train/rejected': '-106.04', 'logps_train/chosen': '-148.32', 'loss/train': '0.64029', 'examples_per_second': '88.693', 'grad_norm': '13.734', 'counters/examples': 3968, 'counters/updates': 62}
skipping logging after 4032 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '0.019673', 'rewards_train/rejected': '-0.10145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12112', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-142.51', 'loss/train': '0.64796', 'examples_per_second': '94.669', 'grad_norm': '14.029', 'counters/examples': 4096, 'counters/updates': 64}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4224 examples: {'rewards_train/chosen': '-0.016512', 'rewards_train/rejected': '-0.12029', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10378', 'logps_train/rejected': '-132.69', 'logps_train/chosen': '-117.7', 'loss/train': '0.654', 'examples_per_second': '91.326', 'grad_norm': '14.334', 'counters/examples': 4224, 'counters/updates': 66}
skipping logging after 4288 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '0.017839', 'rewards_train/rejected': '-0.075753', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.093592', 'logps_train/rejected': '-142.49', 'logps_train/chosen': '-145.91', 'loss/train': '0.65977', 'examples_per_second': '96.41', 'grad_norm': '15.118', 'counters/examples': 4352, 'counters/updates': 68}
skipping logging after 4416 examples to avoid logging too frequently
train stats after 4480 examples: {'rewards_train/chosen': '-0.0076713', 'rewards_train/rejected': '-0.14404', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13637', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-149.96', 'loss/train': '0.6403', 'examples_per_second': '94.37', 'grad_norm': '14.724', 'counters/examples': 4480, 'counters/updates': 70}
skipping logging after 4544 examples to avoid logging too frequently
train stats after 4608 examples: {'rewards_train/chosen': '0.011042', 'rewards_train/rejected': '-0.16126', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1723', 'logps_train/rejected': '-127.82', 'logps_train/chosen': '-141.39', 'loss/train': '0.62436', 'examples_per_second': '90.981', 'grad_norm': '14.316', 'counters/examples': 4608, 'counters/updates': 72}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4736 examples: {'rewards_train/chosen': '0.060843', 'rewards_train/rejected': '-0.10182', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16267', 'logps_train/rejected': '-140.49', 'logps_train/chosen': '-161.64', 'loss/train': '0.62674', 'examples_per_second': '89.782', 'grad_norm': '15.398', 'counters/examples': 4736, 'counters/updates': 74}
skipping logging after 4800 examples to avoid logging too frequently
train stats after 4864 examples: {'rewards_train/chosen': '0.022267', 'rewards_train/rejected': '-0.15686', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.17913', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-157.95', 'loss/train': '0.63556', 'examples_per_second': '88.384', 'grad_norm': '15.634', 'counters/examples': 4864, 'counters/updates': 76}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4992 examples: {'rewards_train/chosen': '-0.037279', 'rewards_train/rejected': '-0.23567', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.19839', 'logps_train/rejected': '-127.88', 'logps_train/chosen': '-158.4', 'loss/train': '0.63289', 'examples_per_second': '91.018', 'grad_norm': '15.318', 'counters/examples': 4992, 'counters/updates': 78}
skipping logging after 5056 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '0.023963', 'rewards_train/rejected': '-0.18836', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.21232', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-119.36', 'loss/train': '0.61623', 'examples_per_second': '91.287', 'grad_norm': '13.889', 'counters/examples': 5120, 'counters/updates': 80}
skipping logging after 5184 examples to avoid logging too frequently
train stats after 5248 examples: {'rewards_train/chosen': '-0.014221', 'rewards_train/rejected': '-0.1927', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17848', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-128.82', 'loss/train': '0.62223', 'examples_per_second': '91.078', 'grad_norm': '14.221', 'counters/examples': 5248, 'counters/updates': 82}
skipping logging after 5312 examples to avoid logging too frequently
train stats after 5376 examples: {'rewards_train/chosen': '-0.014327', 'rewards_train/rejected': '-0.17929', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.16496', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-160.77', 'loss/train': '0.63361', 'examples_per_second': '91.075', 'grad_norm': '15.335', 'counters/examples': 5376, 'counters/updates': 84}
skipping logging after 5440 examples to avoid logging too frequently
train stats after 5504 examples: {'rewards_train/chosen': '-0.028396', 'rewards_train/rejected': '-0.21779', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.18939', 'logps_train/rejected': '-119.55', 'logps_train/chosen': '-144.83', 'loss/train': '0.62429', 'examples_per_second': '97.007', 'grad_norm': '14.173', 'counters/examples': 5504, 'counters/updates': 86}
skipping logging after 5568 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.014161', 'rewards_train/rejected': '-0.21555', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.20139', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-115.97', 'loss/train': '0.62667', 'examples_per_second': '109.09', 'grad_norm': '13.669', 'counters/examples': 5632, 'counters/updates': 88}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5760 examples: {'rewards_train/chosen': '-0.046151', 'rewards_train/rejected': '-0.19273', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.14658', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-152.7', 'loss/train': '0.64799', 'examples_per_second': '94.328', 'grad_norm': '15.121', 'counters/examples': 5760, 'counters/updates': 90}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5888 examples: {'rewards_train/chosen': '0.045465', 'rewards_train/rejected': '-0.18295', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.22842', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-132.39', 'loss/train': '0.61197', 'examples_per_second': '91.025', 'grad_norm': '14.532', 'counters/examples': 5888, 'counters/updates': 92}
skipping logging after 5952 examples to avoid logging too frequently
train stats after 6016 examples: {'rewards_train/chosen': '-0.045092', 'rewards_train/rejected': '-0.20624', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16115', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-133.36', 'loss/train': '0.64972', 'examples_per_second': '90.332', 'grad_norm': '14.945', 'counters/examples': 6016, 'counters/updates': 94}
skipping logging after 6080 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.056342', 'rewards_train/rejected': '-0.29895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24261', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-136.76', 'loss/train': '0.61876', 'examples_per_second': '90.933', 'grad_norm': '14.459', 'counters/examples': 6144, 'counters/updates': 96}
skipping logging after 6208 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '-0.0087001', 'rewards_train/rejected': '-0.22468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21598', 'logps_train/rejected': '-100.88', 'logps_train/chosen': '-154.23', 'loss/train': '0.6333', 'examples_per_second': '91.223', 'grad_norm': '15.557', 'counters/examples': 6272, 'counters/updates': 98}
skipping logging after 6336 examples to avoid logging too frequently
train stats after 6400 examples: {'rewards_train/chosen': '0.010098', 'rewards_train/rejected': '-0.23479', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24489', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-159.51', 'loss/train': '0.61271', 'examples_per_second': '90.713', 'grad_norm': '15.56', 'counters/examples': 6400, 'counters/updates': 100}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6528 examples: {'rewards_train/chosen': '-0.074331', 'rewards_train/rejected': '-0.17112', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.09679', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-120.38', 'loss/train': '0.67756', 'examples_per_second': '90.592', 'grad_norm': '14.045', 'counters/examples': 6528, 'counters/updates': 102}
skipping logging after 6592 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '-0.1467', 'rewards_train/rejected': '-0.23398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087281', 'logps_train/rejected': '-102.55', 'logps_train/chosen': '-131.04', 'loss/train': '0.67598', 'examples_per_second': '97.498', 'grad_norm': '14.625', 'counters/examples': 6656, 'counters/updates': 104}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6784 examples: {'rewards_train/chosen': '-0.099878', 'rewards_train/rejected': '-0.27484', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17496', 'logps_train/rejected': '-120', 'logps_train/chosen': '-146.25', 'loss/train': '0.63626', 'examples_per_second': '89.643', 'grad_norm': '15.007', 'counters/examples': 6784, 'counters/updates': 106}
skipping logging after 6848 examples to avoid logging too frequently
train stats after 6912 examples: {'rewards_train/chosen': '-0.080658', 'rewards_train/rejected': '-0.19177', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.11111', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-152.08', 'loss/train': '0.67006', 'examples_per_second': '87.779', 'grad_norm': '15.771', 'counters/examples': 6912, 'counters/updates': 108}
skipping logging after 6976 examples to avoid logging too frequently
train stats after 7040 examples: {'rewards_train/chosen': '-0.12936', 'rewards_train/rejected': '-0.22293', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.093568', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-152.61', 'loss/train': '0.67435', 'examples_per_second': '90.855', 'grad_norm': '16.981', 'counters/examples': 7040, 'counters/updates': 110}
skipping logging after 7104 examples to avoid logging too frequently
train stats after 7168 examples: {'rewards_train/chosen': '0.028166', 'rewards_train/rejected': '-0.12021', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.14838', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-154.98', 'loss/train': '0.64367', 'examples_per_second': '90.771', 'grad_norm': '15.686', 'counters/examples': 7168, 'counters/updates': 112}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7296 examples: {'rewards_train/chosen': '0.029617', 'rewards_train/rejected': '-0.23339', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.26301', 'logps_train/rejected': '-129.49', 'logps_train/chosen': '-154.49', 'loss/train': '0.60069', 'examples_per_second': '90.548', 'grad_norm': '14.517', 'counters/examples': 7296, 'counters/updates': 114}
skipping logging after 7360 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '-0.12002', 'rewards_train/rejected': '-0.3576', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.23759', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-132.29', 'loss/train': '0.61258', 'examples_per_second': '90.528', 'grad_norm': '13.733', 'counters/examples': 7424, 'counters/updates': 116}
skipping logging after 7488 examples to avoid logging too frequently
train stats after 7552 examples: {'rewards_train/chosen': '-0.037452', 'rewards_train/rejected': '-0.20063', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16318', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-124.22', 'loss/train': '0.64835', 'examples_per_second': '88.526', 'grad_norm': '14.478', 'counters/examples': 7552, 'counters/updates': 118}
skipping logging after 7616 examples to avoid logging too frequently
train stats after 7680 examples: {'rewards_train/chosen': '0.052622', 'rewards_train/rejected': '-0.11027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16289', 'logps_train/rejected': '-144.9', 'logps_train/chosen': '-152.56', 'loss/train': '0.64602', 'examples_per_second': '97.189', 'grad_norm': '15.779', 'counters/examples': 7680, 'counters/updates': 120}
skipping logging after 7744 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '-0.074816', 'rewards_train/rejected': '-0.41297', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33815', 'logps_train/rejected': '-109.36', 'logps_train/chosen': '-166.65', 'loss/train': '0.60325', 'examples_per_second': '93.953', 'grad_norm': '14.523', 'counters/examples': 7808, 'counters/updates': 122}
skipping logging after 7872 examples to avoid logging too frequently
train stats after 7936 examples: {'rewards_train/chosen': '-0.12668', 'rewards_train/rejected': '-0.19956', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.072873', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-114.85', 'loss/train': '0.69914', 'examples_per_second': '88.038', 'grad_norm': '15.067', 'counters/examples': 7936, 'counters/updates': 124}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8064 examples: {'rewards_train/chosen': '0.024834', 'rewards_train/rejected': '-0.13746', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.1623', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-155.07', 'loss/train': '0.64961', 'examples_per_second': '88.387', 'grad_norm': '16.248', 'counters/examples': 8064, 'counters/updates': 126}
skipping logging after 8128 examples to avoid logging too frequently
train stats after 8192 examples: {'rewards_train/chosen': '-0.031397', 'rewards_train/rejected': '-0.24849', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.21709', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-170.86', 'loss/train': '0.62395', 'examples_per_second': '87.688', 'grad_norm': '16.003', 'counters/examples': 8192, 'counters/updates': 128}
skipping logging after 8256 examples to avoid logging too frequently
train stats after 8320 examples: {'rewards_train/chosen': '-0.087975', 'rewards_train/rejected': '-0.32964', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.24167', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-134.38', 'loss/train': '0.62449', 'examples_per_second': '93.561', 'grad_norm': '14.736', 'counters/examples': 8320, 'counters/updates': 130}
skipping logging after 8384 examples to avoid logging too frequently
train stats after 8448 examples: {'rewards_train/chosen': '0.067898', 'rewards_train/rejected': '-0.26073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32863', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-146.49', 'loss/train': '0.59615', 'examples_per_second': '90.55', 'grad_norm': '14.442', 'counters/examples': 8448, 'counters/updates': 132}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8576 examples: {'rewards_train/chosen': '0.10994', 'rewards_train/rejected': '-0.090011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19995', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-150.87', 'loss/train': '0.63752', 'examples_per_second': '88.779', 'grad_norm': '15.179', 'counters/examples': 8576, 'counters/updates': 134}
skipping logging after 8640 examples to avoid logging too frequently
train stats after 8704 examples: {'rewards_train/chosen': '0.013954', 'rewards_train/rejected': '-0.35583', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.36979', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-126.22', 'loss/train': '0.56297', 'examples_per_second': '90.71', 'grad_norm': '12.714', 'counters/examples': 8704, 'counters/updates': 136}
skipping logging after 8768 examples to avoid logging too frequently
train stats after 8832 examples: {'rewards_train/chosen': '0.046769', 'rewards_train/rejected': '-0.28785', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.33462', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-147.7', 'loss/train': '0.59609', 'examples_per_second': '90.236', 'grad_norm': '15.324', 'counters/examples': 8832, 'counters/updates': 138}
skipping logging after 8896 examples to avoid logging too frequently
train stats after 8960 examples: {'rewards_train/chosen': '-0.068709', 'rewards_train/rejected': '-0.26439', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.19568', 'logps_train/rejected': '-109.34', 'logps_train/chosen': '-129.57', 'loss/train': '0.64161', 'examples_per_second': '87.914', 'grad_norm': '14.602', 'counters/examples': 8960, 'counters/updates': 140}
skipping logging after 9024 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '-0.14716', 'rewards_train/rejected': '-0.48687', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.33971', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-135.75', 'loss/train': '0.59326', 'examples_per_second': '90.617', 'grad_norm': '14.694', 'counters/examples': 9088, 'counters/updates': 142}
skipping logging after 9152 examples to avoid logging too frequently
train stats after 9216 examples: {'rewards_train/chosen': '-0.061195', 'rewards_train/rejected': '-0.25749', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.1963', 'logps_train/rejected': '-141.7', 'logps_train/chosen': '-152.29', 'loss/train': '0.64384', 'examples_per_second': '90.486', 'grad_norm': '16.445', 'counters/examples': 9216, 'counters/updates': 144}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9344 examples: {'rewards_train/chosen': '-0.14384', 'rewards_train/rejected': '-0.33814', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1943', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-143.64', 'loss/train': '0.64685', 'examples_per_second': '90.568', 'grad_norm': '15.846', 'counters/examples': 9344, 'counters/updates': 146}
skipping logging after 9408 examples to avoid logging too frequently
train stats after 9472 examples: {'rewards_train/chosen': '-0.16824', 'rewards_train/rejected': '-0.49638', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.32814', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-116.32', 'loss/train': '0.61562', 'examples_per_second': '90.751', 'grad_norm': '13.204', 'counters/examples': 9472, 'counters/updates': 148}
skipping logging after 9536 examples to avoid logging too frequently
train stats after 9600 examples: {'rewards_train/chosen': '-0.048361', 'rewards_train/rejected': '-0.30787', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25951', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-139.53', 'loss/train': '0.61955', 'examples_per_second': '90.556', 'grad_norm': '15.619', 'counters/examples': 9600, 'counters/updates': 150}
skipping logging after 9664 examples to avoid logging too frequently
train stats after 9728 examples: {'rewards_train/chosen': '-0.08708', 'rewards_train/rejected': '-0.46895', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.38187', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-144.13', 'loss/train': '0.56354', 'examples_per_second': '87.119', 'grad_norm': '14.195', 'counters/examples': 9728, 'counters/updates': 152}
skipping logging after 9792 examples to avoid logging too frequently
train stats after 9856 examples: {'rewards_train/chosen': '-0.32767', 'rewards_train/rejected': '-0.65526', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3276', 'logps_train/rejected': '-99.686', 'logps_train/chosen': '-113.99', 'loss/train': '0.58797', 'examples_per_second': '92.96', 'grad_norm': '13.229', 'counters/examples': 9856, 'counters/updates': 154}
skipping logging after 9920 examples to avoid logging too frequently
train stats after 9984 examples: {'rewards_train/chosen': '-0.19358', 'rewards_train/rejected': '-0.72267', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.52909', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-136.8', 'loss/train': '0.54626', 'examples_per_second': '90.269', 'grad_norm': '15.223', 'counters/examples': 9984, 'counters/updates': 156}
skipping logging after 10048 examples to avoid logging too frequently
train stats after 10112 examples: {'rewards_train/chosen': '-0.34083', 'rewards_train/rejected': '-0.67892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33809', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-172.88', 'loss/train': '0.61119', 'examples_per_second': '90.236', 'grad_norm': '17.186', 'counters/examples': 10112, 'counters/updates': 158}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10240 examples: {'rewards_train/chosen': '-0.23736', 'rewards_train/rejected': '-0.44024', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.20288', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-167.3', 'loss/train': '0.65462', 'examples_per_second': '90.418', 'grad_norm': '17.156', 'counters/examples': 10240, 'counters/updates': 160}
skipping logging after 10304 examples to avoid logging too frequently
train stats after 10368 examples: {'rewards_train/chosen': '-0.24902', 'rewards_train/rejected': '-0.63984', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.39082', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-148', 'loss/train': '0.5799', 'examples_per_second': '90.097', 'grad_norm': '14.829', 'counters/examples': 10368, 'counters/updates': 162}
skipping logging after 10432 examples to avoid logging too frequently
train stats after 10496 examples: {'rewards_train/chosen': '-0.31869', 'rewards_train/rejected': '-0.68664', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36795', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-134.2', 'loss/train': '0.57965', 'examples_per_second': '90.487', 'grad_norm': '14.96', 'counters/examples': 10496, 'counters/updates': 164}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10624 examples: {'rewards_train/chosen': '-0.17102', 'rewards_train/rejected': '-0.57592', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.4049', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-130.65', 'loss/train': '0.57071', 'examples_per_second': '88.451', 'grad_norm': '13.614', 'counters/examples': 10624, 'counters/updates': 166}
skipping logging after 10688 examples to avoid logging too frequently
train stats after 10752 examples: {'rewards_train/chosen': '-0.27025', 'rewards_train/rejected': '-0.7022', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43195', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-147.38', 'loss/train': '0.57289', 'examples_per_second': '86.944', 'grad_norm': '15.702', 'counters/examples': 10752, 'counters/updates': 168}
skipping logging after 10816 examples to avoid logging too frequently
train stats after 10880 examples: {'rewards_train/chosen': '-0.4679', 'rewards_train/rejected': '-0.75086', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28296', 'logps_train/rejected': '-178.36', 'logps_train/chosen': '-163.16', 'loss/train': '0.622', 'examples_per_second': '90.381', 'grad_norm': '16.955', 'counters/examples': 10880, 'counters/updates': 170}
skipping logging after 10944 examples to avoid logging too frequently
train stats after 11008 examples: {'rewards_train/chosen': '-0.16751', 'rewards_train/rejected': '-0.61522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44771', 'logps_train/rejected': '-125.62', 'logps_train/chosen': '-151.11', 'loss/train': '0.58748', 'examples_per_second': '89.539', 'grad_norm': '15.148', 'counters/examples': 11008, 'counters/updates': 172}
skipping logging after 11072 examples to avoid logging too frequently
train stats after 11136 examples: {'rewards_train/chosen': '-0.0767', 'rewards_train/rejected': '-0.44265', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.36595', 'logps_train/rejected': '-156.27', 'logps_train/chosen': '-163.65', 'loss/train': '0.58383', 'examples_per_second': '90.596', 'grad_norm': '15.86', 'counters/examples': 11136, 'counters/updates': 174}
skipping logging after 11200 examples to avoid logging too frequently
train stats after 11264 examples: {'rewards_train/chosen': '-0.37425', 'rewards_train/rejected': '-0.51953', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.14527', 'logps_train/rejected': '-147.35', 'logps_train/chosen': '-143.52', 'loss/train': '0.68984', 'examples_per_second': '90.357', 'grad_norm': '17.072', 'counters/examples': 11264, 'counters/updates': 176}
skipping logging after 11328 examples to avoid logging too frequently
train stats after 11392 examples: {'rewards_train/chosen': '-0.13533', 'rewards_train/rejected': '-0.52942', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39409', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-140.9', 'loss/train': '0.57979', 'examples_per_second': '87.633', 'grad_norm': '14.425', 'counters/examples': 11392, 'counters/updates': 178}
skipping logging after 11456 examples to avoid logging too frequently
train stats after 11520 examples: {'rewards_train/chosen': '-0.18957', 'rewards_train/rejected': '-0.46554', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27597', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-161.36', 'loss/train': '0.62412', 'examples_per_second': '92.611', 'grad_norm': '16.176', 'counters/examples': 11520, 'counters/updates': 180}
skipping logging after 11584 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '-0.25301', 'rewards_train/rejected': '-0.51807', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.26506', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-135.4', 'loss/train': '0.63584', 'examples_per_second': '87.818', 'grad_norm': '15.917', 'counters/examples': 11648, 'counters/updates': 182}
skipping logging after 11712 examples to avoid logging too frequently
train stats after 11776 examples: {'rewards_train/chosen': '-0.24984', 'rewards_train/rejected': '-0.72769', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.47785', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-174.66', 'loss/train': '0.57193', 'examples_per_second': '88.071', 'grad_norm': '16.009', 'counters/examples': 11776, 'counters/updates': 184}
skipping logging after 11840 examples to avoid logging too frequently
train stats after 11904 examples: {'rewards_train/chosen': '-0.39378', 'rewards_train/rejected': '-0.66344', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.26966', 'logps_train/rejected': '-164.66', 'logps_train/chosen': '-152.86', 'loss/train': '0.63208', 'examples_per_second': '90.072', 'grad_norm': '17.506', 'counters/examples': 11904, 'counters/updates': 186}
skipping logging after 11968 examples to avoid logging too frequently
Running evaluation after 11968 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.34it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.57it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.81it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.66it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.65it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.62it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.54it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.51it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.57it/s]
eval after 11968: {'rewards_eval/chosen': '-0.28482', 'rewards_eval/rejected': '-0.61626', 'rewards_eval/accuracies': '0.62891', 'rewards_eval/margins': '0.33143', 'logps_eval/rejected': '-127.31', 'logps_eval/chosen': '-143.65', 'loss/eval': '0.62911'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-11968...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-11968/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-11968/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-11968/scheduler.pt...
train stats after 12032 examples: {'rewards_train/chosen': '-0.11979', 'rewards_train/rejected': '-0.56025', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.44046', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-141.21', 'loss/train': '0.58165', 'examples_per_second': '61.621', 'grad_norm': '14.987', 'counters/examples': 12032, 'counters/updates': 188}
skipping logging after 12096 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.45686', 'rewards_train/rejected': '-0.92272', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46586', 'logps_train/rejected': '-137.85', 'logps_train/chosen': '-155.88', 'loss/train': '0.57396', 'examples_per_second': '89.295', 'grad_norm': '15.32', 'counters/examples': 12160, 'counters/updates': 190}
skipping logging after 12224 examples to avoid logging too frequently
train stats after 12288 examples: {'rewards_train/chosen': '-0.48446', 'rewards_train/rejected': '-0.86137', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.37691', 'logps_train/rejected': '-139.97', 'logps_train/chosen': '-146.3', 'loss/train': '0.60614', 'examples_per_second': '91.028', 'grad_norm': '15.651', 'counters/examples': 12288, 'counters/updates': 192}
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12416 examples: {'rewards_train/chosen': '-0.278', 'rewards_train/rejected': '-0.56055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.28256', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-117.25', 'loss/train': '0.64262', 'examples_per_second': '90.846', 'grad_norm': '14.527', 'counters/examples': 12416, 'counters/updates': 194}
skipping logging after 12480 examples to avoid logging too frequently
train stats after 12544 examples: {'rewards_train/chosen': '-0.31153', 'rewards_train/rejected': '-0.64854', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33701', 'logps_train/rejected': '-119.12', 'logps_train/chosen': '-154.46', 'loss/train': '0.61495', 'examples_per_second': '90.974', 'grad_norm': '15.664', 'counters/examples': 12544, 'counters/updates': 196}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12672 examples: {'rewards_train/chosen': '-0.13027', 'rewards_train/rejected': '-0.42527', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.295', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-147.69', 'loss/train': '0.62398', 'examples_per_second': '87.746', 'grad_norm': '16.04', 'counters/examples': 12672, 'counters/updates': 198}
skipping logging after 12736 examples to avoid logging too frequently
train stats after 12800 examples: {'rewards_train/chosen': '-0.26566', 'rewards_train/rejected': '-0.63102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.36536', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-139.21', 'loss/train': '0.61005', 'examples_per_second': '86.514', 'grad_norm': '15.22', 'counters/examples': 12800, 'counters/updates': 200}
skipping logging after 12864 examples to avoid logging too frequently
train stats after 12928 examples: {'rewards_train/chosen': '-0.1582', 'rewards_train/rejected': '-0.43279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2746', 'logps_train/rejected': '-159.95', 'logps_train/chosen': '-147.71', 'loss/train': '0.63692', 'examples_per_second': '90.867', 'grad_norm': '17.357', 'counters/examples': 12928, 'counters/updates': 202}
skipping logging after 12992 examples to avoid logging too frequently
train stats after 13056 examples: {'rewards_train/chosen': '-0.27587', 'rewards_train/rejected': '-0.64123', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.36536', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-118.97', 'loss/train': '0.60501', 'examples_per_second': '90.599', 'grad_norm': '14.267', 'counters/examples': 13056, 'counters/updates': 204}
skipping logging after 13120 examples to avoid logging too frequently
train stats after 13184 examples: {'rewards_train/chosen': '-0.16602', 'rewards_train/rejected': '-0.47808', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.31206', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-143.21', 'loss/train': '0.60104', 'examples_per_second': '90.365', 'grad_norm': '15.329', 'counters/examples': 13184, 'counters/updates': 206}
skipping logging after 13248 examples to avoid logging too frequently
train stats after 13312 examples: {'rewards_train/chosen': '-0.12207', 'rewards_train/rejected': '-0.52986', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.40778', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-169.22', 'loss/train': '0.56125', 'examples_per_second': '90.626', 'grad_norm': '14.983', 'counters/examples': 13312, 'counters/updates': 208}
skipping logging after 13376 examples to avoid logging too frequently
train stats after 13440 examples: {'rewards_train/chosen': '-0.46754', 'rewards_train/rejected': '-0.77904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31151', 'logps_train/rejected': '-140.4', 'logps_train/chosen': '-146.12', 'loss/train': '0.6403', 'examples_per_second': '90.584', 'grad_norm': '16.444', 'counters/examples': 13440, 'counters/updates': 210}
skipping logging after 13504 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '-0.27059', 'rewards_train/rejected': '-0.78312', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.51253', 'logps_train/rejected': '-144.09', 'logps_train/chosen': '-141.7', 'loss/train': '0.52756', 'examples_per_second': '90.469', 'grad_norm': '14.359', 'counters/examples': 13568, 'counters/updates': 212}
skipping logging after 13632 examples to avoid logging too frequently
train stats after 13696 examples: {'rewards_train/chosen': '-0.43114', 'rewards_train/rejected': '-0.66083', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.22969', 'logps_train/rejected': '-146.15', 'logps_train/chosen': '-146.9', 'loss/train': '0.6549', 'examples_per_second': '93.178', 'grad_norm': '17.98', 'counters/examples': 13696, 'counters/updates': 214}
skipping logging after 13760 examples to avoid logging too frequently
train stats after 13824 examples: {'rewards_train/chosen': '-0.30132', 'rewards_train/rejected': '-0.56277', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26145', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-145.45', 'loss/train': '0.65267', 'examples_per_second': '90.295', 'grad_norm': '17.268', 'counters/examples': 13824, 'counters/updates': 216}
skipping logging after 13888 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '-0.10639', 'rewards_train/rejected': '-0.43722', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33083', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-138.45', 'loss/train': '0.61669', 'examples_per_second': '97.12', 'grad_norm': '15.474', 'counters/examples': 13952, 'counters/updates': 218}
skipping logging after 14016 examples to avoid logging too frequently
train stats after 14080 examples: {'rewards_train/chosen': '-0.072771', 'rewards_train/rejected': '-0.35967', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2869', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-122.44', 'loss/train': '0.6505', 'examples_per_second': '90.548', 'grad_norm': '15.752', 'counters/examples': 14080, 'counters/updates': 220}
skipping logging after 14144 examples to avoid logging too frequently
train stats after 14208 examples: {'rewards_train/chosen': '-0.1339', 'rewards_train/rejected': '-0.31557', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18167', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-148.67', 'loss/train': '0.65835', 'examples_per_second': '95.792', 'grad_norm': '16', 'counters/examples': 14208, 'counters/updates': 222}
skipping logging after 14272 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.07658', 'rewards_train/rejected': '-0.2709', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34748', 'logps_train/rejected': '-151.05', 'logps_train/chosen': '-140.64', 'loss/train': '0.58555', 'examples_per_second': '88.954', 'grad_norm': '15.395', 'counters/examples': 14336, 'counters/updates': 224}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14464 examples: {'rewards_train/chosen': '-0.0006827', 'rewards_train/rejected': '-0.18182', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18113', 'logps_train/rejected': '-142.89', 'logps_train/chosen': '-152.78', 'loss/train': '0.65853', 'examples_per_second': '90.957', 'grad_norm': '18.252', 'counters/examples': 14464, 'counters/updates': 226}
skipping logging after 14528 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '0.12104', 'rewards_train/rejected': '-0.23678', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.35782', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-132.35', 'loss/train': '0.57777', 'examples_per_second': '90.365', 'grad_norm': '14.74', 'counters/examples': 14592, 'counters/updates': 228}
skipping logging after 14656 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '-0.032459', 'rewards_train/rejected': '-0.30674', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27428', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-147.08', 'loss/train': '0.61852', 'examples_per_second': '91.805', 'grad_norm': '16.381', 'counters/examples': 14720, 'counters/updates': 230}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14848 examples: {'rewards_train/chosen': '-0.14729', 'rewards_train/rejected': '-0.42166', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.27437', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-135.63', 'loss/train': '0.62977', 'examples_per_second': '90.509', 'grad_norm': '15.312', 'counters/examples': 14848, 'counters/updates': 232}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14976 examples: {'rewards_train/chosen': '-0.11272', 'rewards_train/rejected': '-0.48147', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.36875', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-123.67', 'loss/train': '0.59481', 'examples_per_second': '90.761', 'grad_norm': '13.928', 'counters/examples': 14976, 'counters/updates': 234}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15104 examples: {'rewards_train/chosen': '-0.17779', 'rewards_train/rejected': '-0.45238', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.27459', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-140.48', 'loss/train': '0.6166', 'examples_per_second': '88.431', 'grad_norm': '16.016', 'counters/examples': 15104, 'counters/updates': 236}
skipping logging after 15168 examples to avoid logging too frequently
train stats after 15232 examples: {'rewards_train/chosen': '-0.18846', 'rewards_train/rejected': '-0.34419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15572', 'logps_train/rejected': '-124.51', 'logps_train/chosen': '-142.63', 'loss/train': '0.66912', 'examples_per_second': '90.41', 'grad_norm': '16.15', 'counters/examples': 15232, 'counters/updates': 238}
skipping logging after 15296 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '-0.12086', 'rewards_train/rejected': '-0.48236', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3615', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-156.16', 'loss/train': '0.58562', 'examples_per_second': '83.54', 'grad_norm': '15.425', 'counters/examples': 15360, 'counters/updates': 240}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15488 examples: {'rewards_train/chosen': '-0.16812', 'rewards_train/rejected': '-0.37806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20994', 'logps_train/rejected': '-104.17', 'logps_train/chosen': '-124.71', 'loss/train': '0.64111', 'examples_per_second': '85.691', 'grad_norm': '15.549', 'counters/examples': 15488, 'counters/updates': 242}
skipping logging after 15552 examples to avoid logging too frequently
train stats after 15616 examples: {'rewards_train/chosen': '-0.16293', 'rewards_train/rejected': '-0.59753', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4346', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-148.25', 'loss/train': '0.57306', 'examples_per_second': '90.469', 'grad_norm': '14.636', 'counters/examples': 15616, 'counters/updates': 244}
skipping logging after 15680 examples to avoid logging too frequently
train stats after 15744 examples: {'rewards_train/chosen': '-0.12997', 'rewards_train/rejected': '-0.45883', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.32885', 'logps_train/rejected': '-135.59', 'logps_train/chosen': '-129.26', 'loss/train': '0.59358', 'examples_per_second': '90.589', 'grad_norm': '15.983', 'counters/examples': 15744, 'counters/updates': 246}
skipping logging after 15808 examples to avoid logging too frequently
train stats after 15872 examples: {'rewards_train/chosen': '-0.11728', 'rewards_train/rejected': '-0.56557', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44829', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-131.22', 'loss/train': '0.55679', 'examples_per_second': '90.162', 'grad_norm': '14.957', 'counters/examples': 15872, 'counters/updates': 248}
skipping logging after 15936 examples to avoid logging too frequently
train stats after 16000 examples: {'rewards_train/chosen': '-0.28966', 'rewards_train/rejected': '-0.7121', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.42244', 'logps_train/rejected': '-143.1', 'logps_train/chosen': '-152.63', 'loss/train': '0.56764', 'examples_per_second': '90.545', 'grad_norm': '16.451', 'counters/examples': 16000, 'counters/updates': 250}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16128 examples: {'rewards_train/chosen': '-0.39451', 'rewards_train/rejected': '-0.73651', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.342', 'logps_train/rejected': '-151.64', 'logps_train/chosen': '-159.92', 'loss/train': '0.65501', 'examples_per_second': '90.517', 'grad_norm': '18.465', 'counters/examples': 16128, 'counters/updates': 252}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16256 examples: {'rewards_train/chosen': '-0.47789', 'rewards_train/rejected': '-0.80438', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.32649', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-138.17', 'loss/train': '0.60334', 'examples_per_second': '91.237', 'grad_norm': '16.541', 'counters/examples': 16256, 'counters/updates': 254}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16384 examples: {'rewards_train/chosen': '-0.1113', 'rewards_train/rejected': '-0.43325', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.32194', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-146.57', 'loss/train': '0.59632', 'examples_per_second': '90.185', 'grad_norm': '16.271', 'counters/examples': 16384, 'counters/updates': 256}
skipping logging after 16448 examples to avoid logging too frequently
train stats after 16512 examples: {'rewards_train/chosen': '-0.35966', 'rewards_train/rejected': '-0.54311', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18345', 'logps_train/rejected': '-105.98', 'logps_train/chosen': '-127.79', 'loss/train': '0.70013', 'examples_per_second': '90.536', 'grad_norm': '17.652', 'counters/examples': 16512, 'counters/updates': 258}
skipping logging after 16576 examples to avoid logging too frequently
train stats after 16640 examples: {'rewards_train/chosen': '-0.24026', 'rewards_train/rejected': '-0.58256', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.3423', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-138.74', 'loss/train': '0.59741', 'examples_per_second': '87.071', 'grad_norm': '15.339', 'counters/examples': 16640, 'counters/updates': 260}
skipping logging after 16704 examples to avoid logging too frequently
train stats after 16768 examples: {'rewards_train/chosen': '-0.21428', 'rewards_train/rejected': '-0.50127', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.28699', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-132.59', 'loss/train': '0.63603', 'examples_per_second': '90.244', 'grad_norm': '15.813', 'counters/examples': 16768, 'counters/updates': 262}
skipping logging after 16832 examples to avoid logging too frequently
train stats after 16896 examples: {'rewards_train/chosen': '-0.20748', 'rewards_train/rejected': '-0.62984', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42236', 'logps_train/rejected': '-170.09', 'logps_train/chosen': '-153.27', 'loss/train': '0.57316', 'examples_per_second': '90.497', 'grad_norm': '16.144', 'counters/examples': 16896, 'counters/updates': 264}
skipping logging after 16960 examples to avoid logging too frequently
train stats after 17024 examples: {'rewards_train/chosen': '-0.55632', 'rewards_train/rejected': '-0.74208', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.18576', 'logps_train/rejected': '-171.43', 'logps_train/chosen': '-145.14', 'loss/train': '0.65947', 'examples_per_second': '90.532', 'grad_norm': '17.658', 'counters/examples': 17024, 'counters/updates': 266}
skipping logging after 17088 examples to avoid logging too frequently
train stats after 17152 examples: {'rewards_train/chosen': '-0.26527', 'rewards_train/rejected': '-0.56726', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30199', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-145.07', 'loss/train': '0.6211', 'examples_per_second': '90.199', 'grad_norm': '15.23', 'counters/examples': 17152, 'counters/updates': 268}
skipping logging after 17216 examples to avoid logging too frequently
train stats after 17280 examples: {'rewards_train/chosen': '-0.42375', 'rewards_train/rejected': '-0.77346', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.3497', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-143.62', 'loss/train': '0.61316', 'examples_per_second': '90.602', 'grad_norm': '16.475', 'counters/examples': 17280, 'counters/updates': 270}
skipping logging after 17344 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.41429', 'rewards_train/rejected': '-0.6732', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.25891', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-158.07', 'loss/train': '0.66554', 'examples_per_second': '88.476', 'grad_norm': '17.666', 'counters/examples': 17408, 'counters/updates': 272}
skipping logging after 17472 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '-0.37551', 'rewards_train/rejected': '-0.66443', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28892', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-130.38', 'loss/train': '0.61929', 'examples_per_second': '93.247', 'grad_norm': '15.378', 'counters/examples': 17536, 'counters/updates': 274}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17664 examples: {'rewards_train/chosen': '-0.41979', 'rewards_train/rejected': '-1.1022', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.68246', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-128.01', 'loss/train': '0.50892', 'examples_per_second': '87.93', 'grad_norm': '14.56', 'counters/examples': 17664, 'counters/updates': 276}
skipping logging after 17728 examples to avoid logging too frequently
train stats after 17792 examples: {'rewards_train/chosen': '-0.52655', 'rewards_train/rejected': '-0.9138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38725', 'logps_train/rejected': '-114', 'logps_train/chosen': '-107.88', 'loss/train': '0.60575', 'examples_per_second': '92.998', 'grad_norm': '14.49', 'counters/examples': 17792, 'counters/updates': 278}
skipping logging after 17856 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '-0.31825', 'rewards_train/rejected': '-0.78466', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46641', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-124.37', 'loss/train': '0.57925', 'examples_per_second': '100.16', 'grad_norm': '15.197', 'counters/examples': 17920, 'counters/updates': 280}
skipping logging after 17984 examples to avoid logging too frequently
train stats after 18048 examples: {'rewards_train/chosen': '-0.26526', 'rewards_train/rejected': '-0.71428', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.44903', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-144.31', 'loss/train': '0.57166', 'examples_per_second': '90.387', 'grad_norm': '15.149', 'counters/examples': 18048, 'counters/updates': 282}
skipping logging after 18112 examples to avoid logging too frequently
train stats after 18176 examples: {'rewards_train/chosen': '-0.336', 'rewards_train/rejected': '-0.78551', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.44952', 'logps_train/rejected': '-116.39', 'logps_train/chosen': '-142.73', 'loss/train': '0.59396', 'examples_per_second': '87.131', 'grad_norm': '15.818', 'counters/examples': 18176, 'counters/updates': 284}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18304 examples: {'rewards_train/chosen': '-0.29114', 'rewards_train/rejected': '-0.88289', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59175', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-170.7', 'loss/train': '0.50583', 'examples_per_second': '89.023', 'grad_norm': '14.844', 'counters/examples': 18304, 'counters/updates': 286}
skipping logging after 18368 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '-0.42837', 'rewards_train/rejected': '-0.79795', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.36958', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-137.23', 'loss/train': '0.63091', 'examples_per_second': '87.766', 'grad_norm': '16.326', 'counters/examples': 18432, 'counters/updates': 288}
skipping logging after 18496 examples to avoid logging too frequently
train stats after 18560 examples: {'rewards_train/chosen': '-0.27108', 'rewards_train/rejected': '-0.67131', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.40023', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-166.29', 'loss/train': '0.58976', 'examples_per_second': '90.582', 'grad_norm': '16.037', 'counters/examples': 18560, 'counters/updates': 290}
skipping logging after 18624 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.15457', 'rewards_train/rejected': '-0.47597', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3214', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-138.04', 'loss/train': '0.62328', 'examples_per_second': '91.427', 'grad_norm': '14.93', 'counters/examples': 18688, 'counters/updates': 292}
skipping logging after 18752 examples to avoid logging too frequently
train stats after 18816 examples: {'rewards_train/chosen': '-0.34137', 'rewards_train/rejected': '-0.49501', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.15364', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-147.59', 'loss/train': '0.68412', 'examples_per_second': '90.253', 'grad_norm': '17.101', 'counters/examples': 18816, 'counters/updates': 294}
skipping logging after 18880 examples to avoid logging too frequently
train stats after 18944 examples: {'rewards_train/chosen': '-0.43103', 'rewards_train/rejected': '-0.66203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.231', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-138.66', 'loss/train': '0.64163', 'examples_per_second': '90.358', 'grad_norm': '16.728', 'counters/examples': 18944, 'counters/updates': 296}
skipping logging after 19008 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '-0.26619', 'rewards_train/rejected': '-0.60977', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34357', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-146.23', 'loss/train': '0.60892', 'examples_per_second': '87.151', 'grad_norm': '15.8', 'counters/examples': 19072, 'counters/updates': 298}
skipping logging after 19136 examples to avoid logging too frequently
train stats after 19200 examples: {'rewards_train/chosen': '-0.30876', 'rewards_train/rejected': '-0.68509', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37632', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-164.11', 'loss/train': '0.60017', 'examples_per_second': '86.778', 'grad_norm': '15.909', 'counters/examples': 19200, 'counters/updates': 300}
skipping logging after 19264 examples to avoid logging too frequently
train stats after 19328 examples: {'rewards_train/chosen': '-0.34544', 'rewards_train/rejected': '-0.48248', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.13704', 'logps_train/rejected': '-131.14', 'logps_train/chosen': '-159.01', 'loss/train': '0.68872', 'examples_per_second': '90.515', 'grad_norm': '18.126', 'counters/examples': 19328, 'counters/updates': 302}
skipping logging after 19392 examples to avoid logging too frequently
train stats after 19456 examples: {'rewards_train/chosen': '-0.37115', 'rewards_train/rejected': '-0.79013', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41898', 'logps_train/rejected': '-141.49', 'logps_train/chosen': '-170.03', 'loss/train': '0.57307', 'examples_per_second': '88.241', 'grad_norm': '15.912', 'counters/examples': 19456, 'counters/updates': 304}
skipping logging after 19520 examples to avoid logging too frequently
train stats after 19584 examples: {'rewards_train/chosen': '-0.63056', 'rewards_train/rejected': '-0.85212', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22155', 'logps_train/rejected': '-158.4', 'logps_train/chosen': '-162.07', 'loss/train': '0.63546', 'examples_per_second': '91.056', 'grad_norm': '17.532', 'counters/examples': 19584, 'counters/updates': 306}
skipping logging after 19648 examples to avoid logging too frequently
train stats after 19712 examples: {'rewards_train/chosen': '-0.36313', 'rewards_train/rejected': '-0.71666', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.35353', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-141.51', 'loss/train': '0.64757', 'examples_per_second': '90.15', 'grad_norm': '17.288', 'counters/examples': 19712, 'counters/updates': 308}
skipping logging after 19776 examples to avoid logging too frequently
train stats after 19840 examples: {'rewards_train/chosen': '-0.40749', 'rewards_train/rejected': '-0.70427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29678', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-153.34', 'loss/train': '0.6295', 'examples_per_second': '90.279', 'grad_norm': '16.753', 'counters/examples': 19840, 'counters/updates': 310}
skipping logging after 19904 examples to avoid logging too frequently
train stats after 19968 examples: {'rewards_train/chosen': '-0.3475', 'rewards_train/rejected': '-0.75902', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.41152', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-164.55', 'loss/train': '0.56049', 'examples_per_second': '92.963', 'grad_norm': '15.19', 'counters/examples': 19968, 'counters/updates': 312}
skipping logging after 20032 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '-0.3774', 'rewards_train/rejected': '-0.75388', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37647', 'logps_train/rejected': '-144.1', 'logps_train/chosen': '-158.26', 'loss/train': '0.62735', 'examples_per_second': '103.53', 'grad_norm': '16.644', 'counters/examples': 20096, 'counters/updates': 314}
skipping logging after 20160 examples to avoid logging too frequently
train stats after 20224 examples: {'rewards_train/chosen': '-0.23272', 'rewards_train/rejected': '-0.82237', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.58965', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-165.46', 'loss/train': '0.52335', 'examples_per_second': '89.917', 'grad_norm': '15.634', 'counters/examples': 20224, 'counters/updates': 316}
skipping logging after 20288 examples to avoid logging too frequently
train stats after 20352 examples: {'rewards_train/chosen': '-0.51657', 'rewards_train/rejected': '-0.95056', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.43399', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-142.27', 'loss/train': '0.58029', 'examples_per_second': '90.309', 'grad_norm': '14.616', 'counters/examples': 20352, 'counters/updates': 318}
skipping logging after 20416 examples to avoid logging too frequently
train stats after 20480 examples: {'rewards_train/chosen': '-0.31919', 'rewards_train/rejected': '-0.78198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46279', 'logps_train/rejected': '-173.28', 'logps_train/chosen': '-148.54', 'loss/train': '0.57109', 'examples_per_second': '90.179', 'grad_norm': '16.856', 'counters/examples': 20480, 'counters/updates': 320}
skipping logging after 20544 examples to avoid logging too frequently
train stats after 20608 examples: {'rewards_train/chosen': '-0.28677', 'rewards_train/rejected': '-0.73704', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45027', 'logps_train/rejected': '-143.11', 'logps_train/chosen': '-181.83', 'loss/train': '0.60138', 'examples_per_second': '90.063', 'grad_norm': '18.351', 'counters/examples': 20608, 'counters/updates': 322}
skipping logging after 20672 examples to avoid logging too frequently
train stats after 20736 examples: {'rewards_train/chosen': '-0.23593', 'rewards_train/rejected': '-0.61895', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.38302', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-176.25', 'loss/train': '0.59944', 'examples_per_second': '90.189', 'grad_norm': '16.685', 'counters/examples': 20736, 'counters/updates': 324}
skipping logging after 20800 examples to avoid logging too frequently
train stats after 20864 examples: {'rewards_train/chosen': '-0.37538', 'rewards_train/rejected': '-0.7666', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.39121', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-147.32', 'loss/train': '0.57954', 'examples_per_second': '86.985', 'grad_norm': '14.588', 'counters/examples': 20864, 'counters/updates': 326}
skipping logging after 20928 examples to avoid logging too frequently
train stats after 20992 examples: {'rewards_train/chosen': '-0.50081', 'rewards_train/rejected': '-0.99061', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4898', 'logps_train/rejected': '-135.48', 'logps_train/chosen': '-156.37', 'loss/train': '0.55477', 'examples_per_second': '90.059', 'grad_norm': '15.138', 'counters/examples': 20992, 'counters/updates': 328}
skipping logging after 21056 examples to avoid logging too frequently
train stats after 21120 examples: {'rewards_train/chosen': '-0.71726', 'rewards_train/rejected': '-0.99883', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.28158', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-138.02', 'loss/train': '0.64646', 'examples_per_second': '89.964', 'grad_norm': '17.045', 'counters/examples': 21120, 'counters/updates': 330}
skipping logging after 21184 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '-0.25176', 'rewards_train/rejected': '-0.61475', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.36298', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-124.62', 'loss/train': '0.58253', 'examples_per_second': '93.633', 'grad_norm': '14.238', 'counters/examples': 21248, 'counters/updates': 332}
skipping logging after 21312 examples to avoid logging too frequently
train stats after 21376 examples: {'rewards_train/chosen': '-0.20175', 'rewards_train/rejected': '-0.6822', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48045', 'logps_train/rejected': '-155.23', 'logps_train/chosen': '-137.58', 'loss/train': '0.54409', 'examples_per_second': '91.217', 'grad_norm': '14.316', 'counters/examples': 21376, 'counters/updates': 334}
skipping logging after 21440 examples to avoid logging too frequently
train stats after 21504 examples: {'rewards_train/chosen': '-0.076763', 'rewards_train/rejected': '-0.73637', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.65961', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-153.81', 'loss/train': '0.51963', 'examples_per_second': '90.177', 'grad_norm': '15.378', 'counters/examples': 21504, 'counters/updates': 336}
skipping logging after 21568 examples to avoid logging too frequently
train stats after 21632 examples: {'rewards_train/chosen': '-0.077858', 'rewards_train/rejected': '-0.43052', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35266', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-132.43', 'loss/train': '0.59016', 'examples_per_second': '90.183', 'grad_norm': '15.325', 'counters/examples': 21632, 'counters/updates': 338}
skipping logging after 21696 examples to avoid logging too frequently
train stats after 21760 examples: {'rewards_train/chosen': '-0.2138', 'rewards_train/rejected': '-0.61143', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.39763', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-167.67', 'loss/train': '0.63075', 'examples_per_second': '91.231', 'grad_norm': '17.468', 'counters/examples': 21760, 'counters/updates': 340}
skipping logging after 21824 examples to avoid logging too frequently
train stats after 21888 examples: {'rewards_train/chosen': '-0.39044', 'rewards_train/rejected': '-0.6368', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.24635', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-133.3', 'loss/train': '0.64928', 'examples_per_second': '92.691', 'grad_norm': '15.99', 'counters/examples': 21888, 'counters/updates': 342}
skipping logging after 21952 examples to avoid logging too frequently
train stats after 22016 examples: {'rewards_train/chosen': '-0.39934', 'rewards_train/rejected': '-0.76711', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.36777', 'logps_train/rejected': '-156.39', 'logps_train/chosen': '-137.23', 'loss/train': '0.61968', 'examples_per_second': '90.266', 'grad_norm': '17.058', 'counters/examples': 22016, 'counters/updates': 344}
skipping logging after 22080 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.23675', 'rewards_train/rejected': '-0.66834', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43159', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-127.02', 'loss/train': '0.57423', 'examples_per_second': '88.608', 'grad_norm': '15.12', 'counters/examples': 22144, 'counters/updates': 346}
skipping logging after 22208 examples to avoid logging too frequently
train stats after 22272 examples: {'rewards_train/chosen': '-0.24265', 'rewards_train/rejected': '-0.71671', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47406', 'logps_train/rejected': '-117.71', 'logps_train/chosen': '-129.53', 'loss/train': '0.55651', 'examples_per_second': '87.142', 'grad_norm': '14.108', 'counters/examples': 22272, 'counters/updates': 348}
skipping logging after 22336 examples to avoid logging too frequently
train stats after 22400 examples: {'rewards_train/chosen': '-0.27301', 'rewards_train/rejected': '-0.69335', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42034', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-155.73', 'loss/train': '0.58385', 'examples_per_second': '90.387', 'grad_norm': '15.266', 'counters/examples': 22400, 'counters/updates': 350}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22528 examples: {'rewards_train/chosen': '-0.54395', 'rewards_train/rejected': '-0.85744', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.31349', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-160.56', 'loss/train': '0.63797', 'examples_per_second': '90.165', 'grad_norm': '18.112', 'counters/examples': 22528, 'counters/updates': 352}
skipping logging after 22592 examples to avoid logging too frequently
train stats after 22656 examples: {'rewards_train/chosen': '-0.52639', 'rewards_train/rejected': '-0.86509', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.3387', 'logps_train/rejected': '-130.9', 'logps_train/chosen': '-152.26', 'loss/train': '0.64724', 'examples_per_second': '90.075', 'grad_norm': '18.034', 'counters/examples': 22656, 'counters/updates': 354}
skipping logging after 22720 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '-0.44991', 'rewards_train/rejected': '-0.88276', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43285', 'logps_train/rejected': '-138.28', 'logps_train/chosen': '-142.53', 'loss/train': '0.56231', 'examples_per_second': '94.621', 'grad_norm': '15.464', 'counters/examples': 22784, 'counters/updates': 356}
skipping logging after 22848 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.41848', 'rewards_train/rejected': '-0.7707', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35222', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-148.32', 'loss/train': '0.62804', 'examples_per_second': '103.27', 'grad_norm': '17.768', 'counters/examples': 22912, 'counters/updates': 358}
skipping logging after 22976 examples to avoid logging too frequently
train stats after 23040 examples: {'rewards_train/chosen': '-0.24532', 'rewards_train/rejected': '-0.73124', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.48592', 'logps_train/rejected': '-124.66', 'logps_train/chosen': '-127.71', 'loss/train': '0.52887', 'examples_per_second': '87.612', 'grad_norm': '13.753', 'counters/examples': 23040, 'counters/updates': 360}
skipping logging after 23104 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '-0.42782', 'rewards_train/rejected': '-0.86996', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.44214', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-163.52', 'loss/train': '0.5943', 'examples_per_second': '90.429', 'grad_norm': '19.068', 'counters/examples': 23168, 'counters/updates': 362}
skipping logging after 23232 examples to avoid logging too frequently
train stats after 23296 examples: {'rewards_train/chosen': '-0.40499', 'rewards_train/rejected': '-0.73251', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32752', 'logps_train/rejected': '-150.08', 'logps_train/chosen': '-159.58', 'loss/train': '0.62133', 'examples_per_second': '89.642', 'grad_norm': '17.181', 'counters/examples': 23296, 'counters/updates': 364}
skipping logging after 23360 examples to avoid logging too frequently
train stats after 23424 examples: {'rewards_train/chosen': '-0.49122', 'rewards_train/rejected': '-0.66718', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17596', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-137.06', 'loss/train': '0.6669', 'examples_per_second': '90.091', 'grad_norm': '17.152', 'counters/examples': 23424, 'counters/updates': 366}
skipping logging after 23488 examples to avoid logging too frequently
train stats after 23552 examples: {'rewards_train/chosen': '-0.42128', 'rewards_train/rejected': '-0.91866', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49738', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-133.23', 'loss/train': '0.58129', 'examples_per_second': '90.092', 'grad_norm': '16.36', 'counters/examples': 23552, 'counters/updates': 368}
skipping logging after 23616 examples to avoid logging too frequently
train stats after 23680 examples: {'rewards_train/chosen': '-0.64512', 'rewards_train/rejected': '-1.002', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.35688', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-137.64', 'loss/train': '0.61688', 'examples_per_second': '86.75', 'grad_norm': '16.633', 'counters/examples': 23680, 'counters/updates': 370}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23808 examples: {'rewards_train/chosen': '-0.36001', 'rewards_train/rejected': '-0.64005', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28004', 'logps_train/rejected': '-146.89', 'logps_train/chosen': '-142.66', 'loss/train': '0.64544', 'examples_per_second': '89.039', 'grad_norm': '17.849', 'counters/examples': 23808, 'counters/updates': 372}
skipping logging after 23872 examples to avoid logging too frequently
train stats after 23936 examples: {'rewards_train/chosen': '-0.56198', 'rewards_train/rejected': '-0.75332', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.19133', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-135.92', 'loss/train': '0.65942', 'examples_per_second': '90.317', 'grad_norm': '17.314', 'counters/examples': 23936, 'counters/updates': 374}
Running evaluation after 23936 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  9.83it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01,  9.88it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.26it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.61it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.52it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.54it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.56it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.48it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.46it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.45it/s]
eval after 23936: {'rewards_eval/chosen': '-0.46657', 'rewards_eval/rejected': '-0.82315', 'rewards_eval/accuracies': '0.63281', 'rewards_eval/margins': '0.35659', 'logps_eval/rejected': '-129.38', 'logps_eval/chosen': '-145.47', 'loss/eval': '0.63744'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-23936...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-23936/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-23936/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-23936/scheduler.pt...
train stats after 24000 examples: {'rewards_train/chosen': '-0.44511', 'rewards_train/rejected': '-0.73615', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.29104', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-139.71', 'loss/train': '0.65301', 'examples_per_second': '69.696', 'grad_norm': '16.92', 'counters/examples': 24000, 'counters/updates': 375}
skipping logging after 24064 examples to avoid logging too frequently
train stats after 24128 examples: {'rewards_train/chosen': '-0.47628', 'rewards_train/rejected': '-0.66331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18704', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-147.77', 'loss/train': '0.71194', 'examples_per_second': '89.859', 'grad_norm': '19.459', 'counters/examples': 24128, 'counters/updates': 377}
skipping logging after 24192 examples to avoid logging too frequently
train stats after 24256 examples: {'rewards_train/chosen': '-0.49251', 'rewards_train/rejected': '-0.68614', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19363', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-176.3', 'loss/train': '0.66542', 'examples_per_second': '90.877', 'grad_norm': '18.831', 'counters/examples': 24256, 'counters/updates': 379}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24384 examples: {'rewards_train/chosen': '-0.42852', 'rewards_train/rejected': '-0.90115', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47263', 'logps_train/rejected': '-137.45', 'logps_train/chosen': '-146.87', 'loss/train': '0.57224', 'examples_per_second': '89.419', 'grad_norm': '15.341', 'counters/examples': 24384, 'counters/updates': 381}
skipping logging after 24448 examples to avoid logging too frequently
train stats after 24512 examples: {'rewards_train/chosen': '-0.17058', 'rewards_train/rejected': '-0.63129', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46071', 'logps_train/rejected': '-113.51', 'logps_train/chosen': '-141.87', 'loss/train': '0.57871', 'examples_per_second': '87.773', 'grad_norm': '14.7', 'counters/examples': 24512, 'counters/updates': 383}
skipping logging after 24576 examples to avoid logging too frequently
train stats after 24640 examples: {'rewards_train/chosen': '-0.18487', 'rewards_train/rejected': '-0.69202', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50715', 'logps_train/rejected': '-117.85', 'logps_train/chosen': '-148.6', 'loss/train': '0.55528', 'examples_per_second': '94.682', 'grad_norm': '15.352', 'counters/examples': 24640, 'counters/updates': 385}
skipping logging after 24704 examples to avoid logging too frequently
train stats after 24768 examples: {'rewards_train/chosen': '-0.30818', 'rewards_train/rejected': '-0.87124', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.56305', 'logps_train/rejected': '-150.63', 'logps_train/chosen': '-138.98', 'loss/train': '0.53969', 'examples_per_second': '93.343', 'grad_norm': '15.025', 'counters/examples': 24768, 'counters/updates': 387}
skipping logging after 24832 examples to avoid logging too frequently
train stats after 24896 examples: {'rewards_train/chosen': '-0.38952', 'rewards_train/rejected': '-0.72587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.33635', 'logps_train/rejected': '-172.08', 'logps_train/chosen': '-161.24', 'loss/train': '0.63447', 'examples_per_second': '88.191', 'grad_norm': '17.839', 'counters/examples': 24896, 'counters/updates': 389}
skipping logging after 24960 examples to avoid logging too frequently
train stats after 25024 examples: {'rewards_train/chosen': '-0.29953', 'rewards_train/rejected': '-0.83595', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.53642', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-149.43', 'loss/train': '0.54573', 'examples_per_second': '88.4', 'grad_norm': '16.33', 'counters/examples': 25024, 'counters/updates': 391}
skipping logging after 25088 examples to avoid logging too frequently
train stats after 25152 examples: {'rewards_train/chosen': '-0.24977', 'rewards_train/rejected': '-0.74526', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49549', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-131.73', 'loss/train': '0.5461', 'examples_per_second': '90.379', 'grad_norm': '15.035', 'counters/examples': 25152, 'counters/updates': 393}
skipping logging after 25216 examples to avoid logging too frequently
train stats after 25280 examples: {'rewards_train/chosen': '-0.25645', 'rewards_train/rejected': '-0.72026', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46381', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-125.12', 'loss/train': '0.59498', 'examples_per_second': '90.603', 'grad_norm': '14.659', 'counters/examples': 25280, 'counters/updates': 395}
skipping logging after 25344 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '-0.41184', 'rewards_train/rejected': '-0.71846', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.30663', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-133.85', 'loss/train': '0.65237', 'examples_per_second': '90.649', 'grad_norm': '16.884', 'counters/examples': 25408, 'counters/updates': 397}
skipping logging after 25472 examples to avoid logging too frequently
train stats after 25536 examples: {'rewards_train/chosen': '-0.54059', 'rewards_train/rejected': '-0.8718', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33121', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-139.08', 'loss/train': '0.63097', 'examples_per_second': '91.07', 'grad_norm': '17.095', 'counters/examples': 25536, 'counters/updates': 399}
skipping logging after 25600 examples to avoid logging too frequently
train stats after 25664 examples: {'rewards_train/chosen': '-0.31069', 'rewards_train/rejected': '-0.96817', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.65748', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-135.02', 'loss/train': '0.48952', 'examples_per_second': '88.876', 'grad_norm': '15.202', 'counters/examples': 25664, 'counters/updates': 401}
skipping logging after 25728 examples to avoid logging too frequently
train stats after 25792 examples: {'rewards_train/chosen': '-0.51719', 'rewards_train/rejected': '-1.1758', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.65863', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-167.92', 'loss/train': '0.53107', 'examples_per_second': '88.753', 'grad_norm': '16.593', 'counters/examples': 25792, 'counters/updates': 403}
skipping logging after 25856 examples to avoid logging too frequently
train stats after 25920 examples: {'rewards_train/chosen': '-0.80158', 'rewards_train/rejected': '-1.1136', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31198', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-168.47', 'loss/train': '0.6856', 'examples_per_second': '90.412', 'grad_norm': '21.011', 'counters/examples': 25920, 'counters/updates': 405}
skipping logging after 25984 examples to avoid logging too frequently
train stats after 26048 examples: {'rewards_train/chosen': '-0.60122', 'rewards_train/rejected': '-1.0581', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45686', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-169.66', 'loss/train': '0.59545', 'examples_per_second': '88.242', 'grad_norm': '17.872', 'counters/examples': 26048, 'counters/updates': 407}
skipping logging after 26112 examples to avoid logging too frequently
train stats after 26176 examples: {'rewards_train/chosen': '-0.59165', 'rewards_train/rejected': '-0.96615', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37451', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-131.3', 'loss/train': '0.62589', 'examples_per_second': '87.837', 'grad_norm': '17.415', 'counters/examples': 26176, 'counters/updates': 409}
skipping logging after 26240 examples to avoid logging too frequently
train stats after 26304 examples: {'rewards_train/chosen': '-0.28729', 'rewards_train/rejected': '-0.7498', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46251', 'logps_train/rejected': '-118.97', 'logps_train/chosen': '-153.37', 'loss/train': '0.668', 'examples_per_second': '94.471', 'grad_norm': '17.901', 'counters/examples': 26304, 'counters/updates': 411}
skipping logging after 26368 examples to avoid logging too frequently
train stats after 26432 examples: {'rewards_train/chosen': '-0.32182', 'rewards_train/rejected': '-0.83917', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.51734', 'logps_train/rejected': '-172.99', 'logps_train/chosen': '-154.8', 'loss/train': '0.55269', 'examples_per_second': '88.19', 'grad_norm': '16.475', 'counters/examples': 26432, 'counters/updates': 413}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26560 examples: {'rewards_train/chosen': '-0.35751', 'rewards_train/rejected': '-0.61457', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.25706', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-126.7', 'loss/train': '0.6322', 'examples_per_second': '93.837', 'grad_norm': '14.772', 'counters/examples': 26560, 'counters/updates': 415}
skipping logging after 26624 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '-0.1027', 'rewards_train/rejected': '-0.58371', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48101', 'logps_train/rejected': '-156.47', 'logps_train/chosen': '-172.72', 'loss/train': '0.5565', 'examples_per_second': '90.669', 'grad_norm': '17.075', 'counters/examples': 26688, 'counters/updates': 417}
skipping logging after 26752 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.32235', 'rewards_train/rejected': '-0.81018', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48783', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-160.38', 'loss/train': '0.58195', 'examples_per_second': '90.704', 'grad_norm': '16.949', 'counters/examples': 26816, 'counters/updates': 419}
skipping logging after 26880 examples to avoid logging too frequently
train stats after 26944 examples: {'rewards_train/chosen': '-0.52966', 'rewards_train/rejected': '-0.9534', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.42374', 'logps_train/rejected': '-173.28', 'logps_train/chosen': '-155.38', 'loss/train': '0.65973', 'examples_per_second': '93.889', 'grad_norm': '19.109', 'counters/examples': 26944, 'counters/updates': 421}
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27072 examples: {'rewards_train/chosen': '-0.46712', 'rewards_train/rejected': '-0.74336', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.27625', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-144.76', 'loss/train': '0.64551', 'examples_per_second': '89.524', 'grad_norm': '16.468', 'counters/examples': 27072, 'counters/updates': 423}
skipping logging after 27136 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '-0.55281', 'rewards_train/rejected': '-0.86608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31327', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-124.07', 'loss/train': '0.63785', 'examples_per_second': '87.281', 'grad_norm': '16.267', 'counters/examples': 27200, 'counters/updates': 425}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27328 examples: {'rewards_train/chosen': '-0.10063', 'rewards_train/rejected': '-0.5789', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.47827', 'logps_train/rejected': '-158.67', 'logps_train/chosen': '-128.15', 'loss/train': '0.55038', 'examples_per_second': '90.645', 'grad_norm': '15.82', 'counters/examples': 27328, 'counters/updates': 427}
skipping logging after 27392 examples to avoid logging too frequently
train stats after 27456 examples: {'rewards_train/chosen': '-0.29331', 'rewards_train/rejected': '-0.53147', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.23816', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-178.96', 'loss/train': '0.66377', 'examples_per_second': '90.177', 'grad_norm': '18.523', 'counters/examples': 27456, 'counters/updates': 429}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27584 examples: {'rewards_train/chosen': '-0.33098', 'rewards_train/rejected': '-0.68879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35781', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-144.56', 'loss/train': '0.61105', 'examples_per_second': '88.23', 'grad_norm': '16.594', 'counters/examples': 27584, 'counters/updates': 431}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27712 examples: {'rewards_train/chosen': '-0.33512', 'rewards_train/rejected': '-0.67576', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34064', 'logps_train/rejected': '-131.6', 'logps_train/chosen': '-138.06', 'loss/train': '0.61949', 'examples_per_second': '88.443', 'grad_norm': '16.773', 'counters/examples': 27712, 'counters/updates': 433}
skipping logging after 27776 examples to avoid logging too frequently
train stats after 27840 examples: {'rewards_train/chosen': '-0.3743', 'rewards_train/rejected': '-0.88951', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.51521', 'logps_train/rejected': '-126.44', 'logps_train/chosen': '-136.08', 'loss/train': '0.54359', 'examples_per_second': '90.312', 'grad_norm': '14.606', 'counters/examples': 27840, 'counters/updates': 435}
skipping logging after 27904 examples to avoid logging too frequently
train stats after 27968 examples: {'rewards_train/chosen': '-0.36744', 'rewards_train/rejected': '-0.58708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21964', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-129.37', 'loss/train': '0.66421', 'examples_per_second': '90.597', 'grad_norm': '16.355', 'counters/examples': 27968, 'counters/updates': 437}
skipping logging after 28032 examples to avoid logging too frequently
train stats after 28096 examples: {'rewards_train/chosen': '-0.39377', 'rewards_train/rejected': '-0.77426', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38049', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-169.2', 'loss/train': '0.60218', 'examples_per_second': '90.378', 'grad_norm': '16.673', 'counters/examples': 28096, 'counters/updates': 439}
skipping logging after 28160 examples to avoid logging too frequently
train stats after 28224 examples: {'rewards_train/chosen': '-0.31815', 'rewards_train/rejected': '-0.74514', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.427', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-142.75', 'loss/train': '0.59525', 'examples_per_second': '87.287', 'grad_norm': '16.139', 'counters/examples': 28224, 'counters/updates': 441}
skipping logging after 28288 examples to avoid logging too frequently
train stats after 28352 examples: {'rewards_train/chosen': '-0.21078', 'rewards_train/rejected': '-0.59434', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38356', 'logps_train/rejected': '-143.03', 'logps_train/chosen': '-131.42', 'loss/train': '0.58559', 'examples_per_second': '96.272', 'grad_norm': '14.756', 'counters/examples': 28352, 'counters/updates': 443}
skipping logging after 28416 examples to avoid logging too frequently
train stats after 28480 examples: {'rewards_train/chosen': '-0.19489', 'rewards_train/rejected': '-0.47998', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28509', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-146.05', 'loss/train': '0.64218', 'examples_per_second': '90.674', 'grad_norm': '16.859', 'counters/examples': 28480, 'counters/updates': 445}
skipping logging after 28544 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.34861', 'rewards_train/rejected': '-0.90412', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55551', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-163.53', 'loss/train': '0.568', 'examples_per_second': '96.095', 'grad_norm': '15.814', 'counters/examples': 28608, 'counters/updates': 447}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28736 examples: {'rewards_train/chosen': '-0.33783', 'rewards_train/rejected': '-0.96242', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.62459', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-159.29', 'loss/train': '0.53571', 'examples_per_second': '90.288', 'grad_norm': '16.306', 'counters/examples': 28736, 'counters/updates': 449}
skipping logging after 28800 examples to avoid logging too frequently
train stats after 28864 examples: {'rewards_train/chosen': '-0.35335', 'rewards_train/rejected': '-0.93625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5829', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-124.01', 'loss/train': '0.55151', 'examples_per_second': '90.143', 'grad_norm': '14.489', 'counters/examples': 28864, 'counters/updates': 451}
skipping logging after 28928 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.32209', 'rewards_train/rejected': '-0.9323', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.61021', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-125.2', 'loss/train': '0.53714', 'examples_per_second': '92.63', 'grad_norm': '14.255', 'counters/examples': 28992, 'counters/updates': 453}
skipping logging after 29056 examples to avoid logging too frequently
train stats after 29120 examples: {'rewards_train/chosen': '-0.36444', 'rewards_train/rejected': '-0.8071', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44265', 'logps_train/rejected': '-124.5', 'logps_train/chosen': '-160.02', 'loss/train': '0.56858', 'examples_per_second': '87.244', 'grad_norm': '15.774', 'counters/examples': 29120, 'counters/updates': 455}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29248 examples: {'rewards_train/chosen': '-0.34489', 'rewards_train/rejected': '-0.94311', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.59821', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-152.22', 'loss/train': '0.53706', 'examples_per_second': '87.759', 'grad_norm': '15.223', 'counters/examples': 29248, 'counters/updates': 457}
skipping logging after 29312 examples to avoid logging too frequently
train stats after 29376 examples: {'rewards_train/chosen': '-0.75485', 'rewards_train/rejected': '-1.1052', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.35035', 'logps_train/rejected': '-128.13', 'logps_train/chosen': '-161.31', 'loss/train': '0.60947', 'examples_per_second': '87.486', 'grad_norm': '17.748', 'counters/examples': 29376, 'counters/updates': 459}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29504 examples: {'rewards_train/chosen': '-0.52537', 'rewards_train/rejected': '-1.0851', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55969', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-128.84', 'loss/train': '0.5814', 'examples_per_second': '90.281', 'grad_norm': '13.687', 'counters/examples': 29504, 'counters/updates': 461}
skipping logging after 29568 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '-0.46151', 'rewards_train/rejected': '-1.0005', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53903', 'logps_train/rejected': '-121.5', 'logps_train/chosen': '-132.64', 'loss/train': '0.55376', 'examples_per_second': '85.483', 'grad_norm': '15.082', 'counters/examples': 29632, 'counters/updates': 463}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29760 examples: {'rewards_train/chosen': '-0.53686', 'rewards_train/rejected': '-0.8627', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.32585', 'logps_train/rejected': '-155.16', 'logps_train/chosen': '-144.87', 'loss/train': '0.66194', 'examples_per_second': '90.267', 'grad_norm': '17.418', 'counters/examples': 29760, 'counters/updates': 465}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29888 examples: {'rewards_train/chosen': '-0.62456', 'rewards_train/rejected': '-0.96219', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33763', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-132.16', 'loss/train': '0.61416', 'examples_per_second': '100.09', 'grad_norm': '15.769', 'counters/examples': 29888, 'counters/updates': 467}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 30016 examples: {'rewards_train/chosen': '-0.57418', 'rewards_train/rejected': '-0.99395', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41977', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-130.58', 'loss/train': '0.59031', 'examples_per_second': '98.922', 'grad_norm': '14.16', 'counters/examples': 30016, 'counters/updates': 469}
skipping logging after 30080 examples to avoid logging too frequently
train stats after 30144 examples: {'rewards_train/chosen': '-0.71942', 'rewards_train/rejected': '-1.0334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31393', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-139.61', 'loss/train': '0.62253', 'examples_per_second': '89.326', 'grad_norm': '16.586', 'counters/examples': 30144, 'counters/updates': 471}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30272 examples: {'rewards_train/chosen': '-0.50522', 'rewards_train/rejected': '-0.91297', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.40775', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-151.34', 'loss/train': '0.60253', 'examples_per_second': '90.628', 'grad_norm': '17.617', 'counters/examples': 30272, 'counters/updates': 473}
skipping logging after 30336 examples to avoid logging too frequently
train stats after 30400 examples: {'rewards_train/chosen': '-0.41257', 'rewards_train/rejected': '-0.70707', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.2945', 'logps_train/rejected': '-149.28', 'logps_train/chosen': '-146.39', 'loss/train': '0.65908', 'examples_per_second': '90.802', 'grad_norm': '18.522', 'counters/examples': 30400, 'counters/updates': 475}
skipping logging after 30464 examples to avoid logging too frequently
train stats after 30528 examples: {'rewards_train/chosen': '-0.31497', 'rewards_train/rejected': '-0.83436', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5194', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-131.58', 'loss/train': '0.55307', 'examples_per_second': '90.096', 'grad_norm': '14.636', 'counters/examples': 30528, 'counters/updates': 477}
skipping logging after 30592 examples to avoid logging too frequently
train stats after 30656 examples: {'rewards_train/chosen': '-0.32655', 'rewards_train/rejected': '-0.91527', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58872', 'logps_train/rejected': '-150.43', 'logps_train/chosen': '-143.04', 'loss/train': '0.57094', 'examples_per_second': '87.906', 'grad_norm': '15.623', 'counters/examples': 30656, 'counters/updates': 479}
skipping logging after 30720 examples to avoid logging too frequently
train stats after 30784 examples: {'rewards_train/chosen': '-0.44075', 'rewards_train/rejected': '-1.036', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5953', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-131.44', 'loss/train': '0.53079', 'examples_per_second': '90.353', 'grad_norm': '13.318', 'counters/examples': 30784, 'counters/updates': 481}
skipping logging after 30848 examples to avoid logging too frequently
train stats after 30912 examples: {'rewards_train/chosen': '-0.43847', 'rewards_train/rejected': '-0.81746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37899', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-155.07', 'loss/train': '0.66162', 'examples_per_second': '90.641', 'grad_norm': '18.256', 'counters/examples': 30912, 'counters/updates': 483}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31040 examples: {'rewards_train/chosen': '-0.34727', 'rewards_train/rejected': '-0.6411', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29383', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-127.33', 'loss/train': '0.61799', 'examples_per_second': '90.551', 'grad_norm': '15.694', 'counters/examples': 31040, 'counters/updates': 485}
skipping logging after 31104 examples to avoid logging too frequently
train stats after 31168 examples: {'rewards_train/chosen': '-0.1948', 'rewards_train/rejected': '-0.63313', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43833', 'logps_train/rejected': '-151.05', 'logps_train/chosen': '-146.71', 'loss/train': '0.57771', 'examples_per_second': '89.981', 'grad_norm': '16.187', 'counters/examples': 31168, 'counters/updates': 487}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31296 examples: {'rewards_train/chosen': '-0.38585', 'rewards_train/rejected': '-0.87573', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48989', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-152.38', 'loss/train': '0.55714', 'examples_per_second': '88.958', 'grad_norm': '16.093', 'counters/examples': 31296, 'counters/updates': 489}
skipping logging after 31360 examples to avoid logging too frequently
train stats after 31424 examples: {'rewards_train/chosen': '-0.41959', 'rewards_train/rejected': '-1.0024', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58276', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-152.96', 'loss/train': '0.57257', 'examples_per_second': '87.672', 'grad_norm': '17.396', 'counters/examples': 31424, 'counters/updates': 491}
skipping logging after 31488 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '-0.41934', 'rewards_train/rejected': '-0.85656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43722', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-168.52', 'loss/train': '0.60534', 'examples_per_second': '90.458', 'grad_norm': '17.972', 'counters/examples': 31552, 'counters/updates': 493}
skipping logging after 31616 examples to avoid logging too frequently
train stats after 31680 examples: {'rewards_train/chosen': '-0.40877', 'rewards_train/rejected': '-0.64167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23291', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-155.17', 'loss/train': '0.66252', 'examples_per_second': '90.373', 'grad_norm': '17.365', 'counters/examples': 31680, 'counters/updates': 495}
skipping logging after 31744 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '-0.40065', 'rewards_train/rejected': '-0.57961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17896', 'logps_train/rejected': '-139.01', 'logps_train/chosen': '-158.29', 'loss/train': '0.68645', 'examples_per_second': '90.533', 'grad_norm': '18.683', 'counters/examples': 31808, 'counters/updates': 497}
skipping logging after 31872 examples to avoid logging too frequently
train stats after 31936 examples: {'rewards_train/chosen': '-0.54823', 'rewards_train/rejected': '-0.99155', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.44333', 'logps_train/rejected': '-148.81', 'logps_train/chosen': '-138.87', 'loss/train': '0.60549', 'examples_per_second': '87.838', 'grad_norm': '17.303', 'counters/examples': 31936, 'counters/updates': 499}
skipping logging after 32000 examples to avoid logging too frequently
train stats after 32064 examples: {'rewards_train/chosen': '-0.46359', 'rewards_train/rejected': '-0.81128', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34768', 'logps_train/rejected': '-137.63', 'logps_train/chosen': '-152.91', 'loss/train': '0.61621', 'examples_per_second': '89.676', 'grad_norm': '17.147', 'counters/examples': 32064, 'counters/updates': 501}
skipping logging after 32128 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '-0.44879', 'rewards_train/rejected': '-1.1068', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.65806', 'logps_train/rejected': '-142.27', 'logps_train/chosen': '-140.08', 'loss/train': '0.50356', 'examples_per_second': '87.573', 'grad_norm': '14.103', 'counters/examples': 32192, 'counters/updates': 503}
skipping logging after 32256 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.3354', 'rewards_train/rejected': '-0.82602', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.49061', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-144.28', 'loss/train': '0.54893', 'examples_per_second': '96.049', 'grad_norm': '15.69', 'counters/examples': 32320, 'counters/updates': 505}
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32448 examples: {'rewards_train/chosen': '-0.468', 'rewards_train/rejected': '-0.93542', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.46742', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-153.41', 'loss/train': '0.5481', 'examples_per_second': '90.462', 'grad_norm': '15.391', 'counters/examples': 32448, 'counters/updates': 507}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32576 examples: {'rewards_train/chosen': '-0.47214', 'rewards_train/rejected': '-1.004', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5319', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-142.61', 'loss/train': '0.561', 'examples_per_second': '90.178', 'grad_norm': '15.224', 'counters/examples': 32576, 'counters/updates': 509}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32704 examples: {'rewards_train/chosen': '-0.48779', 'rewards_train/rejected': '-0.76559', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.2778', 'logps_train/rejected': '-135.37', 'logps_train/chosen': '-169.27', 'loss/train': '0.63367', 'examples_per_second': '90.048', 'grad_norm': '17.729', 'counters/examples': 32704, 'counters/updates': 511}
skipping logging after 32768 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '-0.66105', 'rewards_train/rejected': '-1.258', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.59698', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-139.67', 'loss/train': '0.55895', 'examples_per_second': '90.389', 'grad_norm': '16.075', 'counters/examples': 32832, 'counters/updates': 513}
skipping logging after 32896 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.48558', 'rewards_train/rejected': '-0.81529', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.3297', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-152.58', 'loss/train': '0.62905', 'examples_per_second': '86.185', 'grad_norm': '17.648', 'counters/examples': 32960, 'counters/updates': 515}
skipping logging after 33024 examples to avoid logging too frequently
train stats after 33088 examples: {'rewards_train/chosen': '-0.70076', 'rewards_train/rejected': '-1.0931', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39233', 'logps_train/rejected': '-170.79', 'logps_train/chosen': '-166.83', 'loss/train': '0.59669', 'examples_per_second': '89.964', 'grad_norm': '17.318', 'counters/examples': 33088, 'counters/updates': 517}
skipping logging after 33152 examples to avoid logging too frequently
train stats after 33216 examples: {'rewards_train/chosen': '-0.48589', 'rewards_train/rejected': '-1.0156', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.52967', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-141.24', 'loss/train': '0.56597', 'examples_per_second': '90.092', 'grad_norm': '15.663', 'counters/examples': 33216, 'counters/updates': 519}
skipping logging after 33280 examples to avoid logging too frequently
train stats after 33344 examples: {'rewards_train/chosen': '-0.46534', 'rewards_train/rejected': '-0.89533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43', 'logps_train/rejected': '-157.75', 'logps_train/chosen': '-159.77', 'loss/train': '0.59122', 'examples_per_second': '95.506', 'grad_norm': '17.642', 'counters/examples': 33344, 'counters/updates': 521}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33472 examples: {'rewards_train/chosen': '-0.29447', 'rewards_train/rejected': '-0.78346', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48899', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-154.91', 'loss/train': '0.55378', 'examples_per_second': '90.001', 'grad_norm': '15.697', 'counters/examples': 33472, 'counters/updates': 523}
skipping logging after 33536 examples to avoid logging too frequently
train stats after 33600 examples: {'rewards_train/chosen': '-0.37946', 'rewards_train/rejected': '-0.65043', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27097', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-160.36', 'loss/train': '0.68309', 'examples_per_second': '90.321', 'grad_norm': '20.779', 'counters/examples': 33600, 'counters/updates': 525}
skipping logging after 33664 examples to avoid logging too frequently
train stats after 33728 examples: {'rewards_train/chosen': '-0.29104', 'rewards_train/rejected': '-0.71933', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.42829', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-145.67', 'loss/train': '0.57173', 'examples_per_second': '87.068', 'grad_norm': '14.513', 'counters/examples': 33728, 'counters/updates': 527}
skipping logging after 33792 examples to avoid logging too frequently
train stats after 33856 examples: {'rewards_train/chosen': '-0.42768', 'rewards_train/rejected': '-0.71711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28942', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-144.82', 'loss/train': '0.63012', 'examples_per_second': '89.934', 'grad_norm': '16.015', 'counters/examples': 33856, 'counters/updates': 529}
skipping logging after 33920 examples to avoid logging too frequently
train stats after 33984 examples: {'rewards_train/chosen': '-0.3955', 'rewards_train/rejected': '-0.77699', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.38149', 'logps_train/rejected': '-122.36', 'logps_train/chosen': '-149.18', 'loss/train': '0.60218', 'examples_per_second': '89.789', 'grad_norm': '17.527', 'counters/examples': 33984, 'counters/updates': 531}
skipping logging after 34048 examples to avoid logging too frequently
train stats after 34112 examples: {'rewards_train/chosen': '-0.42505', 'rewards_train/rejected': '-0.92827', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.50322', 'logps_train/rejected': '-111.71', 'logps_train/chosen': '-144.66', 'loss/train': '0.53227', 'examples_per_second': '86.873', 'grad_norm': '13.883', 'counters/examples': 34112, 'counters/updates': 533}
skipping logging after 34176 examples to avoid logging too frequently
train stats after 34240 examples: {'rewards_train/chosen': '-0.47017', 'rewards_train/rejected': '-1.181', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.71082', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-142.81', 'loss/train': '0.49258', 'examples_per_second': '93.449', 'grad_norm': '12.777', 'counters/examples': 34240, 'counters/updates': 535}
skipping logging after 34304 examples to avoid logging too frequently
train stats after 34368 examples: {'rewards_train/chosen': '-0.82043', 'rewards_train/rejected': '-1.287', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.46656', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-138.26', 'loss/train': '0.59737', 'examples_per_second': '90.408', 'grad_norm': '15.386', 'counters/examples': 34368, 'counters/updates': 537}
skipping logging after 34432 examples to avoid logging too frequently
train stats after 34496 examples: {'rewards_train/chosen': '-0.6699', 'rewards_train/rejected': '-1.0212', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3513', 'logps_train/rejected': '-140.12', 'logps_train/chosen': '-140.26', 'loss/train': '0.61375', 'examples_per_second': '90.308', 'grad_norm': '15.906', 'counters/examples': 34496, 'counters/updates': 539}
skipping logging after 34560 examples to avoid logging too frequently
train stats after 34624 examples: {'rewards_train/chosen': '-0.56007', 'rewards_train/rejected': '-0.88255', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.32249', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-144.67', 'loss/train': '0.63234', 'examples_per_second': '89.296', 'grad_norm': '17.935', 'counters/examples': 34624, 'counters/updates': 541}
skipping logging after 34688 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '-0.44753', 'rewards_train/rejected': '-0.95721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50968', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-131.37', 'loss/train': '0.54841', 'examples_per_second': '92.837', 'grad_norm': '13.96', 'counters/examples': 34752, 'counters/updates': 543}
skipping logging after 34816 examples to avoid logging too frequently
train stats after 34880 examples: {'rewards_train/chosen': '-0.48637', 'rewards_train/rejected': '-0.82908', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34272', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-142.42', 'loss/train': '0.61013', 'examples_per_second': '89.798', 'grad_norm': '15.09', 'counters/examples': 34880, 'counters/updates': 545}
skipping logging after 34944 examples to avoid logging too frequently
train stats after 35008 examples: {'rewards_train/chosen': '-0.56486', 'rewards_train/rejected': '-0.90955', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.34469', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-146.12', 'loss/train': '0.63776', 'examples_per_second': '92.194', 'grad_norm': '15.88', 'counters/examples': 35008, 'counters/updates': 547}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35136 examples: {'rewards_train/chosen': '-0.51881', 'rewards_train/rejected': '-0.90505', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38624', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-135.86', 'loss/train': '0.62911', 'examples_per_second': '90.366', 'grad_norm': '16.326', 'counters/examples': 35136, 'counters/updates': 549}
skipping logging after 35200 examples to avoid logging too frequently
train stats after 35264 examples: {'rewards_train/chosen': '-0.45552', 'rewards_train/rejected': '-1.0145', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.55899', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-136.98', 'loss/train': '0.51112', 'examples_per_second': '90.019', 'grad_norm': '14.447', 'counters/examples': 35264, 'counters/updates': 551}
skipping logging after 35328 examples to avoid logging too frequently
train stats after 35392 examples: {'rewards_train/chosen': '-0.48743', 'rewards_train/rejected': '-0.84178', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35434', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-160.42', 'loss/train': '0.63069', 'examples_per_second': '90.094', 'grad_norm': '17.997', 'counters/examples': 35392, 'counters/updates': 553}
skipping logging after 35456 examples to avoid logging too frequently
train stats after 35520 examples: {'rewards_train/chosen': '-0.5039', 'rewards_train/rejected': '-1.0979', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59397', 'logps_train/rejected': '-128.19', 'logps_train/chosen': '-155.06', 'loss/train': '0.54457', 'examples_per_second': '90.202', 'grad_norm': '15.369', 'counters/examples': 35520, 'counters/updates': 555}
skipping logging after 35584 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '-0.28889', 'rewards_train/rejected': '-0.89738', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60848', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-139.6', 'loss/train': '0.52765', 'examples_per_second': '90.758', 'grad_norm': '14.499', 'counters/examples': 35648, 'counters/updates': 557}
skipping logging after 35712 examples to avoid logging too frequently
train stats after 35776 examples: {'rewards_train/chosen': '-0.26421', 'rewards_train/rejected': '-0.70462', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44041', 'logps_train/rejected': '-163.16', 'logps_train/chosen': '-148.83', 'loss/train': '0.58289', 'examples_per_second': '87.749', 'grad_norm': '17.72', 'counters/examples': 35776, 'counters/updates': 559}
skipping logging after 35840 examples to avoid logging too frequently
train stats after 35904 examples: {'rewards_train/chosen': '-0.26286', 'rewards_train/rejected': '-0.79616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5333', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-146.9', 'loss/train': '0.57499', 'examples_per_second': '95.834', 'grad_norm': '16.54', 'counters/examples': 35904, 'counters/updates': 561}
Running evaluation after 35904 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.41it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.57it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.83it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.68it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.63it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.62it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.54it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.48it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.57it/s]
eval after 35904: {'rewards_eval/chosen': '-0.20881', 'rewards_eval/rejected': '-0.5842', 'rewards_eval/accuracies': '0.67188', 'rewards_eval/margins': '0.37539', 'logps_eval/rejected': '-126.99', 'logps_eval/chosen': '-142.89', 'loss/eval': '0.63013'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-35904...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-35904/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-35904/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-35904/scheduler.pt...
train stats after 35968 examples: {'rewards_train/chosen': '-0.29314', 'rewards_train/rejected': '-0.74593', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45279', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-126.36', 'loss/train': '0.57615', 'examples_per_second': '69.986', 'grad_norm': '15.304', 'counters/examples': 35968, 'counters/updates': 562}
skipping logging after 36032 examples to avoid logging too frequently
train stats after 36096 examples: {'rewards_train/chosen': '-0.17402', 'rewards_train/rejected': '-0.58467', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41065', 'logps_train/rejected': '-154.71', 'logps_train/chosen': '-130.66', 'loss/train': '0.58562', 'examples_per_second': '90.334', 'grad_norm': '16.233', 'counters/examples': 36096, 'counters/updates': 564}
skipping logging after 36160 examples to avoid logging too frequently
train stats after 36224 examples: {'rewards_train/chosen': '-0.16752', 'rewards_train/rejected': '-0.63284', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46532', 'logps_train/rejected': '-150.53', 'logps_train/chosen': '-146.24', 'loss/train': '0.58426', 'examples_per_second': '90.475', 'grad_norm': '16.947', 'counters/examples': 36224, 'counters/updates': 566}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36352 examples: {'rewards_train/chosen': '-0.3038', 'rewards_train/rejected': '-0.89645', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.59265', 'logps_train/rejected': '-135.36', 'logps_train/chosen': '-154.56', 'loss/train': '0.5299', 'examples_per_second': '94.017', 'grad_norm': '15.924', 'counters/examples': 36352, 'counters/updates': 568}
skipping logging after 36416 examples to avoid logging too frequently
train stats after 36480 examples: {'rewards_train/chosen': '-0.060037', 'rewards_train/rejected': '-0.57651', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.51648', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-140.6', 'loss/train': '0.53601', 'examples_per_second': '94.09', 'grad_norm': '14.934', 'counters/examples': 36480, 'counters/updates': 570}
skipping logging after 36544 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '-0.36838', 'rewards_train/rejected': '-0.76084', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39245', 'logps_train/rejected': '-125', 'logps_train/chosen': '-138.76', 'loss/train': '0.57587', 'examples_per_second': '88.766', 'grad_norm': '15.768', 'counters/examples': 36608, 'counters/updates': 572}
skipping logging after 36672 examples to avoid logging too frequently
train stats after 36736 examples: {'rewards_train/chosen': '-0.5047', 'rewards_train/rejected': '-0.86561', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.3609', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-154.68', 'loss/train': '0.66329', 'examples_per_second': '90.385', 'grad_norm': '18.222', 'counters/examples': 36736, 'counters/updates': 574}
skipping logging after 36800 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '-0.41047', 'rewards_train/rejected': '-0.93761', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.52714', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-158.37', 'loss/train': '0.57244', 'examples_per_second': '88.66', 'grad_norm': '16.102', 'counters/examples': 36864, 'counters/updates': 576}
skipping logging after 36928 examples to avoid logging too frequently
train stats after 36992 examples: {'rewards_train/chosen': '-0.48753', 'rewards_train/rejected': '-0.93562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44809', 'logps_train/rejected': '-153.28', 'logps_train/chosen': '-139.23', 'loss/train': '0.60866', 'examples_per_second': '91.418', 'grad_norm': '18.232', 'counters/examples': 36992, 'counters/updates': 578}
skipping logging after 37056 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '-0.52485', 'rewards_train/rejected': '-1.0346', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50975', 'logps_train/rejected': '-133.82', 'logps_train/chosen': '-161.32', 'loss/train': '0.58584', 'examples_per_second': '100.24', 'grad_norm': '16.896', 'counters/examples': 37120, 'counters/updates': 580}
skipping logging after 37184 examples to avoid logging too frequently
train stats after 37248 examples: {'rewards_train/chosen': '-0.50343', 'rewards_train/rejected': '-1.251', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.74753', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-138.42', 'loss/train': '0.48653', 'examples_per_second': '91.095', 'grad_norm': '14.255', 'counters/examples': 37248, 'counters/updates': 582}
skipping logging after 37312 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.4577', 'rewards_train/rejected': '-1.0955', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63777', 'logps_train/rejected': '-106.51', 'logps_train/chosen': '-132.06', 'loss/train': '0.55229', 'examples_per_second': '90.432', 'grad_norm': '15.261', 'counters/examples': 37376, 'counters/updates': 584}
skipping logging after 37440 examples to avoid logging too frequently
train stats after 37504 examples: {'rewards_train/chosen': '-0.7058', 'rewards_train/rejected': '-1.0516', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34575', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-142.02', 'loss/train': '0.63428', 'examples_per_second': '88.417', 'grad_norm': '17.464', 'counters/examples': 37504, 'counters/updates': 586}
skipping logging after 37568 examples to avoid logging too frequently
train stats after 37632 examples: {'rewards_train/chosen': '-0.47672', 'rewards_train/rejected': '-0.808', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33129', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-147.53', 'loss/train': '0.63347', 'examples_per_second': '87.901', 'grad_norm': '17.685', 'counters/examples': 37632, 'counters/updates': 588}
skipping logging after 37696 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '-0.47179', 'rewards_train/rejected': '-1.0022', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53042', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-155.19', 'loss/train': '0.53403', 'examples_per_second': '90.549', 'grad_norm': '14.176', 'counters/examples': 37760, 'counters/updates': 590}
skipping logging after 37824 examples to avoid logging too frequently
train stats after 37888 examples: {'rewards_train/chosen': '-0.43178', 'rewards_train/rejected': '-0.94151', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50973', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-153.15', 'loss/train': '0.54609', 'examples_per_second': '90.594', 'grad_norm': '15.221', 'counters/examples': 37888, 'counters/updates': 592}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 38016 examples: {'rewards_train/chosen': '-0.41314', 'rewards_train/rejected': '-0.73842', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32529', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-159.69', 'loss/train': '0.63448', 'examples_per_second': '90.508', 'grad_norm': '17.414', 'counters/examples': 38016, 'counters/updates': 594}
skipping logging after 38080 examples to avoid logging too frequently
train stats after 38144 examples: {'rewards_train/chosen': '-0.59872', 'rewards_train/rejected': '-0.99149', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.39277', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-140.29', 'loss/train': '0.63515', 'examples_per_second': '90.676', 'grad_norm': '16.838', 'counters/examples': 38144, 'counters/updates': 596}
skipping logging after 38208 examples to avoid logging too frequently
train stats after 38272 examples: {'rewards_train/chosen': '-0.42894', 'rewards_train/rejected': '-0.87708', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44814', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-148.01', 'loss/train': '0.59875', 'examples_per_second': '88.04', 'grad_norm': '15.611', 'counters/examples': 38272, 'counters/updates': 598}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38400 examples: {'rewards_train/chosen': '-0.39752', 'rewards_train/rejected': '-0.76579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36826', 'logps_train/rejected': '-143.18', 'logps_train/chosen': '-156.52', 'loss/train': '0.61934', 'examples_per_second': '90.764', 'grad_norm': '16.487', 'counters/examples': 38400, 'counters/updates': 600}
skipping logging after 38464 examples to avoid logging too frequently
train stats after 38528 examples: {'rewards_train/chosen': '-0.4695', 'rewards_train/rejected': '-0.95577', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.48627', 'logps_train/rejected': '-145.16', 'logps_train/chosen': '-155.69', 'loss/train': '0.58579', 'examples_per_second': '87.908', 'grad_norm': '17.185', 'counters/examples': 38528, 'counters/updates': 602}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38656 examples: {'rewards_train/chosen': '-0.42563', 'rewards_train/rejected': '-0.82127', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39565', 'logps_train/rejected': '-128.29', 'logps_train/chosen': '-148.84', 'loss/train': '0.62824', 'examples_per_second': '87.524', 'grad_norm': '17.23', 'counters/examples': 38656, 'counters/updates': 604}
skipping logging after 38720 examples to avoid logging too frequently
train stats after 38784 examples: {'rewards_train/chosen': '-0.44282', 'rewards_train/rejected': '-0.8769', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43408', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-156.76', 'loss/train': '0.58042', 'examples_per_second': '87.494', 'grad_norm': '15.381', 'counters/examples': 38784, 'counters/updates': 606}
skipping logging after 38848 examples to avoid logging too frequently
train stats after 38912 examples: {'rewards_train/chosen': '-0.36396', 'rewards_train/rejected': '-1.0855', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.72159', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-142.81', 'loss/train': '0.50132', 'examples_per_second': '90.659', 'grad_norm': '13.872', 'counters/examples': 38912, 'counters/updates': 608}
skipping logging after 38976 examples to avoid logging too frequently
train stats after 39040 examples: {'rewards_train/chosen': '-0.52528', 'rewards_train/rejected': '-1.019', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.49374', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-135', 'loss/train': '0.55363', 'examples_per_second': '90.719', 'grad_norm': '15.244', 'counters/examples': 39040, 'counters/updates': 610}
skipping logging after 39104 examples to avoid logging too frequently
train stats after 39168 examples: {'rewards_train/chosen': '-0.69028', 'rewards_train/rejected': '-1.183', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49268', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-152.18', 'loss/train': '0.6215', 'examples_per_second': '89.476', 'grad_norm': '16.702', 'counters/examples': 39168, 'counters/updates': 612}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39296 examples: {'rewards_train/chosen': '-0.54805', 'rewards_train/rejected': '-0.98175', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.43371', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-142.86', 'loss/train': '0.58986', 'examples_per_second': '92.948', 'grad_norm': '15.628', 'counters/examples': 39296, 'counters/updates': 614}
skipping logging after 39360 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '-0.40949', 'rewards_train/rejected': '-0.73728', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.32779', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-136.4', 'loss/train': '0.6069', 'examples_per_second': '90.257', 'grad_norm': '15.554', 'counters/examples': 39424, 'counters/updates': 616}
skipping logging after 39488 examples to avoid logging too frequently
train stats after 39552 examples: {'rewards_train/chosen': '-0.30203', 'rewards_train/rejected': '-0.84628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54425', 'logps_train/rejected': '-130.43', 'logps_train/chosen': '-131.5', 'loss/train': '0.57496', 'examples_per_second': '87.121', 'grad_norm': '15.794', 'counters/examples': 39552, 'counters/updates': 618}
skipping logging after 39616 examples to avoid logging too frequently
train stats after 39680 examples: {'rewards_train/chosen': '-0.44196', 'rewards_train/rejected': '-0.85428', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41231', 'logps_train/rejected': '-123.93', 'logps_train/chosen': '-147.65', 'loss/train': '0.61115', 'examples_per_second': '90.728', 'grad_norm': '15.204', 'counters/examples': 39680, 'counters/updates': 620}
skipping logging after 39744 examples to avoid logging too frequently
train stats after 39808 examples: {'rewards_train/chosen': '-0.43967', 'rewards_train/rejected': '-0.72938', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28971', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-143.63', 'loss/train': '0.65759', 'examples_per_second': '93.019', 'grad_norm': '17.621', 'counters/examples': 39808, 'counters/updates': 622}
skipping logging after 39872 examples to avoid logging too frequently
train stats after 39936 examples: {'rewards_train/chosen': '-0.40488', 'rewards_train/rejected': '-0.8586', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45372', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-161.1', 'loss/train': '0.5977', 'examples_per_second': '91.478', 'grad_norm': '16.371', 'counters/examples': 39936, 'counters/updates': 624}
skipping logging after 40000 examples to avoid logging too frequently
train stats after 40064 examples: {'rewards_train/chosen': '-0.29929', 'rewards_train/rejected': '-0.95024', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.65095', 'logps_train/rejected': '-121.02', 'logps_train/chosen': '-150.26', 'loss/train': '0.51091', 'examples_per_second': '87.641', 'grad_norm': '16.525', 'counters/examples': 40064, 'counters/updates': 626}
skipping logging after 40128 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '-0.5747', 'rewards_train/rejected': '-1.0691', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4944', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-150.52', 'loss/train': '0.58359', 'examples_per_second': '95.944', 'grad_norm': '14.978', 'counters/examples': 40192, 'counters/updates': 628}
skipping logging after 40256 examples to avoid logging too frequently
train stats after 40320 examples: {'rewards_train/chosen': '-0.45844', 'rewards_train/rejected': '-1.0524', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.594', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-139.33', 'loss/train': '0.5374', 'examples_per_second': '90.521', 'grad_norm': '14.091', 'counters/examples': 40320, 'counters/updates': 630}
skipping logging after 40384 examples to avoid logging too frequently
train stats after 40448 examples: {'rewards_train/chosen': '-0.74637', 'rewards_train/rejected': '-1.3295', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.58315', 'logps_train/rejected': '-164.8', 'logps_train/chosen': '-137.15', 'loss/train': '0.54899', 'examples_per_second': '87.975', 'grad_norm': '15.968', 'counters/examples': 40448, 'counters/updates': 632}
skipping logging after 40512 examples to avoid logging too frequently
train stats after 40576 examples: {'rewards_train/chosen': '-0.47241', 'rewards_train/rejected': '-0.82141', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34899', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-148.12', 'loss/train': '0.60473', 'examples_per_second': '90.297', 'grad_norm': '17.248', 'counters/examples': 40576, 'counters/updates': 634}
skipping logging after 40640 examples to avoid logging too frequently
train stats after 40704 examples: {'rewards_train/chosen': '-0.44942', 'rewards_train/rejected': '-0.84723', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3978', 'logps_train/rejected': '-134.17', 'logps_train/chosen': '-133.39', 'loss/train': '0.64687', 'examples_per_second': '90.343', 'grad_norm': '17.06', 'counters/examples': 40704, 'counters/updates': 636}
skipping logging after 40768 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.55532', 'rewards_train/rejected': '-1.0111', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.45577', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-134.26', 'loss/train': '0.57615', 'examples_per_second': '103.13', 'grad_norm': '15.211', 'counters/examples': 40832, 'counters/updates': 638}
skipping logging after 40896 examples to avoid logging too frequently
train stats after 40960 examples: {'rewards_train/chosen': '-0.60053', 'rewards_train/rejected': '-1.0675', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.46693', 'logps_train/rejected': '-141.43', 'logps_train/chosen': '-126.67', 'loss/train': '0.59393', 'examples_per_second': '93.639', 'grad_norm': '15.807', 'counters/examples': 40960, 'counters/updates': 640}
skipping logging after 41024 examples to avoid logging too frequently
train stats after 41088 examples: {'rewards_train/chosen': '-0.65194', 'rewards_train/rejected': '-0.95136', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.29942', 'logps_train/rejected': '-149.58', 'logps_train/chosen': '-164.08', 'loss/train': '0.65303', 'examples_per_second': '90.282', 'grad_norm': '17.999', 'counters/examples': 41088, 'counters/updates': 642}
skipping logging after 41152 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '-0.61576', 'rewards_train/rejected': '-1.0571', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44134', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-138.78', 'loss/train': '0.57413', 'examples_per_second': '100.35', 'grad_norm': '15.497', 'counters/examples': 41216, 'counters/updates': 644}
skipping logging after 41280 examples to avoid logging too frequently
train stats after 41344 examples: {'rewards_train/chosen': '-0.80361', 'rewards_train/rejected': '-1.072', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.26835', 'logps_train/rejected': '-119.61', 'logps_train/chosen': '-131.32', 'loss/train': '0.68514', 'examples_per_second': '93.514', 'grad_norm': '16.326', 'counters/examples': 41344, 'counters/updates': 646}
skipping logging after 41408 examples to avoid logging too frequently
train stats after 41472 examples: {'rewards_train/chosen': '-0.69695', 'rewards_train/rejected': '-0.90433', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20738', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-162.79', 'loss/train': '0.68543', 'examples_per_second': '90.307', 'grad_norm': '17.717', 'counters/examples': 41472, 'counters/updates': 648}
skipping logging after 41536 examples to avoid logging too frequently
train stats after 41600 examples: {'rewards_train/chosen': '-0.61273', 'rewards_train/rejected': '-1.0019', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38918', 'logps_train/rejected': '-151.56', 'logps_train/chosen': '-132.06', 'loss/train': '0.62493', 'examples_per_second': '88.135', 'grad_norm': '17.394', 'counters/examples': 41600, 'counters/updates': 650}
skipping logging after 41664 examples to avoid logging too frequently
train stats after 41728 examples: {'rewards_train/chosen': '-0.7879', 'rewards_train/rejected': '-0.97537', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.18747', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-150.37', 'loss/train': '0.73006', 'examples_per_second': '89.961', 'grad_norm': '19.523', 'counters/examples': 41728, 'counters/updates': 652}
skipping logging after 41792 examples to avoid logging too frequently
train stats after 41856 examples: {'rewards_train/chosen': '-0.55921', 'rewards_train/rejected': '-1.1124', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.55322', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-144.18', 'loss/train': '0.56553', 'examples_per_second': '90.773', 'grad_norm': '16.285', 'counters/examples': 41856, 'counters/updates': 654}
skipping logging after 41920 examples to avoid logging too frequently
train stats after 41984 examples: {'rewards_train/chosen': '-0.46772', 'rewards_train/rejected': '-0.81322', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34549', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-137.86', 'loss/train': '0.62453', 'examples_per_second': '90.562', 'grad_norm': '17.966', 'counters/examples': 41984, 'counters/updates': 656}
skipping logging after 42048 examples to avoid logging too frequently
train stats after 42112 examples: {'rewards_train/chosen': '-0.40191', 'rewards_train/rejected': '-1.0992', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.6973', 'logps_train/rejected': '-139.16', 'logps_train/chosen': '-123.28', 'loss/train': '0.49767', 'examples_per_second': '89.973', 'grad_norm': '14.76', 'counters/examples': 42112, 'counters/updates': 658}
skipping logging after 42176 examples to avoid logging too frequently
train stats after 42240 examples: {'rewards_train/chosen': '-0.7457', 'rewards_train/rejected': '-1.3876', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.64193', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-156.07', 'loss/train': '0.58537', 'examples_per_second': '87.664', 'grad_norm': '19.191', 'counters/examples': 42240, 'counters/updates': 660}
skipping logging after 42304 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '-0.74786', 'rewards_train/rejected': '-1.2849', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.53702', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-131.01', 'loss/train': '0.58381', 'examples_per_second': '100.59', 'grad_norm': '16.356', 'counters/examples': 42368, 'counters/updates': 662}
skipping logging after 42432 examples to avoid logging too frequently
train stats after 42496 examples: {'rewards_train/chosen': '-0.70761', 'rewards_train/rejected': '-1.0159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30828', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-143.9', 'loss/train': '0.6381', 'examples_per_second': '90.244', 'grad_norm': '16.506', 'counters/examples': 42496, 'counters/updates': 664}
skipping logging after 42560 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '-0.66305', 'rewards_train/rejected': '-1.2258', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56274', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-141.51', 'loss/train': '0.56755', 'examples_per_second': '88.274', 'grad_norm': '14.829', 'counters/examples': 42624, 'counters/updates': 666}
skipping logging after 42688 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '-0.4373', 'rewards_train/rejected': '-1.002', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56469', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-143.31', 'loss/train': '0.53726', 'examples_per_second': '90.542', 'grad_norm': '14.767', 'counters/examples': 42752, 'counters/updates': 668}
skipping logging after 42816 examples to avoid logging too frequently
train stats after 42880 examples: {'rewards_train/chosen': '-0.46472', 'rewards_train/rejected': '-0.98666', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52194', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-118.76', 'loss/train': '0.55126', 'examples_per_second': '93.572', 'grad_norm': '14.662', 'counters/examples': 42880, 'counters/updates': 670}
skipping logging after 42944 examples to avoid logging too frequently
train stats after 43008 examples: {'rewards_train/chosen': '-0.57646', 'rewards_train/rejected': '-0.96342', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38695', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-165.78', 'loss/train': '0.64373', 'examples_per_second': '90.077', 'grad_norm': '18.313', 'counters/examples': 43008, 'counters/updates': 672}
skipping logging after 43072 examples to avoid logging too frequently
train stats after 43136 examples: {'rewards_train/chosen': '-0.68975', 'rewards_train/rejected': '-1.0228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33306', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-145.1', 'loss/train': '0.64375', 'examples_per_second': '90.035', 'grad_norm': '17.11', 'counters/examples': 43136, 'counters/updates': 674}
skipping logging after 43200 examples to avoid logging too frequently
train stats after 43264 examples: {'rewards_train/chosen': '-0.45127', 'rewards_train/rejected': '-0.91602', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.46475', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-150.24', 'loss/train': '0.5688', 'examples_per_second': '86.107', 'grad_norm': '16.202', 'counters/examples': 43264, 'counters/updates': 676}
skipping logging after 43328 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '-0.29815', 'rewards_train/rejected': '-0.6801', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38196', 'logps_train/rejected': '-129.59', 'logps_train/chosen': '-144.8', 'loss/train': '0.61839', 'examples_per_second': '87.896', 'grad_norm': '17.523', 'counters/examples': 43392, 'counters/updates': 678}
skipping logging after 43456 examples to avoid logging too frequently
train stats after 43520 examples: {'rewards_train/chosen': '-0.28516', 'rewards_train/rejected': '-0.88348', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59832', 'logps_train/rejected': '-125.67', 'logps_train/chosen': '-135.56', 'loss/train': '0.52871', 'examples_per_second': '87.415', 'grad_norm': '14.673', 'counters/examples': 43520, 'counters/updates': 680}
skipping logging after 43584 examples to avoid logging too frequently
train stats after 43648 examples: {'rewards_train/chosen': '-0.39116', 'rewards_train/rejected': '-0.97118', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58002', 'logps_train/rejected': '-95.902', 'logps_train/chosen': '-131.58', 'loss/train': '0.52327', 'examples_per_second': '89.538', 'grad_norm': '13.521', 'counters/examples': 43648, 'counters/updates': 682}
skipping logging after 43712 examples to avoid logging too frequently
train stats after 43776 examples: {'rewards_train/chosen': '-0.56613', 'rewards_train/rejected': '-0.97739', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41126', 'logps_train/rejected': '-118.31', 'logps_train/chosen': '-147.27', 'loss/train': '0.58522', 'examples_per_second': '89.684', 'grad_norm': '15.393', 'counters/examples': 43776, 'counters/updates': 684}
skipping logging after 43840 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '-0.6563', 'rewards_train/rejected': '-1.0825', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.42622', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-140.55', 'loss/train': '0.62309', 'examples_per_second': '99.871', 'grad_norm': '17.056', 'counters/examples': 43904, 'counters/updates': 686}
skipping logging after 43968 examples to avoid logging too frequently
train stats after 44032 examples: {'rewards_train/chosen': '-0.52852', 'rewards_train/rejected': '-0.89601', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.36749', 'logps_train/rejected': '-143.85', 'logps_train/chosen': '-155.01', 'loss/train': '0.64868', 'examples_per_second': '91.387', 'grad_norm': '18.346', 'counters/examples': 44032, 'counters/updates': 688}
skipping logging after 44096 examples to avoid logging too frequently
train stats after 44160 examples: {'rewards_train/chosen': '-0.53135', 'rewards_train/rejected': '-0.9603', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42895', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-158.77', 'loss/train': '0.63876', 'examples_per_second': '90.204', 'grad_norm': '19.107', 'counters/examples': 44160, 'counters/updates': 690}
skipping logging after 44224 examples to avoid logging too frequently
train stats after 44288 examples: {'rewards_train/chosen': '-0.56066', 'rewards_train/rejected': '-0.84839', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.28773', 'logps_train/rejected': '-141.91', 'logps_train/chosen': '-138.27', 'loss/train': '0.66258', 'examples_per_second': '93.219', 'grad_norm': '17.096', 'counters/examples': 44288, 'counters/updates': 692}
skipping logging after 44352 examples to avoid logging too frequently
train stats after 44416 examples: {'rewards_train/chosen': '-0.5062', 'rewards_train/rejected': '-0.95503', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.44883', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-144.25', 'loss/train': '0.62964', 'examples_per_second': '90.139', 'grad_norm': '17.007', 'counters/examples': 44416, 'counters/updates': 694}
skipping logging after 44480 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '-0.67896', 'rewards_train/rejected': '-1.2491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57016', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-131.34', 'loss/train': '0.5567', 'examples_per_second': '92.594', 'grad_norm': '15.264', 'counters/examples': 44544, 'counters/updates': 696}
skipping logging after 44608 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '-0.62082', 'rewards_train/rejected': '-0.82495', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20412', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-139.2', 'loss/train': '0.7128', 'examples_per_second': '89.906', 'grad_norm': '18.228', 'counters/examples': 44672, 'counters/updates': 698}
skipping logging after 44736 examples to avoid logging too frequently
train stats after 44800 examples: {'rewards_train/chosen': '-0.63092', 'rewards_train/rejected': '-1.0409', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40997', 'logps_train/rejected': '-150.99', 'logps_train/chosen': '-169.52', 'loss/train': '0.59078', 'examples_per_second': '89.95', 'grad_norm': '18.053', 'counters/examples': 44800, 'counters/updates': 700}
skipping logging after 44864 examples to avoid logging too frequently
train stats after 44928 examples: {'rewards_train/chosen': '-0.69087', 'rewards_train/rejected': '-0.86998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17911', 'logps_train/rejected': '-151.51', 'logps_train/chosen': '-155.03', 'loss/train': '0.69196', 'examples_per_second': '90.003', 'grad_norm': '18.066', 'counters/examples': 44928, 'counters/updates': 702}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45056 examples: {'rewards_train/chosen': '-0.4187', 'rewards_train/rejected': '-0.88329', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.46459', 'logps_train/rejected': '-144.32', 'logps_train/chosen': '-147.51', 'loss/train': '0.5846', 'examples_per_second': '90.33', 'grad_norm': '17.031', 'counters/examples': 45056, 'counters/updates': 704}
skipping logging after 45120 examples to avoid logging too frequently
train stats after 45184 examples: {'rewards_train/chosen': '-0.79898', 'rewards_train/rejected': '-1.3209', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.52195', 'logps_train/rejected': '-122.85', 'logps_train/chosen': '-134.14', 'loss/train': '0.54964', 'examples_per_second': '89.514', 'grad_norm': '14.783', 'counters/examples': 45184, 'counters/updates': 706}
skipping logging after 45248 examples to avoid logging too frequently
train stats after 45312 examples: {'rewards_train/chosen': '-0.74211', 'rewards_train/rejected': '-1.4151', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.67296', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-148.01', 'loss/train': '0.50116', 'examples_per_second': '92.527', 'grad_norm': '13.914', 'counters/examples': 45312, 'counters/updates': 708}
skipping logging after 45376 examples to avoid logging too frequently
train stats after 45440 examples: {'rewards_train/chosen': '-0.71126', 'rewards_train/rejected': '-1.0809', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.36967', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-130.68', 'loss/train': '0.6257', 'examples_per_second': '90.33', 'grad_norm': '17.341', 'counters/examples': 45440, 'counters/updates': 710}
skipping logging after 45504 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '-0.78547', 'rewards_train/rejected': '-1.1075', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.32203', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-127.07', 'loss/train': '0.65365', 'examples_per_second': '96.135', 'grad_norm': '16.585', 'counters/examples': 45568, 'counters/updates': 712}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45696 examples: {'rewards_train/chosen': '-0.76372', 'rewards_train/rejected': '-1.2402', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47653', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-136.21', 'loss/train': '0.60057', 'examples_per_second': '90.072', 'grad_norm': '16.28', 'counters/examples': 45696, 'counters/updates': 714}
skipping logging after 45760 examples to avoid logging too frequently
train stats after 45824 examples: {'rewards_train/chosen': '-0.69442', 'rewards_train/rejected': '-1.1835', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48907', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-137.7', 'loss/train': '0.58084', 'examples_per_second': '90.276', 'grad_norm': '15.508', 'counters/examples': 45824, 'counters/updates': 716}
skipping logging after 45888 examples to avoid logging too frequently
train stats after 45952 examples: {'rewards_train/chosen': '-0.85268', 'rewards_train/rejected': '-1.367', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.5143', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-148.8', 'loss/train': '0.5666', 'examples_per_second': '94.829', 'grad_norm': '16.204', 'counters/examples': 45952, 'counters/updates': 718}
skipping logging after 46016 examples to avoid logging too frequently
train stats after 46080 examples: {'rewards_train/chosen': '-0.76525', 'rewards_train/rejected': '-1.0702', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30498', 'logps_train/rejected': '-155.53', 'logps_train/chosen': '-141.3', 'loss/train': '0.66631', 'examples_per_second': '86.91', 'grad_norm': '17.618', 'counters/examples': 46080, 'counters/updates': 720}
skipping logging after 46144 examples to avoid logging too frequently
train stats after 46208 examples: {'rewards_train/chosen': '-0.64687', 'rewards_train/rejected': '-1.4299', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.78306', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-155.53', 'loss/train': '0.47762', 'examples_per_second': '90.189', 'grad_norm': '13.929', 'counters/examples': 46208, 'counters/updates': 722}
skipping logging after 46272 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '-0.91146', 'rewards_train/rejected': '-1.1337', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.22229', 'logps_train/rejected': '-138.65', 'logps_train/chosen': '-151.78', 'loss/train': '0.72145', 'examples_per_second': '90.916', 'grad_norm': '18.938', 'counters/examples': 46336, 'counters/updates': 724}
skipping logging after 46400 examples to avoid logging too frequently
train stats after 46464 examples: {'rewards_train/chosen': '-0.87052', 'rewards_train/rejected': '-1.2308', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36032', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-123.85', 'loss/train': '0.61783', 'examples_per_second': '90.962', 'grad_norm': '16.109', 'counters/examples': 46464, 'counters/updates': 726}
skipping logging after 46528 examples to avoid logging too frequently
train stats after 46592 examples: {'rewards_train/chosen': '-0.90738', 'rewards_train/rejected': '-1.463', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55566', 'logps_train/rejected': '-146.26', 'logps_train/chosen': '-144.24', 'loss/train': '0.5586', 'examples_per_second': '86.591', 'grad_norm': '16.664', 'counters/examples': 46592, 'counters/updates': 728}
skipping logging after 46656 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '-0.74222', 'rewards_train/rejected': '-1.2025', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46027', 'logps_train/rejected': '-154.57', 'logps_train/chosen': '-167.65', 'loss/train': '0.59414', 'examples_per_second': '88.24', 'grad_norm': '17.503', 'counters/examples': 46720, 'counters/updates': 730}
skipping logging after 46784 examples to avoid logging too frequently
train stats after 46848 examples: {'rewards_train/chosen': '-0.93429', 'rewards_train/rejected': '-1.6497', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.71539', 'logps_train/rejected': '-117.8', 'logps_train/chosen': '-151.15', 'loss/train': '0.53579', 'examples_per_second': '87.556', 'grad_norm': '15.773', 'counters/examples': 46848, 'counters/updates': 732}
skipping logging after 46912 examples to avoid logging too frequently
train stats after 46976 examples: {'rewards_train/chosen': '-1.0148', 'rewards_train/rejected': '-1.5105', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49577', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-153.73', 'loss/train': '0.62009', 'examples_per_second': '87.525', 'grad_norm': '17.328', 'counters/examples': 46976, 'counters/updates': 734}
skipping logging after 47040 examples to avoid logging too frequently
train stats after 47104 examples: {'rewards_train/chosen': '-0.84906', 'rewards_train/rejected': '-1.4643', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61523', 'logps_train/rejected': '-138.39', 'logps_train/chosen': '-156.99', 'loss/train': '0.56226', 'examples_per_second': '92.283', 'grad_norm': '15.997', 'counters/examples': 47104, 'counters/updates': 736}
skipping logging after 47168 examples to avoid logging too frequently
train stats after 47232 examples: {'rewards_train/chosen': '-0.74377', 'rewards_train/rejected': '-1.4504', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70664', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-125.82', 'loss/train': '0.4981', 'examples_per_second': '90.14', 'grad_norm': '13.695', 'counters/examples': 47232, 'counters/updates': 738}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47360 examples: {'rewards_train/chosen': '-0.95843', 'rewards_train/rejected': '-1.5923', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.63382', 'logps_train/rejected': '-135.33', 'logps_train/chosen': '-166.43', 'loss/train': '0.5586', 'examples_per_second': '91.187', 'grad_norm': '16.845', 'counters/examples': 47360, 'counters/updates': 740}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47488 examples: {'rewards_train/chosen': '-1.0631', 'rewards_train/rejected': '-1.5001', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.43695', 'logps_train/rejected': '-149.25', 'logps_train/chosen': '-165.12', 'loss/train': '0.64927', 'examples_per_second': '90.039', 'grad_norm': '19.322', 'counters/examples': 47488, 'counters/updates': 742}
skipping logging after 47552 examples to avoid logging too frequently
train stats after 47616 examples: {'rewards_train/chosen': '-0.83531', 'rewards_train/rejected': '-1.2685', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43322', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-159.79', 'loss/train': '0.61721', 'examples_per_second': '86.933', 'grad_norm': '18.034', 'counters/examples': 47616, 'counters/updates': 744}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47744 examples: {'rewards_train/chosen': '-0.84578', 'rewards_train/rejected': '-1.2189', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37311', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-135.82', 'loss/train': '0.6335', 'examples_per_second': '91.556', 'grad_norm': '16.58', 'counters/examples': 47744, 'counters/updates': 746}
skipping logging after 47808 examples to avoid logging too frequently
train stats after 47872 examples: {'rewards_train/chosen': '-0.81921', 'rewards_train/rejected': '-1.1406', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32142', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-125.61', 'loss/train': '0.61206', 'examples_per_second': '87.052', 'grad_norm': '16.995', 'counters/examples': 47872, 'counters/updates': 748}
Running evaluation after 47872 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.29it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.54it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.78it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.64it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.59it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.56it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.48it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.46it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.52it/s]
eval after 47872: {'rewards_eval/chosen': '-0.62503', 'rewards_eval/rejected': '-1.0656', 'rewards_eval/accuracies': '0.68359', 'rewards_eval/margins': '0.44062', 'logps_eval/rejected': '-131.8', 'logps_eval/chosen': '-147.05', 'loss/eval': '0.6306'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-47872...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-47872/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-47872/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-47872/scheduler.pt...
train stats after 47936 examples: {'rewards_train/chosen': '-0.80416', 'rewards_train/rejected': '-1.0592', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.25506', 'logps_train/rejected': '-140.82', 'logps_train/chosen': '-156.48', 'loss/train': '0.65073', 'examples_per_second': '71.976', 'grad_norm': '18.289', 'counters/examples': 47936, 'counters/updates': 749}
skipping logging after 48000 examples to avoid logging too frequently
train stats after 48064 examples: {'rewards_train/chosen': '-0.66708', 'rewards_train/rejected': '-1.1522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48512', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-153.58', 'loss/train': '0.576', 'examples_per_second': '94.529', 'grad_norm': '17.06', 'counters/examples': 48064, 'counters/updates': 751}
skipping logging after 48128 examples to avoid logging too frequently
train stats after 48192 examples: {'rewards_train/chosen': '-0.71136', 'rewards_train/rejected': '-1.2316', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.52028', 'logps_train/rejected': '-114.9', 'logps_train/chosen': '-131.08', 'loss/train': '0.55335', 'examples_per_second': '90.641', 'grad_norm': '14.68', 'counters/examples': 48192, 'counters/updates': 753}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48320 examples: {'rewards_train/chosen': '-0.89444', 'rewards_train/rejected': '-1.13', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23561', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-137.96', 'loss/train': '0.70653', 'examples_per_second': '89.094', 'grad_norm': '18.482', 'counters/examples': 48320, 'counters/updates': 755}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48448 examples: {'rewards_train/chosen': '-0.60921', 'rewards_train/rejected': '-1.0289', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.41971', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-145.37', 'loss/train': '0.60227', 'examples_per_second': '90.229', 'grad_norm': '16.593', 'counters/examples': 48448, 'counters/updates': 757}
skipping logging after 48512 examples to avoid logging too frequently
train stats after 48576 examples: {'rewards_train/chosen': '-0.67649', 'rewards_train/rejected': '-1.2804', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.60393', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-149.81', 'loss/train': '0.54238', 'examples_per_second': '90.704', 'grad_norm': '15.442', 'counters/examples': 48576, 'counters/updates': 759}
skipping logging after 48640 examples to avoid logging too frequently
train stats after 48704 examples: {'rewards_train/chosen': '-0.5149', 'rewards_train/rejected': '-1.101', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58615', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-137.18', 'loss/train': '0.54078', 'examples_per_second': '90.67', 'grad_norm': '14.774', 'counters/examples': 48704, 'counters/updates': 761}
skipping logging after 48768 examples to avoid logging too frequently
train stats after 48832 examples: {'rewards_train/chosen': '-0.72356', 'rewards_train/rejected': '-1.2577', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5341', 'logps_train/rejected': '-161.63', 'logps_train/chosen': '-170.97', 'loss/train': '0.57109', 'examples_per_second': '90.609', 'grad_norm': '17.245', 'counters/examples': 48832, 'counters/updates': 763}
skipping logging after 48896 examples to avoid logging too frequently
train stats after 48960 examples: {'rewards_train/chosen': '-0.49727', 'rewards_train/rejected': '-0.9738', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.47653', 'logps_train/rejected': '-122.74', 'logps_train/chosen': '-167.42', 'loss/train': '0.58414', 'examples_per_second': '90.666', 'grad_norm': '15.189', 'counters/examples': 48960, 'counters/updates': 765}
skipping logging after 49024 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '-0.3613', 'rewards_train/rejected': '-0.87095', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.50965', 'logps_train/rejected': '-120.75', 'logps_train/chosen': '-125.08', 'loss/train': '0.56284', 'examples_per_second': '92.469', 'grad_norm': '14.575', 'counters/examples': 49088, 'counters/updates': 767}
skipping logging after 49152 examples to avoid logging too frequently
train stats after 49216 examples: {'rewards_train/chosen': '-0.45872', 'rewards_train/rejected': '-0.96116', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.50243', 'logps_train/rejected': '-157.14', 'logps_train/chosen': '-180.38', 'loss/train': '0.57992', 'examples_per_second': '90.707', 'grad_norm': '16.632', 'counters/examples': 49216, 'counters/updates': 769}
skipping logging after 49280 examples to avoid logging too frequently
train stats after 49344 examples: {'rewards_train/chosen': '-0.69043', 'rewards_train/rejected': '-1.3913', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.70082', 'logps_train/rejected': '-132.08', 'logps_train/chosen': '-151.96', 'loss/train': '0.53418', 'examples_per_second': '90.876', 'grad_norm': '15.375', 'counters/examples': 49344, 'counters/updates': 771}
skipping logging after 49408 examples to avoid logging too frequently
train stats after 49472 examples: {'rewards_train/chosen': '-0.9011', 'rewards_train/rejected': '-1.2147', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.31356', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-158.02', 'loss/train': '0.62706', 'examples_per_second': '95.343', 'grad_norm': '17.741', 'counters/examples': 49472, 'counters/updates': 773}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49600 examples: {'rewards_train/chosen': '-0.5972', 'rewards_train/rejected': '-1.1165', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.51931', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-148.89', 'loss/train': '0.56466', 'examples_per_second': '90.335', 'grad_norm': '15.443', 'counters/examples': 49600, 'counters/updates': 775}
skipping logging after 49664 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '-0.82519', 'rewards_train/rejected': '-1.2081', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.38294', 'logps_train/rejected': '-144.27', 'logps_train/chosen': '-141.77', 'loss/train': '0.62856', 'examples_per_second': '90.982', 'grad_norm': '17.714', 'counters/examples': 49728, 'counters/updates': 777}
skipping logging after 49792 examples to avoid logging too frequently
train stats after 49856 examples: {'rewards_train/chosen': '-0.82218', 'rewards_train/rejected': '-1.0546', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.23244', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-139.58', 'loss/train': '0.67901', 'examples_per_second': '90.261', 'grad_norm': '17.202', 'counters/examples': 49856, 'counters/updates': 779}
skipping logging after 49920 examples to avoid logging too frequently
train stats after 49984 examples: {'rewards_train/chosen': '-0.71758', 'rewards_train/rejected': '-1.3382', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62061', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-137.47', 'loss/train': '0.5532', 'examples_per_second': '90.757', 'grad_norm': '14.66', 'counters/examples': 49984, 'counters/updates': 781}
skipping logging after 50048 examples to avoid logging too frequently
train stats after 50112 examples: {'rewards_train/chosen': '-0.63784', 'rewards_train/rejected': '-1.0282', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39034', 'logps_train/rejected': '-148.74', 'logps_train/chosen': '-162.23', 'loss/train': '0.62024', 'examples_per_second': '90.28', 'grad_norm': '19.136', 'counters/examples': 50112, 'counters/updates': 783}
skipping logging after 50176 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '-0.82143', 'rewards_train/rejected': '-1.2743', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.45286', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-138.21', 'loss/train': '0.56588', 'examples_per_second': '93.676', 'grad_norm': '15.263', 'counters/examples': 50240, 'counters/updates': 785}
skipping logging after 50304 examples to avoid logging too frequently
train stats after 50368 examples: {'rewards_train/chosen': '-0.70822', 'rewards_train/rejected': '-1.4405', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.7323', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-147.79', 'loss/train': '0.47822', 'examples_per_second': '90.977', 'grad_norm': '13.374', 'counters/examples': 50368, 'counters/updates': 787}
skipping logging after 50432 examples to avoid logging too frequently
train stats after 50496 examples: {'rewards_train/chosen': '-0.68267', 'rewards_train/rejected': '-1.1454', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.46275', 'logps_train/rejected': '-167.07', 'logps_train/chosen': '-159.24', 'loss/train': '0.60504', 'examples_per_second': '90.56', 'grad_norm': '15.535', 'counters/examples': 50496, 'counters/updates': 789}
skipping logging after 50560 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '-0.70528', 'rewards_train/rejected': '-1.393', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.68771', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-123.07', 'loss/train': '0.51109', 'examples_per_second': '96.022', 'grad_norm': '14.117', 'counters/examples': 50624, 'counters/updates': 791}
skipping logging after 50688 examples to avoid logging too frequently
train stats after 50752 examples: {'rewards_train/chosen': '-0.44467', 'rewards_train/rejected': '-0.99465', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.54998', 'logps_train/rejected': '-152.16', 'logps_train/chosen': '-138.33', 'loss/train': '0.54035', 'examples_per_second': '90.666', 'grad_norm': '15.267', 'counters/examples': 50752, 'counters/updates': 793}
skipping logging after 50816 examples to avoid logging too frequently
train stats after 50880 examples: {'rewards_train/chosen': '-0.96983', 'rewards_train/rejected': '-1.1652', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.19539', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-158.43', 'loss/train': '0.71826', 'examples_per_second': '90.718', 'grad_norm': '20.941', 'counters/examples': 50880, 'counters/updates': 795}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 51008 examples: {'rewards_train/chosen': '-0.8529', 'rewards_train/rejected': '-1.3008', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44793', 'logps_train/rejected': '-145.36', 'logps_train/chosen': '-158.71', 'loss/train': '0.59995', 'examples_per_second': '92.96', 'grad_norm': '16.677', 'counters/examples': 51008, 'counters/updates': 797}
skipping logging after 51072 examples to avoid logging too frequently
train stats after 51136 examples: {'rewards_train/chosen': '-0.79597', 'rewards_train/rejected': '-1.3128', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.51683', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-155.39', 'loss/train': '0.57064', 'examples_per_second': '90.658', 'grad_norm': '15.203', 'counters/examples': 51136, 'counters/updates': 799}
skipping logging after 51200 examples to avoid logging too frequently
train stats after 51264 examples: {'rewards_train/chosen': '-0.71133', 'rewards_train/rejected': '-1.415', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.70368', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-169.2', 'loss/train': '0.51171', 'examples_per_second': '90.566', 'grad_norm': '14.865', 'counters/examples': 51264, 'counters/updates': 801}
skipping logging after 51328 examples to avoid logging too frequently
train stats after 51392 examples: {'rewards_train/chosen': '-0.6216', 'rewards_train/rejected': '-1.157', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53535', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-156.71', 'loss/train': '0.5741', 'examples_per_second': '87.317', 'grad_norm': '15.761', 'counters/examples': 51392, 'counters/updates': 803}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51520 examples: {'rewards_train/chosen': '-0.71638', 'rewards_train/rejected': '-1.3062', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58982', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-151.15', 'loss/train': '0.5325', 'examples_per_second': '90.16', 'grad_norm': '15.076', 'counters/examples': 51520, 'counters/updates': 805}
skipping logging after 51584 examples to avoid logging too frequently
train stats after 51648 examples: {'rewards_train/chosen': '-0.65832', 'rewards_train/rejected': '-1.2256', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.56723', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-150.97', 'loss/train': '0.53377', 'examples_per_second': '90.493', 'grad_norm': '16.11', 'counters/examples': 51648, 'counters/updates': 807}
skipping logging after 51712 examples to avoid logging too frequently
train stats after 51776 examples: {'rewards_train/chosen': '-0.6996', 'rewards_train/rejected': '-1.1986', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49897', 'logps_train/rejected': '-142.11', 'logps_train/chosen': '-130.8', 'loss/train': '0.61039', 'examples_per_second': '90.045', 'grad_norm': '16.311', 'counters/examples': 51776, 'counters/updates': 809}
skipping logging after 51840 examples to avoid logging too frequently
train stats after 51904 examples: {'rewards_train/chosen': '-0.67244', 'rewards_train/rejected': '-1.161', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48857', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-143.6', 'loss/train': '0.60064', 'examples_per_second': '95.516', 'grad_norm': '15.991', 'counters/examples': 51904, 'counters/updates': 811}
skipping logging after 51968 examples to avoid logging too frequently
train stats after 52032 examples: {'rewards_train/chosen': '-0.7292', 'rewards_train/rejected': '-1.0986', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36944', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-163.16', 'loss/train': '0.63294', 'examples_per_second': '96.734', 'grad_norm': '18.411', 'counters/examples': 52032, 'counters/updates': 813}
skipping logging after 52096 examples to avoid logging too frequently
train stats after 52160 examples: {'rewards_train/chosen': '-0.55974', 'rewards_train/rejected': '-1.1381', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57835', 'logps_train/rejected': '-156.07', 'logps_train/chosen': '-145.88', 'loss/train': '0.53557', 'examples_per_second': '90.043', 'grad_norm': '15.243', 'counters/examples': 52160, 'counters/updates': 815}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52288 examples: {'rewards_train/chosen': '-0.65804', 'rewards_train/rejected': '-1.1802', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.52215', 'logps_train/rejected': '-153.77', 'logps_train/chosen': '-176.02', 'loss/train': '0.5742', 'examples_per_second': '89.868', 'grad_norm': '18.7', 'counters/examples': 52288, 'counters/updates': 817}
skipping logging after 52352 examples to avoid logging too frequently
train stats after 52416 examples: {'rewards_train/chosen': '-0.48512', 'rewards_train/rejected': '-1.1605', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.67542', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-149.8', 'loss/train': '0.51533', 'examples_per_second': '89.875', 'grad_norm': '13.336', 'counters/examples': 52416, 'counters/updates': 819}
skipping logging after 52480 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '-0.62519', 'rewards_train/rejected': '-1.2557', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63054', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-129.99', 'loss/train': '0.54295', 'examples_per_second': '87.928', 'grad_norm': '15.498', 'counters/examples': 52544, 'counters/updates': 821}
skipping logging after 52608 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '-0.43307', 'rewards_train/rejected': '-1.0276', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.59457', 'logps_train/rejected': '-142.55', 'logps_train/chosen': '-139.84', 'loss/train': '0.57466', 'examples_per_second': '87.255', 'grad_norm': '16.29', 'counters/examples': 52672, 'counters/updates': 823}
skipping logging after 52736 examples to avoid logging too frequently
train stats after 52800 examples: {'rewards_train/chosen': '-0.39849', 'rewards_train/rejected': '-0.98059', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5821', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-159.67', 'loss/train': '0.55311', 'examples_per_second': '90.085', 'grad_norm': '16.438', 'counters/examples': 52800, 'counters/updates': 825}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52928 examples: {'rewards_train/chosen': '-0.84678', 'rewards_train/rejected': '-1.0443', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19748', 'logps_train/rejected': '-160.11', 'logps_train/chosen': '-168.6', 'loss/train': '0.9261', 'examples_per_second': '87.352', 'grad_norm': '57.736', 'counters/examples': 52928, 'counters/updates': 827}
skipping logging after 52992 examples to avoid logging too frequently
train stats after 53056 examples: {'rewards_train/chosen': '-0.5953', 'rewards_train/rejected': '-1.0943', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49902', 'logps_train/rejected': '-131.06', 'logps_train/chosen': '-138.86', 'loss/train': '0.5737', 'examples_per_second': '87.902', 'grad_norm': '16.746', 'counters/examples': 53056, 'counters/updates': 829}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53184 examples: {'rewards_train/chosen': '-0.38828', 'rewards_train/rejected': '-0.89851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51024', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-115.98', 'loss/train': '0.58096', 'examples_per_second': '90.416', 'grad_norm': '15.116', 'counters/examples': 53184, 'counters/updates': 831}
skipping logging after 53248 examples to avoid logging too frequently
train stats after 53312 examples: {'rewards_train/chosen': '-0.78865', 'rewards_train/rejected': '-1.211', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.42235', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-149.14', 'loss/train': '0.65132', 'examples_per_second': '90.398', 'grad_norm': '18.57', 'counters/examples': 53312, 'counters/updates': 833}
skipping logging after 53376 examples to avoid logging too frequently
train stats after 53440 examples: {'rewards_train/chosen': '-0.58876', 'rewards_train/rejected': '-1.2261', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.63734', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-135.27', 'loss/train': '0.53373', 'examples_per_second': '90.484', 'grad_norm': '15.551', 'counters/examples': 53440, 'counters/updates': 835}
skipping logging after 53504 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '-0.8697', 'rewards_train/rejected': '-1.4846', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61491', 'logps_train/rejected': '-146.99', 'logps_train/chosen': '-146.72', 'loss/train': '0.55209', 'examples_per_second': '98.48', 'grad_norm': '17.144', 'counters/examples': 53568, 'counters/updates': 837}
skipping logging after 53632 examples to avoid logging too frequently
train stats after 53696 examples: {'rewards_train/chosen': '-0.93212', 'rewards_train/rejected': '-1.4128', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.48066', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-159.73', 'loss/train': '0.62657', 'examples_per_second': '90.49', 'grad_norm': '18.958', 'counters/examples': 53696, 'counters/updates': 839}
skipping logging after 53760 examples to avoid logging too frequently
train stats after 53824 examples: {'rewards_train/chosen': '-0.74985', 'rewards_train/rejected': '-1.4863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.73647', 'logps_train/rejected': '-179.05', 'logps_train/chosen': '-190.36', 'loss/train': '0.52603', 'examples_per_second': '90.481', 'grad_norm': '18.41', 'counters/examples': 53824, 'counters/updates': 841}
skipping logging after 53888 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '-0.92875', 'rewards_train/rejected': '-1.4847', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.55591', 'logps_train/rejected': '-142.01', 'logps_train/chosen': '-171.87', 'loss/train': '0.57674', 'examples_per_second': '90.777', 'grad_norm': '17.453', 'counters/examples': 53952, 'counters/updates': 843}
skipping logging after 54016 examples to avoid logging too frequently
train stats after 54080 examples: {'rewards_train/chosen': '-0.76418', 'rewards_train/rejected': '-1.1596', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39539', 'logps_train/rejected': '-157.17', 'logps_train/chosen': '-153.64', 'loss/train': '0.62701', 'examples_per_second': '90.526', 'grad_norm': '18.057', 'counters/examples': 54080, 'counters/updates': 845}
skipping logging after 54144 examples to avoid logging too frequently
train stats after 54208 examples: {'rewards_train/chosen': '-0.91464', 'rewards_train/rejected': '-1.1261', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.21143', 'logps_train/rejected': '-155.07', 'logps_train/chosen': '-134.98', 'loss/train': '0.71743', 'examples_per_second': '90.504', 'grad_norm': '19.676', 'counters/examples': 54208, 'counters/updates': 847}
skipping logging after 54272 examples to avoid logging too frequently
train stats after 54336 examples: {'rewards_train/chosen': '-0.76876', 'rewards_train/rejected': '-1.2203', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.45154', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-153.91', 'loss/train': '0.63116', 'examples_per_second': '90.646', 'grad_norm': '18.314', 'counters/examples': 54336, 'counters/updates': 849}
skipping logging after 54400 examples to avoid logging too frequently
train stats after 54464 examples: {'rewards_train/chosen': '-0.71228', 'rewards_train/rejected': '-1.1179', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.40565', 'logps_train/rejected': '-130.03', 'logps_train/chosen': '-187.14', 'loss/train': '0.62207', 'examples_per_second': '90.464', 'grad_norm': '17.971', 'counters/examples': 54464, 'counters/updates': 851}
skipping logging after 54528 examples to avoid logging too frequently
train stats after 54592 examples: {'rewards_train/chosen': '-0.67899', 'rewards_train/rejected': '-1.1859', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50687', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-183.15', 'loss/train': '0.60365', 'examples_per_second': '90.978', 'grad_norm': '18.675', 'counters/examples': 54592, 'counters/updates': 853}
skipping logging after 54656 examples to avoid logging too frequently
train stats after 54720 examples: {'rewards_train/chosen': '-0.99385', 'rewards_train/rejected': '-1.4158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42192', 'logps_train/rejected': '-144.56', 'logps_train/chosen': '-152.64', 'loss/train': '0.61098', 'examples_per_second': '87.175', 'grad_norm': '17.594', 'counters/examples': 54720, 'counters/updates': 855}
skipping logging after 54784 examples to avoid logging too frequently
train stats after 54848 examples: {'rewards_train/chosen': '-0.65581', 'rewards_train/rejected': '-1.2072', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.55134', 'logps_train/rejected': '-164.31', 'logps_train/chosen': '-147.8', 'loss/train': '0.57613', 'examples_per_second': '89.514', 'grad_norm': '16.463', 'counters/examples': 54848, 'counters/updates': 857}
skipping logging after 54912 examples to avoid logging too frequently
train stats after 54976 examples: {'rewards_train/chosen': '-0.83369', 'rewards_train/rejected': '-1.2231', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38937', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-144.26', 'loss/train': '0.62835', 'examples_per_second': '90.769', 'grad_norm': '18.456', 'counters/examples': 54976, 'counters/updates': 859}
skipping logging after 55040 examples to avoid logging too frequently
train stats after 55104 examples: {'rewards_train/chosen': '-0.8075', 'rewards_train/rejected': '-1.377', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56948', 'logps_train/rejected': '-111.57', 'logps_train/chosen': '-114.31', 'loss/train': '0.53566', 'examples_per_second': '91.152', 'grad_norm': '13.79', 'counters/examples': 55104, 'counters/updates': 861}
skipping logging after 55168 examples to avoid logging too frequently
train stats after 55232 examples: {'rewards_train/chosen': '-0.82521', 'rewards_train/rejected': '-1.1972', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37198', 'logps_train/rejected': '-144.67', 'logps_train/chosen': '-173.16', 'loss/train': '0.62215', 'examples_per_second': '90.624', 'grad_norm': '17.226', 'counters/examples': 55232, 'counters/updates': 863}
skipping logging after 55296 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '-0.6105', 'rewards_train/rejected': '-1.2989', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.68839', 'logps_train/rejected': '-144.82', 'logps_train/chosen': '-135.5', 'loss/train': '0.53986', 'examples_per_second': '87.5', 'grad_norm': '15.552', 'counters/examples': 55360, 'counters/updates': 865}
skipping logging after 55424 examples to avoid logging too frequently
train stats after 55488 examples: {'rewards_train/chosen': '-0.59872', 'rewards_train/rejected': '-1.2335', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.63482', 'logps_train/rejected': '-168.49', 'logps_train/chosen': '-183.98', 'loss/train': '0.55271', 'examples_per_second': '87.789', 'grad_norm': '18.047', 'counters/examples': 55488, 'counters/updates': 867}
skipping logging after 55552 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '-0.66004', 'rewards_train/rejected': '-1.191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53097', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-152.72', 'loss/train': '0.54395', 'examples_per_second': '87.929', 'grad_norm': '16.524', 'counters/examples': 55616, 'counters/updates': 869}
skipping logging after 55680 examples to avoid logging too frequently
train stats after 55744 examples: {'rewards_train/chosen': '-0.95741', 'rewards_train/rejected': '-1.3264', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.369', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-155.69', 'loss/train': '0.63385', 'examples_per_second': '90.891', 'grad_norm': '18.197', 'counters/examples': 55744, 'counters/updates': 871}
skipping logging after 55808 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '-0.74181', 'rewards_train/rejected': '-1.1542', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41239', 'logps_train/rejected': '-144.99', 'logps_train/chosen': '-155.06', 'loss/train': '0.58254', 'examples_per_second': '87.522', 'grad_norm': '16.411', 'counters/examples': 55872, 'counters/updates': 873}
skipping logging after 55936 examples to avoid logging too frequently
train stats after 56000 examples: {'rewards_train/chosen': '-0.73502', 'rewards_train/rejected': '-1.2069', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47191', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-159.26', 'loss/train': '0.58612', 'examples_per_second': '91.09', 'grad_norm': '18.521', 'counters/examples': 56000, 'counters/updates': 875}
skipping logging after 56064 examples to avoid logging too frequently
train stats after 56128 examples: {'rewards_train/chosen': '-0.74285', 'rewards_train/rejected': '-1.1595', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41667', 'logps_train/rejected': '-139.55', 'logps_train/chosen': '-157.96', 'loss/train': '0.63902', 'examples_per_second': '88.972', 'grad_norm': '17.725', 'counters/examples': 56128, 'counters/updates': 877}
skipping logging after 56192 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '-0.70744', 'rewards_train/rejected': '-1.0497', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34225', 'logps_train/rejected': '-168.21', 'logps_train/chosen': '-138.5', 'loss/train': '0.65738', 'examples_per_second': '90.169', 'grad_norm': '18.762', 'counters/examples': 56256, 'counters/updates': 879}
skipping logging after 56320 examples to avoid logging too frequently
train stats after 56384 examples: {'rewards_train/chosen': '-0.71119', 'rewards_train/rejected': '-1.2025', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49134', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-162.56', 'loss/train': '0.56383', 'examples_per_second': '90.867', 'grad_norm': '16.833', 'counters/examples': 56384, 'counters/updates': 881}
skipping logging after 56448 examples to avoid logging too frequently
train stats after 56512 examples: {'rewards_train/chosen': '-0.70082', 'rewards_train/rejected': '-1.0951', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39432', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-133.75', 'loss/train': '0.60967', 'examples_per_second': '89.128', 'grad_norm': '15.344', 'counters/examples': 56512, 'counters/updates': 883}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56640 examples: {'rewards_train/chosen': '-0.91769', 'rewards_train/rejected': '-1.2282', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31055', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-131.29', 'loss/train': '0.62851', 'examples_per_second': '96.355', 'grad_norm': '16.428', 'counters/examples': 56640, 'counters/updates': 885}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56768 examples: {'rewards_train/chosen': '-0.59973', 'rewards_train/rejected': '-1.0532', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.45346', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-150.44', 'loss/train': '0.59421', 'examples_per_second': '90.618', 'grad_norm': '16.408', 'counters/examples': 56768, 'counters/updates': 887}
skipping logging after 56832 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '-0.57084', 'rewards_train/rejected': '-1.1999', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.62902', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-138.33', 'loss/train': '0.50977', 'examples_per_second': '69.442', 'grad_norm': '14.135', 'counters/examples': 56896, 'counters/updates': 889}
skipping logging after 56960 examples to avoid logging too frequently
train stats after 57024 examples: {'rewards_train/chosen': '-0.50372', 'rewards_train/rejected': '-1.2436', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.73985', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-145.39', 'loss/train': '0.51763', 'examples_per_second': '88.482', 'grad_norm': '15.38', 'counters/examples': 57024, 'counters/updates': 891}
skipping logging after 57088 examples to avoid logging too frequently
train stats after 57152 examples: {'rewards_train/chosen': '-0.60881', 'rewards_train/rejected': '-1.183', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.57416', 'logps_train/rejected': '-153.56', 'logps_train/chosen': '-132.49', 'loss/train': '0.55649', 'examples_per_second': '90.285', 'grad_norm': '15.412', 'counters/examples': 57152, 'counters/updates': 893}
skipping logging after 57216 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '-0.70182', 'rewards_train/rejected': '-1.1451', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.44332', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-146.78', 'loss/train': '0.60156', 'examples_per_second': '91.176', 'grad_norm': '17.418', 'counters/examples': 57280, 'counters/updates': 895}
skipping logging after 57344 examples to avoid logging too frequently
train stats after 57408 examples: {'rewards_train/chosen': '-0.59208', 'rewards_train/rejected': '-1.1816', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58949', 'logps_train/rejected': '-160.59', 'logps_train/chosen': '-163.78', 'loss/train': '0.60602', 'examples_per_second': '90.732', 'grad_norm': '17.73', 'counters/examples': 57408, 'counters/updates': 897}
skipping logging after 57472 examples to avoid logging too frequently
train stats after 57536 examples: {'rewards_train/chosen': '-0.59866', 'rewards_train/rejected': '-1.061', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46238', 'logps_train/rejected': '-139.65', 'logps_train/chosen': '-144.68', 'loss/train': '0.56596', 'examples_per_second': '91.976', 'grad_norm': '16.113', 'counters/examples': 57536, 'counters/updates': 899}
skipping logging after 57600 examples to avoid logging too frequently
train stats after 57664 examples: {'rewards_train/chosen': '-0.60271', 'rewards_train/rejected': '-0.95523', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35252', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-141.14', 'loss/train': '0.62878', 'examples_per_second': '89.328', 'grad_norm': '16.039', 'counters/examples': 57664, 'counters/updates': 901}
skipping logging after 57728 examples to avoid logging too frequently
train stats after 57792 examples: {'rewards_train/chosen': '-0.73153', 'rewards_train/rejected': '-1.0771', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34556', 'logps_train/rejected': '-164.54', 'logps_train/chosen': '-173.61', 'loss/train': '0.64744', 'examples_per_second': '90.879', 'grad_norm': '18.588', 'counters/examples': 57792, 'counters/updates': 903}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57920 examples: {'rewards_train/chosen': '-0.55381', 'rewards_train/rejected': '-1.1206', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.56681', 'logps_train/rejected': '-116.61', 'logps_train/chosen': '-134.46', 'loss/train': '0.54436', 'examples_per_second': '91.85', 'grad_norm': '14.789', 'counters/examples': 57920, 'counters/updates': 905}
skipping logging after 57984 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '-0.65539', 'rewards_train/rejected': '-1.2666', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.61119', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-127.67', 'loss/train': '0.54555', 'examples_per_second': '112.39', 'grad_norm': '15.187', 'counters/examples': 58048, 'counters/updates': 907}
skipping logging after 58112 examples to avoid logging too frequently
train stats after 58176 examples: {'rewards_train/chosen': '-0.64417', 'rewards_train/rejected': '-1.1749', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53076', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-155.3', 'loss/train': '0.58393', 'examples_per_second': '88.114', 'grad_norm': '15.596', 'counters/examples': 58176, 'counters/updates': 909}
skipping logging after 58240 examples to avoid logging too frequently
train stats after 58304 examples: {'rewards_train/chosen': '-0.69576', 'rewards_train/rejected': '-1.307', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.61121', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-149.22', 'loss/train': '0.52981', 'examples_per_second': '92.095', 'grad_norm': '16.747', 'counters/examples': 58304, 'counters/updates': 911}
skipping logging after 58368 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '-0.97122', 'rewards_train/rejected': '-1.6187', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.64745', 'logps_train/rejected': '-134.71', 'logps_train/chosen': '-133.26', 'loss/train': '0.51181', 'examples_per_second': '96.96', 'grad_norm': '15.493', 'counters/examples': 58432, 'counters/updates': 913}
skipping logging after 58496 examples to avoid logging too frequently
train stats after 58560 examples: {'rewards_train/chosen': '-0.61669', 'rewards_train/rejected': '-1.1279', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.51123', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-161.52', 'loss/train': '0.59101', 'examples_per_second': '89.554', 'grad_norm': '17.008', 'counters/examples': 58560, 'counters/updates': 915}
skipping logging after 58624 examples to avoid logging too frequently
train stats after 58688 examples: {'rewards_train/chosen': '-0.99362', 'rewards_train/rejected': '-1.3751', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38146', 'logps_train/rejected': '-151.69', 'logps_train/chosen': '-140.5', 'loss/train': '0.63764', 'examples_per_second': '86.474', 'grad_norm': '17.076', 'counters/examples': 58688, 'counters/updates': 917}
skipping logging after 58752 examples to avoid logging too frequently
train stats after 58816 examples: {'rewards_train/chosen': '-0.67198', 'rewards_train/rejected': '-1.1328', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.4608', 'logps_train/rejected': '-160.84', 'logps_train/chosen': '-157.7', 'loss/train': '0.59562', 'examples_per_second': '90.772', 'grad_norm': '16.726', 'counters/examples': 58816, 'counters/updates': 919}
skipping logging after 58880 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '-0.61936', 'rewards_train/rejected': '-1.478', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.85864', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-144.52', 'loss/train': '0.4576', 'examples_per_second': '93.782', 'grad_norm': '14.733', 'counters/examples': 58944, 'counters/updates': 921}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59072 examples: {'rewards_train/chosen': '-0.48532', 'rewards_train/rejected': '-1.0158', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.53052', 'logps_train/rejected': '-149.38', 'logps_train/chosen': '-134.49', 'loss/train': '0.60763', 'examples_per_second': '89.455', 'grad_norm': '16.957', 'counters/examples': 59072, 'counters/updates': 923}
skipping logging after 59136 examples to avoid logging too frequently
train stats after 59200 examples: {'rewards_train/chosen': '-0.49745', 'rewards_train/rejected': '-1.0032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50571', 'logps_train/rejected': '-146.08', 'logps_train/chosen': '-142.11', 'loss/train': '0.54566', 'examples_per_second': '91.023', 'grad_norm': '15.972', 'counters/examples': 59200, 'counters/updates': 925}
skipping logging after 59264 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '-0.46218', 'rewards_train/rejected': '-0.955', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.49282', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-122.74', 'loss/train': '0.61168', 'examples_per_second': '89.299', 'grad_norm': '17.158', 'counters/examples': 59328, 'counters/updates': 927}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59456 examples: {'rewards_train/chosen': '-0.44332', 'rewards_train/rejected': '-0.85802', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4147', 'logps_train/rejected': '-151.07', 'logps_train/chosen': '-159.44', 'loss/train': '0.58303', 'examples_per_second': '90.614', 'grad_norm': '18.221', 'counters/examples': 59456, 'counters/updates': 929}
skipping logging after 59520 examples to avoid logging too frequently
train stats after 59584 examples: {'rewards_train/chosen': '-0.55328', 'rewards_train/rejected': '-0.89667', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34339', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-139.8', 'loss/train': '0.64785', 'examples_per_second': '91.179', 'grad_norm': '19.666', 'counters/examples': 59584, 'counters/updates': 931}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59712 examples: {'rewards_train/chosen': '-0.46123', 'rewards_train/rejected': '-1.2247', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.76351', 'logps_train/rejected': '-132.4', 'logps_train/chosen': '-157.57', 'loss/train': '0.48823', 'examples_per_second': '91.659', 'grad_norm': '14.52', 'counters/examples': 59712, 'counters/updates': 933}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59840 examples: {'rewards_train/chosen': '-0.36101', 'rewards_train/rejected': '-0.81767', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.45666', 'logps_train/rejected': '-130.89', 'logps_train/chosen': '-138.29', 'loss/train': '0.61096', 'examples_per_second': '95.056', 'grad_norm': '17.37', 'counters/examples': 59840, 'counters/updates': 935}
Running evaluation after 59840 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.41it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.61it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.84it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.72it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.68it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.67it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.59it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.56it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.62it/s]
eval after 59840: {'rewards_eval/chosen': '-0.46961', 'rewards_eval/rejected': '-0.89364', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.42403', 'logps_eval/rejected': '-130.08', 'logps_eval/chosen': '-145.5', 'loss/eval': '0.62469'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-59840...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-59840/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-59840/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-59840/scheduler.pt...
train stats after 59904 examples: {'rewards_train/chosen': '-0.67682', 'rewards_train/rejected': '-1.0677', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39092', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-137.48', 'loss/train': '0.65535', 'examples_per_second': '67.921', 'grad_norm': '16.794', 'counters/examples': 59904, 'counters/updates': 936}
skipping logging after 59968 examples to avoid logging too frequently
train stats after 60032 examples: {'rewards_train/chosen': '-0.47072', 'rewards_train/rejected': '-1.1698', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.69911', 'logps_train/rejected': '-159.33', 'logps_train/chosen': '-167.57', 'loss/train': '0.52322', 'examples_per_second': '88.443', 'grad_norm': '17.967', 'counters/examples': 60032, 'counters/updates': 938}
skipping logging after 60096 examples to avoid logging too frequently
train stats after 60160 examples: {'rewards_train/chosen': '-0.39022', 'rewards_train/rejected': '-0.77922', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.389', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-160.8', 'loss/train': '0.62952', 'examples_per_second': '91.181', 'grad_norm': '18.867', 'counters/examples': 60160, 'counters/updates': 940}
skipping logging after 60224 examples to avoid logging too frequently
train stats after 60288 examples: {'rewards_train/chosen': '-0.33341', 'rewards_train/rejected': '-0.7204', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38699', 'logps_train/rejected': '-121.61', 'logps_train/chosen': '-130.85', 'loss/train': '0.61166', 'examples_per_second': '87.519', 'grad_norm': '15.9', 'counters/examples': 60288, 'counters/updates': 942}
skipping logging after 60352 examples to avoid logging too frequently
train stats after 60416 examples: {'rewards_train/chosen': '-0.57424', 'rewards_train/rejected': '-1.1709', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5967', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-152.5', 'loss/train': '0.56881', 'examples_per_second': '91.087', 'grad_norm': '16.761', 'counters/examples': 60416, 'counters/updates': 944}
skipping logging after 60480 examples to avoid logging too frequently
train stats after 60544 examples: {'rewards_train/chosen': '-0.50257', 'rewards_train/rejected': '-0.73362', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.23105', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-129.91', 'loss/train': '0.69397', 'examples_per_second': '91.948', 'grad_norm': '17.552', 'counters/examples': 60544, 'counters/updates': 946}
skipping logging after 60608 examples to avoid logging too frequently
train stats after 60672 examples: {'rewards_train/chosen': '-0.5014', 'rewards_train/rejected': '-0.98453', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48313', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-145.83', 'loss/train': '0.55384', 'examples_per_second': '88.652', 'grad_norm': '15.201', 'counters/examples': 60672, 'counters/updates': 948}
skipping logging after 60736 examples to avoid logging too frequently
train stats after 60800 examples: {'rewards_train/chosen': '-0.73593', 'rewards_train/rejected': '-1.1962', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.46022', 'logps_train/rejected': '-125.34', 'logps_train/chosen': '-132.65', 'loss/train': '0.63445', 'examples_per_second': '88.209', 'grad_norm': '16.541', 'counters/examples': 60800, 'counters/updates': 950}
skipping logging after 60864 examples to avoid logging too frequently
train stats after 60928 examples: {'rewards_train/chosen': '-0.83305', 'rewards_train/rejected': '-1.2032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37013', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-154.43', 'loss/train': '0.62799', 'examples_per_second': '90.657', 'grad_norm': '17.981', 'counters/examples': 60928, 'counters/updates': 952}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': '-0.8467', 'rewards_train/rejected': '-1.2287', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.38196', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-148.23', 'loss/train': '0.59626', 'examples_per_second': '105.12', 'grad_norm': '16.003', 'counters/examples': 61056, 'counters/updates': 954}
skipping logging after 61120 examples to avoid logging too frequently
train stats after 61184 examples: {'rewards_train/chosen': '-0.69369', 'rewards_train/rejected': '-1.2891', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59545', 'logps_train/rejected': '-157.17', 'logps_train/chosen': '-160.19', 'loss/train': '0.5715', 'examples_per_second': '87.739', 'grad_norm': '17.094', 'counters/examples': 61184, 'counters/updates': 956}
skipping logging after 61248 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '-0.7377', 'rewards_train/rejected': '-1.4356', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.69786', 'logps_train/rejected': '-135.02', 'logps_train/chosen': '-150.23', 'loss/train': '0.49488', 'examples_per_second': '88.396', 'grad_norm': '14.937', 'counters/examples': 61312, 'counters/updates': 958}
skipping logging after 61376 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '-0.66565', 'rewards_train/rejected': '-1.1264', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4608', 'logps_train/rejected': '-141.53', 'logps_train/chosen': '-132.46', 'loss/train': '0.58126', 'examples_per_second': '91.082', 'grad_norm': '15.916', 'counters/examples': 61440, 'counters/updates': 960}
skipping logging after 61504 examples to avoid logging too frequently
train stats after 61568 examples: {'rewards_train/chosen': '-0.6645', 'rewards_train/rejected': '-1.1719', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.50742', 'logps_train/rejected': '-161.09', 'logps_train/chosen': '-178.6', 'loss/train': '0.58728', 'examples_per_second': '91.196', 'grad_norm': '19.961', 'counters/examples': 61568, 'counters/updates': 962}
skipping logging after 61632 examples to avoid logging too frequently
train stats after 61696 examples: {'rewards_train/chosen': '-0.44554', 'rewards_train/rejected': '-0.99445', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54891', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-112.46', 'loss/train': '0.54455', 'examples_per_second': '90.963', 'grad_norm': '14.448', 'counters/examples': 61696, 'counters/updates': 964}
skipping logging after 61760 examples to avoid logging too frequently
train stats after 61824 examples: {'rewards_train/chosen': '-0.44515', 'rewards_train/rejected': '-1.0353', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59013', 'logps_train/rejected': '-121.02', 'logps_train/chosen': '-154.58', 'loss/train': '0.56905', 'examples_per_second': '90.501', 'grad_norm': '16.873', 'counters/examples': 61824, 'counters/updates': 966}
skipping logging after 61888 examples to avoid logging too frequently
train stats after 61952 examples: {'rewards_train/chosen': '-0.44773', 'rewards_train/rejected': '-0.99415', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54642', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-159.08', 'loss/train': '0.54171', 'examples_per_second': '90.232', 'grad_norm': '17.155', 'counters/examples': 61952, 'counters/updates': 968}
skipping logging after 62016 examples to avoid logging too frequently
train stats after 62080 examples: {'rewards_train/chosen': '-0.52331', 'rewards_train/rejected': '-1.0918', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.56852', 'logps_train/rejected': '-140.18', 'logps_train/chosen': '-170.12', 'loss/train': '0.56534', 'examples_per_second': '86.306', 'grad_norm': '18.237', 'counters/examples': 62080, 'counters/updates': 970}
skipping logging after 62144 examples to avoid logging too frequently
train stats after 62208 examples: {'rewards_train/chosen': '-0.66987', 'rewards_train/rejected': '-0.93477', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2649', 'logps_train/rejected': '-148.15', 'logps_train/chosen': '-146.59', 'loss/train': '0.66223', 'examples_per_second': '87.831', 'grad_norm': '18.882', 'counters/examples': 62208, 'counters/updates': 972}
skipping logging after 62272 examples to avoid logging too frequently
train stats after 62336 examples: {'rewards_train/chosen': '-0.63655', 'rewards_train/rejected': '-1.2022', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56568', 'logps_train/rejected': '-153.29', 'logps_train/chosen': '-142.97', 'loss/train': '0.55606', 'examples_per_second': '91.089', 'grad_norm': '15.629', 'counters/examples': 62336, 'counters/updates': 974}
skipping logging after 62400 examples to avoid logging too frequently
train stats after 62464 examples: {'rewards_train/chosen': '-0.62381', 'rewards_train/rejected': '-1.0469', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.4231', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-167.09', 'loss/train': '0.59368', 'examples_per_second': '91.013', 'grad_norm': '17.243', 'counters/examples': 62464, 'counters/updates': 976}
skipping logging after 62528 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '-0.68152', 'rewards_train/rejected': '-1.1302', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44869', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-161.6', 'loss/train': '0.57708', 'examples_per_second': '91.142', 'grad_norm': '17.41', 'counters/examples': 62592, 'counters/updates': 978}
skipping logging after 62656 examples to avoid logging too frequently
train stats after 62720 examples: {'rewards_train/chosen': '-0.65012', 'rewards_train/rejected': '-1.2581', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.60794', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-136.04', 'loss/train': '0.57036', 'examples_per_second': '89.814', 'grad_norm': '15.757', 'counters/examples': 62720, 'counters/updates': 980}
skipping logging after 62784 examples to avoid logging too frequently
train stats after 62848 examples: {'rewards_train/chosen': '-0.75365', 'rewards_train/rejected': '-1.2362', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48258', 'logps_train/rejected': '-160.06', 'logps_train/chosen': '-121.15', 'loss/train': '0.61916', 'examples_per_second': '87.321', 'grad_norm': '18.589', 'counters/examples': 62848, 'counters/updates': 982}
skipping logging after 62912 examples to avoid logging too frequently
train stats after 62976 examples: {'rewards_train/chosen': '-0.59023', 'rewards_train/rejected': '-1.1414', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.55117', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-161.08', 'loss/train': '0.60314', 'examples_per_second': '90.874', 'grad_norm': '17.576', 'counters/examples': 62976, 'counters/updates': 984}
skipping logging after 63040 examples to avoid logging too frequently
train stats after 63104 examples: {'rewards_train/chosen': '-0.56861', 'rewards_train/rejected': '-1.0376', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46903', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-140.98', 'loss/train': '0.63023', 'examples_per_second': '89.564', 'grad_norm': '18.046', 'counters/examples': 63104, 'counters/updates': 986}
skipping logging after 63168 examples to avoid logging too frequently
train stats after 63232 examples: {'rewards_train/chosen': '-0.44273', 'rewards_train/rejected': '-1.0291', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.58642', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-139.67', 'loss/train': '0.5288', 'examples_per_second': '90.837', 'grad_norm': '15.335', 'counters/examples': 63232, 'counters/updates': 988}
skipping logging after 63296 examples to avoid logging too frequently
train stats after 63360 examples: {'rewards_train/chosen': '-0.48427', 'rewards_train/rejected': '-1.0643', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57999', 'logps_train/rejected': '-145.19', 'logps_train/chosen': '-133.92', 'loss/train': '0.61414', 'examples_per_second': '93.458', 'grad_norm': '16.218', 'counters/examples': 63360, 'counters/updates': 990}
skipping logging after 63424 examples to avoid logging too frequently
train stats after 63488 examples: {'rewards_train/chosen': '-0.336', 'rewards_train/rejected': '-0.65497', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.31897', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-134.44', 'loss/train': '0.65188', 'examples_per_second': '91.006', 'grad_norm': '17.027', 'counters/examples': 63488, 'counters/updates': 992}
skipping logging after 63552 examples to avoid logging too frequently
train stats after 63616 examples: {'rewards_train/chosen': '-0.089285', 'rewards_train/rejected': '-0.58865', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49936', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-158.23', 'loss/train': '0.58115', 'examples_per_second': '90.555', 'grad_norm': '17.015', 'counters/examples': 63616, 'counters/updates': 994}
skipping logging after 63680 examples to avoid logging too frequently
train stats after 63744 examples: {'rewards_train/chosen': '-0.31806', 'rewards_train/rejected': '-0.91609', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.59803', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-173.81', 'loss/train': '0.52733', 'examples_per_second': '96.584', 'grad_norm': '16.262', 'counters/examples': 63744, 'counters/updates': 996}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63872 examples: {'rewards_train/chosen': '-0.55736', 'rewards_train/rejected': '-0.81589', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.25853', 'logps_train/rejected': '-137.85', 'logps_train/chosen': '-141.57', 'loss/train': '0.66316', 'examples_per_second': '90.871', 'grad_norm': '18.48', 'counters/examples': 63872, 'counters/updates': 998}
skipping logging after 63936 examples to avoid logging too frequently
train stats after 64000 examples: {'rewards_train/chosen': '-0.44967', 'rewards_train/rejected': '-0.91596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46629', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-164.47', 'loss/train': '0.61907', 'examples_per_second': '91.183', 'grad_norm': '19.149', 'counters/examples': 64000, 'counters/updates': 1000}
skipping logging after 64064 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '-0.84632', 'rewards_train/rejected': '-1.2803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43396', 'logps_train/rejected': '-136.52', 'logps_train/chosen': '-145.68', 'loss/train': '0.65648', 'examples_per_second': '90.835', 'grad_norm': '17.868', 'counters/examples': 64128, 'counters/updates': 1002}
skipping logging after 64192 examples to avoid logging too frequently
train stats after 64256 examples: {'rewards_train/chosen': '-0.57749', 'rewards_train/rejected': '-1.0683', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49083', 'logps_train/rejected': '-131.4', 'logps_train/chosen': '-134.14', 'loss/train': '0.55825', 'examples_per_second': '93.264', 'grad_norm': '14.684', 'counters/examples': 64256, 'counters/updates': 1004}
skipping logging after 64320 examples to avoid logging too frequently
train stats after 64384 examples: {'rewards_train/chosen': '-0.53885', 'rewards_train/rejected': '-0.79317', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.25432', 'logps_train/rejected': '-145.15', 'logps_train/chosen': '-149.31', 'loss/train': '0.72548', 'examples_per_second': '90.96', 'grad_norm': '19.589', 'counters/examples': 64384, 'counters/updates': 1006}
skipping logging after 64448 examples to avoid logging too frequently
train stats after 64512 examples: {'rewards_train/chosen': '-0.53591', 'rewards_train/rejected': '-1.0112', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.47527', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-120.56', 'loss/train': '0.54397', 'examples_per_second': '91.033', 'grad_norm': '13.726', 'counters/examples': 64512, 'counters/updates': 1008}
skipping logging after 64576 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '-0.52101', 'rewards_train/rejected': '-1.0206', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49961', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-139.45', 'loss/train': '0.58256', 'examples_per_second': '91.003', 'grad_norm': '15.234', 'counters/examples': 64640, 'counters/updates': 1010}
skipping logging after 64704 examples to avoid logging too frequently
train stats after 64768 examples: {'rewards_train/chosen': '-0.62573', 'rewards_train/rejected': '-1.3085', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.68281', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-135.9', 'loss/train': '0.49794', 'examples_per_second': '88.448', 'grad_norm': '13.943', 'counters/examples': 64768, 'counters/updates': 1012}
skipping logging after 64832 examples to avoid logging too frequently
train stats after 64896 examples: {'rewards_train/chosen': '-0.5455', 'rewards_train/rejected': '-1.0873', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54181', 'logps_train/rejected': '-168.54', 'logps_train/chosen': '-174.83', 'loss/train': '0.58594', 'examples_per_second': '89.424', 'grad_norm': '17.269', 'counters/examples': 64896, 'counters/updates': 1014}
skipping logging after 64960 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '-0.65258', 'rewards_train/rejected': '-1.1255', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47297', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-130.4', 'loss/train': '0.61305', 'examples_per_second': '88.875', 'grad_norm': '14.953', 'counters/examples': 65024, 'counters/updates': 1016}
skipping logging after 65088 examples to avoid logging too frequently
train stats after 65152 examples: {'rewards_train/chosen': '-0.40022', 'rewards_train/rejected': '-1.1245', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.7243', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-148.49', 'loss/train': '0.48005', 'examples_per_second': '91.21', 'grad_norm': '13.821', 'counters/examples': 65152, 'counters/updates': 1018}
skipping logging after 65216 examples to avoid logging too frequently
train stats after 65280 examples: {'rewards_train/chosen': '-0.76814', 'rewards_train/rejected': '-1.5705', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.8024', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-144.77', 'loss/train': '0.50606', 'examples_per_second': '90.865', 'grad_norm': '16.164', 'counters/examples': 65280, 'counters/updates': 1020}
skipping logging after 65344 examples to avoid logging too frequently
train stats after 65408 examples: {'rewards_train/chosen': '-0.99646', 'rewards_train/rejected': '-1.6952', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.69877', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-179.14', 'loss/train': '0.53052', 'examples_per_second': '91.023', 'grad_norm': '17.209', 'counters/examples': 65408, 'counters/updates': 1022}
skipping logging after 65472 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '-0.80063', 'rewards_train/rejected': '-1.2098', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.40918', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-147.35', 'loss/train': '0.61208', 'examples_per_second': '93.23', 'grad_norm': '15.757', 'counters/examples': 65536, 'counters/updates': 1024}
skipping logging after 65600 examples to avoid logging too frequently
train stats after 65664 examples: {'rewards_train/chosen': '-0.70405', 'rewards_train/rejected': '-1.3704', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.66632', 'logps_train/rejected': '-124.22', 'logps_train/chosen': '-162.15', 'loss/train': '0.56781', 'examples_per_second': '93.246', 'grad_norm': '15.172', 'counters/examples': 65664, 'counters/updates': 1026}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65792 examples: {'rewards_train/chosen': '-0.80828', 'rewards_train/rejected': '-1.202', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.39375', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-124.66', 'loss/train': '0.64118', 'examples_per_second': '91.251', 'grad_norm': '16.481', 'counters/examples': 65792, 'counters/updates': 1028}
skipping logging after 65856 examples to avoid logging too frequently
train stats after 65920 examples: {'rewards_train/chosen': '-0.66368', 'rewards_train/rejected': '-1.2839', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.62022', 'logps_train/rejected': '-134.99', 'logps_train/chosen': '-144.83', 'loss/train': '0.56971', 'examples_per_second': '90.752', 'grad_norm': '14.941', 'counters/examples': 65920, 'counters/updates': 1030}
skipping logging after 65984 examples to avoid logging too frequently
train stats after 66048 examples: {'rewards_train/chosen': '-0.83453', 'rewards_train/rejected': '-1.3236', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48904', 'logps_train/rejected': '-138.57', 'logps_train/chosen': '-160.04', 'loss/train': '0.59959', 'examples_per_second': '91.004', 'grad_norm': '18.42', 'counters/examples': 66048, 'counters/updates': 1032}
skipping logging after 66112 examples to avoid logging too frequently
train stats after 66176 examples: {'rewards_train/chosen': '-0.7686', 'rewards_train/rejected': '-1.0544', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.2858', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-141.31', 'loss/train': '0.66506', 'examples_per_second': '89.624', 'grad_norm': '17.168', 'counters/examples': 66176, 'counters/updates': 1034}
skipping logging after 66240 examples to avoid logging too frequently
train stats after 66304 examples: {'rewards_train/chosen': '-0.56288', 'rewards_train/rejected': '-1.2259', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.66301', 'logps_train/rejected': '-139', 'logps_train/chosen': '-150.04', 'loss/train': '0.56648', 'examples_per_second': '87.592', 'grad_norm': '16.118', 'counters/examples': 66304, 'counters/updates': 1036}
skipping logging after 66368 examples to avoid logging too frequently
train stats after 66432 examples: {'rewards_train/chosen': '-0.63632', 'rewards_train/rejected': '-1.0046', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36826', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-146.08', 'loss/train': '0.64239', 'examples_per_second': '87.875', 'grad_norm': '17.843', 'counters/examples': 66432, 'counters/updates': 1038}
skipping logging after 66496 examples to avoid logging too frequently
train stats after 66560 examples: {'rewards_train/chosen': '-0.46025', 'rewards_train/rejected': '-1.1333', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.67309', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-152.89', 'loss/train': '0.53427', 'examples_per_second': '89.018', 'grad_norm': '14.603', 'counters/examples': 66560, 'counters/updates': 1040}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66688 examples: {'rewards_train/chosen': '-0.66942', 'rewards_train/rejected': '-1.2999', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63048', 'logps_train/rejected': '-120.26', 'logps_train/chosen': '-141.04', 'loss/train': '0.5734', 'examples_per_second': '97.285', 'grad_norm': '16.522', 'counters/examples': 66688, 'counters/updates': 1042}
skipping logging after 66752 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '-0.61137', 'rewards_train/rejected': '-1.3601', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.74871', 'logps_train/rejected': '-183.2', 'logps_train/chosen': '-151.93', 'loss/train': '0.48135', 'examples_per_second': '90.985', 'grad_norm': '15.96', 'counters/examples': 66816, 'counters/updates': 1044}
skipping logging after 66880 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '-0.70475', 'rewards_train/rejected': '-1.2482', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54343', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-143.18', 'loss/train': '0.59125', 'examples_per_second': '96.915', 'grad_norm': '16.284', 'counters/examples': 66944, 'counters/updates': 1046}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67072 examples: {'rewards_train/chosen': '-0.3935', 'rewards_train/rejected': '-0.80303', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.40953', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-129.42', 'loss/train': '0.60615', 'examples_per_second': '90.867', 'grad_norm': '15.592', 'counters/examples': 67072, 'counters/updates': 1048}
skipping logging after 67136 examples to avoid logging too frequently
train stats after 67200 examples: {'rewards_train/chosen': '-0.50314', 'rewards_train/rejected': '-1.0073', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.5042', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-120.62', 'loss/train': '0.56238', 'examples_per_second': '90.623', 'grad_norm': '15.155', 'counters/examples': 67200, 'counters/updates': 1050}
skipping logging after 67264 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '-0.67489', 'rewards_train/rejected': '-0.96705', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29216', 'logps_train/rejected': '-140.36', 'logps_train/chosen': '-120.33', 'loss/train': '0.6684', 'examples_per_second': '93.069', 'grad_norm': '17.755', 'counters/examples': 67328, 'counters/updates': 1052}
skipping logging after 67392 examples to avoid logging too frequently
train stats after 67456 examples: {'rewards_train/chosen': '-0.37128', 'rewards_train/rejected': '-0.90142', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.53014', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-138.87', 'loss/train': '0.55369', 'examples_per_second': '90.16', 'grad_norm': '15.746', 'counters/examples': 67456, 'counters/updates': 1054}
skipping logging after 67520 examples to avoid logging too frequently
train stats after 67584 examples: {'rewards_train/chosen': '-0.65417', 'rewards_train/rejected': '-1.1009', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.44674', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-142.76', 'loss/train': '0.57777', 'examples_per_second': '89.558', 'grad_norm': '15.955', 'counters/examples': 67584, 'counters/updates': 1056}
skipping logging after 67648 examples to avoid logging too frequently
train stats after 67712 examples: {'rewards_train/chosen': '-0.47257', 'rewards_train/rejected': '-0.93968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46711', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-153.61', 'loss/train': '0.61914', 'examples_per_second': '87.961', 'grad_norm': '17.994', 'counters/examples': 67712, 'counters/updates': 1058}
skipping logging after 67776 examples to avoid logging too frequently
train stats after 67840 examples: {'rewards_train/chosen': '-0.85762', 'rewards_train/rejected': '-1.1689', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31129', 'logps_train/rejected': '-155.1', 'logps_train/chosen': '-143.58', 'loss/train': '0.65551', 'examples_per_second': '90.836', 'grad_norm': '18.401', 'counters/examples': 67840, 'counters/updates': 1060}
skipping logging after 67904 examples to avoid logging too frequently
train stats after 67968 examples: {'rewards_train/chosen': '-0.59561', 'rewards_train/rejected': '-0.9901', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39449', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-178.58', 'loss/train': '0.64939', 'examples_per_second': '90.867', 'grad_norm': '17.667', 'counters/examples': 67968, 'counters/updates': 1062}
skipping logging after 68032 examples to avoid logging too frequently
train stats after 68096 examples: {'rewards_train/chosen': '-0.49422', 'rewards_train/rejected': '-1.1067', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.61244', 'logps_train/rejected': '-144.68', 'logps_train/chosen': '-164.84', 'loss/train': '0.54363', 'examples_per_second': '87.443', 'grad_norm': '15.462', 'counters/examples': 68096, 'counters/updates': 1064}
skipping logging after 68160 examples to avoid logging too frequently
train stats after 68224 examples: {'rewards_train/chosen': '-0.61651', 'rewards_train/rejected': '-1.0915', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47503', 'logps_train/rejected': '-145.92', 'logps_train/chosen': '-144.2', 'loss/train': '0.57232', 'examples_per_second': '87.224', 'grad_norm': '16.838', 'counters/examples': 68224, 'counters/updates': 1066}
skipping logging after 68288 examples to avoid logging too frequently
train stats after 68352 examples: {'rewards_train/chosen': '-0.68457', 'rewards_train/rejected': '-1.2232', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.53864', 'logps_train/rejected': '-170.85', 'logps_train/chosen': '-160.17', 'loss/train': '0.58534', 'examples_per_second': '73.171', 'grad_norm': '18.593', 'counters/examples': 68352, 'counters/updates': 1068}
skipping logging after 68416 examples to avoid logging too frequently
train stats after 68480 examples: {'rewards_train/chosen': '-0.50839', 'rewards_train/rejected': '-0.82743', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.31904', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-116.92', 'loss/train': '0.6498', 'examples_per_second': '93.446', 'grad_norm': '16.852', 'counters/examples': 68480, 'counters/updates': 1070}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68608 examples: {'rewards_train/chosen': '-0.50166', 'rewards_train/rejected': '-0.94691', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44526', 'logps_train/rejected': '-148.81', 'logps_train/chosen': '-153.94', 'loss/train': '0.60683', 'examples_per_second': '90.512', 'grad_norm': '17.195', 'counters/examples': 68608, 'counters/updates': 1072}
skipping logging after 68672 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '-0.70758', 'rewards_train/rejected': '-1.2672', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55967', 'logps_train/rejected': '-143.48', 'logps_train/chosen': '-153.3', 'loss/train': '0.57319', 'examples_per_second': '90.912', 'grad_norm': '16.69', 'counters/examples': 68736, 'counters/updates': 1074}
skipping logging after 68800 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '-0.60466', 'rewards_train/rejected': '-1.1021', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49749', 'logps_train/rejected': '-144.22', 'logps_train/chosen': '-145.42', 'loss/train': '0.57542', 'examples_per_second': '88.257', 'grad_norm': '16', 'counters/examples': 68864, 'counters/updates': 1076}
skipping logging after 68928 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '-0.53759', 'rewards_train/rejected': '-0.68265', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.14507', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-148.66', 'loss/train': '0.74755', 'examples_per_second': '91.742', 'grad_norm': '19.874', 'counters/examples': 68992, 'counters/updates': 1078}
skipping logging after 69056 examples to avoid logging too frequently
train stats after 69120 examples: {'rewards_train/chosen': '-0.57193', 'rewards_train/rejected': '-0.98687', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41493', 'logps_train/rejected': '-152.17', 'logps_train/chosen': '-139.2', 'loss/train': '0.61679', 'examples_per_second': '87.705', 'grad_norm': '17.826', 'counters/examples': 69120, 'counters/updates': 1080}
skipping logging after 69184 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '-0.43122', 'rewards_train/rejected': '-1.1056', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.67435', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-123.93', 'loss/train': '0.50717', 'examples_per_second': '85.177', 'grad_norm': '13.746', 'counters/examples': 69248, 'counters/updates': 1082}
skipping logging after 69312 examples to avoid logging too frequently
train stats after 69376 examples: {'rewards_train/chosen': '-0.60411', 'rewards_train/rejected': '-1.1214', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.51733', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-136.56', 'loss/train': '0.57601', 'examples_per_second': '87.566', 'grad_norm': '15.12', 'counters/examples': 69376, 'counters/updates': 1084}
skipping logging after 69440 examples to avoid logging too frequently
train stats after 69504 examples: {'rewards_train/chosen': '-0.49028', 'rewards_train/rejected': '-0.90813', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41785', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-157.86', 'loss/train': '0.59139', 'examples_per_second': '89.034', 'grad_norm': '16.275', 'counters/examples': 69504, 'counters/updates': 1086}
skipping logging after 69568 examples to avoid logging too frequently
train stats after 69632 examples: {'rewards_train/chosen': '-0.49665', 'rewards_train/rejected': '-1.0945', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.59783', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-146.3', 'loss/train': '0.51745', 'examples_per_second': '96.172', 'grad_norm': '15.347', 'counters/examples': 69632, 'counters/updates': 1088}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69760 examples: {'rewards_train/chosen': '-0.78976', 'rewards_train/rejected': '-1.3633', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.57358', 'logps_train/rejected': '-159.53', 'logps_train/chosen': '-140.42', 'loss/train': '0.58512', 'examples_per_second': '92.014', 'grad_norm': '17.07', 'counters/examples': 69760, 'counters/updates': 1090}
skipping logging after 69824 examples to avoid logging too frequently
train stats after 69888 examples: {'rewards_train/chosen': '-0.75065', 'rewards_train/rejected': '-1.4119', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.66121', 'logps_train/rejected': '-147.79', 'logps_train/chosen': '-143.9', 'loss/train': '0.51822', 'examples_per_second': '90.759', 'grad_norm': '14.992', 'counters/examples': 69888, 'counters/updates': 1092}
skipping logging after 69952 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '-0.56559', 'rewards_train/rejected': '-0.83421', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.26862', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-121.82', 'loss/train': '0.66241', 'examples_per_second': '94.22', 'grad_norm': '17.937', 'counters/examples': 70016, 'counters/updates': 1094}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70144 examples: {'rewards_train/chosen': '-0.59301', 'rewards_train/rejected': '-1.1796', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58661', 'logps_train/rejected': '-120.52', 'logps_train/chosen': '-150.33', 'loss/train': '0.54588', 'examples_per_second': '87.775', 'grad_norm': '15.579', 'counters/examples': 70144, 'counters/updates': 1096}
skipping logging after 70208 examples to avoid logging too frequently
train stats after 70272 examples: {'rewards_train/chosen': '-0.58762', 'rewards_train/rejected': '-1.0375', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44991', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-139.3', 'loss/train': '0.57249', 'examples_per_second': '87.921', 'grad_norm': '15.92', 'counters/examples': 70272, 'counters/updates': 1098}
skipping logging after 70336 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '-0.73504', 'rewards_train/rejected': '-1.2802', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54519', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-145.17', 'loss/train': '0.60428', 'examples_per_second': '88.353', 'grad_norm': '17.431', 'counters/examples': 70400, 'counters/updates': 1100}
skipping logging after 70464 examples to avoid logging too frequently
train stats after 70528 examples: {'rewards_train/chosen': '-0.55048', 'rewards_train/rejected': '-1.3554', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.80491', 'logps_train/rejected': '-147.05', 'logps_train/chosen': '-164.39', 'loss/train': '0.50495', 'examples_per_second': '87.806', 'grad_norm': '14.956', 'counters/examples': 70528, 'counters/updates': 1102}
skipping logging after 70592 examples to avoid logging too frequently
train stats after 70656 examples: {'rewards_train/chosen': '-0.79716', 'rewards_train/rejected': '-1.2395', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.44233', 'logps_train/rejected': '-141.25', 'logps_train/chosen': '-168.4', 'loss/train': '0.61025', 'examples_per_second': '90.634', 'grad_norm': '19.215', 'counters/examples': 70656, 'counters/updates': 1104}
skipping logging after 70720 examples to avoid logging too frequently
train stats after 70784 examples: {'rewards_train/chosen': '-0.7808', 'rewards_train/rejected': '-1.3311', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55025', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-155.33', 'loss/train': '0.55208', 'examples_per_second': '91.089', 'grad_norm': '16.66', 'counters/examples': 70784, 'counters/updates': 1106}
skipping logging after 70848 examples to avoid logging too frequently
train stats after 70912 examples: {'rewards_train/chosen': '-0.86669', 'rewards_train/rejected': '-1.4501', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.58338', 'logps_train/rejected': '-137.77', 'logps_train/chosen': '-168.2', 'loss/train': '0.55317', 'examples_per_second': '90.323', 'grad_norm': '17.255', 'counters/examples': 70912, 'counters/updates': 1108}
skipping logging after 70976 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '-0.89457', 'rewards_train/rejected': '-1.5045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.60993', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-161.53', 'loss/train': '0.533', 'examples_per_second': '91.043', 'grad_norm': '16.061', 'counters/examples': 71040, 'counters/updates': 1110}
skipping logging after 71104 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '-1.0522', 'rewards_train/rejected': '-1.6533', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.60116', 'logps_train/rejected': '-149.35', 'logps_train/chosen': '-156.37', 'loss/train': '0.53152', 'examples_per_second': '101.04', 'grad_norm': '15.956', 'counters/examples': 71168, 'counters/updates': 1112}
skipping logging after 71232 examples to avoid logging too frequently
train stats after 71296 examples: {'rewards_train/chosen': '-0.75936', 'rewards_train/rejected': '-1.3414', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58205', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-166.73', 'loss/train': '0.54792', 'examples_per_second': '90.714', 'grad_norm': '14.642', 'counters/examples': 71296, 'counters/updates': 1114}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71424 examples: {'rewards_train/chosen': '-0.79691', 'rewards_train/rejected': '-1.2004', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40353', 'logps_train/rejected': '-149.73', 'logps_train/chosen': '-126.35', 'loss/train': '0.64775', 'examples_per_second': '88.072', 'grad_norm': '17.694', 'counters/examples': 71424, 'counters/updates': 1116}
skipping logging after 71488 examples to avoid logging too frequently
train stats after 71552 examples: {'rewards_train/chosen': '-0.83802', 'rewards_train/rejected': '-1.4562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.61814', 'logps_train/rejected': '-129.95', 'logps_train/chosen': '-131.28', 'loss/train': '0.58551', 'examples_per_second': '89.684', 'grad_norm': '14.921', 'counters/examples': 71552, 'counters/updates': 1118}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71680 examples: {'rewards_train/chosen': '-1.0122', 'rewards_train/rejected': '-1.6858', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.67352', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-155.02', 'loss/train': '0.55355', 'examples_per_second': '91.176', 'grad_norm': '17.48', 'counters/examples': 71680, 'counters/updates': 1120}
skipping logging after 71744 examples to avoid logging too frequently
train stats after 71808 examples: {'rewards_train/chosen': '-0.70125', 'rewards_train/rejected': '-1.0742', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37295', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-146.16', 'loss/train': '0.59579', 'examples_per_second': '89.081', 'grad_norm': '17.534', 'counters/examples': 71808, 'counters/updates': 1122}
Running evaluation after 71808 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.48it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.66it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.88it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.74it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.69it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.66it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.58it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.56it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.63it/s]
eval after 71808: {'rewards_eval/chosen': '-0.80787', 'rewards_eval/rejected': '-1.2708', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.46298', 'logps_eval/rejected': '-133.85', 'logps_eval/chosen': '-148.88', 'loss/eval': '0.63251'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-71808...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-71808/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-71808/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-71808/scheduler.pt...
train stats after 71872 examples: {'rewards_train/chosen': '-0.88137', 'rewards_train/rejected': '-1.318', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43662', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-186.47', 'loss/train': '0.59436', 'examples_per_second': '70.022', 'grad_norm': '18.119', 'counters/examples': 71872, 'counters/updates': 1123}
skipping logging after 71936 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '-0.95086', 'rewards_train/rejected': '-1.5572', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.60634', 'logps_train/rejected': '-162.85', 'logps_train/chosen': '-148.38', 'loss/train': '0.55826', 'examples_per_second': '94.693', 'grad_norm': '17.061', 'counters/examples': 72000, 'counters/updates': 1125}
skipping logging after 72064 examples to avoid logging too frequently
train stats after 72128 examples: {'rewards_train/chosen': '-0.97032', 'rewards_train/rejected': '-1.407', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.43664', 'logps_train/rejected': '-150.02', 'logps_train/chosen': '-170.47', 'loss/train': '0.61083', 'examples_per_second': '96.536', 'grad_norm': '18.396', 'counters/examples': 72128, 'counters/updates': 1127}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72256 examples: {'rewards_train/chosen': '-0.93864', 'rewards_train/rejected': '-1.6709', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.73221', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-147.71', 'loss/train': '0.46888', 'examples_per_second': '91.27', 'grad_norm': '13.874', 'counters/examples': 72256, 'counters/updates': 1129}
skipping logging after 72320 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '-1.0868', 'rewards_train/rejected': '-1.6783', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.59153', 'logps_train/rejected': '-165.15', 'logps_train/chosen': '-146.63', 'loss/train': '0.63833', 'examples_per_second': '94.107', 'grad_norm': '19.412', 'counters/examples': 72384, 'counters/updates': 1131}
skipping logging after 72448 examples to avoid logging too frequently
train stats after 72512 examples: {'rewards_train/chosen': '-1.0937', 'rewards_train/rejected': '-1.3156', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.2219', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-145.75', 'loss/train': '0.65629', 'examples_per_second': '91.28', 'grad_norm': '17.05', 'counters/examples': 72512, 'counters/updates': 1133}
skipping logging after 72576 examples to avoid logging too frequently
train stats after 72640 examples: {'rewards_train/chosen': '-0.90478', 'rewards_train/rejected': '-1.5849', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.68015', 'logps_train/rejected': '-140.51', 'logps_train/chosen': '-136.49', 'loss/train': '0.52631', 'examples_per_second': '95.278', 'grad_norm': '15.793', 'counters/examples': 72640, 'counters/updates': 1135}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72768 examples: {'rewards_train/chosen': '-0.8306', 'rewards_train/rejected': '-1.4677', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.6371', 'logps_train/rejected': '-146.41', 'logps_train/chosen': '-137.19', 'loss/train': '0.57865', 'examples_per_second': '91.072', 'grad_norm': '16.414', 'counters/examples': 72768, 'counters/updates': 1137}
skipping logging after 72832 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '-0.82329', 'rewards_train/rejected': '-1.2299', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40657', 'logps_train/rejected': '-156.03', 'logps_train/chosen': '-167.2', 'loss/train': '0.62128', 'examples_per_second': '93.695', 'grad_norm': '18.053', 'counters/examples': 72896, 'counters/updates': 1139}
skipping logging after 72960 examples to avoid logging too frequently
train stats after 73024 examples: {'rewards_train/chosen': '-0.7436', 'rewards_train/rejected': '-1.0219', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27828', 'logps_train/rejected': '-161.74', 'logps_train/chosen': '-150.91', 'loss/train': '0.69027', 'examples_per_second': '90.924', 'grad_norm': '20.411', 'counters/examples': 73024, 'counters/updates': 1141}
skipping logging after 73088 examples to avoid logging too frequently
train stats after 73152 examples: {'rewards_train/chosen': '-0.55211', 'rewards_train/rejected': '-1.2407', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.68864', 'logps_train/rejected': '-159.3', 'logps_train/chosen': '-156.56', 'loss/train': '0.50473', 'examples_per_second': '90.887', 'grad_norm': '16.852', 'counters/examples': 73152, 'counters/updates': 1143}
skipping logging after 73216 examples to avoid logging too frequently
train stats after 73280 examples: {'rewards_train/chosen': '-0.70813', 'rewards_train/rejected': '-1.2767', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.56855', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-129', 'loss/train': '0.56366', 'examples_per_second': '91.373', 'grad_norm': '17.511', 'counters/examples': 73280, 'counters/updates': 1145}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73408 examples: {'rewards_train/chosen': '-0.66789', 'rewards_train/rejected': '-1.2583', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.59036', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-145.26', 'loss/train': '0.51315', 'examples_per_second': '90.485', 'grad_norm': '16.06', 'counters/examples': 73408, 'counters/updates': 1147}
skipping logging after 73472 examples to avoid logging too frequently
train stats after 73536 examples: {'rewards_train/chosen': '-0.65678', 'rewards_train/rejected': '-1.1255', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.46867', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-148.06', 'loss/train': '0.59578', 'examples_per_second': '88.691', 'grad_norm': '17.542', 'counters/examples': 73536, 'counters/updates': 1149}
skipping logging after 73600 examples to avoid logging too frequently
train stats after 73664 examples: {'rewards_train/chosen': '-0.58548', 'rewards_train/rejected': '-1.1827', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.59723', 'logps_train/rejected': '-155.17', 'logps_train/chosen': '-152.34', 'loss/train': '0.57577', 'examples_per_second': '91.112', 'grad_norm': '17.84', 'counters/examples': 73664, 'counters/updates': 1151}
skipping logging after 73728 examples to avoid logging too frequently
train stats after 73792 examples: {'rewards_train/chosen': '-0.48259', 'rewards_train/rejected': '-1.1582', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.67562', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-138.93', 'loss/train': '0.51807', 'examples_per_second': '91.545', 'grad_norm': '15.696', 'counters/examples': 73792, 'counters/updates': 1153}
skipping logging after 73856 examples to avoid logging too frequently
train stats after 73920 examples: {'rewards_train/chosen': '-0.4165', 'rewards_train/rejected': '-1.1529', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.73638', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-150.61', 'loss/train': '0.48234', 'examples_per_second': '89.004', 'grad_norm': '15.062', 'counters/examples': 73920, 'counters/updates': 1155}
skipping logging after 73984 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '-0.57907', 'rewards_train/rejected': '-1.4074', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.82829', 'logps_train/rejected': '-140.24', 'logps_train/chosen': '-131.14', 'loss/train': '0.55063', 'examples_per_second': '100.14', 'grad_norm': '15.791', 'counters/examples': 74048, 'counters/updates': 1157}
skipping logging after 74112 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '-0.45597', 'rewards_train/rejected': '-1.0034', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54739', 'logps_train/rejected': '-107.35', 'logps_train/chosen': '-135.42', 'loss/train': '0.54922', 'examples_per_second': '90.134', 'grad_norm': '15.169', 'counters/examples': 74176, 'counters/updates': 1159}
skipping logging after 74240 examples to avoid logging too frequently
train stats after 74304 examples: {'rewards_train/chosen': '-0.59834', 'rewards_train/rejected': '-1.176', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.57763', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-154.84', 'loss/train': '0.59288', 'examples_per_second': '99.769', 'grad_norm': '16.989', 'counters/examples': 74304, 'counters/updates': 1161}
skipping logging after 74368 examples to avoid logging too frequently
train stats after 74432 examples: {'rewards_train/chosen': '-0.70113', 'rewards_train/rejected': '-1.1624', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4613', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-144.76', 'loss/train': '0.61774', 'examples_per_second': '88.702', 'grad_norm': '17.255', 'counters/examples': 74432, 'counters/updates': 1163}
skipping logging after 74496 examples to avoid logging too frequently
train stats after 74560 examples: {'rewards_train/chosen': '-0.56818', 'rewards_train/rejected': '-1.1903', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62215', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-123.15', 'loss/train': '0.55818', 'examples_per_second': '89.077', 'grad_norm': '15.088', 'counters/examples': 74560, 'counters/updates': 1165}
skipping logging after 74624 examples to avoid logging too frequently
train stats after 74688 examples: {'rewards_train/chosen': '-0.72314', 'rewards_train/rejected': '-1.2025', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.47931', 'logps_train/rejected': '-137.29', 'logps_train/chosen': '-139.7', 'loss/train': '0.64327', 'examples_per_second': '91.176', 'grad_norm': '16.886', 'counters/examples': 74688, 'counters/updates': 1167}
skipping logging after 74752 examples to avoid logging too frequently
train stats after 74816 examples: {'rewards_train/chosen': '-0.55653', 'rewards_train/rejected': '-0.97737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42084', 'logps_train/rejected': '-149.72', 'logps_train/chosen': '-135.31', 'loss/train': '0.63035', 'examples_per_second': '90.851', 'grad_norm': '18.225', 'counters/examples': 74816, 'counters/updates': 1169}
skipping logging after 74880 examples to avoid logging too frequently
train stats after 74944 examples: {'rewards_train/chosen': '-0.57183', 'rewards_train/rejected': '-1.1803', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.60845', 'logps_train/rejected': '-160.05', 'logps_train/chosen': '-136.67', 'loss/train': '0.55363', 'examples_per_second': '91.106', 'grad_norm': '17.423', 'counters/examples': 74944, 'counters/updates': 1171}
skipping logging after 75008 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '-0.62013', 'rewards_train/rejected': '-1.1965', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57641', 'logps_train/rejected': '-171.88', 'logps_train/chosen': '-152.79', 'loss/train': '0.57994', 'examples_per_second': '91.224', 'grad_norm': '19.083', 'counters/examples': 75072, 'counters/updates': 1173}
skipping logging after 75136 examples to avoid logging too frequently
train stats after 75200 examples: {'rewards_train/chosen': '-0.28478', 'rewards_train/rejected': '-0.92439', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63961', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-144.62', 'loss/train': '0.53212', 'examples_per_second': '88.759', 'grad_norm': '15.136', 'counters/examples': 75200, 'counters/updates': 1175}
skipping logging after 75264 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '-0.50578', 'rewards_train/rejected': '-0.99286', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48709', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-140.29', 'loss/train': '0.58156', 'examples_per_second': '88.274', 'grad_norm': '16.563', 'counters/examples': 75328, 'counters/updates': 1177}
skipping logging after 75392 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '-0.4704', 'rewards_train/rejected': '-1.1978', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.72738', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-156.86', 'loss/train': '0.54307', 'examples_per_second': '107.64', 'grad_norm': '17.048', 'counters/examples': 75456, 'counters/updates': 1179}
skipping logging after 75520 examples to avoid logging too frequently
train stats after 75584 examples: {'rewards_train/chosen': '-0.39278', 'rewards_train/rejected': '-1.1119', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.71912', 'logps_train/rejected': '-141', 'logps_train/chosen': '-122.32', 'loss/train': '0.50068', 'examples_per_second': '88.394', 'grad_norm': '14.459', 'counters/examples': 75584, 'counters/updates': 1181}
skipping logging after 75648 examples to avoid logging too frequently
train stats after 75712 examples: {'rewards_train/chosen': '-0.93265', 'rewards_train/rejected': '-1.4353', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.50266', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-121.66', 'loss/train': '0.63592', 'examples_per_second': '91.227', 'grad_norm': '17.996', 'counters/examples': 75712, 'counters/updates': 1183}
skipping logging after 75776 examples to avoid logging too frequently
train stats after 75840 examples: {'rewards_train/chosen': '-0.51569', 'rewards_train/rejected': '-0.97807', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46238', 'logps_train/rejected': '-146.51', 'logps_train/chosen': '-160.84', 'loss/train': '0.61765', 'examples_per_second': '91.291', 'grad_norm': '16.798', 'counters/examples': 75840, 'counters/updates': 1185}
skipping logging after 75904 examples to avoid logging too frequently
train stats after 75968 examples: {'rewards_train/chosen': '-0.53511', 'rewards_train/rejected': '-1.1551', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61998', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-141.42', 'loss/train': '0.57371', 'examples_per_second': '91.424', 'grad_norm': '16.482', 'counters/examples': 75968, 'counters/updates': 1187}
skipping logging after 76032 examples to avoid logging too frequently
train stats after 76096 examples: {'rewards_train/chosen': '-0.60194', 'rewards_train/rejected': '-1.0527', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.45076', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-143.93', 'loss/train': '0.58824', 'examples_per_second': '90.98', 'grad_norm': '17.592', 'counters/examples': 76096, 'counters/updates': 1189}
skipping logging after 76160 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '-0.63003', 'rewards_train/rejected': '-1.1272', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49721', 'logps_train/rejected': '-179.91', 'logps_train/chosen': '-168.23', 'loss/train': '0.61196', 'examples_per_second': '90.788', 'grad_norm': '20.264', 'counters/examples': 76224, 'counters/updates': 1191}
skipping logging after 76288 examples to avoid logging too frequently
train stats after 76352 examples: {'rewards_train/chosen': '-0.56513', 'rewards_train/rejected': '-1.0626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4975', 'logps_train/rejected': '-140.89', 'logps_train/chosen': '-168.44', 'loss/train': '0.56919', 'examples_per_second': '88.707', 'grad_norm': '17.443', 'counters/examples': 76352, 'counters/updates': 1193}
skipping logging after 76416 examples to avoid logging too frequently
train stats after 76480 examples: {'rewards_train/chosen': '-0.68623', 'rewards_train/rejected': '-1.1292', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.44301', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-151.43', 'loss/train': '0.6392', 'examples_per_second': '88.194', 'grad_norm': '17.974', 'counters/examples': 76480, 'counters/updates': 1195}
skipping logging after 76544 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '-0.42603', 'rewards_train/rejected': '-1.0835', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.6575', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-163.61', 'loss/train': '0.52435', 'examples_per_second': '108', 'grad_norm': '16.799', 'counters/examples': 76608, 'counters/updates': 1197}
skipping logging after 76672 examples to avoid logging too frequently
train stats after 76736 examples: {'rewards_train/chosen': '-0.54445', 'rewards_train/rejected': '-1.1244', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57995', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-139.82', 'loss/train': '0.5407', 'examples_per_second': '90.786', 'grad_norm': '14.938', 'counters/examples': 76736, 'counters/updates': 1199}
skipping logging after 76800 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '-0.67643', 'rewards_train/rejected': '-1.2653', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58885', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-130.35', 'loss/train': '0.55469', 'examples_per_second': '91.617', 'grad_norm': '15.142', 'counters/examples': 76864, 'counters/updates': 1201}
skipping logging after 76928 examples to avoid logging too frequently
train stats after 76992 examples: {'rewards_train/chosen': '-0.88822', 'rewards_train/rejected': '-1.605', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.71681', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-154.1', 'loss/train': '0.5703', 'examples_per_second': '87.612', 'grad_norm': '17.185', 'counters/examples': 76992, 'counters/updates': 1203}
skipping logging after 77056 examples to avoid logging too frequently
train stats after 77120 examples: {'rewards_train/chosen': '-0.77366', 'rewards_train/rejected': '-1.299', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52531', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-156.87', 'loss/train': '0.56344', 'examples_per_second': '97.22', 'grad_norm': '16.769', 'counters/examples': 77120, 'counters/updates': 1205}
skipping logging after 77184 examples to avoid logging too frequently
train stats after 77248 examples: {'rewards_train/chosen': '-0.69871', 'rewards_train/rejected': '-1.5853', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.88655', 'logps_train/rejected': '-151.98', 'logps_train/chosen': '-161.94', 'loss/train': '0.44675', 'examples_per_second': '87.673', 'grad_norm': '15.345', 'counters/examples': 77248, 'counters/updates': 1207}
skipping logging after 77312 examples to avoid logging too frequently
train stats after 77376 examples: {'rewards_train/chosen': '-0.68773', 'rewards_train/rejected': '-1.5846', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.89691', 'logps_train/rejected': '-147.76', 'logps_train/chosen': '-134.26', 'loss/train': '0.52489', 'examples_per_second': '92.256', 'grad_norm': '14.839', 'counters/examples': 77376, 'counters/updates': 1209}
skipping logging after 77440 examples to avoid logging too frequently
train stats after 77504 examples: {'rewards_train/chosen': '-0.94059', 'rewards_train/rejected': '-1.2846', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34405', 'logps_train/rejected': '-132.9', 'logps_train/chosen': '-125.58', 'loss/train': '0.6601', 'examples_per_second': '91.123', 'grad_norm': '15.773', 'counters/examples': 77504, 'counters/updates': 1211}
skipping logging after 77568 examples to avoid logging too frequently
train stats after 77632 examples: {'rewards_train/chosen': '-0.67511', 'rewards_train/rejected': '-1.2636', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58845', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-147.68', 'loss/train': '0.55117', 'examples_per_second': '88.632', 'grad_norm': '16.518', 'counters/examples': 77632, 'counters/updates': 1213}
skipping logging after 77696 examples to avoid logging too frequently
train stats after 77760 examples: {'rewards_train/chosen': '-0.63376', 'rewards_train/rejected': '-1.0545', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42075', 'logps_train/rejected': '-145.74', 'logps_train/chosen': '-153.9', 'loss/train': '0.6336', 'examples_per_second': '91.306', 'grad_norm': '17.112', 'counters/examples': 77760, 'counters/updates': 1215}
skipping logging after 77824 examples to avoid logging too frequently
train stats after 77888 examples: {'rewards_train/chosen': '-0.53497', 'rewards_train/rejected': '-1.1727', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.63778', 'logps_train/rejected': '-124.58', 'logps_train/chosen': '-137.2', 'loss/train': '0.54978', 'examples_per_second': '88.261', 'grad_norm': '14.591', 'counters/examples': 77888, 'counters/updates': 1217}
skipping logging after 77952 examples to avoid logging too frequently
train stats after 78016 examples: {'rewards_train/chosen': '-0.67182', 'rewards_train/rejected': '-1.1555', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48368', 'logps_train/rejected': '-130.76', 'logps_train/chosen': '-143.09', 'loss/train': '0.60149', 'examples_per_second': '87.66', 'grad_norm': '18.006', 'counters/examples': 78016, 'counters/updates': 1219}
skipping logging after 78080 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '-0.67867', 'rewards_train/rejected': '-1.2796', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.60097', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-152.22', 'loss/train': '0.5458', 'examples_per_second': '107.16', 'grad_norm': '15.58', 'counters/examples': 78144, 'counters/updates': 1221}
skipping logging after 78208 examples to avoid logging too frequently
train stats after 78272 examples: {'rewards_train/chosen': '-0.67988', 'rewards_train/rejected': '-1.4386', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.7587', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-148.9', 'loss/train': '0.52294', 'examples_per_second': '87.573', 'grad_norm': '14.291', 'counters/examples': 78272, 'counters/updates': 1223}
skipping logging after 78336 examples to avoid logging too frequently
train stats after 78400 examples: {'rewards_train/chosen': '-0.9221', 'rewards_train/rejected': '-1.4546', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.53246', 'logps_train/rejected': '-153.17', 'logps_train/chosen': '-141.86', 'loss/train': '0.58755', 'examples_per_second': '90.852', 'grad_norm': '16.38', 'counters/examples': 78400, 'counters/updates': 1225}
skipping logging after 78464 examples to avoid logging too frequently
train stats after 78528 examples: {'rewards_train/chosen': '-0.80877', 'rewards_train/rejected': '-1.5166', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.70779', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-146.57', 'loss/train': '0.56072', 'examples_per_second': '92.298', 'grad_norm': '15.416', 'counters/examples': 78528, 'counters/updates': 1227}
skipping logging after 78592 examples to avoid logging too frequently
train stats after 78656 examples: {'rewards_train/chosen': '-0.65471', 'rewards_train/rejected': '-1.4222', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.7675', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-161.73', 'loss/train': '0.45932', 'examples_per_second': '91.128', 'grad_norm': '14.998', 'counters/examples': 78656, 'counters/updates': 1229}
skipping logging after 78720 examples to avoid logging too frequently
train stats after 78784 examples: {'rewards_train/chosen': '-0.83558', 'rewards_train/rejected': '-1.5054', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.66981', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-155.59', 'loss/train': '0.54564', 'examples_per_second': '91.147', 'grad_norm': '15.986', 'counters/examples': 78784, 'counters/updates': 1231}
skipping logging after 78848 examples to avoid logging too frequently
train stats after 78912 examples: {'rewards_train/chosen': '-0.71428', 'rewards_train/rejected': '-1.4959', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.78158', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-142.87', 'loss/train': '0.56998', 'examples_per_second': '93.245', 'grad_norm': '15.829', 'counters/examples': 78912, 'counters/updates': 1233}
skipping logging after 78976 examples to avoid logging too frequently
train stats after 79040 examples: {'rewards_train/chosen': '-0.7598', 'rewards_train/rejected': '-0.99255', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.23275', 'logps_train/rejected': '-117.15', 'logps_train/chosen': '-137.5', 'loss/train': '0.7054', 'examples_per_second': '89.676', 'grad_norm': '18.927', 'counters/examples': 79040, 'counters/updates': 1235}
skipping logging after 79104 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '-0.56657', 'rewards_train/rejected': '-1.4169', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.85033', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-153.75', 'loss/train': '0.48806', 'examples_per_second': '92.057', 'grad_norm': '13.797', 'counters/examples': 79168, 'counters/updates': 1237}
skipping logging after 79232 examples to avoid logging too frequently
train stats after 79296 examples: {'rewards_train/chosen': '-0.66096', 'rewards_train/rejected': '-1.1219', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.4609', 'logps_train/rejected': '-147.57', 'logps_train/chosen': '-147.17', 'loss/train': '0.58816', 'examples_per_second': '91.151', 'grad_norm': '17.534', 'counters/examples': 79296, 'counters/updates': 1239}
skipping logging after 79360 examples to avoid logging too frequently
train stats after 79424 examples: {'rewards_train/chosen': '-0.64002', 'rewards_train/rejected': '-1.3965', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.75647', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-140.39', 'loss/train': '0.50199', 'examples_per_second': '90.207', 'grad_norm': '14.461', 'counters/examples': 79424, 'counters/updates': 1241}
skipping logging after 79488 examples to avoid logging too frequently
train stats after 79552 examples: {'rewards_train/chosen': '-0.70759', 'rewards_train/rejected': '-1.1095', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40195', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-122.19', 'loss/train': '0.61395', 'examples_per_second': '90.984', 'grad_norm': '15.849', 'counters/examples': 79552, 'counters/updates': 1243}
skipping logging after 79616 examples to avoid logging too frequently
train stats after 79680 examples: {'rewards_train/chosen': '-0.61046', 'rewards_train/rejected': '-1.1024', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49197', 'logps_train/rejected': '-139.05', 'logps_train/chosen': '-153.61', 'loss/train': '0.63156', 'examples_per_second': '91.225', 'grad_norm': '17.186', 'counters/examples': 79680, 'counters/updates': 1245}
skipping logging after 79744 examples to avoid logging too frequently
train stats after 79808 examples: {'rewards_train/chosen': '-0.7782', 'rewards_train/rejected': '-0.86304', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084845', 'logps_train/rejected': '-160.4', 'logps_train/chosen': '-161.17', 'loss/train': '0.77175', 'examples_per_second': '91.126', 'grad_norm': '21.754', 'counters/examples': 79808, 'counters/updates': 1247}
skipping logging after 79872 examples to avoid logging too frequently
train stats after 79936 examples: {'rewards_train/chosen': '-0.42128', 'rewards_train/rejected': '-1.128', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.70677', 'logps_train/rejected': '-148.11', 'logps_train/chosen': '-154.27', 'loss/train': '0.51275', 'examples_per_second': '91.102', 'grad_norm': '14.996', 'counters/examples': 79936, 'counters/updates': 1249}
skipping logging after 80000 examples to avoid logging too frequently
train stats after 80064 examples: {'rewards_train/chosen': '-0.64996', 'rewards_train/rejected': '-1.09', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44002', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-149.74', 'loss/train': '0.58079', 'examples_per_second': '87.415', 'grad_norm': '15.816', 'counters/examples': 80064, 'counters/updates': 1251}
skipping logging after 80128 examples to avoid logging too frequently
train stats after 80192 examples: {'rewards_train/chosen': '-0.56708', 'rewards_train/rejected': '-0.90178', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3347', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-122.33', 'loss/train': '0.62869', 'examples_per_second': '31.489', 'grad_norm': '16.933', 'counters/examples': 80192, 'counters/updates': 1253}
skipping logging after 80256 examples to avoid logging too frequently
train stats after 80320 examples: {'rewards_train/chosen': '-0.66008', 'rewards_train/rejected': '-1.2535', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.59337', 'logps_train/rejected': '-153.12', 'logps_train/chosen': '-135.19', 'loss/train': '0.54536', 'examples_per_second': '89.061', 'grad_norm': '16.187', 'counters/examples': 80320, 'counters/updates': 1255}
skipping logging after 80384 examples to avoid logging too frequently
train stats after 80448 examples: {'rewards_train/chosen': '-1.0091', 'rewards_train/rejected': '-1.3623', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3532', 'logps_train/rejected': '-137.72', 'logps_train/chosen': '-130.01', 'loss/train': '0.70065', 'examples_per_second': '91.136', 'grad_norm': '18.829', 'counters/examples': 80448, 'counters/updates': 1257}
skipping logging after 80512 examples to avoid logging too frequently
train stats after 80576 examples: {'rewards_train/chosen': '-0.48115', 'rewards_train/rejected': '-1.125', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64389', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-114.59', 'loss/train': '0.51652', 'examples_per_second': '88.687', 'grad_norm': '13.605', 'counters/examples': 80576, 'counters/updates': 1259}
skipping logging after 80640 examples to avoid logging too frequently
train stats after 80704 examples: {'rewards_train/chosen': '-0.73128', 'rewards_train/rejected': '-1.1981', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46687', 'logps_train/rejected': '-131.79', 'logps_train/chosen': '-155.14', 'loss/train': '0.60673', 'examples_per_second': '91.06', 'grad_norm': '17.418', 'counters/examples': 80704, 'counters/updates': 1261}
skipping logging after 80768 examples to avoid logging too frequently
train stats after 80832 examples: {'rewards_train/chosen': '-0.57119', 'rewards_train/rejected': '-1.5927', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '1.0215', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-150.9', 'loss/train': '0.44414', 'examples_per_second': '90.929', 'grad_norm': '14.772', 'counters/examples': 80832, 'counters/updates': 1263}
skipping logging after 80896 examples to avoid logging too frequently
train stats after 80960 examples: {'rewards_train/chosen': '-0.48487', 'rewards_train/rejected': '-1.0796', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59469', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-164.15', 'loss/train': '0.53293', 'examples_per_second': '85.228', 'grad_norm': '15.416', 'counters/examples': 80960, 'counters/updates': 1265}
skipping logging after 81024 examples to avoid logging too frequently
train stats after 81088 examples: {'rewards_train/chosen': '-0.60572', 'rewards_train/rejected': '-1.2377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63199', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-150.05', 'loss/train': '0.51343', 'examples_per_second': '90.867', 'grad_norm': '15.319', 'counters/examples': 81088, 'counters/updates': 1267}
skipping logging after 81152 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '-0.69629', 'rewards_train/rejected': '-1.4308', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.73448', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-150.61', 'loss/train': '0.53161', 'examples_per_second': '97.663', 'grad_norm': '15.119', 'counters/examples': 81216, 'counters/updates': 1269}
skipping logging after 81280 examples to avoid logging too frequently
train stats after 81344 examples: {'rewards_train/chosen': '-0.707', 'rewards_train/rejected': '-1.0733', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.36625', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-150.29', 'loss/train': '0.63231', 'examples_per_second': '88.249', 'grad_norm': '16.112', 'counters/examples': 81344, 'counters/updates': 1271}
skipping logging after 81408 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '-0.61662', 'rewards_train/rejected': '-1.2032', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.58656', 'logps_train/rejected': '-150.47', 'logps_train/chosen': '-167.36', 'loss/train': '0.55054', 'examples_per_second': '88.405', 'grad_norm': '16.846', 'counters/examples': 81472, 'counters/updates': 1273}
skipping logging after 81536 examples to avoid logging too frequently
train stats after 81600 examples: {'rewards_train/chosen': '-0.73258', 'rewards_train/rejected': '-1.0111', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.27854', 'logps_train/rejected': '-142.11', 'logps_train/chosen': '-114.51', 'loss/train': '0.67087', 'examples_per_second': '87.82', 'grad_norm': '17.182', 'counters/examples': 81600, 'counters/updates': 1275}
skipping logging after 81664 examples to avoid logging too frequently
train stats after 81728 examples: {'rewards_train/chosen': '-0.60609', 'rewards_train/rejected': '-1.1326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52652', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-141.58', 'loss/train': '0.54985', 'examples_per_second': '90.809', 'grad_norm': '16.517', 'counters/examples': 81728, 'counters/updates': 1277}
skipping logging after 81792 examples to avoid logging too frequently
train stats after 81856 examples: {'rewards_train/chosen': '-0.63196', 'rewards_train/rejected': '-1.183', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.55103', 'logps_train/rejected': '-124.51', 'logps_train/chosen': '-131.39', 'loss/train': '0.61825', 'examples_per_second': '90.897', 'grad_norm': '16.017', 'counters/examples': 81856, 'counters/updates': 1279}
skipping logging after 81920 examples to avoid logging too frequently
train stats after 81984 examples: {'rewards_train/chosen': '-0.79374', 'rewards_train/rejected': '-1.2095', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41579', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-161.66', 'loss/train': '0.66086', 'examples_per_second': '90.779', 'grad_norm': '18.922', 'counters/examples': 81984, 'counters/updates': 1281}
skipping logging after 82048 examples to avoid logging too frequently
train stats after 82112 examples: {'rewards_train/chosen': '-0.9055', 'rewards_train/rejected': '-1.2274', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32192', 'logps_train/rejected': '-111.84', 'logps_train/chosen': '-134.39', 'loss/train': '0.66943', 'examples_per_second': '93.525', 'grad_norm': '17.301', 'counters/examples': 82112, 'counters/updates': 1283}
skipping logging after 82176 examples to avoid logging too frequently
train stats after 82240 examples: {'rewards_train/chosen': '-0.5667', 'rewards_train/rejected': '-1.1933', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.62656', 'logps_train/rejected': '-120.03', 'logps_train/chosen': '-177.62', 'loss/train': '0.54087', 'examples_per_second': '94.512', 'grad_norm': '16.376', 'counters/examples': 82240, 'counters/updates': 1285}
skipping logging after 82304 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '-0.80044', 'rewards_train/rejected': '-1.4908', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.69036', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-154.01', 'loss/train': '0.54424', 'examples_per_second': '97.811', 'grad_norm': '16.291', 'counters/examples': 82368, 'counters/updates': 1287}
skipping logging after 82432 examples to avoid logging too frequently
train stats after 82496 examples: {'rewards_train/chosen': '-0.72904', 'rewards_train/rejected': '-1.2145', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48546', 'logps_train/rejected': '-151.6', 'logps_train/chosen': '-153.82', 'loss/train': '0.59433', 'examples_per_second': '91.344', 'grad_norm': '16.615', 'counters/examples': 82496, 'counters/updates': 1289}
skipping logging after 82560 examples to avoid logging too frequently
train stats after 82624 examples: {'rewards_train/chosen': '-0.75187', 'rewards_train/rejected': '-1.4536', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.70174', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-149.77', 'loss/train': '0.52639', 'examples_per_second': '91.092', 'grad_norm': '16.185', 'counters/examples': 82624, 'counters/updates': 1291}
skipping logging after 82688 examples to avoid logging too frequently
train stats after 82752 examples: {'rewards_train/chosen': '-1.1254', 'rewards_train/rejected': '-1.6265', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.50112', 'logps_train/rejected': '-143.38', 'logps_train/chosen': '-146.1', 'loss/train': '0.60341', 'examples_per_second': '89.087', 'grad_norm': '17.807', 'counters/examples': 82752, 'counters/updates': 1293}
skipping logging after 82816 examples to avoid logging too frequently
train stats after 82880 examples: {'rewards_train/chosen': '-0.87983', 'rewards_train/rejected': '-1.4085', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52868', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-150.83', 'loss/train': '0.59065', 'examples_per_second': '90.67', 'grad_norm': '17.004', 'counters/examples': 82880, 'counters/updates': 1295}
skipping logging after 82944 examples to avoid logging too frequently
train stats after 83008 examples: {'rewards_train/chosen': '-0.92526', 'rewards_train/rejected': '-1.4443', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.51902', 'logps_train/rejected': '-149.6', 'logps_train/chosen': '-152.96', 'loss/train': '0.57165', 'examples_per_second': '90.978', 'grad_norm': '16.234', 'counters/examples': 83008, 'counters/updates': 1297}
skipping logging after 83072 examples to avoid logging too frequently
train stats after 83136 examples: {'rewards_train/chosen': '-0.96813', 'rewards_train/rejected': '-1.2801', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.31201', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-134.93', 'loss/train': '0.62562', 'examples_per_second': '87.958', 'grad_norm': '16.653', 'counters/examples': 83136, 'counters/updates': 1299}
skipping logging after 83200 examples to avoid logging too frequently
train stats after 83264 examples: {'rewards_train/chosen': '-0.91102', 'rewards_train/rejected': '-1.3667', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45564', 'logps_train/rejected': '-132.92', 'logps_train/chosen': '-134.16', 'loss/train': '0.64973', 'examples_per_second': '91.051', 'grad_norm': '16.961', 'counters/examples': 83264, 'counters/updates': 1301}
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83392 examples: {'rewards_train/chosen': '-0.68607', 'rewards_train/rejected': '-1.0049', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31886', 'logps_train/rejected': '-95.242', 'logps_train/chosen': '-142.75', 'loss/train': '0.66683', 'examples_per_second': '90.726', 'grad_norm': '17.552', 'counters/examples': 83392, 'counters/updates': 1303}
skipping logging after 83456 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '-0.77033', 'rewards_train/rejected': '-1.4274', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.65704', 'logps_train/rejected': '-159.71', 'logps_train/chosen': '-148.05', 'loss/train': '0.52853', 'examples_per_second': '97.153', 'grad_norm': '15.862', 'counters/examples': 83520, 'counters/updates': 1305}
skipping logging after 83584 examples to avoid logging too frequently
train stats after 83648 examples: {'rewards_train/chosen': '-0.74878', 'rewards_train/rejected': '-1.3726', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.62378', 'logps_train/rejected': '-153.72', 'logps_train/chosen': '-144.18', 'loss/train': '0.56318', 'examples_per_second': '99.62', 'grad_norm': '17.4', 'counters/examples': 83648, 'counters/updates': 1307}
skipping logging after 83712 examples to avoid logging too frequently
train stats after 83776 examples: {'rewards_train/chosen': '-0.62806', 'rewards_train/rejected': '-1.193', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.5649', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-152.94', 'loss/train': '0.55443', 'examples_per_second': '88.578', 'grad_norm': '15.42', 'counters/examples': 83776, 'counters/updates': 1309}
Running evaluation after 83776 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.42it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.63it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.85it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.71it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.67it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.66it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.58it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.57it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.62it/s]
eval after 83776: {'rewards_eval/chosen': '-0.72372', 'rewards_eval/rejected': '-1.1215', 'rewards_eval/accuracies': '0.66406', 'rewards_eval/margins': '0.39774', 'logps_eval/rejected': '-132.36', 'logps_eval/chosen': '-148.04', 'loss/eval': '0.63401'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-83776...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-83776/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-83776/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-83776/scheduler.pt...
train stats after 83840 examples: {'rewards_train/chosen': '-0.77285', 'rewards_train/rejected': '-1.1876', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41474', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-113.22', 'loss/train': '0.57866', 'examples_per_second': '73.983', 'grad_norm': '14.181', 'counters/examples': 83840, 'counters/updates': 1310}
skipping logging after 83904 examples to avoid logging too frequently
train stats after 83968 examples: {'rewards_train/chosen': '-0.8803', 'rewards_train/rejected': '-1.322', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.4417', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-155.03', 'loss/train': '0.65364', 'examples_per_second': '90.192', 'grad_norm': '18.095', 'counters/examples': 83968, 'counters/updates': 1312}
skipping logging after 84032 examples to avoid logging too frequently
train stats after 84096 examples: {'rewards_train/chosen': '-0.77898', 'rewards_train/rejected': '-1.3343', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55533', 'logps_train/rejected': '-161.92', 'logps_train/chosen': '-164.25', 'loss/train': '0.5623', 'examples_per_second': '88.75', 'grad_norm': '17.222', 'counters/examples': 84096, 'counters/updates': 1314}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84224 examples: {'rewards_train/chosen': '-0.58605', 'rewards_train/rejected': '-1.1113', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5253', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-139.84', 'loss/train': '0.54479', 'examples_per_second': '91.146', 'grad_norm': '15.157', 'counters/examples': 84224, 'counters/updates': 1316}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84352 examples: {'rewards_train/chosen': '-0.56433', 'rewards_train/rejected': '-0.96023', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.3959', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-147.03', 'loss/train': '0.61704', 'examples_per_second': '91.042', 'grad_norm': '16.605', 'counters/examples': 84352, 'counters/updates': 1318}
skipping logging after 84416 examples to avoid logging too frequently
train stats after 84480 examples: {'rewards_train/chosen': '-0.49636', 'rewards_train/rejected': '-1.1572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.66083', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-149.18', 'loss/train': '0.5489', 'examples_per_second': '88.109', 'grad_norm': '16.209', 'counters/examples': 84480, 'counters/updates': 1320}
skipping logging after 84544 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '-0.4347', 'rewards_train/rejected': '-1.2379', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.80317', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-125.83', 'loss/train': '0.48535', 'examples_per_second': '91.387', 'grad_norm': '14.353', 'counters/examples': 84608, 'counters/updates': 1322}
train stats after 84672 examples: {'rewards_train/chosen': '-0.49324', 'rewards_train/rejected': '-1.0633', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.57003', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-132.22', 'loss/train': '0.58216', 'examples_per_second': '64.141', 'grad_norm': '15.464', 'counters/examples': 84672, 'counters/updates': 1323}
skipping logging after 84736 examples to avoid logging too frequently
train stats after 84800 examples: {'rewards_train/chosen': '-0.61649', 'rewards_train/rejected': '-1.0467', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43024', 'logps_train/rejected': '-121.87', 'logps_train/chosen': '-138.54', 'loss/train': '0.61805', 'examples_per_second': '89.627', 'grad_norm': '16.729', 'counters/examples': 84800, 'counters/updates': 1325}
skipping logging after 84864 examples to avoid logging too frequently
train stats after 84928 examples: {'rewards_train/chosen': '-0.54194', 'rewards_train/rejected': '-1.2664', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.72447', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-143.73', 'loss/train': '0.56814', 'examples_per_second': '90.907', 'grad_norm': '16.094', 'counters/examples': 84928, 'counters/updates': 1327}
skipping logging after 84992 examples to avoid logging too frequently
train stats after 85056 examples: {'rewards_train/chosen': '-0.85637', 'rewards_train/rejected': '-1.3786', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52221', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-151.22', 'loss/train': '0.57119', 'examples_per_second': '91.115', 'grad_norm': '16.372', 'counters/examples': 85056, 'counters/updates': 1329}
skipping logging after 85120 examples to avoid logging too frequently
train stats after 85184 examples: {'rewards_train/chosen': '-0.98239', 'rewards_train/rejected': '-1.3375', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.35514', 'logps_train/rejected': '-146.76', 'logps_train/chosen': '-129.94', 'loss/train': '0.62577', 'examples_per_second': '91.042', 'grad_norm': '16.669', 'counters/examples': 85184, 'counters/updates': 1331}
skipping logging after 85248 examples to avoid logging too frequently
train stats after 85312 examples: {'rewards_train/chosen': '-0.80509', 'rewards_train/rejected': '-1.2986', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49349', 'logps_train/rejected': '-146.84', 'logps_train/chosen': '-159.49', 'loss/train': '0.6351', 'examples_per_second': '87.214', 'grad_norm': '17.501', 'counters/examples': 85312, 'counters/updates': 1333}
skipping logging after 85376 examples to avoid logging too frequently
train stats after 85440 examples: {'rewards_train/chosen': '-0.40011', 'rewards_train/rejected': '-1.1498', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.74973', 'logps_train/rejected': '-114.28', 'logps_train/chosen': '-154.18', 'loss/train': '0.47628', 'examples_per_second': '91.282', 'grad_norm': '14.025', 'counters/examples': 85440, 'counters/updates': 1335}
skipping logging after 85504 examples to avoid logging too frequently
train stats after 85568 examples: {'rewards_train/chosen': '-0.74965', 'rewards_train/rejected': '-1.2547', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50501', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-137.63', 'loss/train': '0.57922', 'examples_per_second': '91.023', 'grad_norm': '15.311', 'counters/examples': 85568, 'counters/updates': 1337}
skipping logging after 85632 examples to avoid logging too frequently
train stats after 85696 examples: {'rewards_train/chosen': '-0.51403', 'rewards_train/rejected': '-0.98823', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4742', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-176.23', 'loss/train': '0.61316', 'examples_per_second': '90.992', 'grad_norm': '18.237', 'counters/examples': 85696, 'counters/updates': 1339}
skipping logging after 85760 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '-0.83879', 'rewards_train/rejected': '-1.4743', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.63554', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-146.26', 'loss/train': '0.51761', 'examples_per_second': '87.598', 'grad_norm': '14.952', 'counters/examples': 85824, 'counters/updates': 1341}
skipping logging after 85888 examples to avoid logging too frequently
train stats after 85952 examples: {'rewards_train/chosen': '-0.85585', 'rewards_train/rejected': '-1.6274', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.77157', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-159.26', 'loss/train': '0.5677', 'examples_per_second': '90.898', 'grad_norm': '17.69', 'counters/examples': 85952, 'counters/updates': 1343}
skipping logging after 86016 examples to avoid logging too frequently
train stats after 86080 examples: {'rewards_train/chosen': '-0.83337', 'rewards_train/rejected': '-1.3837', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.55033', 'logps_train/rejected': '-177.72', 'logps_train/chosen': '-171.17', 'loss/train': '0.57476', 'examples_per_second': '90.909', 'grad_norm': '18.068', 'counters/examples': 86080, 'counters/updates': 1345}
skipping logging after 86144 examples to avoid logging too frequently
train stats after 86208 examples: {'rewards_train/chosen': '-0.67693', 'rewards_train/rejected': '-1.456', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.77908', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-137.67', 'loss/train': '0.47888', 'examples_per_second': '94.124', 'grad_norm': '13.66', 'counters/examples': 86208, 'counters/updates': 1347}
skipping logging after 86272 examples to avoid logging too frequently
train stats after 86336 examples: {'rewards_train/chosen': '-0.98208', 'rewards_train/rejected': '-1.391', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.40896', 'logps_train/rejected': '-152.82', 'logps_train/chosen': '-119.08', 'loss/train': '0.65732', 'examples_per_second': '90.655', 'grad_norm': '17.599', 'counters/examples': 86336, 'counters/updates': 1349}
skipping logging after 86400 examples to avoid logging too frequently
train stats after 86464 examples: {'rewards_train/chosen': '-0.73855', 'rewards_train/rejected': '-1.2366', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49809', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-154.22', 'loss/train': '0.53529', 'examples_per_second': '90.696', 'grad_norm': '15.172', 'counters/examples': 86464, 'counters/updates': 1351}
skipping logging after 86528 examples to avoid logging too frequently
train stats after 86592 examples: {'rewards_train/chosen': '-0.76312', 'rewards_train/rejected': '-1.2144', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45127', 'logps_train/rejected': '-153.79', 'logps_train/chosen': '-141.35', 'loss/train': '0.60878', 'examples_per_second': '90.958', 'grad_norm': '17.684', 'counters/examples': 86592, 'counters/updates': 1353}
skipping logging after 86656 examples to avoid logging too frequently
train stats after 86720 examples: {'rewards_train/chosen': '-0.81029', 'rewards_train/rejected': '-1.1697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.3594', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-123.21', 'loss/train': '0.68084', 'examples_per_second': '87.742', 'grad_norm': '17.803', 'counters/examples': 86720, 'counters/updates': 1355}
skipping logging after 86784 examples to avoid logging too frequently
train stats after 86848 examples: {'rewards_train/chosen': '-0.92415', 'rewards_train/rejected': '-1.5295', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.60539', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-152.9', 'loss/train': '0.56254', 'examples_per_second': '90.927', 'grad_norm': '17.147', 'counters/examples': 86848, 'counters/updates': 1357}
skipping logging after 86912 examples to avoid logging too frequently
train stats after 86976 examples: {'rewards_train/chosen': '-0.74415', 'rewards_train/rejected': '-1.2783', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.53413', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-153.75', 'loss/train': '0.56304', 'examples_per_second': '90.789', 'grad_norm': '16.165', 'counters/examples': 86976, 'counters/updates': 1359}
skipping logging after 87040 examples to avoid logging too frequently
train stats after 87104 examples: {'rewards_train/chosen': '-0.80278', 'rewards_train/rejected': '-1.3913', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58857', 'logps_train/rejected': '-147.29', 'logps_train/chosen': '-138.51', 'loss/train': '0.55296', 'examples_per_second': '91.048', 'grad_norm': '16.632', 'counters/examples': 87104, 'counters/updates': 1361}
skipping logging after 87168 examples to avoid logging too frequently
train stats after 87232 examples: {'rewards_train/chosen': '-0.69849', 'rewards_train/rejected': '-1.2376', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.53914', 'logps_train/rejected': '-140.29', 'logps_train/chosen': '-135.08', 'loss/train': '0.59778', 'examples_per_second': '94.453', 'grad_norm': '17.639', 'counters/examples': 87232, 'counters/updates': 1363}
skipping logging after 87296 examples to avoid logging too frequently
train stats after 87360 examples: {'rewards_train/chosen': '-0.67674', 'rewards_train/rejected': '-1.3818', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.70506', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-135.54', 'loss/train': '0.52955', 'examples_per_second': '87.79', 'grad_norm': '14.597', 'counters/examples': 87360, 'counters/updates': 1365}
skipping logging after 87424 examples to avoid logging too frequently
train stats after 87488 examples: {'rewards_train/chosen': '-0.51574', 'rewards_train/rejected': '-1.0306', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.51489', 'logps_train/rejected': '-146.03', 'logps_train/chosen': '-140.03', 'loss/train': '0.60847', 'examples_per_second': '94.975', 'grad_norm': '17.207', 'counters/examples': 87488, 'counters/updates': 1367}
skipping logging after 87552 examples to avoid logging too frequently
train stats after 87616 examples: {'rewards_train/chosen': '-0.61229', 'rewards_train/rejected': '-1.0701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45776', 'logps_train/rejected': '-150.14', 'logps_train/chosen': '-140.91', 'loss/train': '0.58436', 'examples_per_second': '90.317', 'grad_norm': '17.577', 'counters/examples': 87616, 'counters/updates': 1369}
skipping logging after 87680 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '-0.37521', 'rewards_train/rejected': '-0.94405', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56883', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-151.15', 'loss/train': '0.57992', 'examples_per_second': '93.953', 'grad_norm': '18.068', 'counters/examples': 87744, 'counters/updates': 1371}
skipping logging after 87808 examples to avoid logging too frequently
train stats after 87872 examples: {'rewards_train/chosen': '-0.55422', 'rewards_train/rejected': '-1.1314', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57716', 'logps_train/rejected': '-153.95', 'logps_train/chosen': '-140.53', 'loss/train': '0.54491', 'examples_per_second': '97.174', 'grad_norm': '16.349', 'counters/examples': 87872, 'counters/updates': 1373}
skipping logging after 87936 examples to avoid logging too frequently
train stats after 88000 examples: {'rewards_train/chosen': '-0.72588', 'rewards_train/rejected': '-0.94706', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.22118', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-118.18', 'loss/train': '0.69418', 'examples_per_second': '88.113', 'grad_norm': '17.756', 'counters/examples': 88000, 'counters/updates': 1375}
skipping logging after 88064 examples to avoid logging too frequently
train stats after 88128 examples: {'rewards_train/chosen': '-0.58042', 'rewards_train/rejected': '-0.98067', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.40025', 'logps_train/rejected': '-117.2', 'logps_train/chosen': '-131.68', 'loss/train': '0.59796', 'examples_per_second': '93.172', 'grad_norm': '15.042', 'counters/examples': 88128, 'counters/updates': 1377}
skipping logging after 88192 examples to avoid logging too frequently
train stats after 88256 examples: {'rewards_train/chosen': '-0.57345', 'rewards_train/rejected': '-1.2109', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.6375', 'logps_train/rejected': '-113.61', 'logps_train/chosen': '-136.06', 'loss/train': '0.53148', 'examples_per_second': '90.574', 'grad_norm': '14.361', 'counters/examples': 88256, 'counters/updates': 1379}
skipping logging after 88320 examples to avoid logging too frequently
train stats after 88384 examples: {'rewards_train/chosen': '-0.65737', 'rewards_train/rejected': '-1.1255', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46812', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-116.2', 'loss/train': '0.57743', 'examples_per_second': '87.423', 'grad_norm': '15.681', 'counters/examples': 88384, 'counters/updates': 1381}
skipping logging after 88448 examples to avoid logging too frequently
train stats after 88512 examples: {'rewards_train/chosen': '-0.52797', 'rewards_train/rejected': '-1.142', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.61407', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-168.36', 'loss/train': '0.51446', 'examples_per_second': '87.142', 'grad_norm': '16.528', 'counters/examples': 88512, 'counters/updates': 1383}
skipping logging after 88576 examples to avoid logging too frequently
train stats after 88640 examples: {'rewards_train/chosen': '-0.70064', 'rewards_train/rejected': '-1.1475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44689', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-147.15', 'loss/train': '0.59666', 'examples_per_second': '88.744', 'grad_norm': '17.399', 'counters/examples': 88640, 'counters/updates': 1385}
skipping logging after 88704 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '-0.65878', 'rewards_train/rejected': '-1.1221', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46331', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-132.31', 'loss/train': '0.56147', 'examples_per_second': '89.704', 'grad_norm': '16.633', 'counters/examples': 88768, 'counters/updates': 1387}
skipping logging after 88832 examples to avoid logging too frequently
train stats after 88896 examples: {'rewards_train/chosen': '-0.51057', 'rewards_train/rejected': '-1.2524', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.7418', 'logps_train/rejected': '-145.42', 'logps_train/chosen': '-143.63', 'loss/train': '0.50148', 'examples_per_second': '90.231', 'grad_norm': '14.796', 'counters/examples': 88896, 'counters/updates': 1389}
skipping logging after 88960 examples to avoid logging too frequently
train stats after 89024 examples: {'rewards_train/chosen': '-0.85449', 'rewards_train/rejected': '-1.4132', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5587', 'logps_train/rejected': '-134.3', 'logps_train/chosen': '-137.1', 'loss/train': '0.62996', 'examples_per_second': '90.646', 'grad_norm': '16.641', 'counters/examples': 89024, 'counters/updates': 1391}
skipping logging after 89088 examples to avoid logging too frequently
train stats after 89152 examples: {'rewards_train/chosen': '-0.68051', 'rewards_train/rejected': '-0.85896', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17844', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-148.34', 'loss/train': '0.691', 'examples_per_second': '88.504', 'grad_norm': '18.001', 'counters/examples': 89152, 'counters/updates': 1393}
skipping logging after 89216 examples to avoid logging too frequently
train stats after 89280 examples: {'rewards_train/chosen': '-0.73482', 'rewards_train/rejected': '-1.1484', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41357', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-133.32', 'loss/train': '0.5823', 'examples_per_second': '90.507', 'grad_norm': '14.283', 'counters/examples': 89280, 'counters/updates': 1395}
skipping logging after 89344 examples to avoid logging too frequently
train stats after 89408 examples: {'rewards_train/chosen': '-0.86979', 'rewards_train/rejected': '-1.2247', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.35486', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-163.65', 'loss/train': '0.63588', 'examples_per_second': '90.563', 'grad_norm': '18.525', 'counters/examples': 89408, 'counters/updates': 1397}
skipping logging after 89472 examples to avoid logging too frequently
train stats after 89536 examples: {'rewards_train/chosen': '-0.62494', 'rewards_train/rejected': '-1.033', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.40804', 'logps_train/rejected': '-148.86', 'logps_train/chosen': '-160.43', 'loss/train': '0.65079', 'examples_per_second': '90.679', 'grad_norm': '18.74', 'counters/examples': 89536, 'counters/updates': 1399}
skipping logging after 89600 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '-0.3511', 'rewards_train/rejected': '-0.95725', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60615', 'logps_train/rejected': '-147.32', 'logps_train/chosen': '-149.34', 'loss/train': '0.54591', 'examples_per_second': '94.891', 'grad_norm': '16.852', 'counters/examples': 89664, 'counters/updates': 1401}
skipping logging after 89728 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '-0.60728', 'rewards_train/rejected': '-0.99201', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38474', 'logps_train/rejected': '-151.91', 'logps_train/chosen': '-144.81', 'loss/train': '0.60548', 'examples_per_second': '90.49', 'grad_norm': '16.923', 'counters/examples': 89792, 'counters/updates': 1403}
skipping logging after 89856 examples to avoid logging too frequently
train stats after 89920 examples: {'rewards_train/chosen': '-0.55243', 'rewards_train/rejected': '-1.0201', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.46766', 'logps_train/rejected': '-152.32', 'logps_train/chosen': '-144.15', 'loss/train': '0.63465', 'examples_per_second': '87.881', 'grad_norm': '17.595', 'counters/examples': 89920, 'counters/updates': 1405}
skipping logging after 89984 examples to avoid logging too frequently
train stats after 90048 examples: {'rewards_train/chosen': '-0.53511', 'rewards_train/rejected': '-1.0322', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49711', 'logps_train/rejected': '-153.35', 'logps_train/chosen': '-147.16', 'loss/train': '0.58118', 'examples_per_second': '87.807', 'grad_norm': '15.604', 'counters/examples': 90048, 'counters/updates': 1407}
skipping logging after 90112 examples to avoid logging too frequently
train stats after 90176 examples: {'rewards_train/chosen': '-0.4803', 'rewards_train/rejected': '-1.2673', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.78704', 'logps_train/rejected': '-148.88', 'logps_train/chosen': '-150.98', 'loss/train': '0.50138', 'examples_per_second': '89.197', 'grad_norm': '14.975', 'counters/examples': 90176, 'counters/updates': 1409}
skipping logging after 90240 examples to avoid logging too frequently
train stats after 90304 examples: {'rewards_train/chosen': '-0.62695', 'rewards_train/rejected': '-1.1559', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52896', 'logps_train/rejected': '-152.1', 'logps_train/chosen': '-139.98', 'loss/train': '0.57962', 'examples_per_second': '88.045', 'grad_norm': '16.675', 'counters/examples': 90304, 'counters/updates': 1411}
skipping logging after 90368 examples to avoid logging too frequently
train stats after 90432 examples: {'rewards_train/chosen': '-0.8643', 'rewards_train/rejected': '-1.358', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.49369', 'logps_train/rejected': '-104.5', 'logps_train/chosen': '-126.36', 'loss/train': '0.55935', 'examples_per_second': '96.554', 'grad_norm': '13.552', 'counters/examples': 90432, 'counters/updates': 1413}
skipping logging after 90496 examples to avoid logging too frequently
train stats after 90560 examples: {'rewards_train/chosen': '-0.57729', 'rewards_train/rejected': '-1.0957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51846', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-160.19', 'loss/train': '0.59039', 'examples_per_second': '87.269', 'grad_norm': '16.98', 'counters/examples': 90560, 'counters/updates': 1415}
skipping logging after 90624 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '-0.68194', 'rewards_train/rejected': '-1.3337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.65176', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-131.42', 'loss/train': '0.53682', 'examples_per_second': '97.019', 'grad_norm': '14.126', 'counters/examples': 90688, 'counters/updates': 1417}
skipping logging after 90752 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '-0.61264', 'rewards_train/rejected': '-1.2443', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.63162', 'logps_train/rejected': '-157.18', 'logps_train/chosen': '-136.79', 'loss/train': '0.5211', 'examples_per_second': '90.395', 'grad_norm': '14.995', 'counters/examples': 90816, 'counters/updates': 1419}
skipping logging after 90880 examples to avoid logging too frequently
train stats after 90944 examples: {'rewards_train/chosen': '-0.78493', 'rewards_train/rejected': '-1.3486', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56371', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-165.89', 'loss/train': '0.57603', 'examples_per_second': '89.134', 'grad_norm': '17.541', 'counters/examples': 90944, 'counters/updates': 1421}
skipping logging after 91008 examples to avoid logging too frequently
train stats after 91072 examples: {'rewards_train/chosen': '-0.85576', 'rewards_train/rejected': '-1.2956', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43983', 'logps_train/rejected': '-136.79', 'logps_train/chosen': '-139.35', 'loss/train': '0.59014', 'examples_per_second': '93.594', 'grad_norm': '15.29', 'counters/examples': 91072, 'counters/updates': 1423}
skipping logging after 91136 examples to avoid logging too frequently
train stats after 91200 examples: {'rewards_train/chosen': '-0.64219', 'rewards_train/rejected': '-1.0672', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42505', 'logps_train/rejected': '-112.48', 'logps_train/chosen': '-125.91', 'loss/train': '0.58953', 'examples_per_second': '93.3', 'grad_norm': '15.794', 'counters/examples': 91200, 'counters/updates': 1425}
skipping logging after 91264 examples to avoid logging too frequently
train stats after 91328 examples: {'rewards_train/chosen': '-0.8269', 'rewards_train/rejected': '-1.209', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38207', 'logps_train/rejected': '-135.54', 'logps_train/chosen': '-142.42', 'loss/train': '0.67499', 'examples_per_second': '87.16', 'grad_norm': '18.263', 'counters/examples': 91328, 'counters/updates': 1427}
skipping logging after 91392 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '-0.7806', 'rewards_train/rejected': '-1.3414', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.56077', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-164.91', 'loss/train': '0.57477', 'examples_per_second': '90.468', 'grad_norm': '16.858', 'counters/examples': 91456, 'counters/updates': 1429}
skipping logging after 91520 examples to avoid logging too frequently
train stats after 91584 examples: {'rewards_train/chosen': '-0.92895', 'rewards_train/rejected': '-1.3071', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37811', 'logps_train/rejected': '-160.31', 'logps_train/chosen': '-151.17', 'loss/train': '0.64353', 'examples_per_second': '89.987', 'grad_norm': '18.066', 'counters/examples': 91584, 'counters/updates': 1431}
skipping logging after 91648 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '-0.59675', 'rewards_train/rejected': '-1.0839', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48712', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-140.4', 'loss/train': '0.57696', 'examples_per_second': '90.264', 'grad_norm': '16.041', 'counters/examples': 91712, 'counters/updates': 1433}
skipping logging after 91776 examples to avoid logging too frequently
train stats after 91840 examples: {'rewards_train/chosen': '-0.82037', 'rewards_train/rejected': '-1.2015', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.38112', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-162.59', 'loss/train': '0.63881', 'examples_per_second': '96.024', 'grad_norm': '18.129', 'counters/examples': 91840, 'counters/updates': 1435}
skipping logging after 91904 examples to avoid logging too frequently
train stats after 91968 examples: {'rewards_train/chosen': '-0.7463', 'rewards_train/rejected': '-1.1944', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.44813', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-146.86', 'loss/train': '0.59672', 'examples_per_second': '90.386', 'grad_norm': '16.324', 'counters/examples': 91968, 'counters/updates': 1437}
skipping logging after 92032 examples to avoid logging too frequently
train stats after 92096 examples: {'rewards_train/chosen': '-0.7878', 'rewards_train/rejected': '-1.2558', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46804', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-148.82', 'loss/train': '0.60386', 'examples_per_second': '92.726', 'grad_norm': '17.526', 'counters/examples': 92096, 'counters/updates': 1439}
skipping logging after 92160 examples to avoid logging too frequently
train stats after 92224 examples: {'rewards_train/chosen': '-0.78852', 'rewards_train/rejected': '-1.2987', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.51021', 'logps_train/rejected': '-156.23', 'logps_train/chosen': '-162.52', 'loss/train': '0.58262', 'examples_per_second': '90.33', 'grad_norm': '16.651', 'counters/examples': 92224, 'counters/updates': 1441}
skipping logging after 92288 examples to avoid logging too frequently
train stats after 92352 examples: {'rewards_train/chosen': '-0.9022', 'rewards_train/rejected': '-1.3993', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49706', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-186.93', 'loss/train': '0.61978', 'examples_per_second': '87.807', 'grad_norm': '19.126', 'counters/examples': 92352, 'counters/updates': 1443}
skipping logging after 92416 examples to avoid logging too frequently
train stats after 92480 examples: {'rewards_train/chosen': '-1.0055', 'rewards_train/rejected': '-1.584', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5785', 'logps_train/rejected': '-146', 'logps_train/chosen': '-158.62', 'loss/train': '0.57169', 'examples_per_second': '90.093', 'grad_norm': '18.039', 'counters/examples': 92480, 'counters/updates': 1445}
skipping logging after 92544 examples to avoid logging too frequently
train stats after 92608 examples: {'rewards_train/chosen': '-0.95489', 'rewards_train/rejected': '-1.6769', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.72202', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-157.78', 'loss/train': '0.5166', 'examples_per_second': '87.014', 'grad_norm': '14.497', 'counters/examples': 92608, 'counters/updates': 1447}
skipping logging after 92672 examples to avoid logging too frequently
train stats after 92736 examples: {'rewards_train/chosen': '-0.98169', 'rewards_train/rejected': '-1.3332', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35152', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-146.32', 'loss/train': '0.65706', 'examples_per_second': '93.711', 'grad_norm': '18.44', 'counters/examples': 92736, 'counters/updates': 1449}
skipping logging after 92800 examples to avoid logging too frequently
train stats after 92864 examples: {'rewards_train/chosen': '-0.93853', 'rewards_train/rejected': '-1.4574', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.51887', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-166.76', 'loss/train': '0.57147', 'examples_per_second': '87.494', 'grad_norm': '18.378', 'counters/examples': 92864, 'counters/updates': 1451}
skipping logging after 92928 examples to avoid logging too frequently
train stats after 92992 examples: {'rewards_train/chosen': '-0.92556', 'rewards_train/rejected': '-1.3799', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.45438', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-162.79', 'loss/train': '0.58853', 'examples_per_second': '88.769', 'grad_norm': '16.075', 'counters/examples': 92992, 'counters/updates': 1453}
skipping logging after 93056 examples to avoid logging too frequently
train stats after 93120 examples: {'rewards_train/chosen': '-0.95162', 'rewards_train/rejected': '-1.3372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3856', 'logps_train/rejected': '-172.11', 'logps_train/chosen': '-157.92', 'loss/train': '0.63484', 'examples_per_second': '88.217', 'grad_norm': '18.582', 'counters/examples': 93120, 'counters/updates': 1455}
skipping logging after 93184 examples to avoid logging too frequently
train stats after 93248 examples: {'rewards_train/chosen': '-1.1989', 'rewards_train/rejected': '-1.5439', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34502', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-158.96', 'loss/train': '0.70616', 'examples_per_second': '90.377', 'grad_norm': '19.842', 'counters/examples': 93248, 'counters/updates': 1457}
skipping logging after 93312 examples to avoid logging too frequently
train stats after 93376 examples: {'rewards_train/chosen': '-1.137', 'rewards_train/rejected': '-1.2616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12457', 'logps_train/rejected': '-161.28', 'logps_train/chosen': '-162.9', 'loss/train': '0.74171', 'examples_per_second': '90.254', 'grad_norm': '20.842', 'counters/examples': 93376, 'counters/updates': 1459}
skipping logging after 93440 examples to avoid logging too frequently
train stats after 93504 examples: {'rewards_train/chosen': '-0.98032', 'rewards_train/rejected': '-1.5636', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.5833', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-152.2', 'loss/train': '0.56743', 'examples_per_second': '90.313', 'grad_norm': '16.471', 'counters/examples': 93504, 'counters/updates': 1461}
skipping logging after 93568 examples to avoid logging too frequently
train stats after 93632 examples: {'rewards_train/chosen': '-1.016', 'rewards_train/rejected': '-1.4795', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46348', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-160.35', 'loss/train': '0.59092', 'examples_per_second': '87.002', 'grad_norm': '16.975', 'counters/examples': 93632, 'counters/updates': 1463}
skipping logging after 93696 examples to avoid logging too frequently
train stats after 93760 examples: {'rewards_train/chosen': '-1.2753', 'rewards_train/rejected': '-1.4474', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.17216', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-139.37', 'loss/train': '0.7028', 'examples_per_second': '90.221', 'grad_norm': '19.486', 'counters/examples': 93760, 'counters/updates': 1465}
skipping logging after 93824 examples to avoid logging too frequently
train stats after 93888 examples: {'rewards_train/chosen': '-0.86785', 'rewards_train/rejected': '-1.3149', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4471', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-162.77', 'loss/train': '0.60229', 'examples_per_second': '90.462', 'grad_norm': '16.192', 'counters/examples': 93888, 'counters/updates': 1467}
skipping logging after 93952 examples to avoid logging too frequently
train stats after 94016 examples: {'rewards_train/chosen': '-0.8638', 'rewards_train/rejected': '-1.3356', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47177', 'logps_train/rejected': '-151.37', 'logps_train/chosen': '-157.56', 'loss/train': '0.54305', 'examples_per_second': '87.076', 'grad_norm': '15.783', 'counters/examples': 94016, 'counters/updates': 1469}
skipping logging after 94080 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '-0.99523', 'rewards_train/rejected': '-1.5954', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.60014', 'logps_train/rejected': '-152.33', 'logps_train/chosen': '-175.3', 'loss/train': '0.54112', 'examples_per_second': '90.185', 'grad_norm': '17.116', 'counters/examples': 94144, 'counters/updates': 1471}
skipping logging after 94208 examples to avoid logging too frequently
train stats after 94272 examples: {'rewards_train/chosen': '-1.0174', 'rewards_train/rejected': '-1.5387', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52135', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-145.22', 'loss/train': '0.58625', 'examples_per_second': '86.701', 'grad_norm': '16.828', 'counters/examples': 94272, 'counters/updates': 1473}
skipping logging after 94336 examples to avoid logging too frequently
train stats after 94400 examples: {'rewards_train/chosen': '-1.0045', 'rewards_train/rejected': '-1.4342', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.42978', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-130.06', 'loss/train': '0.59309', 'examples_per_second': '94.708', 'grad_norm': '15.827', 'counters/examples': 94400, 'counters/updates': 1475}
skipping logging after 94464 examples to avoid logging too frequently
train stats after 94528 examples: {'rewards_train/chosen': '-0.73467', 'rewards_train/rejected': '-1.1903', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.45559', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-145.81', 'loss/train': '0.57655', 'examples_per_second': '90.005', 'grad_norm': '14.585', 'counters/examples': 94528, 'counters/updates': 1477}
skipping logging after 94592 examples to avoid logging too frequently
train stats after 94656 examples: {'rewards_train/chosen': '-0.9712', 'rewards_train/rejected': '-1.5483', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.57712', 'logps_train/rejected': '-158.97', 'logps_train/chosen': '-179.41', 'loss/train': '0.55585', 'examples_per_second': '87.507', 'grad_norm': '17.93', 'counters/examples': 94656, 'counters/updates': 1479}
skipping logging after 94720 examples to avoid logging too frequently
train stats after 94784 examples: {'rewards_train/chosen': '-0.984', 'rewards_train/rejected': '-1.3998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41581', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-141.46', 'loss/train': '0.56879', 'examples_per_second': '89.829', 'grad_norm': '17.119', 'counters/examples': 94784, 'counters/updates': 1481}
skipping logging after 94848 examples to avoid logging too frequently
train stats after 94912 examples: {'rewards_train/chosen': '-0.99804', 'rewards_train/rejected': '-1.4753', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47722', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-144.41', 'loss/train': '0.5596', 'examples_per_second': '90.185', 'grad_norm': '16.156', 'counters/examples': 94912, 'counters/updates': 1483}
skipping logging after 94976 examples to avoid logging too frequently
train stats after 95040 examples: {'rewards_train/chosen': '-0.89734', 'rewards_train/rejected': '-1.117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21964', 'logps_train/rejected': '-152.74', 'logps_train/chosen': '-140.02', 'loss/train': '0.69812', 'examples_per_second': '89.316', 'grad_norm': '19.159', 'counters/examples': 95040, 'counters/updates': 1485}
skipping logging after 95104 examples to avoid logging too frequently
train stats after 95168 examples: {'rewards_train/chosen': '-0.97267', 'rewards_train/rejected': '-1.5228', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.55008', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-140.71', 'loss/train': '0.58904', 'examples_per_second': '93.201', 'grad_norm': '16.643', 'counters/examples': 95168, 'counters/updates': 1487}
skipping logging after 95232 examples to avoid logging too frequently
train stats after 95296 examples: {'rewards_train/chosen': '-0.8599', 'rewards_train/rejected': '-1.2565', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.39656', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-135.87', 'loss/train': '0.60024', 'examples_per_second': '90.111', 'grad_norm': '15.817', 'counters/examples': 95296, 'counters/updates': 1489}
skipping logging after 95360 examples to avoid logging too frequently
train stats after 95424 examples: {'rewards_train/chosen': '-0.84864', 'rewards_train/rejected': '-1.3812', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.53253', 'logps_train/rejected': '-154.81', 'logps_train/chosen': '-155.34', 'loss/train': '0.54627', 'examples_per_second': '89.902', 'grad_norm': '16.268', 'counters/examples': 95424, 'counters/updates': 1491}
skipping logging after 95488 examples to avoid logging too frequently
train stats after 95552 examples: {'rewards_train/chosen': '-0.69284', 'rewards_train/rejected': '-1.2299', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53709', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-151.82', 'loss/train': '0.55351', 'examples_per_second': '90.114', 'grad_norm': '15.663', 'counters/examples': 95552, 'counters/updates': 1493}
skipping logging after 95616 examples to avoid logging too frequently
train stats after 95680 examples: {'rewards_train/chosen': '-0.76952', 'rewards_train/rejected': '-1.2764', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50691', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-171.5', 'loss/train': '0.55422', 'examples_per_second': '90.143', 'grad_norm': '15.814', 'counters/examples': 95680, 'counters/updates': 1495}
skipping logging after 95744 examples to avoid logging too frequently
Running evaluation after 95744 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.40it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.57it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.77it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.63it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.62it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.62it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.53it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.50it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.56it/s]
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
eval after 95744: {'rewards_eval/chosen': '-0.85869', 'rewards_eval/rejected': '-1.292', 'rewards_eval/accuracies': '0.65625', 'rewards_eval/margins': '0.43332', 'logps_eval/rejected': '-134.07', 'logps_eval/chosen': '-149.39', 'loss/eval': '0.62782'}
creating checkpoint to write to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-95744...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-95744/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-95744/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/step-95744/scheduler.pt...
train stats after 95808 examples: {'rewards_train/chosen': '-1.0498', 'rewards_train/rejected': '-1.4459', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39614', 'logps_train/rejected': '-147.91', 'logps_train/chosen': '-151.22', 'loss/train': '0.59674', 'examples_per_second': '68.111', 'grad_norm': '17.171', 'counters/examples': 95808, 'counters/updates': 1497}
skipping logging after 95872 examples to avoid logging too frequently
train stats after 95936 examples: {'rewards_train/chosen': '-0.82482', 'rewards_train/rejected': '-1.1022', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.27741', 'logps_train/rejected': '-136.36', 'logps_train/chosen': '-142.1', 'loss/train': '0.67319', 'examples_per_second': '90.677', 'grad_norm': '17.685', 'counters/examples': 95936, 'counters/updates': 1499}
skipping logging after 96000 examples to avoid logging too frequently
train stats after 96064 examples: {'rewards_train/chosen': '-0.81154', 'rewards_train/rejected': '-1.4694', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.65791', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-170.94', 'loss/train': '0.52134', 'examples_per_second': '87.962', 'grad_norm': '15.676', 'counters/examples': 96064, 'counters/updates': 1501}
skipping logging after 96128 examples to avoid logging too frequently
train stats after 96192 examples: {'rewards_train/chosen': '-0.89053', 'rewards_train/rejected': '-1.3404', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.44988', 'logps_train/rejected': '-118.54', 'logps_train/chosen': '-129.71', 'loss/train': '0.59787', 'examples_per_second': '87.963', 'grad_norm': '16.653', 'counters/examples': 96192, 'counters/updates': 1503}
skipping logging after 96256 examples to avoid logging too frequently
Finished generating 1 epochs on train split
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/LATEST/policy.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/LATEST/optimizer.pt...
writing checkpoint to .cache/laura/pythia1b_dpo_seed0_2024-01-12_00-15-24_949895/LATEST/scheduler.pt...
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         counters/updates ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      examples_per_second ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ
wandb:                grad_norm ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÇ‚ñÑ
wandb:        logps_eval/chosen ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb:      logps_eval/rejected ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb:       logps_train/chosen ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÇ
wandb:     logps_train/rejected ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÑ
wandb:                loss/eval ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ
wandb:               loss/train ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÇ‚ñÉ
wandb:  rewards_eval/accuracies ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá
wandb:      rewards_eval/chosen ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÅ
wandb:     rewards_eval/margins ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà
wandb:    rewards_eval/rejected ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: rewards_train/accuracies ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÖ
wandb:     rewards_train/chosen ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÉ
wandb:    rewards_train/margins ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ
wandb:   rewards_train/rejected ‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÉ
wandb: 
wandb: Run summary:
wandb:        counters/examples 96192
wandb:         counters/updates 1503
wandb:      examples_per_second 87.96303
wandb:                grad_norm 16.65256
wandb:        logps_eval/chosen -149.38646
wandb:      logps_eval/rejected -134.0659
wandb:       logps_train/chosen -129.71489
wandb:     logps_train/rejected -118.54096
wandb:                loss/eval 0.62782
wandb:               loss/train 0.59787
wandb:  rewards_eval/accuracies 0.65625
wandb:      rewards_eval/chosen -0.85869
wandb:     rewards_eval/margins 0.43332
wandb:    rewards_eval/rejected -1.29202
wandb: rewards_train/accuracies 0.67188
wandb:     rewards_train/chosen -0.89053
wandb:    rewards_train/margins 0.44988
wandb:   rewards_train/rejected -1.34041
wandb: 
wandb: üöÄ View run pythia1b_dpo_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/0mhjakjz
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240112_001703-0mhjakjz/logs
