WARNING: eval_every must be divisible by batch_size
Setting eval_every to 11968
no FSDP port specified; using open port for FSDP: 56951
seed: 0
exp_name: pythia410m_dpo_seed0
batch_size: 64
eval_batch_size: 16
debug: false
fsdp_port: 56951
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950
lr: 1.0e-06
gradient_accumulation_steps: 1
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 11968
minimum_log_interval_secs: 1.0
model:
  name_or_path: lomahony/pythia-410m-helpful-sft
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-236-68:.cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240111_221023-sb6r4wt7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia410m_dpo_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/sb6r4wt7
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-410m-helpful-sft
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2875/5103 [00:00<00:00, 28746.74it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 28633.49it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   0%|          | 316/96256 [00:00<00:30, 3157.71it/s]Processing HH static:   1%|          | 691/96256 [00:00<00:27, 3505.62it/s]Processing HH static:   1%|          | 1042/96256 [00:00<00:28, 3371.21it/s]Processing HH static:   1%|‚ñè         | 1398/96256 [00:00<00:27, 3443.74it/s]Processing HH static:   2%|‚ñè         | 1743/96256 [00:00<00:39, 2419.00it/s]Processing HH static:   3%|‚ñé         | 2628/96256 [00:00<00:22, 4107.16it/s]Processing HH static:   6%|‚ñå         | 5349/96256 [00:00<00:08, 10407.56it/s]Processing HH static:   8%|‚ñä         | 8159/96256 [00:00<00:05, 15366.88it/s]Processing HH static:  11%|‚ñà‚ñè        | 11038/96256 [00:01<00:04, 19203.83it/s]Processing HH static:  14%|‚ñà‚ñç        | 13900/96256 [00:01<00:03, 21932.77it/s]Processing HH static:  17%|‚ñà‚ñã        | 16791/96256 [00:01<00:03, 23976.22it/s]Processing HH static:  20%|‚ñà‚ñà        | 19673/96256 [00:01<00:03, 25402.98it/s]Processing HH static:  23%|‚ñà‚ñà‚ñé       | 22544/96256 [00:01<00:02, 26381.18it/s]Processing HH static:  26%|‚ñà‚ñà‚ñã       | 25430/96256 [00:01<00:02, 27116.44it/s]Processing HH static:  29%|‚ñà‚ñà‚ñâ       | 28181/96256 [00:01<00:02, 27224.83it/s]Processing HH static:  32%|‚ñà‚ñà‚ñà‚ñè      | 31049/96256 [00:01<00:02, 27656.18it/s]Processing HH static:  35%|‚ñà‚ñà‚ñà‚ñå      | 33834/96256 [00:01<00:02, 27615.11it/s]Processing HH static:  38%|‚ñà‚ñà‚ñà‚ñä      | 36609/96256 [00:02<00:03, 16991.07it/s]Processing HH static:  41%|‚ñà‚ñà‚ñà‚ñà      | 39395/96256 [00:02<00:02, 19247.52it/s]Processing HH static:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 42195/96256 [00:02<00:02, 21248.19it/s]Processing HH static:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 44961/96256 [00:02<00:02, 22827.06it/s]Processing HH static:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 47696/96256 [00:02<00:02, 24003.71it/s]Processing HH static:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 50439/96256 [00:02<00:01, 24931.38it/s]Processing HH static:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 53103/96256 [00:03<00:04, 9023.43it/s] Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 55075/96256 [00:03<00:06, 6456.60it/s]Processing HH static:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 57847/96256 [00:04<00:04, 8572.86it/s]Processing HH static:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 60601/96256 [00:04<00:03, 10933.96it/s]Processing HH static:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 63374/96256 [00:04<00:02, 13469.48it/s]Processing HH static:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 66130/96256 [00:04<00:01, 15968.59it/s]Processing HH static:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 68897/96256 [00:04<00:01, 18334.26it/s]Processing HH static:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 71658/96256 [00:04<00:01, 20411.70it/s]Processing HH static:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 74270/96256 [00:04<00:01, 12462.30it/s]Processing HH static:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 76290/96256 [00:05<00:03, 6621.30it/s] Processing HH static:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 77785/96256 [00:06<00:03, 4905.32it/s]Processing HH static:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 78905/96256 [00:06<00:04, 4119.42it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 79760/96256 [00:07<00:04, 4052.04it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 80471/96256 [00:07<00:04, 3759.80it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81051/96256 [00:07<00:03, 3816.62it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81582/96256 [00:07<00:03, 3777.61it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82062/96256 [00:07<00:03, 3599.96it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82487/96256 [00:07<00:04, 3243.09it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82956/96256 [00:08<00:03, 3487.97it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83353/96256 [00:08<00:04, 3156.14it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83701/96256 [00:08<00:04, 2997.47it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84021/96256 [00:08<00:04, 2847.54it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84317/96256 [00:08<00:04, 2694.11it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84593/96256 [00:08<00:04, 2614.47it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84858/96256 [00:08<00:04, 2565.89it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85116/96256 [00:08<00:04, 2435.33it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85360/96256 [00:09<00:04, 2364.63it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85596/96256 [00:09<00:04, 2335.91it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85857/96256 [00:09<00:04, 2406.51it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86161/96256 [00:09<00:03, 2579.28it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86421/96256 [00:09<00:04, 2451.78it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86669/96256 [00:09<00:03, 2421.80it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86913/96256 [00:09<00:03, 2389.18it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87153/96256 [00:09<00:03, 2359.15it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87390/96256 [00:09<00:03, 2347.89it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87626/96256 [00:10<00:03, 2305.85it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 87857/96256 [00:10<00:03, 2289.72it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88087/96256 [00:10<00:03, 2268.50it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88314/96256 [00:10<00:03, 2223.80it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88540/96256 [00:10<00:03, 2229.79it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88773/96256 [00:10<00:03, 2255.81it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 89005/96256 [00:10<00:03, 2273.39it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89233/96256 [00:10<00:03, 2263.52it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89465/96256 [00:10<00:02, 2276.63it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89701/96256 [00:10<00:02, 2289.09it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89930/96256 [00:11<00:02, 2280.09it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90159/96256 [00:11<00:02, 2256.21it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90393/96256 [00:11<00:02, 2279.01it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90641/96256 [00:11<00:02, 2338.36it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90878/96256 [00:11<00:02, 2347.62it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91113/96256 [00:11<00:02, 2332.41it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91347/96256 [00:11<00:02, 2281.71it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91655/96256 [00:11<00:01, 2511.41it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91909/96256 [00:11<00:01, 2512.37it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92161/96256 [00:11<00:01, 2418.54it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92404/96256 [00:12<00:01, 2365.02it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92642/96256 [00:12<00:01, 2356.38it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 92879/96256 [00:12<00:01, 2339.28it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93114/96256 [00:12<00:01, 2266.12it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93398/96256 [00:12<00:01, 2420.59it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 93913/96256 [00:12<00:00, 3211.02it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94238/96256 [00:12<00:00, 2953.01it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94540/96256 [00:12<00:00, 2788.05it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94825/96256 [00:12<00:00, 2639.42it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95094/96256 [00:13<00:00, 2580.49it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95609/96256 [00:13<00:00, 3269.28it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96091/96256 [00:13<00:00, 3692.62it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:13<00:00, 7234.11it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:02<00:33,  2.26s/it]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:02<00:13,  1.00it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:02<00:07,  1.70it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:02<00:04,  2.51it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:02<00:03,  3.43it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:02<00:02,  4.41it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:02<00:01,  5.29it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:01,  6.17it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:03<00:01,  6.96it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:03<00:00,  7.57it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:03<00:00,  8.04it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:03<00:00,  8.47it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:03<00:00,  9.04it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  9.23it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  9.34it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  4.18it/s]
eval after 0: {'rewards_eval/chosen': '0.0119', 'rewards_eval/rejected': '0.0090005', 'rewards_eval/accuracies': '0.51172', 'rewards_eval/margins': '0.0028991', 'logps_eval/rejected': '-125.99', 'logps_eval/chosen': '-147.78', 'loss/eval': '0.69277'}
train stats after 64 examples: {'rewards_train/chosen': '0.020365', 'rewards_train/rejected': '0.008292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012073', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-144.76', 'loss/train': '0.68818', 'examples_per_second': '59.628', 'grad_norm': '20.799', 'counters/examples': 64, 'counters/updates': 1}
train stats after 128 examples: {'rewards_train/chosen': '0.010627', 'rewards_train/rejected': '0.0088604', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0017668', 'logps_train/rejected': '-139.89', 'logps_train/chosen': '-150.17', 'loss/train': '0.69315', 'examples_per_second': '65.064', 'grad_norm': '20.999', 'counters/examples': 128, 'counters/updates': 2}
skipping logging after 192 examples to avoid logging too frequently
train stats after 256 examples: {'rewards_train/chosen': '0.031228', 'rewards_train/rejected': '0.016463', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.014766', 'logps_train/rejected': '-145.43', 'logps_train/chosen': '-169.13', 'loss/train': '0.6875', 'examples_per_second': '124.41', 'grad_norm': '22.811', 'counters/examples': 256, 'counters/updates': 4}
skipping logging after 320 examples to avoid logging too frequently
train stats after 384 examples: {'rewards_train/chosen': '0.027216', 'rewards_train/rejected': '0.022848', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0043671', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-138.04', 'loss/train': '0.69206', 'examples_per_second': '122.43', 'grad_norm': '20.445', 'counters/examples': 384, 'counters/updates': 6}
skipping logging after 448 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '0.0061356', 'rewards_train/rejected': '0.014716', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00858', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-150.68', 'loss/train': '0.6987', 'examples_per_second': '139.43', 'grad_norm': '21.514', 'counters/examples': 512, 'counters/updates': 8}
skipping logging after 576 examples to avoid logging too frequently
train stats after 640 examples: {'rewards_train/chosen': '0.019439', 'rewards_train/rejected': '0.0019148', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017524', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-152.13', 'loss/train': '0.68534', 'examples_per_second': '119.7', 'grad_norm': '20.628', 'counters/examples': 640, 'counters/updates': 10}
skipping logging after 704 examples to avoid logging too frequently
train stats after 768 examples: {'rewards_train/chosen': '0.011269', 'rewards_train/rejected': '0.020296', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '-0.0090266', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-128.17', 'loss/train': '0.6989', 'examples_per_second': '133.64', 'grad_norm': '19.482', 'counters/examples': 768, 'counters/updates': 12}
skipping logging after 832 examples to avoid logging too frequently
train stats after 896 examples: {'rewards_train/chosen': '0.020296', 'rewards_train/rejected': '0.0063582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013938', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-130.06', 'loss/train': '0.68738', 'examples_per_second': '124.23', 'grad_norm': '19.883', 'counters/examples': 896, 'counters/updates': 14}
skipping logging after 960 examples to avoid logging too frequently
train stats after 1024 examples: {'rewards_train/chosen': '0.0070986', 'rewards_train/rejected': '0.0038353', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0032633', 'logps_train/rejected': '-154.38', 'logps_train/chosen': '-147.9', 'loss/train': '0.69245', 'examples_per_second': '123.97', 'grad_norm': '21.146', 'counters/examples': 1024, 'counters/updates': 16}
skipping logging after 1088 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '0.018228', 'rewards_train/rejected': '0.0080272', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010201', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-123.57', 'loss/train': '0.68905', 'examples_per_second': '118.83', 'grad_norm': '19.614', 'counters/examples': 1152, 'counters/updates': 18}
skipping logging after 1216 examples to avoid logging too frequently
train stats after 1280 examples: {'rewards_train/chosen': '0.029973', 'rewards_train/rejected': '-0.0061848', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.036158', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-148.02', 'loss/train': '0.67638', 'examples_per_second': '124.48', 'grad_norm': '20.567', 'counters/examples': 1280, 'counters/updates': 20}
skipping logging after 1344 examples to avoid logging too frequently
train stats after 1408 examples: {'rewards_train/chosen': '0.0065059', 'rewards_train/rejected': '0.0080034', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0014975', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-164.83', 'loss/train': '0.69515', 'examples_per_second': '121.08', 'grad_norm': '22.338', 'counters/examples': 1408, 'counters/updates': 22}
skipping logging after 1472 examples to avoid logging too frequently
train stats after 1536 examples: {'rewards_train/chosen': '0.01147', 'rewards_train/rejected': '0.00076232', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010708', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-145.51', 'loss/train': '0.68907', 'examples_per_second': '123.7', 'grad_norm': '21.079', 'counters/examples': 1536, 'counters/updates': 24}
skipping logging after 1600 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '0.011067', 'rewards_train/rejected': '-0.010202', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.02127', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-134.4', 'loss/train': '0.68403', 'examples_per_second': '118.14', 'grad_norm': '19.878', 'counters/examples': 1664, 'counters/updates': 26}
skipping logging after 1728 examples to avoid logging too frequently
train stats after 1792 examples: {'rewards_train/chosen': '0.02562', 'rewards_train/rejected': '-0.0064236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032043', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-191.74', 'loss/train': '0.67891', 'examples_per_second': '121.81', 'grad_norm': '22.359', 'counters/examples': 1792, 'counters/updates': 28}
skipping logging after 1856 examples to avoid logging too frequently
train stats after 1920 examples: {'rewards_train/chosen': '-0.010001', 'rewards_train/rejected': '0.0020482', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012049', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-125.2', 'loss/train': '0.70053', 'examples_per_second': '120.33', 'grad_norm': '20.16', 'counters/examples': 1920, 'counters/updates': 30}
skipping logging after 1984 examples to avoid logging too frequently
train stats after 2048 examples: {'rewards_train/chosen': '0.00077605', 'rewards_train/rejected': '-0.012748', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.013524', 'logps_train/rejected': '-164.97', 'logps_train/chosen': '-156.46', 'loss/train': '0.68792', 'examples_per_second': '123.51', 'grad_norm': '21.978', 'counters/examples': 2048, 'counters/updates': 32}
skipping logging after 2112 examples to avoid logging too frequently
train stats after 2176 examples: {'rewards_train/chosen': '-0.0049177', 'rewards_train/rejected': '-0.026543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021626', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-128.86', 'loss/train': '0.68411', 'examples_per_second': '128.44', 'grad_norm': '20.362', 'counters/examples': 2176, 'counters/updates': 34}
skipping logging after 2240 examples to avoid logging too frequently
train stats after 2304 examples: {'rewards_train/chosen': '0.013867', 'rewards_train/rejected': '-0.02691', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040778', 'logps_train/rejected': '-111.02', 'logps_train/chosen': '-136.34', 'loss/train': '0.67436', 'examples_per_second': '123.03', 'grad_norm': '19.72', 'counters/examples': 2304, 'counters/updates': 36}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2432 examples: {'rewards_train/chosen': '-0.015937', 'rewards_train/rejected': '-0.035632', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019696', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-143.44', 'loss/train': '0.68544', 'examples_per_second': '118.88', 'grad_norm': '21.636', 'counters/examples': 2432, 'counters/updates': 38}
skipping logging after 2496 examples to avoid logging too frequently
train stats after 2560 examples: {'rewards_train/chosen': '0.010843', 'rewards_train/rejected': '-0.035833', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.046675', 'logps_train/rejected': '-126.03', 'logps_train/chosen': '-148.71', 'loss/train': '0.67273', 'examples_per_second': '121.16', 'grad_norm': '20.144', 'counters/examples': 2560, 'counters/updates': 40}
skipping logging after 2624 examples to avoid logging too frequently
train stats after 2688 examples: {'rewards_train/chosen': '-0.0038602', 'rewards_train/rejected': '-0.012847', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.0089867', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-152.75', 'loss/train': '0.69098', 'examples_per_second': '120.76', 'grad_norm': '22.241', 'counters/examples': 2688, 'counters/updates': 42}
skipping logging after 2752 examples to avoid logging too frequently
train stats after 2816 examples: {'rewards_train/chosen': '0.0054972', 'rewards_train/rejected': '-0.044267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049765', 'logps_train/rejected': '-143.44', 'logps_train/chosen': '-148.36', 'loss/train': '0.67204', 'examples_per_second': '120.89', 'grad_norm': '20.576', 'counters/examples': 2816, 'counters/updates': 44}
skipping logging after 2880 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '0.018256', 'rewards_train/rejected': '-0.044393', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.062649', 'logps_train/rejected': '-138.72', 'logps_train/chosen': '-160.66', 'loss/train': '0.66584', 'examples_per_second': '136.47', 'grad_norm': '20.785', 'counters/examples': 2944, 'counters/updates': 46}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3072 examples: {'rewards_train/chosen': '0.0012187', 'rewards_train/rejected': '-0.050152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05137', 'logps_train/rejected': '-127.91', 'logps_train/chosen': '-146.33', 'loss/train': '0.67195', 'examples_per_second': '129.03', 'grad_norm': '20.507', 'counters/examples': 3072, 'counters/updates': 48}
skipping logging after 3136 examples to avoid logging too frequently
train stats after 3200 examples: {'rewards_train/chosen': '-0.028551', 'rewards_train/rejected': '-0.072271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04372', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-143.89', 'loss/train': '0.67692', 'examples_per_second': '125.04', 'grad_norm': '20.68', 'counters/examples': 3200, 'counters/updates': 50}
skipping logging after 3264 examples to avoid logging too frequently
train stats after 3328 examples: {'rewards_train/chosen': '0.016025', 'rewards_train/rejected': '-0.056775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0728', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-159.41', 'loss/train': '0.66213', 'examples_per_second': '120.78', 'grad_norm': '20.88', 'counters/examples': 3328, 'counters/updates': 52}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3456 examples: {'rewards_train/chosen': '-0.082914', 'rewards_train/rejected': '-0.12345', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.040537', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-170.82', 'loss/train': '0.68178', 'examples_per_second': '119.14', 'grad_norm': '21.333', 'counters/examples': 3456, 'counters/updates': 54}
skipping logging after 3520 examples to avoid logging too frequently
train stats after 3584 examples: {'rewards_train/chosen': '0.031303', 'rewards_train/rejected': '-0.072096', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.1034', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-176.22', 'loss/train': '0.65104', 'examples_per_second': '124.24', 'grad_norm': '21.656', 'counters/examples': 3584, 'counters/updates': 56}
skipping logging after 3648 examples to avoid logging too frequently
train stats after 3712 examples: {'rewards_train/chosen': '-0.045454', 'rewards_train/rejected': '-0.10555', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.060094', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-152.78', 'loss/train': '0.67229', 'examples_per_second': '124.18', 'grad_norm': '20.801', 'counters/examples': 3712, 'counters/updates': 58}
skipping logging after 3776 examples to avoid logging too frequently
train stats after 3840 examples: {'rewards_train/chosen': '-0.032058', 'rewards_train/rejected': '-0.089729', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.057671', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-137.33', 'loss/train': '0.67129', 'examples_per_second': '123.97', 'grad_norm': '19.48', 'counters/examples': 3840, 'counters/updates': 60}
skipping logging after 3904 examples to avoid logging too frequently
train stats after 3968 examples: {'rewards_train/chosen': '-0.001904', 'rewards_train/rejected': '-0.12302', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12112', 'logps_train/rejected': '-111.02', 'logps_train/chosen': '-156.77', 'loss/train': '0.64468', 'examples_per_second': '119.57', 'grad_norm': '19.785', 'counters/examples': 3968, 'counters/updates': 62}
skipping logging after 4032 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '-0.017133', 'rewards_train/rejected': '-0.10633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089201', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-150.07', 'loss/train': '0.66203', 'examples_per_second': '129.5', 'grad_norm': '20.273', 'counters/examples': 4096, 'counters/updates': 64}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4224 examples: {'rewards_train/chosen': '-0.036095', 'rewards_train/rejected': '-0.14768', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.11158', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-123.56', 'loss/train': '0.64781', 'examples_per_second': '123.72', 'grad_norm': '20.626', 'counters/examples': 4224, 'counters/updates': 66}
skipping logging after 4288 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '0.011097', 'rewards_train/rejected': '-0.06279', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.073887', 'logps_train/rejected': '-149.31', 'logps_train/chosen': '-154.71', 'loss/train': '0.6663', 'examples_per_second': '131.94', 'grad_norm': '22.273', 'counters/examples': 4352, 'counters/updates': 68}
skipping logging after 4416 examples to avoid logging too frequently
train stats after 4480 examples: {'rewards_train/chosen': '-0.013764', 'rewards_train/rejected': '-0.13493', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.12116', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-157.91', 'loss/train': '0.64605', 'examples_per_second': '130.51', 'grad_norm': '20.764', 'counters/examples': 4480, 'counters/updates': 70}
skipping logging after 4544 examples to avoid logging too frequently
train stats after 4608 examples: {'rewards_train/chosen': '-0.077741', 'rewards_train/rejected': '-0.16802', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.090275', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-151.78', 'loss/train': '0.66559', 'examples_per_second': '123.76', 'grad_norm': '20.794', 'counters/examples': 4608, 'counters/updates': 72}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4736 examples: {'rewards_train/chosen': '0.0099799', 'rewards_train/rejected': '-0.1116', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.12158', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-169.82', 'loss/train': '0.64348', 'examples_per_second': '122.58', 'grad_norm': '21.93', 'counters/examples': 4736, 'counters/updates': 74}
skipping logging after 4800 examples to avoid logging too frequently
train stats after 4864 examples: {'rewards_train/chosen': '0.012024', 'rewards_train/rejected': '-0.15383', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.16586', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-166.52', 'loss/train': '0.63562', 'examples_per_second': '119.29', 'grad_norm': '21.341', 'counters/examples': 4864, 'counters/updates': 76}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4992 examples: {'rewards_train/chosen': '-0.053427', 'rewards_train/rejected': '-0.2224', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.16898', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-165.96', 'loss/train': '0.63399', 'examples_per_second': '124.8', 'grad_norm': '20.909', 'counters/examples': 4992, 'counters/updates': 78}
skipping logging after 5056 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '-0.049512', 'rewards_train/rejected': '-0.22619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17668', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-126.53', 'loss/train': '0.63153', 'examples_per_second': '124.2', 'grad_norm': '20.822', 'counters/examples': 5120, 'counters/updates': 80}
skipping logging after 5184 examples to avoid logging too frequently
train stats after 5248 examples: {'rewards_train/chosen': '-0.074433', 'rewards_train/rejected': '-0.19333', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11889', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-136.19', 'loss/train': '0.65534', 'examples_per_second': '124.25', 'grad_norm': '20.708', 'counters/examples': 5248, 'counters/updates': 82}
skipping logging after 5312 examples to avoid logging too frequently
train stats after 5376 examples: {'rewards_train/chosen': '-0.025508', 'rewards_train/rejected': '-0.20969', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.18418', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-168.79', 'loss/train': '0.62616', 'examples_per_second': '124.37', 'grad_norm': '21.722', 'counters/examples': 5376, 'counters/updates': 84}
skipping logging after 5440 examples to avoid logging too frequently
train stats after 5504 examples: {'rewards_train/chosen': '-0.12625', 'rewards_train/rejected': '-0.26378', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13753', 'logps_train/rejected': '-124.69', 'logps_train/chosen': '-154.75', 'loss/train': '0.65085', 'examples_per_second': '132.4', 'grad_norm': '20.728', 'counters/examples': 5504, 'counters/updates': 86}
skipping logging after 5568 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.13365', 'rewards_train/rejected': '-0.31008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17643', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-124.18', 'loss/train': '0.6426', 'examples_per_second': '155.17', 'grad_norm': '20.21', 'counters/examples': 5632, 'counters/updates': 88}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5760 examples: {'rewards_train/chosen': '-0.12449', 'rewards_train/rejected': '-0.25972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13523', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-162', 'loss/train': '0.65312', 'examples_per_second': '130.7', 'grad_norm': '20.692', 'counters/examples': 5760, 'counters/updates': 90}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5888 examples: {'rewards_train/chosen': '-0.024604', 'rewards_train/rejected': '-0.27713', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.25253', 'logps_train/rejected': '-115.65', 'logps_train/chosen': '-142.91', 'loss/train': '0.59946', 'examples_per_second': '123.58', 'grad_norm': '20.316', 'counters/examples': 5888, 'counters/updates': 92}
skipping logging after 5952 examples to avoid logging too frequently
train stats after 6016 examples: {'rewards_train/chosen': '-0.16149', 'rewards_train/rejected': '-0.3379', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17641', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-141.72', 'loss/train': '0.64982', 'examples_per_second': '122.94', 'grad_norm': '22.095', 'counters/examples': 6016, 'counters/updates': 94}
skipping logging after 6080 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.11396', 'rewards_train/rejected': '-0.29739', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.18342', 'logps_train/rejected': '-135.57', 'logps_train/chosen': '-144.4', 'loss/train': '0.64206', 'examples_per_second': '124.09', 'grad_norm': '20.994', 'counters/examples': 6144, 'counters/updates': 96}
skipping logging after 6208 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '-0.0095277', 'rewards_train/rejected': '-0.28333', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2738', 'logps_train/rejected': '-104.41', 'logps_train/chosen': '-162.75', 'loss/train': '0.59751', 'examples_per_second': '124.18', 'grad_norm': '20.321', 'counters/examples': 6272, 'counters/updates': 98}
skipping logging after 6336 examples to avoid logging too frequently
train stats after 6400 examples: {'rewards_train/chosen': '-0.027042', 'rewards_train/rejected': '-0.27664', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2496', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-173.73', 'loss/train': '0.6083', 'examples_per_second': '123.58', 'grad_norm': '21.636', 'counters/examples': 6400, 'counters/updates': 100}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6528 examples: {'rewards_train/chosen': '-0.17089', 'rewards_train/rejected': '-0.25686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085966', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-128.11', 'loss/train': '0.67989', 'examples_per_second': '124.07', 'grad_norm': '20.532', 'counters/examples': 6528, 'counters/updates': 102}
skipping logging after 6592 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '-0.20043', 'rewards_train/rejected': '-0.27527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074839', 'logps_train/rejected': '-106.7', 'logps_train/chosen': '-138.1', 'loss/train': '0.68623', 'examples_per_second': '135.52', 'grad_norm': '21.226', 'counters/examples': 6656, 'counters/updates': 104}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6784 examples: {'rewards_train/chosen': '-0.17224', 'rewards_train/rejected': '-0.31453', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.1423', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-155.11', 'loss/train': '0.65665', 'examples_per_second': '122.02', 'grad_norm': '22.647', 'counters/examples': 6784, 'counters/updates': 106}
skipping logging after 6848 examples to avoid logging too frequently
train stats after 6912 examples: {'rewards_train/chosen': '-0.15488', 'rewards_train/rejected': '-0.33711', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18223', 'logps_train/rejected': '-143.75', 'logps_train/chosen': '-161.82', 'loss/train': '0.64601', 'examples_per_second': '118.63', 'grad_norm': '21.667', 'counters/examples': 6912, 'counters/updates': 108}
skipping logging after 6976 examples to avoid logging too frequently
train stats after 7040 examples: {'rewards_train/chosen': '-0.16264', 'rewards_train/rejected': '-0.31775', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.15511', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-162.03', 'loss/train': '0.64518', 'examples_per_second': '124.38', 'grad_norm': '22.376', 'counters/examples': 7040, 'counters/updates': 110}
skipping logging after 7104 examples to avoid logging too frequently
train stats after 7168 examples: {'rewards_train/chosen': '-0.080725', 'rewards_train/rejected': '-0.24547', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.16474', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-164.1', 'loss/train': '0.64269', 'examples_per_second': '124.69', 'grad_norm': '22.032', 'counters/examples': 7168, 'counters/updates': 112}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7296 examples: {'rewards_train/chosen': '-0.0675', 'rewards_train/rejected': '-0.33946', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27196', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-164.26', 'loss/train': '0.59493', 'examples_per_second': '124.5', 'grad_norm': '20.814', 'counters/examples': 7296, 'counters/updates': 114}
skipping logging after 7360 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '-0.24736', 'rewards_train/rejected': '-0.49772', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25036', 'logps_train/rejected': '-116.39', 'logps_train/chosen': '-141.79', 'loss/train': '0.61375', 'examples_per_second': '124.19', 'grad_norm': '20.194', 'counters/examples': 7424, 'counters/updates': 116}
skipping logging after 7488 examples to avoid logging too frequently
train stats after 7552 examples: {'rewards_train/chosen': '-0.16456', 'rewards_train/rejected': '-0.39538', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23082', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-133.7', 'loss/train': '0.63065', 'examples_per_second': '118.86', 'grad_norm': '20.41', 'counters/examples': 7552, 'counters/updates': 118}
skipping logging after 7616 examples to avoid logging too frequently
train stats after 7680 examples: {'rewards_train/chosen': '-0.14427', 'rewards_train/rejected': '-0.28051', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.13623', 'logps_train/rejected': '-151.82', 'logps_train/chosen': '-162.79', 'loss/train': '0.65826', 'examples_per_second': '135.06', 'grad_norm': '22.946', 'counters/examples': 7680, 'counters/updates': 120}
skipping logging after 7744 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '-0.19479', 'rewards_train/rejected': '-0.51613', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.32133', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-176.74', 'loss/train': '0.61494', 'examples_per_second': '128.88', 'grad_norm': '20.312', 'counters/examples': 7808, 'counters/updates': 122}
skipping logging after 7872 examples to avoid logging too frequently
train stats after 7936 examples: {'rewards_train/chosen': '-0.23952', 'rewards_train/rejected': '-0.32287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083357', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-122.73', 'loss/train': '0.6921', 'examples_per_second': '119.3', 'grad_norm': '22.944', 'counters/examples': 7936, 'counters/updates': 124}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8064 examples: {'rewards_train/chosen': '-0.048286', 'rewards_train/rejected': '-0.20235', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.15406', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-163.81', 'loss/train': '0.64555', 'examples_per_second': '119.47', 'grad_norm': '22.608', 'counters/examples': 8064, 'counters/updates': 126}
skipping logging after 8128 examples to avoid logging too frequently
train stats after 8192 examples: {'rewards_train/chosen': '-0.073642', 'rewards_train/rejected': '-0.31362', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23998', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-180.71', 'loss/train': '0.61602', 'examples_per_second': '118.39', 'grad_norm': '21.794', 'counters/examples': 8192, 'counters/updates': 128}
skipping logging after 8256 examples to avoid logging too frequently
train stats after 8320 examples: {'rewards_train/chosen': '-0.16301', 'rewards_train/rejected': '-0.38247', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.21946', 'logps_train/rejected': '-124.48', 'logps_train/chosen': '-142.42', 'loss/train': '0.62886', 'examples_per_second': '128.29', 'grad_norm': '20.605', 'counters/examples': 8320, 'counters/updates': 130}
skipping logging after 8384 examples to avoid logging too frequently
train stats after 8448 examples: {'rewards_train/chosen': '-0.049177', 'rewards_train/rejected': '-0.36563', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.31645', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-155.57', 'loss/train': '0.60488', 'examples_per_second': '124.7', 'grad_norm': '21.199', 'counters/examples': 8448, 'counters/updates': 132}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8576 examples: {'rewards_train/chosen': '0.067592', 'rewards_train/rejected': '-0.18295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25054', 'logps_train/rejected': '-139.01', 'logps_train/chosen': '-159.44', 'loss/train': '0.61218', 'examples_per_second': '120.66', 'grad_norm': '20.853', 'counters/examples': 8576, 'counters/updates': 134}
skipping logging after 8640 examples to avoid logging too frequently
train stats after 8704 examples: {'rewards_train/chosen': '-0.089376', 'rewards_train/rejected': '-0.38005', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29067', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-134.5', 'loss/train': '0.59303', 'examples_per_second': '124.29', 'grad_norm': '19.446', 'counters/examples': 8704, 'counters/updates': 136}
skipping logging after 8768 examples to avoid logging too frequently
train stats after 8832 examples: {'rewards_train/chosen': '-0.046756', 'rewards_train/rejected': '-0.32157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27481', 'logps_train/rejected': '-148.85', 'logps_train/chosen': '-157.06', 'loss/train': '0.61624', 'examples_per_second': '124.32', 'grad_norm': '22.881', 'counters/examples': 8832, 'counters/updates': 138}
skipping logging after 8896 examples to avoid logging too frequently
train stats after 8960 examples: {'rewards_train/chosen': '-0.15816', 'rewards_train/rejected': '-0.30425', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14609', 'logps_train/rejected': '-114.74', 'logps_train/chosen': '-138.26', 'loss/train': '0.65691', 'examples_per_second': '119.27', 'grad_norm': '20.71', 'counters/examples': 8960, 'counters/updates': 140}
skipping logging after 9024 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '-0.12591', 'rewards_train/rejected': '-0.43495', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.30904', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-142.71', 'loss/train': '0.60064', 'examples_per_second': '123.77', 'grad_norm': '20.008', 'counters/examples': 9088, 'counters/updates': 142}
skipping logging after 9152 examples to avoid logging too frequently
train stats after 9216 examples: {'rewards_train/chosen': '-0.16378', 'rewards_train/rejected': '-0.32157', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.15779', 'logps_train/rejected': '-147.38', 'logps_train/chosen': '-163.14', 'loss/train': '0.65084', 'examples_per_second': '124.09', 'grad_norm': '23.228', 'counters/examples': 9216, 'counters/updates': 144}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9344 examples: {'rewards_train/chosen': '-0.18605', 'rewards_train/rejected': '-0.39084', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.20478', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-150.98', 'loss/train': '0.63398', 'examples_per_second': '124.33', 'grad_norm': '21.892', 'counters/examples': 9344, 'counters/updates': 146}
skipping logging after 9408 examples to avoid logging too frequently
train stats after 9472 examples: {'rewards_train/chosen': '-0.20226', 'rewards_train/rejected': '-0.50698', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.30472', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-123.46', 'loss/train': '0.60802', 'examples_per_second': '124.37', 'grad_norm': '19.518', 'counters/examples': 9472, 'counters/updates': 148}
skipping logging after 9536 examples to avoid logging too frequently
train stats after 9600 examples: {'rewards_train/chosen': '-0.10986', 'rewards_train/rejected': '-0.30875', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.19888', 'logps_train/rejected': '-132.9', 'logps_train/chosen': '-148.23', 'loss/train': '0.65213', 'examples_per_second': '124.36', 'grad_norm': '23.599', 'counters/examples': 9600, 'counters/updates': 150}
skipping logging after 9664 examples to avoid logging too frequently
train stats after 9728 examples: {'rewards_train/chosen': '-0.080189', 'rewards_train/rejected': '-0.41199', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.3318', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-152.08', 'loss/train': '0.58202', 'examples_per_second': '118.02', 'grad_norm': '20.611', 'counters/examples': 9728, 'counters/updates': 152}
skipping logging after 9792 examples to avoid logging too frequently
train stats after 9856 examples: {'rewards_train/chosen': '-0.31722', 'rewards_train/rejected': '-0.61806', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.30084', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-119.99', 'loss/train': '0.60565', 'examples_per_second': '128.08', 'grad_norm': '19.21', 'counters/examples': 9856, 'counters/updates': 154}
skipping logging after 9920 examples to avoid logging too frequently
train stats after 9984 examples: {'rewards_train/chosen': '-0.18633', 'rewards_train/rejected': '-0.64533', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.459', 'logps_train/rejected': '-134.55', 'logps_train/chosen': '-145.27', 'loss/train': '0.56685', 'examples_per_second': '123.04', 'grad_norm': '20.5', 'counters/examples': 9984, 'counters/updates': 156}
skipping logging after 10048 examples to avoid logging too frequently
train stats after 10112 examples: {'rewards_train/chosen': '-0.22469', 'rewards_train/rejected': '-0.46538', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.24069', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-181.45', 'loss/train': '0.64229', 'examples_per_second': '124.53', 'grad_norm': '24.179', 'counters/examples': 10112, 'counters/updates': 158}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10240 examples: {'rewards_train/chosen': '-0.27585', 'rewards_train/rejected': '-0.45477', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.17892', 'logps_train/rejected': '-137.28', 'logps_train/chosen': '-177.03', 'loss/train': '0.65274', 'examples_per_second': '124.26', 'grad_norm': '25.038', 'counters/examples': 10240, 'counters/updates': 160}
skipping logging after 10304 examples to avoid logging too frequently
train stats after 10368 examples: {'rewards_train/chosen': '-0.37592', 'rewards_train/rejected': '-0.75541', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37949', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-158', 'loss/train': '0.58135', 'examples_per_second': '124.14', 'grad_norm': '21.511', 'counters/examples': 10368, 'counters/updates': 162}
skipping logging after 10432 examples to avoid logging too frequently
train stats after 10496 examples: {'rewards_train/chosen': '-0.52891', 'rewards_train/rejected': '-0.87034', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.34143', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-142.86', 'loss/train': '0.59843', 'examples_per_second': '122.76', 'grad_norm': '21.752', 'counters/examples': 10496, 'counters/updates': 164}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10624 examples: {'rewards_train/chosen': '-0.28281', 'rewards_train/rejected': '-0.64572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36291', 'logps_train/rejected': '-125.03', 'logps_train/chosen': '-140.03', 'loss/train': '0.58338', 'examples_per_second': '120.19', 'grad_norm': '20.915', 'counters/examples': 10624, 'counters/updates': 166}
skipping logging after 10688 examples to avoid logging too frequently
train stats after 10752 examples: {'rewards_train/chosen': '-0.29974', 'rewards_train/rejected': '-0.6665', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.36677', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-156.12', 'loss/train': '0.5989', 'examples_per_second': '118.35', 'grad_norm': '22.245', 'counters/examples': 10752, 'counters/updates': 168}
skipping logging after 10816 examples to avoid logging too frequently
train stats after 10880 examples: {'rewards_train/chosen': '-0.51934', 'rewards_train/rejected': '-0.71129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19195', 'logps_train/rejected': '-185.68', 'logps_train/chosen': '-173.13', 'loss/train': '0.66095', 'examples_per_second': '123.77', 'grad_norm': '25.65', 'counters/examples': 10880, 'counters/updates': 170}
skipping logging after 10944 examples to avoid logging too frequently
train stats after 11008 examples: {'rewards_train/chosen': '-0.22506', 'rewards_train/rejected': '-0.62109', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39602', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-160.5', 'loss/train': '0.58741', 'examples_per_second': '122.31', 'grad_norm': '20.89', 'counters/examples': 11008, 'counters/updates': 172}
skipping logging after 11072 examples to avoid logging too frequently
train stats after 11136 examples: {'rewards_train/chosen': '-0.14924', 'rewards_train/rejected': '-0.52276', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37352', 'logps_train/rejected': '-163.05', 'logps_train/chosen': '-173.66', 'loss/train': '0.59509', 'examples_per_second': '124.33', 'grad_norm': '23.853', 'counters/examples': 11136, 'counters/updates': 174}
skipping logging after 11200 examples to avoid logging too frequently
train stats after 11264 examples: {'rewards_train/chosen': '-0.34534', 'rewards_train/rejected': '-0.49371', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.14836', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-150.68', 'loss/train': '0.68381', 'examples_per_second': '123.63', 'grad_norm': '24.657', 'counters/examples': 11264, 'counters/updates': 176}
skipping logging after 11328 examples to avoid logging too frequently
train stats after 11392 examples: {'rewards_train/chosen': '-0.11509', 'rewards_train/rejected': '-0.4772', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36211', 'logps_train/rejected': '-141', 'logps_train/chosen': '-149.94', 'loss/train': '0.58269', 'examples_per_second': '119.63', 'grad_norm': '20.67', 'counters/examples': 11392, 'counters/updates': 178}
skipping logging after 11456 examples to avoid logging too frequently
train stats after 11520 examples: {'rewards_train/chosen': '-0.16524', 'rewards_train/rejected': '-0.43093', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.26568', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-169.89', 'loss/train': '0.62287', 'examples_per_second': '126.69', 'grad_norm': '23.68', 'counters/examples': 11520, 'counters/updates': 180}
skipping logging after 11584 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '-0.31109', 'rewards_train/rejected': '-0.62781', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31672', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-142.12', 'loss/train': '0.61072', 'examples_per_second': '118.43', 'grad_norm': '22.129', 'counters/examples': 11648, 'counters/updates': 182}
skipping logging after 11712 examples to avoid logging too frequently
train stats after 11776 examples: {'rewards_train/chosen': '-0.31802', 'rewards_train/rejected': '-0.67355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35553', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-185.46', 'loss/train': '0.61035', 'examples_per_second': '119.94', 'grad_norm': '23.812', 'counters/examples': 11776, 'counters/updates': 184}
skipping logging after 11840 examples to avoid logging too frequently
train stats after 11904 examples: {'rewards_train/chosen': '-0.37214', 'rewards_train/rejected': '-0.63366', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.26151', 'logps_train/rejected': '-171.92', 'logps_train/chosen': '-159.84', 'loss/train': '0.63668', 'examples_per_second': '124.69', 'grad_norm': '24.074', 'counters/examples': 11904, 'counters/updates': 186}
skipping logging after 11968 examples to avoid logging too frequently
Running evaluation after 11968 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  9.95it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01,  9.59it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  9.92it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01,  9.89it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:00, 10.09it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:00<00:00, 10.08it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00, 10.07it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00, 10.10it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:01<00:00, 10.10it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.04it/s]
eval after 11968: {'rewards_eval/chosen': '-0.3274', 'rewards_eval/rejected': '-0.64598', 'rewards_eval/accuracies': '0.64062', 'rewards_eval/margins': '0.31858', 'logps_eval/rejected': '-132.54', 'logps_eval/chosen': '-151.18', 'loss/eval': '0.63769'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-11968...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-11968/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-11968/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-11968/scheduler.pt...
train stats after 12032 examples: {'rewards_train/chosen': '-0.22061', 'rewards_train/rejected': '-0.58824', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36762', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-151.81', 'loss/train': '0.61011', 'examples_per_second': '119.51', 'grad_norm': '22.357', 'counters/examples': 12032, 'counters/updates': 188}
skipping logging after 12096 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.4232', 'rewards_train/rejected': '-0.79826', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.37506', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-163.76', 'loss/train': '0.59936', 'examples_per_second': '130.25', 'grad_norm': '21.891', 'counters/examples': 12160, 'counters/updates': 190}
skipping logging after 12224 examples to avoid logging too frequently
train stats after 12288 examples: {'rewards_train/chosen': '-0.50411', 'rewards_train/rejected': '-0.77824', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.27413', 'logps_train/rejected': '-143.54', 'logps_train/chosen': '-154.26', 'loss/train': '0.63805', 'examples_per_second': '124.17', 'grad_norm': '22.323', 'counters/examples': 12288, 'counters/updates': 192}
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12416 examples: {'rewards_train/chosen': '-0.15593', 'rewards_train/rejected': '-0.36965', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.21372', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-121.74', 'loss/train': '0.66057', 'examples_per_second': '124.75', 'grad_norm': '21.142', 'counters/examples': 12416, 'counters/updates': 194}
skipping logging after 12480 examples to avoid logging too frequently
train stats after 12544 examples: {'rewards_train/chosen': '-0.26418', 'rewards_train/rejected': '-0.51426', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.25008', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-162.84', 'loss/train': '0.64608', 'examples_per_second': '124.75', 'grad_norm': '23.814', 'counters/examples': 12544, 'counters/updates': 196}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12672 examples: {'rewards_train/chosen': '-0.093724', 'rewards_train/rejected': '-0.49665', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40292', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-156.19', 'loss/train': '0.5691', 'examples_per_second': '119.6', 'grad_norm': '21.138', 'counters/examples': 12672, 'counters/updates': 198}
skipping logging after 12736 examples to avoid logging too frequently
train stats after 12800 examples: {'rewards_train/chosen': '-0.22909', 'rewards_train/rejected': '-0.64887', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41979', 'logps_train/rejected': '-128.14', 'logps_train/chosen': '-145.68', 'loss/train': '0.58076', 'examples_per_second': '125.05', 'grad_norm': '21.029', 'counters/examples': 12800, 'counters/updates': 200}
skipping logging after 12864 examples to avoid logging too frequently
train stats after 12928 examples: {'rewards_train/chosen': '-0.29649', 'rewards_train/rejected': '-0.5867', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.29021', 'logps_train/rejected': '-167.83', 'logps_train/chosen': '-157.39', 'loss/train': '0.63703', 'examples_per_second': '125.01', 'grad_norm': '24.52', 'counters/examples': 12928, 'counters/updates': 202}
skipping logging after 12992 examples to avoid logging too frequently
train stats after 13056 examples: {'rewards_train/chosen': '-0.34387', 'rewards_train/rejected': '-0.75473', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41086', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-123.45', 'loss/train': '0.60197', 'examples_per_second': '124.55', 'grad_norm': '20.033', 'counters/examples': 13056, 'counters/updates': 204}
skipping logging after 13120 examples to avoid logging too frequently
train stats after 13184 examples: {'rewards_train/chosen': '-0.13633', 'rewards_train/rejected': '-0.49493', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3586', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-149.77', 'loss/train': '0.58377', 'examples_per_second': '123.73', 'grad_norm': '21.055', 'counters/examples': 13184, 'counters/updates': 206}
skipping logging after 13248 examples to avoid logging too frequently
train stats after 13312 examples: {'rewards_train/chosen': '-0.20314', 'rewards_train/rejected': '-0.5579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35476', 'logps_train/rejected': '-132.65', 'logps_train/chosen': '-179.54', 'loss/train': '0.58388', 'examples_per_second': '124.95', 'grad_norm': '22.454', 'counters/examples': 13312, 'counters/updates': 208}
skipping logging after 13376 examples to avoid logging too frequently
train stats after 13440 examples: {'rewards_train/chosen': '-0.53179', 'rewards_train/rejected': '-0.73561', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20382', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-154.88', 'loss/train': '0.67594', 'examples_per_second': '124.86', 'grad_norm': '25.533', 'counters/examples': 13440, 'counters/updates': 210}
skipping logging after 13504 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '-0.20582', 'rewards_train/rejected': '-0.7431', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.53727', 'logps_train/rejected': '-150.48', 'logps_train/chosen': '-149.06', 'loss/train': '0.51976', 'examples_per_second': '124.81', 'grad_norm': '20.219', 'counters/examples': 13568, 'counters/updates': 212}
skipping logging after 13632 examples to avoid logging too frequently
train stats after 13696 examples: {'rewards_train/chosen': '-0.37628', 'rewards_train/rejected': '-0.52947', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15319', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-155.12', 'loss/train': '0.69442', 'examples_per_second': '128.76', 'grad_norm': '26.51', 'counters/examples': 13696, 'counters/updates': 214}
skipping logging after 13760 examples to avoid logging too frequently
train stats after 13824 examples: {'rewards_train/chosen': '-0.26277', 'rewards_train/rejected': '-0.50072', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23795', 'logps_train/rejected': '-142.29', 'logps_train/chosen': '-153.1', 'loss/train': '0.65896', 'examples_per_second': '124.76', 'grad_norm': '24.053', 'counters/examples': 13824, 'counters/updates': 216}
skipping logging after 13888 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '-0.033865', 'rewards_train/rejected': '-0.32958', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29571', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-145.32', 'loss/train': '0.61946', 'examples_per_second': '134.14', 'grad_norm': '22.059', 'counters/examples': 13952, 'counters/updates': 218}
skipping logging after 14016 examples to avoid logging too frequently
train stats after 14080 examples: {'rewards_train/chosen': '-0.14961', 'rewards_train/rejected': '-0.39073', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.24112', 'logps_train/rejected': '-142.53', 'logps_train/chosen': '-128.9', 'loss/train': '0.65519', 'examples_per_second': '124.84', 'grad_norm': '23.87', 'counters/examples': 14080, 'counters/updates': 220}
skipping logging after 14144 examples to avoid logging too frequently
train stats after 14208 examples: {'rewards_train/chosen': '-0.16273', 'rewards_train/rejected': '-0.41943', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.2567', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-156.64', 'loss/train': '0.61965', 'examples_per_second': '133.17', 'grad_norm': '22.396', 'counters/examples': 14208, 'counters/updates': 222}
skipping logging after 14272 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.02299', 'rewards_train/rejected': '-0.34294', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.36593', 'logps_train/rejected': '-159.19', 'logps_train/chosen': '-149.32', 'loss/train': '0.5739', 'examples_per_second': '121.91', 'grad_norm': '22.864', 'counters/examples': 14336, 'counters/updates': 224}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14464 examples: {'rewards_train/chosen': '-0.11246', 'rewards_train/rejected': '-0.27447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.162', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-163.69', 'loss/train': '0.66686', 'examples_per_second': '126.13', 'grad_norm': '26.162', 'counters/examples': 14464, 'counters/updates': 226}
skipping logging after 14528 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '-0.029747', 'rewards_train/rejected': '-0.30994', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.2802', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-141.58', 'loss/train': '0.62445', 'examples_per_second': '125.01', 'grad_norm': '22.977', 'counters/examples': 14592, 'counters/updates': 228}
skipping logging after 14656 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '-0.099792', 'rewards_train/rejected': '-0.31014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21034', 'logps_train/rejected': '-149.09', 'logps_train/chosen': '-156.15', 'loss/train': '0.64325', 'examples_per_second': '125.73', 'grad_norm': '25.015', 'counters/examples': 14720, 'counters/updates': 230}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14848 examples: {'rewards_train/chosen': '-0.19933', 'rewards_train/rejected': '-0.49778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29846', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-143.2', 'loss/train': '0.61889', 'examples_per_second': '124.86', 'grad_norm': '22.763', 'counters/examples': 14848, 'counters/updates': 232}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14976 examples: {'rewards_train/chosen': '-0.1977', 'rewards_train/rejected': '-0.53896', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34127', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-130.6', 'loss/train': '0.61026', 'examples_per_second': '125.24', 'grad_norm': '20.118', 'counters/examples': 14976, 'counters/updates': 234}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15104 examples: {'rewards_train/chosen': '-0.34886', 'rewards_train/rejected': '-0.60668', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.25782', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-150', 'loss/train': '0.62376', 'examples_per_second': '120.21', 'grad_norm': '23.314', 'counters/examples': 15104, 'counters/updates': 236}
skipping logging after 15168 examples to avoid logging too frequently
train stats after 15232 examples: {'rewards_train/chosen': '-0.35192', 'rewards_train/rejected': '-0.38765', 'rewards_train/accuracies': '0.45312', 'rewards_train/margins': '0.035731', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-151.82', 'loss/train': '0.7342', 'examples_per_second': '123.52', 'grad_norm': '25.393', 'counters/examples': 15232, 'counters/updates': 238}
skipping logging after 15296 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '-0.17522', 'rewards_train/rejected': '-0.52354', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34832', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-164', 'loss/train': '0.59578', 'examples_per_second': '119.96', 'grad_norm': '22.755', 'counters/examples': 15360, 'counters/updates': 240}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15488 examples: {'rewards_train/chosen': '-0.28865', 'rewards_train/rejected': '-0.44015', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1515', 'logps_train/rejected': '-109.27', 'logps_train/chosen': '-131.16', 'loss/train': '0.675', 'examples_per_second': '124.01', 'grad_norm': '22.131', 'counters/examples': 15488, 'counters/updates': 242}
skipping logging after 15552 examples to avoid logging too frequently
train stats after 15616 examples: {'rewards_train/chosen': '-0.32218', 'rewards_train/rejected': '-0.67488', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3527', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-157.97', 'loss/train': '0.61032', 'examples_per_second': '124.65', 'grad_norm': '22.164', 'counters/examples': 15616, 'counters/updates': 244}
skipping logging after 15680 examples to avoid logging too frequently
train stats after 15744 examples: {'rewards_train/chosen': '-0.2154', 'rewards_train/rejected': '-0.53079', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3154', 'logps_train/rejected': '-141.69', 'logps_train/chosen': '-136.94', 'loss/train': '0.6264', 'examples_per_second': '125.34', 'grad_norm': '25.221', 'counters/examples': 15744, 'counters/updates': 246}
skipping logging after 15808 examples to avoid logging too frequently
train stats after 15872 examples: {'rewards_train/chosen': '-0.24979', 'rewards_train/rejected': '-0.51702', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.26723', 'logps_train/rejected': '-152.54', 'logps_train/chosen': '-140.87', 'loss/train': '0.62996', 'examples_per_second': '124.91', 'grad_norm': '23.715', 'counters/examples': 15872, 'counters/updates': 248}
skipping logging after 15936 examples to avoid logging too frequently
train stats after 16000 examples: {'rewards_train/chosen': '-0.15325', 'rewards_train/rejected': '-0.44593', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29269', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-158.84', 'loss/train': '0.6206', 'examples_per_second': '124.83', 'grad_norm': '24.7', 'counters/examples': 16000, 'counters/updates': 250}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16128 examples: {'rewards_train/chosen': '-0.29676', 'rewards_train/rejected': '-0.58485', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28808', 'logps_train/rejected': '-156.73', 'logps_train/chosen': '-168.54', 'loss/train': '0.65553', 'examples_per_second': '124.62', 'grad_norm': '24.756', 'counters/examples': 16128, 'counters/updates': 252}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16256 examples: {'rewards_train/chosen': '-0.40354', 'rewards_train/rejected': '-0.64164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2381', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-144.43', 'loss/train': '0.63525', 'examples_per_second': '126.26', 'grad_norm': '24.252', 'counters/examples': 16256, 'counters/updates': 254}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16384 examples: {'rewards_train/chosen': '-0.22453', 'rewards_train/rejected': '-0.50529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28076', 'logps_train/rejected': '-137.41', 'logps_train/chosen': '-155.28', 'loss/train': '0.60287', 'examples_per_second': '124.93', 'grad_norm': '26.046', 'counters/examples': 16384, 'counters/updates': 256}
skipping logging after 16448 examples to avoid logging too frequently
train stats after 16512 examples: {'rewards_train/chosen': '-0.60606', 'rewards_train/rejected': '-0.68261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076555', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-136.55', 'loss/train': '0.73921', 'examples_per_second': '123.91', 'grad_norm': '27.515', 'counters/examples': 16512, 'counters/updates': 258}
skipping logging after 16576 examples to avoid logging too frequently
train stats after 16640 examples: {'rewards_train/chosen': '-0.44952', 'rewards_train/rejected': '-0.77068', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32116', 'logps_train/rejected': '-140.49', 'logps_train/chosen': '-151.57', 'loss/train': '0.60464', 'examples_per_second': '119.11', 'grad_norm': '22.356', 'counters/examples': 16640, 'counters/updates': 260}
skipping logging after 16704 examples to avoid logging too frequently
train stats after 16768 examples: {'rewards_train/chosen': '-0.41665', 'rewards_train/rejected': '-0.63354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21689', 'logps_train/rejected': '-139.26', 'logps_train/chosen': '-140.39', 'loss/train': '0.65745', 'examples_per_second': '124.83', 'grad_norm': '24.82', 'counters/examples': 16768, 'counters/updates': 262}
skipping logging after 16832 examples to avoid logging too frequently
train stats after 16896 examples: {'rewards_train/chosen': '-0.3455', 'rewards_train/rejected': '-0.73518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38968', 'logps_train/rejected': '-179.28', 'logps_train/chosen': '-162.13', 'loss/train': '0.58572', 'examples_per_second': '124.86', 'grad_norm': '24.146', 'counters/examples': 16896, 'counters/updates': 264}
skipping logging after 16960 examples to avoid logging too frequently
train stats after 17024 examples: {'rewards_train/chosen': '-0.55024', 'rewards_train/rejected': '-0.76348', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21324', 'logps_train/rejected': '-180.02', 'logps_train/chosen': '-152.36', 'loss/train': '0.64805', 'examples_per_second': '124.11', 'grad_norm': '24.786', 'counters/examples': 17024, 'counters/updates': 266}
skipping logging after 17088 examples to avoid logging too frequently
train stats after 17152 examples: {'rewards_train/chosen': '-0.26177', 'rewards_train/rejected': '-0.57287', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3111', 'logps_train/rejected': '-135.4', 'logps_train/chosen': '-153.39', 'loss/train': '0.62062', 'examples_per_second': '123.63', 'grad_norm': '22.766', 'counters/examples': 17152, 'counters/updates': 268}
skipping logging after 17216 examples to avoid logging too frequently
train stats after 17280 examples: {'rewards_train/chosen': '-0.41267', 'rewards_train/rejected': '-0.68039', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26771', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-150.89', 'loss/train': '0.65175', 'examples_per_second': '124.63', 'grad_norm': '24.096', 'counters/examples': 17280, 'counters/updates': 270}
skipping logging after 17344 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.37926', 'rewards_train/rejected': '-0.67897', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29971', 'logps_train/rejected': '-151.18', 'logps_train/chosen': '-164.53', 'loss/train': '0.64605', 'examples_per_second': '120.7', 'grad_norm': '24.488', 'counters/examples': 17408, 'counters/updates': 272}
skipping logging after 17472 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '-0.52909', 'rewards_train/rejected': '-0.81863', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28954', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-141.39', 'loss/train': '0.62846', 'examples_per_second': '131.37', 'grad_norm': '23.018', 'counters/examples': 17536, 'counters/updates': 274}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17664 examples: {'rewards_train/chosen': '-0.44269', 'rewards_train/rejected': '-1.0453', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.60265', 'logps_train/rejected': '-149.5', 'logps_train/chosen': '-134.4', 'loss/train': '0.53298', 'examples_per_second': '119.19', 'grad_norm': '20.018', 'counters/examples': 17664, 'counters/updates': 276}
skipping logging after 17728 examples to avoid logging too frequently
train stats after 17792 examples: {'rewards_train/chosen': '-0.5595', 'rewards_train/rejected': '-0.80831', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24881', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-114.5', 'loss/train': '0.66968', 'examples_per_second': '128.86', 'grad_norm': '22.583', 'counters/examples': 17792, 'counters/updates': 278}
skipping logging after 17856 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '-0.31578', 'rewards_train/rejected': '-0.66057', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34479', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-129.17', 'loss/train': '0.61505', 'examples_per_second': '141.3', 'grad_norm': '20.918', 'counters/examples': 17920, 'counters/updates': 280}
skipping logging after 17984 examples to avoid logging too frequently
train stats after 18048 examples: {'rewards_train/chosen': '-0.23311', 'rewards_train/rejected': '-0.60961', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3765', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-152.14', 'loss/train': '0.60571', 'examples_per_second': '123.68', 'grad_norm': '23.407', 'counters/examples': 18048, 'counters/updates': 282}
skipping logging after 18112 examples to avoid logging too frequently
train stats after 18176 examples: {'rewards_train/chosen': '-0.33964', 'rewards_train/rejected': '-0.71441', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.37477', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-151.25', 'loss/train': '0.61095', 'examples_per_second': '118.25', 'grad_norm': '22.235', 'counters/examples': 18176, 'counters/updates': 284}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18304 examples: {'rewards_train/chosen': '-0.38356', 'rewards_train/rejected': '-0.88619', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.50263', 'logps_train/rejected': '-155.85', 'logps_train/chosen': '-180.54', 'loss/train': '0.52639', 'examples_per_second': '122.25', 'grad_norm': '21.341', 'counters/examples': 18304, 'counters/updates': 286}
skipping logging after 18368 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '-0.43611', 'rewards_train/rejected': '-0.62771', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19161', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-144.08', 'loss/train': '0.71235', 'examples_per_second': '118.61', 'grad_norm': '30.418', 'counters/examples': 18432, 'counters/updates': 288}
skipping logging after 18496 examples to avoid logging too frequently
train stats after 18560 examples: {'rewards_train/chosen': '-0.30084', 'rewards_train/rejected': '-0.65069', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34985', 'logps_train/rejected': '-134.69', 'logps_train/chosen': '-174.99', 'loss/train': '0.59963', 'examples_per_second': '124.41', 'grad_norm': '25.352', 'counters/examples': 18560, 'counters/updates': 290}
skipping logging after 18624 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.19705', 'rewards_train/rejected': '-0.45183', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.25479', 'logps_train/rejected': '-141.7', 'logps_train/chosen': '-144.67', 'loss/train': '0.64505', 'examples_per_second': '126.23', 'grad_norm': '22.067', 'counters/examples': 18688, 'counters/updates': 292}
skipping logging after 18752 examples to avoid logging too frequently
train stats after 18816 examples: {'rewards_train/chosen': '-0.48064', 'rewards_train/rejected': '-0.60404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12339', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-155.19', 'loss/train': '0.71272', 'examples_per_second': '124.53', 'grad_norm': '25.809', 'counters/examples': 18816, 'counters/updates': 294}
skipping logging after 18880 examples to avoid logging too frequently
train stats after 18944 examples: {'rewards_train/chosen': '-0.51187', 'rewards_train/rejected': '-0.71797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2061', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-146.09', 'loss/train': '0.67069', 'examples_per_second': '124.9', 'grad_norm': '26.17', 'counters/examples': 18944, 'counters/updates': 296}
skipping logging after 19008 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '-0.40776', 'rewards_train/rejected': '-0.63656', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.2288', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-156.8', 'loss/train': '0.6645', 'examples_per_second': '118.83', 'grad_norm': '24.318', 'counters/examples': 19072, 'counters/updates': 298}
skipping logging after 19136 examples to avoid logging too frequently
train stats after 19200 examples: {'rewards_train/chosen': '-0.36551', 'rewards_train/rejected': '-0.7152', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34968', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-173.63', 'loss/train': '0.61549', 'examples_per_second': '118.7', 'grad_norm': '23.518', 'counters/examples': 19200, 'counters/updates': 300}
skipping logging after 19264 examples to avoid logging too frequently
train stats after 19328 examples: {'rewards_train/chosen': '-0.38897', 'rewards_train/rejected': '-0.50851', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11954', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-167.59', 'loss/train': '0.70166', 'examples_per_second': '124.97', 'grad_norm': '26.414', 'counters/examples': 19328, 'counters/updates': 302}
skipping logging after 19392 examples to avoid logging too frequently
train stats after 19456 examples: {'rewards_train/chosen': '-0.18482', 'rewards_train/rejected': '-0.52234', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.33752', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-178.28', 'loss/train': '0.60275', 'examples_per_second': '120.44', 'grad_norm': '23.074', 'counters/examples': 19456, 'counters/updates': 304}
skipping logging after 19520 examples to avoid logging too frequently
train stats after 19584 examples: {'rewards_train/chosen': '-0.34446', 'rewards_train/rejected': '-0.64012', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.29566', 'logps_train/rejected': '-164.97', 'logps_train/chosen': '-167.59', 'loss/train': '0.60029', 'examples_per_second': '125.1', 'grad_norm': '22.932', 'counters/examples': 19584, 'counters/updates': 306}
skipping logging after 19648 examples to avoid logging too frequently
train stats after 19712 examples: {'rewards_train/chosen': '-0.28393', 'rewards_train/rejected': '-0.64283', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3589', 'logps_train/rejected': '-143.7', 'logps_train/chosen': '-149.07', 'loss/train': '0.6406', 'examples_per_second': '123.5', 'grad_norm': '25.872', 'counters/examples': 19712, 'counters/updates': 308}
skipping logging after 19776 examples to avoid logging too frequently
train stats after 19840 examples: {'rewards_train/chosen': '-0.28301', 'rewards_train/rejected': '-0.57449', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29147', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-160.71', 'loss/train': '0.63269', 'examples_per_second': '124.65', 'grad_norm': '23.64', 'counters/examples': 19840, 'counters/updates': 310}
skipping logging after 19904 examples to avoid logging too frequently
train stats after 19968 examples: {'rewards_train/chosen': '-0.1719', 'rewards_train/rejected': '-0.60728', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43538', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-171.3', 'loss/train': '0.55764', 'examples_per_second': '127.88', 'grad_norm': '22.317', 'counters/examples': 19968, 'counters/updates': 312}
skipping logging after 20032 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '-0.31761', 'rewards_train/rejected': '-0.63616', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31856', 'logps_train/rejected': '-150.13', 'logps_train/chosen': '-165.8', 'loss/train': '0.62894', 'examples_per_second': '146.2', 'grad_norm': '25.07', 'counters/examples': 20096, 'counters/updates': 314}
skipping logging after 20160 examples to avoid logging too frequently
train stats after 20224 examples: {'rewards_train/chosen': '-0.16907', 'rewards_train/rejected': '-0.66687', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4978', 'logps_train/rejected': '-148.86', 'logps_train/chosen': '-175.06', 'loss/train': '0.56508', 'examples_per_second': '123.72', 'grad_norm': '23.753', 'counters/examples': 20224, 'counters/updates': 316}
skipping logging after 20288 examples to avoid logging too frequently
train stats after 20352 examples: {'rewards_train/chosen': '-0.27856', 'rewards_train/rejected': '-0.75229', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47373', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-147.03', 'loss/train': '0.55292', 'examples_per_second': '123.95', 'grad_norm': '20.38', 'counters/examples': 20352, 'counters/updates': 318}
skipping logging after 20416 examples to avoid logging too frequently
train stats after 20480 examples: {'rewards_train/chosen': '-0.34789', 'rewards_train/rejected': '-0.62408', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.27619', 'logps_train/rejected': '-179.76', 'logps_train/chosen': '-156.67', 'loss/train': '0.64116', 'examples_per_second': '123.86', 'grad_norm': '25.121', 'counters/examples': 20480, 'counters/updates': 320}
skipping logging after 20544 examples to avoid logging too frequently
train stats after 20608 examples: {'rewards_train/chosen': '-0.10619', 'rewards_train/rejected': '-0.54431', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43812', 'logps_train/rejected': '-149.18', 'logps_train/chosen': '-190.57', 'loss/train': '0.57374', 'examples_per_second': '123.95', 'grad_norm': '25.475', 'counters/examples': 20608, 'counters/updates': 322}
skipping logging after 20672 examples to avoid logging too frequently
train stats after 20736 examples: {'rewards_train/chosen': '-0.26174', 'rewards_train/rejected': '-0.5586', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29687', 'logps_train/rejected': '-143.17', 'logps_train/chosen': '-186.12', 'loss/train': '0.63148', 'examples_per_second': '123.83', 'grad_norm': '24.47', 'counters/examples': 20736, 'counters/updates': 324}
skipping logging after 20800 examples to avoid logging too frequently
train stats after 20864 examples: {'rewards_train/chosen': '-0.38391', 'rewards_train/rejected': '-0.69882', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.3149', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-154.29', 'loss/train': '0.60701', 'examples_per_second': '118.64', 'grad_norm': '20.9', 'counters/examples': 20864, 'counters/updates': 326}
skipping logging after 20928 examples to avoid logging too frequently
train stats after 20992 examples: {'rewards_train/chosen': '-0.27075', 'rewards_train/rejected': '-0.69243', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42168', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-163.32', 'loss/train': '0.56832', 'examples_per_second': '123.3', 'grad_norm': '22.622', 'counters/examples': 20992, 'counters/updates': 328}
skipping logging after 21056 examples to avoid logging too frequently
train stats after 21120 examples: {'rewards_train/chosen': '-0.56194', 'rewards_train/rejected': '-0.83129', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.26934', 'logps_train/rejected': '-149.76', 'logps_train/chosen': '-143.2', 'loss/train': '0.65863', 'examples_per_second': '124.2', 'grad_norm': '24.011', 'counters/examples': 21120, 'counters/updates': 330}
skipping logging after 21184 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '-0.24078', 'rewards_train/rejected': '-0.4763', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.23552', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-129.95', 'loss/train': '0.63354', 'examples_per_second': '128.52', 'grad_norm': '23.469', 'counters/examples': 21248, 'counters/updates': 332}
skipping logging after 21312 examples to avoid logging too frequently
train stats after 21376 examples: {'rewards_train/chosen': '-0.2', 'rewards_train/rejected': '-0.62025', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42025', 'logps_train/rejected': '-161.09', 'logps_train/chosen': '-145.39', 'loss/train': '0.5613', 'examples_per_second': '124.31', 'grad_norm': '20.601', 'counters/examples': 21376, 'counters/updates': 334}
skipping logging after 21440 examples to avoid logging too frequently
train stats after 21504 examples: {'rewards_train/chosen': '-0.074323', 'rewards_train/rejected': '-0.61833', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54401', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-162.92', 'loss/train': '0.54064', 'examples_per_second': '123.91', 'grad_norm': '23.61', 'counters/examples': 21504, 'counters/updates': 336}
skipping logging after 21568 examples to avoid logging too frequently
train stats after 21632 examples: {'rewards_train/chosen': '-0.17055', 'rewards_train/rejected': '-0.46288', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29233', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-140.76', 'loss/train': '0.6024', 'examples_per_second': '123.87', 'grad_norm': '22.892', 'counters/examples': 21632, 'counters/updates': 338}
skipping logging after 21696 examples to avoid logging too frequently
train stats after 21760 examples: {'rewards_train/chosen': '-0.25748', 'rewards_train/rejected': '-0.58198', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3245', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-178.11', 'loss/train': '0.64006', 'examples_per_second': '124.45', 'grad_norm': '25.478', 'counters/examples': 21760, 'counters/updates': 340}
skipping logging after 21824 examples to avoid logging too frequently
train stats after 21888 examples: {'rewards_train/chosen': '-0.3091', 'rewards_train/rejected': '-0.55566', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24656', 'logps_train/rejected': '-150.41', 'logps_train/chosen': '-138.94', 'loss/train': '0.63916', 'examples_per_second': '127.07', 'grad_norm': '23.423', 'counters/examples': 21888, 'counters/updates': 342}
skipping logging after 21952 examples to avoid logging too frequently
train stats after 22016 examples: {'rewards_train/chosen': '-0.34456', 'rewards_train/rejected': '-0.68772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.34317', 'logps_train/rejected': '-162.2', 'logps_train/chosen': '-142.85', 'loss/train': '0.6128', 'examples_per_second': '123.47', 'grad_norm': '23.255', 'counters/examples': 22016, 'counters/updates': 344}
skipping logging after 22080 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.17066', 'rewards_train/rejected': '-0.52368', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35302', 'logps_train/rejected': '-116.44', 'logps_train/chosen': '-133.04', 'loss/train': '0.59803', 'examples_per_second': '120.68', 'grad_norm': '21.014', 'counters/examples': 22144, 'counters/updates': 346}
skipping logging after 22208 examples to avoid logging too frequently
train stats after 22272 examples: {'rewards_train/chosen': '-0.11084', 'rewards_train/rejected': '-0.48258', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.37173', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-136.76', 'loss/train': '0.5681', 'examples_per_second': '118.71', 'grad_norm': '19.957', 'counters/examples': 22272, 'counters/updates': 348}
skipping logging after 22336 examples to avoid logging too frequently
train stats after 22400 examples: {'rewards_train/chosen': '-0.23816', 'rewards_train/rejected': '-0.65921', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42104', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-163.29', 'loss/train': '0.57982', 'examples_per_second': '124.9', 'grad_norm': '21.703', 'counters/examples': 22400, 'counters/updates': 350}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22528 examples: {'rewards_train/chosen': '-0.35066', 'rewards_train/rejected': '-0.65506', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30441', 'logps_train/rejected': '-140.33', 'logps_train/chosen': '-165.33', 'loss/train': '0.61236', 'examples_per_second': '124.49', 'grad_norm': '21.757', 'counters/examples': 22528, 'counters/updates': 352}
skipping logging after 22592 examples to avoid logging too frequently
train stats after 22656 examples: {'rewards_train/chosen': '-0.30867', 'rewards_train/rejected': '-0.60145', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29278', 'logps_train/rejected': '-134.1', 'logps_train/chosen': '-157.29', 'loss/train': '0.64006', 'examples_per_second': '124.24', 'grad_norm': '25.215', 'counters/examples': 22656, 'counters/updates': 354}
skipping logging after 22720 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '-0.34426', 'rewards_train/rejected': '-0.77278', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.42851', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-148.43', 'loss/train': '0.55908', 'examples_per_second': '131.07', 'grad_norm': '21.041', 'counters/examples': 22784, 'counters/updates': 356}
skipping logging after 22848 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.23591', 'rewards_train/rejected': '-0.63827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40236', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-154.97', 'loss/train': '0.60553', 'examples_per_second': '145.65', 'grad_norm': '23.58', 'counters/examples': 22912, 'counters/updates': 358}
skipping logging after 22976 examples to avoid logging too frequently
train stats after 23040 examples: {'rewards_train/chosen': '-0.18784', 'rewards_train/rejected': '-0.55281', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.36497', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-134.24', 'loss/train': '0.58125', 'examples_per_second': '119.5', 'grad_norm': '21.318', 'counters/examples': 23040, 'counters/updates': 360}
skipping logging after 23104 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '-0.22659', 'rewards_train/rejected': '-0.62723', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.40064', 'logps_train/rejected': '-143.92', 'logps_train/chosen': '-169.97', 'loss/train': '0.59166', 'examples_per_second': '124.78', 'grad_norm': '25.028', 'counters/examples': 23168, 'counters/updates': 362}
skipping logging after 23232 examples to avoid logging too frequently
train stats after 23296 examples: {'rewards_train/chosen': '-0.30813', 'rewards_train/rejected': '-0.622', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.31386', 'logps_train/rejected': '-157.01', 'logps_train/chosen': '-167.38', 'loss/train': '0.63814', 'examples_per_second': '124.54', 'grad_norm': '24.685', 'counters/examples': 23296, 'counters/updates': 364}
skipping logging after 23360 examples to avoid logging too frequently
train stats after 23424 examples: {'rewards_train/chosen': '-0.26902', 'rewards_train/rejected': '-0.50056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23155', 'logps_train/rejected': '-131.03', 'logps_train/chosen': '-141.56', 'loss/train': '0.63002', 'examples_per_second': '124.33', 'grad_norm': '22.099', 'counters/examples': 23424, 'counters/updates': 366}
skipping logging after 23488 examples to avoid logging too frequently
train stats after 23552 examples: {'rewards_train/chosen': '-0.23742', 'rewards_train/rejected': '-0.77283', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53541', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-138.72', 'loss/train': '0.57934', 'examples_per_second': '124.29', 'grad_norm': '22.545', 'counters/examples': 23552, 'counters/updates': 368}
skipping logging after 23616 examples to avoid logging too frequently
train stats after 23680 examples: {'rewards_train/chosen': '-0.44677', 'rewards_train/rejected': '-0.7383', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29153', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-142.76', 'loss/train': '0.63504', 'examples_per_second': '118.2', 'grad_norm': '24.468', 'counters/examples': 23680, 'counters/updates': 370}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23808 examples: {'rewards_train/chosen': '-0.20237', 'rewards_train/rejected': '-0.51662', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31425', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-149.94', 'loss/train': '0.6195', 'examples_per_second': '121.82', 'grad_norm': '24.182', 'counters/examples': 23808, 'counters/updates': 372}
skipping logging after 23872 examples to avoid logging too frequently
train stats after 23936 examples: {'rewards_train/chosen': '-0.41314', 'rewards_train/rejected': '-0.62666', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21352', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-141.43', 'loss/train': '0.6519', 'examples_per_second': '123.62', 'grad_norm': '22.448', 'counters/examples': 23936, 'counters/updates': 374}
Running evaluation after 23936 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.17it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.20it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.01it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00,  9.93it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.03it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.07it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.09it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.08it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.06it/s]
eval after 23936: {'rewards_eval/chosen': '-0.3682', 'rewards_eval/rejected': '-0.69644', 'rewards_eval/accuracies': '0.625', 'rewards_eval/margins': '0.32824', 'logps_eval/rejected': '-133.04', 'logps_eval/chosen': '-151.58', 'loss/eval': '0.64724'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-23936...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-23936/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-23936/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-23936/scheduler.pt...
train stats after 24000 examples: {'rewards_train/chosen': '-0.36926', 'rewards_train/rejected': '-0.59819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22893', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-146.27', 'loss/train': '0.65757', 'examples_per_second': '111.96', 'grad_norm': '24.537', 'counters/examples': 24000, 'counters/updates': 375}
skipping logging after 24064 examples to avoid logging too frequently
train stats after 24128 examples: {'rewards_train/chosen': '-0.41594', 'rewards_train/rejected': '-0.57198', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.15603', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-155.79', 'loss/train': '0.72894', 'examples_per_second': '124.35', 'grad_norm': '25.685', 'counters/examples': 24128, 'counters/updates': 377}
skipping logging after 24192 examples to avoid logging too frequently
train stats after 24256 examples: {'rewards_train/chosen': '-0.50943', 'rewards_train/rejected': '-0.78518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27575', 'logps_train/rejected': '-131.1', 'logps_train/chosen': '-185.42', 'loss/train': '0.63186', 'examples_per_second': '124.54', 'grad_norm': '26.785', 'counters/examples': 24256, 'counters/updates': 379}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24384 examples: {'rewards_train/chosen': '-0.51518', 'rewards_train/rejected': '-0.98692', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47174', 'logps_train/rejected': '-142.3', 'logps_train/chosen': '-156.47', 'loss/train': '0.60057', 'examples_per_second': '120.46', 'grad_norm': '22.531', 'counters/examples': 24384, 'counters/updates': 381}
skipping logging after 24448 examples to avoid logging too frequently
train stats after 24512 examples: {'rewards_train/chosen': '-0.23152', 'rewards_train/rejected': '-0.70752', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.476', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-150.45', 'loss/train': '0.55405', 'examples_per_second': '118.97', 'grad_norm': '19.799', 'counters/examples': 24512, 'counters/updates': 383}
skipping logging after 24576 examples to avoid logging too frequently
train stats after 24640 examples: {'rewards_train/chosen': '-0.30083', 'rewards_train/rejected': '-0.71479', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41396', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-157.18', 'loss/train': '0.58724', 'examples_per_second': '130.21', 'grad_norm': '22.614', 'counters/examples': 24640, 'counters/updates': 385}
skipping logging after 24704 examples to avoid logging too frequently
train stats after 24768 examples: {'rewards_train/chosen': '-0.35416', 'rewards_train/rejected': '-0.912', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.55784', 'logps_train/rejected': '-157.55', 'logps_train/chosen': '-146.96', 'loss/train': '0.54022', 'examples_per_second': '127.88', 'grad_norm': '20.331', 'counters/examples': 24768, 'counters/updates': 387}
skipping logging after 24832 examples to avoid logging too frequently
train stats after 24896 examples: {'rewards_train/chosen': '-0.51106', 'rewards_train/rejected': '-0.81094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.29988', 'logps_train/rejected': '-178.82', 'logps_train/chosen': '-171.42', 'loss/train': '0.67487', 'examples_per_second': '119.75', 'grad_norm': '25.156', 'counters/examples': 24896, 'counters/updates': 389}
skipping logging after 24960 examples to avoid logging too frequently
train stats after 25024 examples: {'rewards_train/chosen': '-0.35404', 'rewards_train/rejected': '-0.89686', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54283', 'logps_train/rejected': '-147.82', 'logps_train/chosen': '-158.45', 'loss/train': '0.53071', 'examples_per_second': '121.14', 'grad_norm': '21.242', 'counters/examples': 25024, 'counters/updates': 391}
skipping logging after 25088 examples to avoid logging too frequently
train stats after 25152 examples: {'rewards_train/chosen': '-0.38212', 'rewards_train/rejected': '-0.79011', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.40799', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-140.58', 'loss/train': '0.56102', 'examples_per_second': '124.84', 'grad_norm': '21.472', 'counters/examples': 25152, 'counters/updates': 393}
skipping logging after 25216 examples to avoid logging too frequently
train stats after 25280 examples: {'rewards_train/chosen': '-0.33002', 'rewards_train/rejected': '-0.71508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38506', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-133.05', 'loss/train': '0.60765', 'examples_per_second': '124.85', 'grad_norm': '21.245', 'counters/examples': 25280, 'counters/updates': 395}
skipping logging after 25344 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '-0.51845', 'rewards_train/rejected': '-0.74979', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.23133', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-141.72', 'loss/train': '0.66908', 'examples_per_second': '124.78', 'grad_norm': '24.851', 'counters/examples': 25408, 'counters/updates': 397}
skipping logging after 25472 examples to avoid logging too frequently
train stats after 25536 examples: {'rewards_train/chosen': '-0.53179', 'rewards_train/rejected': '-0.84812', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31633', 'logps_train/rejected': '-120.72', 'logps_train/chosen': '-146.14', 'loss/train': '0.64614', 'examples_per_second': '124.87', 'grad_norm': '25.289', 'counters/examples': 25536, 'counters/updates': 399}
skipping logging after 25600 examples to avoid logging too frequently
train stats after 25664 examples: {'rewards_train/chosen': '-0.24084', 'rewards_train/rejected': '-0.85102', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61018', 'logps_train/rejected': '-131.24', 'logps_train/chosen': '-141.15', 'loss/train': '0.52122', 'examples_per_second': '121.19', 'grad_norm': '20.44', 'counters/examples': 25664, 'counters/updates': 401}
skipping logging after 25728 examples to avoid logging too frequently
train stats after 25792 examples: {'rewards_train/chosen': '-0.40454', 'rewards_train/rejected': '-0.96329', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55874', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-175.77', 'loss/train': '0.56413', 'examples_per_second': '121.5', 'grad_norm': '25.402', 'counters/examples': 25792, 'counters/updates': 403}
skipping logging after 25856 examples to avoid logging too frequently
train stats after 25920 examples: {'rewards_train/chosen': '-0.52091', 'rewards_train/rejected': '-0.83486', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31395', 'logps_train/rejected': '-147.73', 'logps_train/chosen': '-173.91', 'loss/train': '0.63534', 'examples_per_second': '124.94', 'grad_norm': '24.707', 'counters/examples': 25920, 'counters/updates': 405}
skipping logging after 25984 examples to avoid logging too frequently
train stats after 26048 examples: {'rewards_train/chosen': '-0.49945', 'rewards_train/rejected': '-0.91332', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41387', 'logps_train/rejected': '-136.33', 'logps_train/chosen': '-178.12', 'loss/train': '0.59806', 'examples_per_second': '119.68', 'grad_norm': '24.691', 'counters/examples': 26048, 'counters/updates': 407}
skipping logging after 26112 examples to avoid logging too frequently
train stats after 26176 examples: {'rewards_train/chosen': '-0.58954', 'rewards_train/rejected': '-0.98627', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.39673', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-137.83', 'loss/train': '0.63042', 'examples_per_second': '119.85', 'grad_norm': '22.85', 'counters/examples': 26176, 'counters/updates': 409}
skipping logging after 26240 examples to avoid logging too frequently
train stats after 26304 examples: {'rewards_train/chosen': '-0.37763', 'rewards_train/rejected': '-0.7795', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.40187', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-164.36', 'loss/train': '0.66871', 'examples_per_second': '129.48', 'grad_norm': '24.958', 'counters/examples': 26304, 'counters/updates': 411}
skipping logging after 26368 examples to avoid logging too frequently
train stats after 26432 examples: {'rewards_train/chosen': '-0.28449', 'rewards_train/rejected': '-0.76103', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47654', 'logps_train/rejected': '-179.24', 'logps_train/chosen': '-162.8', 'loss/train': '0.5642', 'examples_per_second': '119.46', 'grad_norm': '22.973', 'counters/examples': 26432, 'counters/updates': 413}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26560 examples: {'rewards_train/chosen': '-0.44484', 'rewards_train/rejected': '-0.56294', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11811', 'logps_train/rejected': '-133.5', 'logps_train/chosen': '-133.85', 'loss/train': '0.70186', 'examples_per_second': '128.78', 'grad_norm': '22.724', 'counters/examples': 26560, 'counters/updates': 415}
skipping logging after 26624 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '-0.087111', 'rewards_train/rejected': '-0.55822', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47111', 'logps_train/rejected': '-163.49', 'logps_train/chosen': '-181.02', 'loss/train': '0.55491', 'examples_per_second': '114.06', 'grad_norm': '23.424', 'counters/examples': 26688, 'counters/updates': 417}
skipping logging after 26752 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.30565', 'rewards_train/rejected': '-0.81381', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.50817', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-168.55', 'loss/train': '0.55719', 'examples_per_second': '124.49', 'grad_norm': '21.529', 'counters/examples': 26816, 'counters/updates': 419}
skipping logging after 26880 examples to avoid logging too frequently
train stats after 26944 examples: {'rewards_train/chosen': '-0.51365', 'rewards_train/rejected': '-0.88406', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.37041', 'logps_train/rejected': '-181.79', 'logps_train/chosen': '-163.89', 'loss/train': '0.67314', 'examples_per_second': '130.33', 'grad_norm': '27.817', 'counters/examples': 26944, 'counters/updates': 421}
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27072 examples: {'rewards_train/chosen': '-0.50705', 'rewards_train/rejected': '-0.79883', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.29178', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-153.12', 'loss/train': '0.62851', 'examples_per_second': '122.53', 'grad_norm': '22.199', 'counters/examples': 27072, 'counters/updates': 423}
skipping logging after 27136 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '-0.54321', 'rewards_train/rejected': '-0.89386', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35065', 'logps_train/rejected': '-133', 'logps_train/chosen': '-130.76', 'loss/train': '0.61608', 'examples_per_second': '118.47', 'grad_norm': '22.93', 'counters/examples': 27200, 'counters/updates': 425}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27328 examples: {'rewards_train/chosen': '-0.26062', 'rewards_train/rejected': '-0.61181', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.35119', 'logps_train/rejected': '-165.64', 'logps_train/chosen': '-137.34', 'loss/train': '0.5951', 'examples_per_second': '124.96', 'grad_norm': '23.868', 'counters/examples': 27328, 'counters/updates': 427}
skipping logging after 27392 examples to avoid logging too frequently
train stats after 27456 examples: {'rewards_train/chosen': '-0.31464', 'rewards_train/rejected': '-0.42934', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.11469', 'logps_train/rejected': '-165.78', 'logps_train/chosen': '-191.95', 'loss/train': '0.70436', 'examples_per_second': '124.74', 'grad_norm': '28.988', 'counters/examples': 27456, 'counters/updates': 429}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27584 examples: {'rewards_train/chosen': '-0.30188', 'rewards_train/rejected': '-0.66907', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.36719', 'logps_train/rejected': '-135.16', 'logps_train/chosen': '-152.48', 'loss/train': '0.60649', 'examples_per_second': '120.59', 'grad_norm': '22.463', 'counters/examples': 27584, 'counters/updates': 431}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27712 examples: {'rewards_train/chosen': '-0.38611', 'rewards_train/rejected': '-0.72069', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33458', 'logps_train/rejected': '-137.87', 'logps_train/chosen': '-145.52', 'loss/train': '0.60683', 'examples_per_second': '120.89', 'grad_norm': '23.237', 'counters/examples': 27712, 'counters/updates': 433}
skipping logging after 27776 examples to avoid logging too frequently
train stats after 27840 examples: {'rewards_train/chosen': '-0.51963', 'rewards_train/rejected': '-0.91942', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.39979', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-144.24', 'loss/train': '0.59906', 'examples_per_second': '124.38', 'grad_norm': '23', 'counters/examples': 27840, 'counters/updates': 435}
skipping logging after 27904 examples to avoid logging too frequently
train stats after 27968 examples: {'rewards_train/chosen': '-0.42168', 'rewards_train/rejected': '-0.60876', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.18707', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-135.8', 'loss/train': '0.67459', 'examples_per_second': '124.5', 'grad_norm': '23.379', 'counters/examples': 27968, 'counters/updates': 437}
skipping logging after 28032 examples to avoid logging too frequently
train stats after 28096 examples: {'rewards_train/chosen': '-0.42537', 'rewards_train/rejected': '-0.76112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33575', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-177.41', 'loss/train': '0.61631', 'examples_per_second': '124.81', 'grad_norm': '23.017', 'counters/examples': 28096, 'counters/updates': 439}
skipping logging after 28160 examples to avoid logging too frequently
train stats after 28224 examples: {'rewards_train/chosen': '-0.35425', 'rewards_train/rejected': '-0.7386', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38436', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-151.2', 'loss/train': '0.59429', 'examples_per_second': '119.14', 'grad_norm': '21.751', 'counters/examples': 28224, 'counters/updates': 441}
skipping logging after 28288 examples to avoid logging too frequently
train stats after 28352 examples: {'rewards_train/chosen': '-0.36136', 'rewards_train/rejected': '-0.60059', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.23923', 'logps_train/rejected': '-148.88', 'logps_train/chosen': '-140.05', 'loss/train': '0.6369', 'examples_per_second': '132.34', 'grad_norm': '23.297', 'counters/examples': 28352, 'counters/updates': 443}
skipping logging after 28416 examples to avoid logging too frequently
train stats after 28480 examples: {'rewards_train/chosen': '-0.35588', 'rewards_train/rejected': '-0.57099', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2151', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-155.67', 'loss/train': '0.6575', 'examples_per_second': '124.88', 'grad_norm': '24.469', 'counters/examples': 28480, 'counters/updates': 445}
skipping logging after 28544 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.44546', 'rewards_train/rejected': '-0.80769', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.36222', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-174.22', 'loss/train': '0.63813', 'examples_per_second': '132.67', 'grad_norm': '24.626', 'counters/examples': 28608, 'counters/updates': 447}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28736 examples: {'rewards_train/chosen': '-0.43038', 'rewards_train/rejected': '-0.89594', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46557', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-168.84', 'loss/train': '0.57315', 'examples_per_second': '124.39', 'grad_norm': '21.774', 'counters/examples': 28736, 'counters/updates': 449}
skipping logging after 28800 examples to avoid logging too frequently
train stats after 28864 examples: {'rewards_train/chosen': '-0.38205', 'rewards_train/rejected': '-0.96924', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58719', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-131.59', 'loss/train': '0.55668', 'examples_per_second': '123.66', 'grad_norm': '20.623', 'counters/examples': 28864, 'counters/updates': 451}
skipping logging after 28928 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.37732', 'rewards_train/rejected': '-0.93872', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56139', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-131.95', 'loss/train': '0.54869', 'examples_per_second': '129.28', 'grad_norm': '20.926', 'counters/examples': 28992, 'counters/updates': 453}
skipping logging after 29056 examples to avoid logging too frequently
train stats after 29120 examples: {'rewards_train/chosen': '-0.55411', 'rewards_train/rejected': '-0.92702', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.37291', 'logps_train/rejected': '-130.65', 'logps_train/chosen': '-170.42', 'loss/train': '0.59618', 'examples_per_second': '119.19', 'grad_norm': '22.189', 'counters/examples': 29120, 'counters/updates': 455}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29248 examples: {'rewards_train/chosen': '-0.58622', 'rewards_train/rejected': '-1.1318', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.54559', 'logps_train/rejected': '-140.81', 'logps_train/chosen': '-164.92', 'loss/train': '0.56974', 'examples_per_second': '118.41', 'grad_norm': '22.907', 'counters/examples': 29248, 'counters/updates': 457}
skipping logging after 29312 examples to avoid logging too frequently
train stats after 29376 examples: {'rewards_train/chosen': '-0.86105', 'rewards_train/rejected': '-1.1818', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.32076', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-170.76', 'loss/train': '0.61744', 'examples_per_second': '119.92', 'grad_norm': '24.506', 'counters/examples': 29376, 'counters/updates': 459}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29504 examples: {'rewards_train/chosen': '-0.55515', 'rewards_train/rejected': '-1.0545', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49939', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-135', 'loss/train': '0.60987', 'examples_per_second': '123.65', 'grad_norm': '22.398', 'counters/examples': 29504, 'counters/updates': 461}
skipping logging after 29568 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '-0.36297', 'rewards_train/rejected': '-0.85519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49222', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-139.29', 'loss/train': '0.57391', 'examples_per_second': '124.63', 'grad_norm': '21.021', 'counters/examples': 29632, 'counters/updates': 463}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29760 examples: {'rewards_train/chosen': '-0.39099', 'rewards_train/rejected': '-0.77233', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38134', 'logps_train/rejected': '-161.34', 'logps_train/chosen': '-149.62', 'loss/train': '0.63235', 'examples_per_second': '123.53', 'grad_norm': '23.567', 'counters/examples': 29760, 'counters/updates': 465}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29888 examples: {'rewards_train/chosen': '-0.49983', 'rewards_train/rejected': '-0.8093', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30947', 'logps_train/rejected': '-123.87', 'logps_train/chosen': '-137.11', 'loss/train': '0.61731', 'examples_per_second': '139.2', 'grad_norm': '21.575', 'counters/examples': 29888, 'counters/updates': 467}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 30016 examples: {'rewards_train/chosen': '-0.43569', 'rewards_train/rejected': '-0.77217', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.33649', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-137.38', 'loss/train': '0.60655', 'examples_per_second': '137.92', 'grad_norm': '20.878', 'counters/examples': 30016, 'counters/updates': 469}
skipping logging after 30080 examples to avoid logging too frequently
train stats after 30144 examples: {'rewards_train/chosen': '-0.61626', 'rewards_train/rejected': '-0.92869', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.31243', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-144.72', 'loss/train': '0.62235', 'examples_per_second': '122.37', 'grad_norm': '22.006', 'counters/examples': 30144, 'counters/updates': 471}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30272 examples: {'rewards_train/chosen': '-0.49375', 'rewards_train/rejected': '-0.82295', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.32921', 'logps_train/rejected': '-154.02', 'logps_train/chosen': '-158.52', 'loss/train': '0.6455', 'examples_per_second': '124.64', 'grad_norm': '25.104', 'counters/examples': 30272, 'counters/updates': 473}
skipping logging after 30336 examples to avoid logging too frequently
train stats after 30400 examples: {'rewards_train/chosen': '-0.26569', 'rewards_train/rejected': '-0.6299', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36421', 'logps_train/rejected': '-154.84', 'logps_train/chosen': '-151.77', 'loss/train': '0.62626', 'examples_per_second': '124.38', 'grad_norm': '23.774', 'counters/examples': 30400, 'counters/updates': 475}
skipping logging after 30464 examples to avoid logging too frequently
train stats after 30528 examples: {'rewards_train/chosen': '-0.30476', 'rewards_train/rejected': '-0.67646', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.3717', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-139.24', 'loss/train': '0.6045', 'examples_per_second': '124.77', 'grad_norm': '22.432', 'counters/examples': 30528, 'counters/updates': 477}
skipping logging after 30592 examples to avoid logging too frequently
train stats after 30656 examples: {'rewards_train/chosen': '-0.25443', 'rewards_train/rejected': '-0.74805', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49362', 'logps_train/rejected': '-153.88', 'logps_train/chosen': '-149.46', 'loss/train': '0.60251', 'examples_per_second': '119.92', 'grad_norm': '22.497', 'counters/examples': 30656, 'counters/updates': 479}
skipping logging after 30720 examples to avoid logging too frequently
train stats after 30784 examples: {'rewards_train/chosen': '-0.37472', 'rewards_train/rejected': '-0.91483', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54011', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-136.56', 'loss/train': '0.5443', 'examples_per_second': '124.79', 'grad_norm': '18.478', 'counters/examples': 30784, 'counters/updates': 481}
skipping logging after 30848 examples to avoid logging too frequently
train stats after 30912 examples: {'rewards_train/chosen': '-0.38997', 'rewards_train/rejected': '-0.84225', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45228', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-165.52', 'loss/train': '0.58787', 'examples_per_second': '124.87', 'grad_norm': '23.257', 'counters/examples': 30912, 'counters/updates': 483}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31040 examples: {'rewards_train/chosen': '-0.439', 'rewards_train/rejected': '-0.679', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-135.85', 'loss/train': '0.65905', 'examples_per_second': '124.7', 'grad_norm': '22.031', 'counters/examples': 31040, 'counters/updates': 485}
skipping logging after 31104 examples to avoid logging too frequently
train stats after 31168 examples: {'rewards_train/chosen': '-0.20637', 'rewards_train/rejected': '-0.58915', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38278', 'logps_train/rejected': '-158.05', 'logps_train/chosen': '-155.22', 'loss/train': '0.59514', 'examples_per_second': '124.88', 'grad_norm': '23.759', 'counters/examples': 31168, 'counters/updates': 487}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31296 examples: {'rewards_train/chosen': '-0.26705', 'rewards_train/rejected': '-0.75618', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48913', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-158.46', 'loss/train': '0.54717', 'examples_per_second': '122.09', 'grad_norm': '20.745', 'counters/examples': 31296, 'counters/updates': 489}
skipping logging after 31360 examples to avoid logging too frequently
train stats after 31424 examples: {'rewards_train/chosen': '-0.42831', 'rewards_train/rejected': '-0.98962', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.56131', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-161.47', 'loss/train': '0.55385', 'examples_per_second': '119.25', 'grad_norm': '22.794', 'counters/examples': 31424, 'counters/updates': 491}
skipping logging after 31488 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '-0.4582', 'rewards_train/rejected': '-0.88422', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.42602', 'logps_train/rejected': '-158.89', 'logps_train/chosen': '-177.15', 'loss/train': '0.6191', 'examples_per_second': '124.78', 'grad_norm': '25.225', 'counters/examples': 31552, 'counters/updates': 493}
skipping logging after 31616 examples to avoid logging too frequently
train stats after 31680 examples: {'rewards_train/chosen': '-0.40063', 'rewards_train/rejected': '-0.66603', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26539', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-164.23', 'loss/train': '0.66176', 'examples_per_second': '124.94', 'grad_norm': '24.874', 'counters/examples': 31680, 'counters/updates': 495}
skipping logging after 31744 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '-0.38171', 'rewards_train/rejected': '-0.57647', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.19476', 'logps_train/rejected': '-145.83', 'logps_train/chosen': '-166.05', 'loss/train': '0.67845', 'examples_per_second': '125.27', 'grad_norm': '27.587', 'counters/examples': 31808, 'counters/updates': 497}
skipping logging after 31872 examples to avoid logging too frequently
train stats after 31936 examples: {'rewards_train/chosen': '-0.51638', 'rewards_train/rejected': '-0.88153', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36515', 'logps_train/rejected': '-155.36', 'logps_train/chosen': '-146.13', 'loss/train': '0.62148', 'examples_per_second': '119.44', 'grad_norm': '24.804', 'counters/examples': 31936, 'counters/updates': 499}
skipping logging after 32000 examples to avoid logging too frequently
train stats after 32064 examples: {'rewards_train/chosen': '-0.34587', 'rewards_train/rejected': '-0.74803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.40215', 'logps_train/rejected': '-143.59', 'logps_train/chosen': '-160.63', 'loss/train': '0.59233', 'examples_per_second': '124.91', 'grad_norm': '22.683', 'counters/examples': 32064, 'counters/updates': 501}
skipping logging after 32128 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '-0.38502', 'rewards_train/rejected': '-1.0055', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.62047', 'logps_train/rejected': '-148.03', 'logps_train/chosen': '-145', 'loss/train': '0.52757', 'examples_per_second': '119.65', 'grad_norm': '20.61', 'counters/examples': 32192, 'counters/updates': 503}
skipping logging after 32256 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.32154', 'rewards_train/rejected': '-0.86437', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.54283', 'logps_train/rejected': '-124.12', 'logps_train/chosen': '-151.43', 'loss/train': '0.52935', 'examples_per_second': '134.79', 'grad_norm': '20.422', 'counters/examples': 32320, 'counters/updates': 505}
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32448 examples: {'rewards_train/chosen': '-0.49715', 'rewards_train/rejected': '-0.9446', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.44744', 'logps_train/rejected': '-151.61', 'logps_train/chosen': '-163.32', 'loss/train': '0.565', 'examples_per_second': '125.04', 'grad_norm': '22.638', 'counters/examples': 32448, 'counters/updates': 507}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32576 examples: {'rewards_train/chosen': '-0.49734', 'rewards_train/rejected': '-0.9015', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40416', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-150.93', 'loss/train': '0.60772', 'examples_per_second': '124.24', 'grad_norm': '22.904', 'counters/examples': 32576, 'counters/updates': 509}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32704 examples: {'rewards_train/chosen': '-0.39437', 'rewards_train/rejected': '-0.58665', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19228', 'logps_train/rejected': '-139.93', 'logps_train/chosen': '-177.46', 'loss/train': '0.66546', 'examples_per_second': '124.36', 'grad_norm': '26.425', 'counters/examples': 32704, 'counters/updates': 511}
skipping logging after 32768 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '-0.60194', 'rewards_train/rejected': '-1.1004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.49849', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-145.99', 'loss/train': '0.57466', 'examples_per_second': '125.01', 'grad_norm': '23.585', 'counters/examples': 32832, 'counters/updates': 513}
skipping logging after 32896 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.42569', 'rewards_train/rejected': '-0.70613', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28044', 'logps_train/rejected': '-155.93', 'logps_train/chosen': '-160.57', 'loss/train': '0.63526', 'examples_per_second': '119.34', 'grad_norm': '24.534', 'counters/examples': 32960, 'counters/updates': 515}
skipping logging after 33024 examples to avoid logging too frequently
train stats after 33088 examples: {'rewards_train/chosen': '-0.59622', 'rewards_train/rejected': '-0.92933', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33311', 'logps_train/rejected': '-176.52', 'logps_train/chosen': '-174.14', 'loss/train': '0.62366', 'examples_per_second': '123.87', 'grad_norm': '23.513', 'counters/examples': 33088, 'counters/updates': 517}
skipping logging after 33152 examples to avoid logging too frequently
train stats after 33216 examples: {'rewards_train/chosen': '-0.38578', 'rewards_train/rejected': '-0.77915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39337', 'logps_train/rejected': '-147.94', 'logps_train/chosen': '-147.32', 'loss/train': '0.61706', 'examples_per_second': '124.49', 'grad_norm': '23.608', 'counters/examples': 33216, 'counters/updates': 519}
skipping logging after 33280 examples to avoid logging too frequently
train stats after 33344 examples: {'rewards_train/chosen': '-0.32043', 'rewards_train/rejected': '-0.68748', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36705', 'logps_train/rejected': '-162.99', 'logps_train/chosen': '-165.39', 'loss/train': '0.6042', 'examples_per_second': '133.14', 'grad_norm': '26.351', 'counters/examples': 33344, 'counters/updates': 521}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33472 examples: {'rewards_train/chosen': '-0.33674', 'rewards_train/rejected': '-0.75732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42057', 'logps_train/rejected': '-141.43', 'logps_train/chosen': '-165.3', 'loss/train': '0.60724', 'examples_per_second': '123.89', 'grad_norm': '25.149', 'counters/examples': 33472, 'counters/updates': 523}
skipping logging after 33536 examples to avoid logging too frequently
train stats after 33600 examples: {'rewards_train/chosen': '-0.41167', 'rewards_train/rejected': '-0.66055', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24888', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-168.85', 'loss/train': '0.67401', 'examples_per_second': '124.67', 'grad_norm': '28.842', 'counters/examples': 33600, 'counters/updates': 525}
skipping logging after 33664 examples to avoid logging too frequently
train stats after 33728 examples: {'rewards_train/chosen': '-0.39295', 'rewards_train/rejected': '-0.69045', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.2975', 'logps_train/rejected': '-143.2', 'logps_train/chosen': '-154.71', 'loss/train': '0.61798', 'examples_per_second': '118.4', 'grad_norm': '23.96', 'counters/examples': 33728, 'counters/updates': 527}
skipping logging after 33792 examples to avoid logging too frequently
train stats after 33856 examples: {'rewards_train/chosen': '-0.46634', 'rewards_train/rejected': '-0.69375', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22741', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-152.86', 'loss/train': '0.6485', 'examples_per_second': '123.77', 'grad_norm': '24.27', 'counters/examples': 33856, 'counters/updates': 529}
skipping logging after 33920 examples to avoid logging too frequently
train stats after 33984 examples: {'rewards_train/chosen': '-0.45828', 'rewards_train/rejected': '-0.81343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.35514', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-156.91', 'loss/train': '0.60407', 'examples_per_second': '124.74', 'grad_norm': '24.573', 'counters/examples': 33984, 'counters/updates': 531}
skipping logging after 34048 examples to avoid logging too frequently
train stats after 34112 examples: {'rewards_train/chosen': '-0.46029', 'rewards_train/rejected': '-0.90416', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44386', 'logps_train/rejected': '-115.4', 'logps_train/chosen': '-152.85', 'loss/train': '0.56645', 'examples_per_second': '118.04', 'grad_norm': '21.536', 'counters/examples': 34112, 'counters/updates': 533}
skipping logging after 34176 examples to avoid logging too frequently
train stats after 34240 examples: {'rewards_train/chosen': '-0.35903', 'rewards_train/rejected': '-1.1152', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.75618', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-149.23', 'loss/train': '0.46536', 'examples_per_second': '129.86', 'grad_norm': '17.875', 'counters/examples': 34240, 'counters/updates': 535}
skipping logging after 34304 examples to avoid logging too frequently
train stats after 34368 examples: {'rewards_train/chosen': '-0.77282', 'rewards_train/rejected': '-1.2815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50868', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-143.1', 'loss/train': '0.57454', 'examples_per_second': '124.55', 'grad_norm': '22.911', 'counters/examples': 34368, 'counters/updates': 537}
skipping logging after 34432 examples to avoid logging too frequently
train stats after 34496 examples: {'rewards_train/chosen': '-0.59718', 'rewards_train/rejected': '-1.0105', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.41328', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-146.45', 'loss/train': '0.57301', 'examples_per_second': '123.89', 'grad_norm': '21.258', 'counters/examples': 34496, 'counters/updates': 539}
skipping logging after 34560 examples to avoid logging too frequently
train stats after 34624 examples: {'rewards_train/chosen': '-0.41145', 'rewards_train/rejected': '-0.79426', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.38281', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-149.96', 'loss/train': '0.61476', 'examples_per_second': '122.19', 'grad_norm': '25.59', 'counters/examples': 34624, 'counters/updates': 541}
skipping logging after 34688 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '-0.39898', 'rewards_train/rejected': '-0.83336', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43439', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-138.71', 'loss/train': '0.59133', 'examples_per_second': '127.99', 'grad_norm': '22.255', 'counters/examples': 34752, 'counters/updates': 543}
skipping logging after 34816 examples to avoid logging too frequently
train stats after 34880 examples: {'rewards_train/chosen': '-0.3865', 'rewards_train/rejected': '-0.70488', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31838', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-149.52', 'loss/train': '0.63385', 'examples_per_second': '125.26', 'grad_norm': '23.477', 'counters/examples': 34880, 'counters/updates': 545}
skipping logging after 34944 examples to avoid logging too frequently
train stats after 35008 examples: {'rewards_train/chosen': '-0.57234', 'rewards_train/rejected': '-0.80398', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.23163', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-155.08', 'loss/train': '0.67364', 'examples_per_second': '128.72', 'grad_norm': '24.238', 'counters/examples': 35008, 'counters/updates': 547}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35136 examples: {'rewards_train/chosen': '-0.4484', 'rewards_train/rejected': '-0.80988', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36148', 'logps_train/rejected': '-131.73', 'logps_train/chosen': '-140.4', 'loss/train': '0.63441', 'examples_per_second': '125.78', 'grad_norm': '25.906', 'counters/examples': 35136, 'counters/updates': 549}
skipping logging after 35200 examples to avoid logging too frequently
train stats after 35264 examples: {'rewards_train/chosen': '-0.36189', 'rewards_train/rejected': '-0.83871', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47682', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-142.1', 'loss/train': '0.55052', 'examples_per_second': '125.34', 'grad_norm': '20.465', 'counters/examples': 35264, 'counters/updates': 551}
skipping logging after 35328 examples to avoid logging too frequently
train stats after 35392 examples: {'rewards_train/chosen': '-0.37424', 'rewards_train/rejected': '-0.75333', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37908', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-167.81', 'loss/train': '0.60287', 'examples_per_second': '125.05', 'grad_norm': '25.554', 'counters/examples': 35392, 'counters/updates': 553}
skipping logging after 35456 examples to avoid logging too frequently
train stats after 35520 examples: {'rewards_train/chosen': '-0.41815', 'rewards_train/rejected': '-1.0677', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.64953', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-161.94', 'loss/train': '0.5078', 'examples_per_second': '125.04', 'grad_norm': '21.407', 'counters/examples': 35520, 'counters/updates': 555}
skipping logging after 35584 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '-0.36284', 'rewards_train/rejected': '-0.88258', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.51973', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-147.36', 'loss/train': '0.54791', 'examples_per_second': '125.86', 'grad_norm': '22.415', 'counters/examples': 35648, 'counters/updates': 557}
skipping logging after 35712 examples to avoid logging too frequently
train stats after 35776 examples: {'rewards_train/chosen': '-0.28375', 'rewards_train/rejected': '-0.67876', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39501', 'logps_train/rejected': '-169.95', 'logps_train/chosen': '-156.76', 'loss/train': '0.5943', 'examples_per_second': '119.27', 'grad_norm': '25.433', 'counters/examples': 35776, 'counters/updates': 559}
skipping logging after 35840 examples to avoid logging too frequently
train stats after 35904 examples: {'rewards_train/chosen': '-0.37941', 'rewards_train/rejected': '-0.69187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31246', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-155.18', 'loss/train': '0.65397', 'examples_per_second': '84.704', 'grad_norm': '25.522', 'counters/examples': 35904, 'counters/updates': 561}
Running evaluation after 35904 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.20it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.26it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.28it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.25it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.23it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.21it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.14it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.14it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.19it/s]
eval after 35904: {'rewards_eval/chosen': '-0.25855', 'rewards_eval/rejected': '-0.60594', 'rewards_eval/accuracies': '0.65625', 'rewards_eval/margins': '0.34738', 'logps_eval/rejected': '-132.14', 'logps_eval/chosen': '-150.49', 'loss/eval': '0.63774'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-35904...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-35904/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-35904/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-35904/scheduler.pt...
train stats after 35968 examples: {'rewards_train/chosen': '-0.33306', 'rewards_train/rejected': '-0.69329', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36024', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-132.22', 'loss/train': '0.59128', 'examples_per_second': '114.3', 'grad_norm': '22.727', 'counters/examples': 35968, 'counters/updates': 562}
skipping logging after 36032 examples to avoid logging too frequently
train stats after 36096 examples: {'rewards_train/chosen': '-0.25347', 'rewards_train/rejected': '-0.59208', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3386', 'logps_train/rejected': '-160.65', 'logps_train/chosen': '-138.31', 'loss/train': '0.61551', 'examples_per_second': '125.11', 'grad_norm': '24.225', 'counters/examples': 36096, 'counters/updates': 564}
skipping logging after 36160 examples to avoid logging too frequently
train stats after 36224 examples: {'rewards_train/chosen': '-0.27515', 'rewards_train/rejected': '-0.55908', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28392', 'logps_train/rejected': '-156.29', 'logps_train/chosen': '-155.61', 'loss/train': '0.62968', 'examples_per_second': '115.79', 'grad_norm': '24.624', 'counters/examples': 36224, 'counters/updates': 566}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36352 examples: {'rewards_train/chosen': '-0.4085', 'rewards_train/rejected': '-0.92668', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.51819', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-165.7', 'loss/train': '0.56029', 'examples_per_second': '130.39', 'grad_norm': '23.618', 'counters/examples': 36352, 'counters/updates': 568}
skipping logging after 36416 examples to avoid logging too frequently
train stats after 36480 examples: {'rewards_train/chosen': '-0.17664', 'rewards_train/rejected': '-0.60207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42542', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-149.42', 'loss/train': '0.57467', 'examples_per_second': '130.12', 'grad_norm': '22.52', 'counters/examples': 36480, 'counters/updates': 570}
skipping logging after 36544 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '-0.4166', 'rewards_train/rejected': '-0.81561', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.39902', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-146.07', 'loss/train': '0.56994', 'examples_per_second': '121.61', 'grad_norm': '22.903', 'counters/examples': 36608, 'counters/updates': 572}
skipping logging after 36672 examples to avoid logging too frequently
train stats after 36736 examples: {'rewards_train/chosen': '-0.59096', 'rewards_train/rejected': '-0.89892', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.30796', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-163.09', 'loss/train': '0.67441', 'examples_per_second': '124.71', 'grad_norm': '24.822', 'counters/examples': 36736, 'counters/updates': 574}
skipping logging after 36800 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '-0.43732', 'rewards_train/rejected': '-0.94017', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.50285', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-169.25', 'loss/train': '0.58128', 'examples_per_second': '121.11', 'grad_norm': '23.628', 'counters/examples': 36864, 'counters/updates': 576}
skipping logging after 36928 examples to avoid logging too frequently
train stats after 36992 examples: {'rewards_train/chosen': '-0.39619', 'rewards_train/rejected': '-0.73034', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.33414', 'logps_train/rejected': '-156.85', 'logps_train/chosen': '-144.56', 'loss/train': '0.63043', 'examples_per_second': '125.87', 'grad_norm': '25.016', 'counters/examples': 36992, 'counters/updates': 578}
skipping logging after 37056 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '-0.45326', 'rewards_train/rejected': '-0.95612', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50286', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-168.92', 'loss/train': '0.5848', 'examples_per_second': '138.81', 'grad_norm': '25.906', 'counters/examples': 37120, 'counters/updates': 580}
skipping logging after 37184 examples to avoid logging too frequently
train stats after 37248 examples: {'rewards_train/chosen': '-0.51178', 'rewards_train/rejected': '-1.0878', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57603', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-146.52', 'loss/train': '0.54092', 'examples_per_second': '125.79', 'grad_norm': '23.199', 'counters/examples': 37248, 'counters/updates': 582}
skipping logging after 37312 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.50258', 'rewards_train/rejected': '-1.1237', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.62114', 'logps_train/rejected': '-109.98', 'logps_train/chosen': '-139.11', 'loss/train': '0.53263', 'examples_per_second': '124.34', 'grad_norm': '21.515', 'counters/examples': 37376, 'counters/updates': 584}
skipping logging after 37440 examples to avoid logging too frequently
train stats after 37504 examples: {'rewards_train/chosen': '-0.6859', 'rewards_train/rejected': '-1.0377', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.35183', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-148.4', 'loss/train': '0.62128', 'examples_per_second': '120.74', 'grad_norm': '25.632', 'counters/examples': 37504, 'counters/updates': 586}
skipping logging after 37568 examples to avoid logging too frequently
train stats after 37632 examples: {'rewards_train/chosen': '-0.51141', 'rewards_train/rejected': '-0.85683', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34542', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-155.14', 'loss/train': '0.6293', 'examples_per_second': '119.63', 'grad_norm': '23.648', 'counters/examples': 37632, 'counters/updates': 588}
skipping logging after 37696 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '-0.59338', 'rewards_train/rejected': '-1.071', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4776', 'logps_train/rejected': '-131.03', 'logps_train/chosen': '-165.24', 'loss/train': '0.56087', 'examples_per_second': '124.57', 'grad_norm': '23.444', 'counters/examples': 37760, 'counters/updates': 590}
skipping logging after 37824 examples to avoid logging too frequently
train stats after 37888 examples: {'rewards_train/chosen': '-0.47358', 'rewards_train/rejected': '-0.98564', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.51206', 'logps_train/rejected': '-126.99', 'logps_train/chosen': '-161.57', 'loss/train': '0.53829', 'examples_per_second': '124.89', 'grad_norm': '21.92', 'counters/examples': 37888, 'counters/updates': 592}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 38016 examples: {'rewards_train/chosen': '-0.4242', 'rewards_train/rejected': '-0.78435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36015', 'logps_train/rejected': '-143.92', 'logps_train/chosen': '-167.06', 'loss/train': '0.62738', 'examples_per_second': '124.49', 'grad_norm': '24.412', 'counters/examples': 38016, 'counters/updates': 594}
skipping logging after 38080 examples to avoid logging too frequently
train stats after 38144 examples: {'rewards_train/chosen': '-0.7389', 'rewards_train/rejected': '-0.97794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23904', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-149.32', 'loss/train': '0.69214', 'examples_per_second': '125.13', 'grad_norm': '26.892', 'counters/examples': 38144, 'counters/updates': 596}
skipping logging after 38208 examples to avoid logging too frequently
train stats after 38272 examples: {'rewards_train/chosen': '-0.52529', 'rewards_train/rejected': '-0.88699', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3617', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-156.08', 'loss/train': '0.63764', 'examples_per_second': '119.61', 'grad_norm': '24.216', 'counters/examples': 38272, 'counters/updates': 598}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38400 examples: {'rewards_train/chosen': '-0.50504', 'rewards_train/rejected': '-0.82492', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31988', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-166.26', 'loss/train': '0.63666', 'examples_per_second': '124.55', 'grad_norm': '26.516', 'counters/examples': 38400, 'counters/updates': 600}
skipping logging after 38464 examples to avoid logging too frequently
train stats after 38528 examples: {'rewards_train/chosen': '-0.5236', 'rewards_train/rejected': '-0.93653', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.41292', 'logps_train/rejected': '-150.63', 'logps_train/chosen': '-164.34', 'loss/train': '0.61678', 'examples_per_second': '119.74', 'grad_norm': '25.778', 'counters/examples': 38528, 'counters/updates': 602}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38656 examples: {'rewards_train/chosen': '-0.44962', 'rewards_train/rejected': '-0.75112', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3015', 'logps_train/rejected': '-133.44', 'logps_train/chosen': '-158.64', 'loss/train': '0.6431', 'examples_per_second': '119.22', 'grad_norm': '27.189', 'counters/examples': 38656, 'counters/updates': 604}
skipping logging after 38720 examples to avoid logging too frequently
train stats after 38784 examples: {'rewards_train/chosen': '-0.46997', 'rewards_train/rejected': '-0.90858', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.4386', 'logps_train/rejected': '-155.55', 'logps_train/chosen': '-165.56', 'loss/train': '0.56296', 'examples_per_second': '117.21', 'grad_norm': '23.044', 'counters/examples': 38784, 'counters/updates': 606}
skipping logging after 38848 examples to avoid logging too frequently
train stats after 38912 examples: {'rewards_train/chosen': '-0.44336', 'rewards_train/rejected': '-1.0874', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.644', 'logps_train/rejected': '-135.01', 'logps_train/chosen': '-151.9', 'loss/train': '0.5334', 'examples_per_second': '124.72', 'grad_norm': '20.513', 'counters/examples': 38912, 'counters/updates': 608}
skipping logging after 38976 examples to avoid logging too frequently
train stats after 39040 examples: {'rewards_train/chosen': '-0.51815', 'rewards_train/rejected': '-1.1176', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.59943', 'logps_train/rejected': '-150.69', 'logps_train/chosen': '-142.34', 'loss/train': '0.52487', 'examples_per_second': '123.91', 'grad_norm': '21.189', 'counters/examples': 39040, 'counters/updates': 610}
skipping logging after 39104 examples to avoid logging too frequently
train stats after 39168 examples: {'rewards_train/chosen': '-0.80351', 'rewards_train/rejected': '-1.2366', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.43305', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-162.26', 'loss/train': '0.63817', 'examples_per_second': '121.49', 'grad_norm': '27.018', 'counters/examples': 39168, 'counters/updates': 612}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39296 examples: {'rewards_train/chosen': '-0.61403', 'rewards_train/rejected': '-1.007', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39298', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-151.28', 'loss/train': '0.61451', 'examples_per_second': '127.94', 'grad_norm': '22.964', 'counters/examples': 39296, 'counters/updates': 614}
skipping logging after 39360 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '-0.49067', 'rewards_train/rejected': '-0.88488', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39421', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-143.36', 'loss/train': '0.59074', 'examples_per_second': '124.49', 'grad_norm': '21.651', 'counters/examples': 39424, 'counters/updates': 616}
skipping logging after 39488 examples to avoid logging too frequently
train stats after 39552 examples: {'rewards_train/chosen': '-0.40376', 'rewards_train/rejected': '-0.82356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4198', 'logps_train/rejected': '-136.39', 'logps_train/chosen': '-139.53', 'loss/train': '0.61169', 'examples_per_second': '117.16', 'grad_norm': '22.324', 'counters/examples': 39552, 'counters/updates': 618}
skipping logging after 39616 examples to avoid logging too frequently
train stats after 39680 examples: {'rewards_train/chosen': '-0.51125', 'rewards_train/rejected': '-0.83173', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.32048', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-157.68', 'loss/train': '0.64198', 'examples_per_second': '124.15', 'grad_norm': '24.057', 'counters/examples': 39680, 'counters/updates': 620}
skipping logging after 39744 examples to avoid logging too frequently
train stats after 39808 examples: {'rewards_train/chosen': '-0.47351', 'rewards_train/rejected': '-0.75277', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.27926', 'logps_train/rejected': '-141.74', 'logps_train/chosen': '-151.34', 'loss/train': '0.6404', 'examples_per_second': '128.68', 'grad_norm': '24.972', 'counters/examples': 39808, 'counters/updates': 622}
skipping logging after 39872 examples to avoid logging too frequently
train stats after 39936 examples: {'rewards_train/chosen': '-0.39431', 'rewards_train/rejected': '-0.76969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37537', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-169.6', 'loss/train': '0.5961', 'examples_per_second': '125.47', 'grad_norm': '24.189', 'counters/examples': 39936, 'counters/updates': 624}
skipping logging after 40000 examples to avoid logging too frequently
train stats after 40064 examples: {'rewards_train/chosen': '-0.33206', 'rewards_train/rejected': '-0.91381', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58175', 'logps_train/rejected': '-126.17', 'logps_train/chosen': '-158.11', 'loss/train': '0.53748', 'examples_per_second': '119.76', 'grad_norm': '25.246', 'counters/examples': 40064, 'counters/updates': 626}
skipping logging after 40128 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '-0.49733', 'rewards_train/rejected': '-1.0149', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.5176', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-156.79', 'loss/train': '0.56741', 'examples_per_second': '134', 'grad_norm': '21.691', 'counters/examples': 40192, 'counters/updates': 628}
skipping logging after 40256 examples to avoid logging too frequently
train stats after 40320 examples: {'rewards_train/chosen': '-0.4578', 'rewards_train/rejected': '-0.96975', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.51195', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-147.71', 'loss/train': '0.5592', 'examples_per_second': '124.46', 'grad_norm': '21.023', 'counters/examples': 40320, 'counters/updates': 630}
skipping logging after 40384 examples to avoid logging too frequently
train stats after 40448 examples: {'rewards_train/chosen': '-0.65107', 'rewards_train/rejected': '-1.1311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48008', 'logps_train/rejected': '-171.06', 'logps_train/chosen': '-141.83', 'loss/train': '0.58566', 'examples_per_second': '119.4', 'grad_norm': '23.809', 'counters/examples': 40448, 'counters/updates': 632}
skipping logging after 40512 examples to avoid logging too frequently
train stats after 40576 examples: {'rewards_train/chosen': '-0.47224', 'rewards_train/rejected': '-0.68393', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.21169', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-156.98', 'loss/train': '0.66527', 'examples_per_second': '124.47', 'grad_norm': '27.019', 'counters/examples': 40576, 'counters/updates': 634}
skipping logging after 40640 examples to avoid logging too frequently
train stats after 40704 examples: {'rewards_train/chosen': '-0.40655', 'rewards_train/rejected': '-0.75058', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34403', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-139.08', 'loss/train': '0.6718', 'examples_per_second': '125.02', 'grad_norm': '24.468', 'counters/examples': 40704, 'counters/updates': 636}
skipping logging after 40768 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.53818', 'rewards_train/rejected': '-0.95241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41423', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-141.09', 'loss/train': '0.59825', 'examples_per_second': '146.2', 'grad_norm': '22.326', 'counters/examples': 40832, 'counters/updates': 638}
skipping logging after 40896 examples to avoid logging too frequently
train stats after 40960 examples: {'rewards_train/chosen': '-0.54788', 'rewards_train/rejected': '-0.92736', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37948', 'logps_train/rejected': '-145.66', 'logps_train/chosen': '-134.82', 'loss/train': '0.60608', 'examples_per_second': '128.84', 'grad_norm': '23.2', 'counters/examples': 40960, 'counters/updates': 640}
skipping logging after 41024 examples to avoid logging too frequently
train stats after 41088 examples: {'rewards_train/chosen': '-0.55793', 'rewards_train/rejected': '-0.7442', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18627', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-171.96', 'loss/train': '0.70614', 'examples_per_second': '124.91', 'grad_norm': '27.304', 'counters/examples': 41088, 'counters/updates': 642}
skipping logging after 41152 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '-0.54572', 'rewards_train/rejected': '-0.90597', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36025', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-145.02', 'loss/train': '0.60462', 'examples_per_second': '138.02', 'grad_norm': '23.792', 'counters/examples': 41216, 'counters/updates': 644}
skipping logging after 41280 examples to avoid logging too frequently
train stats after 41344 examples: {'rewards_train/chosen': '-0.61556', 'rewards_train/rejected': '-0.9163', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30074', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-135.95', 'loss/train': '0.68603', 'examples_per_second': '131.04', 'grad_norm': '23.102', 'counters/examples': 41344, 'counters/updates': 646}
skipping logging after 41408 examples to avoid logging too frequently
train stats after 41472 examples: {'rewards_train/chosen': '-0.62386', 'rewards_train/rejected': '-0.79956', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.1757', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-170.67', 'loss/train': '0.6874', 'examples_per_second': '124.97', 'grad_norm': '25.257', 'counters/examples': 41472, 'counters/updates': 648}
skipping logging after 41536 examples to avoid logging too frequently
train stats after 41600 examples: {'rewards_train/chosen': '-0.43898', 'rewards_train/rejected': '-0.72109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28211', 'logps_train/rejected': '-154.03', 'logps_train/chosen': '-136.64', 'loss/train': '0.64764', 'examples_per_second': '119.38', 'grad_norm': '24.907', 'counters/examples': 41600, 'counters/updates': 650}
skipping logging after 41664 examples to avoid logging too frequently
train stats after 41728 examples: {'rewards_train/chosen': '-0.62749', 'rewards_train/rejected': '-0.84338', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21589', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-156.91', 'loss/train': '0.70742', 'examples_per_second': '124.33', 'grad_norm': '27.406', 'counters/examples': 41728, 'counters/updates': 652}
skipping logging after 41792 examples to avoid logging too frequently
train stats after 41856 examples: {'rewards_train/chosen': '-0.48409', 'rewards_train/rejected': '-0.93493', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.45084', 'logps_train/rejected': '-132.54', 'logps_train/chosen': '-150.85', 'loss/train': '0.59335', 'examples_per_second': '124.58', 'grad_norm': '28.042', 'counters/examples': 41856, 'counters/updates': 654}
skipping logging after 41920 examples to avoid logging too frequently
train stats after 41984 examples: {'rewards_train/chosen': '-0.32257', 'rewards_train/rejected': '-0.65574', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33317', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-143.77', 'loss/train': '0.61709', 'examples_per_second': '124.96', 'grad_norm': '24.75', 'counters/examples': 41984, 'counters/updates': 656}
skipping logging after 42048 examples to avoid logging too frequently
train stats after 42112 examples: {'rewards_train/chosen': '-0.28875', 'rewards_train/rejected': '-0.93995', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.6512', 'logps_train/rejected': '-142.98', 'logps_train/chosen': '-128.96', 'loss/train': '0.49993', 'examples_per_second': '124.42', 'grad_norm': '23.129', 'counters/examples': 42112, 'counters/updates': 658}
skipping logging after 42176 examples to avoid logging too frequently
train stats after 42240 examples: {'rewards_train/chosen': '-0.54653', 'rewards_train/rejected': '-1.1636', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.61705', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-162.76', 'loss/train': '0.54145', 'examples_per_second': '119.13', 'grad_norm': '23.14', 'counters/examples': 42240, 'counters/updates': 660}
skipping logging after 42304 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '-0.65118', 'rewards_train/rejected': '-1.2236', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57237', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-135.65', 'loss/train': '0.55421', 'examples_per_second': '139.91', 'grad_norm': '23.616', 'counters/examples': 42368, 'counters/updates': 662}
skipping logging after 42432 examples to avoid logging too frequently
train stats after 42496 examples: {'rewards_train/chosen': '-0.52917', 'rewards_train/rejected': '-0.92891', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39974', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-148.63', 'loss/train': '0.61417', 'examples_per_second': '123.67', 'grad_norm': '23.215', 'counters/examples': 42496, 'counters/updates': 664}
skipping logging after 42560 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '-0.5272', 'rewards_train/rejected': '-1.0105', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48333', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-148.35', 'loss/train': '0.58826', 'examples_per_second': '120.4', 'grad_norm': '20.989', 'counters/examples': 42624, 'counters/updates': 666}
skipping logging after 42688 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '-0.54728', 'rewards_train/rejected': '-0.98365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43637', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-151.67', 'loss/train': '0.5934', 'examples_per_second': '124.65', 'grad_norm': '23.541', 'counters/examples': 42752, 'counters/updates': 668}
skipping logging after 42816 examples to avoid logging too frequently
train stats after 42880 examples: {'rewards_train/chosen': '-0.4398', 'rewards_train/rejected': '-0.92481', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48501', 'logps_train/rejected': '-143.72', 'logps_train/chosen': '-123.75', 'loss/train': '0.56294', 'examples_per_second': '130.3', 'grad_norm': '20.441', 'counters/examples': 42880, 'counters/updates': 670}
skipping logging after 42944 examples to avoid logging too frequently
train stats after 43008 examples: {'rewards_train/chosen': '-0.54478', 'rewards_train/rejected': '-0.85999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31521', 'logps_train/rejected': '-150.09', 'logps_train/chosen': '-174.83', 'loss/train': '0.64779', 'examples_per_second': '124.03', 'grad_norm': '28.088', 'counters/examples': 43008, 'counters/updates': 672}
skipping logging after 43072 examples to avoid logging too frequently
train stats after 43136 examples: {'rewards_train/chosen': '-0.54789', 'rewards_train/rejected': '-0.89779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3499', 'logps_train/rejected': '-142.5', 'logps_train/chosen': '-151.18', 'loss/train': '0.59864', 'examples_per_second': '125.11', 'grad_norm': '22.774', 'counters/examples': 43136, 'counters/updates': 674}
skipping logging after 43200 examples to avoid logging too frequently
train stats after 43264 examples: {'rewards_train/chosen': '-0.40199', 'rewards_train/rejected': '-0.96643', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56444', 'logps_train/rejected': '-137.82', 'logps_train/chosen': '-158.03', 'loss/train': '0.54336', 'examples_per_second': '121.52', 'grad_norm': '23.58', 'counters/examples': 43264, 'counters/updates': 676}
skipping logging after 43328 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '-0.34433', 'rewards_train/rejected': '-0.76539', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42107', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-152.49', 'loss/train': '0.62346', 'examples_per_second': '121.4', 'grad_norm': '24.612', 'counters/examples': 43392, 'counters/updates': 678}
skipping logging after 43456 examples to avoid logging too frequently
train stats after 43520 examples: {'rewards_train/chosen': '-0.36315', 'rewards_train/rejected': '-0.90839', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54524', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-144.6', 'loss/train': '0.5502', 'examples_per_second': '118.87', 'grad_norm': '21.813', 'counters/examples': 43520, 'counters/updates': 680}
skipping logging after 43584 examples to avoid logging too frequently
train stats after 43648 examples: {'rewards_train/chosen': '-0.47565', 'rewards_train/rejected': '-1.0431', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5675', 'logps_train/rejected': '-100.24', 'logps_train/chosen': '-140.74', 'loss/train': '0.55484', 'examples_per_second': '123.11', 'grad_norm': '21.793', 'counters/examples': 43648, 'counters/updates': 682}
skipping logging after 43712 examples to avoid logging too frequently
train stats after 43776 examples: {'rewards_train/chosen': '-0.61197', 'rewards_train/rejected': '-0.9321', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.32013', 'logps_train/rejected': '-123.15', 'logps_train/chosen': '-155.27', 'loss/train': '0.62517', 'examples_per_second': '124.6', 'grad_norm': '25.117', 'counters/examples': 43776, 'counters/updates': 684}
skipping logging after 43840 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '-0.60841', 'rewards_train/rejected': '-1.0404', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.432', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-148.3', 'loss/train': '0.64413', 'examples_per_second': '138.76', 'grad_norm': '25.266', 'counters/examples': 43904, 'counters/updates': 686}
skipping logging after 43968 examples to avoid logging too frequently
train stats after 44032 examples: {'rewards_train/chosen': '-0.54764', 'rewards_train/rejected': '-0.81437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.26673', 'logps_train/rejected': '-148.96', 'logps_train/chosen': '-163.9', 'loss/train': '0.70024', 'examples_per_second': '125.86', 'grad_norm': '28.526', 'counters/examples': 44032, 'counters/updates': 688}
skipping logging after 44096 examples to avoid logging too frequently
train stats after 44160 examples: {'rewards_train/chosen': '-0.58078', 'rewards_train/rejected': '-0.90222', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32144', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-168.57', 'loss/train': '0.6899', 'examples_per_second': '124.52', 'grad_norm': '29.701', 'counters/examples': 44160, 'counters/updates': 690}
skipping logging after 44224 examples to avoid logging too frequently
train stats after 44288 examples: {'rewards_train/chosen': '-0.63954', 'rewards_train/rejected': '-0.82896', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18941', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-145.49', 'loss/train': '0.69575', 'examples_per_second': '127.98', 'grad_norm': '25.823', 'counters/examples': 44288, 'counters/updates': 692}
skipping logging after 44352 examples to avoid logging too frequently
train stats after 44416 examples: {'rewards_train/chosen': '-0.45678', 'rewards_train/rejected': '-0.86942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41264', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-151.8', 'loss/train': '0.64148', 'examples_per_second': '124.48', 'grad_norm': '24.944', 'counters/examples': 44416, 'counters/updates': 694}
skipping logging after 44480 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '-0.59255', 'rewards_train/rejected': '-1.0735', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48091', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-136.26', 'loss/train': '0.59166', 'examples_per_second': '127.58', 'grad_norm': '22.824', 'counters/examples': 44544, 'counters/updates': 696}
skipping logging after 44608 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '-0.51876', 'rewards_train/rejected': '-0.75385', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.23509', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-145.68', 'loss/train': '0.66513', 'examples_per_second': '124.54', 'grad_norm': '24.901', 'counters/examples': 44672, 'counters/updates': 698}
skipping logging after 44736 examples to avoid logging too frequently
train stats after 44800 examples: {'rewards_train/chosen': '-0.55437', 'rewards_train/rejected': '-0.94718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3928', 'logps_train/rejected': '-155.29', 'logps_train/chosen': '-178.04', 'loss/train': '0.61168', 'examples_per_second': '124.71', 'grad_norm': '28.993', 'counters/examples': 44800, 'counters/updates': 700}
skipping logging after 44864 examples to avoid logging too frequently
train stats after 44928 examples: {'rewards_train/chosen': '-0.54504', 'rewards_train/rejected': '-0.79234', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.2473', 'logps_train/rejected': '-158.08', 'logps_train/chosen': '-161.69', 'loss/train': '0.65261', 'examples_per_second': '124.01', 'grad_norm': '24.367', 'counters/examples': 44928, 'counters/updates': 702}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45056 examples: {'rewards_train/chosen': '-0.33452', 'rewards_train/rejected': '-0.67201', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33749', 'logps_train/rejected': '-148.34', 'logps_train/chosen': '-154.85', 'loss/train': '0.61745', 'examples_per_second': '124.69', 'grad_norm': '25.265', 'counters/examples': 45056, 'counters/updates': 704}
skipping logging after 45120 examples to avoid logging too frequently
train stats after 45184 examples: {'rewards_train/chosen': '-0.63733', 'rewards_train/rejected': '-1.0943', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45694', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-138.74', 'loss/train': '0.56781', 'examples_per_second': '122.76', 'grad_norm': '20.309', 'counters/examples': 45184, 'counters/updates': 706}
skipping logging after 45248 examples to avoid logging too frequently
train stats after 45312 examples: {'rewards_train/chosen': '-0.63784', 'rewards_train/rejected': '-1.2085', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.57064', 'logps_train/rejected': '-139.47', 'logps_train/chosen': '-153.9', 'loss/train': '0.52503', 'examples_per_second': '128.27', 'grad_norm': '19.765', 'counters/examples': 45312, 'counters/updates': 708}
skipping logging after 45376 examples to avoid logging too frequently
train stats after 45440 examples: {'rewards_train/chosen': '-0.73773', 'rewards_train/rejected': '-0.95398', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21625', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-137.07', 'loss/train': '0.69348', 'examples_per_second': '123.77', 'grad_norm': '26.169', 'counters/examples': 45440, 'counters/updates': 710}
skipping logging after 45504 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '-0.68504', 'rewards_train/rejected': '-0.94056', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.25552', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-133.5', 'loss/train': '0.66507', 'examples_per_second': '134.13', 'grad_norm': '24.117', 'counters/examples': 45568, 'counters/updates': 712}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45696 examples: {'rewards_train/chosen': '-0.59428', 'rewards_train/rejected': '-1.0392', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.44496', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-141.91', 'loss/train': '0.58376', 'examples_per_second': '124.54', 'grad_norm': '22.95', 'counters/examples': 45696, 'counters/updates': 714}
skipping logging after 45760 examples to avoid logging too frequently
train stats after 45824 examples: {'rewards_train/chosen': '-0.54596', 'rewards_train/rejected': '-1.069', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52303', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-143.82', 'loss/train': '0.55311', 'examples_per_second': '124.55', 'grad_norm': '20.936', 'counters/examples': 45824, 'counters/updates': 716}
skipping logging after 45888 examples to avoid logging too frequently
train stats after 45952 examples: {'rewards_train/chosen': '-0.71216', 'rewards_train/rejected': '-1.0997', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38753', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-154.78', 'loss/train': '0.60824', 'examples_per_second': '130.93', 'grad_norm': '24.802', 'counters/examples': 45952, 'counters/updates': 718}
skipping logging after 46016 examples to avoid logging too frequently
train stats after 46080 examples: {'rewards_train/chosen': '-0.60492', 'rewards_train/rejected': '-0.914', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.30908', 'logps_train/rejected': '-160.53', 'logps_train/chosen': '-145.47', 'loss/train': '0.62982', 'examples_per_second': '119.19', 'grad_norm': '22.942', 'counters/examples': 46080, 'counters/updates': 720}
skipping logging after 46144 examples to avoid logging too frequently
train stats after 46208 examples: {'rewards_train/chosen': '-0.59351', 'rewards_train/rejected': '-1.1749', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58141', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-163.44', 'loss/train': '0.54138', 'examples_per_second': '124.6', 'grad_norm': '22.603', 'counters/examples': 46208, 'counters/updates': 722}
skipping logging after 46272 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '-0.65786', 'rewards_train/rejected': '-0.84383', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.18596', 'logps_train/rejected': '-140.27', 'logps_train/chosen': '-157.38', 'loss/train': '0.7207', 'examples_per_second': '126.2', 'grad_norm': '26.996', 'counters/examples': 46336, 'counters/updates': 724}
skipping logging after 46400 examples to avoid logging too frequently
train stats after 46464 examples: {'rewards_train/chosen': '-0.65077', 'rewards_train/rejected': '-0.98358', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33281', 'logps_train/rejected': '-140.19', 'logps_train/chosen': '-127.98', 'loss/train': '0.60687', 'examples_per_second': '125.58', 'grad_norm': '22.129', 'counters/examples': 46464, 'counters/updates': 726}
skipping logging after 46528 examples to avoid logging too frequently
train stats after 46592 examples: {'rewards_train/chosen': '-0.66932', 'rewards_train/rejected': '-1.1631', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49374', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-147.63', 'loss/train': '0.57309', 'examples_per_second': '118.84', 'grad_norm': '23.871', 'counters/examples': 46592, 'counters/updates': 728}
skipping logging after 46656 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '-0.57592', 'rewards_train/rejected': '-0.92921', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.35329', 'logps_train/rejected': '-157.48', 'logps_train/chosen': '-173.16', 'loss/train': '0.63631', 'examples_per_second': '121.21', 'grad_norm': '25.953', 'counters/examples': 46720, 'counters/updates': 730}
skipping logging after 46784 examples to avoid logging too frequently
train stats after 46848 examples: {'rewards_train/chosen': '-0.67776', 'rewards_train/rejected': '-1.2682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59044', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-157.18', 'loss/train': '0.5379', 'examples_per_second': '119.05', 'grad_norm': '21.205', 'counters/examples': 46848, 'counters/updates': 732}
skipping logging after 46912 examples to avoid logging too frequently
train stats after 46976 examples: {'rewards_train/chosen': '-0.67443', 'rewards_train/rejected': '-1.0844', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40998', 'logps_train/rejected': '-124.05', 'logps_train/chosen': '-158.82', 'loss/train': '0.62765', 'examples_per_second': '119.5', 'grad_norm': '23.073', 'counters/examples': 46976, 'counters/updates': 734}
skipping logging after 47040 examples to avoid logging too frequently
train stats after 47104 examples: {'rewards_train/chosen': '-0.44835', 'rewards_train/rejected': '-1.0909', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.64259', 'logps_train/rejected': '-141.33', 'logps_train/chosen': '-161.02', 'loss/train': '0.54386', 'examples_per_second': '127.72', 'grad_norm': '21.798', 'counters/examples': 47104, 'counters/updates': 736}
skipping logging after 47168 examples to avoid logging too frequently
train stats after 47232 examples: {'rewards_train/chosen': '-0.5274', 'rewards_train/rejected': '-1.1426', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61521', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-131.15', 'loss/train': '0.51606', 'examples_per_second': '124.96', 'grad_norm': '19.232', 'counters/examples': 47232, 'counters/updates': 738}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47360 examples: {'rewards_train/chosen': '-0.65274', 'rewards_train/rejected': '-1.104', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.45126', 'logps_train/rejected': '-136.62', 'logps_train/chosen': '-171.94', 'loss/train': '0.59084', 'examples_per_second': '126.34', 'grad_norm': '23.647', 'counters/examples': 47360, 'counters/updates': 740}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47488 examples: {'rewards_train/chosen': '-0.73442', 'rewards_train/rejected': '-1.1406', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.40618', 'logps_train/rejected': '-151.94', 'logps_train/chosen': '-170.8', 'loss/train': '0.63879', 'examples_per_second': '125.16', 'grad_norm': '26.237', 'counters/examples': 47488, 'counters/updates': 742}
skipping logging after 47552 examples to avoid logging too frequently
train stats after 47616 examples: {'rewards_train/chosen': '-0.53597', 'rewards_train/rejected': '-0.97185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43588', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-164.62', 'loss/train': '0.60457', 'examples_per_second': '119.06', 'grad_norm': '26.671', 'counters/examples': 47616, 'counters/updates': 744}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47744 examples: {'rewards_train/chosen': '-0.55411', 'rewards_train/rejected': '-0.96069', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.40659', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-140.08', 'loss/train': '0.59694', 'examples_per_second': '126.18', 'grad_norm': '22.511', 'counters/examples': 47744, 'counters/updates': 746}
skipping logging after 47808 examples to avoid logging too frequently
train stats after 47872 examples: {'rewards_train/chosen': '-0.5612', 'rewards_train/rejected': '-0.88388', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.32268', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-129.9', 'loss/train': '0.60607', 'examples_per_second': '119.09', 'grad_norm': '25.265', 'counters/examples': 47872, 'counters/updates': 748}
Running evaluation after 47872 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  9.96it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01, 10.07it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01, 10.07it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:00, 10.14it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:00<00:00, 10.18it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00, 10.11it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00, 10.11it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:01<00:00, 10.10it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.10it/s]
eval after 47872: {'rewards_eval/chosen': '-0.49751', 'rewards_eval/rejected': '-0.92178', 'rewards_eval/accuracies': '0.67578', 'rewards_eval/margins': '0.42427', 'logps_eval/rejected': '-135.3', 'logps_eval/chosen': '-152.88', 'loss/eval': '0.6309'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-47872...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-47872/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-47872/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-47872/scheduler.pt...
train stats after 47936 examples: {'rewards_train/chosen': '-0.64498', 'rewards_train/rejected': '-0.94159', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29661', 'logps_train/rejected': '-144.89', 'logps_train/chosen': '-163.86', 'loss/train': '0.64016', 'examples_per_second': '119.35', 'grad_norm': '24.482', 'counters/examples': 47936, 'counters/updates': 749}
skipping logging after 48000 examples to avoid logging too frequently
train stats after 48064 examples: {'rewards_train/chosen': '-0.48601', 'rewards_train/rejected': '-0.93463', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.44862', 'logps_train/rejected': '-136.6', 'logps_train/chosen': '-161.17', 'loss/train': '0.58021', 'examples_per_second': '128.89', 'grad_norm': '25.394', 'counters/examples': 48064, 'counters/updates': 751}
skipping logging after 48128 examples to avoid logging too frequently
train stats after 48192 examples: {'rewards_train/chosen': '-0.58186', 'rewards_train/rejected': '-1.087', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.50515', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-135.97', 'loss/train': '0.56676', 'examples_per_second': '123.05', 'grad_norm': '20.942', 'counters/examples': 48192, 'counters/updates': 753}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48320 examples: {'rewards_train/chosen': '-0.71157', 'rewards_train/rejected': '-0.94584', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.23427', 'logps_train/rejected': '-143.41', 'logps_train/chosen': '-142.17', 'loss/train': '0.67612', 'examples_per_second': '121.77', 'grad_norm': '25.132', 'counters/examples': 48320, 'counters/updates': 755}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48448 examples: {'rewards_train/chosen': '-0.50137', 'rewards_train/rejected': '-0.91415', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41278', 'logps_train/rejected': '-135.95', 'logps_train/chosen': '-151.49', 'loss/train': '0.59839', 'examples_per_second': '124.41', 'grad_norm': '26.598', 'counters/examples': 48448, 'counters/updates': 757}
skipping logging after 48512 examples to avoid logging too frequently
train stats after 48576 examples: {'rewards_train/chosen': '-0.56009', 'rewards_train/rejected': '-1.1797', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61958', 'logps_train/rejected': '-145.73', 'logps_train/chosen': '-156.07', 'loss/train': '0.54272', 'examples_per_second': '124.9', 'grad_norm': '23.343', 'counters/examples': 48576, 'counters/updates': 759}
skipping logging after 48640 examples to avoid logging too frequently
train stats after 48704 examples: {'rewards_train/chosen': '-0.47829', 'rewards_train/rejected': '-1.0206', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54229', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-144.8', 'loss/train': '0.55929', 'examples_per_second': '124.86', 'grad_norm': '21.281', 'counters/examples': 48704, 'counters/updates': 761}
skipping logging after 48768 examples to avoid logging too frequently
train stats after 48832 examples: {'rewards_train/chosen': '-0.7227', 'rewards_train/rejected': '-1.1888', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.4661', 'logps_train/rejected': '-167.38', 'logps_train/chosen': '-180.34', 'loss/train': '0.60925', 'examples_per_second': '124.67', 'grad_norm': '26.259', 'counters/examples': 48832, 'counters/updates': 763}
skipping logging after 48896 examples to avoid logging too frequently
train stats after 48960 examples: {'rewards_train/chosen': '-0.4655', 'rewards_train/rejected': '-0.94265', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47715', 'logps_train/rejected': '-126.45', 'logps_train/chosen': '-176.7', 'loss/train': '0.58093', 'examples_per_second': '124.69', 'grad_norm': '23.825', 'counters/examples': 48960, 'counters/updates': 765}
skipping logging after 49024 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '-0.31163', 'rewards_train/rejected': '-0.77087', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45925', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-131.4', 'loss/train': '0.57404', 'examples_per_second': '127.29', 'grad_norm': '20.766', 'counters/examples': 49088, 'counters/updates': 767}
skipping logging after 49152 examples to avoid logging too frequently
train stats after 49216 examples: {'rewards_train/chosen': '-0.44727', 'rewards_train/rejected': '-0.83134', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.38407', 'logps_train/rejected': '-161.43', 'logps_train/chosen': '-190.3', 'loss/train': '0.62149', 'examples_per_second': '124.38', 'grad_norm': '25.878', 'counters/examples': 49216, 'counters/updates': 769}
skipping logging after 49280 examples to avoid logging too frequently
train stats after 49344 examples: {'rewards_train/chosen': '-0.65125', 'rewards_train/rejected': '-1.1383', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48705', 'logps_train/rejected': '-135.03', 'logps_train/chosen': '-159.59', 'loss/train': '0.58202', 'examples_per_second': '124.39', 'grad_norm': '23.047', 'counters/examples': 49344, 'counters/updates': 771}
skipping logging after 49408 examples to avoid logging too frequently
train stats after 49472 examples: {'rewards_train/chosen': '-0.73325', 'rewards_train/rejected': '-1.0196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28636', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-165.63', 'loss/train': '0.65133', 'examples_per_second': '131.88', 'grad_norm': '24.373', 'counters/examples': 49472, 'counters/updates': 773}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49600 examples: {'rewards_train/chosen': '-0.48677', 'rewards_train/rejected': '-0.88182', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.39505', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-157.61', 'loss/train': '0.60535', 'examples_per_second': '124.97', 'grad_norm': '23.229', 'counters/examples': 49600, 'counters/updates': 775}
skipping logging after 49664 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '-0.59366', 'rewards_train/rejected': '-0.93818', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34452', 'logps_train/rejected': '-148.09', 'logps_train/chosen': '-146.54', 'loss/train': '0.63623', 'examples_per_second': '124.52', 'grad_norm': '24.362', 'counters/examples': 49728, 'counters/updates': 777}
skipping logging after 49792 examples to avoid logging too frequently
train stats after 49856 examples: {'rewards_train/chosen': '-0.54944', 'rewards_train/rejected': '-0.69177', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.14232', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-143.06', 'loss/train': '0.71596', 'examples_per_second': '123.21', 'grad_norm': '25.949', 'counters/examples': 49856, 'counters/updates': 779}
skipping logging after 49920 examples to avoid logging too frequently
train stats after 49984 examples: {'rewards_train/chosen': '-0.47448', 'rewards_train/rejected': '-1.0464', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57193', 'logps_train/rejected': '-131.05', 'logps_train/chosen': '-143.96', 'loss/train': '0.54843', 'examples_per_second': '117.68', 'grad_norm': '20.689', 'counters/examples': 49984, 'counters/updates': 781}
skipping logging after 50048 examples to avoid logging too frequently
train stats after 50112 examples: {'rewards_train/chosen': '-0.44976', 'rewards_train/rejected': '-0.73573', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.28597', 'logps_train/rejected': '-151.73', 'logps_train/chosen': '-170.7', 'loss/train': '0.64109', 'examples_per_second': '124.46', 'grad_norm': '25.346', 'counters/examples': 50112, 'counters/updates': 783}
skipping logging after 50176 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '-0.68793', 'rewards_train/rejected': '-1.0976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40963', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-143', 'loss/train': '0.57793', 'examples_per_second': '129.02', 'grad_norm': '23.612', 'counters/examples': 50240, 'counters/updates': 785}
skipping logging after 50304 examples to avoid logging too frequently
train stats after 50368 examples: {'rewards_train/chosen': '-0.57055', 'rewards_train/rejected': '-1.2637', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.69313', 'logps_train/rejected': '-117.92', 'logps_train/chosen': '-154.83', 'loss/train': '0.48835', 'examples_per_second': '125.33', 'grad_norm': '18.981', 'counters/examples': 50368, 'counters/updates': 787}
skipping logging after 50432 examples to avoid logging too frequently
train stats after 50496 examples: {'rewards_train/chosen': '-0.62608', 'rewards_train/rejected': '-1.048', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.42196', 'logps_train/rejected': '-173.42', 'logps_train/chosen': '-166.76', 'loss/train': '0.58496', 'examples_per_second': '124.51', 'grad_norm': '22.312', 'counters/examples': 50496, 'counters/updates': 789}
skipping logging after 50560 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '-0.5839', 'rewards_train/rejected': '-1.2197', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.6358', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-130.49', 'loss/train': '0.53102', 'examples_per_second': '132.54', 'grad_norm': '20.663', 'counters/examples': 50624, 'counters/updates': 791}
skipping logging after 50688 examples to avoid logging too frequently
train stats after 50752 examples: {'rewards_train/chosen': '-0.33016', 'rewards_train/rejected': '-0.80172', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47156', 'logps_train/rejected': '-156.23', 'logps_train/chosen': '-145.35', 'loss/train': '0.58097', 'examples_per_second': '123.83', 'grad_norm': '23.311', 'counters/examples': 50752, 'counters/updates': 793}
skipping logging after 50816 examples to avoid logging too frequently
train stats after 50880 examples: {'rewards_train/chosen': '-0.71898', 'rewards_train/rejected': '-0.88214', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16317', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-162.77', 'loss/train': '0.72048', 'examples_per_second': '124.81', 'grad_norm': '30.728', 'counters/examples': 50880, 'counters/updates': 795}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 51008 examples: {'rewards_train/chosen': '-0.76307', 'rewards_train/rejected': '-1.0868', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.32372', 'logps_train/rejected': '-148.02', 'logps_train/chosen': '-166.02', 'loss/train': '0.62836', 'examples_per_second': '127.25', 'grad_norm': '25.38', 'counters/examples': 51008, 'counters/updates': 797}
skipping logging after 51072 examples to avoid logging too frequently
train stats after 51136 examples: {'rewards_train/chosen': '-0.7367', 'rewards_train/rejected': '-1.1629', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.42621', 'logps_train/rejected': '-137.92', 'logps_train/chosen': '-163.01', 'loss/train': '0.61814', 'examples_per_second': '124.6', 'grad_norm': '23.44', 'counters/examples': 51136, 'counters/updates': 799}
skipping logging after 51200 examples to avoid logging too frequently
train stats after 51264 examples: {'rewards_train/chosen': '-0.59991', 'rewards_train/rejected': '-1.2688', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66885', 'logps_train/rejected': '-133.5', 'logps_train/chosen': '-175.75', 'loss/train': '0.50531', 'examples_per_second': '124.4', 'grad_norm': '21.835', 'counters/examples': 51264, 'counters/updates': 801}
skipping logging after 51328 examples to avoid logging too frequently
train stats after 51392 examples: {'rewards_train/chosen': '-0.4647', 'rewards_train/rejected': '-1.0014', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.53669', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-163.27', 'loss/train': '0.55034', 'examples_per_second': '118.82', 'grad_norm': '22.664', 'counters/examples': 51392, 'counters/updates': 803}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51520 examples: {'rewards_train/chosen': '-0.66384', 'rewards_train/rejected': '-1.1816', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.51774', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-159.14', 'loss/train': '0.55252', 'examples_per_second': '116.5', 'grad_norm': '24.655', 'counters/examples': 51520, 'counters/updates': 805}
skipping logging after 51584 examples to avoid logging too frequently
train stats after 51648 examples: {'rewards_train/chosen': '-0.46666', 'rewards_train/rejected': '-0.93884', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.47218', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-157.44', 'loss/train': '0.56592', 'examples_per_second': '123.83', 'grad_norm': '23.534', 'counters/examples': 51648, 'counters/updates': 807}
skipping logging after 51712 examples to avoid logging too frequently
train stats after 51776 examples: {'rewards_train/chosen': '-0.54759', 'rewards_train/rejected': '-1.0227', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47516', 'logps_train/rejected': '-147.01', 'logps_train/chosen': '-135.66', 'loss/train': '0.59468', 'examples_per_second': '124.39', 'grad_norm': '25.256', 'counters/examples': 51776, 'counters/updates': 809}
skipping logging after 51840 examples to avoid logging too frequently
train stats after 51904 examples: {'rewards_train/chosen': '-0.54966', 'rewards_train/rejected': '-0.91485', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36519', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-149.93', 'loss/train': '0.62771', 'examples_per_second': '132.31', 'grad_norm': '23.217', 'counters/examples': 51904, 'counters/updates': 811}
skipping logging after 51968 examples to avoid logging too frequently
train stats after 52032 examples: {'rewards_train/chosen': '-0.57365', 'rewards_train/rejected': '-0.89273', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31908', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-169.45', 'loss/train': '0.65546', 'examples_per_second': '136.01', 'grad_norm': '26.506', 'counters/examples': 52032, 'counters/updates': 813}
skipping logging after 52096 examples to avoid logging too frequently
train stats after 52160 examples: {'rewards_train/chosen': '-0.55397', 'rewards_train/rejected': '-1.0349', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48093', 'logps_train/rejected': '-161.36', 'logps_train/chosen': '-154.03', 'loss/train': '0.57713', 'examples_per_second': '123.83', 'grad_norm': '23.025', 'counters/examples': 52160, 'counters/updates': 815}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52288 examples: {'rewards_train/chosen': '-0.70809', 'rewards_train/rejected': '-1.1825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47439', 'logps_train/rejected': '-160.11', 'logps_train/chosen': '-186.99', 'loss/train': '0.59027', 'examples_per_second': '124.11', 'grad_norm': '26.053', 'counters/examples': 52288, 'counters/updates': 817}
skipping logging after 52352 examples to avoid logging too frequently
train stats after 52416 examples: {'rewards_train/chosen': '-0.53171', 'rewards_train/rejected': '-1.1479', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61614', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-158.13', 'loss/train': '0.52625', 'examples_per_second': '124.96', 'grad_norm': '21.426', 'counters/examples': 52416, 'counters/updates': 819}
skipping logging after 52480 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '-0.66601', 'rewards_train/rejected': '-1.2778', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61176', 'logps_train/rejected': '-139.02', 'logps_train/chosen': '-136.03', 'loss/train': '0.54602', 'examples_per_second': '120.36', 'grad_norm': '21.692', 'counters/examples': 52544, 'counters/updates': 821}
skipping logging after 52608 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '-0.4601', 'rewards_train/rejected': '-1.0073', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.54719', 'logps_train/rejected': '-148.85', 'logps_train/chosen': '-147.53', 'loss/train': '0.59841', 'examples_per_second': '118.84', 'grad_norm': '25.559', 'counters/examples': 52672, 'counters/updates': 823}
skipping logging after 52736 examples to avoid logging too frequently
train stats after 52800 examples: {'rewards_train/chosen': '-0.2928', 'rewards_train/rejected': '-0.94376', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.65096', 'logps_train/rejected': '-154.15', 'logps_train/chosen': '-168.33', 'loss/train': '0.51478', 'examples_per_second': '124.58', 'grad_norm': '22.952', 'counters/examples': 52800, 'counters/updates': 825}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52928 examples: {'rewards_train/chosen': '-0.62814', 'rewards_train/rejected': '-1.082', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.4539', 'logps_train/rejected': '-167.33', 'logps_train/chosen': '-176.54', 'loss/train': '0.68479', 'examples_per_second': '118.91', 'grad_norm': '31.501', 'counters/examples': 52928, 'counters/updates': 827}
skipping logging after 52992 examples to avoid logging too frequently
train stats after 53056 examples: {'rewards_train/chosen': '-0.52421', 'rewards_train/rejected': '-1.0872', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.56302', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-145.1', 'loss/train': '0.57215', 'examples_per_second': '120.18', 'grad_norm': '24.52', 'counters/examples': 53056, 'counters/updates': 829}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53184 examples: {'rewards_train/chosen': '-0.43717', 'rewards_train/rejected': '-0.86857', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43141', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-123.18', 'loss/train': '0.63256', 'examples_per_second': '123.36', 'grad_norm': '23.367', 'counters/examples': 53184, 'counters/updates': 831}
skipping logging after 53248 examples to avoid logging too frequently
train stats after 53312 examples: {'rewards_train/chosen': '-0.69859', 'rewards_train/rejected': '-1.1408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44222', 'logps_train/rejected': '-148.47', 'logps_train/chosen': '-155.34', 'loss/train': '0.60818', 'examples_per_second': '124.64', 'grad_norm': '26.414', 'counters/examples': 53312, 'counters/updates': 833}
skipping logging after 53376 examples to avoid logging too frequently
train stats after 53440 examples: {'rewards_train/chosen': '-0.48101', 'rewards_train/rejected': '-1.1099', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.62892', 'logps_train/rejected': '-130.76', 'logps_train/chosen': '-141.39', 'loss/train': '0.52803', 'examples_per_second': '124.65', 'grad_norm': '22.889', 'counters/examples': 53440, 'counters/updates': 835}
skipping logging after 53504 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '-0.70373', 'rewards_train/rejected': '-1.2945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59079', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-151.66', 'loss/train': '0.55946', 'examples_per_second': '138.6', 'grad_norm': '25.849', 'counters/examples': 53568, 'counters/updates': 837}
skipping logging after 53632 examples to avoid logging too frequently
train stats after 53696 examples: {'rewards_train/chosen': '-0.66243', 'rewards_train/rejected': '-1.208', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.54555', 'logps_train/rejected': '-135.99', 'logps_train/chosen': '-163.18', 'loss/train': '0.58112', 'examples_per_second': '123.63', 'grad_norm': '25.849', 'counters/examples': 53696, 'counters/updates': 839}
skipping logging after 53760 examples to avoid logging too frequently
train stats after 53824 examples: {'rewards_train/chosen': '-0.58568', 'rewards_train/rejected': '-1.2513', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.66566', 'logps_train/rejected': '-185.42', 'logps_train/chosen': '-199.95', 'loss/train': '0.52934', 'examples_per_second': '124.82', 'grad_norm': '26.725', 'counters/examples': 53824, 'counters/updates': 841}
skipping logging after 53888 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '-0.68143', 'rewards_train/rejected': '-1.2922', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61081', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-177.11', 'loss/train': '0.5487', 'examples_per_second': '124.48', 'grad_norm': '24.595', 'counters/examples': 53952, 'counters/updates': 843}
skipping logging after 54016 examples to avoid logging too frequently
train stats after 54080 examples: {'rewards_train/chosen': '-0.57774', 'rewards_train/rejected': '-0.93028', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.35254', 'logps_train/rejected': '-161.73', 'logps_train/chosen': '-161.18', 'loss/train': '0.64932', 'examples_per_second': '124.45', 'grad_norm': '28.101', 'counters/examples': 54080, 'counters/updates': 845}
skipping logging after 54144 examples to avoid logging too frequently
train stats after 54208 examples: {'rewards_train/chosen': '-0.56873', 'rewards_train/rejected': '-0.84517', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27644', 'logps_train/rejected': '-159.02', 'logps_train/chosen': '-136.89', 'loss/train': '0.69451', 'examples_per_second': '125.09', 'grad_norm': '29.371', 'counters/examples': 54208, 'counters/updates': 847}
skipping logging after 54272 examples to avoid logging too frequently
train stats after 54336 examples: {'rewards_train/chosen': '-0.53455', 'rewards_train/rejected': '-0.92207', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.38752', 'logps_train/rejected': '-144.64', 'logps_train/chosen': '-159.74', 'loss/train': '0.64198', 'examples_per_second': '124.71', 'grad_norm': '27.236', 'counters/examples': 54336, 'counters/updates': 849}
skipping logging after 54400 examples to avoid logging too frequently
train stats after 54464 examples: {'rewards_train/chosen': '-0.57017', 'rewards_train/rejected': '-0.92327', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.3531', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-195.5', 'loss/train': '0.66475', 'examples_per_second': '124.55', 'grad_norm': '29.389', 'counters/examples': 54464, 'counters/updates': 851}
skipping logging after 54528 examples to avoid logging too frequently
train stats after 54592 examples: {'rewards_train/chosen': '-0.49595', 'rewards_train/rejected': '-0.99508', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.49912', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-191.45', 'loss/train': '0.61187', 'examples_per_second': '124.51', 'grad_norm': '25.785', 'counters/examples': 54592, 'counters/updates': 853}
skipping logging after 54656 examples to avoid logging too frequently
train stats after 54720 examples: {'rewards_train/chosen': '-0.82304', 'rewards_train/rejected': '-1.2231', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.40002', 'logps_train/rejected': '-147.88', 'logps_train/chosen': '-157.39', 'loss/train': '0.65567', 'examples_per_second': '118.46', 'grad_norm': '27.762', 'counters/examples': 54720, 'counters/updates': 855}
skipping logging after 54784 examples to avoid logging too frequently
train stats after 54848 examples: {'rewards_train/chosen': '-0.58098', 'rewards_train/rejected': '-1.0199', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43893', 'logps_train/rejected': '-168.65', 'logps_train/chosen': '-155.9', 'loss/train': '0.60713', 'examples_per_second': '121.08', 'grad_norm': '27.1', 'counters/examples': 54848, 'counters/updates': 857}
skipping logging after 54912 examples to avoid logging too frequently
train stats after 54976 examples: {'rewards_train/chosen': '-0.70251', 'rewards_train/rejected': '-1.0654', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.36289', 'logps_train/rejected': '-144.8', 'logps_train/chosen': '-149.68', 'loss/train': '0.63592', 'examples_per_second': '124.81', 'grad_norm': '26.213', 'counters/examples': 54976, 'counters/updates': 859}
skipping logging after 55040 examples to avoid logging too frequently
train stats after 55104 examples: {'rewards_train/chosen': '-0.69302', 'rewards_train/rejected': '-1.2114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.51834', 'logps_train/rejected': '-114.46', 'logps_train/chosen': '-118.6', 'loss/train': '0.55712', 'examples_per_second': '124.17', 'grad_norm': '19.935', 'counters/examples': 55104, 'counters/updates': 861}
skipping logging after 55168 examples to avoid logging too frequently
train stats after 55232 examples: {'rewards_train/chosen': '-0.71493', 'rewards_train/rejected': '-1.1047', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.38979', 'logps_train/rejected': '-148.71', 'logps_train/chosen': '-181.87', 'loss/train': '0.60718', 'examples_per_second': '123.94', 'grad_norm': '25.42', 'counters/examples': 55232, 'counters/updates': 863}
skipping logging after 55296 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '-0.49727', 'rewards_train/rejected': '-1.1712', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.67392', 'logps_train/rejected': '-150.05', 'logps_train/chosen': '-143.19', 'loss/train': '0.52055', 'examples_per_second': '119.03', 'grad_norm': '21.918', 'counters/examples': 55360, 'counters/updates': 865}
skipping logging after 55424 examples to avoid logging too frequently
train stats after 55488 examples: {'rewards_train/chosen': '-0.4023', 'rewards_train/rejected': '-0.97696', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57466', 'logps_train/rejected': '-171.26', 'logps_train/chosen': '-192.97', 'loss/train': '0.55445', 'examples_per_second': '118.38', 'grad_norm': '25.847', 'counters/examples': 55488, 'counters/updates': 867}
skipping logging after 55552 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '-0.58069', 'rewards_train/rejected': '-1.1135', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.53277', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-159.94', 'loss/train': '0.5444', 'examples_per_second': '119.14', 'grad_norm': '22.757', 'counters/examples': 55616, 'counters/updates': 869}
skipping logging after 55680 examples to avoid logging too frequently
train stats after 55744 examples: {'rewards_train/chosen': '-0.8626', 'rewards_train/rejected': '-1.2716', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.40905', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-160.77', 'loss/train': '0.6283', 'examples_per_second': '124.39', 'grad_norm': '26.165', 'counters/examples': 55744, 'counters/updates': 871}
skipping logging after 55808 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '-0.64259', 'rewards_train/rejected': '-1.0086', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36605', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-161.67', 'loss/train': '0.6109', 'examples_per_second': '118.59', 'grad_norm': '23.154', 'counters/examples': 55872, 'counters/updates': 873}
skipping logging after 55936 examples to avoid logging too frequently
train stats after 56000 examples: {'rewards_train/chosen': '-0.57696', 'rewards_train/rejected': '-1.0808', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.50386', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-165.04', 'loss/train': '0.57772', 'examples_per_second': '123.38', 'grad_norm': '25.446', 'counters/examples': 56000, 'counters/updates': 875}
skipping logging after 56064 examples to avoid logging too frequently
train stats after 56128 examples: {'rewards_train/chosen': '-0.74038', 'rewards_train/rejected': '-1.0306', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.29018', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-168.17', 'loss/train': '0.69948', 'examples_per_second': '120.96', 'grad_norm': '27.872', 'counters/examples': 56128, 'counters/updates': 877}
skipping logging after 56192 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '-0.67686', 'rewards_train/rejected': '-0.88019', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.20333', 'logps_train/rejected': '-173', 'logps_train/chosen': '-146.03', 'loss/train': '0.72623', 'examples_per_second': '123.43', 'grad_norm': '28.606', 'counters/examples': 56256, 'counters/updates': 879}
skipping logging after 56320 examples to avoid logging too frequently
train stats after 56384 examples: {'rewards_train/chosen': '-0.7566', 'rewards_train/rejected': '-1.0964', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.33976', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-172.41', 'loss/train': '0.64201', 'examples_per_second': '124.04', 'grad_norm': '26.508', 'counters/examples': 56384, 'counters/updates': 881}
skipping logging after 56448 examples to avoid logging too frequently
train stats after 56512 examples: {'rewards_train/chosen': '-0.61951', 'rewards_train/rejected': '-0.96451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.345', 'logps_train/rejected': '-133.63', 'logps_train/chosen': '-138.55', 'loss/train': '0.63751', 'examples_per_second': '120.85', 'grad_norm': '22.542', 'counters/examples': 56512, 'counters/updates': 883}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56640 examples: {'rewards_train/chosen': '-0.72181', 'rewards_train/rejected': '-1.0703', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34851', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-135.14', 'loss/train': '0.61745', 'examples_per_second': '134.29', 'grad_norm': '22.9', 'counters/examples': 56640, 'counters/updates': 885}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56768 examples: {'rewards_train/chosen': '-0.55732', 'rewards_train/rejected': '-1.0038', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44649', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-157.19', 'loss/train': '0.58013', 'examples_per_second': '124.59', 'grad_norm': '21.929', 'counters/examples': 56768, 'counters/updates': 887}
skipping logging after 56832 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '-0.62059', 'rewards_train/rejected': '-1.2041', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.58351', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-145.5', 'loss/train': '0.53952', 'examples_per_second': '144.98', 'grad_norm': '21.445', 'counters/examples': 56896, 'counters/updates': 889}
skipping logging after 56960 examples to avoid logging too frequently
train stats after 57024 examples: {'rewards_train/chosen': '-0.5894', 'rewards_train/rejected': '-1.2051', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61573', 'logps_train/rejected': '-154.04', 'logps_train/chosen': '-154.69', 'loss/train': '0.56003', 'examples_per_second': '119.25', 'grad_norm': '23.216', 'counters/examples': 57024, 'counters/updates': 891}
skipping logging after 57088 examples to avoid logging too frequently
train stats after 57152 examples: {'rewards_train/chosen': '-0.69802', 'rewards_train/rejected': '-1.2348', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.53673', 'logps_train/rejected': '-160.03', 'logps_train/chosen': '-139.8', 'loss/train': '0.56296', 'examples_per_second': '124.58', 'grad_norm': '23.222', 'counters/examples': 57152, 'counters/updates': 893}
skipping logging after 57216 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '-0.72208', 'rewards_train/rejected': '-1.1276', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.40555', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-155.27', 'loss/train': '0.62196', 'examples_per_second': '124.81', 'grad_norm': '24.901', 'counters/examples': 57280, 'counters/updates': 895}
skipping logging after 57344 examples to avoid logging too frequently
train stats after 57408 examples: {'rewards_train/chosen': '-0.63709', 'rewards_train/rejected': '-1.2346', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.59751', 'logps_train/rejected': '-167.08', 'logps_train/chosen': '-172.93', 'loss/train': '0.59702', 'examples_per_second': '124.69', 'grad_norm': '24.95', 'counters/examples': 57408, 'counters/updates': 897}
skipping logging after 57472 examples to avoid logging too frequently
train stats after 57536 examples: {'rewards_train/chosen': '-0.69902', 'rewards_train/rejected': '-1.0913', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.3923', 'logps_train/rejected': '-146.89', 'logps_train/chosen': '-154.05', 'loss/train': '0.6122', 'examples_per_second': '126.06', 'grad_norm': '26.594', 'counters/examples': 57536, 'counters/updates': 899}
skipping logging after 57600 examples to avoid logging too frequently
train stats after 57664 examples: {'rewards_train/chosen': '-0.58982', 'rewards_train/rejected': '-0.92095', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33113', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-149.44', 'loss/train': '0.64601', 'examples_per_second': '121.48', 'grad_norm': '24.235', 'counters/examples': 57664, 'counters/updates': 901}
skipping logging after 57728 examples to avoid logging too frequently
train stats after 57792 examples: {'rewards_train/chosen': '-0.70625', 'rewards_train/rejected': '-0.9933', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28706', 'logps_train/rejected': '-170.13', 'logps_train/chosen': '-181.91', 'loss/train': '0.67287', 'examples_per_second': '124.26', 'grad_norm': '27.112', 'counters/examples': 57792, 'counters/updates': 903}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57920 examples: {'rewards_train/chosen': '-0.62777', 'rewards_train/rejected': '-1.1119', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48415', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-143.92', 'loss/train': '0.56535', 'examples_per_second': '125.31', 'grad_norm': '23.137', 'counters/examples': 57920, 'counters/updates': 905}
skipping logging after 57984 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '-0.52796', 'rewards_train/rejected': '-1.1435', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.6155', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-132.29', 'loss/train': '0.5342', 'examples_per_second': '156.29', 'grad_norm': '20.882', 'counters/examples': 58048, 'counters/updates': 907}
skipping logging after 58112 examples to avoid logging too frequently
train stats after 58176 examples: {'rewards_train/chosen': '-0.65303', 'rewards_train/rejected': '-1.0774', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42436', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-163.62', 'loss/train': '0.59582', 'examples_per_second': '119.27', 'grad_norm': '24.097', 'counters/examples': 58176, 'counters/updates': 909}
skipping logging after 58240 examples to avoid logging too frequently
train stats after 58304 examples: {'rewards_train/chosen': '-0.59622', 'rewards_train/rejected': '-1.3412', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.74503', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-156.64', 'loss/train': '0.48386', 'examples_per_second': '126.15', 'grad_norm': '21.213', 'counters/examples': 58304, 'counters/updates': 911}
skipping logging after 58368 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '-0.90842', 'rewards_train/rejected': '-1.5261', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.61765', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-137.84', 'loss/train': '0.53992', 'examples_per_second': '133.1', 'grad_norm': '24.02', 'counters/examples': 58432, 'counters/updates': 913}
skipping logging after 58496 examples to avoid logging too frequently
train stats after 58560 examples: {'rewards_train/chosen': '-0.69714', 'rewards_train/rejected': '-1.1672', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47009', 'logps_train/rejected': '-153.15', 'logps_train/chosen': '-171.55', 'loss/train': '0.5985', 'examples_per_second': '121', 'grad_norm': '26.066', 'counters/examples': 58560, 'counters/updates': 915}
skipping logging after 58624 examples to avoid logging too frequently
train stats after 58688 examples: {'rewards_train/chosen': '-0.97247', 'rewards_train/rejected': '-1.203', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.23053', 'logps_train/rejected': '-157.08', 'logps_train/chosen': '-145.95', 'loss/train': '0.68171', 'examples_per_second': '117.54', 'grad_norm': '27.42', 'counters/examples': 58688, 'counters/updates': 917}
skipping logging after 58752 examples to avoid logging too frequently
train stats after 58816 examples: {'rewards_train/chosen': '-0.74155', 'rewards_train/rejected': '-1.1029', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.36135', 'logps_train/rejected': '-165.97', 'logps_train/chosen': '-167.93', 'loss/train': '0.64155', 'examples_per_second': '124.81', 'grad_norm': '26.508', 'counters/examples': 58816, 'counters/updates': 919}
skipping logging after 58880 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '-0.52917', 'rewards_train/rejected': '-1.1563', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6271', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-153.06', 'loss/train': '0.55614', 'examples_per_second': '127.65', 'grad_norm': '23.328', 'counters/examples': 58944, 'counters/updates': 921}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59072 examples: {'rewards_train/chosen': '-0.43659', 'rewards_train/rejected': '-0.84888', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41229', 'logps_train/rejected': '-153.42', 'logps_train/chosen': '-142', 'loss/train': '0.63047', 'examples_per_second': '121.11', 'grad_norm': '26.177', 'counters/examples': 59072, 'counters/updates': 923}
skipping logging after 59136 examples to avoid logging too frequently
train stats after 59200 examples: {'rewards_train/chosen': '-0.47899', 'rewards_train/rejected': '-0.83408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35509', 'logps_train/rejected': '-149.67', 'logps_train/chosen': '-149.01', 'loss/train': '0.61333', 'examples_per_second': '124.26', 'grad_norm': '23.506', 'counters/examples': 59200, 'counters/updates': 925}
skipping logging after 59264 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '-0.29668', 'rewards_train/rejected': '-0.79568', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.49901', 'logps_train/rejected': '-134.06', 'logps_train/chosen': '-127.54', 'loss/train': '0.5855', 'examples_per_second': '119.98', 'grad_norm': '23.518', 'counters/examples': 59328, 'counters/updates': 927}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59456 examples: {'rewards_train/chosen': '-0.39462', 'rewards_train/rejected': '-0.74025', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34563', 'logps_train/rejected': '-156.94', 'logps_train/chosen': '-166.63', 'loss/train': '0.6293', 'examples_per_second': '123.35', 'grad_norm': '28.488', 'counters/examples': 59456, 'counters/updates': 929}
skipping logging after 59520 examples to avoid logging too frequently
train stats after 59584 examples: {'rewards_train/chosen': '-0.33315', 'rewards_train/rejected': '-0.68485', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35171', 'logps_train/rejected': '-152.08', 'logps_train/chosen': '-143.52', 'loss/train': '0.61425', 'examples_per_second': '124.37', 'grad_norm': '25.137', 'counters/examples': 59584, 'counters/updates': 931}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59712 examples: {'rewards_train/chosen': '-0.42826', 'rewards_train/rejected': '-0.99121', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.56295', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-163.69', 'loss/train': '0.52252', 'examples_per_second': '125.29', 'grad_norm': '22.26', 'counters/examples': 59712, 'counters/updates': 933}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59840 examples: {'rewards_train/chosen': '-0.4403', 'rewards_train/rejected': '-0.68633', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.24603', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-145.99', 'loss/train': '0.68369', 'examples_per_second': '129.51', 'grad_norm': '28.157', 'counters/examples': 59840, 'counters/updates': 935}
Running evaluation after 59840 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.11it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.13it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.18it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.13it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.17it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.16it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.12it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.08it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.12it/s]
eval after 59840: {'rewards_eval/chosen': '-0.38015', 'rewards_eval/rejected': '-0.79669', 'rewards_eval/accuracies': '0.66406', 'rewards_eval/margins': '0.41654', 'logps_eval/rejected': '-134.04', 'logps_eval/chosen': '-151.7', 'loss/eval': '0.63686'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-59840...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-59840/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-59840/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-59840/scheduler.pt...
train stats after 59904 examples: {'rewards_train/chosen': '-0.56854', 'rewards_train/rejected': '-0.99274', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.4242', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-142.17', 'loss/train': '0.6483', 'examples_per_second': '109.08', 'grad_norm': '23.681', 'counters/examples': 59904, 'counters/updates': 936}
skipping logging after 59968 examples to avoid logging too frequently
train stats after 60032 examples: {'rewards_train/chosen': '-0.39261', 'rewards_train/rejected': '-1.1284', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.73582', 'logps_train/rejected': '-165.31', 'logps_train/chosen': '-175.47', 'loss/train': '0.49967', 'examples_per_second': '119.58', 'grad_norm': '24.926', 'counters/examples': 60032, 'counters/updates': 938}
skipping logging after 60096 examples to avoid logging too frequently
train stats after 60160 examples: {'rewards_train/chosen': '-0.34743', 'rewards_train/rejected': '-0.74596', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39854', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-168.49', 'loss/train': '0.60342', 'examples_per_second': '124.04', 'grad_norm': '26.211', 'counters/examples': 60160, 'counters/updates': 940}
skipping logging after 60224 examples to avoid logging too frequently
train stats after 60288 examples: {'rewards_train/chosen': '-0.25325', 'rewards_train/rejected': '-0.66944', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.41619', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-136.87', 'loss/train': '0.6079', 'examples_per_second': '117.71', 'grad_norm': '22.74', 'counters/examples': 60288, 'counters/updates': 942}
skipping logging after 60352 examples to avoid logging too frequently
train stats after 60416 examples: {'rewards_train/chosen': '-0.53547', 'rewards_train/rejected': '-1.0667', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.53119', 'logps_train/rejected': '-137.72', 'logps_train/chosen': '-160.47', 'loss/train': '0.61083', 'examples_per_second': '124.79', 'grad_norm': '26.242', 'counters/examples': 60416, 'counters/updates': 944}
skipping logging after 60480 examples to avoid logging too frequently
train stats after 60544 examples: {'rewards_train/chosen': '-0.40211', 'rewards_train/rejected': '-0.62165', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.21954', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-135.06', 'loss/train': '0.67236', 'examples_per_second': '126.64', 'grad_norm': '25.881', 'counters/examples': 60544, 'counters/updates': 946}
skipping logging after 60608 examples to avoid logging too frequently
train stats after 60672 examples: {'rewards_train/chosen': '-0.493', 'rewards_train/rejected': '-0.93863', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.44563', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-152.23', 'loss/train': '0.58033', 'examples_per_second': '120.29', 'grad_norm': '23.09', 'counters/examples': 60672, 'counters/updates': 948}
skipping logging after 60736 examples to avoid logging too frequently
train stats after 60800 examples: {'rewards_train/chosen': '-0.55607', 'rewards_train/rejected': '-1.1477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.59159', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-136.93', 'loss/train': '0.59606', 'examples_per_second': '119.11', 'grad_norm': '23.61', 'counters/examples': 60800, 'counters/updates': 950}
skipping logging after 60864 examples to avoid logging too frequently
train stats after 60928 examples: {'rewards_train/chosen': '-0.68567', 'rewards_train/rejected': '-0.98408', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.29841', 'logps_train/rejected': '-140.44', 'logps_train/chosen': '-160.5', 'loss/train': '0.64313', 'examples_per_second': '124.55', 'grad_norm': '25.167', 'counters/examples': 60928, 'counters/updates': 952}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': '-0.71788', 'rewards_train/rejected': '-1.083', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.36509', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-153.8', 'loss/train': '0.61472', 'examples_per_second': '145.86', 'grad_norm': '23.385', 'counters/examples': 61056, 'counters/updates': 954}
skipping logging after 61120 examples to avoid logging too frequently
train stats after 61184 examples: {'rewards_train/chosen': '-0.54446', 'rewards_train/rejected': '-1.0505', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50606', 'logps_train/rejected': '-161.34', 'logps_train/chosen': '-168.12', 'loss/train': '0.5872', 'examples_per_second': '118.69', 'grad_norm': '24.5', 'counters/examples': 61184, 'counters/updates': 956}
skipping logging after 61248 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '-0.68681', 'rewards_train/rejected': '-1.2083', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.52148', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-157.73', 'loss/train': '0.5382', 'examples_per_second': '119.6', 'grad_norm': '23.024', 'counters/examples': 61312, 'counters/updates': 958}
skipping logging after 61376 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '-0.63355', 'rewards_train/rejected': '-0.94421', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31066', 'logps_train/rejected': '-144.26', 'logps_train/chosen': '-138.36', 'loss/train': '0.63897', 'examples_per_second': '124.02', 'grad_norm': '24.796', 'counters/examples': 61440, 'counters/updates': 960}
skipping logging after 61504 examples to avoid logging too frequently
train stats after 61568 examples: {'rewards_train/chosen': '-0.57755', 'rewards_train/rejected': '-0.98937', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41182', 'logps_train/rejected': '-165.41', 'logps_train/chosen': '-186.09', 'loss/train': '0.62117', 'examples_per_second': '124.52', 'grad_norm': '29.173', 'counters/examples': 61568, 'counters/updates': 962}
skipping logging after 61632 examples to avoid logging too frequently
train stats after 61696 examples: {'rewards_train/chosen': '-0.41172', 'rewards_train/rejected': '-0.91847', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.50675', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-117.21', 'loss/train': '0.57733', 'examples_per_second': '124.8', 'grad_norm': '21.292', 'counters/examples': 61696, 'counters/updates': 964}
skipping logging after 61760 examples to avoid logging too frequently
train stats after 61824 examples: {'rewards_train/chosen': '-0.3863', 'rewards_train/rejected': '-1.0074', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62114', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-162.54', 'loss/train': '0.55157', 'examples_per_second': '124.79', 'grad_norm': '23.03', 'counters/examples': 61824, 'counters/updates': 966}
skipping logging after 61888 examples to avoid logging too frequently
train stats after 61952 examples: {'rewards_train/chosen': '-0.47836', 'rewards_train/rejected': '-0.89707', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41871', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-167.58', 'loss/train': '0.60615', 'examples_per_second': '121.7', 'grad_norm': '27.651', 'counters/examples': 61952, 'counters/updates': 968}
skipping logging after 62016 examples to avoid logging too frequently
train stats after 62080 examples: {'rewards_train/chosen': '-0.53722', 'rewards_train/rejected': '-1.085', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54779', 'logps_train/rejected': '-146.37', 'logps_train/chosen': '-178.85', 'loss/train': '0.56891', 'examples_per_second': '118.96', 'grad_norm': '25.973', 'counters/examples': 62080, 'counters/updates': 970}
skipping logging after 62144 examples to avoid logging too frequently
train stats after 62208 examples: {'rewards_train/chosen': '-0.73188', 'rewards_train/rejected': '-1.043', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31116', 'logps_train/rejected': '-155.99', 'logps_train/chosen': '-154.39', 'loss/train': '0.66253', 'examples_per_second': '118.98', 'grad_norm': '28.826', 'counters/examples': 62208, 'counters/updates': 972}
skipping logging after 62272 examples to avoid logging too frequently
train stats after 62336 examples: {'rewards_train/chosen': '-0.67569', 'rewards_train/rejected': '-1.2843', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.60862', 'logps_train/rejected': '-159.18', 'logps_train/chosen': '-150.98', 'loss/train': '0.57269', 'examples_per_second': '124.88', 'grad_norm': '22.583', 'counters/examples': 62336, 'counters/updates': 974}
skipping logging after 62400 examples to avoid logging too frequently
train stats after 62464 examples: {'rewards_train/chosen': '-0.66569', 'rewards_train/rejected': '-1.0504', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.38474', 'logps_train/rejected': '-157.07', 'logps_train/chosen': '-175.73', 'loss/train': '0.59409', 'examples_per_second': '123.16', 'grad_norm': '26.298', 'counters/examples': 62464, 'counters/updates': 976}
skipping logging after 62528 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '-0.79451', 'rewards_train/rejected': '-1.1825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38798', 'logps_train/rejected': '-142.12', 'logps_train/chosen': '-171.07', 'loss/train': '0.62922', 'examples_per_second': '56.276', 'grad_norm': '28.054', 'counters/examples': 62592, 'counters/updates': 978}
skipping logging after 62656 examples to avoid logging too frequently
train stats after 62720 examples: {'rewards_train/chosen': '-0.78298', 'rewards_train/rejected': '-1.3328', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.54986', 'logps_train/rejected': '-140', 'logps_train/chosen': '-142.99', 'loss/train': '0.60172', 'examples_per_second': '121.37', 'grad_norm': '23.852', 'counters/examples': 62720, 'counters/updates': 980}
skipping logging after 62784 examples to avoid logging too frequently
train stats after 62848 examples: {'rewards_train/chosen': '-0.85704', 'rewards_train/rejected': '-1.2797', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.42266', 'logps_train/rejected': '-165.68', 'logps_train/chosen': '-127.76', 'loss/train': '0.61753', 'examples_per_second': '117.93', 'grad_norm': '27.531', 'counters/examples': 62848, 'counters/updates': 982}
skipping logging after 62912 examples to avoid logging too frequently
train stats after 62976 examples: {'rewards_train/chosen': '-0.83372', 'rewards_train/rejected': '-1.315', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.4813', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-171.41', 'loss/train': '0.62614', 'examples_per_second': '78.26', 'grad_norm': '29.856', 'counters/examples': 62976, 'counters/updates': 984}
skipping logging after 63040 examples to avoid logging too frequently
train stats after 63104 examples: {'rewards_train/chosen': '-0.67953', 'rewards_train/rejected': '-1.1532', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.47363', 'logps_train/rejected': '-147.41', 'logps_train/chosen': '-149.3', 'loss/train': '0.62157', 'examples_per_second': '122.08', 'grad_norm': '28.629', 'counters/examples': 63104, 'counters/updates': 986}
skipping logging after 63168 examples to avoid logging too frequently
train stats after 63232 examples: {'rewards_train/chosen': '-0.67548', 'rewards_train/rejected': '-1.0795', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.404', 'logps_train/rejected': '-142.31', 'logps_train/chosen': '-149.2', 'loss/train': '0.60223', 'examples_per_second': '123.55', 'grad_norm': '23.763', 'counters/examples': 63232, 'counters/updates': 988}
skipping logging after 63296 examples to avoid logging too frequently
train stats after 63360 examples: {'rewards_train/chosen': '-0.62909', 'rewards_train/rejected': '-1.1863', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.55718', 'logps_train/rejected': '-152.77', 'logps_train/chosen': '-142.8', 'loss/train': '0.58164', 'examples_per_second': '127.62', 'grad_norm': '22.413', 'counters/examples': 63360, 'counters/updates': 990}
skipping logging after 63424 examples to avoid logging too frequently
train stats after 63488 examples: {'rewards_train/chosen': '-0.63493', 'rewards_train/rejected': '-0.91266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27773', 'logps_train/rejected': '-136.57', 'logps_train/chosen': '-147.03', 'loss/train': '0.65453', 'examples_per_second': '124.9', 'grad_norm': '26.879', 'counters/examples': 63488, 'counters/updates': 992}
skipping logging after 63552 examples to avoid logging too frequently
train stats after 63616 examples: {'rewards_train/chosen': '-0.41943', 'rewards_train/rejected': '-0.80905', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38961', 'logps_train/rejected': '-135.29', 'logps_train/chosen': '-170.45', 'loss/train': '0.6141', 'examples_per_second': '124.49', 'grad_norm': '28.258', 'counters/examples': 63616, 'counters/updates': 994}
skipping logging after 63680 examples to avoid logging too frequently
train stats after 63744 examples: {'rewards_train/chosen': '-0.48622', 'rewards_train/rejected': '-1.0269', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.54068', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-186.39', 'loss/train': '0.54254', 'examples_per_second': '133.71', 'grad_norm': '22.483', 'counters/examples': 63744, 'counters/updates': 996}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63872 examples: {'rewards_train/chosen': '-0.69879', 'rewards_train/rejected': '-0.85312', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.15433', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-151.07', 'loss/train': '0.70623', 'examples_per_second': '123.87', 'grad_norm': '32.425', 'counters/examples': 63872, 'counters/updates': 998}
skipping logging after 63936 examples to avoid logging too frequently
train stats after 64000 examples: {'rewards_train/chosen': '-0.51659', 'rewards_train/rejected': '-0.93014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41355', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-174.32', 'loss/train': '0.6102', 'examples_per_second': '124.53', 'grad_norm': '26.876', 'counters/examples': 64000, 'counters/updates': 1000}
skipping logging after 64064 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '-0.83741', 'rewards_train/rejected': '-1.1954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35802', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-153.14', 'loss/train': '0.67136', 'examples_per_second': '124.65', 'grad_norm': '27.068', 'counters/examples': 64128, 'counters/updates': 1002}
skipping logging after 64192 examples to avoid logging too frequently
train stats after 64256 examples: {'rewards_train/chosen': '-0.61746', 'rewards_train/rejected': '-1.0169', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.39946', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-141.32', 'loss/train': '0.59123', 'examples_per_second': '127.76', 'grad_norm': '22.835', 'counters/examples': 64256, 'counters/updates': 1004}
skipping logging after 64320 examples to avoid logging too frequently
train stats after 64384 examples: {'rewards_train/chosen': '-0.53908', 'rewards_train/rejected': '-0.87281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33373', 'logps_train/rejected': '-153.65', 'logps_train/chosen': '-156.83', 'loss/train': '0.65792', 'examples_per_second': '122.64', 'grad_norm': '28.559', 'counters/examples': 64384, 'counters/updates': 1006}
skipping logging after 64448 examples to avoid logging too frequently
train stats after 64512 examples: {'rewards_train/chosen': '-0.55398', 'rewards_train/rejected': '-1.0259', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47188', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-125.85', 'loss/train': '0.54246', 'examples_per_second': '124.28', 'grad_norm': '21.007', 'counters/examples': 64512, 'counters/updates': 1008}
skipping logging after 64576 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '-0.58499', 'rewards_train/rejected': '-1.0168', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43177', 'logps_train/rejected': '-129.59', 'logps_train/chosen': '-147.49', 'loss/train': '0.62546', 'examples_per_second': '124.56', 'grad_norm': '25.645', 'counters/examples': 64640, 'counters/updates': 1010}
skipping logging after 64704 examples to avoid logging too frequently
train stats after 64768 examples: {'rewards_train/chosen': '-0.70486', 'rewards_train/rejected': '-1.275', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5701', 'logps_train/rejected': '-141.82', 'logps_train/chosen': '-142.83', 'loss/train': '0.54057', 'examples_per_second': '118.59', 'grad_norm': '22.657', 'counters/examples': 64768, 'counters/updates': 1012}
skipping logging after 64832 examples to avoid logging too frequently
train stats after 64896 examples: {'rewards_train/chosen': '-0.61528', 'rewards_train/rejected': '-1.0007', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.38543', 'logps_train/rejected': '-173.97', 'logps_train/chosen': '-185.04', 'loss/train': '0.61908', 'examples_per_second': '120.23', 'grad_norm': '27.726', 'counters/examples': 64896, 'counters/updates': 1014}
skipping logging after 64960 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '-0.65088', 'rewards_train/rejected': '-1.1049', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.45406', 'logps_train/rejected': '-126.2', 'logps_train/chosen': '-137.08', 'loss/train': '0.62033', 'examples_per_second': '120.06', 'grad_norm': '23.434', 'counters/examples': 65024, 'counters/updates': 1016}
skipping logging after 65088 examples to avoid logging too frequently
train stats after 65152 examples: {'rewards_train/chosen': '-0.44318', 'rewards_train/rejected': '-1.1193', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.67611', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-158.94', 'loss/train': '0.48768', 'examples_per_second': '124.88', 'grad_norm': '21.828', 'counters/examples': 65152, 'counters/updates': 1018}
skipping logging after 65216 examples to avoid logging too frequently
train stats after 65280 examples: {'rewards_train/chosen': '-0.66959', 'rewards_train/rejected': '-1.4145', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.74487', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-152.27', 'loss/train': '0.49823', 'examples_per_second': '124.3', 'grad_norm': '23.876', 'counters/examples': 65280, 'counters/updates': 1020}
skipping logging after 65344 examples to avoid logging too frequently
train stats after 65408 examples: {'rewards_train/chosen': '-0.9362', 'rewards_train/rejected': '-1.5518', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61559', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-187.76', 'loss/train': '0.54856', 'examples_per_second': '124.44', 'grad_norm': '24.512', 'counters/examples': 65408, 'counters/updates': 1022}
skipping logging after 65472 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '-0.72292', 'rewards_train/rejected': '-1.0912', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36828', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-153.47', 'loss/train': '0.61819', 'examples_per_second': '128.45', 'grad_norm': '23.64', 'counters/examples': 65536, 'counters/updates': 1024}
skipping logging after 65600 examples to avoid logging too frequently
train stats after 65664 examples: {'rewards_train/chosen': '-0.68761', 'rewards_train/rejected': '-1.3352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.64758', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-170.59', 'loss/train': '0.59001', 'examples_per_second': '128.42', 'grad_norm': '23.723', 'counters/examples': 65664, 'counters/updates': 1026}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65792 examples: {'rewards_train/chosen': '-0.77748', 'rewards_train/rejected': '-1.1033', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32578', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-130.93', 'loss/train': '0.66402', 'examples_per_second': '125.23', 'grad_norm': '25.829', 'counters/examples': 65792, 'counters/updates': 1028}
skipping logging after 65856 examples to avoid logging too frequently
train stats after 65920 examples: {'rewards_train/chosen': '-0.62745', 'rewards_train/rejected': '-1.2093', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.58186', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-153.23', 'loss/train': '0.59995', 'examples_per_second': '124.16', 'grad_norm': '23.728', 'counters/examples': 65920, 'counters/updates': 1030}
skipping logging after 65984 examples to avoid logging too frequently
train stats after 66048 examples: {'rewards_train/chosen': '-0.80077', 'rewards_train/rejected': '-1.2706', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46979', 'logps_train/rejected': '-143.36', 'logps_train/chosen': '-169.92', 'loss/train': '0.60203', 'examples_per_second': '124.75', 'grad_norm': '28.687', 'counters/examples': 66048, 'counters/updates': 1032}
skipping logging after 66112 examples to avoid logging too frequently
train stats after 66176 examples: {'rewards_train/chosen': '-0.65587', 'rewards_train/rejected': '-0.97263', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31675', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-146.35', 'loss/train': '0.6383', 'examples_per_second': '122.14', 'grad_norm': '26.759', 'counters/examples': 66176, 'counters/updates': 1034}
skipping logging after 66240 examples to avoid logging too frequently
train stats after 66304 examples: {'rewards_train/chosen': '-0.47737', 'rewards_train/rejected': '-1.1845', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.70716', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-158.06', 'loss/train': '0.50821', 'examples_per_second': '118.6', 'grad_norm': '22.208', 'counters/examples': 66304, 'counters/updates': 1036}
skipping logging after 66368 examples to avoid logging too frequently
train stats after 66432 examples: {'rewards_train/chosen': '-0.74016', 'rewards_train/rejected': '-1.0331', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.29297', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-154.61', 'loss/train': '0.69859', 'examples_per_second': '119.47', 'grad_norm': '28.52', 'counters/examples': 66432, 'counters/updates': 1038}
skipping logging after 66496 examples to avoid logging too frequently
train stats after 66560 examples: {'rewards_train/chosen': '-0.59294', 'rewards_train/rejected': '-1.2679', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.67493', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-163.23', 'loss/train': '0.53272', 'examples_per_second': '121.3', 'grad_norm': '22.037', 'counters/examples': 66560, 'counters/updates': 1040}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66688 examples: {'rewards_train/chosen': '-0.72305', 'rewards_train/rejected': '-1.2689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54583', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-148.19', 'loss/train': '0.6015', 'examples_per_second': '135.75', 'grad_norm': '25.632', 'counters/examples': 66688, 'counters/updates': 1042}
skipping logging after 66752 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '-0.58373', 'rewards_train/rejected': '-1.2494', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.66569', 'logps_train/rejected': '-189.42', 'logps_train/chosen': '-159.86', 'loss/train': '0.50119', 'examples_per_second': '124.56', 'grad_norm': '24.314', 'counters/examples': 66816, 'counters/updates': 1044}
skipping logging after 66880 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '-0.69055', 'rewards_train/rejected': '-1.1012', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.41067', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-149.49', 'loss/train': '0.64609', 'examples_per_second': '133.13', 'grad_norm': '24.718', 'counters/examples': 66944, 'counters/updates': 1046}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67072 examples: {'rewards_train/chosen': '-0.43543', 'rewards_train/rejected': '-0.83607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40064', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-136.65', 'loss/train': '0.63078', 'examples_per_second': '121.33', 'grad_norm': '24.244', 'counters/examples': 67072, 'counters/updates': 1048}
skipping logging after 67136 examples to avoid logging too frequently
train stats after 67200 examples: {'rewards_train/chosen': '-0.49735', 'rewards_train/rejected': '-0.94774', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45039', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-125.04', 'loss/train': '0.58333', 'examples_per_second': '124.39', 'grad_norm': '23.117', 'counters/examples': 67200, 'counters/updates': 1050}
skipping logging after 67264 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '-0.62533', 'rewards_train/rejected': '-0.93512', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30979', 'logps_train/rejected': '-147.55', 'logps_train/chosen': '-125.99', 'loss/train': '0.64682', 'examples_per_second': '128.21', 'grad_norm': '25.146', 'counters/examples': 67328, 'counters/updates': 1052}
skipping logging after 67392 examples to avoid logging too frequently
train stats after 67456 examples: {'rewards_train/chosen': '-0.37926', 'rewards_train/rejected': '-0.80066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42139', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-146.21', 'loss/train': '0.58733', 'examples_per_second': '124.25', 'grad_norm': '25.255', 'counters/examples': 67456, 'counters/updates': 1054}
skipping logging after 67520 examples to avoid logging too frequently
train stats after 67584 examples: {'rewards_train/chosen': '-0.53294', 'rewards_train/rejected': '-0.93128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39834', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-148.38', 'loss/train': '0.59427', 'examples_per_second': '121.81', 'grad_norm': '22.895', 'counters/examples': 67584, 'counters/updates': 1056}
skipping logging after 67648 examples to avoid logging too frequently
train stats after 67712 examples: {'rewards_train/chosen': '-0.38651', 'rewards_train/rejected': '-0.85528', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46877', 'logps_train/rejected': '-157.45', 'logps_train/chosen': '-162.07', 'loss/train': '0.60321', 'examples_per_second': '118.69', 'grad_norm': '27.137', 'counters/examples': 67712, 'counters/updates': 1058}
skipping logging after 67776 examples to avoid logging too frequently
train stats after 67840 examples: {'rewards_train/chosen': '-0.72913', 'rewards_train/rejected': '-0.97719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24805', 'logps_train/rejected': '-159.97', 'logps_train/chosen': '-149.33', 'loss/train': '0.68819', 'examples_per_second': '122.75', 'grad_norm': '27.536', 'counters/examples': 67840, 'counters/updates': 1060}
skipping logging after 67904 examples to avoid logging too frequently
train stats after 67968 examples: {'rewards_train/chosen': '-0.55503', 'rewards_train/rejected': '-0.84642', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29139', 'logps_train/rejected': '-130.29', 'logps_train/chosen': '-189.88', 'loss/train': '0.69159', 'examples_per_second': '124.56', 'grad_norm': '26.859', 'counters/examples': 67968, 'counters/updates': 1062}
skipping logging after 68032 examples to avoid logging too frequently
train stats after 68096 examples: {'rewards_train/chosen': '-0.38193', 'rewards_train/rejected': '-1.0215', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.63959', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-171.99', 'loss/train': '0.51294', 'examples_per_second': '117.89', 'grad_norm': '22.847', 'counters/examples': 68096, 'counters/updates': 1064}
skipping logging after 68160 examples to avoid logging too frequently
train stats after 68224 examples: {'rewards_train/chosen': '-0.54438', 'rewards_train/rejected': '-1.0522', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.5078', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-151.12', 'loss/train': '0.56487', 'examples_per_second': '117.81', 'grad_norm': '23.967', 'counters/examples': 68224, 'counters/updates': 1066}
skipping logging after 68288 examples to avoid logging too frequently
train stats after 68352 examples: {'rewards_train/chosen': '-0.63071', 'rewards_train/rejected': '-1.0648', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.43408', 'logps_train/rejected': '-175.37', 'logps_train/chosen': '-166.54', 'loss/train': '0.59025', 'examples_per_second': '124.1', 'grad_norm': '25.061', 'counters/examples': 68352, 'counters/updates': 1068}
skipping logging after 68416 examples to avoid logging too frequently
train stats after 68480 examples: {'rewards_train/chosen': '-0.53199', 'rewards_train/rejected': '-0.80545', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.27346', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-122.62', 'loss/train': '0.66953', 'examples_per_second': '126.79', 'grad_norm': '25.334', 'counters/examples': 68480, 'counters/updates': 1070}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68608 examples: {'rewards_train/chosen': '-0.56499', 'rewards_train/rejected': '-0.97855', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41356', 'logps_train/rejected': '-155.11', 'logps_train/chosen': '-162.69', 'loss/train': '0.63252', 'examples_per_second': '122.34', 'grad_norm': '26.471', 'counters/examples': 68608, 'counters/updates': 1072}
skipping logging after 68672 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '-0.65043', 'rewards_train/rejected': '-1.2202', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56972', 'logps_train/rejected': '-149.91', 'logps_train/chosen': '-162.17', 'loss/train': '0.55518', 'examples_per_second': '122.83', 'grad_norm': '24.211', 'counters/examples': 68736, 'counters/updates': 1074}
skipping logging after 68800 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '-0.44965', 'rewards_train/rejected': '-0.99972', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55006', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-151.73', 'loss/train': '0.56063', 'examples_per_second': '119.21', 'grad_norm': '22.447', 'counters/examples': 68864, 'counters/updates': 1076}
skipping logging after 68928 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '-0.46558', 'rewards_train/rejected': '-0.72935', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.26378', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-156.92', 'loss/train': '0.68356', 'examples_per_second': '124.9', 'grad_norm': '28.077', 'counters/examples': 68992, 'counters/updates': 1078}
skipping logging after 69056 examples to avoid logging too frequently
train stats after 69120 examples: {'rewards_train/chosen': '-0.40154', 'rewards_train/rejected': '-0.8795', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47795', 'logps_train/rejected': '-159.36', 'logps_train/chosen': '-144.18', 'loss/train': '0.59994', 'examples_per_second': '108', 'grad_norm': '25.802', 'counters/examples': 69120, 'counters/updates': 1080}
skipping logging after 69184 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '-0.42842', 'rewards_train/rejected': '-0.88611', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.45769', 'logps_train/rejected': '-145.93', 'logps_train/chosen': '-131.89', 'loss/train': '0.56762', 'examples_per_second': '119.81', 'grad_norm': '22.487', 'counters/examples': 69248, 'counters/updates': 1082}
skipping logging after 69312 examples to avoid logging too frequently
train stats after 69376 examples: {'rewards_train/chosen': '-0.5799', 'rewards_train/rejected': '-1.0142', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43428', 'logps_train/rejected': '-137.29', 'logps_train/chosen': '-142.4', 'loss/train': '0.6155', 'examples_per_second': '118.62', 'grad_norm': '24.867', 'counters/examples': 69376, 'counters/updates': 1084}
skipping logging after 69440 examples to avoid logging too frequently
train stats after 69504 examples: {'rewards_train/chosen': '-0.36585', 'rewards_train/rejected': '-0.79094', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.42509', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-165.34', 'loss/train': '0.57722', 'examples_per_second': '120.92', 'grad_norm': '23.22', 'counters/examples': 69504, 'counters/updates': 1086}
skipping logging after 69568 examples to avoid logging too frequently
train stats after 69632 examples: {'rewards_train/chosen': '-0.43157', 'rewards_train/rejected': '-0.89643', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.46485', 'logps_train/rejected': '-135.91', 'logps_train/chosen': '-154.32', 'loss/train': '0.56963', 'examples_per_second': '131.75', 'grad_norm': '22.611', 'counters/examples': 69632, 'counters/updates': 1088}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69760 examples: {'rewards_train/chosen': '-0.77333', 'rewards_train/rejected': '-1.1593', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38602', 'logps_train/rejected': '-163.84', 'logps_train/chosen': '-147.8', 'loss/train': '0.64807', 'examples_per_second': '125.94', 'grad_norm': '26', 'counters/examples': 69760, 'counters/updates': 1090}
skipping logging after 69824 examples to avoid logging too frequently
train stats after 69888 examples: {'rewards_train/chosen': '-0.65662', 'rewards_train/rejected': '-1.1635', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5069', 'logps_train/rejected': '-151.33', 'logps_train/chosen': '-153.43', 'loss/train': '0.56813', 'examples_per_second': '122.51', 'grad_norm': '23.759', 'counters/examples': 69888, 'counters/updates': 1092}
skipping logging after 69952 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '-0.53939', 'rewards_train/rejected': '-0.70656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16717', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-128.99', 'loss/train': '0.69443', 'examples_per_second': '130.18', 'grad_norm': '26.343', 'counters/examples': 70016, 'counters/updates': 1094}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70144 examples: {'rewards_train/chosen': '-0.53073', 'rewards_train/rejected': '-1.0249', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49415', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-157.56', 'loss/train': '0.54621', 'examples_per_second': '118.38', 'grad_norm': '24.619', 'counters/examples': 70144, 'counters/updates': 1096}
skipping logging after 70208 examples to avoid logging too frequently
train stats after 70272 examples: {'rewards_train/chosen': '-0.54579', 'rewards_train/rejected': '-0.97282', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42703', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-146.46', 'loss/train': '0.57263', 'examples_per_second': '118.79', 'grad_norm': '21.449', 'counters/examples': 70272, 'counters/updates': 1098}
skipping logging after 70336 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '-0.64101', 'rewards_train/rejected': '-1.1337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4927', 'logps_train/rejected': '-129.28', 'logps_train/chosen': '-152.92', 'loss/train': '0.63876', 'examples_per_second': '120.09', 'grad_norm': '26.077', 'counters/examples': 70400, 'counters/updates': 1100}
skipping logging after 70464 examples to avoid logging too frequently
train stats after 70528 examples: {'rewards_train/chosen': '-0.48662', 'rewards_train/rejected': '-1.2045', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.7179', 'logps_train/rejected': '-150.96', 'logps_train/chosen': '-171.9', 'loss/train': '0.51035', 'examples_per_second': '118.37', 'grad_norm': '23.26', 'counters/examples': 70528, 'counters/updates': 1102}
skipping logging after 70592 examples to avoid logging too frequently
train stats after 70656 examples: {'rewards_train/chosen': '-0.76901', 'rewards_train/rejected': '-1.1873', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41826', 'logps_train/rejected': '-147.5', 'logps_train/chosen': '-175.5', 'loss/train': '0.60921', 'examples_per_second': '124.63', 'grad_norm': '27.779', 'counters/examples': 70656, 'counters/updates': 1104}
skipping logging after 70720 examples to avoid logging too frequently
train stats after 70784 examples: {'rewards_train/chosen': '-0.8259', 'rewards_train/rejected': '-1.3475', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.52161', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-163.75', 'loss/train': '0.57631', 'examples_per_second': '124.46', 'grad_norm': '26.624', 'counters/examples': 70784, 'counters/updates': 1106}
skipping logging after 70848 examples to avoid logging too frequently
train stats after 70912 examples: {'rewards_train/chosen': '-0.81115', 'rewards_train/rejected': '-1.29', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.47882', 'logps_train/rejected': '-141.23', 'logps_train/chosen': '-174.79', 'loss/train': '0.59499', 'examples_per_second': '124.79', 'grad_norm': '25.954', 'counters/examples': 70912, 'counters/updates': 1108}
skipping logging after 70976 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '-0.68839', 'rewards_train/rejected': '-1.4087', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.72027', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-167.4', 'loss/train': '0.49552', 'examples_per_second': '124', 'grad_norm': '21.615', 'counters/examples': 71040, 'counters/updates': 1110}
skipping logging after 71104 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '-0.94907', 'rewards_train/rejected': '-1.4296', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48051', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-163.47', 'loss/train': '0.60837', 'examples_per_second': '140.47', 'grad_norm': '24.992', 'counters/examples': 71168, 'counters/updates': 1112}
skipping logging after 71232 examples to avoid logging too frequently
train stats after 71296 examples: {'rewards_train/chosen': '-0.68596', 'rewards_train/rejected': '-1.1354', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.44947', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-174.76', 'loss/train': '0.61801', 'examples_per_second': '123.7', 'grad_norm': '23.517', 'counters/examples': 71296, 'counters/updates': 1114}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71424 examples: {'rewards_train/chosen': '-0.78701', 'rewards_train/rejected': '-1.1373', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.35024', 'logps_train/rejected': '-155.8', 'logps_train/chosen': '-133.18', 'loss/train': '0.66767', 'examples_per_second': '117.64', 'grad_norm': '27.615', 'counters/examples': 71424, 'counters/updates': 1116}
skipping logging after 71488 examples to avoid logging too frequently
train stats after 71552 examples: {'rewards_train/chosen': '-0.86673', 'rewards_train/rejected': '-1.2571', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39037', 'logps_train/rejected': '-132.8', 'logps_train/chosen': '-138.32', 'loss/train': '0.66106', 'examples_per_second': '122.3', 'grad_norm': '23.687', 'counters/examples': 71552, 'counters/updates': 1118}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71680 examples: {'rewards_train/chosen': '-0.69993', 'rewards_train/rejected': '-1.2062', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50628', 'logps_train/rejected': '-135.03', 'logps_train/chosen': '-159.2', 'loss/train': '0.61363', 'examples_per_second': '124.59', 'grad_norm': '26.423', 'counters/examples': 71680, 'counters/updates': 1120}
skipping logging after 71744 examples to avoid logging too frequently
train stats after 71808 examples: {'rewards_train/chosen': '-0.43023', 'rewards_train/rejected': '-0.77731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.34708', 'logps_train/rejected': '-111.98', 'logps_train/chosen': '-151.85', 'loss/train': '0.59524', 'examples_per_second': '119.19', 'grad_norm': '23.557', 'counters/examples': 71808, 'counters/updates': 1122}
Running evaluation after 71808 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.20it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.16it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.25it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.25it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.25it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.25it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.22it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.22it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.23it/s]
eval after 71808: {'rewards_eval/chosen': '-0.53655', 'rewards_eval/rejected': '-0.98332', 'rewards_eval/accuracies': '0.68359', 'rewards_eval/margins': '0.44677', 'logps_eval/rejected': '-135.91', 'logps_eval/chosen': '-153.27', 'loss/eval': '0.61504'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-71808...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-71808/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-71808/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-71808/scheduler.pt...
train stats after 71872 examples: {'rewards_train/chosen': '-0.58773', 'rewards_train/rejected': '-0.95509', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36736', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-193.24', 'loss/train': '0.59686', 'examples_per_second': '108.39', 'grad_norm': '25.234', 'counters/examples': 71872, 'counters/updates': 1123}
skipping logging after 71936 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '-0.66727', 'rewards_train/rejected': '-1.2322', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.56493', 'logps_train/rejected': '-165.18', 'logps_train/chosen': '-153.2', 'loss/train': '0.57851', 'examples_per_second': '130.92', 'grad_norm': '25.413', 'counters/examples': 72000, 'counters/updates': 1125}
skipping logging after 72064 examples to avoid logging too frequently
train stats after 72128 examples: {'rewards_train/chosen': '-0.63508', 'rewards_train/rejected': '-0.94727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31219', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-176.76', 'loss/train': '0.66612', 'examples_per_second': '131.86', 'grad_norm': '25.732', 'counters/examples': 72128, 'counters/updates': 1127}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72256 examples: {'rewards_train/chosen': '-0.52516', 'rewards_train/rejected': '-1.1285', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.60332', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-150.77', 'loss/train': '0.50317', 'examples_per_second': '124.65', 'grad_norm': '21.814', 'counters/examples': 72256, 'counters/updates': 1129}
skipping logging after 72320 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '-0.76648', 'rewards_train/rejected': '-1.1111', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.3446', 'logps_train/rejected': '-166.62', 'logps_train/chosen': '-149.91', 'loss/train': '0.68515', 'examples_per_second': '130.99', 'grad_norm': '28.056', 'counters/examples': 72384, 'counters/updates': 1131}
skipping logging after 72448 examples to avoid logging too frequently
train stats after 72512 examples: {'rewards_train/chosen': '-0.75928', 'rewards_train/rejected': '-0.95964', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.20036', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-150.08', 'loss/train': '0.6828', 'examples_per_second': '124.51', 'grad_norm': '25.135', 'counters/examples': 72512, 'counters/updates': 1133}
skipping logging after 72576 examples to avoid logging too frequently
train stats after 72640 examples: {'rewards_train/chosen': '-0.66328', 'rewards_train/rejected': '-1.1445', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.48124', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-141.69', 'loss/train': '0.56436', 'examples_per_second': '131.07', 'grad_norm': '23.223', 'counters/examples': 72640, 'counters/updates': 1135}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72768 examples: {'rewards_train/chosen': '-0.56676', 'rewards_train/rejected': '-1.1078', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.54099', 'logps_train/rejected': '-147.16', 'logps_train/chosen': '-140.98', 'loss/train': '0.58329', 'examples_per_second': '124.56', 'grad_norm': '23.317', 'counters/examples': 72768, 'counters/updates': 1137}
train stats after 72832 examples: {'rewards_train/chosen': '-0.65898', 'rewards_train/rejected': '-0.93633', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.27735', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-132.71', 'loss/train': '0.66845', 'examples_per_second': '57.071', 'grad_norm': '26.881', 'counters/examples': 72832, 'counters/updates': 1138}
skipping logging after 72896 examples to avoid logging too frequently
train stats after 72960 examples: {'rewards_train/chosen': '-0.51604', 'rewards_train/rejected': '-1.1662', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.65016', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-127.95', 'loss/train': '0.52886', 'examples_per_second': '122.65', 'grad_norm': '21.211', 'counters/examples': 72960, 'counters/updates': 1140}
skipping logging after 73024 examples to avoid logging too frequently
train stats after 73088 examples: {'rewards_train/chosen': '-0.50978', 'rewards_train/rejected': '-0.80255', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29278', 'logps_train/rejected': '-149.46', 'logps_train/chosen': '-157.06', 'loss/train': '0.64723', 'examples_per_second': '121.43', 'grad_norm': '26.374', 'counters/examples': 73088, 'counters/updates': 1142}
skipping logging after 73152 examples to avoid logging too frequently
train stats after 73216 examples: {'rewards_train/chosen': '-0.5049', 'rewards_train/rejected': '-0.92155', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41665', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-137.03', 'loss/train': '0.60829', 'examples_per_second': '124.66', 'grad_norm': '21.948', 'counters/examples': 73216, 'counters/updates': 1144}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73344 examples: {'rewards_train/chosen': '-0.42714', 'rewards_train/rejected': '-0.78411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35698', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-147.74', 'loss/train': '0.66993', 'examples_per_second': '134.22', 'grad_norm': '27.865', 'counters/examples': 73344, 'counters/updates': 1146}
skipping logging after 73408 examples to avoid logging too frequently
train stats after 73472 examples: {'rewards_train/chosen': '-0.53688', 'rewards_train/rejected': '-0.91993', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.38304', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-140.75', 'loss/train': '0.61269', 'examples_per_second': '136.58', 'grad_norm': '24.205', 'counters/examples': 73472, 'counters/updates': 1148}
skipping logging after 73536 examples to avoid logging too frequently
train stats after 73600 examples: {'rewards_train/chosen': '-0.62488', 'rewards_train/rejected': '-0.92037', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29549', 'logps_train/rejected': '-151.1', 'logps_train/chosen': '-178.35', 'loss/train': '0.66766', 'examples_per_second': '124.83', 'grad_norm': '29.18', 'counters/examples': 73600, 'counters/updates': 1150}
skipping logging after 73664 examples to avoid logging too frequently
train stats after 73728 examples: {'rewards_train/chosen': '-0.50246', 'rewards_train/rejected': '-1.0256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52316', 'logps_train/rejected': '-160.91', 'logps_train/chosen': '-144.93', 'loss/train': '0.57917', 'examples_per_second': '119.6', 'grad_norm': '27.899', 'counters/examples': 73728, 'counters/updates': 1152}
skipping logging after 73792 examples to avoid logging too frequently
train stats after 73856 examples: {'rewards_train/chosen': '-0.32204', 'rewards_train/rejected': '-1.1745', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.85244', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-154.3', 'loss/train': '0.47423', 'examples_per_second': '124.17', 'grad_norm': '23.309', 'counters/examples': 73856, 'counters/updates': 1154}
skipping logging after 73920 examples to avoid logging too frequently
train stats after 73984 examples: {'rewards_train/chosen': '-0.54095', 'rewards_train/rejected': '-1.1678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.62689', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-154.94', 'loss/train': '0.5169', 'examples_per_second': '118.63', 'grad_norm': '22.126', 'counters/examples': 73984, 'counters/updates': 1156}
skipping logging after 74048 examples to avoid logging too frequently
train stats after 74112 examples: {'rewards_train/chosen': '-0.63494', 'rewards_train/rejected': '-1.0035', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.36857', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-144.85', 'loss/train': '0.67021', 'examples_per_second': '124.64', 'grad_norm': '26.635', 'counters/examples': 74112, 'counters/updates': 1158}
skipping logging after 74176 examples to avoid logging too frequently
train stats after 74240 examples: {'rewards_train/chosen': '-0.50608', 'rewards_train/rejected': '-0.94991', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.44383', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-156.42', 'loss/train': '0.57986', 'examples_per_second': '124.77', 'grad_norm': '22.95', 'counters/examples': 74240, 'counters/updates': 1160}
skipping logging after 74304 examples to avoid logging too frequently
train stats after 74368 examples: {'rewards_train/chosen': '-0.35962', 'rewards_train/rejected': '-0.80335', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.44372', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-170.44', 'loss/train': '0.59561', 'examples_per_second': '121.49', 'grad_norm': '25.476', 'counters/examples': 74368, 'counters/updates': 1162}
skipping logging after 74432 examples to avoid logging too frequently
train stats after 74496 examples: {'rewards_train/chosen': '-0.44727', 'rewards_train/rejected': '-0.91836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.47109', 'logps_train/rejected': '-155.41', 'logps_train/chosen': '-159.04', 'loss/train': '0.62402', 'examples_per_second': '126.54', 'grad_norm': '25.348', 'counters/examples': 74496, 'counters/updates': 1164}
skipping logging after 74560 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '-0.42968', 'rewards_train/rejected': '-0.88887', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45919', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-138.4', 'loss/train': '0.61784', 'examples_per_second': '121.77', 'grad_norm': '23.151', 'counters/examples': 74624, 'counters/updates': 1166}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74752 examples: {'rewards_train/chosen': '-0.59955', 'rewards_train/rejected': '-1.174', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57446', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-168.47', 'loss/train': '0.5911', 'examples_per_second': '123.94', 'grad_norm': '25.307', 'counters/examples': 74752, 'counters/updates': 1168}
skipping logging after 74816 examples to avoid logging too frequently
train stats after 74880 examples: {'rewards_train/chosen': '-0.58763', 'rewards_train/rejected': '-1.0563', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46867', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-149.06', 'loss/train': '0.57765', 'examples_per_second': '121.25', 'grad_norm': '22.679', 'counters/examples': 74880, 'counters/updates': 1170}
skipping logging after 74944 examples to avoid logging too frequently
train stats after 75008 examples: {'rewards_train/chosen': '-0.56352', 'rewards_train/rejected': '-0.97172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40821', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-151.45', 'loss/train': '0.64005', 'examples_per_second': '124.75', 'grad_norm': '26.698', 'counters/examples': 75008, 'counters/updates': 1172}
skipping logging after 75072 examples to avoid logging too frequently
train stats after 75136 examples: {'rewards_train/chosen': '-0.57816', 'rewards_train/rejected': '-0.93778', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.35962', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-132.17', 'loss/train': '0.65314', 'examples_per_second': '123.87', 'grad_norm': '23.704', 'counters/examples': 75136, 'counters/updates': 1174}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75264 examples: {'rewards_train/chosen': '-0.39989', 'rewards_train/rejected': '-0.88363', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48373', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-164.32', 'loss/train': '0.58638', 'examples_per_second': '120.42', 'grad_norm': '23.796', 'counters/examples': 75264, 'counters/updates': 1176}
skipping logging after 75328 examples to avoid logging too frequently
train stats after 75392 examples: {'rewards_train/chosen': '-0.5814', 'rewards_train/rejected': '-1.0928', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.51136', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-134.05', 'loss/train': '0.59392', 'examples_per_second': '118.27', 'grad_norm': '24.629', 'counters/examples': 75392, 'counters/updates': 1178}
skipping logging after 75456 examples to avoid logging too frequently
train stats after 75520 examples: {'rewards_train/chosen': '-0.51052', 'rewards_train/rejected': '-1.0592', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.54868', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-177.74', 'loss/train': '0.58899', 'examples_per_second': '123.67', 'grad_norm': '22.867', 'counters/examples': 75520, 'counters/updates': 1180}
skipping logging after 75584 examples to avoid logging too frequently
train stats after 75648 examples: {'rewards_train/chosen': '-0.54727', 'rewards_train/rejected': '-0.98742', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.44015', 'logps_train/rejected': '-144', 'logps_train/chosen': '-157.27', 'loss/train': '0.59038', 'examples_per_second': '126.47', 'grad_norm': '27.637', 'counters/examples': 75648, 'counters/updates': 1182}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75776 examples: {'rewards_train/chosen': '-0.3495', 'rewards_train/rejected': '-0.85741', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.50791', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-152.57', 'loss/train': '0.59905', 'examples_per_second': '119.12', 'grad_norm': '22.943', 'counters/examples': 75776, 'counters/updates': 1184}
skipping logging after 75840 examples to avoid logging too frequently
train stats after 75904 examples: {'rewards_train/chosen': '-0.47836', 'rewards_train/rejected': '-0.7762', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.29784', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-147.35', 'loss/train': '0.62974', 'examples_per_second': '123.28', 'grad_norm': '23.931', 'counters/examples': 75904, 'counters/updates': 1186}
skipping logging after 75968 examples to avoid logging too frequently
train stats after 76032 examples: {'rewards_train/chosen': '-0.65264', 'rewards_train/rejected': '-1.0061', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35348', 'logps_train/rejected': '-157.09', 'logps_train/chosen': '-159.58', 'loss/train': '0.64997', 'examples_per_second': '124.84', 'grad_norm': '25.112', 'counters/examples': 76032, 'counters/updates': 1188}
skipping logging after 76096 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '-0.60206', 'rewards_train/rejected': '-0.96122', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.35916', 'logps_train/rejected': '-121.93', 'logps_train/chosen': '-145.16', 'loss/train': '0.62442', 'examples_per_second': '143.61', 'grad_norm': '25.055', 'counters/examples': 76160, 'counters/updates': 1190}
skipping logging after 76224 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '-0.16742', 'rewards_train/rejected': '-0.7659', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.59848', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-143.69', 'loss/train': '0.5206', 'examples_per_second': '127.88', 'grad_norm': '21.23', 'counters/examples': 76288, 'counters/updates': 1192}
skipping logging after 76352 examples to avoid logging too frequently
train stats after 76416 examples: {'rewards_train/chosen': '-0.59637', 'rewards_train/rejected': '-1.0154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.41898', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-152.16', 'loss/train': '0.64362', 'examples_per_second': '127.9', 'grad_norm': '24.979', 'counters/examples': 76416, 'counters/updates': 1194}
skipping logging after 76480 examples to avoid logging too frequently
train stats after 76544 examples: {'rewards_train/chosen': '-0.48565', 'rewards_train/rejected': '-0.91656', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43091', 'logps_train/rejected': '-128.95', 'logps_train/chosen': '-150.08', 'loss/train': '0.59576', 'examples_per_second': '123.91', 'grad_norm': '22.763', 'counters/examples': 76544, 'counters/updates': 1196}
skipping logging after 76608 examples to avoid logging too frequently
train stats after 76672 examples: {'rewards_train/chosen': '-0.54806', 'rewards_train/rejected': '-0.9334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38535', 'logps_train/rejected': '-149.56', 'logps_train/chosen': '-150.25', 'loss/train': '0.65108', 'examples_per_second': '124.51', 'grad_norm': '25.136', 'counters/examples': 76672, 'counters/updates': 1198}
skipping logging after 76736 examples to avoid logging too frequently
train stats after 76800 examples: {'rewards_train/chosen': '-0.81928', 'rewards_train/rejected': '-1.1333', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.31401', 'logps_train/rejected': '-152.93', 'logps_train/chosen': '-155.78', 'loss/train': '0.62767', 'examples_per_second': '124.82', 'grad_norm': '24.437', 'counters/examples': 76800, 'counters/updates': 1200}
skipping logging after 76864 examples to avoid logging too frequently
train stats after 76928 examples: {'rewards_train/chosen': '-0.78033', 'rewards_train/rejected': '-1.1341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35377', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-159.06', 'loss/train': '0.66979', 'examples_per_second': '124.49', 'grad_norm': '25.979', 'counters/examples': 76928, 'counters/updates': 1202}
skipping logging after 76992 examples to avoid logging too frequently
train stats after 77056 examples: {'rewards_train/chosen': '-0.83752', 'rewards_train/rejected': '-1.181', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34352', 'logps_train/rejected': '-141.91', 'logps_train/chosen': '-151.39', 'loss/train': '0.61667', 'examples_per_second': '122.49', 'grad_norm': '24.821', 'counters/examples': 77056, 'counters/updates': 1204}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77184 examples: {'rewards_train/chosen': '-0.65663', 'rewards_train/rejected': '-1.2813', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.62467', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-169.66', 'loss/train': '0.53173', 'examples_per_second': '124.18', 'grad_norm': '22.551', 'counters/examples': 77184, 'counters/updates': 1206}
skipping logging after 77248 examples to avoid logging too frequently
train stats after 77312 examples: {'rewards_train/chosen': '-0.73157', 'rewards_train/rejected': '-1.2181', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48649', 'logps_train/rejected': '-165.15', 'logps_train/chosen': '-148.37', 'loss/train': '0.58064', 'examples_per_second': '118.57', 'grad_norm': '23.297', 'counters/examples': 77312, 'counters/updates': 1208}
skipping logging after 77376 examples to avoid logging too frequently
train stats after 77440 examples: {'rewards_train/chosen': '-0.74558', 'rewards_train/rejected': '-1.3751', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.62949', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-151.71', 'loss/train': '0.55952', 'examples_per_second': '125.81', 'grad_norm': '22.678', 'counters/examples': 77440, 'counters/updates': 1210}
skipping logging after 77504 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '-0.69444', 'rewards_train/rejected': '-1.1857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49128', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-129.64', 'loss/train': '0.57424', 'examples_per_second': '119.28', 'grad_norm': '24.872', 'counters/examples': 77568, 'counters/updates': 1212}
skipping logging after 77632 examples to avoid logging too frequently
train stats after 77696 examples: {'rewards_train/chosen': '-0.58504', 'rewards_train/rejected': '-1.1615', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57646', 'logps_train/rejected': '-146.24', 'logps_train/chosen': '-158.29', 'loss/train': '0.55502', 'examples_per_second': '119.12', 'grad_norm': '24.513', 'counters/examples': 77696, 'counters/updates': 1214}
skipping logging after 77760 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '-0.60439', 'rewards_train/rejected': '-1.2192', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.61481', 'logps_train/rejected': '-142.11', 'logps_train/chosen': '-159.66', 'loss/train': '0.53568', 'examples_per_second': '118.47', 'grad_norm': '24.923', 'counters/examples': 77824, 'counters/updates': 1216}
skipping logging after 77888 examples to avoid logging too frequently
train stats after 77952 examples: {'rewards_train/chosen': '-0.65966', 'rewards_train/rejected': '-1.1441', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48439', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-161.47', 'loss/train': '0.59204', 'examples_per_second': '128.81', 'grad_norm': '24.32', 'counters/examples': 77952, 'counters/updates': 1218}
skipping logging after 78016 examples to avoid logging too frequently
train stats after 78080 examples: {'rewards_train/chosen': '-0.63381', 'rewards_train/rejected': '-1.2763', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.64244', 'logps_train/rejected': '-152.23', 'logps_train/chosen': '-146.81', 'loss/train': '0.52081', 'examples_per_second': '123.22', 'grad_norm': '23.909', 'counters/examples': 78080, 'counters/updates': 1220}
skipping logging after 78144 examples to avoid logging too frequently
train stats after 78208 examples: {'rewards_train/chosen': '-0.81871', 'rewards_train/rejected': '-1.2809', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46224', 'logps_train/rejected': '-162.67', 'logps_train/chosen': '-161.22', 'loss/train': '0.63515', 'examples_per_second': '118.83', 'grad_norm': '25.281', 'counters/examples': 78208, 'counters/updates': 1222}
skipping logging after 78272 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '-0.78826', 'rewards_train/rejected': '-1.1519', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.36367', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-160.16', 'loss/train': '0.63165', 'examples_per_second': '124.67', 'grad_norm': '27.765', 'counters/examples': 78336, 'counters/updates': 1224}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78464 examples: {'rewards_train/chosen': '-0.75288', 'rewards_train/rejected': '-1.1458', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.39292', 'logps_train/rejected': '-127.93', 'logps_train/chosen': '-136.31', 'loss/train': '0.60116', 'examples_per_second': '124.25', 'grad_norm': '23.832', 'counters/examples': 78464, 'counters/updates': 1226}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78592 examples: {'rewards_train/chosen': '-0.65984', 'rewards_train/rejected': '-1.0416', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38177', 'logps_train/rejected': '-155.14', 'logps_train/chosen': '-126.51', 'loss/train': '0.63274', 'examples_per_second': '126.96', 'grad_norm': '25.214', 'counters/examples': 78592, 'counters/updates': 1228}
skipping logging after 78656 examples to avoid logging too frequently
train stats after 78720 examples: {'rewards_train/chosen': '-0.67801', 'rewards_train/rejected': '-0.99963', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.32162', 'logps_train/rejected': '-128.81', 'logps_train/chosen': '-165.08', 'loss/train': '0.65493', 'examples_per_second': '121.65', 'grad_norm': '24.299', 'counters/examples': 78720, 'counters/updates': 1230}
skipping logging after 78784 examples to avoid logging too frequently
train stats after 78848 examples: {'rewards_train/chosen': '-0.75745', 'rewards_train/rejected': '-1.2662', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.50872', 'logps_train/rejected': '-164.65', 'logps_train/chosen': '-170.26', 'loss/train': '0.56373', 'examples_per_second': '133.22', 'grad_norm': '26.659', 'counters/examples': 78848, 'counters/updates': 1232}
skipping logging after 78912 examples to avoid logging too frequently
train stats after 78976 examples: {'rewards_train/chosen': '-0.70758', 'rewards_train/rejected': '-1.3939', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.68636', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-150.65', 'loss/train': '0.54674', 'examples_per_second': '118.16', 'grad_norm': '25.669', 'counters/examples': 78976, 'counters/updates': 1234}
skipping logging after 79040 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '-0.67559', 'rewards_train/rejected': '-1.2181', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.54256', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-147.61', 'loss/train': '0.56696', 'examples_per_second': '143.43', 'grad_norm': '24.185', 'counters/examples': 79104, 'counters/updates': 1236}
skipping logging after 79168 examples to avoid logging too frequently
train stats after 79232 examples: {'rewards_train/chosen': '-0.62149', 'rewards_train/rejected': '-1.3591', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.73763', 'logps_train/rejected': '-150.12', 'logps_train/chosen': '-163.63', 'loss/train': '0.53564', 'examples_per_second': '118.95', 'grad_norm': '24.095', 'counters/examples': 79232, 'counters/updates': 1238}
skipping logging after 79296 examples to avoid logging too frequently
train stats after 79360 examples: {'rewards_train/chosen': '-0.75524', 'rewards_train/rejected': '-1.1777', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42248', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-146.32', 'loss/train': '0.62051', 'examples_per_second': '123.99', 'grad_norm': '24.242', 'counters/examples': 79360, 'counters/updates': 1240}
skipping logging after 79424 examples to avoid logging too frequently
train stats after 79488 examples: {'rewards_train/chosen': '-0.77048', 'rewards_train/rejected': '-1.2711', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.50066', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-158.76', 'loss/train': '0.60458', 'examples_per_second': '128.29', 'grad_norm': '24.529', 'counters/examples': 79488, 'counters/updates': 1242}
skipping logging after 79552 examples to avoid logging too frequently
train stats after 79616 examples: {'rewards_train/chosen': '-0.63347', 'rewards_train/rejected': '-1.1479', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51448', 'logps_train/rejected': '-165.47', 'logps_train/chosen': '-191.96', 'loss/train': '0.58905', 'examples_per_second': '124.61', 'grad_norm': '27.718', 'counters/examples': 79616, 'counters/updates': 1244}
skipping logging after 79680 examples to avoid logging too frequently
train stats after 79744 examples: {'rewards_train/chosen': '-0.85635', 'rewards_train/rejected': '-1.1896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.33322', 'logps_train/rejected': '-142.28', 'logps_train/chosen': '-187.01', 'loss/train': '0.65966', 'examples_per_second': '119.05', 'grad_norm': '28.703', 'counters/examples': 79744, 'counters/updates': 1246}
skipping logging after 79808 examples to avoid logging too frequently
train stats after 79872 examples: {'rewards_train/chosen': '-0.78648', 'rewards_train/rejected': '-1.1123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32585', 'logps_train/rejected': '-164.92', 'logps_train/chosen': '-141.6', 'loss/train': '0.65998', 'examples_per_second': '124.49', 'grad_norm': '24.577', 'counters/examples': 79872, 'counters/updates': 1248}
skipping logging after 79936 examples to avoid logging too frequently
train stats after 80000 examples: {'rewards_train/chosen': '-0.64308', 'rewards_train/rejected': '-1.1994', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55636', 'logps_train/rejected': '-164.3', 'logps_train/chosen': '-146.25', 'loss/train': '0.59662', 'examples_per_second': '124.65', 'grad_norm': '25.185', 'counters/examples': 80000, 'counters/updates': 1250}
skipping logging after 80064 examples to avoid logging too frequently
train stats after 80128 examples: {'rewards_train/chosen': '-0.76318', 'rewards_train/rejected': '-1.111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34778', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-150.74', 'loss/train': '0.65306', 'examples_per_second': '138.62', 'grad_norm': '25.93', 'counters/examples': 80128, 'counters/updates': 1252}
skipping logging after 80192 examples to avoid logging too frequently
train stats after 80256 examples: {'rewards_train/chosen': '-0.61145', 'rewards_train/rejected': '-0.92018', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.30873', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-153.19', 'loss/train': '0.66048', 'examples_per_second': '124.66', 'grad_norm': '24.83', 'counters/examples': 80256, 'counters/updates': 1254}
skipping logging after 80320 examples to avoid logging too frequently
train stats after 80384 examples: {'rewards_train/chosen': '-0.8556', 'rewards_train/rejected': '-1.4119', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.55627', 'logps_train/rejected': '-133.8', 'logps_train/chosen': '-174.78', 'loss/train': '0.55901', 'examples_per_second': '118.95', 'grad_norm': '23.329', 'counters/examples': 80384, 'counters/updates': 1256}
skipping logging after 80448 examples to avoid logging too frequently
train stats after 80512 examples: {'rewards_train/chosen': '-0.68214', 'rewards_train/rejected': '-1.3804', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.69826', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-141.8', 'loss/train': '0.5161', 'examples_per_second': '125.62', 'grad_norm': '20.578', 'counters/examples': 80512, 'counters/updates': 1258}
skipping logging after 80576 examples to avoid logging too frequently
train stats after 80640 examples: {'rewards_train/chosen': '-0.7731', 'rewards_train/rejected': '-1.1096', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33651', 'logps_train/rejected': '-152.98', 'logps_train/chosen': '-161.28', 'loss/train': '0.62702', 'examples_per_second': '119.42', 'grad_norm': '26.023', 'counters/examples': 80640, 'counters/updates': 1260}
skipping logging after 80704 examples to avoid logging too frequently
train stats after 80768 examples: {'rewards_train/chosen': '-0.58183', 'rewards_train/rejected': '-1.1398', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.55798', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-143.74', 'loss/train': '0.54458', 'examples_per_second': '120.47', 'grad_norm': '22.117', 'counters/examples': 80768, 'counters/updates': 1262}
skipping logging after 80832 examples to avoid logging too frequently
train stats after 80896 examples: {'rewards_train/chosen': '-0.67828', 'rewards_train/rejected': '-0.85848', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1802', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-153.39', 'loss/train': '0.70502', 'examples_per_second': '124.31', 'grad_norm': '27.382', 'counters/examples': 80896, 'counters/updates': 1264}
skipping logging after 80960 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '-0.69336', 'rewards_train/rejected': '-1.1751', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48172', 'logps_train/rejected': '-119.97', 'logps_train/chosen': '-171.42', 'loss/train': '0.57764', 'examples_per_second': '142.76', 'grad_norm': '25.764', 'counters/examples': 81024, 'counters/updates': 1266}
skipping logging after 81088 examples to avoid logging too frequently
train stats after 81152 examples: {'rewards_train/chosen': '-0.75926', 'rewards_train/rejected': '-1.364', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.60474', 'logps_train/rejected': '-134.29', 'logps_train/chosen': '-158.76', 'loss/train': '0.54389', 'examples_per_second': '122.06', 'grad_norm': '21.036', 'counters/examples': 81152, 'counters/updates': 1268}
skipping logging after 81216 examples to avoid logging too frequently
train stats after 81280 examples: {'rewards_train/chosen': '-0.83382', 'rewards_train/rejected': '-1.4184', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58458', 'logps_train/rejected': '-155.66', 'logps_train/chosen': '-158.11', 'loss/train': '0.57787', 'examples_per_second': '125.99', 'grad_norm': '23.205', 'counters/examples': 81280, 'counters/updates': 1270}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81408 examples: {'rewards_train/chosen': '-0.7869', 'rewards_train/rejected': '-1.0448', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25789', 'logps_train/rejected': '-142.09', 'logps_train/chosen': '-151.73', 'loss/train': '0.67993', 'examples_per_second': '123.51', 'grad_norm': '23.711', 'counters/examples': 81408, 'counters/updates': 1272}
skipping logging after 81472 examples to avoid logging too frequently
train stats after 81536 examples: {'rewards_train/chosen': '-0.79969', 'rewards_train/rejected': '-1.3789', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57926', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-153.15', 'loss/train': '0.55455', 'examples_per_second': '126.16', 'grad_norm': '24.305', 'counters/examples': 81536, 'counters/updates': 1274}
skipping logging after 81600 examples to avoid logging too frequently
train stats after 81664 examples: {'rewards_train/chosen': '-0.55637', 'rewards_train/rejected': '-1.1077', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55134', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-151.4', 'loss/train': '0.53995', 'examples_per_second': '124.39', 'grad_norm': '22.71', 'counters/examples': 81664, 'counters/updates': 1276}
skipping logging after 81728 examples to avoid logging too frequently
train stats after 81792 examples: {'rewards_train/chosen': '-0.68293', 'rewards_train/rejected': '-0.85773', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.1748', 'logps_train/rejected': '-161.94', 'logps_train/chosen': '-168.5', 'loss/train': '0.69695', 'examples_per_second': '125.59', 'grad_norm': '27.522', 'counters/examples': 81792, 'counters/updates': 1278}
skipping logging after 81856 examples to avoid logging too frequently
train stats after 81920 examples: {'rewards_train/chosen': '-0.72618', 'rewards_train/rejected': '-1.3983', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.67213', 'logps_train/rejected': '-159.13', 'logps_train/chosen': '-160.57', 'loss/train': '0.52551', 'examples_per_second': '123.27', 'grad_norm': '22.218', 'counters/examples': 81920, 'counters/updates': 1280}
skipping logging after 81984 examples to avoid logging too frequently
train stats after 82048 examples: {'rewards_train/chosen': '-0.79311', 'rewards_train/rejected': '-1.1032', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31009', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-144.92', 'loss/train': '0.64462', 'examples_per_second': '124.34', 'grad_norm': '23.641', 'counters/examples': 82048, 'counters/updates': 1282}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82176 examples: {'rewards_train/chosen': '-0.64664', 'rewards_train/rejected': '-1.2265', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.5799', 'logps_train/rejected': '-153.25', 'logps_train/chosen': '-150.98', 'loss/train': '0.53619', 'examples_per_second': '123.78', 'grad_norm': '23.647', 'counters/examples': 82176, 'counters/updates': 1284}
skipping logging after 82240 examples to avoid logging too frequently
train stats after 82304 examples: {'rewards_train/chosen': '-0.5059', 'rewards_train/rejected': '-1.2053', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.69936', 'logps_train/rejected': '-150.06', 'logps_train/chosen': '-154.94', 'loss/train': '0.49818', 'examples_per_second': '126.74', 'grad_norm': '22.078', 'counters/examples': 82304, 'counters/updates': 1286}
skipping logging after 82368 examples to avoid logging too frequently
train stats after 82432 examples: {'rewards_train/chosen': '-0.9445', 'rewards_train/rejected': '-1.1906', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24612', 'logps_train/rejected': '-180.73', 'logps_train/chosen': '-173.05', 'loss/train': '0.65582', 'examples_per_second': '124.21', 'grad_norm': '27.675', 'counters/examples': 82432, 'counters/updates': 1288}
skipping logging after 82496 examples to avoid logging too frequently
train stats after 82560 examples: {'rewards_train/chosen': '-0.59331', 'rewards_train/rejected': '-1.2242', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.63089', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-166.46', 'loss/train': '0.54661', 'examples_per_second': '121.48', 'grad_norm': '21.76', 'counters/examples': 82560, 'counters/updates': 1290}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82688 examples: {'rewards_train/chosen': '-0.48958', 'rewards_train/rejected': '-1.0865', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.59695', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-141.96', 'loss/train': '0.54133', 'examples_per_second': '118.39', 'grad_norm': '22.441', 'counters/examples': 82688, 'counters/updates': 1292}
skipping logging after 82752 examples to avoid logging too frequently
train stats after 82816 examples: {'rewards_train/chosen': '-0.71603', 'rewards_train/rejected': '-1.367', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.65098', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-144.43', 'loss/train': '0.57154', 'examples_per_second': '124.45', 'grad_norm': '22.863', 'counters/examples': 82816, 'counters/updates': 1294}
skipping logging after 82880 examples to avoid logging too frequently
train stats after 82944 examples: {'rewards_train/chosen': '-0.77123', 'rewards_train/rejected': '-1.1622', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39101', 'logps_train/rejected': '-170.47', 'logps_train/chosen': '-174.88', 'loss/train': '0.63254', 'examples_per_second': '123.97', 'grad_norm': '24.746', 'counters/examples': 82944, 'counters/updates': 1296}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83072 examples: {'rewards_train/chosen': '-0.64075', 'rewards_train/rejected': '-1.3451', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.70431', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-159.26', 'loss/train': '0.5278', 'examples_per_second': '118.05', 'grad_norm': '23.333', 'counters/examples': 83072, 'counters/updates': 1298}
skipping logging after 83136 examples to avoid logging too frequently
train stats after 83200 examples: {'rewards_train/chosen': '-0.72545', 'rewards_train/rejected': '-1.069', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.34352', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-148.65', 'loss/train': '0.63493', 'examples_per_second': '119.23', 'grad_norm': '23.828', 'counters/examples': 83200, 'counters/updates': 1300}
skipping logging after 83264 examples to avoid logging too frequently
train stats after 83328 examples: {'rewards_train/chosen': '-0.78793', 'rewards_train/rejected': '-1.0443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25637', 'logps_train/rejected': '-169.16', 'logps_train/chosen': '-170.63', 'loss/train': '0.69225', 'examples_per_second': '130.93', 'grad_norm': '27.106', 'counters/examples': 83328, 'counters/updates': 1302}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83456 examples: {'rewards_train/chosen': '-0.74419', 'rewards_train/rejected': '-1.077', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33284', 'logps_train/rejected': '-147.28', 'logps_train/chosen': '-164.44', 'loss/train': '0.63594', 'examples_per_second': '124.71', 'grad_norm': '25.205', 'counters/examples': 83456, 'counters/updates': 1304}
skipping logging after 83520 examples to avoid logging too frequently
train stats after 83584 examples: {'rewards_train/chosen': '-0.73923', 'rewards_train/rejected': '-1.1268', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38752', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-142.21', 'loss/train': '0.64604', 'examples_per_second': '137.03', 'grad_norm': '23.044', 'counters/examples': 83584, 'counters/updates': 1306}
skipping logging after 83648 examples to avoid logging too frequently
train stats after 83712 examples: {'rewards_train/chosen': '-0.7546', 'rewards_train/rejected': '-1.0552', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.30057', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-144.49', 'loss/train': '0.68004', 'examples_per_second': '128.41', 'grad_norm': '23.639', 'counters/examples': 83712, 'counters/updates': 1308}
skipping logging after 83776 examples to avoid logging too frequently
Running evaluation after 83776 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  9.93it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01, 10.17it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01, 10.14it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:00, 10.23it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:00<00:00, 10.25it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00, 10.13it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00, 10.15it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:01<00:00, 10.15it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.13it/s]
eval after 83776: {'rewards_eval/chosen': '-0.63834', 'rewards_eval/rejected': '-1.0428', 'rewards_eval/accuracies': '0.66406', 'rewards_eval/margins': '0.40443', 'logps_eval/rejected': '-136.51', 'logps_eval/chosen': '-154.29', 'loss/eval': '0.62218'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-83776...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-83776/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-83776/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-83776/scheduler.pt...
train stats after 83840 examples: {'rewards_train/chosen': '-0.61921', 'rewards_train/rejected': '-1.0732', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45399', 'logps_train/rejected': '-118.75', 'logps_train/chosen': '-116.4', 'loss/train': '0.5685', 'examples_per_second': '126.13', 'grad_norm': '19.674', 'counters/examples': 83840, 'counters/updates': 1310}
skipping logging after 83904 examples to avoid logging too frequently
train stats after 83968 examples: {'rewards_train/chosen': '-0.73861', 'rewards_train/rejected': '-1.1964', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.45783', 'logps_train/rejected': '-145.05', 'logps_train/chosen': '-161.39', 'loss/train': '0.59126', 'examples_per_second': '122.57', 'grad_norm': '24.499', 'counters/examples': 83968, 'counters/updates': 1312}
skipping logging after 84032 examples to avoid logging too frequently
train stats after 84096 examples: {'rewards_train/chosen': '-0.66382', 'rewards_train/rejected': '-1.1921', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.52832', 'logps_train/rejected': '-168.08', 'logps_train/chosen': '-172.87', 'loss/train': '0.56525', 'examples_per_second': '120.1', 'grad_norm': '26.271', 'counters/examples': 84096, 'counters/updates': 1314}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84224 examples: {'rewards_train/chosen': '-0.56244', 'rewards_train/rejected': '-1.0057', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44325', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-147.22', 'loss/train': '0.56723', 'examples_per_second': '124.71', 'grad_norm': '22.719', 'counters/examples': 84224, 'counters/updates': 1316}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84352 examples: {'rewards_train/chosen': '-0.49674', 'rewards_train/rejected': '-0.81671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31996', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-155.15', 'loss/train': '0.62974', 'examples_per_second': '124.57', 'grad_norm': '23.924', 'counters/examples': 84352, 'counters/updates': 1318}
skipping logging after 84416 examples to avoid logging too frequently
train stats after 84480 examples: {'rewards_train/chosen': '-0.42084', 'rewards_train/rejected': '-0.98651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.56567', 'logps_train/rejected': '-134.16', 'logps_train/chosen': '-156.17', 'loss/train': '0.56305', 'examples_per_second': '119.17', 'grad_norm': '25.748', 'counters/examples': 84480, 'counters/updates': 1320}
skipping logging after 84544 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '-0.50811', 'rewards_train/rejected': '-1.1093', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.60115', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-133.45', 'loss/train': '0.5358', 'examples_per_second': '124.9', 'grad_norm': '22.589', 'counters/examples': 84608, 'counters/updates': 1322}
skipping logging after 84672 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '-0.68868', 'rewards_train/rejected': '-1.1745', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48585', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-151.26', 'loss/train': '0.59921', 'examples_per_second': '124.97', 'grad_norm': '22.545', 'counters/examples': 84736, 'counters/updates': 1324}
skipping logging after 84800 examples to avoid logging too frequently
train stats after 84864 examples: {'rewards_train/chosen': '-0.29709', 'rewards_train/rejected': '-1.0347', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.73761', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-135.46', 'loss/train': '0.49295', 'examples_per_second': '119.99', 'grad_norm': '21.86', 'counters/examples': 84864, 'counters/updates': 1326}
skipping logging after 84928 examples to avoid logging too frequently
train stats after 84992 examples: {'rewards_train/chosen': '-0.5537', 'rewards_train/rejected': '-1.0479', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.49424', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-139.21', 'loss/train': '0.54235', 'examples_per_second': '124.86', 'grad_norm': '20.425', 'counters/examples': 84992, 'counters/updates': 1328}
skipping logging after 85056 examples to avoid logging too frequently
train stats after 85120 examples: {'rewards_train/chosen': '-0.63172', 'rewards_train/rejected': '-1.1009', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46915', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-153.93', 'loss/train': '0.59993', 'examples_per_second': '125.08', 'grad_norm': '25.563', 'counters/examples': 85120, 'counters/updates': 1330}
skipping logging after 85184 examples to avoid logging too frequently
train stats after 85248 examples: {'rewards_train/chosen': '-0.72484', 'rewards_train/rejected': '-1.1543', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42944', 'logps_train/rejected': '-144.67', 'logps_train/chosen': '-153.8', 'loss/train': '0.63954', 'examples_per_second': '124.8', 'grad_norm': '24.947', 'counters/examples': 85248, 'counters/updates': 1332}
skipping logging after 85312 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '-0.55636', 'rewards_train/rejected': '-1.2377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.68136', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-148.83', 'loss/train': '0.51659', 'examples_per_second': '130.78', 'grad_norm': '19.86', 'counters/examples': 85376, 'counters/updates': 1334}
skipping logging after 85440 examples to avoid logging too frequently
train stats after 85504 examples: {'rewards_train/chosen': '-0.82013', 'rewards_train/rejected': '-1.4599', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.63979', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-167.42', 'loss/train': '0.56889', 'examples_per_second': '126.46', 'grad_norm': '23.066', 'counters/examples': 85504, 'counters/updates': 1336}
skipping logging after 85568 examples to avoid logging too frequently
train stats after 85632 examples: {'rewards_train/chosen': '-0.52383', 'rewards_train/rejected': '-1.2288', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.70498', 'logps_train/rejected': '-150.63', 'logps_train/chosen': '-156.9', 'loss/train': '0.51664', 'examples_per_second': '125.02', 'grad_norm': '20.642', 'counters/examples': 85632, 'counters/updates': 1338}
skipping logging after 85696 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '-0.7504', 'rewards_train/rejected': '-1.1736', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42321', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-137.33', 'loss/train': '0.60041', 'examples_per_second': '124.93', 'grad_norm': '23.344', 'counters/examples': 85760, 'counters/updates': 1340}
skipping logging after 85824 examples to avoid logging too frequently
train stats after 85888 examples: {'rewards_train/chosen': '-0.76731', 'rewards_train/rejected': '-1.3147', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.5474', 'logps_train/rejected': '-145.46', 'logps_train/chosen': '-174.07', 'loss/train': '0.62894', 'examples_per_second': '119.55', 'grad_norm': '25.955', 'counters/examples': 85888, 'counters/updates': 1342}
skipping logging after 85952 examples to avoid logging too frequently
train stats after 86016 examples: {'rewards_train/chosen': '-0.81438', 'rewards_train/rejected': '-1.1498', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.33541', 'logps_train/rejected': '-154.01', 'logps_train/chosen': '-151.2', 'loss/train': '0.67207', 'examples_per_second': '124.95', 'grad_norm': '25.658', 'counters/examples': 86016, 'counters/updates': 1344}
skipping logging after 86080 examples to avoid logging too frequently
train stats after 86144 examples: {'rewards_train/chosen': '-0.69142', 'rewards_train/rejected': '-1.0748', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38338', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-150.27', 'loss/train': '0.64623', 'examples_per_second': '124.2', 'grad_norm': '25.186', 'counters/examples': 86144, 'counters/updates': 1346}
skipping logging after 86208 examples to avoid logging too frequently
train stats after 86272 examples: {'rewards_train/chosen': '-0.85615', 'rewards_train/rejected': '-1.253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39687', 'logps_train/rejected': '-148.44', 'logps_train/chosen': '-130.61', 'loss/train': '0.6339', 'examples_per_second': '126.46', 'grad_norm': '26.794', 'counters/examples': 86272, 'counters/updates': 1348}
skipping logging after 86336 examples to avoid logging too frequently
train stats after 86400 examples: {'rewards_train/chosen': '-0.53684', 'rewards_train/rejected': '-0.96351', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.42668', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-158.49', 'loss/train': '0.58373', 'examples_per_second': '124.91', 'grad_norm': '23.276', 'counters/examples': 86400, 'counters/updates': 1350}
skipping logging after 86464 examples to avoid logging too frequently
train stats after 86528 examples: {'rewards_train/chosen': '-0.71751', 'rewards_train/rejected': '-1.0488', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.33131', 'logps_train/rejected': '-138.63', 'logps_train/chosen': '-151.41', 'loss/train': '0.65204', 'examples_per_second': '125.22', 'grad_norm': '25.364', 'counters/examples': 86528, 'counters/updates': 1352}
skipping logging after 86592 examples to avoid logging too frequently
train stats after 86656 examples: {'rewards_train/chosen': '-0.63135', 'rewards_train/rejected': '-0.94044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3091', 'logps_train/rejected': '-164.68', 'logps_train/chosen': '-186.16', 'loss/train': '0.66673', 'examples_per_second': '124.8', 'grad_norm': '27.177', 'counters/examples': 86656, 'counters/updates': 1354}
skipping logging after 86720 examples to avoid logging too frequently
train stats after 86784 examples: {'rewards_train/chosen': '-0.52657', 'rewards_train/rejected': '-1.0089', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48228', 'logps_train/rejected': '-150', 'logps_train/chosen': '-131.89', 'loss/train': '0.57206', 'examples_per_second': '118.97', 'grad_norm': '22.159', 'counters/examples': 86784, 'counters/updates': 1356}
skipping logging after 86848 examples to avoid logging too frequently
train stats after 86912 examples: {'rewards_train/chosen': '-0.54837', 'rewards_train/rejected': '-1.0755', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52714', 'logps_train/rejected': '-170.38', 'logps_train/chosen': '-171.87', 'loss/train': '0.56423', 'examples_per_second': '125.1', 'grad_norm': '24.482', 'counters/examples': 86912, 'counters/updates': 1358}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87040 examples: {'rewards_train/chosen': '-0.6061', 'rewards_train/rejected': '-1.1931', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.58697', 'logps_train/rejected': '-163.69', 'logps_train/chosen': '-159.56', 'loss/train': '0.53367', 'examples_per_second': '124.89', 'grad_norm': '24.543', 'counters/examples': 87040, 'counters/updates': 1360}
skipping logging after 87104 examples to avoid logging too frequently
train stats after 87168 examples: {'rewards_train/chosen': '-0.6451', 'rewards_train/rejected': '-0.90598', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26088', 'logps_train/rejected': '-166.77', 'logps_train/chosen': '-140.04', 'loss/train': '0.65405', 'examples_per_second': '119.29', 'grad_norm': '29.647', 'counters/examples': 87168, 'counters/updates': 1362}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87296 examples: {'rewards_train/chosen': '-0.75497', 'rewards_train/rejected': '-1.0675', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.31256', 'logps_train/rejected': '-155.48', 'logps_train/chosen': '-180.02', 'loss/train': '0.65329', 'examples_per_second': '127.56', 'grad_norm': '25.293', 'counters/examples': 87296, 'counters/updates': 1364}
skipping logging after 87360 examples to avoid logging too frequently
train stats after 87424 examples: {'rewards_train/chosen': '-0.62982', 'rewards_train/rejected': '-1.0516', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42174', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-138.88', 'loss/train': '0.62269', 'examples_per_second': '128.26', 'grad_norm': '22.617', 'counters/examples': 87424, 'counters/updates': 1366}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87552 examples: {'rewards_train/chosen': '-0.48754', 'rewards_train/rejected': '-0.92484', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43731', 'logps_train/rejected': '-145.46', 'logps_train/chosen': '-153.81', 'loss/train': '0.61677', 'examples_per_second': '124.66', 'grad_norm': '24.995', 'counters/examples': 87552, 'counters/updates': 1368}
skipping logging after 87616 examples to avoid logging too frequently
train stats after 87680 examples: {'rewards_train/chosen': '-0.43489', 'rewards_train/rejected': '-0.97877', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.54388', 'logps_train/rejected': '-129.49', 'logps_train/chosen': '-138.51', 'loss/train': '0.58668', 'examples_per_second': '124.77', 'grad_norm': '23.8', 'counters/examples': 87680, 'counters/updates': 1370}
skipping logging after 87744 examples to avoid logging too frequently
train stats after 87808 examples: {'rewards_train/chosen': '-0.35374', 'rewards_train/rejected': '-0.93416', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58042', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-140.62', 'loss/train': '0.56299', 'examples_per_second': '125.61', 'grad_norm': '21.722', 'counters/examples': 87808, 'counters/updates': 1372}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87936 examples: {'rewards_train/chosen': '-0.28484', 'rewards_train/rejected': '-0.81112', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.52628', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-131.12', 'loss/train': '0.54096', 'examples_per_second': '124.77', 'grad_norm': '21.866', 'counters/examples': 87936, 'counters/updates': 1374}
skipping logging after 88000 examples to avoid logging too frequently
train stats after 88064 examples: {'rewards_train/chosen': '-0.43867', 'rewards_train/rejected': '-0.79872', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36005', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-164.8', 'loss/train': '0.61645', 'examples_per_second': '125.02', 'grad_norm': '23.517', 'counters/examples': 88064, 'counters/updates': 1376}
skipping logging after 88128 examples to avoid logging too frequently
train stats after 88192 examples: {'rewards_train/chosen': '-0.64351', 'rewards_train/rejected': '-0.88513', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24163', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-157.67', 'loss/train': '0.68597', 'examples_per_second': '124.98', 'grad_norm': '25.588', 'counters/examples': 88192, 'counters/updates': 1378}
skipping logging after 88256 examples to avoid logging too frequently
train stats after 88320 examples: {'rewards_train/chosen': '-0.59537', 'rewards_train/rejected': '-0.90545', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.31008', 'logps_train/rejected': '-144.7', 'logps_train/chosen': '-152.74', 'loss/train': '0.65267', 'examples_per_second': '122.95', 'grad_norm': '25.533', 'counters/examples': 88320, 'counters/updates': 1380}
skipping logging after 88384 examples to avoid logging too frequently
train stats after 88448 examples: {'rewards_train/chosen': '-0.58682', 'rewards_train/rejected': '-1.0832', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.49635', 'logps_train/rejected': '-136.97', 'logps_train/chosen': '-167.82', 'loss/train': '0.55721', 'examples_per_second': '125.67', 'grad_norm': '22.312', 'counters/examples': 88448, 'counters/updates': 1382}
skipping logging after 88512 examples to avoid logging too frequently
train stats after 88576 examples: {'rewards_train/chosen': '-0.75298', 'rewards_train/rejected': '-1.4065', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.6535', 'logps_train/rejected': '-138.35', 'logps_train/chosen': '-160.61', 'loss/train': '0.56479', 'examples_per_second': '124.93', 'grad_norm': '25.404', 'counters/examples': 88576, 'counters/updates': 1384}
skipping logging after 88640 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '-0.85666', 'rewards_train/rejected': '-1.1766', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31993', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-133.46', 'loss/train': '0.65512', 'examples_per_second': '140.93', 'grad_norm': '23.314', 'counters/examples': 88704, 'counters/updates': 1386}
skipping logging after 88768 examples to avoid logging too frequently
train stats after 88832 examples: {'rewards_train/chosen': '-0.75501', 'rewards_train/rejected': '-1.1602', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.40517', 'logps_train/rejected': '-133.35', 'logps_train/chosen': '-155.09', 'loss/train': '0.64781', 'examples_per_second': '120.15', 'grad_norm': '25.35', 'counters/examples': 88832, 'counters/updates': 1388}
skipping logging after 88896 examples to avoid logging too frequently
train stats after 88960 examples: {'rewards_train/chosen': '-0.96528', 'rewards_train/rejected': '-1.4165', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.45123', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-153.63', 'loss/train': '0.62947', 'examples_per_second': '122.2', 'grad_norm': '24.023', 'counters/examples': 88960, 'counters/updates': 1390}
skipping logging after 89024 examples to avoid logging too frequently
train stats after 89088 examples: {'rewards_train/chosen': '-0.77229', 'rewards_train/rejected': '-1.2139', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.44164', 'logps_train/rejected': '-140.01', 'logps_train/chosen': '-162.2', 'loss/train': '0.64608', 'examples_per_second': '123.21', 'grad_norm': '25.464', 'counters/examples': 89088, 'counters/updates': 1392}
skipping logging after 89152 examples to avoid logging too frequently
train stats after 89216 examples: {'rewards_train/chosen': '-0.74957', 'rewards_train/rejected': '-1.1171', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36754', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-186.97', 'loss/train': '0.64601', 'examples_per_second': '118.28', 'grad_norm': '29.097', 'counters/examples': 89216, 'counters/updates': 1394}
skipping logging after 89280 examples to avoid logging too frequently
train stats after 89344 examples: {'rewards_train/chosen': '-0.93704', 'rewards_train/rejected': '-1.0044', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067343', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-176.78', 'loss/train': '0.75898', 'examples_per_second': '124.18', 'grad_norm': '29.578', 'counters/examples': 89344, 'counters/updates': 1396}
skipping logging after 89408 examples to avoid logging too frequently
train stats after 89472 examples: {'rewards_train/chosen': '-0.5473', 'rewards_train/rejected': '-1.2085', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.66124', 'logps_train/rejected': '-143.77', 'logps_train/chosen': '-155.1', 'loss/train': '0.50435', 'examples_per_second': '84.055', 'grad_norm': '23.71', 'counters/examples': 89472, 'counters/updates': 1398}
skipping logging after 89536 examples to avoid logging too frequently
train stats after 89600 examples: {'rewards_train/chosen': '-0.8029', 'rewards_train/rejected': '-1.0907', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.28775', 'logps_train/rejected': '-139.84', 'logps_train/chosen': '-148.85', 'loss/train': '0.66875', 'examples_per_second': '120.82', 'grad_norm': '27.249', 'counters/examples': 89600, 'counters/updates': 1400}
skipping logging after 89664 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '-0.57879', 'rewards_train/rejected': '-1.1061', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.52732', 'logps_train/rejected': '-144.61', 'logps_train/chosen': '-166.57', 'loss/train': '0.56916', 'examples_per_second': '124.1', 'grad_norm': '24.234', 'counters/examples': 89728, 'counters/updates': 1402}
skipping logging after 89792 examples to avoid logging too frequently
train stats after 89856 examples: {'rewards_train/chosen': '-0.83306', 'rewards_train/rejected': '-1.1748', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.3417', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-155.19', 'loss/train': '0.66149', 'examples_per_second': '120.66', 'grad_norm': '25.705', 'counters/examples': 89856, 'counters/updates': 1404}
skipping logging after 89920 examples to avoid logging too frequently
train stats after 89984 examples: {'rewards_train/chosen': '-0.71055', 'rewards_train/rejected': '-1.1325', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42195', 'logps_train/rejected': '-149.28', 'logps_train/chosen': '-188.81', 'loss/train': '0.60705', 'examples_per_second': '124.47', 'grad_norm': '26.678', 'counters/examples': 89984, 'counters/updates': 1406}
skipping logging after 90048 examples to avoid logging too frequently
train stats after 90112 examples: {'rewards_train/chosen': '-0.71592', 'rewards_train/rejected': '-1.1589', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.443', 'logps_train/rejected': '-154.76', 'logps_train/chosen': '-169.03', 'loss/train': '0.60397', 'examples_per_second': '127.19', 'grad_norm': '25.263', 'counters/examples': 90112, 'counters/updates': 1408}
skipping logging after 90176 examples to avoid logging too frequently
train stats after 90240 examples: {'rewards_train/chosen': '-0.71372', 'rewards_train/rejected': '-1.0602', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.3465', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-181.34', 'loss/train': '0.68654', 'examples_per_second': '121.17', 'grad_norm': '25.98', 'counters/examples': 90240, 'counters/updates': 1410}
skipping logging after 90304 examples to avoid logging too frequently
train stats after 90368 examples: {'rewards_train/chosen': '-0.50796', 'rewards_train/rejected': '-1.1087', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.60072', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-134.66', 'loss/train': '0.53259', 'examples_per_second': '122.18', 'grad_norm': '21.187', 'counters/examples': 90368, 'counters/updates': 1412}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90496 examples: {'rewards_train/chosen': '-0.77993', 'rewards_train/rejected': '-1.2191', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4392', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-140.59', 'loss/train': '0.59568', 'examples_per_second': '118.87', 'grad_norm': '23.725', 'counters/examples': 90496, 'counters/updates': 1414}
skipping logging after 90560 examples to avoid logging too frequently
train stats after 90624 examples: {'rewards_train/chosen': '-0.49502', 'rewards_train/rejected': '-1.0768', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58177', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-124.16', 'loss/train': '0.55481', 'examples_per_second': '128.16', 'grad_norm': '22.924', 'counters/examples': 90624, 'counters/updates': 1416}
skipping logging after 90688 examples to avoid logging too frequently
train stats after 90752 examples: {'rewards_train/chosen': '-0.46108', 'rewards_train/rejected': '-0.94058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4795', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-147.79', 'loss/train': '0.57076', 'examples_per_second': '124.76', 'grad_norm': '21.481', 'counters/examples': 90752, 'counters/updates': 1418}
skipping logging after 90816 examples to avoid logging too frequently
train stats after 90880 examples: {'rewards_train/chosen': '-0.62624', 'rewards_train/rejected': '-0.99935', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3731', 'logps_train/rejected': '-171.45', 'logps_train/chosen': '-180.44', 'loss/train': '0.65807', 'examples_per_second': '124.45', 'grad_norm': '28.162', 'counters/examples': 90880, 'counters/updates': 1420}
skipping logging after 90944 examples to avoid logging too frequently
train stats after 91008 examples: {'rewards_train/chosen': '-0.75696', 'rewards_train/rejected': '-0.98619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22923', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-143.02', 'loss/train': '0.6829', 'examples_per_second': '124.17', 'grad_norm': '25.05', 'counters/examples': 91008, 'counters/updates': 1422}
skipping logging after 91072 examples to avoid logging too frequently
train stats after 91136 examples: {'rewards_train/chosen': '-0.38424', 'rewards_train/rejected': '-0.94154', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.5573', 'logps_train/rejected': '-122.04', 'logps_train/chosen': '-150.62', 'loss/train': '0.53466', 'examples_per_second': '124.27', 'grad_norm': '22.378', 'counters/examples': 91136, 'counters/updates': 1424}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91264 examples: {'rewards_train/chosen': '-0.7761', 'rewards_train/rejected': '-1.2544', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.47827', 'logps_train/rejected': '-159.74', 'logps_train/chosen': '-155.54', 'loss/train': '0.6335', 'examples_per_second': '121.82', 'grad_norm': '27.621', 'counters/examples': 91264, 'counters/updates': 1426}
skipping logging after 91328 examples to avoid logging too frequently
train stats after 91392 examples: {'rewards_train/chosen': '-0.64385', 'rewards_train/rejected': '-1.0815', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43767', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-153.9', 'loss/train': '0.57865', 'examples_per_second': '119.07', 'grad_norm': '23.785', 'counters/examples': 91392, 'counters/updates': 1428}
skipping logging after 91456 examples to avoid logging too frequently
train stats after 91520 examples: {'rewards_train/chosen': '-0.54757', 'rewards_train/rejected': '-1.1604', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61283', 'logps_train/rejected': '-140.93', 'logps_train/chosen': '-133.11', 'loss/train': '0.53245', 'examples_per_second': '129.98', 'grad_norm': '21', 'counters/examples': 91520, 'counters/updates': 1430}
skipping logging after 91584 examples to avoid logging too frequently
train stats after 91648 examples: {'rewards_train/chosen': '-0.61526', 'rewards_train/rejected': '-1.119', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.5037', 'logps_train/rejected': '-126.03', 'logps_train/chosen': '-135.52', 'loss/train': '0.58917', 'examples_per_second': '121.06', 'grad_norm': '23.59', 'counters/examples': 91648, 'counters/updates': 1432}
skipping logging after 91712 examples to avoid logging too frequently
train stats after 91776 examples: {'rewards_train/chosen': '-0.54046', 'rewards_train/rejected': '-1.1323', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59187', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-143.89', 'loss/train': '0.57809', 'examples_per_second': '121.8', 'grad_norm': '21.449', 'counters/examples': 91776, 'counters/updates': 1434}
skipping logging after 91840 examples to avoid logging too frequently
train stats after 91904 examples: {'rewards_train/chosen': '-0.60553', 'rewards_train/rejected': '-1.1032', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.49766', 'logps_train/rejected': '-155.31', 'logps_train/chosen': '-150.43', 'loss/train': '0.58244', 'examples_per_second': '125.39', 'grad_norm': '23.793', 'counters/examples': 91904, 'counters/updates': 1436}
skipping logging after 91968 examples to avoid logging too frequently
train stats after 92032 examples: {'rewards_train/chosen': '-0.61015', 'rewards_train/rejected': '-1.2476', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.63742', 'logps_train/rejected': '-151.72', 'logps_train/chosen': '-157.27', 'loss/train': '0.55911', 'examples_per_second': '119.35', 'grad_norm': '22.528', 'counters/examples': 92032, 'counters/updates': 1438}
skipping logging after 92096 examples to avoid logging too frequently
train stats after 92160 examples: {'rewards_train/chosen': '-0.58912', 'rewards_train/rejected': '-0.96729', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.37818', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-166.32', 'loss/train': '0.62196', 'examples_per_second': '124.78', 'grad_norm': '23.78', 'counters/examples': 92160, 'counters/updates': 1440}
skipping logging after 92224 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '-0.59327', 'rewards_train/rejected': '-1.2526', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.65928', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-152.62', 'loss/train': '0.50929', 'examples_per_second': '123.52', 'grad_norm': '21.103', 'counters/examples': 92288, 'counters/updates': 1442}
skipping logging after 92352 examples to avoid logging too frequently
train stats after 92416 examples: {'rewards_train/chosen': '-0.82811', 'rewards_train/rejected': '-1.4523', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.62415', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-135.02', 'loss/train': '0.53394', 'examples_per_second': '125.26', 'grad_norm': '20.517', 'counters/examples': 92416, 'counters/updates': 1444}
skipping logging after 92480 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '-0.88751', 'rewards_train/rejected': '-1.3571', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.46959', 'logps_train/rejected': '-143.06', 'logps_train/chosen': '-145', 'loss/train': '0.58377', 'examples_per_second': '129.48', 'grad_norm': '23.597', 'counters/examples': 92544, 'counters/updates': 1446}
skipping logging after 92608 examples to avoid logging too frequently
train stats after 92672 examples: {'rewards_train/chosen': '-1.0048', 'rewards_train/rejected': '-1.5067', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50199', 'logps_train/rejected': '-137.25', 'logps_train/chosen': '-173.34', 'loss/train': '0.59545', 'examples_per_second': '117.24', 'grad_norm': '27.61', 'counters/examples': 92672, 'counters/updates': 1448}
skipping logging after 92736 examples to avoid logging too frequently
train stats after 92800 examples: {'rewards_train/chosen': '-0.83359', 'rewards_train/rejected': '-1.4161', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58247', 'logps_train/rejected': '-135.56', 'logps_train/chosen': '-125.12', 'loss/train': '0.54788', 'examples_per_second': '124.78', 'grad_norm': '20.994', 'counters/examples': 92800, 'counters/updates': 1450}
skipping logging after 92864 examples to avoid logging too frequently
train stats after 92928 examples: {'rewards_train/chosen': '-0.97052', 'rewards_train/rejected': '-1.4108', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.44024', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-135.45', 'loss/train': '0.61651', 'examples_per_second': '81.327', 'grad_norm': '24.269', 'counters/examples': 92928, 'counters/updates': 1452}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93056 examples: {'rewards_train/chosen': '-1.0273', 'rewards_train/rejected': '-1.5213', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49401', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-171.73', 'loss/train': '0.59229', 'examples_per_second': '120.86', 'grad_norm': '25.825', 'counters/examples': 93056, 'counters/updates': 1454}
skipping logging after 93120 examples to avoid logging too frequently
train stats after 93184 examples: {'rewards_train/chosen': '-1.0501', 'rewards_train/rejected': '-1.6218', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.57174', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-143.88', 'loss/train': '0.5566', 'examples_per_second': '123.67', 'grad_norm': '21.14', 'counters/examples': 93184, 'counters/updates': 1456}
skipping logging after 93248 examples to avoid logging too frequently
train stats after 93312 examples: {'rewards_train/chosen': '-0.96282', 'rewards_train/rejected': '-1.3382', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37538', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-146.47', 'loss/train': '0.65776', 'examples_per_second': '78.856', 'grad_norm': '24.903', 'counters/examples': 93312, 'counters/updates': 1458}
skipping logging after 93376 examples to avoid logging too frequently
train stats after 93440 examples: {'rewards_train/chosen': '-0.72302', 'rewards_train/rejected': '-1.1136', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.39058', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-179.62', 'loss/train': '0.59465', 'examples_per_second': '124.4', 'grad_norm': '25.029', 'counters/examples': 93440, 'counters/updates': 1460}
skipping logging after 93504 examples to avoid logging too frequently
train stats after 93568 examples: {'rewards_train/chosen': '-0.96523', 'rewards_train/rejected': '-1.3919', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42671', 'logps_train/rejected': '-168.84', 'logps_train/chosen': '-194.56', 'loss/train': '0.60934', 'examples_per_second': '124.29', 'grad_norm': '27.056', 'counters/examples': 93568, 'counters/updates': 1462}
skipping logging after 93632 examples to avoid logging too frequently
train stats after 93696 examples: {'rewards_train/chosen': '-0.9033', 'rewards_train/rejected': '-1.184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.28069', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-180.67', 'loss/train': '0.6602', 'examples_per_second': '124.21', 'grad_norm': '25.429', 'counters/examples': 93696, 'counters/updates': 1464}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93824 examples: {'rewards_train/chosen': '-0.88781', 'rewards_train/rejected': '-1.4289', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.54111', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-148.3', 'loss/train': '0.57667', 'examples_per_second': '124.57', 'grad_norm': '24.411', 'counters/examples': 93824, 'counters/updates': 1466}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93952 examples: {'rewards_train/chosen': '-0.88939', 'rewards_train/rejected': '-1.1984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30898', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-161.29', 'loss/train': '0.69785', 'examples_per_second': '118.64', 'grad_norm': '27.925', 'counters/examples': 93952, 'counters/updates': 1468}
skipping logging after 94016 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '-0.84768', 'rewards_train/rejected': '-1.1827', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33502', 'logps_train/rejected': '-148.89', 'logps_train/chosen': '-166.29', 'loss/train': '0.68385', 'examples_per_second': '124.44', 'grad_norm': '26.596', 'counters/examples': 94080, 'counters/updates': 1470}
skipping logging after 94144 examples to avoid logging too frequently
train stats after 94208 examples: {'rewards_train/chosen': '-0.68116', 'rewards_train/rejected': '-1.3401', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.65889', 'logps_train/rejected': '-161.72', 'logps_train/chosen': '-180.55', 'loss/train': '0.50733', 'examples_per_second': '124.53', 'grad_norm': '22.046', 'counters/examples': 94208, 'counters/updates': 1472}
skipping logging after 94272 examples to avoid logging too frequently
train stats after 94336 examples: {'rewards_train/chosen': '-0.82161', 'rewards_train/rejected': '-1.2402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41857', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-145.51', 'loss/train': '0.63605', 'examples_per_second': '118.61', 'grad_norm': '25.333', 'counters/examples': 94336, 'counters/updates': 1474}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94464 examples: {'rewards_train/chosen': '-0.77901', 'rewards_train/rejected': '-1.3932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.61415', 'logps_train/rejected': '-160.34', 'logps_train/chosen': '-168.36', 'loss/train': '0.55437', 'examples_per_second': '123.74', 'grad_norm': '23.479', 'counters/examples': 94464, 'counters/updates': 1476}
skipping logging after 94528 examples to avoid logging too frequently
train stats after 94592 examples: {'rewards_train/chosen': '-0.71443', 'rewards_train/rejected': '-1.0889', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37443', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-158.29', 'loss/train': '0.61537', 'examples_per_second': '123.41', 'grad_norm': '23.64', 'counters/examples': 94592, 'counters/updates': 1478}
skipping logging after 94656 examples to avoid logging too frequently
train stats after 94720 examples: {'rewards_train/chosen': '-1.0957', 'rewards_train/rejected': '-1.3535', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.25782', 'logps_train/rejected': '-111.06', 'logps_train/chosen': '-134.89', 'loss/train': '0.66574', 'examples_per_second': '122.9', 'grad_norm': '21.671', 'counters/examples': 94720, 'counters/updates': 1480}
skipping logging after 94784 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '-0.93714', 'rewards_train/rejected': '-1.409', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4719', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-143.8', 'loss/train': '0.6065', 'examples_per_second': '128.75', 'grad_norm': '22.892', 'counters/examples': 94848, 'counters/updates': 1482}
skipping logging after 94912 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '-1.0544', 'rewards_train/rejected': '-1.3032', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24879', 'logps_train/rejected': '-146.91', 'logps_train/chosen': '-161.82', 'loss/train': '0.69634', 'examples_per_second': '129.96', 'grad_norm': '25.82', 'counters/examples': 94976, 'counters/updates': 1484}
skipping logging after 95040 examples to avoid logging too frequently
train stats after 95104 examples: {'rewards_train/chosen': '-0.45638', 'rewards_train/rejected': '-0.88147', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4251', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-170.6', 'loss/train': '0.6091', 'examples_per_second': '123.15', 'grad_norm': '24.82', 'counters/examples': 95104, 'counters/updates': 1486}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95232 examples: {'rewards_train/chosen': '-0.66772', 'rewards_train/rejected': '-1.0157', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34793', 'logps_train/rejected': '-133.25', 'logps_train/chosen': '-139.09', 'loss/train': '0.65064', 'examples_per_second': '134.11', 'grad_norm': '28.092', 'counters/examples': 95232, 'counters/updates': 1488}
skipping logging after 95296 examples to avoid logging too frequently
train stats after 95360 examples: {'rewards_train/chosen': '-0.7209', 'rewards_train/rejected': '-0.93647', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.21557', 'logps_train/rejected': '-165.68', 'logps_train/chosen': '-150.27', 'loss/train': '0.69505', 'examples_per_second': '123.39', 'grad_norm': '26.229', 'counters/examples': 95360, 'counters/updates': 1490}
skipping logging after 95424 examples to avoid logging too frequently
train stats after 95488 examples: {'rewards_train/chosen': '-0.7072', 'rewards_train/rejected': '-1.1188', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41165', 'logps_train/rejected': '-149.2', 'logps_train/chosen': '-149.96', 'loss/train': '0.61886', 'examples_per_second': '123.55', 'grad_norm': '23.671', 'counters/examples': 95488, 'counters/updates': 1492}
skipping logging after 95552 examples to avoid logging too frequently
train stats after 95616 examples: {'rewards_train/chosen': '-0.66141', 'rewards_train/rejected': '-1.0872', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.42581', 'logps_train/rejected': '-147.19', 'logps_train/chosen': '-173.09', 'loss/train': '0.57555', 'examples_per_second': '124.84', 'grad_norm': '26.214', 'counters/examples': 95616, 'counters/updates': 1494}
skipping logging after 95680 examples to avoid logging too frequently
train stats after 95744 examples: {'rewards_train/chosen': '-1.0261', 'rewards_train/rejected': '-1.3883', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36221', 'logps_train/rejected': '-157.19', 'logps_train/chosen': '-148.25', 'loss/train': '0.6467', 'examples_per_second': '118.75', 'grad_norm': '24.582', 'counters/examples': 95744, 'counters/updates': 1496}
Running evaluation after 95744 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01,  9.99it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.09it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.23it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.22it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.21it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.20it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.16it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.11it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.15it/s]
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
eval after 95744: {'rewards_eval/chosen': '-0.72286', 'rewards_eval/rejected': '-1.1592', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.43638', 'logps_eval/rejected': '-137.67', 'logps_eval/chosen': '-155.13', 'loss/eval': '0.62854'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-95744...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-95744/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-95744/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/step-95744/scheduler.pt...
train stats after 95808 examples: {'rewards_train/chosen': '-0.87634', 'rewards_train/rejected': '-1.2616', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38524', 'logps_train/rejected': '-152.11', 'logps_train/chosen': '-155.11', 'loss/train': '0.59646', 'examples_per_second': '101.27', 'grad_norm': '25.097', 'counters/examples': 95808, 'counters/updates': 1497}
skipping logging after 95872 examples to avoid logging too frequently
train stats after 95936 examples: {'rewards_train/chosen': '-0.67487', 'rewards_train/rejected': '-0.91409', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23922', 'logps_train/rejected': '-140.14', 'logps_train/chosen': '-146.44', 'loss/train': '0.66969', 'examples_per_second': '125.03', 'grad_norm': '26.228', 'counters/examples': 95936, 'counters/updates': 1499}
skipping logging after 96000 examples to avoid logging too frequently
train stats after 96064 examples: {'rewards_train/chosen': '-0.70138', 'rewards_train/rejected': '-1.2343', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53293', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-178.7', 'loss/train': '0.54739', 'examples_per_second': '120.91', 'grad_norm': '21.946', 'counters/examples': 96064, 'counters/updates': 1501}
skipping logging after 96128 examples to avoid logging too frequently
train stats after 96192 examples: {'rewards_train/chosen': '-0.79404', 'rewards_train/rejected': '-1.1179', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.3239', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-135.16', 'loss/train': '0.64607', 'examples_per_second': '120', 'grad_norm': '30.435', 'counters/examples': 96192, 'counters/updates': 1503}
skipping logging after 96256 examples to avoid logging too frequently
Finished generating 1 epochs on train split
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/LATEST/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/LATEST/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-11_22-08-55_480950/LATEST/scheduler.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         counters/updates ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      examples_per_second ‚ñÉ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÅ
wandb:                grad_norm ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñá‚ñÖ
wandb:        logps_eval/chosen ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:      logps_eval/rejected ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:       logps_train/chosen ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñÉ‚ñÜ
wandb:     logps_train/rejected ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ
wandb:                loss/eval ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb:               loss/train ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ
wandb:  rewards_eval/accuracies ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá
wandb:      rewards_eval/chosen ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ
wandb:     rewards_eval/margins ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:    rewards_eval/rejected ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: rewards_train/accuracies ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñà‚ñÖ‚ñà‚ñÖ‚ñà‚ñá‚ñá
wandb:     rewards_train/chosen ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÖ‚ñÅ‚ñÅ
wandb:    rewards_train/margins ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ
wandb:   rewards_train/rejected ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÇ
wandb: 
wandb: Run summary:
wandb:        counters/examples 96192
wandb:         counters/updates 1503
wandb:      examples_per_second 120.00387
wandb:                grad_norm 30.43496
wandb:        logps_eval/chosen -155.13096
wandb:      logps_eval/rejected -137.67036
wandb:       logps_train/chosen -135.15965
wandb:     logps_train/rejected -121.76776
wandb:                loss/eval 0.62854
wandb:               loss/train 0.64607
wandb:  rewards_eval/accuracies 0.66797
wandb:      rewards_eval/chosen -0.72286
wandb:     rewards_eval/margins 0.43638
wandb:    rewards_eval/rejected -1.15924
wandb: rewards_train/accuracies 0.60938
wandb:     rewards_train/chosen -0.79404
wandb:    rewards_train/margins 0.3239
wandb:   rewards_train/rejected -1.11794
wandb: 
wandb: üöÄ View run pythia410m_dpo_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/sb6r4wt7
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240111_221023-sb6r4wt7/logs
