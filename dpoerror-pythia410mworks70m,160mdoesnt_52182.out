WARNING: eval_every must be divisible by batch_size
Setting eval_every to 11968
no FSDP port specified; using open port for FSDP: 56651
seed: 0
exp_name: pythia410m_dpo_seed0
batch_size: 64
eval_batch_size: 16
debug: false
fsdp_port: 56651
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329
lr: 1.0e-06
gradient_accumulation_steps: 1
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 11968
minimum_log_interval_secs: 1.0
model:
  name_or_path: lomahony/pythia-410m-helpful-sft
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-222-166:.cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240112_140401-5tkfezyq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia410m_dpo_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/5tkfezyq
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-410m-helpful-sft
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2908/5103 [00:00<00:00, 29073.65it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 29003.41it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   2%|‚ñè         | 1550/96256 [00:00<00:14, 6412.34it/s]Processing HH static:   5%|‚ñç         | 4406/96256 [00:00<00:06, 14637.61it/s]Processing HH static:   8%|‚ñä         | 7271/96256 [00:00<00:04, 19493.06it/s]Processing HH static:  11%|‚ñà         | 10127/96256 [00:00<00:03, 22494.03it/s]Processing HH static:  14%|‚ñà‚ñé        | 13018/96256 [00:00<00:03, 24552.84it/s]Processing HH static:  16%|‚ñà‚ñã        | 15736/96256 [00:00<00:03, 25377.22it/s]Processing HH static:  19%|‚ñà‚ñâ        | 18579/96256 [00:00<00:02, 26320.20it/s]Processing HH static:  22%|‚ñà‚ñà‚ñè       | 21461/96256 [00:00<00:02, 27086.55it/s]Processing HH static:  25%|‚ñà‚ñà‚ñå       | 24312/96256 [00:01<00:02, 27519.02it/s]Processing HH static:  28%|‚ñà‚ñà‚ñä       | 27187/96256 [00:01<00:02, 27890.44it/s]Processing HH static:  31%|‚ñà‚ñà‚ñà       | 30014/96256 [00:01<00:02, 27874.90it/s]Processing HH static:  34%|‚ñà‚ñà‚ñà‚ñç      | 32872/96256 [00:01<00:02, 28085.97it/s]Processing HH static:  37%|‚ñà‚ñà‚ñà‚ñã      | 35700/96256 [00:01<00:03, 16661.95it/s]Processing HH static:  40%|‚ñà‚ñà‚ñà‚ñà      | 38525/96256 [00:01<00:03, 19008.38it/s]Processing HH static:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 41391/96256 [00:01<00:02, 21173.88it/s]Processing HH static:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 44179/96256 [00:01<00:02, 22800.62it/s]Processing HH static:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 46943/96256 [00:02<00:02, 24044.74it/s]Processing HH static:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 49695/96256 [00:02<00:01, 24978.89it/s]Processing HH static:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 52448/96256 [00:02<00:01, 25686.50it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 55205/96256 [00:02<00:01, 26218.79it/s]Processing HH static:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 57928/96256 [00:02<00:01, 26418.74it/s]Processing HH static:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 60673/96256 [00:02<00:01, 26717.77it/s]Processing HH static:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 63396/96256 [00:02<00:01, 26207.55it/s]Processing HH static:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 66116/96256 [00:02<00:01, 26493.99it/s]Processing HH static:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 68793/96256 [00:02<00:01, 26516.52it/s]Processing HH static:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 71497/96256 [00:02<00:00, 26670.10it/s]Processing HH static:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 74220/96256 [00:03<00:00, 26834.58it/s]Processing HH static:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 76914/96256 [00:03<00:01, 15314.04it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 79649/96256 [00:03<00:00, 17660.35it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82367/96256 [00:03<00:00, 19733.71it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85090/96256 [00:03<00:00, 21512.63it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87695/96256 [00:03<00:00, 22660.35it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90437/96256 [00:03<00:00, 23922.71it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93159/96256 [00:04<00:00, 24827.82it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95882/96256 [00:04<00:00, 25503.66it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:04<00:00, 23185.14it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:01<00:23,  1.58s/it]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:01<00:10,  1.39it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:03,  3.02it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:02<00:01,  5.40it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:02<00:01,  6.64it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:02<00:00,  7.17it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:02<00:00,  7.67it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:02<00:00,  8.48it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:02<00:00,  8.75it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:03<00:00,  9.00it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  9.18it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:03<00:00,  5.15it/s]
eval after 0: {'rewards_eval/chosen': '0.0119', 'rewards_eval/rejected': '0.0090005', 'rewards_eval/accuracies': '0.51172', 'rewards_eval/margins': '0.0028991', 'logps_eval/rejected': '-125.99', 'logps_eval/chosen': '-147.78', 'loss/eval': '0.69277'}
train stats after 64 examples: {'rewards_train/chosen': '0.020365', 'rewards_train/rejected': '0.008292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012073', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-144.76', 'loss/train': '0.68818', 'examples_per_second': '76.479', 'grad_norm': '20.799', 'counters/examples': 64, 'counters/updates': 1}
train stats after 128 examples: {'rewards_train/chosen': '0.010627', 'rewards_train/rejected': '0.0088604', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0017668', 'logps_train/rejected': '-139.89', 'logps_train/chosen': '-150.17', 'loss/train': '0.69315', 'examples_per_second': '67.214', 'grad_norm': '20.999', 'counters/examples': 128, 'counters/updates': 2}
skipping logging after 192 examples to avoid logging too frequently
train stats after 256 examples: {'rewards_train/chosen': '0.0292', 'rewards_train/rejected': '0.01688', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01232', 'logps_train/rejected': '-145.43', 'logps_train/chosen': '-169.15', 'loss/train': '0.68861', 'examples_per_second': '123.46', 'grad_norm': '22.827', 'counters/examples': 256, 'counters/updates': 4}
skipping logging after 320 examples to avoid logging too frequently
train stats after 384 examples: {'rewards_train/chosen': '0.027319', 'rewards_train/rejected': '0.022934', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0043848', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-138.04', 'loss/train': '0.69205', 'examples_per_second': '127.59', 'grad_norm': '20.439', 'counters/examples': 384, 'counters/updates': 6}
skipping logging after 448 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '0.0060925', 'rewards_train/rejected': '0.014762', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0086699', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-150.68', 'loss/train': '0.69875', 'examples_per_second': '139.86', 'grad_norm': '21.535', 'counters/examples': 512, 'counters/updates': 8}
skipping logging after 576 examples to avoid logging too frequently
train stats after 640 examples: {'rewards_train/chosen': '0.019408', 'rewards_train/rejected': '0.0018792', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017529', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-152.13', 'loss/train': '0.68534', 'examples_per_second': '119.91', 'grad_norm': '20.647', 'counters/examples': 640, 'counters/updates': 10}
skipping logging after 704 examples to avoid logging too frequently
train stats after 768 examples: {'rewards_train/chosen': '0.011105', 'rewards_train/rejected': '0.021727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010622', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-128.18', 'loss/train': '0.69967', 'examples_per_second': '133.04', 'grad_norm': '19.526', 'counters/examples': 768, 'counters/updates': 12}
skipping logging after 832 examples to avoid logging too frequently
train stats after 896 examples: {'rewards_train/chosen': '0.020252', 'rewards_train/rejected': '0.006312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01394', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-130.06', 'loss/train': '0.68737', 'examples_per_second': '124.36', 'grad_norm': '19.884', 'counters/examples': 896, 'counters/updates': 14}
skipping logging after 960 examples to avoid logging too frequently
train stats after 1024 examples: {'rewards_train/chosen': '0.0070547', 'rewards_train/rejected': '0.0039129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0031418', 'logps_train/rejected': '-154.38', 'logps_train/chosen': '-147.9', 'loss/train': '0.69252', 'examples_per_second': '123.44', 'grad_norm': '21.167', 'counters/examples': 1024, 'counters/updates': 16}
skipping logging after 1088 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '0.018383', 'rewards_train/rejected': '0.0079791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010404', 'logps_train/rejected': '-122.64', 'logps_train/chosen': '-123.57', 'loss/train': '0.68895', 'examples_per_second': '119.48', 'grad_norm': '19.626', 'counters/examples': 1152, 'counters/updates': 18}
skipping logging after 1216 examples to avoid logging too frequently
train stats after 1280 examples: {'rewards_train/chosen': '0.028265', 'rewards_train/rejected': '-0.0087459', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.037011', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-148.04', 'loss/train': '0.67593', 'examples_per_second': '124.18', 'grad_norm': '20.598', 'counters/examples': 1280, 'counters/updates': 20}
skipping logging after 1344 examples to avoid logging too frequently
train stats after 1408 examples: {'rewards_train/chosen': '0.0088714', 'rewards_train/rejected': '0.0064707', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0024007', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-164.81', 'loss/train': '0.69328', 'examples_per_second': '123.62', 'grad_norm': '22.23', 'counters/examples': 1408, 'counters/updates': 22}
skipping logging after 1472 examples to avoid logging too frequently
train stats after 1536 examples: {'rewards_train/chosen': '0.011001', 'rewards_train/rejected': '0.00060687', 'rewards_train/accuracies': '0.45312', 'rewards_train/margins': '0.010394', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-145.51', 'loss/train': '0.68924', 'examples_per_second': '122.43', 'grad_norm': '21.096', 'counters/examples': 1536, 'counters/updates': 24}
skipping logging after 1600 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '0.011169', 'rewards_train/rejected': '-0.010392', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.021561', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-134.4', 'loss/train': '0.68389', 'examples_per_second': '118.34', 'grad_norm': '19.866', 'counters/examples': 1664, 'counters/updates': 26}
skipping logging after 1728 examples to avoid logging too frequently
train stats after 1792 examples: {'rewards_train/chosen': '0.025258', 'rewards_train/rejected': '-0.0063806', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031638', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-191.75', 'loss/train': '0.67911', 'examples_per_second': '122.31', 'grad_norm': '22.368', 'counters/examples': 1792, 'counters/updates': 28}
skipping logging after 1856 examples to avoid logging too frequently
train stats after 1920 examples: {'rewards_train/chosen': '-0.010541', 'rewards_train/rejected': '0.0016143', 'rewards_train/accuracies': '0.42188', 'rewards_train/margins': '-0.012155', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-125.21', 'loss/train': '0.70059', 'examples_per_second': '120.27', 'grad_norm': '20.155', 'counters/examples': 1920, 'counters/updates': 30}
skipping logging after 1984 examples to avoid logging too frequently
train stats after 2048 examples: {'rewards_train/chosen': '0.00042646', 'rewards_train/rejected': '-0.013398', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.013824', 'logps_train/rejected': '-164.98', 'logps_train/chosen': '-156.46', 'loss/train': '0.68777', 'examples_per_second': '124.2', 'grad_norm': '21.975', 'counters/examples': 2048, 'counters/updates': 32}
skipping logging after 2112 examples to avoid logging too frequently
train stats after 2176 examples: {'rewards_train/chosen': '-0.00653', 'rewards_train/rejected': '-0.025039', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.018509', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-128.88', 'loss/train': '0.6857', 'examples_per_second': '128.09', 'grad_norm': '20.411', 'counters/examples': 2176, 'counters/updates': 34}
skipping logging after 2240 examples to avoid logging too frequently
train stats after 2304 examples: {'rewards_train/chosen': '0.012747', 'rewards_train/rejected': '-0.029758', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042505', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-136.35', 'loss/train': '0.67353', 'examples_per_second': '125.42', 'grad_norm': '19.662', 'counters/examples': 2304, 'counters/updates': 36}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2432 examples: {'rewards_train/chosen': '-0.015654', 'rewards_train/rejected': '-0.035797', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.020143', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-143.44', 'loss/train': '0.68523', 'examples_per_second': '119.04', 'grad_norm': '21.626', 'counters/examples': 2432, 'counters/updates': 38}
skipping logging after 2496 examples to avoid logging too frequently
train stats after 2560 examples: {'rewards_train/chosen': '0.01048', 'rewards_train/rejected': '-0.036026', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.046506', 'logps_train/rejected': '-126.03', 'logps_train/chosen': '-148.71', 'loss/train': '0.67282', 'examples_per_second': '120.01', 'grad_norm': '20.15', 'counters/examples': 2560, 'counters/updates': 40}
skipping logging after 2624 examples to avoid logging too frequently
train stats after 2688 examples: {'rewards_train/chosen': '-0.0044342', 'rewards_train/rejected': '-0.013308', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.008874', 'logps_train/rejected': '-148.26', 'logps_train/chosen': '-152.75', 'loss/train': '0.69105', 'examples_per_second': '119.23', 'grad_norm': '22.231', 'counters/examples': 2688, 'counters/updates': 42}
skipping logging after 2752 examples to avoid logging too frequently
train stats after 2816 examples: {'rewards_train/chosen': '0.0053282', 'rewards_train/rejected': '-0.044951', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050279', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-148.37', 'loss/train': '0.67179', 'examples_per_second': '121.38', 'grad_norm': '20.601', 'counters/examples': 2816, 'counters/updates': 44}
skipping logging after 2880 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '0.017564', 'rewards_train/rejected': '-0.044503', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.062067', 'logps_train/rejected': '-138.72', 'logps_train/chosen': '-160.67', 'loss/train': '0.66606', 'examples_per_second': '136.22', 'grad_norm': '20.881', 'counters/examples': 2944, 'counters/updates': 46}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3072 examples: {'rewards_train/chosen': '0.00066129', 'rewards_train/rejected': '-0.050249', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.050911', 'logps_train/rejected': '-127.92', 'logps_train/chosen': '-146.33', 'loss/train': '0.6722', 'examples_per_second': '129.14', 'grad_norm': '20.57', 'counters/examples': 3072, 'counters/updates': 48}
skipping logging after 3136 examples to avoid logging too frequently
train stats after 3200 examples: {'rewards_train/chosen': '-0.027895', 'rewards_train/rejected': '-0.075136', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.047241', 'logps_train/rejected': '-119.99', 'logps_train/chosen': '-143.88', 'loss/train': '0.67524', 'examples_per_second': '124.39', 'grad_norm': '20.683', 'counters/examples': 3200, 'counters/updates': 50}
skipping logging after 3264 examples to avoid logging too frequently
train stats after 3328 examples: {'rewards_train/chosen': '0.015699', 'rewards_train/rejected': '-0.056977', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072676', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-159.41', 'loss/train': '0.66225', 'examples_per_second': '119.57', 'grad_norm': '20.828', 'counters/examples': 3328, 'counters/updates': 52}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3456 examples: {'rewards_train/chosen': '-0.08376', 'rewards_train/rejected': '-0.12364', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039881', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-170.82', 'loss/train': '0.68213', 'examples_per_second': '119.17', 'grad_norm': '21.388', 'counters/examples': 3456, 'counters/updates': 54}
skipping logging after 3520 examples to avoid logging too frequently
train stats after 3584 examples: {'rewards_train/chosen': '0.027804', 'rewards_train/rejected': '-0.070969', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098773', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-176.26', 'loss/train': '0.65353', 'examples_per_second': '123.58', 'grad_norm': '21.944', 'counters/examples': 3584, 'counters/updates': 56}
skipping logging after 3648 examples to avoid logging too frequently
train stats after 3712 examples: {'rewards_train/chosen': '-0.043326', 'rewards_train/rejected': '-0.10576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062432', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-152.76', 'loss/train': '0.67092', 'examples_per_second': '124.21', 'grad_norm': '20.764', 'counters/examples': 3712, 'counters/updates': 58}
skipping logging after 3776 examples to avoid logging too frequently
train stats after 3840 examples: {'rewards_train/chosen': '-0.034399', 'rewards_train/rejected': '-0.092644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058245', 'logps_train/rejected': '-116.84', 'logps_train/chosen': '-137.36', 'loss/train': '0.67104', 'examples_per_second': '124.89', 'grad_norm': '19.395', 'counters/examples': 3840, 'counters/updates': 60}
skipping logging after 3904 examples to avoid logging too frequently
train stats after 3968 examples: {'rewards_train/chosen': '-0.0035088', 'rewards_train/rejected': '-0.12401', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1205', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-156.79', 'loss/train': '0.64502', 'examples_per_second': '119.04', 'grad_norm': '19.806', 'counters/examples': 3968, 'counters/updates': 62}
skipping logging after 4032 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '-0.01727', 'rewards_train/rejected': '-0.10704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089773', 'logps_train/rejected': '-118.54', 'logps_train/chosen': '-150.07', 'loss/train': '0.6618', 'examples_per_second': '128.76', 'grad_norm': '20.259', 'counters/examples': 4096, 'counters/updates': 64}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4224 examples: {'rewards_train/chosen': '-0.034692', 'rewards_train/rejected': '-0.14722', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.11253', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-123.55', 'loss/train': '0.64724', 'examples_per_second': '124.48', 'grad_norm': '20.48', 'counters/examples': 4224, 'counters/updates': 66}
skipping logging after 4288 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '0.011918', 'rewards_train/rejected': '-0.062269', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074187', 'logps_train/rejected': '-149.3', 'logps_train/chosen': '-154.7', 'loss/train': '0.66618', 'examples_per_second': '132.32', 'grad_norm': '22.259', 'counters/examples': 4352, 'counters/updates': 68}
skipping logging after 4416 examples to avoid logging too frequently
train stats after 4480 examples: {'rewards_train/chosen': '-0.01881', 'rewards_train/rejected': '-0.13737', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11856', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-157.96', 'loss/train': '0.64705', 'examples_per_second': '130.66', 'grad_norm': '20.708', 'counters/examples': 4480, 'counters/updates': 70}
skipping logging after 4544 examples to avoid logging too frequently
train stats after 4608 examples: {'rewards_train/chosen': '-0.080198', 'rewards_train/rejected': '-0.16695', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.086748', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-151.81', 'loss/train': '0.66764', 'examples_per_second': '123.81', 'grad_norm': '20.766', 'counters/examples': 4608, 'counters/updates': 72}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4736 examples: {'rewards_train/chosen': '0.011305', 'rewards_train/rejected': '-0.10986', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.12116', 'logps_train/rejected': '-146.85', 'logps_train/chosen': '-169.81', 'loss/train': '0.64376', 'examples_per_second': '122.06', 'grad_norm': '21.918', 'counters/examples': 4736, 'counters/updates': 74}
skipping logging after 4800 examples to avoid logging too frequently
train stats after 4864 examples: {'rewards_train/chosen': '0.014329', 'rewards_train/rejected': '-0.15147', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.1658', 'logps_train/rejected': '-128.56', 'logps_train/chosen': '-166.5', 'loss/train': '0.63569', 'examples_per_second': '119.37', 'grad_norm': '21.376', 'counters/examples': 4864, 'counters/updates': 76}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4992 examples: {'rewards_train/chosen': '-0.052822', 'rewards_train/rejected': '-0.22225', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.16943', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-165.95', 'loss/train': '0.63348', 'examples_per_second': '123.1', 'grad_norm': '20.945', 'counters/examples': 4992, 'counters/updates': 78}
skipping logging after 5056 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '-0.050477', 'rewards_train/rejected': '-0.22672', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.17625', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-126.54', 'loss/train': '0.63178', 'examples_per_second': '124.4', 'grad_norm': '20.549', 'counters/examples': 5120, 'counters/updates': 80}
skipping logging after 5184 examples to avoid logging too frequently
train stats after 5248 examples: {'rewards_train/chosen': '-0.073029', 'rewards_train/rejected': '-0.19443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1214', 'logps_train/rejected': '-123.41', 'logps_train/chosen': '-136.18', 'loss/train': '0.65404', 'examples_per_second': '124.59', 'grad_norm': '20.711', 'counters/examples': 5248, 'counters/updates': 82}
skipping logging after 5312 examples to avoid logging too frequently
train stats after 5376 examples: {'rewards_train/chosen': '-0.023249', 'rewards_train/rejected': '-0.20712', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.18387', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-168.77', 'loss/train': '0.62637', 'examples_per_second': '124.62', 'grad_norm': '21.66', 'counters/examples': 5376, 'counters/updates': 84}
skipping logging after 5440 examples to avoid logging too frequently
train stats after 5504 examples: {'rewards_train/chosen': '-0.12377', 'rewards_train/rejected': '-0.26155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13778', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-154.72', 'loss/train': '0.65079', 'examples_per_second': '133.15', 'grad_norm': '20.752', 'counters/examples': 5504, 'counters/updates': 86}
skipping logging after 5568 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.1309', 'rewards_train/rejected': '-0.30665', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.17576', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-124.15', 'loss/train': '0.64276', 'examples_per_second': '154.66', 'grad_norm': '20.241', 'counters/examples': 5632, 'counters/updates': 88}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5760 examples: {'rewards_train/chosen': '-0.12101', 'rewards_train/rejected': '-0.26026', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.13925', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-161.96', 'loss/train': '0.65146', 'examples_per_second': '131.12', 'grad_norm': '20.55', 'counters/examples': 5760, 'counters/updates': 90}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5888 examples: {'rewards_train/chosen': '-0.018783', 'rewards_train/rejected': '-0.27278', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.254', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-142.85', 'loss/train': '0.59899', 'examples_per_second': '123.72', 'grad_norm': '20.157', 'counters/examples': 5888, 'counters/updates': 92}
skipping logging after 5952 examples to avoid logging too frequently
train stats after 6016 examples: {'rewards_train/chosen': '-0.16172', 'rewards_train/rejected': '-0.33583', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.17412', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-141.72', 'loss/train': '0.65113', 'examples_per_second': '122.28', 'grad_norm': '22.204', 'counters/examples': 6016, 'counters/updates': 94}
skipping logging after 6080 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.11492', 'rewards_train/rejected': '-0.29857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18365', 'logps_train/rejected': '-135.58', 'logps_train/chosen': '-144.41', 'loss/train': '0.64215', 'examples_per_second': '124.7', 'grad_norm': '20.894', 'counters/examples': 6144, 'counters/updates': 96}
skipping logging after 6208 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '-0.0093775', 'rewards_train/rejected': '-0.27987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27049', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-162.74', 'loss/train': '0.5989', 'examples_per_second': '124.43', 'grad_norm': '20.373', 'counters/examples': 6272, 'counters/updates': 98}
skipping logging after 6336 examples to avoid logging too frequently
train stats after 6400 examples: {'rewards_train/chosen': '-0.026515', 'rewards_train/rejected': '-0.27423', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24772', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-173.73', 'loss/train': '0.60899', 'examples_per_second': '123.99', 'grad_norm': '21.691', 'counters/examples': 6400, 'counters/updates': 100}
skipping logging after 6464 examples to avoid logging too frequently
train stats after 6528 examples: {'rewards_train/chosen': '-0.16687', 'rewards_train/rejected': '-0.25785', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.09098', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-128.07', 'loss/train': '0.67726', 'examples_per_second': '123.98', 'grad_norm': '20.592', 'counters/examples': 6528, 'counters/updates': 102}
skipping logging after 6592 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '-0.20062', 'rewards_train/rejected': '-0.27578', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.075162', 'logps_train/rejected': '-106.71', 'logps_train/chosen': '-138.1', 'loss/train': '0.68598', 'examples_per_second': '135.06', 'grad_norm': '21.266', 'counters/examples': 6656, 'counters/updates': 104}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6784 examples: {'rewards_train/chosen': '-0.17459', 'rewards_train/rejected': '-0.31776', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.14316', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-155.13', 'loss/train': '0.65642', 'examples_per_second': '121.85', 'grad_norm': '22.572', 'counters/examples': 6784, 'counters/updates': 106}
skipping logging after 6848 examples to avoid logging too frequently
train stats after 6912 examples: {'rewards_train/chosen': '-0.15348', 'rewards_train/rejected': '-0.33413', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18065', 'logps_train/rejected': '-143.72', 'logps_train/chosen': '-161.81', 'loss/train': '0.64669', 'examples_per_second': '118.9', 'grad_norm': '21.633', 'counters/examples': 6912, 'counters/updates': 108}
skipping logging after 6976 examples to avoid logging too frequently
train stats after 7040 examples: {'rewards_train/chosen': '-0.1629', 'rewards_train/rejected': '-0.30914', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.14624', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-162.03', 'loss/train': '0.64877', 'examples_per_second': '124.74', 'grad_norm': '22.404', 'counters/examples': 7040, 'counters/updates': 110}
skipping logging after 7104 examples to avoid logging too frequently
train stats after 7168 examples: {'rewards_train/chosen': '-0.081486', 'rewards_train/rejected': '-0.24642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16494', 'logps_train/rejected': '-132.69', 'logps_train/chosen': '-164.11', 'loss/train': '0.64215', 'examples_per_second': '124.34', 'grad_norm': '22.019', 'counters/examples': 7168, 'counters/updates': 112}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7296 examples: {'rewards_train/chosen': '-0.060752', 'rewards_train/rejected': '-0.33235', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2716', 'logps_train/rejected': '-135.96', 'logps_train/chosen': '-164.19', 'loss/train': '0.59465', 'examples_per_second': '124.38', 'grad_norm': '20.957', 'counters/examples': 7296, 'counters/updates': 114}
skipping logging after 7360 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '-0.2402', 'rewards_train/rejected': '-0.49106', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25086', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-141.72', 'loss/train': '0.61285', 'examples_per_second': '124.08', 'grad_norm': '20.039', 'counters/examples': 7424, 'counters/updates': 116}
skipping logging after 7488 examples to avoid logging too frequently
train stats after 7552 examples: {'rewards_train/chosen': '-0.16213', 'rewards_train/rejected': '-0.3913', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.22918', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-133.68', 'loss/train': '0.63097', 'examples_per_second': '118.97', 'grad_norm': '20.424', 'counters/examples': 7552, 'counters/updates': 118}
skipping logging after 7616 examples to avoid logging too frequently
train stats after 7680 examples: {'rewards_train/chosen': '-0.14138', 'rewards_train/rejected': '-0.27952', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.13814', 'logps_train/rejected': '-151.81', 'logps_train/chosen': '-162.76', 'loss/train': '0.65718', 'examples_per_second': '135.42', 'grad_norm': '22.659', 'counters/examples': 7680, 'counters/updates': 120}
skipping logging after 7744 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '-0.19021', 'rewards_train/rejected': '-0.51389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32368', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-176.69', 'loss/train': '0.61399', 'examples_per_second': '128.12', 'grad_norm': '20.28', 'counters/examples': 7808, 'counters/updates': 122}
skipping logging after 7872 examples to avoid logging too frequently
train stats after 7936 examples: {'rewards_train/chosen': '-0.23289', 'rewards_train/rejected': '-0.31828', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.08539', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-122.67', 'loss/train': '0.69103', 'examples_per_second': '119.03', 'grad_norm': '22.952', 'counters/examples': 7936, 'counters/updates': 124}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8064 examples: {'rewards_train/chosen': '-0.045239', 'rewards_train/rejected': '-0.1995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15426', 'logps_train/rejected': '-140.18', 'logps_train/chosen': '-163.78', 'loss/train': '0.64548', 'examples_per_second': '118.68', 'grad_norm': '22.791', 'counters/examples': 8064, 'counters/updates': 126}
skipping logging after 8128 examples to avoid logging too frequently
train stats after 8192 examples: {'rewards_train/chosen': '-0.075585', 'rewards_train/rejected': '-0.31223', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23664', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-180.73', 'loss/train': '0.61664', 'examples_per_second': '119', 'grad_norm': '21.695', 'counters/examples': 8192, 'counters/updates': 128}
skipping logging after 8256 examples to avoid logging too frequently
train stats after 8320 examples: {'rewards_train/chosen': '-0.16201', 'rewards_train/rejected': '-0.3825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22049', 'logps_train/rejected': '-124.48', 'logps_train/chosen': '-142.41', 'loss/train': '0.62858', 'examples_per_second': '128.19', 'grad_norm': '20.583', 'counters/examples': 8320, 'counters/updates': 130}
skipping logging after 8384 examples to avoid logging too frequently
train stats after 8448 examples: {'rewards_train/chosen': '-0.04005', 'rewards_train/rejected': '-0.36269', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32264', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-155.48', 'loss/train': '0.60288', 'examples_per_second': '123.78', 'grad_norm': '21.132', 'counters/examples': 8448, 'counters/updates': 132}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8576 examples: {'rewards_train/chosen': '0.073214', 'rewards_train/rejected': '-0.17575', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24896', 'logps_train/rejected': '-138.94', 'logps_train/chosen': '-159.39', 'loss/train': '0.61301', 'examples_per_second': '120.73', 'grad_norm': '20.895', 'counters/examples': 8576, 'counters/updates': 134}
skipping logging after 8640 examples to avoid logging too frequently
train stats after 8704 examples: {'rewards_train/chosen': '-0.084147', 'rewards_train/rejected': '-0.37452', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29037', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-134.45', 'loss/train': '0.59281', 'examples_per_second': '124.56', 'grad_norm': '19.346', 'counters/examples': 8704, 'counters/updates': 136}
skipping logging after 8768 examples to avoid logging too frequently
train stats after 8832 examples: {'rewards_train/chosen': '-0.048892', 'rewards_train/rejected': '-0.32838', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.27948', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-157.08', 'loss/train': '0.6144', 'examples_per_second': '124.47', 'grad_norm': '22.848', 'counters/examples': 8832, 'counters/updates': 138}
skipping logging after 8896 examples to avoid logging too frequently
train stats after 8960 examples: {'rewards_train/chosen': '-0.15438', 'rewards_train/rejected': '-0.30102', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.14664', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-138.23', 'loss/train': '0.65676', 'examples_per_second': '118.42', 'grad_norm': '20.807', 'counters/examples': 8960, 'counters/updates': 140}
skipping logging after 9024 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '-0.12336', 'rewards_train/rejected': '-0.43544', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31208', 'logps_train/rejected': '-128.45', 'logps_train/chosen': '-142.68', 'loss/train': '0.60117', 'examples_per_second': '123.45', 'grad_norm': '20.161', 'counters/examples': 9088, 'counters/updates': 142}
skipping logging after 9152 examples to avoid logging too frequently
train stats after 9216 examples: {'rewards_train/chosen': '-0.1658', 'rewards_train/rejected': '-0.31469', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.14889', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-163.16', 'loss/train': '0.65528', 'examples_per_second': '123.76', 'grad_norm': '23.305', 'counters/examples': 9216, 'counters/updates': 144}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9344 examples: {'rewards_train/chosen': '-0.18169', 'rewards_train/rejected': '-0.38073', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19904', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-150.93', 'loss/train': '0.63662', 'examples_per_second': '123.76', 'grad_norm': '22.053', 'counters/examples': 9344, 'counters/updates': 146}
skipping logging after 9408 examples to avoid logging too frequently
train stats after 9472 examples: {'rewards_train/chosen': '-0.19956', 'rewards_train/rejected': '-0.51124', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31168', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-123.43', 'loss/train': '0.6057', 'examples_per_second': '124.25', 'grad_norm': '19.543', 'counters/examples': 9472, 'counters/updates': 148}
skipping logging after 9536 examples to avoid logging too frequently
train stats after 9600 examples: {'rewards_train/chosen': '-0.10328', 'rewards_train/rejected': '-0.30842', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.20513', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-148.16', 'loss/train': '0.64949', 'examples_per_second': '124.29', 'grad_norm': '23.383', 'counters/examples': 9600, 'counters/updates': 150}
skipping logging after 9664 examples to avoid logging too frequently
train stats after 9728 examples: {'rewards_train/chosen': '-0.08311', 'rewards_train/rejected': '-0.41442', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.33131', 'logps_train/rejected': '-135.57', 'logps_train/chosen': '-152.11', 'loss/train': '0.58235', 'examples_per_second': '117.99', 'grad_norm': '20.634', 'counters/examples': 9728, 'counters/updates': 152}
skipping logging after 9792 examples to avoid logging too frequently
train stats after 9856 examples: {'rewards_train/chosen': '-0.32399', 'rewards_train/rejected': '-0.6185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29451', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-120.06', 'loss/train': '0.60758', 'examples_per_second': '127.63', 'grad_norm': '19.545', 'counters/examples': 9856, 'counters/updates': 154}
skipping logging after 9920 examples to avoid logging too frequently
train stats after 9984 examples: {'rewards_train/chosen': '-0.18494', 'rewards_train/rejected': '-0.64085', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.45591', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-145.26', 'loss/train': '0.56854', 'examples_per_second': '122.76', 'grad_norm': '20.511', 'counters/examples': 9984, 'counters/updates': 156}
skipping logging after 10048 examples to avoid logging too frequently
train stats after 10112 examples: {'rewards_train/chosen': '-0.21792', 'rewards_train/rejected': '-0.46516', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.24724', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-181.39', 'loss/train': '0.63931', 'examples_per_second': '124.25', 'grad_norm': '24.144', 'counters/examples': 10112, 'counters/updates': 158}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10240 examples: {'rewards_train/chosen': '-0.26768', 'rewards_train/rejected': '-0.43932', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.17165', 'logps_train/rejected': '-137.13', 'logps_train/chosen': '-176.95', 'loss/train': '0.65646', 'examples_per_second': '124.29', 'grad_norm': '25.014', 'counters/examples': 10240, 'counters/updates': 160}
skipping logging after 10304 examples to avoid logging too frequently
train stats after 10368 examples: {'rewards_train/chosen': '-0.37015', 'rewards_train/rejected': '-0.74551', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37537', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-157.94', 'loss/train': '0.58296', 'examples_per_second': '124.09', 'grad_norm': '21.41', 'counters/examples': 10368, 'counters/updates': 162}
skipping logging after 10432 examples to avoid logging too frequently
train stats after 10496 examples: {'rewards_train/chosen': '-0.53974', 'rewards_train/rejected': '-0.87535', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.33561', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-142.97', 'loss/train': '0.60106', 'examples_per_second': '124.3', 'grad_norm': '21.498', 'counters/examples': 10496, 'counters/updates': 164}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10624 examples: {'rewards_train/chosen': '-0.29203', 'rewards_train/rejected': '-0.65214', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36011', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-140.12', 'loss/train': '0.58475', 'examples_per_second': '118.83', 'grad_norm': '21.133', 'counters/examples': 10624, 'counters/updates': 166}
skipping logging after 10688 examples to avoid logging too frequently
train stats after 10752 examples: {'rewards_train/chosen': '-0.28531', 'rewards_train/rejected': '-0.65603', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.37072', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-155.98', 'loss/train': '0.59659', 'examples_per_second': '118.26', 'grad_norm': '22.204', 'counters/examples': 10752, 'counters/updates': 168}
skipping logging after 10816 examples to avoid logging too frequently
train stats after 10880 examples: {'rewards_train/chosen': '-0.51255', 'rewards_train/rejected': '-0.70212', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.18956', 'logps_train/rejected': '-185.59', 'logps_train/chosen': '-173.06', 'loss/train': '0.66145', 'examples_per_second': '124.2', 'grad_norm': '25.457', 'counters/examples': 10880, 'counters/updates': 170}
skipping logging after 10944 examples to avoid logging too frequently
train stats after 11008 examples: {'rewards_train/chosen': '-0.21926', 'rewards_train/rejected': '-0.61305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39379', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-160.44', 'loss/train': '0.58736', 'examples_per_second': '122.09', 'grad_norm': '20.862', 'counters/examples': 11008, 'counters/updates': 172}
skipping logging after 11072 examples to avoid logging too frequently
train stats after 11136 examples: {'rewards_train/chosen': '-0.1481', 'rewards_train/rejected': '-0.512', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3639', 'logps_train/rejected': '-162.94', 'logps_train/chosen': '-173.65', 'loss/train': '0.59929', 'examples_per_second': '124.25', 'grad_norm': '24.222', 'counters/examples': 11136, 'counters/updates': 174}
skipping logging after 11200 examples to avoid logging too frequently
train stats after 11264 examples: {'rewards_train/chosen': '-0.35405', 'rewards_train/rejected': '-0.49483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14078', 'logps_train/rejected': '-153.2', 'logps_train/chosen': '-150.76', 'loss/train': '0.68812', 'examples_per_second': '124.35', 'grad_norm': '24.828', 'counters/examples': 11264, 'counters/updates': 176}
skipping logging after 11328 examples to avoid logging too frequently
train stats after 11392 examples: {'rewards_train/chosen': '-0.11299', 'rewards_train/rejected': '-0.47516', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36217', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-149.92', 'loss/train': '0.58284', 'examples_per_second': '119.29', 'grad_norm': '20.912', 'counters/examples': 11392, 'counters/updates': 178}
skipping logging after 11456 examples to avoid logging too frequently
train stats after 11520 examples: {'rewards_train/chosen': '-0.16253', 'rewards_train/rejected': '-0.42935', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.26682', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-169.86', 'loss/train': '0.62288', 'examples_per_second': '125.94', 'grad_norm': '23.476', 'counters/examples': 11520, 'counters/updates': 180}
skipping logging after 11584 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '-0.30913', 'rewards_train/rejected': '-0.61932', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31019', 'logps_train/rejected': '-139.37', 'logps_train/chosen': '-142.1', 'loss/train': '0.61334', 'examples_per_second': '118.95', 'grad_norm': '22.243', 'counters/examples': 11648, 'counters/updates': 182}
skipping logging after 11712 examples to avoid logging too frequently
train stats after 11776 examples: {'rewards_train/chosen': '-0.30842', 'rewards_train/rejected': '-0.66', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35158', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-185.36', 'loss/train': '0.61034', 'examples_per_second': '120', 'grad_norm': '23.585', 'counters/examples': 11776, 'counters/updates': 184}
skipping logging after 11840 examples to avoid logging too frequently
train stats after 11904 examples: {'rewards_train/chosen': '-0.36023', 'rewards_train/rejected': '-0.62095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.26072', 'logps_train/rejected': '-171.8', 'logps_train/chosen': '-159.72', 'loss/train': '0.63606', 'examples_per_second': '124.13', 'grad_norm': '23.968', 'counters/examples': 11904, 'counters/updates': 186}
skipping logging after 11968 examples to avoid logging too frequently
Running evaluation after 11968 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  8.90it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01,  9.50it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  9.83it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.03it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.08it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00, 10.10it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.10it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.10it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.14it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.04it/s]
eval after 11968: {'rewards_eval/chosen': '-0.31451', 'rewards_eval/rejected': '-0.62784', 'rewards_eval/accuracies': '0.64844', 'rewards_eval/margins': '0.31333', 'logps_eval/rejected': '-132.36', 'logps_eval/chosen': '-151.05', 'loss/eval': '0.63862'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-11968...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-11968/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-11968/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-11968/scheduler.pt...
train stats after 12032 examples: {'rewards_train/chosen': '-0.21366', 'rewards_train/rejected': '-0.57278', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35912', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-151.74', 'loss/train': '0.61176', 'examples_per_second': '120.27', 'grad_norm': '22.268', 'counters/examples': 12032, 'counters/updates': 188}
skipping logging after 12096 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.43182', 'rewards_train/rejected': '-0.80296', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37114', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-163.84', 'loss/train': '0.60221', 'examples_per_second': '130.28', 'grad_norm': '21.932', 'counters/examples': 12160, 'counters/updates': 190}
skipping logging after 12224 examples to avoid logging too frequently
train stats after 12288 examples: {'rewards_train/chosen': '-0.49348', 'rewards_train/rejected': '-0.76651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27303', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-154.15', 'loss/train': '0.63696', 'examples_per_second': '124.43', 'grad_norm': '22.226', 'counters/examples': 12288, 'counters/updates': 192}
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12416 examples: {'rewards_train/chosen': '-0.15416', 'rewards_train/rejected': '-0.36574', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.21157', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-121.72', 'loss/train': '0.66124', 'examples_per_second': '123.79', 'grad_norm': '21.099', 'counters/examples': 12416, 'counters/updates': 194}
skipping logging after 12480 examples to avoid logging too frequently
train stats after 12544 examples: {'rewards_train/chosen': '-0.2698', 'rewards_train/rejected': '-0.51973', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24993', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-162.9', 'loss/train': '0.64415', 'examples_per_second': '124.8', 'grad_norm': '23.779', 'counters/examples': 12544, 'counters/updates': 196}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12672 examples: {'rewards_train/chosen': '-0.087311', 'rewards_train/rejected': '-0.4792', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.39189', 'logps_train/rejected': '-144.28', 'logps_train/chosen': '-156.13', 'loss/train': '0.57328', 'examples_per_second': '119.33', 'grad_norm': '21.193', 'counters/examples': 12672, 'counters/updates': 198}
skipping logging after 12736 examples to avoid logging too frequently
train stats after 12800 examples: {'rewards_train/chosen': '-0.21863', 'rewards_train/rejected': '-0.63433', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4157', 'logps_train/rejected': '-127.99', 'logps_train/chosen': '-145.57', 'loss/train': '0.58245', 'examples_per_second': '124.53', 'grad_norm': '21.073', 'counters/examples': 12800, 'counters/updates': 200}
skipping logging after 12864 examples to avoid logging too frequently
train stats after 12928 examples: {'rewards_train/chosen': '-0.28669', 'rewards_train/rejected': '-0.56896', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28228', 'logps_train/rejected': '-167.65', 'logps_train/chosen': '-157.29', 'loss/train': '0.63926', 'examples_per_second': '124.78', 'grad_norm': '24.827', 'counters/examples': 12928, 'counters/updates': 202}
skipping logging after 12992 examples to avoid logging too frequently
train stats after 13056 examples: {'rewards_train/chosen': '-0.33448', 'rewards_train/rejected': '-0.73956', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40509', 'logps_train/rejected': '-121.29', 'logps_train/chosen': '-123.36', 'loss/train': '0.60246', 'examples_per_second': '123.53', 'grad_norm': '19.909', 'counters/examples': 13056, 'counters/updates': 204}
skipping logging after 13120 examples to avoid logging too frequently
train stats after 13184 examples: {'rewards_train/chosen': '-0.13267', 'rewards_train/rejected': '-0.48338', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35071', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-149.74', 'loss/train': '0.58663', 'examples_per_second': '123.36', 'grad_norm': '21.071', 'counters/examples': 13184, 'counters/updates': 206}
skipping logging after 13248 examples to avoid logging too frequently
train stats after 13312 examples: {'rewards_train/chosen': '-0.19669', 'rewards_train/rejected': '-0.55003', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35334', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-179.48', 'loss/train': '0.58416', 'examples_per_second': '124.52', 'grad_norm': '22.095', 'counters/examples': 13312, 'counters/updates': 208}
skipping logging after 13376 examples to avoid logging too frequently
train stats after 13440 examples: {'rewards_train/chosen': '-0.51624', 'rewards_train/rejected': '-0.72045', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.20421', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-154.73', 'loss/train': '0.67155', 'examples_per_second': '124.73', 'grad_norm': '24.971', 'counters/examples': 13440, 'counters/updates': 210}
skipping logging after 13504 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '-0.20818', 'rewards_train/rejected': '-0.74486', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.53668', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-149.08', 'loss/train': '0.51908', 'examples_per_second': '124.47', 'grad_norm': '20.41', 'counters/examples': 13568, 'counters/updates': 212}
skipping logging after 13632 examples to avoid logging too frequently
train stats after 13696 examples: {'rewards_train/chosen': '-0.38209', 'rewards_train/rejected': '-0.52981', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14773', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-155.17', 'loss/train': '0.69661', 'examples_per_second': '127.66', 'grad_norm': '26.612', 'counters/examples': 13696, 'counters/updates': 214}
skipping logging after 13760 examples to avoid logging too frequently
train stats after 13824 examples: {'rewards_train/chosen': '-0.26367', 'rewards_train/rejected': '-0.49711', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23343', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-153.1', 'loss/train': '0.66006', 'examples_per_second': '124.59', 'grad_norm': '23.98', 'counters/examples': 13824, 'counters/updates': 216}
skipping logging after 13888 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '-0.044745', 'rewards_train/rejected': '-0.33037', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.28562', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-145.43', 'loss/train': '0.62178', 'examples_per_second': '134.51', 'grad_norm': '21.84', 'counters/examples': 13952, 'counters/updates': 218}
skipping logging after 14016 examples to avoid logging too frequently
train stats after 14080 examples: {'rewards_train/chosen': '-0.14418', 'rewards_train/rejected': '-0.37458', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.2304', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-128.84', 'loss/train': '0.65873', 'examples_per_second': '124.61', 'grad_norm': '23.703', 'counters/examples': 14080, 'counters/updates': 220}
skipping logging after 14144 examples to avoid logging too frequently
train stats after 14208 examples: {'rewards_train/chosen': '-0.15984', 'rewards_train/rejected': '-0.4106', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25076', 'logps_train/rejected': '-128.27', 'logps_train/chosen': '-156.61', 'loss/train': '0.62151', 'examples_per_second': '133.83', 'grad_norm': '22.654', 'counters/examples': 14208, 'counters/updates': 222}
skipping logging after 14272 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.027121', 'rewards_train/rejected': '-0.33199', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.35911', 'logps_train/rejected': '-159.08', 'logps_train/chosen': '-149.28', 'loss/train': '0.57528', 'examples_per_second': '120.18', 'grad_norm': '22.625', 'counters/examples': 14336, 'counters/updates': 224}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14464 examples: {'rewards_train/chosen': '-0.10643', 'rewards_train/rejected': '-0.25998', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.15355', 'logps_train/rejected': '-151.07', 'logps_train/chosen': '-163.63', 'loss/train': '0.67068', 'examples_per_second': '121.34', 'grad_norm': '25.998', 'counters/examples': 14464, 'counters/updates': 226}
skipping logging after 14528 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '-0.01352', 'rewards_train/rejected': '-0.29122', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.2777', 'logps_train/rejected': '-134.46', 'logps_train/chosen': '-141.41', 'loss/train': '0.62511', 'examples_per_second': '124.63', 'grad_norm': '23.825', 'counters/examples': 14592, 'counters/updates': 228}
skipping logging after 14656 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '-0.095354', 'rewards_train/rejected': '-0.29862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20327', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-156.11', 'loss/train': '0.64484', 'examples_per_second': '126.34', 'grad_norm': '23.579', 'counters/examples': 14720, 'counters/updates': 230}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14848 examples: {'rewards_train/chosen': '-0.19194', 'rewards_train/rejected': '-0.49003', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29808', 'logps_train/rejected': '-138.6', 'logps_train/chosen': '-143.13', 'loss/train': '0.61813', 'examples_per_second': '124.72', 'grad_norm': '22.471', 'counters/examples': 14848, 'counters/updates': 232}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14976 examples: {'rewards_train/chosen': '-0.18699', 'rewards_train/rejected': '-0.52696', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.33997', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-130.49', 'loss/train': '0.60768', 'examples_per_second': '124.74', 'grad_norm': '20.059', 'counters/examples': 14976, 'counters/updates': 234}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15104 examples: {'rewards_train/chosen': '-0.34215', 'rewards_train/rejected': '-0.60134', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.25918', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-149.93', 'loss/train': '0.62287', 'examples_per_second': '120.6', 'grad_norm': '23.444', 'counters/examples': 15104, 'counters/updates': 236}
skipping logging after 15168 examples to avoid logging too frequently
train stats after 15232 examples: {'rewards_train/chosen': '-0.33892', 'rewards_train/rejected': '-0.37619', 'rewards_train/accuracies': '0.45312', 'rewards_train/margins': '0.037275', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-151.69', 'loss/train': '0.73165', 'examples_per_second': '123.02', 'grad_norm': '25.258', 'counters/examples': 15232, 'counters/updates': 238}
skipping logging after 15296 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '-0.16756', 'rewards_train/rejected': '-0.51529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34773', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-163.92', 'loss/train': '0.59528', 'examples_per_second': '119.54', 'grad_norm': '22.758', 'counters/examples': 15360, 'counters/updates': 240}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15488 examples: {'rewards_train/chosen': '-0.27917', 'rewards_train/rejected': '-0.41747', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1383', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-131.06', 'loss/train': '0.68068', 'examples_per_second': '124.54', 'grad_norm': '22.32', 'counters/examples': 15488, 'counters/updates': 242}
skipping logging after 15552 examples to avoid logging too frequently
train stats after 15616 examples: {'rewards_train/chosen': '-0.29326', 'rewards_train/rejected': '-0.63246', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.3392', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-157.68', 'loss/train': '0.61389', 'examples_per_second': '124.26', 'grad_norm': '22.15', 'counters/examples': 15616, 'counters/updates': 244}
skipping logging after 15680 examples to avoid logging too frequently
train stats after 15744 examples: {'rewards_train/chosen': '-0.1851', 'rewards_train/rejected': '-0.48854', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30343', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-136.64', 'loss/train': '0.62683', 'examples_per_second': '124.9', 'grad_norm': '25.489', 'counters/examples': 15744, 'counters/updates': 246}
skipping logging after 15808 examples to avoid logging too frequently
train stats after 15872 examples: {'rewards_train/chosen': '-0.20649', 'rewards_train/rejected': '-0.47278', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26629', 'logps_train/rejected': '-152.1', 'logps_train/chosen': '-140.44', 'loss/train': '0.62504', 'examples_per_second': '124.44', 'grad_norm': '24.789', 'counters/examples': 15872, 'counters/updates': 248}
skipping logging after 15936 examples to avoid logging too frequently
train stats after 16000 examples: {'rewards_train/chosen': '-0.12832', 'rewards_train/rejected': '-0.41159', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28327', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-158.59', 'loss/train': '0.62321', 'examples_per_second': '123.99', 'grad_norm': '24.942', 'counters/examples': 16000, 'counters/updates': 250}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16128 examples: {'rewards_train/chosen': '-0.26148', 'rewards_train/rejected': '-0.54173', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.28025', 'logps_train/rejected': '-156.3', 'logps_train/chosen': '-168.18', 'loss/train': '0.65546', 'examples_per_second': '124.43', 'grad_norm': '24.237', 'counters/examples': 16128, 'counters/updates': 252}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16256 examples: {'rewards_train/chosen': '-0.34235', 'rewards_train/rejected': '-0.57616', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.23381', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-143.82', 'loss/train': '0.63531', 'examples_per_second': '126.39', 'grad_norm': '24.252', 'counters/examples': 16256, 'counters/updates': 254}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16384 examples: {'rewards_train/chosen': '-0.17644', 'rewards_train/rejected': '-0.45315', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.27672', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-154.79', 'loss/train': '0.60477', 'examples_per_second': '124.58', 'grad_norm': '25.106', 'counters/examples': 16384, 'counters/updates': 256}
skipping logging after 16448 examples to avoid logging too frequently
train stats after 16512 examples: {'rewards_train/chosen': '-0.58819', 'rewards_train/rejected': '-0.67164', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.083451', 'logps_train/rejected': '-110.84', 'logps_train/chosen': '-136.37', 'loss/train': '0.7371', 'examples_per_second': '124.3', 'grad_norm': '27.742', 'counters/examples': 16512, 'counters/updates': 258}
skipping logging after 16576 examples to avoid logging too frequently
train stats after 16640 examples: {'rewards_train/chosen': '-0.42132', 'rewards_train/rejected': '-0.74217', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.32085', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-151.29', 'loss/train': '0.60463', 'examples_per_second': '118.06', 'grad_norm': '22.189', 'counters/examples': 16640, 'counters/updates': 260}
skipping logging after 16704 examples to avoid logging too frequently
train stats after 16768 examples: {'rewards_train/chosen': '-0.41186', 'rewards_train/rejected': '-0.63825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22639', 'logps_train/rejected': '-139.31', 'logps_train/chosen': '-140.34', 'loss/train': '0.65456', 'examples_per_second': '124.53', 'grad_norm': '24.726', 'counters/examples': 16768, 'counters/updates': 262}
skipping logging after 16832 examples to avoid logging too frequently
train stats after 16896 examples: {'rewards_train/chosen': '-0.33444', 'rewards_train/rejected': '-0.73144', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.397', 'logps_train/rejected': '-179.25', 'logps_train/chosen': '-162.02', 'loss/train': '0.5821', 'examples_per_second': '122.85', 'grad_norm': '24.138', 'counters/examples': 16896, 'counters/updates': 264}
skipping logging after 16960 examples to avoid logging too frequently
train stats after 17024 examples: {'rewards_train/chosen': '-0.54665', 'rewards_train/rejected': '-0.75079', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.20413', 'logps_train/rejected': '-179.9', 'logps_train/chosen': '-152.33', 'loss/train': '0.65128', 'examples_per_second': '124.27', 'grad_norm': '25.326', 'counters/examples': 17024, 'counters/updates': 266}
skipping logging after 17088 examples to avoid logging too frequently
train stats after 17152 examples: {'rewards_train/chosen': '-0.26649', 'rewards_train/rejected': '-0.58601', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31951', 'logps_train/rejected': '-135.53', 'logps_train/chosen': '-153.44', 'loss/train': '0.61923', 'examples_per_second': '124.75', 'grad_norm': '22.554', 'counters/examples': 17152, 'counters/updates': 268}
skipping logging after 17216 examples to avoid logging too frequently
train stats after 17280 examples: {'rewards_train/chosen': '-0.43041', 'rewards_train/rejected': '-0.70578', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27537', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-151.07', 'loss/train': '0.64947', 'examples_per_second': '124.31', 'grad_norm': '23.979', 'counters/examples': 17280, 'counters/updates': 270}
skipping logging after 17344 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.3779', 'rewards_train/rejected': '-0.67781', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29991', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-164.51', 'loss/train': '0.64628', 'examples_per_second': '120.77', 'grad_norm': '24.245', 'counters/examples': 17408, 'counters/updates': 272}
skipping logging after 17472 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '-0.52883', 'rewards_train/rejected': '-0.81276', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28393', 'logps_train/rejected': '-134.3', 'logps_train/chosen': '-141.38', 'loss/train': '0.63182', 'examples_per_second': '130.58', 'grad_norm': '22.851', 'counters/examples': 17536, 'counters/updates': 274}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17664 examples: {'rewards_train/chosen': '-0.42964', 'rewards_train/rejected': '-1.0274', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.59779', 'logps_train/rejected': '-149.32', 'logps_train/chosen': '-134.27', 'loss/train': '0.53495', 'examples_per_second': '119.03', 'grad_norm': '20.221', 'counters/examples': 17664, 'counters/updates': 276}
skipping logging after 17728 examples to avoid logging too frequently
train stats after 17792 examples: {'rewards_train/chosen': '-0.55659', 'rewards_train/rejected': '-0.80558', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24899', 'logps_train/rejected': '-117', 'logps_train/chosen': '-114.47', 'loss/train': '0.67041', 'examples_per_second': '128.33', 'grad_norm': '22.567', 'counters/examples': 17792, 'counters/updates': 278}
skipping logging after 17856 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '-0.33128', 'rewards_train/rejected': '-0.67837', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34709', 'logps_train/rejected': '-137.13', 'logps_train/chosen': '-129.32', 'loss/train': '0.61663', 'examples_per_second': '140.99', 'grad_norm': '21.889', 'counters/examples': 17920, 'counters/updates': 280}
skipping logging after 17984 examples to avoid logging too frequently
train stats after 18048 examples: {'rewards_train/chosen': '-0.25428', 'rewards_train/rejected': '-0.62222', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36794', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-152.35', 'loss/train': '0.60909', 'examples_per_second': '124.56', 'grad_norm': '23.055', 'counters/examples': 18048, 'counters/updates': 282}
skipping logging after 18112 examples to avoid logging too frequently
train stats after 18176 examples: {'rewards_train/chosen': '-0.35193', 'rewards_train/rejected': '-0.72011', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36818', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-151.37', 'loss/train': '0.61404', 'examples_per_second': '117.96', 'grad_norm': '22.312', 'counters/examples': 18176, 'counters/updates': 284}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18304 examples: {'rewards_train/chosen': '-0.38082', 'rewards_train/rejected': '-0.88286', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.50205', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-180.51', 'loss/train': '0.52692', 'examples_per_second': '123.05', 'grad_norm': '21.262', 'counters/examples': 18304, 'counters/updates': 286}
skipping logging after 18368 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '-0.45103', 'rewards_train/rejected': '-0.64787', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19684', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-144.23', 'loss/train': '0.70951', 'examples_per_second': '119.51', 'grad_norm': '29.277', 'counters/examples': 18432, 'counters/updates': 288}
skipping logging after 18496 examples to avoid logging too frequently
train stats after 18560 examples: {'rewards_train/chosen': '-0.2929', 'rewards_train/rejected': '-0.64267', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34977', 'logps_train/rejected': '-134.61', 'logps_train/chosen': '-174.91', 'loss/train': '0.59756', 'examples_per_second': '123.71', 'grad_norm': '24.878', 'counters/examples': 18560, 'counters/updates': 290}
skipping logging after 18624 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.20036', 'rewards_train/rejected': '-0.4543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.25393', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-144.7', 'loss/train': '0.64383', 'examples_per_second': '126.08', 'grad_norm': '21.672', 'counters/examples': 18688, 'counters/updates': 292}
skipping logging after 18752 examples to avoid logging too frequently
train stats after 18816 examples: {'rewards_train/chosen': '-0.48124', 'rewards_train/rejected': '-0.61572', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.13448', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-155.2', 'loss/train': '0.70914', 'examples_per_second': '124.4', 'grad_norm': '26.026', 'counters/examples': 18816, 'counters/updates': 294}
skipping logging after 18880 examples to avoid logging too frequently
train stats after 18944 examples: {'rewards_train/chosen': '-0.52354', 'rewards_train/rejected': '-0.73342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20988', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-146.21', 'loss/train': '0.67065', 'examples_per_second': '122.79', 'grad_norm': '26.289', 'counters/examples': 18944, 'counters/updates': 296}
skipping logging after 19008 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '-0.40862', 'rewards_train/rejected': '-0.63575', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.22712', 'logps_train/rejected': '-148.53', 'logps_train/chosen': '-156.81', 'loss/train': '0.66498', 'examples_per_second': '118.41', 'grad_norm': '23.758', 'counters/examples': 19072, 'counters/updates': 298}
skipping logging after 19136 examples to avoid logging too frequently
train stats after 19200 examples: {'rewards_train/chosen': '-0.35929', 'rewards_train/rejected': '-0.70891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34962', 'logps_train/rejected': '-148.14', 'logps_train/chosen': '-173.57', 'loss/train': '0.61119', 'examples_per_second': '118.49', 'grad_norm': '22.898', 'counters/examples': 19200, 'counters/updates': 300}
skipping logging after 19264 examples to avoid logging too frequently
train stats after 19328 examples: {'rewards_train/chosen': '-0.38513', 'rewards_train/rejected': '-0.50537', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.12024', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-167.56', 'loss/train': '0.69984', 'examples_per_second': '124.38', 'grad_norm': '26.152', 'counters/examples': 19328, 'counters/updates': 302}
skipping logging after 19392 examples to avoid logging too frequently
train stats after 19456 examples: {'rewards_train/chosen': '-0.19237', 'rewards_train/rejected': '-0.51138', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31901', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-178.36', 'loss/train': '0.60678', 'examples_per_second': '119.97', 'grad_norm': '23.048', 'counters/examples': 19456, 'counters/updates': 304}
skipping logging after 19520 examples to avoid logging too frequently
train stats after 19584 examples: {'rewards_train/chosen': '-0.34374', 'rewards_train/rejected': '-0.64756', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.30382', 'logps_train/rejected': '-165.04', 'logps_train/chosen': '-167.58', 'loss/train': '0.59701', 'examples_per_second': '125.68', 'grad_norm': '22.697', 'counters/examples': 19584, 'counters/updates': 306}
skipping logging after 19648 examples to avoid logging too frequently
train stats after 19712 examples: {'rewards_train/chosen': '-0.29127', 'rewards_train/rejected': '-0.65942', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36815', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-149.14', 'loss/train': '0.63824', 'examples_per_second': '123.21', 'grad_norm': '25.893', 'counters/examples': 19712, 'counters/updates': 308}
skipping logging after 19776 examples to avoid logging too frequently
train stats after 19840 examples: {'rewards_train/chosen': '-0.29508', 'rewards_train/rejected': '-0.5857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29062', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-160.84', 'loss/train': '0.63284', 'examples_per_second': '124.63', 'grad_norm': '23.528', 'counters/examples': 19840, 'counters/updates': 310}
skipping logging after 19904 examples to avoid logging too frequently
train stats after 19968 examples: {'rewards_train/chosen': '-0.16746', 'rewards_train/rejected': '-0.60417', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43671', 'logps_train/rejected': '-119.98', 'logps_train/chosen': '-171.26', 'loss/train': '0.55589', 'examples_per_second': '127.9', 'grad_norm': '22.044', 'counters/examples': 19968, 'counters/updates': 312}
skipping logging after 20032 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '-0.30692', 'rewards_train/rejected': '-0.6275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32058', 'logps_train/rejected': '-150.04', 'logps_train/chosen': '-165.69', 'loss/train': '0.62654', 'examples_per_second': '145.91', 'grad_norm': '24.309', 'counters/examples': 20096, 'counters/updates': 314}
skipping logging after 20160 examples to avoid logging too frequently
train stats after 20224 examples: {'rewards_train/chosen': '-0.18381', 'rewards_train/rejected': '-0.68496', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50116', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-175.2', 'loss/train': '0.56418', 'examples_per_second': '124.46', 'grad_norm': '23.325', 'counters/examples': 20224, 'counters/updates': 316}
skipping logging after 20288 examples to avoid logging too frequently
train stats after 20352 examples: {'rewards_train/chosen': '-0.28138', 'rewards_train/rejected': '-0.76426', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48289', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-147.06', 'loss/train': '0.55153', 'examples_per_second': '123.52', 'grad_norm': '20.034', 'counters/examples': 20352, 'counters/updates': 318}
skipping logging after 20416 examples to avoid logging too frequently
train stats after 20480 examples: {'rewards_train/chosen': '-0.34535', 'rewards_train/rejected': '-0.61863', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.27328', 'logps_train/rejected': '-179.71', 'logps_train/chosen': '-156.64', 'loss/train': '0.64411', 'examples_per_second': '124.54', 'grad_norm': '25.119', 'counters/examples': 20480, 'counters/updates': 320}
skipping logging after 20544 examples to avoid logging too frequently
train stats after 20608 examples: {'rewards_train/chosen': '-0.11072', 'rewards_train/rejected': '-0.54638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.43566', 'logps_train/rejected': '-149.2', 'logps_train/chosen': '-190.61', 'loss/train': '0.57359', 'examples_per_second': '122.6', 'grad_norm': '25.164', 'counters/examples': 20608, 'counters/updates': 322}
skipping logging after 20672 examples to avoid logging too frequently
train stats after 20736 examples: {'rewards_train/chosen': '-0.25776', 'rewards_train/rejected': '-0.54807', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29032', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-186.08', 'loss/train': '0.63406', 'examples_per_second': '124.52', 'grad_norm': '25.332', 'counters/examples': 20736, 'counters/updates': 324}
skipping logging after 20800 examples to avoid logging too frequently
train stats after 20864 examples: {'rewards_train/chosen': '-0.37576', 'rewards_train/rejected': '-0.69491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.31914', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-154.2', 'loss/train': '0.60421', 'examples_per_second': '118.53', 'grad_norm': '20.734', 'counters/examples': 20864, 'counters/updates': 326}
skipping logging after 20928 examples to avoid logging too frequently
train stats after 20992 examples: {'rewards_train/chosen': '-0.2771', 'rewards_train/rejected': '-0.69955', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42246', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-163.38', 'loss/train': '0.56949', 'examples_per_second': '124.09', 'grad_norm': '22.182', 'counters/examples': 20992, 'counters/updates': 328}
skipping logging after 21056 examples to avoid logging too frequently
train stats after 21120 examples: {'rewards_train/chosen': '-0.56609', 'rewards_train/rejected': '-0.84038', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.27429', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-143.25', 'loss/train': '0.6574', 'examples_per_second': '124.56', 'grad_norm': '23.627', 'counters/examples': 21120, 'counters/updates': 330}
skipping logging after 21184 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '-0.22988', 'rewards_train/rejected': '-0.46336', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23348', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-129.84', 'loss/train': '0.63367', 'examples_per_second': '128.43', 'grad_norm': '22.458', 'counters/examples': 21248, 'counters/updates': 332}
skipping logging after 21312 examples to avoid logging too frequently
train stats after 21376 examples: {'rewards_train/chosen': '-0.19274', 'rewards_train/rejected': '-0.60715', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41441', 'logps_train/rejected': '-160.96', 'logps_train/chosen': '-145.31', 'loss/train': '0.56441', 'examples_per_second': '126.34', 'grad_norm': '20.229', 'counters/examples': 21376, 'counters/updates': 334}
skipping logging after 21440 examples to avoid logging too frequently
train stats after 21504 examples: {'rewards_train/chosen': '-0.055139', 'rewards_train/rejected': '-0.59721', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54207', 'logps_train/rejected': '-134.16', 'logps_train/chosen': '-162.73', 'loss/train': '0.54091', 'examples_per_second': '124.16', 'grad_norm': '23.849', 'counters/examples': 21504, 'counters/updates': 336}
skipping logging after 21568 examples to avoid logging too frequently
train stats after 21632 examples: {'rewards_train/chosen': '-0.14868', 'rewards_train/rejected': '-0.44511', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.29643', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-140.54', 'loss/train': '0.60047', 'examples_per_second': '124.48', 'grad_norm': '23.379', 'counters/examples': 21632, 'counters/updates': 338}
skipping logging after 21696 examples to avoid logging too frequently
train stats after 21760 examples: {'rewards_train/chosen': '-0.23621', 'rewards_train/rejected': '-0.56565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32944', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-177.9', 'loss/train': '0.63593', 'examples_per_second': '125.98', 'grad_norm': '25.274', 'counters/examples': 21760, 'counters/updates': 340}
skipping logging after 21824 examples to avoid logging too frequently
train stats after 21888 examples: {'rewards_train/chosen': '-0.28376', 'rewards_train/rejected': '-0.53416', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.2504', 'logps_train/rejected': '-150.19', 'logps_train/chosen': '-138.68', 'loss/train': '0.63593', 'examples_per_second': '127.14', 'grad_norm': '23.21', 'counters/examples': 21888, 'counters/updates': 342}
skipping logging after 21952 examples to avoid logging too frequently
train stats after 22016 examples: {'rewards_train/chosen': '-0.326', 'rewards_train/rejected': '-0.6721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3461', 'logps_train/rejected': '-162.04', 'logps_train/chosen': '-142.66', 'loss/train': '0.61063', 'examples_per_second': '123.85', 'grad_norm': '23.128', 'counters/examples': 22016, 'counters/updates': 344}
skipping logging after 22080 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.16907', 'rewards_train/rejected': '-0.52324', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.35417', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-133.02', 'loss/train': '0.59816', 'examples_per_second': '120.44', 'grad_norm': '20.659', 'counters/examples': 22144, 'counters/updates': 346}
skipping logging after 22208 examples to avoid logging too frequently
train stats after 22272 examples: {'rewards_train/chosen': '-0.10342', 'rewards_train/rejected': '-0.47687', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.37345', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-136.68', 'loss/train': '0.56691', 'examples_per_second': '112.65', 'grad_norm': '20.793', 'counters/examples': 22272, 'counters/updates': 348}
skipping logging after 22336 examples to avoid logging too frequently
train stats after 22400 examples: {'rewards_train/chosen': '-0.2236', 'rewards_train/rejected': '-0.64274', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41914', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-163.14', 'loss/train': '0.57916', 'examples_per_second': '124.57', 'grad_norm': '21.41', 'counters/examples': 22400, 'counters/updates': 350}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22528 examples: {'rewards_train/chosen': '-0.33635', 'rewards_train/rejected': '-0.62626', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.28991', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-165.18', 'loss/train': '0.61926', 'examples_per_second': '124.43', 'grad_norm': '21.844', 'counters/examples': 22528, 'counters/updates': 352}
skipping logging after 22592 examples to avoid logging too frequently
train stats after 22656 examples: {'rewards_train/chosen': '-0.30152', 'rewards_train/rejected': '-0.58005', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.27853', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-157.22', 'loss/train': '0.64456', 'examples_per_second': '124.52', 'grad_norm': '24.829', 'counters/examples': 22656, 'counters/updates': 354}
skipping logging after 22720 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '-0.32971', 'rewards_train/rejected': '-0.74463', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.41492', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-148.28', 'loss/train': '0.56339', 'examples_per_second': '131.34', 'grad_norm': '21.237', 'counters/examples': 22784, 'counters/updates': 356}
skipping logging after 22848 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.23375', 'rewards_train/rejected': '-0.62352', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38977', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-154.95', 'loss/train': '0.61295', 'examples_per_second': '144.44', 'grad_norm': '24.336', 'counters/examples': 22912, 'counters/updates': 358}
skipping logging after 22976 examples to avoid logging too frequently
train stats after 23040 examples: {'rewards_train/chosen': '-0.17165', 'rewards_train/rejected': '-0.55327', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.38161', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-134.08', 'loss/train': '0.5752', 'examples_per_second': '119.02', 'grad_norm': '20.877', 'counters/examples': 23040, 'counters/updates': 360}
skipping logging after 23104 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '-0.21433', 'rewards_train/rejected': '-0.62253', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4082', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-169.85', 'loss/train': '0.58705', 'examples_per_second': '124.38', 'grad_norm': '24.823', 'counters/examples': 23168, 'counters/updates': 362}
skipping logging after 23232 examples to avoid logging too frequently
train stats after 23296 examples: {'rewards_train/chosen': '-0.30787', 'rewards_train/rejected': '-0.61856', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3107', 'logps_train/rejected': '-156.97', 'logps_train/chosen': '-167.38', 'loss/train': '0.64205', 'examples_per_second': '124.51', 'grad_norm': '24.762', 'counters/examples': 23296, 'counters/updates': 364}
skipping logging after 23360 examples to avoid logging too frequently
train stats after 23424 examples: {'rewards_train/chosen': '-0.26561', 'rewards_train/rejected': '-0.49304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22743', 'logps_train/rejected': '-130.95', 'logps_train/chosen': '-141.52', 'loss/train': '0.63203', 'examples_per_second': '124.42', 'grad_norm': '22.258', 'counters/examples': 23424, 'counters/updates': 366}
skipping logging after 23488 examples to avoid logging too frequently
train stats after 23552 examples: {'rewards_train/chosen': '-0.22775', 'rewards_train/rejected': '-0.78327', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.55552', 'logps_train/rejected': '-146.02', 'logps_train/chosen': '-138.62', 'loss/train': '0.57361', 'examples_per_second': '124.32', 'grad_norm': '22.005', 'counters/examples': 23552, 'counters/updates': 368}
skipping logging after 23616 examples to avoid logging too frequently
train stats after 23680 examples: {'rewards_train/chosen': '-0.43714', 'rewards_train/rejected': '-0.7451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.30796', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-142.66', 'loss/train': '0.63031', 'examples_per_second': '118.82', 'grad_norm': '24.361', 'counters/examples': 23680, 'counters/updates': 370}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23808 examples: {'rewards_train/chosen': '-0.1963', 'rewards_train/rejected': '-0.51332', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31702', 'logps_train/rejected': '-152.46', 'logps_train/chosen': '-149.88', 'loss/train': '0.62254', 'examples_per_second': '121.72', 'grad_norm': '24.135', 'counters/examples': 23808, 'counters/updates': 372}
skipping logging after 23872 examples to avoid logging too frequently
train stats after 23936 examples: {'rewards_train/chosen': '-0.41909', 'rewards_train/rejected': '-0.63253', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21343', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-141.49', 'loss/train': '0.65273', 'examples_per_second': '122.51', 'grad_norm': '22.735', 'counters/examples': 23936, 'counters/updates': 374}
Running evaluation after 23936 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.38it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.41it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.48it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.45it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.44it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.42it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.35it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.35it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.39it/s]
eval after 23936: {'rewards_eval/chosen': '-0.37351', 'rewards_eval/rejected': '-0.70029', 'rewards_eval/accuracies': '0.62891', 'rewards_eval/margins': '0.32678', 'logps_eval/rejected': '-133.08', 'logps_eval/chosen': '-151.64', 'loss/eval': '0.64885'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-23936...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-23936/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-23936/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-23936/scheduler.pt...
train stats after 24000 examples: {'rewards_train/chosen': '-0.37599', 'rewards_train/rejected': '-0.59945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22347', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-146.34', 'loss/train': '0.66013', 'examples_per_second': '117.13', 'grad_norm': '24.509', 'counters/examples': 24000, 'counters/updates': 375}
skipping logging after 24064 examples to avoid logging too frequently
train stats after 24128 examples: {'rewards_train/chosen': '-0.41182', 'rewards_train/rejected': '-0.57928', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16746', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-155.75', 'loss/train': '0.72412', 'examples_per_second': '124.81', 'grad_norm': '26.043', 'counters/examples': 24128, 'counters/updates': 377}
skipping logging after 24192 examples to avoid logging too frequently
train stats after 24256 examples: {'rewards_train/chosen': '-0.50275', 'rewards_train/rejected': '-0.78153', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.27878', 'logps_train/rejected': '-131.06', 'logps_train/chosen': '-185.35', 'loss/train': '0.62902', 'examples_per_second': '124.96', 'grad_norm': '26.596', 'counters/examples': 24256, 'counters/updates': 379}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24384 examples: {'rewards_train/chosen': '-0.51559', 'rewards_train/rejected': '-0.99614', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48055', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-156.48', 'loss/train': '0.59832', 'examples_per_second': '121.85', 'grad_norm': '21.992', 'counters/examples': 24384, 'counters/updates': 381}
skipping logging after 24448 examples to avoid logging too frequently
train stats after 24512 examples: {'rewards_train/chosen': '-0.24241', 'rewards_train/rejected': '-0.72317', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48076', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-150.56', 'loss/train': '0.55362', 'examples_per_second': '119.27', 'grad_norm': '19.847', 'counters/examples': 24512, 'counters/updates': 383}
skipping logging after 24576 examples to avoid logging too frequently
train stats after 24640 examples: {'rewards_train/chosen': '-0.30593', 'rewards_train/rejected': '-0.71819', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41226', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-157.23', 'loss/train': '0.58801', 'examples_per_second': '129.74', 'grad_norm': '22.837', 'counters/examples': 24640, 'counters/updates': 385}
skipping logging after 24704 examples to avoid logging too frequently
train stats after 24768 examples: {'rewards_train/chosen': '-0.35206', 'rewards_train/rejected': '-0.9114', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.55935', 'logps_train/rejected': '-157.54', 'logps_train/chosen': '-146.94', 'loss/train': '0.53938', 'examples_per_second': '128.29', 'grad_norm': '20.737', 'counters/examples': 24768, 'counters/updates': 387}
skipping logging after 24832 examples to avoid logging too frequently
train stats after 24896 examples: {'rewards_train/chosen': '-0.51243', 'rewards_train/rejected': '-0.80762', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.29519', 'logps_train/rejected': '-178.79', 'logps_train/chosen': '-171.44', 'loss/train': '0.68122', 'examples_per_second': '119.66', 'grad_norm': '25.396', 'counters/examples': 24896, 'counters/updates': 389}
skipping logging after 24960 examples to avoid logging too frequently
train stats after 25024 examples: {'rewards_train/chosen': '-0.34043', 'rewards_train/rejected': '-0.89252', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.55209', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-158.31', 'loss/train': '0.52822', 'examples_per_second': '121.98', 'grad_norm': '21.128', 'counters/examples': 25024, 'counters/updates': 391}
skipping logging after 25088 examples to avoid logging too frequently
train stats after 25152 examples: {'rewards_train/chosen': '-0.38077', 'rewards_train/rejected': '-0.77912', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.39835', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-140.57', 'loss/train': '0.56327', 'examples_per_second': '124.99', 'grad_norm': '21.464', 'counters/examples': 25152, 'counters/updates': 393}
skipping logging after 25216 examples to avoid logging too frequently
train stats after 25280 examples: {'rewards_train/chosen': '-0.31632', 'rewards_train/rejected': '-0.70327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38695', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-132.91', 'loss/train': '0.60814', 'examples_per_second': '124.77', 'grad_norm': '21.039', 'counters/examples': 25280, 'counters/updates': 395}
skipping logging after 25344 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '-0.51806', 'rewards_train/rejected': '-0.75141', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.23335', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-141.72', 'loss/train': '0.67224', 'examples_per_second': '125.15', 'grad_norm': '24.95', 'counters/examples': 25408, 'counters/updates': 397}
skipping logging after 25472 examples to avoid logging too frequently
train stats after 25536 examples: {'rewards_train/chosen': '-0.52186', 'rewards_train/rejected': '-0.83828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31642', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-146.04', 'loss/train': '0.64654', 'examples_per_second': '125.03', 'grad_norm': '25.519', 'counters/examples': 25536, 'counters/updates': 399}
skipping logging after 25600 examples to avoid logging too frequently
train stats after 25664 examples: {'rewards_train/chosen': '-0.24047', 'rewards_train/rejected': '-0.85219', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.61171', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-141.15', 'loss/train': '0.51964', 'examples_per_second': '121.47', 'grad_norm': '20.556', 'counters/examples': 25664, 'counters/updates': 401}
skipping logging after 25728 examples to avoid logging too frequently
train stats after 25792 examples: {'rewards_train/chosen': '-0.40479', 'rewards_train/rejected': '-0.96702', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.56223', 'logps_train/rejected': '-145.93', 'logps_train/chosen': '-175.78', 'loss/train': '0.56482', 'examples_per_second': '121.68', 'grad_norm': '25.543', 'counters/examples': 25792, 'counters/updates': 403}
skipping logging after 25856 examples to avoid logging too frequently
train stats after 25920 examples: {'rewards_train/chosen': '-0.49423', 'rewards_train/rejected': '-0.82625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.33202', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-173.64', 'loss/train': '0.62893', 'examples_per_second': '124.99', 'grad_norm': '24.383', 'counters/examples': 25920, 'counters/updates': 405}
skipping logging after 25984 examples to avoid logging too frequently
train stats after 26048 examples: {'rewards_train/chosen': '-0.49314', 'rewards_train/rejected': '-0.90792', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41478', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-178.06', 'loss/train': '0.59823', 'examples_per_second': '120.19', 'grad_norm': '24.14', 'counters/examples': 26048, 'counters/updates': 407}
skipping logging after 26112 examples to avoid logging too frequently
train stats after 26176 examples: {'rewards_train/chosen': '-0.57252', 'rewards_train/rejected': '-0.95572', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.3832', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-137.66', 'loss/train': '0.63415', 'examples_per_second': '120.03', 'grad_norm': '23.052', 'counters/examples': 26176, 'counters/updates': 409}
skipping logging after 26240 examples to avoid logging too frequently
train stats after 26304 examples: {'rewards_train/chosen': '-0.3576', 'rewards_train/rejected': '-0.74744', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.38984', 'logps_train/rejected': '-123.98', 'logps_train/chosen': '-164.16', 'loss/train': '0.66971', 'examples_per_second': '130.08', 'grad_norm': '25.28', 'counters/examples': 26304, 'counters/updates': 411}
skipping logging after 26368 examples to avoid logging too frequently
train stats after 26432 examples: {'rewards_train/chosen': '-0.25965', 'rewards_train/rejected': '-0.70862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44897', 'logps_train/rejected': '-178.71', 'logps_train/chosen': '-162.55', 'loss/train': '0.57263', 'examples_per_second': '120.05', 'grad_norm': '23.029', 'counters/examples': 26432, 'counters/updates': 413}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26560 examples: {'rewards_train/chosen': '-0.42556', 'rewards_train/rejected': '-0.53699', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.11143', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-133.66', 'loss/train': '0.70255', 'examples_per_second': '129.34', 'grad_norm': '22.284', 'counters/examples': 26560, 'counters/updates': 415}
skipping logging after 26624 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '-0.079813', 'rewards_train/rejected': '-0.55822', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47841', 'logps_train/rejected': '-163.49', 'logps_train/chosen': '-180.95', 'loss/train': '0.5522', 'examples_per_second': '124.61', 'grad_norm': '23.448', 'counters/examples': 26688, 'counters/updates': 417}
skipping logging after 26752 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.30004', 'rewards_train/rejected': '-0.80167', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.50163', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-168.5', 'loss/train': '0.55884', 'examples_per_second': '124.97', 'grad_norm': '21.843', 'counters/examples': 26816, 'counters/updates': 419}
skipping logging after 26880 examples to avoid logging too frequently
train stats after 26944 examples: {'rewards_train/chosen': '-0.48646', 'rewards_train/rejected': '-0.87052', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38406', 'logps_train/rejected': '-181.66', 'logps_train/chosen': '-163.62', 'loss/train': '0.66212', 'examples_per_second': '129.51', 'grad_norm': '27.32', 'counters/examples': 26944, 'counters/updates': 421}
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27072 examples: {'rewards_train/chosen': '-0.47955', 'rewards_train/rejected': '-0.76477', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.28522', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-152.85', 'loss/train': '0.6311', 'examples_per_second': '122.46', 'grad_norm': '22.627', 'counters/examples': 27072, 'counters/updates': 423}
skipping logging after 27136 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '-0.51393', 'rewards_train/rejected': '-0.85591', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.34199', 'logps_train/rejected': '-132.62', 'logps_train/chosen': '-130.46', 'loss/train': '0.61683', 'examples_per_second': '118.94', 'grad_norm': '22.428', 'counters/examples': 27200, 'counters/updates': 425}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27328 examples: {'rewards_train/chosen': '-0.26158', 'rewards_train/rejected': '-0.62562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36404', 'logps_train/rejected': '-165.77', 'logps_train/chosen': '-137.35', 'loss/train': '0.58913', 'examples_per_second': '125.1', 'grad_norm': '23.519', 'counters/examples': 27328, 'counters/updates': 427}
skipping logging after 27392 examples to avoid logging too frequently
train stats after 27456 examples: {'rewards_train/chosen': '-0.32728', 'rewards_train/rejected': '-0.43176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10448', 'logps_train/rejected': '-165.8', 'logps_train/chosen': '-192.07', 'loss/train': '0.70914', 'examples_per_second': '125.1', 'grad_norm': '30.275', 'counters/examples': 27456, 'counters/updates': 429}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27584 examples: {'rewards_train/chosen': '-0.2997', 'rewards_train/rejected': '-0.66737', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.36767', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-152.46', 'loss/train': '0.60562', 'examples_per_second': '120.99', 'grad_norm': '22.67', 'counters/examples': 27584, 'counters/updates': 431}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27712 examples: {'rewards_train/chosen': '-0.38412', 'rewards_train/rejected': '-0.727', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34288', 'logps_train/rejected': '-137.93', 'logps_train/chosen': '-145.5', 'loss/train': '0.60191', 'examples_per_second': '120.67', 'grad_norm': '23.255', 'counters/examples': 27712, 'counters/updates': 433}
skipping logging after 27776 examples to avoid logging too frequently
train stats after 27840 examples: {'rewards_train/chosen': '-0.515', 'rewards_train/rejected': '-0.91189', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.39689', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-144.2', 'loss/train': '0.60093', 'examples_per_second': '125.05', 'grad_norm': '23.248', 'counters/examples': 27840, 'counters/updates': 435}
skipping logging after 27904 examples to avoid logging too frequently
train stats after 27968 examples: {'rewards_train/chosen': '-0.4073', 'rewards_train/rejected': '-0.59882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19152', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-135.65', 'loss/train': '0.6715', 'examples_per_second': '119.33', 'grad_norm': '23.302', 'counters/examples': 27968, 'counters/updates': 437}
skipping logging after 28032 examples to avoid logging too frequently
train stats after 28096 examples: {'rewards_train/chosen': '-0.39836', 'rewards_train/rejected': '-0.74425', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3459', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-177.14', 'loss/train': '0.61003', 'examples_per_second': '124.49', 'grad_norm': '22.971', 'counters/examples': 28096, 'counters/updates': 439}
skipping logging after 28160 examples to avoid logging too frequently
train stats after 28224 examples: {'rewards_train/chosen': '-0.36398', 'rewards_train/rejected': '-0.73394', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36995', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-151.29', 'loss/train': '0.60074', 'examples_per_second': '118.97', 'grad_norm': '22.251', 'counters/examples': 28224, 'counters/updates': 441}
skipping logging after 28288 examples to avoid logging too frequently
train stats after 28352 examples: {'rewards_train/chosen': '-0.34146', 'rewards_train/rejected': '-0.58408', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.24263', 'logps_train/rejected': '-148.71', 'logps_train/chosen': '-139.85', 'loss/train': '0.63667', 'examples_per_second': '132.86', 'grad_norm': '23.896', 'counters/examples': 28352, 'counters/updates': 443}
skipping logging after 28416 examples to avoid logging too frequently
train stats after 28480 examples: {'rewards_train/chosen': '-0.34015', 'rewards_train/rejected': '-0.56721', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.22707', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-155.51', 'loss/train': '0.6521', 'examples_per_second': '125.06', 'grad_norm': '24.427', 'counters/examples': 28480, 'counters/updates': 445}
skipping logging after 28544 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.43113', 'rewards_train/rejected': '-0.79793', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.36679', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-174.08', 'loss/train': '0.64082', 'examples_per_second': '132.26', 'grad_norm': '24.717', 'counters/examples': 28608, 'counters/updates': 447}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28736 examples: {'rewards_train/chosen': '-0.41904', 'rewards_train/rejected': '-0.87575', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.45672', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-168.73', 'loss/train': '0.57529', 'examples_per_second': '124.7', 'grad_norm': '21.93', 'counters/examples': 28736, 'counters/updates': 449}
skipping logging after 28800 examples to avoid logging too frequently
train stats after 28864 examples: {'rewards_train/chosen': '-0.35693', 'rewards_train/rejected': '-0.92833', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5714', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-131.33', 'loss/train': '0.56464', 'examples_per_second': '123.34', 'grad_norm': '20.661', 'counters/examples': 28864, 'counters/updates': 451}
skipping logging after 28928 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.36336', 'rewards_train/rejected': '-0.93132', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56797', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-131.81', 'loss/train': '0.54845', 'examples_per_second': '129.1', 'grad_norm': '21.05', 'counters/examples': 28992, 'counters/updates': 453}
skipping logging after 29056 examples to avoid logging too frequently
train stats after 29120 examples: {'rewards_train/chosen': '-0.52363', 'rewards_train/rejected': '-0.90383', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38021', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-170.12', 'loss/train': '0.59489', 'examples_per_second': '119.33', 'grad_norm': '22.615', 'counters/examples': 29120, 'counters/updates': 455}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29248 examples: {'rewards_train/chosen': '-0.56078', 'rewards_train/rejected': '-1.1122', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55137', 'logps_train/rejected': '-140.61', 'logps_train/chosen': '-164.67', 'loss/train': '0.56915', 'examples_per_second': '119.75', 'grad_norm': '23.15', 'counters/examples': 29248, 'counters/updates': 457}
skipping logging after 29312 examples to avoid logging too frequently
train stats after 29376 examples: {'rewards_train/chosen': '-0.87575', 'rewards_train/rejected': '-1.1832', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.30747', 'logps_train/rejected': '-134.54', 'logps_train/chosen': '-170.9', 'loss/train': '0.62514', 'examples_per_second': '119.56', 'grad_norm': '25.658', 'counters/examples': 29376, 'counters/updates': 459}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29504 examples: {'rewards_train/chosen': '-0.56387', 'rewards_train/rejected': '-1.0418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.47796', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-135.09', 'loss/train': '0.61827', 'examples_per_second': '124.07', 'grad_norm': '23.103', 'counters/examples': 29504, 'counters/updates': 461}
skipping logging after 29568 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '-0.34731', 'rewards_train/rejected': '-0.85072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50341', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-139.14', 'loss/train': '0.57014', 'examples_per_second': '124.99', 'grad_norm': '21.66', 'counters/examples': 29632, 'counters/updates': 463}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29760 examples: {'rewards_train/chosen': '-0.377', 'rewards_train/rejected': '-0.74791', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3709', 'logps_train/rejected': '-161.1', 'logps_train/chosen': '-149.48', 'loss/train': '0.63479', 'examples_per_second': '124.04', 'grad_norm': '24.061', 'counters/examples': 29760, 'counters/updates': 465}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29888 examples: {'rewards_train/chosen': '-0.48907', 'rewards_train/rejected': '-0.79495', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.30587', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-137.01', 'loss/train': '0.61875', 'examples_per_second': '140.22', 'grad_norm': '22.024', 'counters/examples': 29888, 'counters/updates': 467}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 30016 examples: {'rewards_train/chosen': '-0.43792', 'rewards_train/rejected': '-0.76315', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32524', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-137.41', 'loss/train': '0.61341', 'examples_per_second': '138.11', 'grad_norm': '21.396', 'counters/examples': 30016, 'counters/updates': 469}
skipping logging after 30080 examples to avoid logging too frequently
train stats after 30144 examples: {'rewards_train/chosen': '-0.61035', 'rewards_train/rejected': '-0.92218', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31183', 'logps_train/rejected': '-143.01', 'logps_train/chosen': '-144.67', 'loss/train': '0.62645', 'examples_per_second': '122.7', 'grad_norm': '22.247', 'counters/examples': 30144, 'counters/updates': 471}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30272 examples: {'rewards_train/chosen': '-0.49959', 'rewards_train/rejected': '-0.81095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.31136', 'logps_train/rejected': '-153.9', 'logps_train/chosen': '-158.58', 'loss/train': '0.65456', 'examples_per_second': '124.87', 'grad_norm': '26.039', 'counters/examples': 30272, 'counters/updates': 473}
skipping logging after 30336 examples to avoid logging too frequently
train stats after 30400 examples: {'rewards_train/chosen': '-0.27085', 'rewards_train/rejected': '-0.62827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35741', 'logps_train/rejected': '-154.82', 'logps_train/chosen': '-151.82', 'loss/train': '0.63113', 'examples_per_second': '124.7', 'grad_norm': '24.16', 'counters/examples': 30400, 'counters/updates': 475}
skipping logging after 30464 examples to avoid logging too frequently
train stats after 30528 examples: {'rewards_train/chosen': '-0.30484', 'rewards_train/rejected': '-0.67761', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37276', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-139.24', 'loss/train': '0.60309', 'examples_per_second': '124.83', 'grad_norm': '22.703', 'counters/examples': 30528, 'counters/updates': 477}
skipping logging after 30592 examples to avoid logging too frequently
train stats after 30656 examples: {'rewards_train/chosen': '-0.26098', 'rewards_train/rejected': '-0.75352', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49254', 'logps_train/rejected': '-153.93', 'logps_train/chosen': '-149.52', 'loss/train': '0.60475', 'examples_per_second': '119.89', 'grad_norm': '23.195', 'counters/examples': 30656, 'counters/updates': 479}
skipping logging after 30720 examples to avoid logging too frequently
train stats after 30784 examples: {'rewards_train/chosen': '-0.38995', 'rewards_train/rejected': '-0.93615', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.5462', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-136.71', 'loss/train': '0.54405', 'examples_per_second': '124.78', 'grad_norm': '18.526', 'counters/examples': 30784, 'counters/updates': 481}
skipping logging after 30848 examples to avoid logging too frequently
train stats after 30912 examples: {'rewards_train/chosen': '-0.38151', 'rewards_train/rejected': '-0.8421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.4606', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-165.44', 'loss/train': '0.58388', 'examples_per_second': '124.99', 'grad_norm': '24.855', 'counters/examples': 30912, 'counters/updates': 483}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31040 examples: {'rewards_train/chosen': '-0.4554', 'rewards_train/rejected': '-0.68595', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.23055', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-136.01', 'loss/train': '0.66293', 'examples_per_second': '124.88', 'grad_norm': '22.706', 'counters/examples': 31040, 'counters/updates': 485}
skipping logging after 31104 examples to avoid logging too frequently
train stats after 31168 examples: {'rewards_train/chosen': '-0.20661', 'rewards_train/rejected': '-0.58273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37611', 'logps_train/rejected': '-157.99', 'logps_train/chosen': '-155.23', 'loss/train': '0.59984', 'examples_per_second': '125.08', 'grad_norm': '24.281', 'counters/examples': 31168, 'counters/updates': 487}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31296 examples: {'rewards_train/chosen': '-0.26958', 'rewards_train/rejected': '-0.75797', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.48839', 'logps_train/rejected': '-133.89', 'logps_train/chosen': '-158.49', 'loss/train': '0.5474', 'examples_per_second': '122', 'grad_norm': '20.946', 'counters/examples': 31296, 'counters/updates': 489}
skipping logging after 31360 examples to avoid logging too frequently
train stats after 31424 examples: {'rewards_train/chosen': '-0.4111', 'rewards_train/rejected': '-0.9727', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.5616', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-161.3', 'loss/train': '0.55046', 'examples_per_second': '109.11', 'grad_norm': '22.98', 'counters/examples': 31424, 'counters/updates': 491}
skipping logging after 31488 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '-0.43865', 'rewards_train/rejected': '-0.85209', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.41344', 'logps_train/rejected': '-158.57', 'logps_train/chosen': '-176.96', 'loss/train': '0.62434', 'examples_per_second': '124.92', 'grad_norm': '24.987', 'counters/examples': 31552, 'counters/updates': 493}
skipping logging after 31616 examples to avoid logging too frequently
train stats after 31680 examples: {'rewards_train/chosen': '-0.38357', 'rewards_train/rejected': '-0.6492', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.26563', 'logps_train/rejected': '-141.55', 'logps_train/chosen': '-164.06', 'loss/train': '0.66083', 'examples_per_second': '125.21', 'grad_norm': '24.603', 'counters/examples': 31680, 'counters/updates': 495}
skipping logging after 31744 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '-0.36479', 'rewards_train/rejected': '-0.5611', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.19631', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-165.88', 'loss/train': '0.67496', 'examples_per_second': '124.74', 'grad_norm': '26.024', 'counters/examples': 31808, 'counters/updates': 497}
skipping logging after 31872 examples to avoid logging too frequently
train stats after 31936 examples: {'rewards_train/chosen': '-0.50542', 'rewards_train/rejected': '-0.87187', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36645', 'logps_train/rejected': '-155.26', 'logps_train/chosen': '-146.02', 'loss/train': '0.61969', 'examples_per_second': '119.28', 'grad_norm': '24.81', 'counters/examples': 31936, 'counters/updates': 499}
skipping logging after 32000 examples to avoid logging too frequently
train stats after 32064 examples: {'rewards_train/chosen': '-0.34117', 'rewards_train/rejected': '-0.73905', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39788', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-160.59', 'loss/train': '0.59716', 'examples_per_second': '125', 'grad_norm': '23.273', 'counters/examples': 32064, 'counters/updates': 501}
skipping logging after 32128 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '-0.37564', 'rewards_train/rejected': '-1.0016', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62591', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-144.91', 'loss/train': '0.52272', 'examples_per_second': '119.94', 'grad_norm': '20.697', 'counters/examples': 32192, 'counters/updates': 503}
skipping logging after 32256 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.31178', 'rewards_train/rejected': '-0.84368', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.5319', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-151.34', 'loss/train': '0.5313', 'examples_per_second': '134.96', 'grad_norm': '20.512', 'counters/examples': 32320, 'counters/updates': 505}
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32448 examples: {'rewards_train/chosen': '-0.44945', 'rewards_train/rejected': '-0.88821', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.43876', 'logps_train/rejected': '-151.04', 'logps_train/chosen': '-162.85', 'loss/train': '0.56787', 'examples_per_second': '125.05', 'grad_norm': '22.098', 'counters/examples': 32448, 'counters/updates': 507}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32576 examples: {'rewards_train/chosen': '-0.45947', 'rewards_train/rejected': '-0.85699', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.39752', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-150.55', 'loss/train': '0.6076', 'examples_per_second': '125.27', 'grad_norm': '22.633', 'counters/examples': 32576, 'counters/updates': 509}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32704 examples: {'rewards_train/chosen': '-0.36081', 'rewards_train/rejected': '-0.55507', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.19426', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-177.12', 'loss/train': '0.66146', 'examples_per_second': '124.81', 'grad_norm': '26.319', 'counters/examples': 32704, 'counters/updates': 511}
skipping logging after 32768 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '-0.56346', 'rewards_train/rejected': '-1.066', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.50254', 'logps_train/rejected': '-140.86', 'logps_train/chosen': '-145.61', 'loss/train': '0.57491', 'examples_per_second': '125.19', 'grad_norm': '22.922', 'counters/examples': 32832, 'counters/updates': 513}
skipping logging after 32896 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.40627', 'rewards_train/rejected': '-0.67624', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26997', 'logps_train/rejected': '-155.63', 'logps_train/chosen': '-160.37', 'loss/train': '0.6356', 'examples_per_second': '119.28', 'grad_norm': '24.107', 'counters/examples': 32960, 'counters/updates': 515}
skipping logging after 33024 examples to avoid logging too frequently
train stats after 33088 examples: {'rewards_train/chosen': '-0.57648', 'rewards_train/rejected': '-0.90822', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33174', 'logps_train/rejected': '-176.3', 'logps_train/chosen': '-173.94', 'loss/train': '0.62235', 'examples_per_second': '125.04', 'grad_norm': '23.484', 'counters/examples': 33088, 'counters/updates': 517}
skipping logging after 33152 examples to avoid logging too frequently
train stats after 33216 examples: {'rewards_train/chosen': '-0.40098', 'rewards_train/rejected': '-0.78393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38295', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-147.47', 'loss/train': '0.61982', 'examples_per_second': '125.16', 'grad_norm': '23.006', 'counters/examples': 33216, 'counters/updates': 519}
skipping logging after 33280 examples to avoid logging too frequently
train stats after 33344 examples: {'rewards_train/chosen': '-0.31017', 'rewards_train/rejected': '-0.67245', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36228', 'logps_train/rejected': '-162.84', 'logps_train/chosen': '-165.29', 'loss/train': '0.60419', 'examples_per_second': '134.32', 'grad_norm': '26.157', 'counters/examples': 33344, 'counters/updates': 521}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33472 examples: {'rewards_train/chosen': '-0.30466', 'rewards_train/rejected': '-0.7217', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41704', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-164.98', 'loss/train': '0.60787', 'examples_per_second': '125.19', 'grad_norm': '24.923', 'counters/examples': 33472, 'counters/updates': 523}
skipping logging after 33536 examples to avoid logging too frequently
train stats after 33600 examples: {'rewards_train/chosen': '-0.38827', 'rewards_train/rejected': '-0.6308', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.24252', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-168.62', 'loss/train': '0.67447', 'examples_per_second': '125.11', 'grad_norm': '28.587', 'counters/examples': 33600, 'counters/updates': 525}
skipping logging after 33664 examples to avoid logging too frequently
train stats after 33728 examples: {'rewards_train/chosen': '-0.36012', 'rewards_train/rejected': '-0.65184', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.29172', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-154.38', 'loss/train': '0.61765', 'examples_per_second': '118.8', 'grad_norm': '23.375', 'counters/examples': 33728, 'counters/updates': 527}
skipping logging after 33792 examples to avoid logging too frequently
train stats after 33856 examples: {'rewards_train/chosen': '-0.43363', 'rewards_train/rejected': '-0.64549', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21186', 'logps_train/rejected': '-134.26', 'logps_train/chosen': '-152.53', 'loss/train': '0.65324', 'examples_per_second': '125.31', 'grad_norm': '24.294', 'counters/examples': 33856, 'counters/updates': 529}
skipping logging after 33920 examples to avoid logging too frequently
train stats after 33984 examples: {'rewards_train/chosen': '-0.45184', 'rewards_train/rejected': '-0.77958', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.32774', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-156.85', 'loss/train': '0.61423', 'examples_per_second': '124.22', 'grad_norm': '24.989', 'counters/examples': 33984, 'counters/updates': 531}
skipping logging after 34048 examples to avoid logging too frequently
train stats after 34112 examples: {'rewards_train/chosen': '-0.4335', 'rewards_train/rejected': '-0.86438', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43089', 'logps_train/rejected': '-115', 'logps_train/chosen': '-152.58', 'loss/train': '0.56904', 'examples_per_second': '119.67', 'grad_norm': '21.091', 'counters/examples': 34112, 'counters/updates': 533}
skipping logging after 34176 examples to avoid logging too frequently
train stats after 34240 examples: {'rewards_train/chosen': '-0.34015', 'rewards_train/rejected': '-1.0999', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.75972', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-149.04', 'loss/train': '0.4609', 'examples_per_second': '131.46', 'grad_norm': '17.178', 'counters/examples': 34240, 'counters/updates': 535}
skipping logging after 34304 examples to avoid logging too frequently
train stats after 34368 examples: {'rewards_train/chosen': '-0.75445', 'rewards_train/rejected': '-1.2579', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.5034', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-142.91', 'loss/train': '0.57111', 'examples_per_second': '124.86', 'grad_norm': '22.844', 'counters/examples': 34368, 'counters/updates': 537}
skipping logging after 34432 examples to avoid logging too frequently
train stats after 34496 examples: {'rewards_train/chosen': '-0.58586', 'rewards_train/rejected': '-0.99133', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.40548', 'logps_train/rejected': '-145.49', 'logps_train/chosen': '-146.34', 'loss/train': '0.57161', 'examples_per_second': '125.23', 'grad_norm': '21.154', 'counters/examples': 34496, 'counters/updates': 539}
skipping logging after 34560 examples to avoid logging too frequently
train stats after 34624 examples: {'rewards_train/chosen': '-0.39763', 'rewards_train/rejected': '-0.78396', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.38632', 'logps_train/rejected': '-140.07', 'logps_train/chosen': '-149.82', 'loss/train': '0.61362', 'examples_per_second': '122.17', 'grad_norm': '25.888', 'counters/examples': 34624, 'counters/updates': 541}
skipping logging after 34688 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '-0.3825', 'rewards_train/rejected': '-0.80842', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42592', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-138.54', 'loss/train': '0.59563', 'examples_per_second': '127.57', 'grad_norm': '21.213', 'counters/examples': 34752, 'counters/updates': 543}
skipping logging after 34816 examples to avoid logging too frequently
train stats after 34880 examples: {'rewards_train/chosen': '-0.37882', 'rewards_train/rejected': '-0.68099', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.30218', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-149.45', 'loss/train': '0.63753', 'examples_per_second': '124.95', 'grad_norm': '23.138', 'counters/examples': 34880, 'counters/updates': 545}
skipping logging after 34944 examples to avoid logging too frequently
train stats after 35008 examples: {'rewards_train/chosen': '-0.54918', 'rewards_train/rejected': '-0.79477', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.24559', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-154.85', 'loss/train': '0.66572', 'examples_per_second': '128.59', 'grad_norm': '23.978', 'counters/examples': 35008, 'counters/updates': 547}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35136 examples: {'rewards_train/chosen': '-0.42872', 'rewards_train/rejected': '-0.79682', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.3681', 'logps_train/rejected': '-131.6', 'logps_train/chosen': '-140.21', 'loss/train': '0.62961', 'examples_per_second': '125.78', 'grad_norm': '22.666', 'counters/examples': 35136, 'counters/updates': 549}
skipping logging after 35200 examples to avoid logging too frequently
train stats after 35264 examples: {'rewards_train/chosen': '-0.33965', 'rewards_train/rejected': '-0.82983', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49018', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-141.88', 'loss/train': '0.54473', 'examples_per_second': '125.11', 'grad_norm': '20.046', 'counters/examples': 35264, 'counters/updates': 551}
skipping logging after 35328 examples to avoid logging too frequently
train stats after 35392 examples: {'rewards_train/chosen': '-0.35049', 'rewards_train/rejected': '-0.72068', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.37019', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-167.57', 'loss/train': '0.60526', 'examples_per_second': '125.07', 'grad_norm': '25.516', 'counters/examples': 35392, 'counters/updates': 553}
skipping logging after 35456 examples to avoid logging too frequently
train stats after 35520 examples: {'rewards_train/chosen': '-0.3956', 'rewards_train/rejected': '-1.0581', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.66254', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-161.72', 'loss/train': '0.50394', 'examples_per_second': '124.92', 'grad_norm': '20.81', 'counters/examples': 35520, 'counters/updates': 555}
skipping logging after 35584 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '-0.34722', 'rewards_train/rejected': '-0.87396', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.52674', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-147.2', 'loss/train': '0.54351', 'examples_per_second': '124.88', 'grad_norm': '22.038', 'counters/examples': 35648, 'counters/updates': 557}
skipping logging after 35712 examples to avoid logging too frequently
train stats after 35776 examples: {'rewards_train/chosen': '-0.27965', 'rewards_train/rejected': '-0.67946', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.39981', 'logps_train/rejected': '-169.96', 'logps_train/chosen': '-156.72', 'loss/train': '0.59307', 'examples_per_second': '119.9', 'grad_norm': '25.704', 'counters/examples': 35776, 'counters/updates': 559}
skipping logging after 35840 examples to avoid logging too frequently
train stats after 35904 examples: {'rewards_train/chosen': '-0.36717', 'rewards_train/rejected': '-0.67077', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3036', 'logps_train/rejected': '-140.76', 'logps_train/chosen': '-155.06', 'loss/train': '0.65763', 'examples_per_second': '85.3', 'grad_norm': '25.37', 'counters/examples': 35904, 'counters/updates': 561}
Running evaluation after 35904 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.35it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.42it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.47it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.45it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.41it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.41it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.40it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.40it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.41it/s]
eval after 35904: {'rewards_eval/chosen': '-0.25245', 'rewards_eval/rejected': '-0.61077', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.35832', 'logps_eval/rejected': '-132.19', 'logps_eval/chosen': '-150.43', 'loss/eval': '0.63382'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-35904...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-35904/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-35904/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-35904/scheduler.pt...
train stats after 35968 examples: {'rewards_train/chosen': '-0.32876', 'rewards_train/rejected': '-0.69079', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36203', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-132.18', 'loss/train': '0.5936', 'examples_per_second': '112.77', 'grad_norm': '22.431', 'counters/examples': 35968, 'counters/updates': 562}
skipping logging after 36032 examples to avoid logging too frequently
train stats after 36096 examples: {'rewards_train/chosen': '-0.22924', 'rewards_train/rejected': '-0.57581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.34657', 'logps_train/rejected': '-160.48', 'logps_train/chosen': '-138.07', 'loss/train': '0.61112', 'examples_per_second': '124.92', 'grad_norm': '24.305', 'counters/examples': 36096, 'counters/updates': 564}
skipping logging after 36160 examples to avoid logging too frequently
train stats after 36224 examples: {'rewards_train/chosen': '-0.24783', 'rewards_train/rejected': '-0.5341', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28627', 'logps_train/rejected': '-156.04', 'logps_train/chosen': '-155.33', 'loss/train': '0.6274', 'examples_per_second': '125.27', 'grad_norm': '24.452', 'counters/examples': 36224, 'counters/updates': 566}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36352 examples: {'rewards_train/chosen': '-0.38996', 'rewards_train/rejected': '-0.91598', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52602', 'logps_train/rejected': '-140.32', 'logps_train/chosen': '-165.51', 'loss/train': '0.55449', 'examples_per_second': '130.48', 'grad_norm': '23.221', 'counters/examples': 36352, 'counters/updates': 568}
skipping logging after 36416 examples to avoid logging too frequently
train stats after 36480 examples: {'rewards_train/chosen': '-0.16188', 'rewards_train/rejected': '-0.58207', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4202', 'logps_train/rejected': '-131.73', 'logps_train/chosen': '-149.28', 'loss/train': '0.57871', 'examples_per_second': '130.8', 'grad_norm': '22.807', 'counters/examples': 36480, 'counters/updates': 570}
skipping logging after 36544 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '-0.39975', 'rewards_train/rejected': '-0.81509', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.41534', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-145.9', 'loss/train': '0.56521', 'examples_per_second': '121.72', 'grad_norm': '22.736', 'counters/examples': 36608, 'counters/updates': 572}
skipping logging after 36672 examples to avoid logging too frequently
train stats after 36736 examples: {'rewards_train/chosen': '-0.56961', 'rewards_train/rejected': '-0.88204', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31243', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-162.88', 'loss/train': '0.67194', 'examples_per_second': '124.74', 'grad_norm': '25.293', 'counters/examples': 36736, 'counters/updates': 574}
skipping logging after 36800 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '-0.43014', 'rewards_train/rejected': '-0.91384', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48371', 'logps_train/rejected': '-144.09', 'logps_train/chosen': '-169.18', 'loss/train': '0.58638', 'examples_per_second': '121.36', 'grad_norm': '23.118', 'counters/examples': 36864, 'counters/updates': 576}
skipping logging after 36928 examples to avoid logging too frequently
train stats after 36992 examples: {'rewards_train/chosen': '-0.41285', 'rewards_train/rejected': '-0.71514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30229', 'logps_train/rejected': '-156.69', 'logps_train/chosen': '-144.73', 'loss/train': '0.64405', 'examples_per_second': '127.11', 'grad_norm': '25.181', 'counters/examples': 36992, 'counters/updates': 578}
skipping logging after 37056 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '-0.45058', 'rewards_train/rejected': '-0.95629', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50572', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-168.89', 'loss/train': '0.58113', 'examples_per_second': '139.64', 'grad_norm': '24.665', 'counters/examples': 37120, 'counters/updates': 580}
skipping logging after 37184 examples to avoid logging too frequently
train stats after 37248 examples: {'rewards_train/chosen': '-0.50344', 'rewards_train/rejected': '-1.0827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57931', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-146.43', 'loss/train': '0.54145', 'examples_per_second': '125.93', 'grad_norm': '22.272', 'counters/examples': 37248, 'counters/updates': 582}
skipping logging after 37312 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.47265', 'rewards_train/rejected': '-1.1135', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.64082', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-138.81', 'loss/train': '0.52387', 'examples_per_second': '124.81', 'grad_norm': '20.733', 'counters/examples': 37376, 'counters/updates': 584}
skipping logging after 37440 examples to avoid logging too frequently
train stats after 37504 examples: {'rewards_train/chosen': '-0.64368', 'rewards_train/rejected': '-1.0124', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.36875', 'logps_train/rejected': '-141.99', 'logps_train/chosen': '-147.98', 'loss/train': '0.61071', 'examples_per_second': '121.08', 'grad_norm': '24.804', 'counters/examples': 37504, 'counters/updates': 586}
skipping logging after 37568 examples to avoid logging too frequently
train stats after 37632 examples: {'rewards_train/chosen': '-0.49733', 'rewards_train/rejected': '-0.83796', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34063', 'logps_train/rejected': '-130.78', 'logps_train/chosen': '-155', 'loss/train': '0.63252', 'examples_per_second': '119.38', 'grad_norm': '24.215', 'counters/examples': 37632, 'counters/updates': 588}
skipping logging after 37696 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '-0.55812', 'rewards_train/rejected': '-1.0355', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4774', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-164.89', 'loss/train': '0.56159', 'examples_per_second': '125.16', 'grad_norm': '22.859', 'counters/examples': 37760, 'counters/updates': 590}
skipping logging after 37824 examples to avoid logging too frequently
train stats after 37888 examples: {'rewards_train/chosen': '-0.45018', 'rewards_train/rejected': '-0.95597', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.50579', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-161.33', 'loss/train': '0.53942', 'examples_per_second': '124.98', 'grad_norm': '21.272', 'counters/examples': 37888, 'counters/updates': 592}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 38016 examples: {'rewards_train/chosen': '-0.42729', 'rewards_train/rejected': '-0.77055', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.34326', 'logps_train/rejected': '-143.78', 'logps_train/chosen': '-167.09', 'loss/train': '0.63491', 'examples_per_second': '125.17', 'grad_norm': '24.787', 'counters/examples': 38016, 'counters/updates': 594}
skipping logging after 38080 examples to avoid logging too frequently
train stats after 38144 examples: {'rewards_train/chosen': '-0.70344', 'rewards_train/rejected': '-0.96558', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.26214', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-148.96', 'loss/train': '0.68193', 'examples_per_second': '124.95', 'grad_norm': '26.191', 'counters/examples': 38144, 'counters/updates': 596}
skipping logging after 38208 examples to avoid logging too frequently
train stats after 38272 examples: {'rewards_train/chosen': '-0.50205', 'rewards_train/rejected': '-0.87711', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37506', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-155.85', 'loss/train': '0.6305', 'examples_per_second': '119.92', 'grad_norm': '23.643', 'counters/examples': 38272, 'counters/updates': 598}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38400 examples: {'rewards_train/chosen': '-0.46675', 'rewards_train/rejected': '-0.80792', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34117', 'logps_train/rejected': '-150.65', 'logps_train/chosen': '-165.88', 'loss/train': '0.62956', 'examples_per_second': '124.72', 'grad_norm': '25.563', 'counters/examples': 38400, 'counters/updates': 600}
skipping logging after 38464 examples to avoid logging too frequently
train stats after 38528 examples: {'rewards_train/chosen': '-0.50606', 'rewards_train/rejected': '-0.91618', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.41011', 'logps_train/rejected': '-150.43', 'logps_train/chosen': '-164.16', 'loss/train': '0.61855', 'examples_per_second': '119.95', 'grad_norm': '25.568', 'counters/examples': 38528, 'counters/updates': 602}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38656 examples: {'rewards_train/chosen': '-0.41214', 'rewards_train/rejected': '-0.72286', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.31072', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-158.26', 'loss/train': '0.63674', 'examples_per_second': '119.49', 'grad_norm': '26.714', 'counters/examples': 38656, 'counters/updates': 604}
skipping logging after 38720 examples to avoid logging too frequently
train stats after 38784 examples: {'rewards_train/chosen': '-0.46182', 'rewards_train/rejected': '-0.89874', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.43692', 'logps_train/rejected': '-155.46', 'logps_train/chosen': '-165.48', 'loss/train': '0.56351', 'examples_per_second': '118.98', 'grad_norm': '21.907', 'counters/examples': 38784, 'counters/updates': 606}
skipping logging after 38848 examples to avoid logging too frequently
train stats after 38912 examples: {'rewards_train/chosen': '-0.43516', 'rewards_train/rejected': '-1.0643', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.62911', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-151.81', 'loss/train': '0.53219', 'examples_per_second': '124.51', 'grad_norm': '19.881', 'counters/examples': 38912, 'counters/updates': 608}
skipping logging after 38976 examples to avoid logging too frequently
train stats after 39040 examples: {'rewards_train/chosen': '-0.50767', 'rewards_train/rejected': '-1.0883', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58059', 'logps_train/rejected': '-150.39', 'logps_train/chosen': '-142.24', 'loss/train': '0.52943', 'examples_per_second': '124.9', 'grad_norm': '21.829', 'counters/examples': 39040, 'counters/updates': 610}
skipping logging after 39104 examples to avoid logging too frequently
train stats after 39168 examples: {'rewards_train/chosen': '-0.77508', 'rewards_train/rejected': '-1.2109', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.43579', 'logps_train/rejected': '-137.5', 'logps_train/chosen': '-161.97', 'loss/train': '0.63481', 'examples_per_second': '122.36', 'grad_norm': '25.911', 'counters/examples': 39168, 'counters/updates': 612}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39296 examples: {'rewards_train/chosen': '-0.60438', 'rewards_train/rejected': '-0.99593', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.39156', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-151.18', 'loss/train': '0.61549', 'examples_per_second': '127.51', 'grad_norm': '22.876', 'counters/examples': 39296, 'counters/updates': 614}
skipping logging after 39360 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '-0.48865', 'rewards_train/rejected': '-0.89273', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.40408', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-143.34', 'loss/train': '0.58995', 'examples_per_second': '124.26', 'grad_norm': '21.677', 'counters/examples': 39424, 'counters/updates': 616}
skipping logging after 39488 examples to avoid logging too frequently
train stats after 39552 examples: {'rewards_train/chosen': '-0.41267', 'rewards_train/rejected': '-0.82801', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.41535', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-139.62', 'loss/train': '0.61318', 'examples_per_second': '118.93', 'grad_norm': '23.113', 'counters/examples': 39552, 'counters/updates': 618}
skipping logging after 39616 examples to avoid logging too frequently
train stats after 39680 examples: {'rewards_train/chosen': '-0.52901', 'rewards_train/rejected': '-0.85414', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32513', 'logps_train/rejected': '-128.45', 'logps_train/chosen': '-157.86', 'loss/train': '0.64323', 'examples_per_second': '125.08', 'grad_norm': '23.706', 'counters/examples': 39680, 'counters/updates': 620}
skipping logging after 39744 examples to avoid logging too frequently
train stats after 39808 examples: {'rewards_train/chosen': '-0.49316', 'rewards_train/rejected': '-0.7704', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.27723', 'logps_train/rejected': '-141.91', 'logps_train/chosen': '-151.54', 'loss/train': '0.64185', 'examples_per_second': '128.66', 'grad_norm': '25.287', 'counters/examples': 39808, 'counters/updates': 622}
skipping logging after 39872 examples to avoid logging too frequently
train stats after 39936 examples: {'rewards_train/chosen': '-0.42532', 'rewards_train/rejected': '-0.79785', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37252', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-169.91', 'loss/train': '0.59844', 'examples_per_second': '126.7', 'grad_norm': '24.566', 'counters/examples': 39936, 'counters/updates': 624}
skipping logging after 40000 examples to avoid logging too frequently
train stats after 40064 examples: {'rewards_train/chosen': '-0.35246', 'rewards_train/rejected': '-0.94582', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.59336', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-158.31', 'loss/train': '0.53253', 'examples_per_second': '119.68', 'grad_norm': '25.968', 'counters/examples': 40064, 'counters/updates': 626}
skipping logging after 40128 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '-0.51237', 'rewards_train/rejected': '-1.0229', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.51051', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-156.94', 'loss/train': '0.57186', 'examples_per_second': '133.87', 'grad_norm': '21.738', 'counters/examples': 40192, 'counters/updates': 628}
skipping logging after 40256 examples to avoid logging too frequently
train stats after 40320 examples: {'rewards_train/chosen': '-0.47302', 'rewards_train/rejected': '-0.99078', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.51776', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-147.86', 'loss/train': '0.55945', 'examples_per_second': '124.76', 'grad_norm': '20.667', 'counters/examples': 40320, 'counters/updates': 630}
skipping logging after 40384 examples to avoid logging too frequently
train stats after 40448 examples: {'rewards_train/chosen': '-0.6546', 'rewards_train/rejected': '-1.1401', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48546', 'logps_train/rejected': '-171.15', 'logps_train/chosen': '-141.87', 'loss/train': '0.58246', 'examples_per_second': '119.94', 'grad_norm': '23.192', 'counters/examples': 40448, 'counters/updates': 632}
skipping logging after 40512 examples to avoid logging too frequently
train stats after 40576 examples: {'rewards_train/chosen': '-0.46257', 'rewards_train/rejected': '-0.71606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25349', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-156.88', 'loss/train': '0.64257', 'examples_per_second': '124.8', 'grad_norm': '24.841', 'counters/examples': 40576, 'counters/updates': 634}
skipping logging after 40640 examples to avoid logging too frequently
train stats after 40704 examples: {'rewards_train/chosen': '-0.41468', 'rewards_train/rejected': '-0.76517', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.3505', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-139.17', 'loss/train': '0.66747', 'examples_per_second': '124.68', 'grad_norm': '24.517', 'counters/examples': 40704, 'counters/updates': 636}
skipping logging after 40768 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.54371', 'rewards_train/rejected': '-0.9607', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41698', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-141.14', 'loss/train': '0.59738', 'examples_per_second': '146.17', 'grad_norm': '22.035', 'counters/examples': 40832, 'counters/updates': 638}
skipping logging after 40896 examples to avoid logging too frequently
train stats after 40960 examples: {'rewards_train/chosen': '-0.54027', 'rewards_train/rejected': '-0.91712', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37685', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-134.75', 'loss/train': '0.60451', 'examples_per_second': '128.71', 'grad_norm': '23.431', 'counters/examples': 40960, 'counters/updates': 640}
skipping logging after 41024 examples to avoid logging too frequently
train stats after 41088 examples: {'rewards_train/chosen': '-0.5527', 'rewards_train/rejected': '-0.76859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21589', 'logps_train/rejected': '-153.56', 'logps_train/chosen': '-171.91', 'loss/train': '0.70062', 'examples_per_second': '124.89', 'grad_norm': '27.98', 'counters/examples': 41088, 'counters/updates': 642}
skipping logging after 41152 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '-0.52874', 'rewards_train/rejected': '-0.88976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36103', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-144.85', 'loss/train': '0.60032', 'examples_per_second': '141.01', 'grad_norm': '24.209', 'counters/examples': 41216, 'counters/updates': 644}
skipping logging after 41280 examples to avoid logging too frequently
train stats after 41344 examples: {'rewards_train/chosen': '-0.6198', 'rewards_train/rejected': '-0.89995', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.28016', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-135.99', 'loss/train': '0.69152', 'examples_per_second': '131.66', 'grad_norm': '23.079', 'counters/examples': 41344, 'counters/updates': 646}
skipping logging after 41408 examples to avoid logging too frequently
train stats after 41472 examples: {'rewards_train/chosen': '-0.62442', 'rewards_train/rejected': '-0.79508', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.17066', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-170.67', 'loss/train': '0.68599', 'examples_per_second': '125.06', 'grad_norm': '25.426', 'counters/examples': 41472, 'counters/updates': 648}
skipping logging after 41536 examples to avoid logging too frequently
train stats after 41600 examples: {'rewards_train/chosen': '-0.42786', 'rewards_train/rejected': '-0.70378', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.27592', 'logps_train/rejected': '-153.85', 'logps_train/chosen': '-136.52', 'loss/train': '0.65104', 'examples_per_second': '119.95', 'grad_norm': '24.484', 'counters/examples': 41600, 'counters/updates': 650}
skipping logging after 41664 examples to avoid logging too frequently
train stats after 41728 examples: {'rewards_train/chosen': '-0.64488', 'rewards_train/rejected': '-0.84821', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.20333', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-157.08', 'loss/train': '0.71305', 'examples_per_second': '124.58', 'grad_norm': '27.319', 'counters/examples': 41728, 'counters/updates': 652}
skipping logging after 41792 examples to avoid logging too frequently
train stats after 41856 examples: {'rewards_train/chosen': '-0.47493', 'rewards_train/rejected': '-0.91457', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.43963', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-150.76', 'loss/train': '0.59746', 'examples_per_second': '124.76', 'grad_norm': '26.548', 'counters/examples': 41856, 'counters/updates': 654}
skipping logging after 41920 examples to avoid logging too frequently
train stats after 41984 examples: {'rewards_train/chosen': '-0.32566', 'rewards_train/rejected': '-0.66052', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33486', 'logps_train/rejected': '-126.62', 'logps_train/chosen': '-143.8', 'loss/train': '0.61293', 'examples_per_second': '125.54', 'grad_norm': '23.701', 'counters/examples': 41984, 'counters/updates': 656}
skipping logging after 42048 examples to avoid logging too frequently
train stats after 42112 examples: {'rewards_train/chosen': '-0.29693', 'rewards_train/rejected': '-0.9463', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.64936', 'logps_train/rejected': '-143.04', 'logps_train/chosen': '-129.04', 'loss/train': '0.49741', 'examples_per_second': '123.93', 'grad_norm': '21.148', 'counters/examples': 42112, 'counters/updates': 658}
skipping logging after 42176 examples to avoid logging too frequently
train stats after 42240 examples: {'rewards_train/chosen': '-0.53908', 'rewards_train/rejected': '-1.1517', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61267', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-162.69', 'loss/train': '0.53516', 'examples_per_second': '119.76', 'grad_norm': '22.512', 'counters/examples': 42240, 'counters/updates': 660}
skipping logging after 42304 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '-0.65169', 'rewards_train/rejected': '-1.2401', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.58845', 'logps_train/rejected': '-133.83', 'logps_train/chosen': '-135.66', 'loss/train': '0.5449', 'examples_per_second': '140.57', 'grad_norm': '23.058', 'counters/examples': 42368, 'counters/updates': 662}
skipping logging after 42432 examples to avoid logging too frequently
train stats after 42496 examples: {'rewards_train/chosen': '-0.55902', 'rewards_train/rejected': '-0.95335', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.39433', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-148.92', 'loss/train': '0.61562', 'examples_per_second': '123.89', 'grad_norm': '23.44', 'counters/examples': 42496, 'counters/updates': 664}
skipping logging after 42560 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '-0.55759', 'rewards_train/rejected': '-1.0397', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48211', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-148.65', 'loss/train': '0.59005', 'examples_per_second': '120.61', 'grad_norm': '21.011', 'counters/examples': 42624, 'counters/updates': 666}
skipping logging after 42688 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '-0.55276', 'rewards_train/rejected': '-0.99059', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43784', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-151.73', 'loss/train': '0.59167', 'examples_per_second': '125', 'grad_norm': '23.042', 'counters/examples': 42752, 'counters/updates': 668}
skipping logging after 42816 examples to avoid logging too frequently
train stats after 42880 examples: {'rewards_train/chosen': '-0.46817', 'rewards_train/rejected': '-0.94331', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.47514', 'logps_train/rejected': '-143.9', 'logps_train/chosen': '-124.03', 'loss/train': '0.56581', 'examples_per_second': '130.82', 'grad_norm': '20.81', 'counters/examples': 42880, 'counters/updates': 670}
skipping logging after 42944 examples to avoid logging too frequently
train stats after 43008 examples: {'rewards_train/chosen': '-0.54418', 'rewards_train/rejected': '-0.86678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32261', 'logps_train/rejected': '-150.16', 'logps_train/chosen': '-174.82', 'loss/train': '0.6408', 'examples_per_second': '124.69', 'grad_norm': '25.727', 'counters/examples': 43008, 'counters/updates': 672}
skipping logging after 43072 examples to avoid logging too frequently
train stats after 43136 examples: {'rewards_train/chosen': '-0.53904', 'rewards_train/rejected': '-0.89579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35675', 'logps_train/rejected': '-142.48', 'logps_train/chosen': '-151.09', 'loss/train': '0.59389', 'examples_per_second': '124.65', 'grad_norm': '21.949', 'counters/examples': 43136, 'counters/updates': 674}
skipping logging after 43200 examples to avoid logging too frequently
train stats after 43264 examples: {'rewards_train/chosen': '-0.39797', 'rewards_train/rejected': '-0.96019', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.56223', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-157.99', 'loss/train': '0.53993', 'examples_per_second': '122.02', 'grad_norm': '23.561', 'counters/examples': 43264, 'counters/updates': 676}
skipping logging after 43328 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '-0.34131', 'rewards_train/rejected': '-0.76014', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41883', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-152.46', 'loss/train': '0.62048', 'examples_per_second': '121.71', 'grad_norm': '23.87', 'counters/examples': 43392, 'counters/updates': 678}
skipping logging after 43456 examples to avoid logging too frequently
train stats after 43520 examples: {'rewards_train/chosen': '-0.36072', 'rewards_train/rejected': '-0.90032', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.5396', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-144.58', 'loss/train': '0.55305', 'examples_per_second': '119.04', 'grad_norm': '22.147', 'counters/examples': 43520, 'counters/updates': 680}
skipping logging after 43584 examples to avoid logging too frequently
train stats after 43648 examples: {'rewards_train/chosen': '-0.48291', 'rewards_train/rejected': '-1.0441', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5612', 'logps_train/rejected': '-100.25', 'logps_train/chosen': '-140.81', 'loss/train': '0.55714', 'examples_per_second': '123.6', 'grad_norm': '20.965', 'counters/examples': 43648, 'counters/updates': 682}
skipping logging after 43712 examples to avoid logging too frequently
train stats after 43776 examples: {'rewards_train/chosen': '-0.60872', 'rewards_train/rejected': '-0.93047', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32175', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-155.24', 'loss/train': '0.62775', 'examples_per_second': '125', 'grad_norm': '24.497', 'counters/examples': 43776, 'counters/updates': 684}
skipping logging after 43840 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '-0.6177', 'rewards_train/rejected': '-1.0512', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43349', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-148.39', 'loss/train': '0.64247', 'examples_per_second': '138.98', 'grad_norm': '24.683', 'counters/examples': 43904, 'counters/updates': 686}
skipping logging after 43968 examples to avoid logging too frequently
train stats after 44032 examples: {'rewards_train/chosen': '-0.55479', 'rewards_train/rejected': '-0.82094', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.26615', 'logps_train/rejected': '-149.03', 'logps_train/chosen': '-163.97', 'loss/train': '0.69396', 'examples_per_second': '126.16', 'grad_norm': '28.376', 'counters/examples': 44032, 'counters/updates': 688}
skipping logging after 44096 examples to avoid logging too frequently
train stats after 44160 examples: {'rewards_train/chosen': '-0.59796', 'rewards_train/rejected': '-0.91268', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31472', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-168.74', 'loss/train': '0.68909', 'examples_per_second': '125', 'grad_norm': '29.313', 'counters/examples': 44160, 'counters/updates': 690}
skipping logging after 44224 examples to avoid logging too frequently
train stats after 44288 examples: {'rewards_train/chosen': '-0.63598', 'rewards_train/rejected': '-0.83799', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.20202', 'logps_train/rejected': '-149.52', 'logps_train/chosen': '-145.46', 'loss/train': '0.6915', 'examples_per_second': '129.11', 'grad_norm': '25.151', 'counters/examples': 44288, 'counters/updates': 692}
skipping logging after 44352 examples to avoid logging too frequently
train stats after 44416 examples: {'rewards_train/chosen': '-0.44279', 'rewards_train/rejected': '-0.86402', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.42122', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-151.66', 'loss/train': '0.63585', 'examples_per_second': '124.67', 'grad_norm': '24.391', 'counters/examples': 44416, 'counters/updates': 694}
skipping logging after 44480 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '-0.57286', 'rewards_train/rejected': '-1.0607', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.48788', 'logps_train/rejected': '-130.67', 'logps_train/chosen': '-136.06', 'loss/train': '0.59112', 'examples_per_second': '127.33', 'grad_norm': '22.686', 'counters/examples': 44544, 'counters/updates': 696}
skipping logging after 44608 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '-0.50938', 'rewards_train/rejected': '-0.75461', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24523', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-145.59', 'loss/train': '0.65623', 'examples_per_second': '124.52', 'grad_norm': '24.615', 'counters/examples': 44672, 'counters/updates': 698}
skipping logging after 44736 examples to avoid logging too frequently
train stats after 44800 examples: {'rewards_train/chosen': '-0.5588', 'rewards_train/rejected': '-0.96419', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40539', 'logps_train/rejected': '-155.46', 'logps_train/chosen': '-178.08', 'loss/train': '0.60711', 'examples_per_second': '124.65', 'grad_norm': '26.79', 'counters/examples': 44800, 'counters/updates': 700}
skipping logging after 44864 examples to avoid logging too frequently
train stats after 44928 examples: {'rewards_train/chosen': '-0.54981', 'rewards_train/rejected': '-0.7937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24389', 'logps_train/rejected': '-158.09', 'logps_train/chosen': '-161.73', 'loss/train': '0.65741', 'examples_per_second': '124.76', 'grad_norm': '24.75', 'counters/examples': 44928, 'counters/updates': 702}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45056 examples: {'rewards_train/chosen': '-0.34034', 'rewards_train/rejected': '-0.66235', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.32201', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-154.91', 'loss/train': '0.62519', 'examples_per_second': '124.82', 'grad_norm': '24.903', 'counters/examples': 45056, 'counters/updates': 704}
skipping logging after 45120 examples to avoid logging too frequently
train stats after 45184 examples: {'rewards_train/chosen': '-0.64897', 'rewards_train/rejected': '-1.0993', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.45034', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-138.85', 'loss/train': '0.57221', 'examples_per_second': '122.71', 'grad_norm': '19.559', 'counters/examples': 45184, 'counters/updates': 706}
skipping logging after 45248 examples to avoid logging too frequently
train stats after 45312 examples: {'rewards_train/chosen': '-0.63439', 'rewards_train/rejected': '-1.2051', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.57073', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-153.86', 'loss/train': '0.52461', 'examples_per_second': '128.68', 'grad_norm': '19.844', 'counters/examples': 45312, 'counters/updates': 708}
skipping logging after 45376 examples to avoid logging too frequently
train stats after 45440 examples: {'rewards_train/chosen': '-0.7204', 'rewards_train/rejected': '-0.94679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22639', 'logps_train/rejected': '-130.88', 'logps_train/chosen': '-136.9', 'loss/train': '0.68853', 'examples_per_second': '123.83', 'grad_norm': '25.765', 'counters/examples': 45440, 'counters/updates': 710}
skipping logging after 45504 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '-0.67949', 'rewards_train/rejected': '-0.92537', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.24588', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-133.44', 'loss/train': '0.67319', 'examples_per_second': '134.15', 'grad_norm': '23.358', 'counters/examples': 45568, 'counters/updates': 712}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45696 examples: {'rewards_train/chosen': '-0.58531', 'rewards_train/rejected': '-1.0346', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.44928', 'logps_train/rejected': '-141.42', 'logps_train/chosen': '-141.82', 'loss/train': '0.58041', 'examples_per_second': '124.87', 'grad_norm': '22.378', 'counters/examples': 45696, 'counters/updates': 714}
skipping logging after 45760 examples to avoid logging too frequently
train stats after 45824 examples: {'rewards_train/chosen': '-0.54006', 'rewards_train/rejected': '-1.0473', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50727', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-143.76', 'loss/train': '0.56019', 'examples_per_second': '124.95', 'grad_norm': '20.789', 'counters/examples': 45824, 'counters/updates': 716}
skipping logging after 45888 examples to avoid logging too frequently
train stats after 45952 examples: {'rewards_train/chosen': '-0.71215', 'rewards_train/rejected': '-1.091', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37881', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-154.78', 'loss/train': '0.61345', 'examples_per_second': '131.21', 'grad_norm': '24.561', 'counters/examples': 45952, 'counters/updates': 718}
skipping logging after 46016 examples to avoid logging too frequently
train stats after 46080 examples: {'rewards_train/chosen': '-0.59094', 'rewards_train/rejected': '-0.89573', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30479', 'logps_train/rejected': '-160.34', 'logps_train/chosen': '-145.33', 'loss/train': '0.63495', 'examples_per_second': '119.16', 'grad_norm': '23.336', 'counters/examples': 46080, 'counters/updates': 720}
skipping logging after 46144 examples to avoid logging too frequently
train stats after 46208 examples: {'rewards_train/chosen': '-0.56547', 'rewards_train/rejected': '-1.1648', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.59936', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-163.16', 'loss/train': '0.5348', 'examples_per_second': '124.16', 'grad_norm': '21.629', 'counters/examples': 46208, 'counters/updates': 722}
skipping logging after 46272 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '-0.68349', 'rewards_train/rejected': '-0.84723', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.16373', 'logps_train/rejected': '-140.3', 'logps_train/chosen': '-157.63', 'loss/train': '0.73243', 'examples_per_second': '126.28', 'grad_norm': '26.855', 'counters/examples': 46336, 'counters/updates': 724}
skipping logging after 46400 examples to avoid logging too frequently
train stats after 46464 examples: {'rewards_train/chosen': '-0.64442', 'rewards_train/rejected': '-1.0005', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.35611', 'logps_train/rejected': '-140.36', 'logps_train/chosen': '-127.91', 'loss/train': '0.59613', 'examples_per_second': '125.64', 'grad_norm': '21.165', 'counters/examples': 46464, 'counters/updates': 726}
skipping logging after 46528 examples to avoid logging too frequently
train stats after 46592 examples: {'rewards_train/chosen': '-0.64828', 'rewards_train/rejected': '-1.1565', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.50825', 'logps_train/rejected': '-147.71', 'logps_train/chosen': '-147.42', 'loss/train': '0.56683', 'examples_per_second': '118.65', 'grad_norm': '23.579', 'counters/examples': 46592, 'counters/updates': 728}
skipping logging after 46656 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '-0.57334', 'rewards_train/rejected': '-0.92848', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35515', 'logps_train/rejected': '-157.47', 'logps_train/chosen': '-173.14', 'loss/train': '0.63673', 'examples_per_second': '121.44', 'grad_norm': '25.435', 'counters/examples': 46720, 'counters/updates': 730}
skipping logging after 46784 examples to avoid logging too frequently
train stats after 46848 examples: {'rewards_train/chosen': '-0.66744', 'rewards_train/rejected': '-1.2505', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.58302', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-157.08', 'loss/train': '0.54405', 'examples_per_second': '119.29', 'grad_norm': '21.923', 'counters/examples': 46848, 'counters/updates': 732}
skipping logging after 46912 examples to avoid logging too frequently
train stats after 46976 examples: {'rewards_train/chosen': '-0.63891', 'rewards_train/rejected': '-1.052', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.41308', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-158.46', 'loss/train': '0.62436', 'examples_per_second': '118.99', 'grad_norm': '23.841', 'counters/examples': 46976, 'counters/updates': 734}
skipping logging after 47040 examples to avoid logging too frequently
train stats after 47104 examples: {'rewards_train/chosen': '-0.40494', 'rewards_train/rejected': '-1.0537', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.6488', 'logps_train/rejected': '-140.96', 'logps_train/chosen': '-160.59', 'loss/train': '0.53835', 'examples_per_second': '127.6', 'grad_norm': '21.383', 'counters/examples': 47104, 'counters/updates': 736}
skipping logging after 47168 examples to avoid logging too frequently
train stats after 47232 examples: {'rewards_train/chosen': '-0.50665', 'rewards_train/rejected': '-1.1194', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.61278', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-130.94', 'loss/train': '0.5123', 'examples_per_second': '124.72', 'grad_norm': '19.062', 'counters/examples': 47232, 'counters/updates': 738}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47360 examples: {'rewards_train/chosen': '-0.64694', 'rewards_train/rejected': '-1.0863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43933', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-171.88', 'loss/train': '0.59054', 'examples_per_second': '126.07', 'grad_norm': '23.001', 'counters/examples': 47360, 'counters/updates': 740}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47488 examples: {'rewards_train/chosen': '-0.73552', 'rewards_train/rejected': '-1.1503', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.41478', 'logps_train/rejected': '-152.04', 'logps_train/chosen': '-170.82', 'loss/train': '0.64047', 'examples_per_second': '124.85', 'grad_norm': '25.594', 'counters/examples': 47488, 'counters/updates': 742}
skipping logging after 47552 examples to avoid logging too frequently
train stats after 47616 examples: {'rewards_train/chosen': '-0.54257', 'rewards_train/rejected': '-0.95513', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.41256', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-164.69', 'loss/train': '0.61401', 'examples_per_second': '118.57', 'grad_norm': '26.954', 'counters/examples': 47616, 'counters/updates': 744}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47744 examples: {'rewards_train/chosen': '-0.55405', 'rewards_train/rejected': '-0.94341', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.38936', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-140.08', 'loss/train': '0.60345', 'examples_per_second': '126.02', 'grad_norm': '22.544', 'counters/examples': 47744, 'counters/updates': 746}
skipping logging after 47808 examples to avoid logging too frequently
train stats after 47872 examples: {'rewards_train/chosen': '-0.55709', 'rewards_train/rejected': '-0.90502', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.34793', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-129.86', 'loss/train': '0.59618', 'examples_per_second': '118.41', 'grad_norm': '24.471', 'counters/examples': 47872, 'counters/updates': 748}
Running evaluation after 47872 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.40it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.39it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.44it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.42it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.40it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.35it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.30it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.27it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.33it/s]
eval after 47872: {'rewards_eval/chosen': '-0.4936', 'rewards_eval/rejected': '-0.91786', 'rewards_eval/accuracies': '0.67969', 'rewards_eval/margins': '0.42426', 'logps_eval/rejected': '-135.26', 'logps_eval/chosen': '-152.84', 'loss/eval': '0.62764'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-47872...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-47872/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-47872/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-47872/scheduler.pt...
train stats after 47936 examples: {'rewards_train/chosen': '-0.63508', 'rewards_train/rejected': '-0.94107', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.30599', 'logps_train/rejected': '-144.89', 'logps_train/chosen': '-163.76', 'loss/train': '0.63561', 'examples_per_second': '117.18', 'grad_norm': '24.63', 'counters/examples': 47936, 'counters/updates': 749}
skipping logging after 48000 examples to avoid logging too frequently
train stats after 48064 examples: {'rewards_train/chosen': '-0.46753', 'rewards_train/rejected': '-0.9223', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.45477', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-160.98', 'loss/train': '0.5767', 'examples_per_second': '130.19', 'grad_norm': '25.037', 'counters/examples': 48064, 'counters/updates': 751}
skipping logging after 48128 examples to avoid logging too frequently
train stats after 48192 examples: {'rewards_train/chosen': '-0.58326', 'rewards_train/rejected': '-1.0901', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.50681', 'logps_train/rejected': '-118', 'logps_train/chosen': '-135.98', 'loss/train': '0.56588', 'examples_per_second': '123.88', 'grad_norm': '20.768', 'counters/examples': 48192, 'counters/updates': 753}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48320 examples: {'rewards_train/chosen': '-0.73549', 'rewards_train/rejected': '-0.9857', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2502', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-142.41', 'loss/train': '0.67419', 'examples_per_second': '121.68', 'grad_norm': '24.975', 'counters/examples': 48320, 'counters/updates': 755}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48448 examples: {'rewards_train/chosen': '-0.49554', 'rewards_train/rejected': '-0.91938', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42383', 'logps_train/rejected': '-136', 'logps_train/chosen': '-151.43', 'loss/train': '0.59315', 'examples_per_second': '124.57', 'grad_norm': '26.716', 'counters/examples': 48448, 'counters/updates': 757}
skipping logging after 48512 examples to avoid logging too frequently
train stats after 48576 examples: {'rewards_train/chosen': '-0.56963', 'rewards_train/rejected': '-1.1688', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.59916', 'logps_train/rejected': '-145.62', 'logps_train/chosen': '-156.17', 'loss/train': '0.55285', 'examples_per_second': '124.73', 'grad_norm': '23.386', 'counters/examples': 48576, 'counters/updates': 759}
skipping logging after 48640 examples to avoid logging too frequently
train stats after 48704 examples: {'rewards_train/chosen': '-0.48426', 'rewards_train/rejected': '-1.019', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.53476', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-144.86', 'loss/train': '0.55779', 'examples_per_second': '124.55', 'grad_norm': '20.631', 'counters/examples': 48704, 'counters/updates': 761}
skipping logging after 48768 examples to avoid logging too frequently
train stats after 48832 examples: {'rewards_train/chosen': '-0.7443', 'rewards_train/rejected': '-1.2084', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.46405', 'logps_train/rejected': '-167.57', 'logps_train/chosen': '-180.56', 'loss/train': '0.60966', 'examples_per_second': '124.51', 'grad_norm': '25.889', 'counters/examples': 48832, 'counters/updates': 763}
skipping logging after 48896 examples to avoid logging too frequently
train stats after 48960 examples: {'rewards_train/chosen': '-0.48101', 'rewards_train/rejected': '-0.9484', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46739', 'logps_train/rejected': '-126.5', 'logps_train/chosen': '-176.85', 'loss/train': '0.58305', 'examples_per_second': '124.72', 'grad_norm': '23.418', 'counters/examples': 48960, 'counters/updates': 765}
skipping logging after 49024 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '-0.32481', 'rewards_train/rejected': '-0.78496', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.46015', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-131.53', 'loss/train': '0.5737', 'examples_per_second': '126.87', 'grad_norm': '20.982', 'counters/examples': 49088, 'counters/updates': 767}
skipping logging after 49152 examples to avoid logging too frequently
train stats after 49216 examples: {'rewards_train/chosen': '-0.45647', 'rewards_train/rejected': '-0.82768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37121', 'logps_train/rejected': '-161.39', 'logps_train/chosen': '-190.39', 'loss/train': '0.62475', 'examples_per_second': '124.3', 'grad_norm': '26.352', 'counters/examples': 49216, 'counters/updates': 769}
skipping logging after 49280 examples to avoid logging too frequently
train stats after 49344 examples: {'rewards_train/chosen': '-0.67216', 'rewards_train/rejected': '-1.1582', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.48609', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-159.8', 'loss/train': '0.58839', 'examples_per_second': '124.76', 'grad_norm': '23.154', 'counters/examples': 49344, 'counters/updates': 771}
skipping logging after 49408 examples to avoid logging too frequently
train stats after 49472 examples: {'rewards_train/chosen': '-0.77742', 'rewards_train/rejected': '-1.0628', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.28536', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-166.07', 'loss/train': '0.65529', 'examples_per_second': '132', 'grad_norm': '24.2', 'counters/examples': 49472, 'counters/updates': 773}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49600 examples: {'rewards_train/chosen': '-0.49855', 'rewards_train/rejected': '-0.89828', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39973', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-157.73', 'loss/train': '0.60281', 'examples_per_second': '124.8', 'grad_norm': '22.566', 'counters/examples': 49600, 'counters/updates': 775}
skipping logging after 49664 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '-0.6173', 'rewards_train/rejected': '-0.95903', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34173', 'logps_train/rejected': '-148.3', 'logps_train/chosen': '-146.77', 'loss/train': '0.6385', 'examples_per_second': '123.75', 'grad_norm': '24.488', 'counters/examples': 49728, 'counters/updates': 777}
skipping logging after 49792 examples to avoid logging too frequently
train stats after 49856 examples: {'rewards_train/chosen': '-0.55592', 'rewards_train/rejected': '-0.72378', 'rewards_train/accuracies': '0.48438', 'rewards_train/margins': '0.16785', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-143.12', 'loss/train': '0.70658', 'examples_per_second': '123.79', 'grad_norm': '26.128', 'counters/examples': 49856, 'counters/updates': 779}
skipping logging after 49920 examples to avoid logging too frequently
train stats after 49984 examples: {'rewards_train/chosen': '-0.5035', 'rewards_train/rejected': '-1.0707', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.56725', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-144.25', 'loss/train': '0.55257', 'examples_per_second': '124.65', 'grad_norm': '20.575', 'counters/examples': 49984, 'counters/updates': 781}
skipping logging after 50048 examples to avoid logging too frequently
train stats after 50112 examples: {'rewards_train/chosen': '-0.45127', 'rewards_train/rejected': '-0.75577', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3045', 'logps_train/rejected': '-151.93', 'logps_train/chosen': '-170.72', 'loss/train': '0.63436', 'examples_per_second': '124.73', 'grad_norm': '25.229', 'counters/examples': 50112, 'counters/updates': 783}
skipping logging after 50176 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '-0.69765', 'rewards_train/rejected': '-1.1067', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40903', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-143.09', 'loss/train': '0.57807', 'examples_per_second': '128.25', 'grad_norm': '23.224', 'counters/examples': 50240, 'counters/updates': 785}
skipping logging after 50304 examples to avoid logging too frequently
train stats after 50368 examples: {'rewards_train/chosen': '-0.58381', 'rewards_train/rejected': '-1.2614', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.67756', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-154.97', 'loss/train': '0.48899', 'examples_per_second': '125.64', 'grad_norm': '19.336', 'counters/examples': 50368, 'counters/updates': 787}
skipping logging after 50432 examples to avoid logging too frequently
train stats after 50496 examples: {'rewards_train/chosen': '-0.63232', 'rewards_train/rejected': '-1.0479', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.41553', 'logps_train/rejected': '-173.42', 'logps_train/chosen': '-166.82', 'loss/train': '0.586', 'examples_per_second': '124.55', 'grad_norm': '21.859', 'counters/examples': 50496, 'counters/updates': 789}
skipping logging after 50560 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '-0.5822', 'rewards_train/rejected': '-1.2116', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.62943', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-130.48', 'loss/train': '0.53561', 'examples_per_second': '131.65', 'grad_norm': '20.28', 'counters/examples': 50624, 'counters/updates': 791}
skipping logging after 50688 examples to avoid logging too frequently
train stats after 50752 examples: {'rewards_train/chosen': '-0.34041', 'rewards_train/rejected': '-0.81187', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.47146', 'logps_train/rejected': '-156.34', 'logps_train/chosen': '-145.45', 'loss/train': '0.582', 'examples_per_second': '115.14', 'grad_norm': '23.229', 'counters/examples': 50752, 'counters/updates': 793}
skipping logging after 50816 examples to avoid logging too frequently
train stats after 50880 examples: {'rewards_train/chosen': '-0.74039', 'rewards_train/rejected': '-0.89821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15782', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-162.99', 'loss/train': '0.72199', 'examples_per_second': '124.57', 'grad_norm': '29.79', 'counters/examples': 50880, 'counters/updates': 795}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 51008 examples: {'rewards_train/chosen': '-0.78308', 'rewards_train/rejected': '-1.1063', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.32324', 'logps_train/rejected': '-148.21', 'logps_train/chosen': '-166.22', 'loss/train': '0.62375', 'examples_per_second': '127.47', 'grad_norm': '24.284', 'counters/examples': 51008, 'counters/updates': 797}
skipping logging after 51072 examples to avoid logging too frequently
train stats after 51136 examples: {'rewards_train/chosen': '-0.73965', 'rewards_train/rejected': '-1.1688', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.42917', 'logps_train/rejected': '-137.98', 'logps_train/chosen': '-163.04', 'loss/train': '0.62016', 'examples_per_second': '124.51', 'grad_norm': '23.103', 'counters/examples': 51136, 'counters/updates': 799}
skipping logging after 51200 examples to avoid logging too frequently
train stats after 51264 examples: {'rewards_train/chosen': '-0.59865', 'rewards_train/rejected': '-1.2629', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.66426', 'logps_train/rejected': '-133.44', 'logps_train/chosen': '-175.74', 'loss/train': '0.50971', 'examples_per_second': '124.5', 'grad_norm': '21.513', 'counters/examples': 51264, 'counters/updates': 801}
skipping logging after 51328 examples to avoid logging too frequently
train stats after 51392 examples: {'rewards_train/chosen': '-0.48166', 'rewards_train/rejected': '-1.0371', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.55539', 'logps_train/rejected': '-132.15', 'logps_train/chosen': '-163.44', 'loss/train': '0.53869', 'examples_per_second': '118.59', 'grad_norm': '22.288', 'counters/examples': 51392, 'counters/updates': 803}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51520 examples: {'rewards_train/chosen': '-0.6739', 'rewards_train/rejected': '-1.1811', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5072', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-159.24', 'loss/train': '0.55703', 'examples_per_second': '124.75', 'grad_norm': '23.782', 'counters/examples': 51520, 'counters/updates': 805}
skipping logging after 51584 examples to avoid logging too frequently
train stats after 51648 examples: {'rewards_train/chosen': '-0.47077', 'rewards_train/rejected': '-0.96715', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.49638', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-157.48', 'loss/train': '0.55638', 'examples_per_second': '124.66', 'grad_norm': '22.943', 'counters/examples': 51648, 'counters/updates': 807}
skipping logging after 51712 examples to avoid logging too frequently
train stats after 51776 examples: {'rewards_train/chosen': '-0.54803', 'rewards_train/rejected': '-1.0264', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47841', 'logps_train/rejected': '-147.04', 'logps_train/chosen': '-135.66', 'loss/train': '0.59614', 'examples_per_second': '124.85', 'grad_norm': '24.064', 'counters/examples': 51776, 'counters/updates': 809}
skipping logging after 51840 examples to avoid logging too frequently
train stats after 51904 examples: {'rewards_train/chosen': '-0.54105', 'rewards_train/rejected': '-0.90919', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.36814', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-149.85', 'loss/train': '0.62708', 'examples_per_second': '131.81', 'grad_norm': '23.445', 'counters/examples': 51904, 'counters/updates': 811}
skipping logging after 51968 examples to avoid logging too frequently
train stats after 52032 examples: {'rewards_train/chosen': '-0.56958', 'rewards_train/rejected': '-0.89111', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32153', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-169.41', 'loss/train': '0.65492', 'examples_per_second': '134.63', 'grad_norm': '26.14', 'counters/examples': 52032, 'counters/updates': 813}
skipping logging after 52096 examples to avoid logging too frequently
train stats after 52160 examples: {'rewards_train/chosen': '-0.57319', 'rewards_train/rejected': '-1.0467', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4735', 'logps_train/rejected': '-161.47', 'logps_train/chosen': '-154.22', 'loss/train': '0.58356', 'examples_per_second': '123.93', 'grad_norm': '23.557', 'counters/examples': 52160, 'counters/updates': 815}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52288 examples: {'rewards_train/chosen': '-0.70254', 'rewards_train/rejected': '-1.1736', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.47109', 'logps_train/rejected': '-160.02', 'logps_train/chosen': '-186.93', 'loss/train': '0.59025', 'examples_per_second': '124.66', 'grad_norm': '25.698', 'counters/examples': 52288, 'counters/updates': 817}
skipping logging after 52352 examples to avoid logging too frequently
train stats after 52416 examples: {'rewards_train/chosen': '-0.5323', 'rewards_train/rejected': '-1.1518', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.61952', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-158.14', 'loss/train': '0.52366', 'examples_per_second': '124.73', 'grad_norm': '20.169', 'counters/examples': 52416, 'counters/updates': 819}
skipping logging after 52480 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '-0.66309', 'rewards_train/rejected': '-1.2725', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.6094', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-136', 'loss/train': '0.54885', 'examples_per_second': '120.28', 'grad_norm': '21.51', 'counters/examples': 52544, 'counters/updates': 821}
skipping logging after 52608 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '-0.45441', 'rewards_train/rejected': '-1.011', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.55655', 'logps_train/rejected': '-148.89', 'logps_train/chosen': '-147.47', 'loss/train': '0.60127', 'examples_per_second': '119.49', 'grad_norm': '25.623', 'counters/examples': 52672, 'counters/updates': 823}
skipping logging after 52736 examples to avoid logging too frequently
train stats after 52800 examples: {'rewards_train/chosen': '-0.29534', 'rewards_train/rejected': '-0.95282', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.65748', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-168.36', 'loss/train': '0.51366', 'examples_per_second': '124.54', 'grad_norm': '22.711', 'counters/examples': 52800, 'counters/updates': 825}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52928 examples: {'rewards_train/chosen': '-0.62138', 'rewards_train/rejected': '-1.0845', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.46313', 'logps_train/rejected': '-167.35', 'logps_train/chosen': '-176.47', 'loss/train': '0.68038', 'examples_per_second': '118.87', 'grad_norm': '31.193', 'counters/examples': 52928, 'counters/updates': 827}
skipping logging after 52992 examples to avoid logging too frequently
train stats after 53056 examples: {'rewards_train/chosen': '-0.53667', 'rewards_train/rejected': '-1.0922', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5555', 'logps_train/rejected': '-136.1', 'logps_train/chosen': '-145.22', 'loss/train': '0.57895', 'examples_per_second': '120.35', 'grad_norm': '24.43', 'counters/examples': 53056, 'counters/updates': 829}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53184 examples: {'rewards_train/chosen': '-0.44081', 'rewards_train/rejected': '-0.86293', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42211', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-123.22', 'loss/train': '0.63355', 'examples_per_second': '123.66', 'grad_norm': '22.417', 'counters/examples': 53184, 'counters/updates': 831}
skipping logging after 53248 examples to avoid logging too frequently
train stats after 53312 examples: {'rewards_train/chosen': '-0.68191', 'rewards_train/rejected': '-1.1345', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45262', 'logps_train/rejected': '-148.4', 'logps_train/chosen': '-155.17', 'loss/train': '0.60246', 'examples_per_second': '124.36', 'grad_norm': '25.501', 'counters/examples': 53312, 'counters/updates': 833}
skipping logging after 53376 examples to avoid logging too frequently
train stats after 53440 examples: {'rewards_train/chosen': '-0.48395', 'rewards_train/rejected': '-1.1129', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.6289', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-141.42', 'loss/train': '0.53051', 'examples_per_second': '124.5', 'grad_norm': '22.734', 'counters/examples': 53440, 'counters/updates': 835}
skipping logging after 53504 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '-0.71778', 'rewards_train/rejected': '-1.3013', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.58356', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-151.8', 'loss/train': '0.56524', 'examples_per_second': '138.63', 'grad_norm': '25.219', 'counters/examples': 53568, 'counters/updates': 837}
skipping logging after 53632 examples to avoid logging too frequently
train stats after 53696 examples: {'rewards_train/chosen': '-0.66933', 'rewards_train/rejected': '-1.202', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.53268', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-163.25', 'loss/train': '0.58434', 'examples_per_second': '123.61', 'grad_norm': '24.537', 'counters/examples': 53696, 'counters/updates': 839}
skipping logging after 53760 examples to avoid logging too frequently
train stats after 53824 examples: {'rewards_train/chosen': '-0.56606', 'rewards_train/rejected': '-1.2431', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.67706', 'logps_train/rejected': '-185.34', 'logps_train/chosen': '-199.75', 'loss/train': '0.52582', 'examples_per_second': '124.37', 'grad_norm': '26.426', 'counters/examples': 53824, 'counters/updates': 841}
skipping logging after 53888 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '-0.68417', 'rewards_train/rejected': '-1.2577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.57349', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-177.14', 'loss/train': '0.56469', 'examples_per_second': '124.42', 'grad_norm': '24.693', 'counters/examples': 53952, 'counters/updates': 843}
skipping logging after 54016 examples to avoid logging too frequently
train stats after 54080 examples: {'rewards_train/chosen': '-0.57669', 'rewards_train/rejected': '-0.90835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33165', 'logps_train/rejected': '-161.51', 'logps_train/chosen': '-161.17', 'loss/train': '0.65559', 'examples_per_second': '124.18', 'grad_norm': '27.214', 'counters/examples': 54080, 'counters/updates': 845}
skipping logging after 54144 examples to avoid logging too frequently
train stats after 54208 examples: {'rewards_train/chosen': '-0.55766', 'rewards_train/rejected': '-0.82634', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26868', 'logps_train/rejected': '-158.83', 'logps_train/chosen': '-136.78', 'loss/train': '0.69572', 'examples_per_second': '114.98', 'grad_norm': '26.794', 'counters/examples': 54208, 'counters/updates': 847}
skipping logging after 54272 examples to avoid logging too frequently
train stats after 54336 examples: {'rewards_train/chosen': '-0.52234', 'rewards_train/rejected': '-0.89612', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37378', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-159.62', 'loss/train': '0.64152', 'examples_per_second': '124.87', 'grad_norm': '25.594', 'counters/examples': 54336, 'counters/updates': 849}
skipping logging after 54400 examples to avoid logging too frequently
train stats after 54464 examples: {'rewards_train/chosen': '-0.56519', 'rewards_train/rejected': '-0.90742', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34223', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-195.45', 'loss/train': '0.66525', 'examples_per_second': '124.08', 'grad_norm': '27.933', 'counters/examples': 54464, 'counters/updates': 851}
skipping logging after 54528 examples to avoid logging too frequently
train stats after 54592 examples: {'rewards_train/chosen': '-0.46819', 'rewards_train/rejected': '-0.9556', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48741', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-191.18', 'loss/train': '0.61284', 'examples_per_second': '124.42', 'grad_norm': '25.968', 'counters/examples': 54592, 'counters/updates': 853}
skipping logging after 54656 examples to avoid logging too frequently
train stats after 54720 examples: {'rewards_train/chosen': '-0.7976', 'rewards_train/rejected': '-1.1855', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.38792', 'logps_train/rejected': '-147.5', 'logps_train/chosen': '-157.14', 'loss/train': '0.65756', 'examples_per_second': '118.56', 'grad_norm': '26.523', 'counters/examples': 54720, 'counters/updates': 855}
skipping logging after 54784 examples to avoid logging too frequently
train stats after 54848 examples: {'rewards_train/chosen': '-0.57425', 'rewards_train/rejected': '-1.0137', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43944', 'logps_train/rejected': '-168.58', 'logps_train/chosen': '-155.83', 'loss/train': '0.60598', 'examples_per_second': '120.52', 'grad_norm': '26.104', 'counters/examples': 54848, 'counters/updates': 857}
skipping logging after 54912 examples to avoid logging too frequently
train stats after 54976 examples: {'rewards_train/chosen': '-0.69861', 'rewards_train/rejected': '-1.0662', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.36762', 'logps_train/rejected': '-144.8', 'logps_train/chosen': '-149.64', 'loss/train': '0.63819', 'examples_per_second': '124.42', 'grad_norm': '26.075', 'counters/examples': 54976, 'counters/updates': 859}
skipping logging after 55040 examples to avoid logging too frequently
train stats after 55104 examples: {'rewards_train/chosen': '-0.69866', 'rewards_train/rejected': '-1.2242', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.52549', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-118.66', 'loss/train': '0.55656', 'examples_per_second': '124.83', 'grad_norm': '19.735', 'counters/examples': 55104, 'counters/updates': 861}
skipping logging after 55168 examples to avoid logging too frequently
train stats after 55232 examples: {'rewards_train/chosen': '-0.71585', 'rewards_train/rejected': '-1.0922', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37632', 'logps_train/rejected': '-148.59', 'logps_train/chosen': '-181.87', 'loss/train': '0.61032', 'examples_per_second': '124.42', 'grad_norm': '25.052', 'counters/examples': 55232, 'counters/updates': 863}
skipping logging after 55296 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '-0.50092', 'rewards_train/rejected': '-1.1743', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.67336', 'logps_train/rejected': '-150.08', 'logps_train/chosen': '-143.23', 'loss/train': '0.52582', 'examples_per_second': '119.06', 'grad_norm': '21.701', 'counters/examples': 55360, 'counters/updates': 865}
skipping logging after 55424 examples to avoid logging too frequently
train stats after 55488 examples: {'rewards_train/chosen': '-0.3925', 'rewards_train/rejected': '-0.95688', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56438', 'logps_train/rejected': '-171.06', 'logps_train/chosen': '-192.87', 'loss/train': '0.5572', 'examples_per_second': '119.35', 'grad_norm': '26.025', 'counters/examples': 55488, 'counters/updates': 867}
skipping logging after 55552 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '-0.55213', 'rewards_train/rejected': '-1.0769', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.52481', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-159.65', 'loss/train': '0.54143', 'examples_per_second': '118.97', 'grad_norm': '22.049', 'counters/examples': 55616, 'counters/updates': 869}
skipping logging after 55680 examples to avoid logging too frequently
train stats after 55744 examples: {'rewards_train/chosen': '-0.85398', 'rewards_train/rejected': '-1.2504', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39637', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-160.69', 'loss/train': '0.63436', 'examples_per_second': '124.68', 'grad_norm': '26.255', 'counters/examples': 55744, 'counters/updates': 871}
skipping logging after 55808 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '-0.64088', 'rewards_train/rejected': '-1.0017', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.3608', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-161.65', 'loss/train': '0.61167', 'examples_per_second': '118.69', 'grad_norm': '22.857', 'counters/examples': 55872, 'counters/updates': 873}
skipping logging after 55936 examples to avoid logging too frequently
train stats after 56000 examples: {'rewards_train/chosen': '-0.5592', 'rewards_train/rejected': '-1.0561', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49689', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-164.86', 'loss/train': '0.57754', 'examples_per_second': '124.58', 'grad_norm': '25.222', 'counters/examples': 56000, 'counters/updates': 875}
skipping logging after 56064 examples to avoid logging too frequently
train stats after 56128 examples: {'rewards_train/chosen': '-0.75156', 'rewards_train/rejected': '-1.0067', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2551', 'logps_train/rejected': '-142.36', 'logps_train/chosen': '-168.28', 'loss/train': '0.71964', 'examples_per_second': '121.29', 'grad_norm': '28.628', 'counters/examples': 56128, 'counters/updates': 877}
skipping logging after 56192 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '-0.64557', 'rewards_train/rejected': '-0.83987', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1943', 'logps_train/rejected': '-172.6', 'logps_train/chosen': '-145.72', 'loss/train': '0.72983', 'examples_per_second': '123.13', 'grad_norm': '28.523', 'counters/examples': 56256, 'counters/updates': 879}
skipping logging after 56320 examples to avoid logging too frequently
train stats after 56384 examples: {'rewards_train/chosen': '-0.70754', 'rewards_train/rejected': '-1.0397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.33216', 'logps_train/rejected': '-141.76', 'logps_train/chosen': '-171.92', 'loss/train': '0.63758', 'examples_per_second': '124.11', 'grad_norm': '25.685', 'counters/examples': 56384, 'counters/updates': 881}
skipping logging after 56448 examples to avoid logging too frequently
train stats after 56512 examples: {'rewards_train/chosen': '-0.60534', 'rewards_train/rejected': '-0.95058', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.34524', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-138.41', 'loss/train': '0.63127', 'examples_per_second': '120.54', 'grad_norm': '23.142', 'counters/examples': 56512, 'counters/updates': 883}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56640 examples: {'rewards_train/chosen': '-0.71379', 'rewards_train/rejected': '-1.0425', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.32872', 'logps_train/rejected': '-118.24', 'logps_train/chosen': '-135.06', 'loss/train': '0.62603', 'examples_per_second': '132.94', 'grad_norm': '22.753', 'counters/examples': 56640, 'counters/updates': 885}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56768 examples: {'rewards_train/chosen': '-0.56315', 'rewards_train/rejected': '-1.0028', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43965', 'logps_train/rejected': '-132.44', 'logps_train/chosen': '-157.25', 'loss/train': '0.5854', 'examples_per_second': '124.48', 'grad_norm': '22.898', 'counters/examples': 56768, 'counters/updates': 887}
skipping logging after 56832 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '-0.60996', 'rewards_train/rejected': '-1.1846', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57459', 'logps_train/rejected': '-133.22', 'logps_train/chosen': '-145.39', 'loss/train': '0.54186', 'examples_per_second': '144.93', 'grad_norm': '20.634', 'counters/examples': 56896, 'counters/updates': 889}
skipping logging after 56960 examples to avoid logging too frequently
train stats after 57024 examples: {'rewards_train/chosen': '-0.57137', 'rewards_train/rejected': '-1.1751', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.60368', 'logps_train/rejected': '-153.74', 'logps_train/chosen': '-154.51', 'loss/train': '0.56252', 'examples_per_second': '119.13', 'grad_norm': '22.14', 'counters/examples': 57024, 'counters/updates': 891}
skipping logging after 57088 examples to avoid logging too frequently
train stats after 57152 examples: {'rewards_train/chosen': '-0.67818', 'rewards_train/rejected': '-1.1889', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.5107', 'logps_train/rejected': '-159.57', 'logps_train/chosen': '-139.6', 'loss/train': '0.57437', 'examples_per_second': '124.48', 'grad_norm': '22.973', 'counters/examples': 57152, 'counters/updates': 893}
skipping logging after 57216 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '-0.71504', 'rewards_train/rejected': '-1.1141', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39909', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-155.2', 'loss/train': '0.62383', 'examples_per_second': '123.46', 'grad_norm': '25.45', 'counters/examples': 57280, 'counters/updates': 895}
skipping logging after 57344 examples to avoid logging too frequently
train stats after 57408 examples: {'rewards_train/chosen': '-0.60419', 'rewards_train/rejected': '-1.2142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.60999', 'logps_train/rejected': '-166.87', 'logps_train/chosen': '-172.6', 'loss/train': '0.5964', 'examples_per_second': '124.24', 'grad_norm': '24.615', 'counters/examples': 57408, 'counters/updates': 897}
skipping logging after 57472 examples to avoid logging too frequently
train stats after 57536 examples: {'rewards_train/chosen': '-0.67164', 'rewards_train/rejected': '-1.0534', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.38176', 'logps_train/rejected': '-146.51', 'logps_train/chosen': '-153.78', 'loss/train': '0.61755', 'examples_per_second': '125.98', 'grad_norm': '27.269', 'counters/examples': 57536, 'counters/updates': 899}
skipping logging after 57600 examples to avoid logging too frequently
train stats after 57664 examples: {'rewards_train/chosen': '-0.58114', 'rewards_train/rejected': '-0.90245', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.32131', 'logps_train/rejected': '-149.26', 'logps_train/chosen': '-149.35', 'loss/train': '0.64868', 'examples_per_second': '120.46', 'grad_norm': '23.926', 'counters/examples': 57664, 'counters/updates': 901}
skipping logging after 57728 examples to avoid logging too frequently
train stats after 57792 examples: {'rewards_train/chosen': '-0.71679', 'rewards_train/rejected': '-0.98761', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27081', 'logps_train/rejected': '-170.07', 'logps_train/chosen': '-182.01', 'loss/train': '0.67631', 'examples_per_second': '124.4', 'grad_norm': '26.158', 'counters/examples': 57792, 'counters/updates': 903}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57920 examples: {'rewards_train/chosen': '-0.59341', 'rewards_train/rejected': '-1.0879', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49452', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-143.58', 'loss/train': '0.55657', 'examples_per_second': '125.31', 'grad_norm': '22.553', 'counters/examples': 57920, 'counters/updates': 905}
skipping logging after 57984 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '-0.55897', 'rewards_train/rejected': '-1.1748', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.61587', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-132.6', 'loss/train': '0.53403', 'examples_per_second': '157.51', 'grad_norm': '20.506', 'counters/examples': 58048, 'counters/updates': 907}
skipping logging after 58112 examples to avoid logging too frequently
train stats after 58176 examples: {'rewards_train/chosen': '-0.69671', 'rewards_train/rejected': '-1.1169', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.42016', 'logps_train/rejected': '-119.86', 'logps_train/chosen': '-164.05', 'loss/train': '0.59839', 'examples_per_second': '119.39', 'grad_norm': '23.795', 'counters/examples': 58176, 'counters/updates': 909}
skipping logging after 58240 examples to avoid logging too frequently
train stats after 58304 examples: {'rewards_train/chosen': '-0.60959', 'rewards_train/rejected': '-1.3599', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.7503', 'logps_train/rejected': '-148.13', 'logps_train/chosen': '-156.78', 'loss/train': '0.4847', 'examples_per_second': '126.34', 'grad_norm': '21.523', 'counters/examples': 58304, 'counters/updates': 911}
skipping logging after 58368 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '-0.95263', 'rewards_train/rejected': '-1.5567', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.60402', 'logps_train/rejected': '-139.08', 'logps_train/chosen': '-138.28', 'loss/train': '0.54831', 'examples_per_second': '132.87', 'grad_norm': '20.882', 'counters/examples': 58432, 'counters/updates': 913}
skipping logging after 58496 examples to avoid logging too frequently
train stats after 58560 examples: {'rewards_train/chosen': '-0.72681', 'rewards_train/rejected': '-1.1999', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47309', 'logps_train/rejected': '-153.48', 'logps_train/chosen': '-171.85', 'loss/train': '0.59891', 'examples_per_second': '120.93', 'grad_norm': '26.139', 'counters/examples': 58560, 'counters/updates': 915}
skipping logging after 58624 examples to avoid logging too frequently
train stats after 58688 examples: {'rewards_train/chosen': '-1.0512', 'rewards_train/rejected': '-1.2705', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21928', 'logps_train/rejected': '-157.76', 'logps_train/chosen': '-146.74', 'loss/train': '0.68522', 'examples_per_second': '118.47', 'grad_norm': '26.789', 'counters/examples': 58688, 'counters/updates': 917}
skipping logging after 58752 examples to avoid logging too frequently
train stats after 58816 examples: {'rewards_train/chosen': '-0.8076', 'rewards_train/rejected': '-1.1644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.35681', 'logps_train/rejected': '-166.59', 'logps_train/chosen': '-168.59', 'loss/train': '0.64097', 'examples_per_second': '124.42', 'grad_norm': '25.907', 'counters/examples': 58816, 'counters/updates': 919}
skipping logging after 58880 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '-0.58969', 'rewards_train/rejected': '-1.2139', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.62426', 'logps_train/rejected': '-128.73', 'logps_train/chosen': '-153.66', 'loss/train': '0.55641', 'examples_per_second': '124.48', 'grad_norm': '22.903', 'counters/examples': 58944, 'counters/updates': 921}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59072 examples: {'rewards_train/chosen': '-0.49323', 'rewards_train/rejected': '-0.90292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.40969', 'logps_train/rejected': '-153.96', 'logps_train/chosen': '-142.57', 'loss/train': '0.63053', 'examples_per_second': '117.32', 'grad_norm': '25.562', 'counters/examples': 59072, 'counters/updates': 923}
skipping logging after 59136 examples to avoid logging too frequently
train stats after 59200 examples: {'rewards_train/chosen': '-0.55514', 'rewards_train/rejected': '-0.89919', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34405', 'logps_train/rejected': '-150.32', 'logps_train/chosen': '-149.77', 'loss/train': '0.61698', 'examples_per_second': '124.52', 'grad_norm': '23.132', 'counters/examples': 59200, 'counters/updates': 925}
skipping logging after 59264 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '-0.35612', 'rewards_train/rejected': '-0.83785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.48172', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-128.14', 'loss/train': '0.58975', 'examples_per_second': '120.88', 'grad_norm': '24.032', 'counters/examples': 59328, 'counters/updates': 927}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59456 examples: {'rewards_train/chosen': '-0.43548', 'rewards_train/rejected': '-0.77741', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.34193', 'logps_train/rejected': '-157.31', 'logps_train/chosen': '-167.04', 'loss/train': '0.62875', 'examples_per_second': '123.53', 'grad_norm': '26.955', 'counters/examples': 59456, 'counters/updates': 929}
skipping logging after 59520 examples to avoid logging too frequently
train stats after 59584 examples: {'rewards_train/chosen': '-0.38029', 'rewards_train/rejected': '-0.71397', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33369', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-143.99', 'loss/train': '0.61932', 'examples_per_second': '124.69', 'grad_norm': '24.754', 'counters/examples': 59584, 'counters/updates': 931}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59712 examples: {'rewards_train/chosen': '-0.4565', 'rewards_train/rejected': '-1.0158', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.55927', 'logps_train/rejected': '-135.36', 'logps_train/chosen': '-163.97', 'loss/train': '0.52216', 'examples_per_second': '124.24', 'grad_norm': '22.381', 'counters/examples': 59712, 'counters/updates': 933}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59840 examples: {'rewards_train/chosen': '-0.45896', 'rewards_train/rejected': '-0.70299', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.24403', 'logps_train/rejected': '-134.69', 'logps_train/chosen': '-146.18', 'loss/train': '0.68295', 'examples_per_second': '130.26', 'grad_norm': '27.954', 'counters/examples': 59840, 'counters/updates': 935}
Running evaluation after 59840 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.13it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.12it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.29it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.31it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.33it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.33it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.32it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.24it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.19it/s]
eval after 59840: {'rewards_eval/chosen': '-0.40436', 'rewards_eval/rejected': '-0.82614', 'rewards_eval/accuracies': '0.67578', 'rewards_eval/margins': '0.42178', 'logps_eval/rejected': '-134.34', 'logps_eval/chosen': '-151.95', 'loss/eval': '0.6312'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-59840...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-59840/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-59840/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-59840/scheduler.pt...
train stats after 59904 examples: {'rewards_train/chosen': '-0.58561', 'rewards_train/rejected': '-1.0111', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.42545', 'logps_train/rejected': '-133.6', 'logps_train/chosen': '-142.34', 'loss/train': '0.64257', 'examples_per_second': '109.19', 'grad_norm': '22.995', 'counters/examples': 59904, 'counters/updates': 936}
skipping logging after 59968 examples to avoid logging too frequently
train stats after 60032 examples: {'rewards_train/chosen': '-0.42944', 'rewards_train/rejected': '-1.1586', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.72914', 'logps_train/rejected': '-165.62', 'logps_train/chosen': '-175.84', 'loss/train': '0.50703', 'examples_per_second': '119.7', 'grad_norm': '24.977', 'counters/examples': 60032, 'counters/updates': 938}
skipping logging after 60096 examples to avoid logging too frequently
train stats after 60160 examples: {'rewards_train/chosen': '-0.38386', 'rewards_train/rejected': '-0.77752', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.39366', 'logps_train/rejected': '-135.86', 'logps_train/chosen': '-168.86', 'loss/train': '0.60491', 'examples_per_second': '123.78', 'grad_norm': '25.91', 'counters/examples': 60160, 'counters/updates': 940}
skipping logging after 60224 examples to avoid logging too frequently
train stats after 60288 examples: {'rewards_train/chosen': '-0.25217', 'rewards_train/rejected': '-0.66646', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.4143', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-136.86', 'loss/train': '0.60727', 'examples_per_second': '118.17', 'grad_norm': '22.471', 'counters/examples': 60288, 'counters/updates': 942}
skipping logging after 60352 examples to avoid logging too frequently
train stats after 60416 examples: {'rewards_train/chosen': '-0.53466', 'rewards_train/rejected': '-1.0349', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.50023', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-160.46', 'loss/train': '0.61384', 'examples_per_second': '124.54', 'grad_norm': '26.156', 'counters/examples': 60416, 'counters/updates': 944}
skipping logging after 60480 examples to avoid logging too frequently
train stats after 60544 examples: {'rewards_train/chosen': '-0.41396', 'rewards_train/rejected': '-0.61796', 'rewards_train/accuracies': '0.51562', 'rewards_train/margins': '0.20401', 'logps_train/rejected': '-127', 'logps_train/chosen': '-135.18', 'loss/train': '0.68037', 'examples_per_second': '126.64', 'grad_norm': '25.298', 'counters/examples': 60544, 'counters/updates': 946}
skipping logging after 60608 examples to avoid logging too frequently
train stats after 60672 examples: {'rewards_train/chosen': '-0.45616', 'rewards_train/rejected': '-0.88616', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.42999', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-151.86', 'loss/train': '0.58483', 'examples_per_second': '120.44', 'grad_norm': '23.587', 'counters/examples': 60672, 'counters/updates': 948}
skipping logging after 60736 examples to avoid logging too frequently
train stats after 60800 examples: {'rewards_train/chosen': '-0.52196', 'rewards_train/rejected': '-1.0952', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5732', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-136.59', 'loss/train': '0.59439', 'examples_per_second': '119.2', 'grad_norm': '22.947', 'counters/examples': 60800, 'counters/updates': 950}
skipping logging after 60864 examples to avoid logging too frequently
train stats after 60928 examples: {'rewards_train/chosen': '-0.63861', 'rewards_train/rejected': '-0.94542', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30682', 'logps_train/rejected': '-140.06', 'logps_train/chosen': '-160.03', 'loss/train': '0.63556', 'examples_per_second': '124.68', 'grad_norm': '25.311', 'counters/examples': 60928, 'counters/updates': 952}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': '-0.68345', 'rewards_train/rejected': '-1.0465', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.36302', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-153.45', 'loss/train': '0.61348', 'examples_per_second': '145.78', 'grad_norm': '22.487', 'counters/examples': 61056, 'counters/updates': 954}
skipping logging after 61120 examples to avoid logging too frequently
train stats after 61184 examples: {'rewards_train/chosen': '-0.51889', 'rewards_train/rejected': '-1.0145', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.49565', 'logps_train/rejected': '-160.98', 'logps_train/chosen': '-167.87', 'loss/train': '0.59533', 'examples_per_second': '118.2', 'grad_norm': '24.609', 'counters/examples': 61184, 'counters/updates': 956}
skipping logging after 61248 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '-0.64741', 'rewards_train/rejected': '-1.1877', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54029', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-157.34', 'loss/train': '0.5321', 'examples_per_second': '119.23', 'grad_norm': '22.39', 'counters/examples': 61312, 'counters/updates': 958}
train stats after 61376 examples: {'rewards_train/chosen': '-0.53531', 'rewards_train/rejected': '-0.86671', 'rewards_train/accuracies': '0.48438', 'rewards_train/margins': '0.33141', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-149.75', 'loss/train': '0.65878', 'examples_per_second': '67.817', 'grad_norm': '24.304', 'counters/examples': 61376, 'counters/updates': 959}
skipping logging after 61440 examples to avoid logging too frequently
train stats after 61504 examples: {'rewards_train/chosen': '-0.49783', 'rewards_train/rejected': '-0.943', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44517', 'logps_train/rejected': '-149.77', 'logps_train/chosen': '-158.23', 'loss/train': '0.59839', 'examples_per_second': '118.29', 'grad_norm': '25.182', 'counters/examples': 61504, 'counters/updates': 961}
skipping logging after 61568 examples to avoid logging too frequently
train stats after 61632 examples: {'rewards_train/chosen': '-0.35593', 'rewards_train/rejected': '-1.0285', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.67259', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-145.94', 'loss/train': '0.50725', 'examples_per_second': '124.54', 'grad_norm': '20.367', 'counters/examples': 61632, 'counters/updates': 963}
skipping logging after 61696 examples to avoid logging too frequently
train stats after 61760 examples: {'rewards_train/chosen': '-0.45277', 'rewards_train/rejected': '-0.97371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52095', 'logps_train/rejected': '-154.92', 'logps_train/chosen': '-156.87', 'loss/train': '0.5644', 'examples_per_second': '126.56', 'grad_norm': '22.584', 'counters/examples': 61760, 'counters/updates': 965}
skipping logging after 61824 examples to avoid logging too frequently
train stats after 61888 examples: {'rewards_train/chosen': '-0.50391', 'rewards_train/rejected': '-0.99163', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48772', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-142.05', 'loss/train': '0.54626', 'examples_per_second': '124.64', 'grad_norm': '22.552', 'counters/examples': 61888, 'counters/updates': 967}
skipping logging after 61952 examples to avoid logging too frequently
train stats after 62016 examples: {'rewards_train/chosen': '-0.48543', 'rewards_train/rejected': '-1.0055', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.52009', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-160.19', 'loss/train': '0.59018', 'examples_per_second': '124.28', 'grad_norm': '25.41', 'counters/examples': 62016, 'counters/updates': 969}
skipping logging after 62080 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '-0.53341', 'rewards_train/rejected': '-1.1071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.57364', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-160.6', 'loss/train': '0.56241', 'examples_per_second': '124.23', 'grad_norm': '24.468', 'counters/examples': 62144, 'counters/updates': 971}
skipping logging after 62208 examples to avoid logging too frequently
train stats after 62272 examples: {'rewards_train/chosen': '-0.54894', 'rewards_train/rejected': '-0.80663', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.2577', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-155.1', 'loss/train': '0.70212', 'examples_per_second': '124.19', 'grad_norm': '28.515', 'counters/examples': 62272, 'counters/updates': 973}
skipping logging after 62336 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '-0.50688', 'rewards_train/rejected': '-1.085', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.57815', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-138.92', 'loss/train': '0.5172', 'examples_per_second': '127.58', 'grad_norm': '21.331', 'counters/examples': 62400, 'counters/updates': 975}
skipping logging after 62464 examples to avoid logging too frequently
train stats after 62528 examples: {'rewards_train/chosen': '-0.58556', 'rewards_train/rejected': '-1.1647', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.57911', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-155.53', 'loss/train': '0.5907', 'examples_per_second': '127.46', 'grad_norm': '24.519', 'counters/examples': 62528, 'counters/updates': 977}
train stats after 62592 examples: {'rewards_train/chosen': '-0.75602', 'rewards_train/rejected': '-1.1675', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41153', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-170.68', 'loss/train': '0.62397', 'examples_per_second': '56.64', 'grad_norm': '26.002', 'counters/examples': 62592, 'counters/updates': 978}
skipping logging after 62656 examples to avoid logging too frequently
train stats after 62720 examples: {'rewards_train/chosen': '-0.7659', 'rewards_train/rejected': '-1.3307', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56477', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-142.82', 'loss/train': '0.59728', 'examples_per_second': '121.96', 'grad_norm': '23.808', 'counters/examples': 62720, 'counters/updates': 980}
skipping logging after 62784 examples to avoid logging too frequently
train stats after 62848 examples: {'rewards_train/chosen': '-0.84761', 'rewards_train/rejected': '-1.2752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.4276', 'logps_train/rejected': '-165.64', 'logps_train/chosen': '-127.66', 'loss/train': '0.62316', 'examples_per_second': '117.68', 'grad_norm': '25.97', 'counters/examples': 62848, 'counters/updates': 982}
skipping logging after 62912 examples to avoid logging too frequently
train stats after 62976 examples: {'rewards_train/chosen': '-0.77053', 'rewards_train/rejected': '-1.2403', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.46972', 'logps_train/rejected': '-150.83', 'logps_train/chosen': '-170.78', 'loss/train': '0.63174', 'examples_per_second': '78.72', 'grad_norm': '27.685', 'counters/examples': 62976, 'counters/updates': 984}
skipping logging after 63040 examples to avoid logging too frequently
train stats after 63104 examples: {'rewards_train/chosen': '-0.63776', 'rewards_train/rejected': '-1.1', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.46227', 'logps_train/rejected': '-146.88', 'logps_train/chosen': '-148.88', 'loss/train': '0.62158', 'examples_per_second': '122.05', 'grad_norm': '26.677', 'counters/examples': 63104, 'counters/updates': 986}
skipping logging after 63168 examples to avoid logging too frequently
train stats after 63232 examples: {'rewards_train/chosen': '-0.59672', 'rewards_train/rejected': '-0.98856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.39184', 'logps_train/rejected': '-141.4', 'logps_train/chosen': '-148.42', 'loss/train': '0.60345', 'examples_per_second': '122.97', 'grad_norm': '22.921', 'counters/examples': 63232, 'counters/updates': 988}
skipping logging after 63296 examples to avoid logging too frequently
train stats after 63360 examples: {'rewards_train/chosen': '-0.54892', 'rewards_train/rejected': '-1.1144', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56551', 'logps_train/rejected': '-152.05', 'logps_train/chosen': '-142', 'loss/train': '0.58118', 'examples_per_second': '126.65', 'grad_norm': '21.558', 'counters/examples': 63360, 'counters/updates': 990}
skipping logging after 63424 examples to avoid logging too frequently
train stats after 63488 examples: {'rewards_train/chosen': '-0.56838', 'rewards_train/rejected': '-0.83941', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.27103', 'logps_train/rejected': '-135.84', 'logps_train/chosen': '-146.37', 'loss/train': '0.66081', 'examples_per_second': '123.67', 'grad_norm': '26.047', 'counters/examples': 63488, 'counters/updates': 992}
skipping logging after 63552 examples to avoid logging too frequently
train stats after 63616 examples: {'rewards_train/chosen': '-0.33781', 'rewards_train/rejected': '-0.73226', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39445', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-169.63', 'loss/train': '0.6149', 'examples_per_second': '124.14', 'grad_norm': '27.393', 'counters/examples': 63616, 'counters/updates': 994}
skipping logging after 63680 examples to avoid logging too frequently
train stats after 63744 examples: {'rewards_train/chosen': '-0.43817', 'rewards_train/rejected': '-0.96528', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.52711', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-185.91', 'loss/train': '0.54637', 'examples_per_second': '133.52', 'grad_norm': '22.602', 'counters/examples': 63744, 'counters/updates': 996}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63872 examples: {'rewards_train/chosen': '-0.63035', 'rewards_train/rejected': '-0.79237', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16202', 'logps_train/rejected': '-144.01', 'logps_train/chosen': '-150.38', 'loss/train': '0.69994', 'examples_per_second': '123.08', 'grad_norm': '29.635', 'counters/examples': 63872, 'counters/updates': 998}
skipping logging after 63936 examples to avoid logging too frequently
train stats after 64000 examples: {'rewards_train/chosen': '-0.45812', 'rewards_train/rejected': '-0.89488', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.43676', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-173.74', 'loss/train': '0.60323', 'examples_per_second': '123.92', 'grad_norm': '27.049', 'counters/examples': 64000, 'counters/updates': 1000}
skipping logging after 64064 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '-0.807', 'rewards_train/rejected': '-1.1593', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.35234', 'logps_train/rejected': '-141.15', 'logps_train/chosen': '-152.84', 'loss/train': '0.66432', 'examples_per_second': '124.2', 'grad_norm': '25.359', 'counters/examples': 64128, 'counters/updates': 1002}
skipping logging after 64192 examples to avoid logging too frequently
train stats after 64256 examples: {'rewards_train/chosen': '-0.57472', 'rewards_train/rejected': '-0.97427', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.39955', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-140.89', 'loss/train': '0.59326', 'examples_per_second': '127.3', 'grad_norm': '23.451', 'counters/examples': 64256, 'counters/updates': 1004}
skipping logging after 64320 examples to avoid logging too frequently
train stats after 64384 examples: {'rewards_train/chosen': '-0.49234', 'rewards_train/rejected': '-0.82239', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.33005', 'logps_train/rejected': '-153.15', 'logps_train/chosen': '-156.36', 'loss/train': '0.65887', 'examples_per_second': '123.29', 'grad_norm': '28.265', 'counters/examples': 64384, 'counters/updates': 1006}
skipping logging after 64448 examples to avoid logging too frequently
train stats after 64512 examples: {'rewards_train/chosen': '-0.50569', 'rewards_train/rejected': '-0.9759', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47021', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-125.37', 'loss/train': '0.54495', 'examples_per_second': '124.1', 'grad_norm': '20.561', 'counters/examples': 64512, 'counters/updates': 1008}
skipping logging after 64576 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '-0.56584', 'rewards_train/rejected': '-1.0002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43432', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-147.29', 'loss/train': '0.61908', 'examples_per_second': '122.69', 'grad_norm': '26.1', 'counters/examples': 64640, 'counters/updates': 1010}
skipping logging after 64704 examples to avoid logging too frequently
train stats after 64768 examples: {'rewards_train/chosen': '-0.63541', 'rewards_train/rejected': '-1.2239', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.58853', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-142.14', 'loss/train': '0.53355', 'examples_per_second': '119.06', 'grad_norm': '21.889', 'counters/examples': 64768, 'counters/updates': 1012}
skipping logging after 64832 examples to avoid logging too frequently
train stats after 64896 examples: {'rewards_train/chosen': '-0.55257', 'rewards_train/rejected': '-0.97294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42037', 'logps_train/rejected': '-173.69', 'logps_train/chosen': '-184.41', 'loss/train': '0.60929', 'examples_per_second': '120.5', 'grad_norm': '26.541', 'counters/examples': 64896, 'counters/updates': 1014}
skipping logging after 64960 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '-0.64296', 'rewards_train/rejected': '-1.0917', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.44875', 'logps_train/rejected': '-126.07', 'logps_train/chosen': '-137.01', 'loss/train': '0.63054', 'examples_per_second': '119.83', 'grad_norm': '23.584', 'counters/examples': 65024, 'counters/updates': 1016}
skipping logging after 65088 examples to avoid logging too frequently
train stats after 65152 examples: {'rewards_train/chosen': '-0.47704', 'rewards_train/rejected': '-1.158', 'rewards_train/accuracies': '0.82812', 'rewards_train/margins': '0.68092', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-159.28', 'loss/train': '0.48292', 'examples_per_second': '124.5', 'grad_norm': '20.578', 'counters/examples': 65152, 'counters/updates': 1018}
skipping logging after 65216 examples to avoid logging too frequently
train stats after 65280 examples: {'rewards_train/chosen': '-0.69694', 'rewards_train/rejected': '-1.4693', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.77235', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-152.54', 'loss/train': '0.48744', 'examples_per_second': '124.44', 'grad_norm': '22.877', 'counters/examples': 65280, 'counters/updates': 1020}
skipping logging after 65344 examples to avoid logging too frequently
train stats after 65408 examples: {'rewards_train/chosen': '-0.95789', 'rewards_train/rejected': '-1.5762', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61833', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-187.98', 'loss/train': '0.54439', 'examples_per_second': '124.46', 'grad_norm': '24.744', 'counters/examples': 65408, 'counters/updates': 1022}
skipping logging after 65472 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '-0.74723', 'rewards_train/rejected': '-1.1359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.38868', 'logps_train/rejected': '-146.09', 'logps_train/chosen': '-153.71', 'loss/train': '0.60368', 'examples_per_second': '128.31', 'grad_norm': '24.006', 'counters/examples': 65536, 'counters/updates': 1024}
skipping logging after 65600 examples to avoid logging too frequently
train stats after 65664 examples: {'rewards_train/chosen': '-0.72851', 'rewards_train/rejected': '-1.3724', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.64392', 'logps_train/rejected': '-130.78', 'logps_train/chosen': '-171', 'loss/train': '0.59012', 'examples_per_second': '128.06', 'grad_norm': '23.113', 'counters/examples': 65664, 'counters/updates': 1026}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65792 examples: {'rewards_train/chosen': '-0.83995', 'rewards_train/rejected': '-1.1314', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.29146', 'logps_train/rejected': '-138.54', 'logps_train/chosen': '-131.55', 'loss/train': '0.68278', 'examples_per_second': '124.57', 'grad_norm': '26.339', 'counters/examples': 65792, 'counters/updates': 1028}
skipping logging after 65856 examples to avoid logging too frequently
train stats after 65920 examples: {'rewards_train/chosen': '-0.66934', 'rewards_train/rejected': '-1.2375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.5682', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-153.65', 'loss/train': '0.60736', 'examples_per_second': '124.09', 'grad_norm': '23.453', 'counters/examples': 65920, 'counters/updates': 1030}
skipping logging after 65984 examples to avoid logging too frequently
train stats after 66048 examples: {'rewards_train/chosen': '-0.81115', 'rewards_train/rejected': '-1.3104', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49922', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-170.03', 'loss/train': '0.58637', 'examples_per_second': '123.57', 'grad_norm': '27.431', 'counters/examples': 66048, 'counters/updates': 1032}
skipping logging after 66112 examples to avoid logging too frequently
train stats after 66176 examples: {'rewards_train/chosen': '-0.68602', 'rewards_train/rejected': '-1.0084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.32236', 'logps_train/rejected': '-145.09', 'logps_train/chosen': '-146.65', 'loss/train': '0.63517', 'examples_per_second': '121.97', 'grad_norm': '25.515', 'counters/examples': 66176, 'counters/updates': 1034}
skipping logging after 66240 examples to avoid logging too frequently
train stats after 66304 examples: {'rewards_train/chosen': '-0.51543', 'rewards_train/rejected': '-1.1994', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.68401', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-158.44', 'loss/train': '0.51258', 'examples_per_second': '118.07', 'grad_norm': '21.377', 'counters/examples': 66304, 'counters/updates': 1036}
skipping logging after 66368 examples to avoid logging too frequently
train stats after 66432 examples: {'rewards_train/chosen': '-0.73664', 'rewards_train/rejected': '-1.0405', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.3039', 'logps_train/rejected': '-153.38', 'logps_train/chosen': '-154.58', 'loss/train': '0.68587', 'examples_per_second': '119.28', 'grad_norm': '27.931', 'counters/examples': 66432, 'counters/updates': 1038}
skipping logging after 66496 examples to avoid logging too frequently
train stats after 66560 examples: {'rewards_train/chosen': '-0.57903', 'rewards_train/rejected': '-1.2509', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.67184', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-163.09', 'loss/train': '0.53627', 'examples_per_second': '120.92', 'grad_norm': '22.48', 'counters/examples': 66560, 'counters/updates': 1040}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66688 examples: {'rewards_train/chosen': '-0.73954', 'rewards_train/rejected': '-1.2868', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.54721', 'logps_train/rejected': '-124.51', 'logps_train/chosen': '-148.35', 'loss/train': '0.59627', 'examples_per_second': '134.92', 'grad_norm': '25.631', 'counters/examples': 66688, 'counters/updates': 1042}
skipping logging after 66752 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '-0.54634', 'rewards_train/rejected': '-1.2032', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.65688', 'logps_train/rejected': '-188.95', 'logps_train/chosen': '-159.48', 'loss/train': '0.49858', 'examples_per_second': '122.73', 'grad_norm': '24.077', 'counters/examples': 66816, 'counters/updates': 1044}
skipping logging after 66880 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '-0.63429', 'rewards_train/rejected': '-1.0519', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41762', 'logps_train/rejected': '-149.1', 'logps_train/chosen': '-148.93', 'loss/train': '0.63735', 'examples_per_second': '132.84', 'grad_norm': '24.091', 'counters/examples': 66944, 'counters/updates': 1046}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67072 examples: {'rewards_train/chosen': '-0.39352', 'rewards_train/rejected': '-0.76574', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.37223', 'logps_train/rejected': '-141.86', 'logps_train/chosen': '-136.23', 'loss/train': '0.63497', 'examples_per_second': '123.19', 'grad_norm': '23.225', 'counters/examples': 67072, 'counters/updates': 1048}
skipping logging after 67136 examples to avoid logging too frequently
train stats after 67200 examples: {'rewards_train/chosen': '-0.45839', 'rewards_train/rejected': '-0.8702', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.41181', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-124.65', 'loss/train': '0.59257', 'examples_per_second': '124.16', 'grad_norm': '22.529', 'counters/examples': 67200, 'counters/updates': 1050}
skipping logging after 67264 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '-0.57976', 'rewards_train/rejected': '-0.88081', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.30105', 'logps_train/rejected': '-147', 'logps_train/chosen': '-125.53', 'loss/train': '0.6455', 'examples_per_second': '127.82', 'grad_norm': '24.43', 'counters/examples': 67328, 'counters/updates': 1052}
skipping logging after 67392 examples to avoid logging too frequently
train stats after 67456 examples: {'rewards_train/chosen': '-0.33557', 'rewards_train/rejected': '-0.75649', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.42092', 'logps_train/rejected': '-135.67', 'logps_train/chosen': '-145.78', 'loss/train': '0.58622', 'examples_per_second': '124.1', 'grad_norm': '24.883', 'counters/examples': 67456, 'counters/updates': 1054}
skipping logging after 67520 examples to avoid logging too frequently
train stats after 67584 examples: {'rewards_train/chosen': '-0.49208', 'rewards_train/rejected': '-0.88342', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.39135', 'logps_train/rejected': '-122.62', 'logps_train/chosen': '-147.97', 'loss/train': '0.59229', 'examples_per_second': '121.54', 'grad_norm': '22.211', 'counters/examples': 67584, 'counters/updates': 1056}
skipping logging after 67648 examples to avoid logging too frequently
train stats after 67712 examples: {'rewards_train/chosen': '-0.35449', 'rewards_train/rejected': '-0.80204', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.44755', 'logps_train/rejected': '-156.92', 'logps_train/chosen': '-161.75', 'loss/train': '0.6093', 'examples_per_second': '119.02', 'grad_norm': '25.229', 'counters/examples': 67712, 'counters/updates': 1058}
skipping logging after 67776 examples to avoid logging too frequently
train stats after 67840 examples: {'rewards_train/chosen': '-0.67395', 'rewards_train/rejected': '-0.91667', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.24271', 'logps_train/rejected': '-159.36', 'logps_train/chosen': '-148.78', 'loss/train': '0.68388', 'examples_per_second': '123.83', 'grad_norm': '26.591', 'counters/examples': 67840, 'counters/updates': 1060}
skipping logging after 67904 examples to avoid logging too frequently
train stats after 67968 examples: {'rewards_train/chosen': '-0.49771', 'rewards_train/rejected': '-0.77567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27796', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-189.31', 'loss/train': '0.69251', 'examples_per_second': '125.2', 'grad_norm': '26.148', 'counters/examples': 67968, 'counters/updates': 1062}
skipping logging after 68032 examples to avoid logging too frequently
train stats after 68096 examples: {'rewards_train/chosen': '-0.34121', 'rewards_train/rejected': '-0.96701', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62579', 'logps_train/rejected': '-148.9', 'logps_train/chosen': '-171.58', 'loss/train': '0.5128', 'examples_per_second': '118.43', 'grad_norm': '22.576', 'counters/examples': 68096, 'counters/updates': 1064}
skipping logging after 68160 examples to avoid logging too frequently
train stats after 68224 examples: {'rewards_train/chosen': '-0.47329', 'rewards_train/rejected': '-0.98132', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.50803', 'logps_train/rejected': '-150.31', 'logps_train/chosen': '-150.41', 'loss/train': '0.55837', 'examples_per_second': '118.66', 'grad_norm': '23.346', 'counters/examples': 68224, 'counters/updates': 1066}
skipping logging after 68288 examples to avoid logging too frequently
train stats after 68352 examples: {'rewards_train/chosen': '-0.57249', 'rewards_train/rejected': '-1.017', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.44455', 'logps_train/rejected': '-174.9', 'logps_train/chosen': '-165.96', 'loss/train': '0.58897', 'examples_per_second': '124.41', 'grad_norm': '24.749', 'counters/examples': 68352, 'counters/updates': 1068}
skipping logging after 68416 examples to avoid logging too frequently
train stats after 68480 examples: {'rewards_train/chosen': '-0.50291', 'rewards_train/rejected': '-0.76639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.26348', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-122.33', 'loss/train': '0.67012', 'examples_per_second': '127.94', 'grad_norm': '24.116', 'counters/examples': 68480, 'counters/updates': 1070}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68608 examples: {'rewards_train/chosen': '-0.54045', 'rewards_train/rejected': '-0.94621', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40576', 'logps_train/rejected': '-154.79', 'logps_train/chosen': '-162.45', 'loss/train': '0.63751', 'examples_per_second': '124.51', 'grad_norm': '26.261', 'counters/examples': 68608, 'counters/updates': 1072}
skipping logging after 68672 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '-0.61436', 'rewards_train/rejected': '-1.196', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.58161', 'logps_train/rejected': '-149.67', 'logps_train/chosen': '-161.8', 'loss/train': '0.55153', 'examples_per_second': '124.41', 'grad_norm': '24.645', 'counters/examples': 68736, 'counters/updates': 1074}
skipping logging after 68800 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '-0.41936', 'rewards_train/rejected': '-0.96157', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.54221', 'logps_train/rejected': '-147.74', 'logps_train/chosen': '-151.43', 'loss/train': '0.55944', 'examples_per_second': '120.35', 'grad_norm': '22.07', 'counters/examples': 68864, 'counters/updates': 1076}
skipping logging after 68928 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '-0.43884', 'rewards_train/rejected': '-0.69185', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25301', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-156.65', 'loss/train': '0.68612', 'examples_per_second': '125.67', 'grad_norm': '27.554', 'counters/examples': 68992, 'counters/updates': 1078}
skipping logging after 69056 examples to avoid logging too frequently
train stats after 69120 examples: {'rewards_train/chosen': '-0.39322', 'rewards_train/rejected': '-0.86499', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.47176', 'logps_train/rejected': '-159.21', 'logps_train/chosen': '-144.1', 'loss/train': '0.59713', 'examples_per_second': '118.16', 'grad_norm': '25.202', 'counters/examples': 69120, 'counters/updates': 1080}
skipping logging after 69184 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '-0.42166', 'rewards_train/rejected': '-0.87338', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.45172', 'logps_train/rejected': '-145.8', 'logps_train/chosen': '-131.82', 'loss/train': '0.56878', 'examples_per_second': '119.86', 'grad_norm': '22.932', 'counters/examples': 69248, 'counters/updates': 1082}
skipping logging after 69312 examples to avoid logging too frequently
train stats after 69376 examples: {'rewards_train/chosen': '-0.56287', 'rewards_train/rejected': '-1.0117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44885', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-142.23', 'loss/train': '0.60931', 'examples_per_second': '118.52', 'grad_norm': '23.006', 'counters/examples': 69376, 'counters/updates': 1084}
skipping logging after 69440 examples to avoid logging too frequently
train stats after 69504 examples: {'rewards_train/chosen': '-0.36384', 'rewards_train/rejected': '-0.78899', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.42515', 'logps_train/rejected': '-145.05', 'logps_train/chosen': '-165.32', 'loss/train': '0.57887', 'examples_per_second': '120.98', 'grad_norm': '22.526', 'counters/examples': 69504, 'counters/updates': 1086}
skipping logging after 69568 examples to avoid logging too frequently
train stats after 69632 examples: {'rewards_train/chosen': '-0.42495', 'rewards_train/rejected': '-0.90259', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47765', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-154.26', 'loss/train': '0.56705', 'examples_per_second': '131.84', 'grad_norm': '23.51', 'counters/examples': 69632, 'counters/updates': 1088}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69760 examples: {'rewards_train/chosen': '-0.76138', 'rewards_train/rejected': '-1.1615', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.40008', 'logps_train/rejected': '-163.86', 'logps_train/chosen': '-147.68', 'loss/train': '0.64483', 'examples_per_second': '126.2', 'grad_norm': '25.571', 'counters/examples': 69760, 'counters/updates': 1090}
skipping logging after 69824 examples to avoid logging too frequently
train stats after 69888 examples: {'rewards_train/chosen': '-0.67517', 'rewards_train/rejected': '-1.1737', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4985', 'logps_train/rejected': '-151.43', 'logps_train/chosen': '-153.62', 'loss/train': '0.57539', 'examples_per_second': '123.11', 'grad_norm': '24.088', 'counters/examples': 69888, 'counters/updates': 1092}
skipping logging after 69952 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '-0.57075', 'rewards_train/rejected': '-0.72946', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15871', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-129.3', 'loss/train': '0.70341', 'examples_per_second': '130.59', 'grad_norm': '27.506', 'counters/examples': 70016, 'counters/updates': 1094}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70144 examples: {'rewards_train/chosen': '-0.55321', 'rewards_train/rejected': '-1.0426', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.48943', 'logps_train/rejected': '-122.69', 'logps_train/chosen': '-157.79', 'loss/train': '0.55247', 'examples_per_second': '118.36', 'grad_norm': '24.168', 'counters/examples': 70144, 'counters/updates': 1096}
skipping logging after 70208 examples to avoid logging too frequently
train stats after 70272 examples: {'rewards_train/chosen': '-0.58087', 'rewards_train/rejected': '-0.99479', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41392', 'logps_train/rejected': '-136.56', 'logps_train/chosen': '-146.81', 'loss/train': '0.57905', 'examples_per_second': '118.78', 'grad_norm': '21.757', 'counters/examples': 70272, 'counters/updates': 1098}
skipping logging after 70336 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '-0.65225', 'rewards_train/rejected': '-1.1519', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.49966', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-153.03', 'loss/train': '0.63814', 'examples_per_second': '120.33', 'grad_norm': '25.347', 'counters/examples': 70400, 'counters/updates': 1100}
skipping logging after 70464 examples to avoid logging too frequently
train stats after 70528 examples: {'rewards_train/chosen': '-0.49775', 'rewards_train/rejected': '-1.2242', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.72642', 'logps_train/rejected': '-151.15', 'logps_train/chosen': '-172.01', 'loss/train': '0.50735', 'examples_per_second': '118.71', 'grad_norm': '23.022', 'counters/examples': 70528, 'counters/updates': 1102}
skipping logging after 70592 examples to avoid logging too frequently
train stats after 70656 examples: {'rewards_train/chosen': '-0.77523', 'rewards_train/rejected': '-1.1938', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.41853', 'logps_train/rejected': '-147.57', 'logps_train/chosen': '-175.56', 'loss/train': '0.60525', 'examples_per_second': '124.08', 'grad_norm': '27.072', 'counters/examples': 70656, 'counters/updates': 1104}
skipping logging after 70720 examples to avoid logging too frequently
train stats after 70784 examples: {'rewards_train/chosen': '-0.83697', 'rewards_train/rejected': '-1.3452', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5082', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-163.86', 'loss/train': '0.58114', 'examples_per_second': '124.17', 'grad_norm': '25.5', 'counters/examples': 70784, 'counters/updates': 1106}
skipping logging after 70848 examples to avoid logging too frequently
train stats after 70912 examples: {'rewards_train/chosen': '-0.81769', 'rewards_train/rejected': '-1.2914', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.47374', 'logps_train/rejected': '-141.24', 'logps_train/chosen': '-174.85', 'loss/train': '0.59324', 'examples_per_second': '124.22', 'grad_norm': '25.338', 'counters/examples': 70912, 'counters/updates': 1108}
skipping logging after 70976 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '-0.69354', 'rewards_train/rejected': '-1.3741', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.68056', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-167.45', 'loss/train': '0.50809', 'examples_per_second': '123.74', 'grad_norm': '22.302', 'counters/examples': 71040, 'counters/updates': 1110}
skipping logging after 71104 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '-0.94058', 'rewards_train/rejected': '-1.4336', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49305', 'logps_train/rejected': '-152.94', 'logps_train/chosen': '-163.38', 'loss/train': '0.59956', 'examples_per_second': '140.44', 'grad_norm': '24.618', 'counters/examples': 71168, 'counters/updates': 1112}
skipping logging after 71232 examples to avoid logging too frequently
train stats after 71296 examples: {'rewards_train/chosen': '-0.66474', 'rewards_train/rejected': '-1.133', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.46825', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-174.55', 'loss/train': '0.60747', 'examples_per_second': '123.7', 'grad_norm': '22.244', 'counters/examples': 71296, 'counters/updates': 1114}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71424 examples: {'rewards_train/chosen': '-0.76566', 'rewards_train/rejected': '-1.1195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35386', 'logps_train/rejected': '-155.62', 'logps_train/chosen': '-132.96', 'loss/train': '0.65927', 'examples_per_second': '118.65', 'grad_norm': '27.178', 'counters/examples': 71424, 'counters/updates': 1116}
skipping logging after 71488 examples to avoid logging too frequently
train stats after 71552 examples: {'rewards_train/chosen': '-0.876', 'rewards_train/rejected': '-1.2534', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37739', 'logps_train/rejected': '-132.76', 'logps_train/chosen': '-138.41', 'loss/train': '0.6635', 'examples_per_second': '122.3', 'grad_norm': '23.862', 'counters/examples': 71552, 'counters/updates': 1118}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71680 examples: {'rewards_train/chosen': '-0.75674', 'rewards_train/rejected': '-1.2764', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.5197', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-159.77', 'loss/train': '0.61529', 'examples_per_second': '124.58', 'grad_norm': '27.433', 'counters/examples': 71680, 'counters/updates': 1120}
skipping logging after 71744 examples to avoid logging too frequently
train stats after 71808 examples: {'rewards_train/chosen': '-0.48315', 'rewards_train/rejected': '-0.83404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35089', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-152.38', 'loss/train': '0.59776', 'examples_per_second': '119.96', 'grad_norm': '23.399', 'counters/examples': 71808, 'counters/updates': 1122}
Running evaluation after 71808 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.25it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01,  9.12it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:01,  9.53it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00,  9.77it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:01<00:00,  9.90it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.03it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.06it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.13it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00,  9.93it/s]
eval after 71808: {'rewards_eval/chosen': '-0.5823', 'rewards_eval/rejected': '-1.0361', 'rewards_eval/accuracies': '0.69922', 'rewards_eval/margins': '0.4538', 'logps_eval/rejected': '-136.44', 'logps_eval/chosen': '-153.73', 'loss/eval': '0.61188'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-71808...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-71808/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-71808/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-71808/scheduler.pt...
train stats after 71872 examples: {'rewards_train/chosen': '-0.63176', 'rewards_train/rejected': '-1.0077', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37596', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-193.68', 'loss/train': '0.59397', 'examples_per_second': '115.99', 'grad_norm': '25.525', 'counters/examples': 71872, 'counters/updates': 1123}
skipping logging after 71936 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '-0.71438', 'rewards_train/rejected': '-1.2627', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.54835', 'logps_train/rejected': '-165.49', 'logps_train/chosen': '-153.67', 'loss/train': '0.58485', 'examples_per_second': '130.42', 'grad_norm': '25.253', 'counters/examples': 72000, 'counters/updates': 1125}
skipping logging after 72064 examples to avoid logging too frequently
train stats after 72128 examples: {'rewards_train/chosen': '-0.64501', 'rewards_train/rejected': '-0.96126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.31626', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-176.86', 'loss/train': '0.66451', 'examples_per_second': '132.06', 'grad_norm': '25.022', 'counters/examples': 72128, 'counters/updates': 1127}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72256 examples: {'rewards_train/chosen': '-0.56541', 'rewards_train/rejected': '-1.1574', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59198', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-151.17', 'loss/train': '0.5056', 'examples_per_second': '124.71', 'grad_norm': '21.873', 'counters/examples': 72256, 'counters/updates': 1129}
skipping logging after 72320 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '-0.79133', 'rewards_train/rejected': '-1.1385', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.34719', 'logps_train/rejected': '-166.9', 'logps_train/chosen': '-150.16', 'loss/train': '0.68325', 'examples_per_second': '130.41', 'grad_norm': '27.449', 'counters/examples': 72384, 'counters/updates': 1131}
skipping logging after 72448 examples to avoid logging too frequently
train stats after 72512 examples: {'rewards_train/chosen': '-0.78161', 'rewards_train/rejected': '-0.99973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21812', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-150.3', 'loss/train': '0.67732', 'examples_per_second': '124.03', 'grad_norm': '25.621', 'counters/examples': 72512, 'counters/updates': 1133}
skipping logging after 72576 examples to avoid logging too frequently
train stats after 72640 examples: {'rewards_train/chosen': '-0.69994', 'rewards_train/rejected': '-1.1664', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.46648', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-142.06', 'loss/train': '0.57138', 'examples_per_second': '130.43', 'grad_norm': '23.651', 'counters/examples': 72640, 'counters/updates': 1135}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72768 examples: {'rewards_train/chosen': '-0.55537', 'rewards_train/rejected': '-1.1148', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.55945', 'logps_train/rejected': '-147.23', 'logps_train/chosen': '-140.86', 'loss/train': '0.57747', 'examples_per_second': '123.91', 'grad_norm': '24.12', 'counters/examples': 72768, 'counters/updates': 1137}
skipping logging after 72832 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '-0.56519', 'rewards_train/rejected': '-0.8754', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.31021', 'logps_train/rejected': '-158.78', 'logps_train/chosen': '-172.4', 'loss/train': '0.61171', 'examples_per_second': '127.81', 'grad_norm': '25.941', 'counters/examples': 72896, 'counters/updates': 1139}
skipping logging after 72960 examples to avoid logging too frequently
train stats after 73024 examples: {'rewards_train/chosen': '-0.60067', 'rewards_train/rejected': '-0.83516', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.23449', 'logps_train/rejected': '-166.1', 'logps_train/chosen': '-157.09', 'loss/train': '0.68463', 'examples_per_second': '124.09', 'grad_norm': '28.553', 'counters/examples': 73024, 'counters/updates': 1141}
skipping logging after 73088 examples to avoid logging too frequently
train stats after 73152 examples: {'rewards_train/chosen': '-0.30378', 'rewards_train/rejected': '-0.96843', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.66465', 'logps_train/rejected': '-163.43', 'logps_train/chosen': '-163.16', 'loss/train': '0.50685', 'examples_per_second': '124.06', 'grad_norm': '25.112', 'counters/examples': 73152, 'counters/updates': 1143}
skipping logging after 73216 examples to avoid logging too frequently
train stats after 73280 examples: {'rewards_train/chosen': '-0.46779', 'rewards_train/rejected': '-1.009', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.54122', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-134.6', 'loss/train': '0.5694', 'examples_per_second': '114.11', 'grad_norm': '26.164', 'counters/examples': 73280, 'counters/updates': 1145}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73408 examples: {'rewards_train/chosen': '-0.57721', 'rewards_train/rejected': '-1.006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.42875', 'logps_train/rejected': '-144.54', 'logps_train/chosen': '-151.9', 'loss/train': '0.59262', 'examples_per_second': '122.45', 'grad_norm': '24.726', 'counters/examples': 73408, 'counters/updates': 1147}
skipping logging after 73472 examples to avoid logging too frequently
train stats after 73536 examples: {'rewards_train/chosen': '-0.49405', 'rewards_train/rejected': '-0.8844', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.39035', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-154.09', 'loss/train': '0.60575', 'examples_per_second': '119.1', 'grad_norm': '24.891', 'counters/examples': 73536, 'counters/updates': 1149}
skipping logging after 73600 examples to avoid logging too frequently
train stats after 73664 examples: {'rewards_train/chosen': '-0.5266', 'rewards_train/rejected': '-1.0263', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.4997', 'logps_train/rejected': '-161.46', 'logps_train/chosen': '-159.73', 'loss/train': '0.59458', 'examples_per_second': '124.55', 'grad_norm': '26.9', 'counters/examples': 73664, 'counters/updates': 1151}
train stats after 73728 examples: {'rewards_train/chosen': '-0.50347', 'rewards_train/rejected': '-1.0209', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.5174', 'logps_train/rejected': '-160.86', 'logps_train/chosen': '-144.94', 'loss/train': '0.57898', 'examples_per_second': '61.854', 'grad_norm': '27.388', 'counters/examples': 73728, 'counters/updates': 1152}
skipping logging after 73792 examples to avoid logging too frequently
train stats after 73856 examples: {'rewards_train/chosen': '-0.30372', 'rewards_train/rejected': '-1.1666', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.86292', 'logps_train/rejected': '-142.89', 'logps_train/chosen': '-154.11', 'loss/train': '0.46422', 'examples_per_second': '124.75', 'grad_norm': '22.544', 'counters/examples': 73856, 'counters/updates': 1154}
skipping logging after 73920 examples to avoid logging too frequently
train stats after 73984 examples: {'rewards_train/chosen': '-0.53604', 'rewards_train/rejected': '-1.1629', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.62687', 'logps_train/rejected': '-141.92', 'logps_train/chosen': '-154.89', 'loss/train': '0.51484', 'examples_per_second': '118.58', 'grad_norm': '21.306', 'counters/examples': 73984, 'counters/updates': 1156}
skipping logging after 74048 examples to avoid logging too frequently
train stats after 74112 examples: {'rewards_train/chosen': '-0.63487', 'rewards_train/rejected': '-0.98855', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.35368', 'logps_train/rejected': '-138.63', 'logps_train/chosen': '-144.85', 'loss/train': '0.67839', 'examples_per_second': '124.55', 'grad_norm': '27.514', 'counters/examples': 74112, 'counters/updates': 1158}
skipping logging after 74176 examples to avoid logging too frequently
train stats after 74240 examples: {'rewards_train/chosen': '-0.5086', 'rewards_train/rejected': '-0.94415', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.43555', 'logps_train/rejected': '-122.05', 'logps_train/chosen': '-156.45', 'loss/train': '0.58541', 'examples_per_second': '124.19', 'grad_norm': '22.717', 'counters/examples': 74240, 'counters/updates': 1160}
skipping logging after 74304 examples to avoid logging too frequently
train stats after 74368 examples: {'rewards_train/chosen': '-0.2849', 'rewards_train/rejected': '-0.75557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47067', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-169.69', 'loss/train': '0.58472', 'examples_per_second': '129.51', 'grad_norm': '25.036', 'counters/examples': 74368, 'counters/updates': 1162}
skipping logging after 74432 examples to avoid logging too frequently
train stats after 74496 examples: {'rewards_train/chosen': '-0.41185', 'rewards_train/rejected': '-0.88092', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.46907', 'logps_train/rejected': '-155.03', 'logps_train/chosen': '-158.69', 'loss/train': '0.62594', 'examples_per_second': '125.32', 'grad_norm': '25.208', 'counters/examples': 74496, 'counters/updates': 1164}
skipping logging after 74560 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '-0.40047', 'rewards_train/rejected': '-0.85382', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45335', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-138.11', 'loss/train': '0.62257', 'examples_per_second': '121.29', 'grad_norm': '22.858', 'counters/examples': 74624, 'counters/updates': 1166}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74752 examples: {'rewards_train/chosen': '-0.5609', 'rewards_train/rejected': '-1.1465', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.58555', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-168.08', 'loss/train': '0.58499', 'examples_per_second': '122.38', 'grad_norm': '25.435', 'counters/examples': 74752, 'counters/updates': 1168}
skipping logging after 74816 examples to avoid logging too frequently
train stats after 74880 examples: {'rewards_train/chosen': '-0.56301', 'rewards_train/rejected': '-1.0414', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.47843', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-148.81', 'loss/train': '0.57538', 'examples_per_second': '122.5', 'grad_norm': '22.449', 'counters/examples': 74880, 'counters/updates': 1170}
skipping logging after 74944 examples to avoid logging too frequently
train stats after 75008 examples: {'rewards_train/chosen': '-0.54922', 'rewards_train/rejected': '-0.94841', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39919', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-151.3', 'loss/train': '0.64133', 'examples_per_second': '124.62', 'grad_norm': '26.126', 'counters/examples': 75008, 'counters/updates': 1172}
skipping logging after 75072 examples to avoid logging too frequently
train stats after 75136 examples: {'rewards_train/chosen': '-0.58625', 'rewards_train/rejected': '-0.91825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.332', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-132.25', 'loss/train': '0.66128', 'examples_per_second': '123.82', 'grad_norm': '23.917', 'counters/examples': 75136, 'counters/updates': 1174}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75264 examples: {'rewards_train/chosen': '-0.39411', 'rewards_train/rejected': '-0.88642', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.49231', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-164.26', 'loss/train': '0.58258', 'examples_per_second': '121.82', 'grad_norm': '23.292', 'counters/examples': 75264, 'counters/updates': 1176}
skipping logging after 75328 examples to avoid logging too frequently
train stats after 75392 examples: {'rewards_train/chosen': '-0.59212', 'rewards_train/rejected': '-1.1081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.51598', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-134.16', 'loss/train': '0.58957', 'examples_per_second': '118.56', 'grad_norm': '25.992', 'counters/examples': 75392, 'counters/updates': 1178}
skipping logging after 75456 examples to avoid logging too frequently
train stats after 75520 examples: {'rewards_train/chosen': '-0.48603', 'rewards_train/rejected': '-1.0659', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.57991', 'logps_train/rejected': '-154.27', 'logps_train/chosen': '-177.49', 'loss/train': '0.57759', 'examples_per_second': '124.77', 'grad_norm': '22.466', 'counters/examples': 75520, 'counters/updates': 1180}
skipping logging after 75584 examples to avoid logging too frequently
train stats after 75648 examples: {'rewards_train/chosen': '-0.53657', 'rewards_train/rejected': '-0.98889', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45231', 'logps_train/rejected': '-144.02', 'logps_train/chosen': '-157.16', 'loss/train': '0.58389', 'examples_per_second': '126.51', 'grad_norm': '26.756', 'counters/examples': 75648, 'counters/updates': 1182}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75776 examples: {'rewards_train/chosen': '-0.34166', 'rewards_train/rejected': '-0.84968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50802', 'logps_train/rejected': '-128.51', 'logps_train/chosen': '-152.49', 'loss/train': '0.5929', 'examples_per_second': '119.48', 'grad_norm': '23.585', 'counters/examples': 75776, 'counters/updates': 1184}
skipping logging after 75840 examples to avoid logging too frequently
train stats after 75904 examples: {'rewards_train/chosen': '-0.47667', 'rewards_train/rejected': '-0.77121', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29454', 'logps_train/rejected': '-120.31', 'logps_train/chosen': '-147.33', 'loss/train': '0.6305', 'examples_per_second': '124.04', 'grad_norm': '23.581', 'counters/examples': 75904, 'counters/updates': 1186}
skipping logging after 75968 examples to avoid logging too frequently
train stats after 76032 examples: {'rewards_train/chosen': '-0.61824', 'rewards_train/rejected': '-0.99036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.37213', 'logps_train/rejected': '-156.93', 'logps_train/chosen': '-159.24', 'loss/train': '0.64202', 'examples_per_second': '124.51', 'grad_norm': '24.953', 'counters/examples': 76032, 'counters/updates': 1188}
skipping logging after 76096 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '-0.59778', 'rewards_train/rejected': '-0.94771', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.34992', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-145.12', 'loss/train': '0.63006', 'examples_per_second': '143.43', 'grad_norm': '27.72', 'counters/examples': 76160, 'counters/updates': 1190}
skipping logging after 76224 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '-0.15818', 'rewards_train/rejected': '-0.7455', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.58732', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-143.59', 'loss/train': '0.5266', 'examples_per_second': '128.37', 'grad_norm': '21.463', 'counters/examples': 76288, 'counters/updates': 1192}
skipping logging after 76352 examples to avoid logging too frequently
train stats after 76416 examples: {'rewards_train/chosen': '-0.59129', 'rewards_train/rejected': '-1.0008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.40955', 'logps_train/rejected': '-151.51', 'logps_train/chosen': '-152.11', 'loss/train': '0.65029', 'examples_per_second': '127.72', 'grad_norm': '25.082', 'counters/examples': 76416, 'counters/updates': 1194}
skipping logging after 76480 examples to avoid logging too frequently
train stats after 76544 examples: {'rewards_train/chosen': '-0.48752', 'rewards_train/rejected': '-0.96538', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.47786', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-150.09', 'loss/train': '0.583', 'examples_per_second': '124.41', 'grad_norm': '21.934', 'counters/examples': 76544, 'counters/updates': 1196}
skipping logging after 76608 examples to avoid logging too frequently
train stats after 76672 examples: {'rewards_train/chosen': '-0.56171', 'rewards_train/rejected': '-0.95087', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.38917', 'logps_train/rejected': '-149.73', 'logps_train/chosen': '-150.39', 'loss/train': '0.6565', 'examples_per_second': '124.14', 'grad_norm': '25.241', 'counters/examples': 76672, 'counters/updates': 1198}
skipping logging after 76736 examples to avoid logging too frequently
train stats after 76800 examples: {'rewards_train/chosen': '-0.84251', 'rewards_train/rejected': '-1.1617', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31919', 'logps_train/rejected': '-153.21', 'logps_train/chosen': '-156.02', 'loss/train': '0.62667', 'examples_per_second': '124.39', 'grad_norm': '24.919', 'counters/examples': 76800, 'counters/updates': 1200}
skipping logging after 76864 examples to avoid logging too frequently
train stats after 76928 examples: {'rewards_train/chosen': '-0.80832', 'rewards_train/rejected': '-1.1885', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38021', 'logps_train/rejected': '-145.03', 'logps_train/chosen': '-159.34', 'loss/train': '0.66463', 'examples_per_second': '124.08', 'grad_norm': '25.555', 'counters/examples': 76928, 'counters/updates': 1202}
skipping logging after 76992 examples to avoid logging too frequently
train stats after 77056 examples: {'rewards_train/chosen': '-0.85433', 'rewards_train/rejected': '-1.2312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.37689', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-151.56', 'loss/train': '0.60422', 'examples_per_second': '123.53', 'grad_norm': '24.593', 'counters/examples': 77056, 'counters/updates': 1204}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77184 examples: {'rewards_train/chosen': '-0.6869', 'rewards_train/rejected': '-1.3375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.65057', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-169.96', 'loss/train': '0.52655', 'examples_per_second': '123.91', 'grad_norm': '22.132', 'counters/examples': 77184, 'counters/updates': 1206}
skipping logging after 77248 examples to avoid logging too frequently
train stats after 77312 examples: {'rewards_train/chosen': '-0.74699', 'rewards_train/rejected': '-1.229', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.48198', 'logps_train/rejected': '-165.26', 'logps_train/chosen': '-148.53', 'loss/train': '0.58975', 'examples_per_second': '118.7', 'grad_norm': '23.889', 'counters/examples': 77312, 'counters/updates': 1208}
skipping logging after 77376 examples to avoid logging too frequently
train stats after 77440 examples: {'rewards_train/chosen': '-0.73505', 'rewards_train/rejected': '-1.3872', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.65218', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-151.6', 'loss/train': '0.55409', 'examples_per_second': '127.63', 'grad_norm': '21.817', 'counters/examples': 77440, 'counters/updates': 1210}
skipping logging after 77504 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '-0.69275', 'rewards_train/rejected': '-1.1961', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.50338', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-129.62', 'loss/train': '0.57363', 'examples_per_second': '119.56', 'grad_norm': '22.857', 'counters/examples': 77568, 'counters/updates': 1212}
skipping logging after 77632 examples to avoid logging too frequently
train stats after 77696 examples: {'rewards_train/chosen': '-0.60373', 'rewards_train/rejected': '-1.1662', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56247', 'logps_train/rejected': '-146.29', 'logps_train/chosen': '-158.48', 'loss/train': '0.56154', 'examples_per_second': '124.59', 'grad_norm': '24.323', 'counters/examples': 77696, 'counters/updates': 1214}
skipping logging after 77760 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '-0.61762', 'rewards_train/rejected': '-1.2369', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.61924', 'logps_train/rejected': '-142.28', 'logps_train/chosen': '-159.8', 'loss/train': '0.53565', 'examples_per_second': '118.77', 'grad_norm': '24.589', 'counters/examples': 77824, 'counters/updates': 1216}
skipping logging after 77888 examples to avoid logging too frequently
train stats after 77952 examples: {'rewards_train/chosen': '-0.68249', 'rewards_train/rejected': '-1.1673', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48481', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-161.69', 'loss/train': '0.5903', 'examples_per_second': '128.65', 'grad_norm': '24.188', 'counters/examples': 77952, 'counters/updates': 1218}
skipping logging after 78016 examples to avoid logging too frequently
train stats after 78080 examples: {'rewards_train/chosen': '-0.64463', 'rewards_train/rejected': '-1.2727', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.62803', 'logps_train/rejected': '-152.2', 'logps_train/chosen': '-146.92', 'loss/train': '0.52535', 'examples_per_second': '124.27', 'grad_norm': '21.852', 'counters/examples': 78080, 'counters/updates': 1220}
skipping logging after 78144 examples to avoid logging too frequently
train stats after 78208 examples: {'rewards_train/chosen': '-0.84464', 'rewards_train/rejected': '-1.3048', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.46017', 'logps_train/rejected': '-162.91', 'logps_train/chosen': '-161.48', 'loss/train': '0.63603', 'examples_per_second': '118.15', 'grad_norm': '25.225', 'counters/examples': 78208, 'counters/updates': 1222}
skipping logging after 78272 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '-0.81819', 'rewards_train/rejected': '-1.1735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.35528', 'logps_train/rejected': '-142.9', 'logps_train/chosen': '-160.45', 'loss/train': '0.63416', 'examples_per_second': '124.82', 'grad_norm': '27.536', 'counters/examples': 78336, 'counters/updates': 1224}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78464 examples: {'rewards_train/chosen': '-0.78998', 'rewards_train/rejected': '-1.1885', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.3985', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-136.68', 'loss/train': '0.5993', 'examples_per_second': '124.64', 'grad_norm': '22.95', 'counters/examples': 78464, 'counters/updates': 1226}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78592 examples: {'rewards_train/chosen': '-0.64761', 'rewards_train/rejected': '-1.0319', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.38432', 'logps_train/rejected': '-155.05', 'logps_train/chosen': '-126.38', 'loss/train': '0.62941', 'examples_per_second': '126.68', 'grad_norm': '25.347', 'counters/examples': 78592, 'counters/updates': 1228}
skipping logging after 78656 examples to avoid logging too frequently
train stats after 78720 examples: {'rewards_train/chosen': '-0.70095', 'rewards_train/rejected': '-1.0347', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.33373', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-165.31', 'loss/train': '0.64997', 'examples_per_second': '121.59', 'grad_norm': '23.97', 'counters/examples': 78720, 'counters/updates': 1230}
skipping logging after 78784 examples to avoid logging too frequently
train stats after 78848 examples: {'rewards_train/chosen': '-0.81178', 'rewards_train/rejected': '-1.3002', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.48842', 'logps_train/rejected': '-164.99', 'logps_train/chosen': '-170.8', 'loss/train': '0.56831', 'examples_per_second': '133.69', 'grad_norm': '27.22', 'counters/examples': 78848, 'counters/updates': 1232}
skipping logging after 78912 examples to avoid logging too frequently
train stats after 78976 examples: {'rewards_train/chosen': '-0.73175', 'rewards_train/rejected': '-1.398', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66623', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-150.89', 'loss/train': '0.54953', 'examples_per_second': '118.69', 'grad_norm': '25.927', 'counters/examples': 78976, 'counters/updates': 1234}
skipping logging after 79040 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '-0.71943', 'rewards_train/rejected': '-1.2685', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.54908', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-148.05', 'loss/train': '0.56481', 'examples_per_second': '143.31', 'grad_norm': '24.482', 'counters/examples': 79104, 'counters/updates': 1236}
skipping logging after 79168 examples to avoid logging too frequently
train stats after 79232 examples: {'rewards_train/chosen': '-0.71226', 'rewards_train/rejected': '-1.4498', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.7375', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-164.54', 'loss/train': '0.5394', 'examples_per_second': '119.38', 'grad_norm': '23.103', 'counters/examples': 79232, 'counters/updates': 1238}
skipping logging after 79296 examples to avoid logging too frequently
train stats after 79360 examples: {'rewards_train/chosen': '-0.86419', 'rewards_train/rejected': '-1.2418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.37759', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-147.41', 'loss/train': '0.63302', 'examples_per_second': '124.13', 'grad_norm': '24.402', 'counters/examples': 79360, 'counters/updates': 1240}
skipping logging after 79424 examples to avoid logging too frequently
train stats after 79488 examples: {'rewards_train/chosen': '-0.84215', 'rewards_train/rejected': '-1.3664', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52422', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-159.48', 'loss/train': '0.5912', 'examples_per_second': '128.32', 'grad_norm': '24.659', 'counters/examples': 79488, 'counters/updates': 1242}
skipping logging after 79552 examples to avoid logging too frequently
train stats after 79616 examples: {'rewards_train/chosen': '-0.72447', 'rewards_train/rejected': '-1.2353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.51083', 'logps_train/rejected': '-166.35', 'logps_train/chosen': '-192.87', 'loss/train': '0.59008', 'examples_per_second': '124.18', 'grad_norm': '28.068', 'counters/examples': 79616, 'counters/updates': 1244}
skipping logging after 79680 examples to avoid logging too frequently
train stats after 79744 examples: {'rewards_train/chosen': '-0.91585', 'rewards_train/rejected': '-1.2423', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.32648', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-187.61', 'loss/train': '0.65892', 'examples_per_second': '119.33', 'grad_norm': '28.905', 'counters/examples': 79744, 'counters/updates': 1246}
skipping logging after 79808 examples to avoid logging too frequently
train stats after 79872 examples: {'rewards_train/chosen': '-0.83489', 'rewards_train/rejected': '-1.1398', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.30494', 'logps_train/rejected': '-165.19', 'logps_train/chosen': '-142.09', 'loss/train': '0.66258', 'examples_per_second': '124.43', 'grad_norm': '25.452', 'counters/examples': 79872, 'counters/updates': 1248}
skipping logging after 79936 examples to avoid logging too frequently
train stats after 80000 examples: {'rewards_train/chosen': '-0.69999', 'rewards_train/rejected': '-1.2607', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.56068', 'logps_train/rejected': '-164.91', 'logps_train/chosen': '-146.82', 'loss/train': '0.58612', 'examples_per_second': '124.62', 'grad_norm': '25.035', 'counters/examples': 80000, 'counters/updates': 1250}
skipping logging after 80064 examples to avoid logging too frequently
train stats after 80128 examples: {'rewards_train/chosen': '-0.79654', 'rewards_train/rejected': '-1.1348', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.33829', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-151.08', 'loss/train': '0.65385', 'examples_per_second': '138.92', 'grad_norm': '24.769', 'counters/examples': 80128, 'counters/updates': 1252}
skipping logging after 80192 examples to avoid logging too frequently
train stats after 80256 examples: {'rewards_train/chosen': '-0.63192', 'rewards_train/rejected': '-0.96607', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.33416', 'logps_train/rejected': '-138.26', 'logps_train/chosen': '-153.4', 'loss/train': '0.64564', 'examples_per_second': '123.89', 'grad_norm': '24.057', 'counters/examples': 80256, 'counters/updates': 1254}
skipping logging after 80320 examples to avoid logging too frequently
train stats after 80384 examples: {'rewards_train/chosen': '-0.88063', 'rewards_train/rejected': '-1.434', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.55336', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-175.03', 'loss/train': '0.55383', 'examples_per_second': '118.7', 'grad_norm': '22.855', 'counters/examples': 80384, 'counters/updates': 1256}
skipping logging after 80448 examples to avoid logging too frequently
train stats after 80512 examples: {'rewards_train/chosen': '-0.73462', 'rewards_train/rejected': '-1.4016', 'rewards_train/accuracies': '0.79688', 'rewards_train/margins': '0.66697', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-142.33', 'loss/train': '0.52556', 'examples_per_second': '126.08', 'grad_norm': '20.31', 'counters/examples': 80512, 'counters/updates': 1258}
skipping logging after 80576 examples to avoid logging too frequently
train stats after 80640 examples: {'rewards_train/chosen': '-0.79329', 'rewards_train/rejected': '-1.1335', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.34019', 'logps_train/rejected': '-153.22', 'logps_train/chosen': '-161.48', 'loss/train': '0.62851', 'examples_per_second': '120.07', 'grad_norm': '25.908', 'counters/examples': 80640, 'counters/updates': 1260}
skipping logging after 80704 examples to avoid logging too frequently
train stats after 80768 examples: {'rewards_train/chosen': '-0.60667', 'rewards_train/rejected': '-1.1469', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.54021', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-143.99', 'loss/train': '0.54938', 'examples_per_second': '121.49', 'grad_norm': '22.298', 'counters/examples': 80768, 'counters/updates': 1262}
skipping logging after 80832 examples to avoid logging too frequently
train stats after 80896 examples: {'rewards_train/chosen': '-0.69174', 'rewards_train/rejected': '-0.87069', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17894', 'logps_train/rejected': '-138.53', 'logps_train/chosen': '-153.53', 'loss/train': '0.70431', 'examples_per_second': '124.25', 'grad_norm': '27.122', 'counters/examples': 80896, 'counters/updates': 1264}
skipping logging after 80960 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '-0.72476', 'rewards_train/rejected': '-1.1951', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.47038', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-171.73', 'loss/train': '0.58061', 'examples_per_second': '143.34', 'grad_norm': '25.15', 'counters/examples': 81024, 'counters/updates': 1266}
skipping logging after 81088 examples to avoid logging too frequently
train stats after 81152 examples: {'rewards_train/chosen': '-0.78039', 'rewards_train/rejected': '-1.3865', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.60614', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-158.97', 'loss/train': '0.54362', 'examples_per_second': '122.93', 'grad_norm': '20.871', 'counters/examples': 81152, 'counters/updates': 1268}
skipping logging after 81216 examples to avoid logging too frequently
train stats after 81280 examples: {'rewards_train/chosen': '-0.86049', 'rewards_train/rejected': '-1.4223', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56178', 'logps_train/rejected': '-155.7', 'logps_train/chosen': '-158.37', 'loss/train': '0.58355', 'examples_per_second': '126.15', 'grad_norm': '22.279', 'counters/examples': 81280, 'counters/updates': 1270}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81408 examples: {'rewards_train/chosen': '-0.78933', 'rewards_train/rejected': '-1.064', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2747', 'logps_train/rejected': '-142.29', 'logps_train/chosen': '-151.75', 'loss/train': '0.67657', 'examples_per_second': '123.15', 'grad_norm': '23.505', 'counters/examples': 81408, 'counters/updates': 1272}
skipping logging after 81472 examples to avoid logging too frequently
train stats after 81536 examples: {'rewards_train/chosen': '-0.8253', 'rewards_train/rejected': '-1.3748', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.5495', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-153.4', 'loss/train': '0.56941', 'examples_per_second': '127.57', 'grad_norm': '23.38', 'counters/examples': 81536, 'counters/updates': 1274}
skipping logging after 81600 examples to avoid logging too frequently
train stats after 81664 examples: {'rewards_train/chosen': '-0.55483', 'rewards_train/rejected': '-1.0933', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.53851', 'logps_train/rejected': '-138.32', 'logps_train/chosen': '-151.38', 'loss/train': '0.54098', 'examples_per_second': '124.05', 'grad_norm': '22.577', 'counters/examples': 81664, 'counters/updates': 1276}
skipping logging after 81728 examples to avoid logging too frequently
train stats after 81792 examples: {'rewards_train/chosen': '-0.70105', 'rewards_train/rejected': '-0.88553', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.18448', 'logps_train/rejected': '-162.22', 'logps_train/chosen': '-168.68', 'loss/train': '0.69397', 'examples_per_second': '125.73', 'grad_norm': '26.509', 'counters/examples': 81792, 'counters/updates': 1278}
skipping logging after 81856 examples to avoid logging too frequently
train stats after 81920 examples: {'rewards_train/chosen': '-0.71148', 'rewards_train/rejected': '-1.375', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66356', 'logps_train/rejected': '-158.9', 'logps_train/chosen': '-160.42', 'loss/train': '0.52406', 'examples_per_second': '123.21', 'grad_norm': '22.246', 'counters/examples': 81920, 'counters/updates': 1280}
skipping logging after 81984 examples to avoid logging too frequently
train stats after 82048 examples: {'rewards_train/chosen': '-0.81554', 'rewards_train/rejected': '-1.1218', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3063', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-145.14', 'loss/train': '0.64477', 'examples_per_second': '123.66', 'grad_norm': '22.878', 'counters/examples': 82048, 'counters/updates': 1282}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82176 examples: {'rewards_train/chosen': '-0.62502', 'rewards_train/rejected': '-1.1818', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.5568', 'logps_train/rejected': '-152.8', 'logps_train/chosen': '-150.76', 'loss/train': '0.54646', 'examples_per_second': '123.82', 'grad_norm': '22.998', 'counters/examples': 82176, 'counters/updates': 1284}
skipping logging after 82240 examples to avoid logging too frequently
train stats after 82304 examples: {'rewards_train/chosen': '-0.51886', 'rewards_train/rejected': '-1.2221', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.70327', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-155.07', 'loss/train': '0.492', 'examples_per_second': '127.8', 'grad_norm': '21.06', 'counters/examples': 82304, 'counters/updates': 1286}
skipping logging after 82368 examples to avoid logging too frequently
train stats after 82432 examples: {'rewards_train/chosen': '-0.94278', 'rewards_train/rejected': '-1.2189', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.27609', 'logps_train/rejected': '-181.01', 'logps_train/chosen': '-173.03', 'loss/train': '0.6459', 'examples_per_second': '124.26', 'grad_norm': '27.331', 'counters/examples': 82432, 'counters/updates': 1288}
skipping logging after 82496 examples to avoid logging too frequently
train stats after 82560 examples: {'rewards_train/chosen': '-0.63614', 'rewards_train/rejected': '-1.2403', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.60413', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-166.89', 'loss/train': '0.55551', 'examples_per_second': '120.47', 'grad_norm': '20.752', 'counters/examples': 82560, 'counters/updates': 1290}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82688 examples: {'rewards_train/chosen': '-0.49627', 'rewards_train/rejected': '-1.1087', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.61246', 'logps_train/rejected': '-134.07', 'logps_train/chosen': '-142.03', 'loss/train': '0.53102', 'examples_per_second': '118.54', 'grad_norm': '20.795', 'counters/examples': 82688, 'counters/updates': 1292}
skipping logging after 82752 examples to avoid logging too frequently
train stats after 82816 examples: {'rewards_train/chosen': '-0.7567', 'rewards_train/rejected': '-1.4099', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.65316', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-144.83', 'loss/train': '0.57264', 'examples_per_second': '124', 'grad_norm': '23.482', 'counters/examples': 82816, 'counters/updates': 1294}
skipping logging after 82880 examples to avoid logging too frequently
train stats after 82944 examples: {'rewards_train/chosen': '-0.7945', 'rewards_train/rejected': '-1.1618', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.36731', 'logps_train/rejected': '-170.47', 'logps_train/chosen': '-175.11', 'loss/train': '0.63936', 'examples_per_second': '123.98', 'grad_norm': '24.634', 'counters/examples': 82944, 'counters/updates': 1296}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83072 examples: {'rewards_train/chosen': '-0.63457', 'rewards_train/rejected': '-1.3431', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.70851', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-159.2', 'loss/train': '0.5264', 'examples_per_second': '118.81', 'grad_norm': '22.947', 'counters/examples': 83072, 'counters/updates': 1298}
skipping logging after 83136 examples to avoid logging too frequently
train stats after 83200 examples: {'rewards_train/chosen': '-0.71598', 'rewards_train/rejected': '-1.0598', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.34386', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-148.56', 'loss/train': '0.63017', 'examples_per_second': '119.29', 'grad_norm': '23.291', 'counters/examples': 83200, 'counters/updates': 1300}
skipping logging after 83264 examples to avoid logging too frequently
train stats after 83328 examples: {'rewards_train/chosen': '-0.77044', 'rewards_train/rejected': '-1.0077', 'rewards_train/accuracies': '0.54688', 'rewards_train/margins': '0.23722', 'logps_train/rejected': '-168.79', 'logps_train/chosen': '-170.45', 'loss/train': '0.69895', 'examples_per_second': '129.79', 'grad_norm': '26.656', 'counters/examples': 83328, 'counters/updates': 1302}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83456 examples: {'rewards_train/chosen': '-0.73829', 'rewards_train/rejected': '-1.0743', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.33603', 'logps_train/rejected': '-147.25', 'logps_train/chosen': '-164.39', 'loss/train': '0.63148', 'examples_per_second': '124.09', 'grad_norm': '24.308', 'counters/examples': 83456, 'counters/updates': 1304}
skipping logging after 83520 examples to avoid logging too frequently
train stats after 83584 examples: {'rewards_train/chosen': '-0.71129', 'rewards_train/rejected': '-1.0994', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38811', 'logps_train/rejected': '-141.41', 'logps_train/chosen': '-141.93', 'loss/train': '0.64687', 'examples_per_second': '135.77', 'grad_norm': '22.578', 'counters/examples': 83584, 'counters/updates': 1306}
skipping logging after 83648 examples to avoid logging too frequently
train stats after 83712 examples: {'rewards_train/chosen': '-0.71281', 'rewards_train/rejected': '-1.0485', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.33565', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-144.07', 'loss/train': '0.66898', 'examples_per_second': '128.75', 'grad_norm': '22.884', 'counters/examples': 83712, 'counters/updates': 1308}
skipping logging after 83776 examples to avoid logging too frequently
Running evaluation after 83776 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:00<00:01,  9.76it/s]Computing eval metrics:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:01, 10.17it/s]Computing eval metrics:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:00<00:01, 10.23it/s]Computing eval metrics:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:00<00:00, 10.40it/s]Computing eval metrics:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:00<00:00, 10.40it/s]Computing eval metrics:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [00:01<00:00, 10.36it/s]Computing eval metrics:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [00:01<00:00, 10.36it/s]Computing eval metrics:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [00:01<00:00, 10.34it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.31it/s]
eval after 83776: {'rewards_eval/chosen': '-0.6286', 'rewards_eval/rejected': '-1.0389', 'rewards_eval/accuracies': '0.66797', 'rewards_eval/margins': '0.41029', 'logps_eval/rejected': '-136.47', 'logps_eval/chosen': '-154.19', 'loss/eval': '0.61764'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-83776...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-83776/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-83776/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-83776/scheduler.pt...
train stats after 83840 examples: {'rewards_train/chosen': '-0.60464', 'rewards_train/rejected': '-1.0493', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.44465', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-116.25', 'loss/train': '0.56962', 'examples_per_second': '123.15', 'grad_norm': '18.989', 'counters/examples': 83840, 'counters/updates': 1310}
skipping logging after 83904 examples to avoid logging too frequently
train stats after 83968 examples: {'rewards_train/chosen': '-0.71903', 'rewards_train/rejected': '-1.1897', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.47065', 'logps_train/rejected': '-144.99', 'logps_train/chosen': '-161.19', 'loss/train': '0.58737', 'examples_per_second': '121.78', 'grad_norm': '23.971', 'counters/examples': 83968, 'counters/updates': 1312}
skipping logging after 84032 examples to avoid logging too frequently
train stats after 84096 examples: {'rewards_train/chosen': '-0.63753', 'rewards_train/rejected': '-1.1814', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.54388', 'logps_train/rejected': '-167.97', 'logps_train/chosen': '-172.6', 'loss/train': '0.55752', 'examples_per_second': '119.42', 'grad_norm': '25.741', 'counters/examples': 84096, 'counters/updates': 1314}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84224 examples: {'rewards_train/chosen': '-0.57182', 'rewards_train/rejected': '-1.0227', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.45086', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-147.31', 'loss/train': '0.55878', 'examples_per_second': '124.42', 'grad_norm': '21.121', 'counters/examples': 84224, 'counters/updates': 1316}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84352 examples: {'rewards_train/chosen': '-0.48961', 'rewards_train/rejected': '-0.81258', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32297', 'logps_train/rejected': '-127.12', 'logps_train/chosen': '-155.08', 'loss/train': '0.6299', 'examples_per_second': '124.89', 'grad_norm': '22.964', 'counters/examples': 84352, 'counters/updates': 1318}
skipping logging after 84416 examples to avoid logging too frequently
train stats after 84480 examples: {'rewards_train/chosen': '-0.43173', 'rewards_train/rejected': '-0.99749', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.56576', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-156.28', 'loss/train': '0.56553', 'examples_per_second': '118.61', 'grad_norm': '24.977', 'counters/examples': 84480, 'counters/updates': 1320}
skipping logging after 84544 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '-0.49968', 'rewards_train/rejected': '-1.1274', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.62767', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-133.36', 'loss/train': '0.5279', 'examples_per_second': '124.5', 'grad_norm': '23', 'counters/examples': 84608, 'counters/updates': 1322}
skipping logging after 84672 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '-0.69392', 'rewards_train/rejected': '-1.1833', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.48941', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-151.32', 'loss/train': '0.59501', 'examples_per_second': '124.53', 'grad_norm': '22.091', 'counters/examples': 84736, 'counters/updates': 1324}
skipping logging after 84800 examples to avoid logging too frequently
train stats after 84864 examples: {'rewards_train/chosen': '-0.32182', 'rewards_train/rejected': '-1.0587', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.73689', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-135.71', 'loss/train': '0.49534', 'examples_per_second': '119.7', 'grad_norm': '22.186', 'counters/examples': 84864, 'counters/updates': 1326}
skipping logging after 84928 examples to avoid logging too frequently
train stats after 84992 examples: {'rewards_train/chosen': '-0.59459', 'rewards_train/rejected': '-1.0883', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.49374', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-139.62', 'loss/train': '0.54198', 'examples_per_second': '124.5', 'grad_norm': '20.675', 'counters/examples': 84992, 'counters/updates': 1328}
skipping logging after 85056 examples to avoid logging too frequently
train stats after 85120 examples: {'rewards_train/chosen': '-0.62349', 'rewards_train/rejected': '-1.1282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.50469', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-153.85', 'loss/train': '0.59344', 'examples_per_second': '118.83', 'grad_norm': '25.132', 'counters/examples': 85120, 'counters/updates': 1330}
skipping logging after 85184 examples to avoid logging too frequently
train stats after 85248 examples: {'rewards_train/chosen': '-0.75955', 'rewards_train/rejected': '-1.1872', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42766', 'logps_train/rejected': '-145', 'logps_train/chosen': '-154.15', 'loss/train': '0.6431', 'examples_per_second': '124.55', 'grad_norm': '25.317', 'counters/examples': 85248, 'counters/updates': 1332}
skipping logging after 85312 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '-0.58093', 'rewards_train/rejected': '-1.2821', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.70114', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-149.08', 'loss/train': '0.51063', 'examples_per_second': '130.58', 'grad_norm': '19.378', 'counters/examples': 85376, 'counters/updates': 1334}
skipping logging after 85440 examples to avoid logging too frequently
train stats after 85504 examples: {'rewards_train/chosen': '-0.82526', 'rewards_train/rejected': '-1.5216', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.69629', 'logps_train/rejected': '-142.59', 'logps_train/chosen': '-167.47', 'loss/train': '0.55531', 'examples_per_second': '124.66', 'grad_norm': '23.537', 'counters/examples': 85504, 'counters/updates': 1336}
skipping logging after 85568 examples to avoid logging too frequently
train stats after 85632 examples: {'rewards_train/chosen': '-0.56515', 'rewards_train/rejected': '-1.2945', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.72931', 'logps_train/rejected': '-151.29', 'logps_train/chosen': '-157.31', 'loss/train': '0.51387', 'examples_per_second': '124.31', 'grad_norm': '19.968', 'counters/examples': 85632, 'counters/updates': 1338}
skipping logging after 85696 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '-0.82083', 'rewards_train/rejected': '-1.2355', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.4147', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-138.03', 'loss/train': '0.60699', 'examples_per_second': '124.32', 'grad_norm': '22.736', 'counters/examples': 85760, 'counters/updates': 1340}
skipping logging after 85824 examples to avoid logging too frequently
train stats after 85888 examples: {'rewards_train/chosen': '-0.78432', 'rewards_train/rejected': '-1.3526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.5683', 'logps_train/rejected': '-145.83', 'logps_train/chosen': '-174.24', 'loss/train': '0.62298', 'examples_per_second': '118.57', 'grad_norm': '25.633', 'counters/examples': 85888, 'counters/updates': 1342}
skipping logging after 85952 examples to avoid logging too frequently
train stats after 86016 examples: {'rewards_train/chosen': '-0.84447', 'rewards_train/rejected': '-1.1622', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.31777', 'logps_train/rejected': '-154.14', 'logps_train/chosen': '-151.5', 'loss/train': '0.68121', 'examples_per_second': '124.71', 'grad_norm': '25.604', 'counters/examples': 86016, 'counters/updates': 1344}
skipping logging after 86080 examples to avoid logging too frequently
train stats after 86144 examples: {'rewards_train/chosen': '-0.70264', 'rewards_train/rejected': '-1.0728', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.37021', 'logps_train/rejected': '-146.54', 'logps_train/chosen': '-150.38', 'loss/train': '0.65532', 'examples_per_second': '123.92', 'grad_norm': '25.244', 'counters/examples': 86144, 'counters/updates': 1346}
skipping logging after 86208 examples to avoid logging too frequently
train stats after 86272 examples: {'rewards_train/chosen': '-0.84102', 'rewards_train/rejected': '-1.2408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.39974', 'logps_train/rejected': '-148.31', 'logps_train/chosen': '-130.46', 'loss/train': '0.63443', 'examples_per_second': '126.02', 'grad_norm': '25.912', 'counters/examples': 86272, 'counters/updates': 1348}
skipping logging after 86336 examples to avoid logging too frequently
train stats after 86400 examples: {'rewards_train/chosen': '-0.52259', 'rewards_train/rejected': '-0.96184', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.43925', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-158.35', 'loss/train': '0.58239', 'examples_per_second': '124.4', 'grad_norm': '22.667', 'counters/examples': 86400, 'counters/updates': 1350}
skipping logging after 86464 examples to avoid logging too frequently
train stats after 86528 examples: {'rewards_train/chosen': '-0.68669', 'rewards_train/rejected': '-1.0295', 'rewards_train/accuracies': '0.57812', 'rewards_train/margins': '0.34284', 'logps_train/rejected': '-138.44', 'logps_train/chosen': '-151.11', 'loss/train': '0.64591', 'examples_per_second': '124.66', 'grad_norm': '23.861', 'counters/examples': 86528, 'counters/updates': 1352}
skipping logging after 86592 examples to avoid logging too frequently
train stats after 86656 examples: {'rewards_train/chosen': '-0.62132', 'rewards_train/rejected': '-0.91489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29357', 'logps_train/rejected': '-164.42', 'logps_train/chosen': '-186.05', 'loss/train': '0.67515', 'examples_per_second': '124.4', 'grad_norm': '27.511', 'counters/examples': 86656, 'counters/updates': 1354}
skipping logging after 86720 examples to avoid logging too frequently
train stats after 86784 examples: {'rewards_train/chosen': '-0.47073', 'rewards_train/rejected': '-0.99572', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52498', 'logps_train/rejected': '-149.87', 'logps_train/chosen': '-131.34', 'loss/train': '0.562', 'examples_per_second': '118.67', 'grad_norm': '21.673', 'counters/examples': 86784, 'counters/updates': 1356}
skipping logging after 86848 examples to avoid logging too frequently
train stats after 86912 examples: {'rewards_train/chosen': '-0.53723', 'rewards_train/rejected': '-1.0588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.52159', 'logps_train/rejected': '-170.21', 'logps_train/chosen': '-171.76', 'loss/train': '0.56705', 'examples_per_second': '124.49', 'grad_norm': '23.994', 'counters/examples': 86912, 'counters/updates': 1358}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87040 examples: {'rewards_train/chosen': '-0.59047', 'rewards_train/rejected': '-1.1796', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.5891', 'logps_train/rejected': '-163.56', 'logps_train/chosen': '-159.41', 'loss/train': '0.53652', 'examples_per_second': '124.48', 'grad_norm': '25.403', 'counters/examples': 87040, 'counters/updates': 1360}
skipping logging after 87104 examples to avoid logging too frequently
train stats after 87168 examples: {'rewards_train/chosen': '-0.63674', 'rewards_train/rejected': '-0.89877', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26203', 'logps_train/rejected': '-166.7', 'logps_train/chosen': '-139.95', 'loss/train': '0.65697', 'examples_per_second': '118.86', 'grad_norm': '29.498', 'counters/examples': 87168, 'counters/updates': 1362}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87296 examples: {'rewards_train/chosen': '-0.73058', 'rewards_train/rejected': '-1.0533', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.32275', 'logps_train/rejected': '-155.34', 'logps_train/chosen': '-179.78', 'loss/train': '0.64962', 'examples_per_second': '126.58', 'grad_norm': '26.755', 'counters/examples': 87296, 'counters/updates': 1364}
skipping logging after 87360 examples to avoid logging too frequently
train stats after 87424 examples: {'rewards_train/chosen': '-0.59251', 'rewards_train/rejected': '-1.0272', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.43465', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-138.5', 'loss/train': '0.62604', 'examples_per_second': '128.49', 'grad_norm': '22.993', 'counters/examples': 87424, 'counters/updates': 1366}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87552 examples: {'rewards_train/chosen': '-0.469', 'rewards_train/rejected': '-0.89964', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43064', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-153.63', 'loss/train': '0.61409', 'examples_per_second': '124.44', 'grad_norm': '24.602', 'counters/examples': 87552, 'counters/updates': 1368}
skipping logging after 87616 examples to avoid logging too frequently
train stats after 87680 examples: {'rewards_train/chosen': '-0.41698', 'rewards_train/rejected': '-0.9591', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.54212', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-138.33', 'loss/train': '0.58834', 'examples_per_second': '124.66', 'grad_norm': '23.865', 'counters/examples': 87680, 'counters/updates': 1370}
skipping logging after 87744 examples to avoid logging too frequently
train stats after 87808 examples: {'rewards_train/chosen': '-0.35444', 'rewards_train/rejected': '-0.94985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59541', 'logps_train/rejected': '-148.33', 'logps_train/chosen': '-140.63', 'loss/train': '0.5637', 'examples_per_second': '126.49', 'grad_norm': '21.508', 'counters/examples': 87808, 'counters/updates': 1372}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87936 examples: {'rewards_train/chosen': '-0.27717', 'rewards_train/rejected': '-0.80213', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.52497', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-131.04', 'loss/train': '0.53823', 'examples_per_second': '124.95', 'grad_norm': '20.897', 'counters/examples': 87936, 'counters/updates': 1374}
skipping logging after 88000 examples to avoid logging too frequently
train stats after 88064 examples: {'rewards_train/chosen': '-0.41492', 'rewards_train/rejected': '-0.75416', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.33924', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-164.56', 'loss/train': '0.62211', 'examples_per_second': '124.06', 'grad_norm': '24.175', 'counters/examples': 88064, 'counters/updates': 1376}
skipping logging after 88128 examples to avoid logging too frequently
train stats after 88192 examples: {'rewards_train/chosen': '-0.62112', 'rewards_train/rejected': '-0.85926', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.23813', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-157.45', 'loss/train': '0.68973', 'examples_per_second': '125.1', 'grad_norm': '25.802', 'counters/examples': 88192, 'counters/updates': 1378}
skipping logging after 88256 examples to avoid logging too frequently
train stats after 88320 examples: {'rewards_train/chosen': '-0.57338', 'rewards_train/rejected': '-0.86891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29552', 'logps_train/rejected': '-144.33', 'logps_train/chosen': '-152.52', 'loss/train': '0.6557', 'examples_per_second': '124.12', 'grad_norm': '24.461', 'counters/examples': 88320, 'counters/updates': 1380}
skipping logging after 88384 examples to avoid logging too frequently
train stats after 88448 examples: {'rewards_train/chosen': '-0.56817', 'rewards_train/rejected': '-1.0614', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.49326', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-167.64', 'loss/train': '0.55551', 'examples_per_second': '125.51', 'grad_norm': '21.841', 'counters/examples': 88448, 'counters/updates': 1382}
skipping logging after 88512 examples to avoid logging too frequently
train stats after 88576 examples: {'rewards_train/chosen': '-0.73021', 'rewards_train/rejected': '-1.3718', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.6416', 'logps_train/rejected': '-138', 'logps_train/chosen': '-160.38', 'loss/train': '0.57256', 'examples_per_second': '124.99', 'grad_norm': '24.408', 'counters/examples': 88576, 'counters/updates': 1384}
skipping logging after 88640 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '-0.84238', 'rewards_train/rejected': '-1.1723', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.32987', 'logps_train/rejected': '-128', 'logps_train/chosen': '-133.31', 'loss/train': '0.65783', 'examples_per_second': '141.03', 'grad_norm': '23.208', 'counters/examples': 88704, 'counters/updates': 1386}
skipping logging after 88768 examples to avoid logging too frequently
train stats after 88832 examples: {'rewards_train/chosen': '-0.71683', 'rewards_train/rejected': '-1.1132', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.3964', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-154.71', 'loss/train': '0.64809', 'examples_per_second': '120.08', 'grad_norm': '25.328', 'counters/examples': 88832, 'counters/updates': 1388}
skipping logging after 88896 examples to avoid logging too frequently
train stats after 88960 examples: {'rewards_train/chosen': '-0.90599', 'rewards_train/rejected': '-1.3421', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.43613', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-153.04', 'loss/train': '0.63075', 'examples_per_second': '121.84', 'grad_norm': '23.494', 'counters/examples': 88960, 'counters/updates': 1390}
skipping logging after 89024 examples to avoid logging too frequently
train stats after 89088 examples: {'rewards_train/chosen': '-0.73691', 'rewards_train/rejected': '-1.1354', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.39852', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-161.85', 'loss/train': '0.66537', 'examples_per_second': '125.01', 'grad_norm': '24.973', 'counters/examples': 89088, 'counters/updates': 1392}
skipping logging after 89152 examples to avoid logging too frequently
train stats after 89216 examples: {'rewards_train/chosen': '-0.71895', 'rewards_train/rejected': '-1.077', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.35803', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-186.66', 'loss/train': '0.64526', 'examples_per_second': '119.22', 'grad_norm': '27.591', 'counters/examples': 89216, 'counters/updates': 1394}
skipping logging after 89280 examples to avoid logging too frequently
train stats after 89344 examples: {'rewards_train/chosen': '-0.86759', 'rewards_train/rejected': '-0.94664', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07905', 'logps_train/rejected': '-156.08', 'logps_train/chosen': '-176.09', 'loss/train': '0.74674', 'examples_per_second': '122.69', 'grad_norm': '28.263', 'counters/examples': 89344, 'counters/updates': 1396}
skipping logging after 89408 examples to avoid logging too frequently
train stats after 89472 examples: {'rewards_train/chosen': '-0.48949', 'rewards_train/rejected': '-1.1306', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.64107', 'logps_train/rejected': '-142.99', 'logps_train/chosen': '-154.52', 'loss/train': '0.50463', 'examples_per_second': '130.54', 'grad_norm': '22.061', 'counters/examples': 89472, 'counters/updates': 1398}
skipping logging after 89536 examples to avoid logging too frequently
train stats after 89600 examples: {'rewards_train/chosen': '-0.74699', 'rewards_train/rejected': '-1.0206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.27356', 'logps_train/rejected': '-139.14', 'logps_train/chosen': '-148.29', 'loss/train': '0.67152', 'examples_per_second': '121.57', 'grad_norm': '26.462', 'counters/examples': 89600, 'counters/updates': 1400}
skipping logging after 89664 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '-0.57025', 'rewards_train/rejected': '-1.0582', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.48795', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-166.49', 'loss/train': '0.57735', 'examples_per_second': '124.77', 'grad_norm': '24.373', 'counters/examples': 89728, 'counters/updates': 1402}
skipping logging after 89792 examples to avoid logging too frequently
train stats after 89856 examples: {'rewards_train/chosen': '-0.79144', 'rewards_train/rejected': '-1.1189', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.32744', 'logps_train/rejected': '-139.75', 'logps_train/chosen': '-154.77', 'loss/train': '0.65787', 'examples_per_second': '125.02', 'grad_norm': '25.143', 'counters/examples': 89856, 'counters/updates': 1404}
skipping logging after 89920 examples to avoid logging too frequently
train stats after 89984 examples: {'rewards_train/chosen': '-0.6503', 'rewards_train/rejected': '-1.1124', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.46214', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-188.21', 'loss/train': '0.58412', 'examples_per_second': '124.83', 'grad_norm': '24.313', 'counters/examples': 89984, 'counters/updates': 1406}
skipping logging after 90048 examples to avoid logging too frequently
train stats after 90112 examples: {'rewards_train/chosen': '-0.67115', 'rewards_train/rejected': '-1.0974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.42621', 'logps_train/rejected': '-154.15', 'logps_train/chosen': '-168.59', 'loss/train': '0.60738', 'examples_per_second': '127.96', 'grad_norm': '24.029', 'counters/examples': 90112, 'counters/updates': 1408}
skipping logging after 90176 examples to avoid logging too frequently
train stats after 90240 examples: {'rewards_train/chosen': '-0.65564', 'rewards_train/rejected': '-1.038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.3824', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-180.76', 'loss/train': '0.66556', 'examples_per_second': '122.01', 'grad_norm': '25.035', 'counters/examples': 90240, 'counters/updates': 1410}
skipping logging after 90304 examples to avoid logging too frequently
train stats after 90368 examples: {'rewards_train/chosen': '-0.47304', 'rewards_train/rejected': '-1.0728', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59974', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-134.31', 'loss/train': '0.53073', 'examples_per_second': '122.16', 'grad_norm': '20.824', 'counters/examples': 90368, 'counters/updates': 1412}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90496 examples: {'rewards_train/chosen': '-0.74471', 'rewards_train/rejected': '-1.192', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.44728', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-140.23', 'loss/train': '0.58849', 'examples_per_second': '110.81', 'grad_norm': '23.918', 'counters/examples': 90496, 'counters/updates': 1414}
skipping logging after 90560 examples to avoid logging too frequently
train stats after 90624 examples: {'rewards_train/chosen': '-0.48314', 'rewards_train/rejected': '-1.0562', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.57301', 'logps_train/rejected': '-144.19', 'logps_train/chosen': '-124.04', 'loss/train': '0.55646', 'examples_per_second': '128.35', 'grad_norm': '23.015', 'counters/examples': 90624, 'counters/updates': 1416}
skipping logging after 90688 examples to avoid logging too frequently
train stats after 90752 examples: {'rewards_train/chosen': '-0.45764', 'rewards_train/rejected': '-0.90472', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.44708', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-147.75', 'loss/train': '0.57896', 'examples_per_second': '124.05', 'grad_norm': '22.259', 'counters/examples': 90752, 'counters/updates': 1418}
skipping logging after 90816 examples to avoid logging too frequently
train stats after 90880 examples: {'rewards_train/chosen': '-0.62215', 'rewards_train/rejected': '-0.98169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.35954', 'logps_train/rejected': '-171.27', 'logps_train/chosen': '-180.4', 'loss/train': '0.66124', 'examples_per_second': '124.73', 'grad_norm': '27.202', 'counters/examples': 90880, 'counters/updates': 1420}
skipping logging after 90944 examples to avoid logging too frequently
train stats after 91008 examples: {'rewards_train/chosen': '-0.75327', 'rewards_train/rejected': '-0.97606', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22279', 'logps_train/rejected': '-143.36', 'logps_train/chosen': '-142.98', 'loss/train': '0.68534', 'examples_per_second': '125.03', 'grad_norm': '24.635', 'counters/examples': 91008, 'counters/updates': 1422}
skipping logging after 91072 examples to avoid logging too frequently
train stats after 91136 examples: {'rewards_train/chosen': '-0.38749', 'rewards_train/rejected': '-0.95696', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.56947', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-150.66', 'loss/train': '0.53204', 'examples_per_second': '124.89', 'grad_norm': '21.345', 'counters/examples': 91136, 'counters/updates': 1424}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91264 examples: {'rewards_train/chosen': '-0.78539', 'rewards_train/rejected': '-1.239', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.45361', 'logps_train/rejected': '-159.58', 'logps_train/chosen': '-155.63', 'loss/train': '0.63942', 'examples_per_second': '122.21', 'grad_norm': '26.383', 'counters/examples': 91264, 'counters/updates': 1426}
skipping logging after 91328 examples to avoid logging too frequently
train stats after 91392 examples: {'rewards_train/chosen': '-0.62488', 'rewards_train/rejected': '-1.0638', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.43895', 'logps_train/rejected': '-145.69', 'logps_train/chosen': '-153.71', 'loss/train': '0.57909', 'examples_per_second': '119.29', 'grad_norm': '23.745', 'counters/examples': 91392, 'counters/updates': 1428}
skipping logging after 91456 examples to avoid logging too frequently
train stats after 91520 examples: {'rewards_train/chosen': '-0.53874', 'rewards_train/rejected': '-1.1383', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.59952', 'logps_train/rejected': '-140.7', 'logps_train/chosen': '-133.02', 'loss/train': '0.53454', 'examples_per_second': '132.33', 'grad_norm': '21.143', 'counters/examples': 91520, 'counters/updates': 1430}
skipping logging after 91584 examples to avoid logging too frequently
train stats after 91648 examples: {'rewards_train/chosen': '-0.57819', 'rewards_train/rejected': '-1.0863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.50809', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-135.15', 'loss/train': '0.58762', 'examples_per_second': '121.13', 'grad_norm': '22.346', 'counters/examples': 91648, 'counters/updates': 1432}
skipping logging after 91712 examples to avoid logging too frequently
train stats after 91776 examples: {'rewards_train/chosen': '-0.49967', 'rewards_train/rejected': '-1.0955', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.59585', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-143.48', 'loss/train': '0.57322', 'examples_per_second': '122.56', 'grad_norm': '21.032', 'counters/examples': 91776, 'counters/updates': 1434}
skipping logging after 91840 examples to avoid logging too frequently
train stats after 91904 examples: {'rewards_train/chosen': '-0.56998', 'rewards_train/rejected': '-1.0616', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.49164', 'logps_train/rejected': '-154.89', 'logps_train/chosen': '-150.07', 'loss/train': '0.58329', 'examples_per_second': '125.36', 'grad_norm': '23.598', 'counters/examples': 91904, 'counters/updates': 1436}
skipping logging after 91968 examples to avoid logging too frequently
train stats after 92032 examples: {'rewards_train/chosen': '-0.57379', 'rewards_train/rejected': '-1.1975', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.62375', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-156.9', 'loss/train': '0.56371', 'examples_per_second': '119.8', 'grad_norm': '22.84', 'counters/examples': 92032, 'counters/updates': 1438}
skipping logging after 92096 examples to avoid logging too frequently
train stats after 92160 examples: {'rewards_train/chosen': '-0.56397', 'rewards_train/rejected': '-0.94664', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.38267', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-166.07', 'loss/train': '0.61972', 'examples_per_second': '124.65', 'grad_norm': '23.47', 'counters/examples': 92160, 'counters/updates': 1440}
skipping logging after 92224 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '-0.55279', 'rewards_train/rejected': '-1.2061', 'rewards_train/accuracies': '0.76562', 'rewards_train/margins': '0.65336', 'logps_train/rejected': '-123.33', 'logps_train/chosen': '-152.21', 'loss/train': '0.50502', 'examples_per_second': '123.98', 'grad_norm': '20.552', 'counters/examples': 92288, 'counters/updates': 1442}
skipping logging after 92352 examples to avoid logging too frequently
train stats after 92416 examples: {'rewards_train/chosen': '-0.81722', 'rewards_train/rejected': '-1.4281', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.61091', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-134.91', 'loss/train': '0.53477', 'examples_per_second': '125.37', 'grad_norm': '20.633', 'counters/examples': 92416, 'counters/updates': 1444}
skipping logging after 92480 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '-0.83724', 'rewards_train/rejected': '-1.327', 'rewards_train/accuracies': '0.70312', 'rewards_train/margins': '0.48979', 'logps_train/rejected': '-142.76', 'logps_train/chosen': '-144.5', 'loss/train': '0.57604', 'examples_per_second': '129.09', 'grad_norm': '22.446', 'counters/examples': 92544, 'counters/updates': 1446}
skipping logging after 92608 examples to avoid logging too frequently
train stats after 92672 examples: {'rewards_train/chosen': '-0.97114', 'rewards_train/rejected': '-1.4719', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.50072', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-173', 'loss/train': '0.59638', 'examples_per_second': '119.13', 'grad_norm': '26.475', 'counters/examples': 92672, 'counters/updates': 1448}
skipping logging after 92736 examples to avoid logging too frequently
train stats after 92800 examples: {'rewards_train/chosen': '-0.79662', 'rewards_train/rejected': '-1.3696', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.57295', 'logps_train/rejected': '-135.1', 'logps_train/chosen': '-124.75', 'loss/train': '0.5547', 'examples_per_second': '124.94', 'grad_norm': '21.062', 'counters/examples': 92800, 'counters/updates': 1450}
skipping logging after 92864 examples to avoid logging too frequently
train stats after 92928 examples: {'rewards_train/chosen': '-0.936', 'rewards_train/rejected': '-1.3602', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.42421', 'logps_train/rejected': '-135.25', 'logps_train/chosen': '-135.11', 'loss/train': '0.62226', 'examples_per_second': '81.514', 'grad_norm': '23.899', 'counters/examples': 92928, 'counters/updates': 1452}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93056 examples: {'rewards_train/chosen': '-0.966', 'rewards_train/rejected': '-1.4437', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.47765', 'logps_train/rejected': '-139.65', 'logps_train/chosen': '-171.12', 'loss/train': '0.60129', 'examples_per_second': '121.3', 'grad_norm': '24.743', 'counters/examples': 93056, 'counters/updates': 1454}
skipping logging after 93120 examples to avoid logging too frequently
train stats after 93184 examples: {'rewards_train/chosen': '-1.0069', 'rewards_train/rejected': '-1.5927', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.58578', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-143.45', 'loss/train': '0.55331', 'examples_per_second': '125.08', 'grad_norm': '20.685', 'counters/examples': 93184, 'counters/updates': 1456}
skipping logging after 93248 examples to avoid logging too frequently
train stats after 93312 examples: {'rewards_train/chosen': '-0.90955', 'rewards_train/rejected': '-1.2976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.38807', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-145.94', 'loss/train': '0.64535', 'examples_per_second': '78.7', 'grad_norm': '23.406', 'counters/examples': 93312, 'counters/updates': 1458}
skipping logging after 93376 examples to avoid logging too frequently
train stats after 93440 examples: {'rewards_train/chosen': '-0.70308', 'rewards_train/rejected': '-1.104', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.40097', 'logps_train/rejected': '-147.22', 'logps_train/chosen': '-179.42', 'loss/train': '0.58529', 'examples_per_second': '124.52', 'grad_norm': '24.667', 'counters/examples': 93440, 'counters/updates': 1460}
skipping logging after 93504 examples to avoid logging too frequently
train stats after 93568 examples: {'rewards_train/chosen': '-0.95846', 'rewards_train/rejected': '-1.3533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.39484', 'logps_train/rejected': '-168.46', 'logps_train/chosen': '-194.49', 'loss/train': '0.61877', 'examples_per_second': '124.53', 'grad_norm': '26.912', 'counters/examples': 93568, 'counters/updates': 1462}
skipping logging after 93632 examples to avoid logging too frequently
train stats after 93696 examples: {'rewards_train/chosen': '-0.8793', 'rewards_train/rejected': '-1.1853', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.30596', 'logps_train/rejected': '-144.25', 'logps_train/chosen': '-180.43', 'loss/train': '0.64263', 'examples_per_second': '125.11', 'grad_norm': '24.952', 'counters/examples': 93696, 'counters/updates': 1464}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93824 examples: {'rewards_train/chosen': '-0.86752', 'rewards_train/rejected': '-1.4003', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.53275', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-148.1', 'loss/train': '0.57694', 'examples_per_second': '125.35', 'grad_norm': '23.214', 'counters/examples': 93824, 'counters/updates': 1466}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93952 examples: {'rewards_train/chosen': '-0.86896', 'rewards_train/rejected': '-1.1742', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30521', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-161.09', 'loss/train': '0.6938', 'examples_per_second': '119.15', 'grad_norm': '26.869', 'counters/examples': 93952, 'counters/updates': 1468}
skipping logging after 94016 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '-0.84177', 'rewards_train/rejected': '-1.1494', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.30765', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-166.23', 'loss/train': '0.69221', 'examples_per_second': '124.58', 'grad_norm': '27.091', 'counters/examples': 94080, 'counters/updates': 1470}
skipping logging after 94144 examples to avoid logging too frequently
train stats after 94208 examples: {'rewards_train/chosen': '-0.63429', 'rewards_train/rejected': '-1.296', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.66168', 'logps_train/rejected': '-161.28', 'logps_train/chosen': '-180.08', 'loss/train': '0.51075', 'examples_per_second': '124.94', 'grad_norm': '22.889', 'counters/examples': 94208, 'counters/updates': 1472}
skipping logging after 94272 examples to avoid logging too frequently
train stats after 94336 examples: {'rewards_train/chosen': '-0.78759', 'rewards_train/rejected': '-1.1926', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.40502', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-145.17', 'loss/train': '0.63602', 'examples_per_second': '119.69', 'grad_norm': '24.384', 'counters/examples': 94336, 'counters/updates': 1474}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94464 examples: {'rewards_train/chosen': '-0.72026', 'rewards_train/rejected': '-1.3328', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.61255', 'logps_train/rejected': '-159.74', 'logps_train/chosen': '-167.77', 'loss/train': '0.55075', 'examples_per_second': '124.33', 'grad_norm': '23.374', 'counters/examples': 94464, 'counters/updates': 1476}
skipping logging after 94528 examples to avoid logging too frequently
train stats after 94592 examples: {'rewards_train/chosen': '-0.66145', 'rewards_train/rejected': '-1.0217', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.36024', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-157.76', 'loss/train': '0.62193', 'examples_per_second': '123.96', 'grad_norm': '24.028', 'counters/examples': 94592, 'counters/updates': 1478}
skipping logging after 94656 examples to avoid logging too frequently
train stats after 94720 examples: {'rewards_train/chosen': '-1.0471', 'rewards_train/rejected': '-1.3126', 'rewards_train/accuracies': '0.60938', 'rewards_train/margins': '0.26554', 'logps_train/rejected': '-110.65', 'logps_train/chosen': '-134.4', 'loss/train': '0.65586', 'examples_per_second': '124.52', 'grad_norm': '21.122', 'counters/examples': 94720, 'counters/updates': 1480}
skipping logging after 94784 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '-0.91244', 'rewards_train/rejected': '-1.3665', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.4541', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-143.55', 'loss/train': '0.61319', 'examples_per_second': '129.09', 'grad_norm': '23.049', 'counters/examples': 94848, 'counters/updates': 1482}
skipping logging after 94912 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '-1.0156', 'rewards_train/rejected': '-1.2684', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25279', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-161.43', 'loss/train': '0.6925', 'examples_per_second': '130.44', 'grad_norm': '25.653', 'counters/examples': 94976, 'counters/updates': 1484}
skipping logging after 95040 examples to avoid logging too frequently
train stats after 95104 examples: {'rewards_train/chosen': '-0.43462', 'rewards_train/rejected': '-0.84353', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.40891', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-170.38', 'loss/train': '0.61243', 'examples_per_second': '124.96', 'grad_norm': '24.014', 'counters/examples': 95104, 'counters/updates': 1486}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95232 examples: {'rewards_train/chosen': '-0.60954', 'rewards_train/rejected': '-0.94561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.33607', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-138.51', 'loss/train': '0.65007', 'examples_per_second': '134.79', 'grad_norm': '25.494', 'counters/examples': 95232, 'counters/updates': 1488}
skipping logging after 95296 examples to avoid logging too frequently
train stats after 95360 examples: {'rewards_train/chosen': '-0.7063', 'rewards_train/rejected': '-0.8901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1838', 'logps_train/rejected': '-165.21', 'logps_train/chosen': '-150.13', 'loss/train': '0.70759', 'examples_per_second': '124.48', 'grad_norm': '26.394', 'counters/examples': 95360, 'counters/updates': 1490}
skipping logging after 95424 examples to avoid logging too frequently
train stats after 95488 examples: {'rewards_train/chosen': '-0.69385', 'rewards_train/rejected': '-1.106', 'rewards_train/accuracies': '0.64062', 'rewards_train/margins': '0.41214', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-149.82', 'loss/train': '0.61968', 'examples_per_second': '124.11', 'grad_norm': '24.083', 'counters/examples': 95488, 'counters/updates': 1492}
skipping logging after 95552 examples to avoid logging too frequently
train stats after 95616 examples: {'rewards_train/chosen': '-0.63434', 'rewards_train/rejected': '-1.0499', 'rewards_train/accuracies': '0.67188', 'rewards_train/margins': '0.41558', 'logps_train/rejected': '-146.81', 'logps_train/chosen': '-172.82', 'loss/train': '0.57761', 'examples_per_second': '124.68', 'grad_norm': '24.983', 'counters/examples': 95616, 'counters/updates': 1494}
skipping logging after 95680 examples to avoid logging too frequently
train stats after 95744 examples: {'rewards_train/chosen': '-0.97239', 'rewards_train/rejected': '-1.3244', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.35201', 'logps_train/rejected': '-156.55', 'logps_train/chosen': '-147.72', 'loss/train': '0.65514', 'examples_per_second': '118.93', 'grad_norm': '24.614', 'counters/examples': 95744, 'counters/updates': 1496}
Running evaluation after 95744 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  12%|‚ñà‚ñé        | 2/16 [00:00<00:01, 10.18it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:01, 10.21it/s]Computing eval metrics:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:00<00:00, 10.29it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 10.26it/s]Computing eval metrics:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [00:00<00:00, 10.26it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:01<00:00, 10.25it/s]Computing eval metrics:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [00:01<00:00, 10.22it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.22it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00, 10.23it/s]
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
eval after 95744: {'rewards_eval/chosen': '-0.68255', 'rewards_eval/rejected': '-1.1279', 'rewards_eval/accuracies': '0.67578', 'rewards_eval/margins': '0.44531', 'logps_eval/rejected': '-137.36', 'logps_eval/chosen': '-154.73', 'loss/eval': '0.62103'}
creating checkpoint to write to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-95744...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-95744/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-95744/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/step-95744/scheduler.pt...
train stats after 95808 examples: {'rewards_train/chosen': '-0.82954', 'rewards_train/rejected': '-1.233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.40345', 'logps_train/rejected': '-151.82', 'logps_train/chosen': '-154.64', 'loss/train': '0.59282', 'examples_per_second': '102.83', 'grad_norm': '23.603', 'counters/examples': 95808, 'counters/updates': 1497}
skipping logging after 95872 examples to avoid logging too frequently
train stats after 95936 examples: {'rewards_train/chosen': '-0.64353', 'rewards_train/rejected': '-0.8862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24268', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-146.13', 'loss/train': '0.66895', 'examples_per_second': '124.52', 'grad_norm': '25.937', 'counters/examples': 95936, 'counters/updates': 1499}
skipping logging after 96000 examples to avoid logging too frequently
train stats after 96064 examples: {'rewards_train/chosen': '-0.66764', 'rewards_train/rejected': '-1.2168', 'rewards_train/accuracies': '0.73438', 'rewards_train/margins': '0.54916', 'logps_train/rejected': '-126.6', 'logps_train/chosen': '-178.36', 'loss/train': '0.54481', 'examples_per_second': '120.55', 'grad_norm': '21.517', 'counters/examples': 96064, 'counters/updates': 1501}
skipping logging after 96128 examples to avoid logging too frequently
train stats after 96192 examples: {'rewards_train/chosen': '-0.79864', 'rewards_train/rejected': '-1.1116', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.31292', 'logps_train/rejected': '-121.7', 'logps_train/chosen': '-135.21', 'loss/train': '0.64624', 'examples_per_second': '119.43', 'grad_norm': '27.451', 'counters/examples': 96192, 'counters/updates': 1503}
skipping logging after 96256 examples to avoid logging too frequently
Finished generating 1 epochs on train split
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/LATEST/policy.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/LATEST/optimizer.pt...
writing checkpoint to .cache/laura/pythia410m_dpo_seed0_2024-01-12_14-02-29_416329/LATEST/scheduler.pt...
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         counters/updates ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:      examples_per_second ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÑ
wandb:                grad_norm ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÖ
wandb:        logps_eval/chosen ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:      logps_eval/rejected ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:       logps_train/chosen ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÉ
wandb:     logps_train/rejected ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ
wandb:                loss/eval ‚ñà‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:               loss/train ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÉ
wandb:  rewards_eval/accuracies ‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:      rewards_eval/chosen ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:     rewards_eval/margins ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:    rewards_eval/rejected ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb: rewards_train/accuracies ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ
wandb:     rewards_train/chosen ‚ñà‚ñà‚ñá‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÅ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ
wandb:    rewards_train/margins ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ
wandb:   rewards_train/rejected ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ
wandb: 
wandb: Run summary:
wandb:        counters/examples 96192
wandb:         counters/updates 1503
wandb:      examples_per_second 119.43209
wandb:                grad_norm 27.45059
wandb:        logps_eval/chosen -154.72778
wandb:      logps_eval/rejected -137.35648
wandb:       logps_train/chosen -135.20567
wandb:     logps_train/rejected -121.70401
wandb:                loss/eval 0.62103
wandb:               loss/train 0.64624
wandb:  rewards_eval/accuracies 0.67578
wandb:      rewards_eval/chosen -0.68255
wandb:     rewards_eval/margins 0.44531
wandb:    rewards_eval/rejected -1.12785
wandb: rewards_train/accuracies 0.59375
wandb:     rewards_train/chosen -0.79864
wandb:    rewards_train/margins 0.31292
wandb:   rewards_train/rejected -1.11157
wandb: 
wandb: üöÄ View run pythia410m_dpo_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/5tkfezyq
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240112_140401-5tkfezyq/logs
WARNING: eval_every must be divisible by batch_size
Setting eval_every to 11968
no FSDP port specified; using open port for FSDP: 59727
seed: 0
exp_name: pythia70m_dpo_seed0
batch_size: 64
eval_batch_size: 16
debug: false
fsdp_port: 59727
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090
lr: 1.0e-06
gradient_accumulation_steps: 1
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 1
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 11968
minimum_log_interval_secs: 1.0
model:
  name_or_path: lomahony/pythia-70m-helpful-sft
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: float32
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-222-166:.cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090
================================================================================
building policy
building reference model
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240112_142239-natbqexw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia70m_dpo_seed0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: üöÄ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/natbqexw
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-70m-helpful-sft
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2888/5103 [00:00<00:00, 28870.46it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5103/5103 [00:00<00:00, 28979.58it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   3%|‚ñé         | 2726/96256 [00:00<00:03, 27246.77it/s]Processing HH static:   6%|‚ñå         | 5558/96256 [00:00<00:03, 27874.03it/s]Processing HH static:   9%|‚ñä         | 8412/96256 [00:00<00:03, 28174.92it/s]Processing HH static:  12%|‚ñà‚ñè        | 11230/96256 [00:00<00:05, 15916.23it/s]Processing HH static:  15%|‚ñà‚ñç        | 14075/96256 [00:00<00:04, 18942.59it/s]Processing HH static:  18%|‚ñà‚ñä        | 16908/96256 [00:00<00:03, 21350.27it/s]Processing HH static:  21%|‚ñà‚ñà        | 19747/96256 [00:00<00:03, 23237.42it/s]Processing HH static:  23%|‚ñà‚ñà‚ñé       | 22582/96256 [00:00<00:02, 24651.52it/s]Processing HH static:  26%|‚ñà‚ñà‚ñã       | 25419/96256 [00:01<00:02, 25703.87it/s]Processing HH static:  29%|‚ñà‚ñà‚ñâ       | 28257/96256 [00:01<00:02, 26474.93it/s]Processing HH static:  32%|‚ñà‚ñà‚ñà‚ñè      | 31025/96256 [00:01<00:02, 26805.74it/s]Processing HH static:  35%|‚ñà‚ñà‚ñà‚ñå      | 33791/96256 [00:01<00:02, 25245.91it/s]Processing HH static:  38%|‚ñà‚ñà‚ñà‚ñä      | 36458/96256 [00:01<00:02, 25645.24it/s]Processing HH static:  41%|‚ñà‚ñà‚ñà‚ñà      | 39262/96256 [00:01<00:02, 26329.51it/s]Processing HH static:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 41938/96256 [00:01<00:03, 15780.81it/s]Processing HH static:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 44641/96256 [00:02<00:02, 18012.56it/s]Processing HH static:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 47323/96256 [00:02<00:02, 19956.59it/s]Processing HH static:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 50021/96256 [00:02<00:02, 21640.19it/s]Processing HH static:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 52516/96256 [00:02<00:02, 14996.51it/s]Processing HH static:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 54515/96256 [00:03<00:04, 9230.67it/s] Processing HH static:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 57172/96256 [00:03<00:03, 11640.72it/s]Processing HH static:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 59866/96256 [00:03<00:02, 14174.65it/s]Processing HH static:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 62347/96256 [00:03<00:02, 16209.69it/s]Processing HH static:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 64998/96256 [00:03<00:01, 18420.18it/s]Processing HH static:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 67350/96256 [00:03<00:01, 19237.18it/s]Processing HH static:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 70031/96256 [00:03<00:01, 21110.06it/s]Processing HH static:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 72709/96256 [00:03<00:01, 22590.56it/s]Processing HH static:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 75201/96256 [00:04<00:02, 9487.86it/s] Processing HH static:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 77064/96256 [00:05<00:03, 5598.35it/s]Processing HH static:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 78441/96256 [00:05<00:03, 4593.65it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 79485/96256 [00:06<00:04, 4153.64it/s]Processing HH static:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 80302/96256 [00:06<00:03, 4013.78it/s]Processing HH static:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 80977/96256 [00:06<00:04, 3760.10it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 81535/96256 [00:06<00:04, 3457.68it/s]Processing HH static:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 81999/96256 [00:06<00:04, 3482.96it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 82433/96256 [00:07<00:05, 2698.98it/s]Processing HH static:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83215/96256 [00:07<00:03, 3393.10it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 83688/96256 [00:07<00:04, 3101.05it/s]Processing HH static:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84090/96256 [00:07<00:04, 3007.92it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84452/96256 [00:07<00:04, 2926.44it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 84785/96256 [00:07<00:03, 2933.11it/s]Processing HH static:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85107/96256 [00:07<00:03, 2880.40it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85414/96256 [00:08<00:03, 2775.18it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 85704/96256 [00:08<00:03, 2685.20it/s]Processing HH static:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86117/96256 [00:08<00:03, 3030.64it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 86433/96256 [00:08<00:03, 2786.81it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86723/96256 [00:08<00:03, 2669.33it/s]Processing HH static:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 86998/96256 [00:08<00:03, 2610.01it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87264/96256 [00:08<00:03, 2506.66it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87518/96256 [00:08<00:03, 2429.27it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 87763/96256 [00:09<00:03, 2342.59it/s]Processing HH static:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88033/96256 [00:09<00:03, 2433.66it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88279/96256 [00:09<00:03, 2320.94it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88513/96256 [00:09<00:03, 2267.91it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 88741/96256 [00:09<00:03, 2243.00it/s]Processing HH static:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 89001/96256 [00:09<00:03, 2311.76it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89233/96256 [00:09<00:03, 2301.72it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89464/96256 [00:09<00:02, 2296.66it/s]Processing HH static:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 89780/96256 [00:09<00:02, 2545.31it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90071/96256 [00:09<00:02, 2647.45it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90337/96256 [00:10<00:02, 2534.37it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90592/96256 [00:10<00:02, 2463.31it/s]Processing HH static:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 90840/96256 [00:10<00:02, 2445.30it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91143/96256 [00:10<00:01, 2606.80it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91405/96256 [00:10<00:01, 2499.00it/s]Processing HH static:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 91757/96256 [00:10<00:01, 2784.53it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92038/96256 [00:10<00:01, 2640.45it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92306/96256 [00:10<00:01, 2547.04it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 92564/96256 [00:10<00:01, 2486.30it/s]Processing HH static:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 92815/96256 [00:11<00:01, 2477.01it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93064/96256 [00:11<00:01, 2363.57it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93302/96256 [00:11<00:01, 2345.37it/s]Processing HH static:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 93665/96256 [00:11<00:00, 2707.88it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94070/96256 [00:11<00:00, 3085.90it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94382/96256 [00:11<00:00, 2881.50it/s]Processing HH static:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94675/96256 [00:11<00:00, 2771.43it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 94956/96256 [00:11<00:00, 2639.27it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95224/96256 [00:11<00:00, 2494.30it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95477/96256 [00:12<00:00, 2454.87it/s]Processing HH static:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 95759/96256 [00:12<00:00, 2549.79it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96146/96256 [00:12<00:00, 2914.43it/s]Processing HH static: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96256/96256 [00:12<00:00, 7795.10it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|‚ñã         | 1/16 [00:01<00:27,  1.81s/it]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:01<00:04,  2.70it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:02<00:01,  6.09it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:02<00:00,  9.90it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00, 13.69it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  7.04it/s]
eval after 0: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': '-186.19', 'logps_eval/chosen': '-222.17', 'loss/eval': 'nan'}
train stats after 64 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': '-212.74', 'logps_train/chosen': '-219.17', 'loss/train': 'nan', 'examples_per_second': '334.54', 'grad_norm': 'nan', 'counters/examples': 64, 'counters/updates': 1}
skipping logging after 128 examples to avoid logging too frequently
skipping logging after 192 examples to avoid logging too frequently
skipping logging after 256 examples to avoid logging too frequently
skipping logging after 320 examples to avoid logging too frequently
skipping logging after 384 examples to avoid logging too frequently
train stats after 448 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.57', 'grad_norm': 'nan', 'counters/examples': 448, 'counters/updates': 7}
skipping logging after 512 examples to avoid logging too frequently
skipping logging after 576 examples to avoid logging too frequently
skipping logging after 640 examples to avoid logging too frequently
skipping logging after 704 examples to avoid logging too frequently
skipping logging after 768 examples to avoid logging too frequently
train stats after 832 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '564.02', 'grad_norm': 'nan', 'counters/examples': 832, 'counters/updates': 13}
skipping logging after 896 examples to avoid logging too frequently
skipping logging after 960 examples to avoid logging too frequently
skipping logging after 1024 examples to avoid logging too frequently
skipping logging after 1088 examples to avoid logging too frequently
skipping logging after 1152 examples to avoid logging too frequently
train stats after 1216 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '580.41', 'grad_norm': 'nan', 'counters/examples': 1216, 'counters/updates': 19}
skipping logging after 1280 examples to avoid logging too frequently
skipping logging after 1344 examples to avoid logging too frequently
skipping logging after 1408 examples to avoid logging too frequently
skipping logging after 1472 examples to avoid logging too frequently
skipping logging after 1536 examples to avoid logging too frequently
train stats after 1600 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '585.23', 'grad_norm': 'nan', 'counters/examples': 1600, 'counters/updates': 25}
skipping logging after 1664 examples to avoid logging too frequently
skipping logging after 1728 examples to avoid logging too frequently
skipping logging after 1792 examples to avoid logging too frequently
skipping logging after 1856 examples to avoid logging too frequently
skipping logging after 1920 examples to avoid logging too frequently
train stats after 1984 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.51', 'grad_norm': 'nan', 'counters/examples': 1984, 'counters/updates': 31}
skipping logging after 2048 examples to avoid logging too frequently
skipping logging after 2112 examples to avoid logging too frequently
skipping logging after 2176 examples to avoid logging too frequently
skipping logging after 2240 examples to avoid logging too frequently
skipping logging after 2304 examples to avoid logging too frequently
train stats after 2368 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.21', 'grad_norm': 'nan', 'counters/examples': 2368, 'counters/updates': 37}
skipping logging after 2432 examples to avoid logging too frequently
skipping logging after 2496 examples to avoid logging too frequently
skipping logging after 2560 examples to avoid logging too frequently
skipping logging after 2624 examples to avoid logging too frequently
skipping logging after 2688 examples to avoid logging too frequently
train stats after 2752 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '606.44', 'grad_norm': 'nan', 'counters/examples': 2752, 'counters/updates': 43}
skipping logging after 2816 examples to avoid logging too frequently
skipping logging after 2880 examples to avoid logging too frequently
skipping logging after 2944 examples to avoid logging too frequently
skipping logging after 3008 examples to avoid logging too frequently
skipping logging after 3072 examples to avoid logging too frequently
train stats after 3136 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '626', 'grad_norm': 'nan', 'counters/examples': 3136, 'counters/updates': 49}
skipping logging after 3200 examples to avoid logging too frequently
skipping logging after 3264 examples to avoid logging too frequently
skipping logging after 3328 examples to avoid logging too frequently
skipping logging after 3392 examples to avoid logging too frequently
skipping logging after 3456 examples to avoid logging too frequently
train stats after 3520 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '585.56', 'grad_norm': 'nan', 'counters/examples': 3520, 'counters/updates': 55}
skipping logging after 3584 examples to avoid logging too frequently
skipping logging after 3648 examples to avoid logging too frequently
skipping logging after 3712 examples to avoid logging too frequently
skipping logging after 3776 examples to avoid logging too frequently
skipping logging after 3840 examples to avoid logging too frequently
train stats after 3904 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '594.81', 'grad_norm': 'nan', 'counters/examples': 3904, 'counters/updates': 61}
skipping logging after 3968 examples to avoid logging too frequently
skipping logging after 4032 examples to avoid logging too frequently
skipping logging after 4096 examples to avoid logging too frequently
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4224 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '589.75', 'grad_norm': 'nan', 'counters/examples': 4224, 'counters/updates': 66}
skipping logging after 4288 examples to avoid logging too frequently
skipping logging after 4352 examples to avoid logging too frequently
skipping logging after 4416 examples to avoid logging too frequently
skipping logging after 4480 examples to avoid logging too frequently
skipping logging after 4544 examples to avoid logging too frequently
train stats after 4608 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.48', 'grad_norm': 'nan', 'counters/examples': 4608, 'counters/updates': 72}
skipping logging after 4672 examples to avoid logging too frequently
skipping logging after 4736 examples to avoid logging too frequently
skipping logging after 4800 examples to avoid logging too frequently
skipping logging after 4864 examples to avoid logging too frequently
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4992 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '594.4', 'grad_norm': 'nan', 'counters/examples': 4992, 'counters/updates': 78}
skipping logging after 5056 examples to avoid logging too frequently
skipping logging after 5120 examples to avoid logging too frequently
skipping logging after 5184 examples to avoid logging too frequently
skipping logging after 5248 examples to avoid logging too frequently
skipping logging after 5312 examples to avoid logging too frequently
train stats after 5376 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.63', 'grad_norm': 'nan', 'counters/examples': 5376, 'counters/updates': 84}
skipping logging after 5440 examples to avoid logging too frequently
skipping logging after 5504 examples to avoid logging too frequently
skipping logging after 5568 examples to avoid logging too frequently
skipping logging after 5632 examples to avoid logging too frequently
skipping logging after 5696 examples to avoid logging too frequently
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5824 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '605.94', 'grad_norm': 'nan', 'counters/examples': 5824, 'counters/updates': 91}
skipping logging after 5888 examples to avoid logging too frequently
skipping logging after 5952 examples to avoid logging too frequently
skipping logging after 6016 examples to avoid logging too frequently
skipping logging after 6080 examples to avoid logging too frequently
skipping logging after 6144 examples to avoid logging too frequently
train stats after 6208 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '619.19', 'grad_norm': 'nan', 'counters/examples': 6208, 'counters/updates': 97}
skipping logging after 6272 examples to avoid logging too frequently
skipping logging after 6336 examples to avoid logging too frequently
skipping logging after 6400 examples to avoid logging too frequently
skipping logging after 6464 examples to avoid logging too frequently
skipping logging after 6528 examples to avoid logging too frequently
train stats after 6592 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.85', 'grad_norm': 'nan', 'counters/examples': 6592, 'counters/updates': 103}
skipping logging after 6656 examples to avoid logging too frequently
skipping logging after 6720 examples to avoid logging too frequently
skipping logging after 6784 examples to avoid logging too frequently
skipping logging after 6848 examples to avoid logging too frequently
skipping logging after 6912 examples to avoid logging too frequently
train stats after 6976 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.91', 'grad_norm': 'nan', 'counters/examples': 6976, 'counters/updates': 109}
skipping logging after 7040 examples to avoid logging too frequently
skipping logging after 7104 examples to avoid logging too frequently
skipping logging after 7168 examples to avoid logging too frequently
skipping logging after 7232 examples to avoid logging too frequently
skipping logging after 7296 examples to avoid logging too frequently
train stats after 7360 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '619.86', 'grad_norm': 'nan', 'counters/examples': 7360, 'counters/updates': 115}
skipping logging after 7424 examples to avoid logging too frequently
skipping logging after 7488 examples to avoid logging too frequently
skipping logging after 7552 examples to avoid logging too frequently
skipping logging after 7616 examples to avoid logging too frequently
skipping logging after 7680 examples to avoid logging too frequently
train stats after 7744 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '589.36', 'grad_norm': 'nan', 'counters/examples': 7744, 'counters/updates': 121}
skipping logging after 7808 examples to avoid logging too frequently
skipping logging after 7872 examples to avoid logging too frequently
skipping logging after 7936 examples to avoid logging too frequently
skipping logging after 8000 examples to avoid logging too frequently
skipping logging after 8064 examples to avoid logging too frequently
train stats after 8128 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.02', 'grad_norm': 'nan', 'counters/examples': 8128, 'counters/updates': 127}
skipping logging after 8192 examples to avoid logging too frequently
skipping logging after 8256 examples to avoid logging too frequently
skipping logging after 8320 examples to avoid logging too frequently
skipping logging after 8384 examples to avoid logging too frequently
skipping logging after 8448 examples to avoid logging too frequently
train stats after 8512 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.93', 'grad_norm': 'nan', 'counters/examples': 8512, 'counters/updates': 133}
skipping logging after 8576 examples to avoid logging too frequently
skipping logging after 8640 examples to avoid logging too frequently
skipping logging after 8704 examples to avoid logging too frequently
train stats after 8768 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '148.41', 'grad_norm': 'nan', 'counters/examples': 8768, 'counters/updates': 137}
skipping logging after 8832 examples to avoid logging too frequently
skipping logging after 8896 examples to avoid logging too frequently
skipping logging after 8960 examples to avoid logging too frequently
skipping logging after 9024 examples to avoid logging too frequently
skipping logging after 9088 examples to avoid logging too frequently
train stats after 9152 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '591.69', 'grad_norm': 'nan', 'counters/examples': 9152, 'counters/updates': 143}
skipping logging after 9216 examples to avoid logging too frequently
skipping logging after 9280 examples to avoid logging too frequently
skipping logging after 9344 examples to avoid logging too frequently
skipping logging after 9408 examples to avoid logging too frequently
skipping logging after 9472 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.86', 'grad_norm': 'nan', 'counters/examples': 9536, 'counters/updates': 149}
skipping logging after 9600 examples to avoid logging too frequently
skipping logging after 9664 examples to avoid logging too frequently
skipping logging after 9728 examples to avoid logging too frequently
skipping logging after 9792 examples to avoid logging too frequently
skipping logging after 9856 examples to avoid logging too frequently
train stats after 9920 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.04', 'grad_norm': 'nan', 'counters/examples': 9920, 'counters/updates': 155}
skipping logging after 9984 examples to avoid logging too frequently
skipping logging after 10048 examples to avoid logging too frequently
skipping logging after 10112 examples to avoid logging too frequently
skipping logging after 10176 examples to avoid logging too frequently
skipping logging after 10240 examples to avoid logging too frequently
train stats after 10304 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.48', 'grad_norm': 'nan', 'counters/examples': 10304, 'counters/updates': 161}
skipping logging after 10368 examples to avoid logging too frequently
skipping logging after 10432 examples to avoid logging too frequently
skipping logging after 10496 examples to avoid logging too frequently
skipping logging after 10560 examples to avoid logging too frequently
skipping logging after 10624 examples to avoid logging too frequently
train stats after 10688 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.01', 'grad_norm': 'nan', 'counters/examples': 10688, 'counters/updates': 167}
skipping logging after 10752 examples to avoid logging too frequently
skipping logging after 10816 examples to avoid logging too frequently
skipping logging after 10880 examples to avoid logging too frequently
skipping logging after 10944 examples to avoid logging too frequently
skipping logging after 11008 examples to avoid logging too frequently
train stats after 11072 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.85', 'grad_norm': 'nan', 'counters/examples': 11072, 'counters/updates': 173}
skipping logging after 11136 examples to avoid logging too frequently
skipping logging after 11200 examples to avoid logging too frequently
skipping logging after 11264 examples to avoid logging too frequently
skipping logging after 11328 examples to avoid logging too frequently
skipping logging after 11392 examples to avoid logging too frequently
train stats after 11456 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '581.24', 'grad_norm': 'nan', 'counters/examples': 11456, 'counters/updates': 179}
skipping logging after 11520 examples to avoid logging too frequently
skipping logging after 11584 examples to avoid logging too frequently
skipping logging after 11648 examples to avoid logging too frequently
skipping logging after 11712 examples to avoid logging too frequently
skipping logging after 11776 examples to avoid logging too frequently
train stats after 11840 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '580.15', 'grad_norm': 'nan', 'counters/examples': 11840, 'counters/updates': 185}
skipping logging after 11904 examples to avoid logging too frequently
skipping logging after 11968 examples to avoid logging too frequently
Running evaluation after 11968 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 35.88it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 34.93it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 35.58it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 35.85it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 35.65it/s]
eval after 11968: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-11968...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-11968/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-11968/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-11968/scheduler.pt...
train stats after 12032 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '530.64', 'grad_norm': 'nan', 'counters/examples': 12032, 'counters/updates': 188}
skipping logging after 12096 examples to avoid logging too frequently
skipping logging after 12160 examples to avoid logging too frequently
skipping logging after 12224 examples to avoid logging too frequently
skipping logging after 12288 examples to avoid logging too frequently
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12416 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.54', 'grad_norm': 'nan', 'counters/examples': 12416, 'counters/updates': 194}
skipping logging after 12480 examples to avoid logging too frequently
skipping logging after 12544 examples to avoid logging too frequently
skipping logging after 12608 examples to avoid logging too frequently
skipping logging after 12672 examples to avoid logging too frequently
skipping logging after 12736 examples to avoid logging too frequently
train stats after 12800 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.38', 'grad_norm': 'nan', 'counters/examples': 12800, 'counters/updates': 200}
skipping logging after 12864 examples to avoid logging too frequently
skipping logging after 12928 examples to avoid logging too frequently
skipping logging after 12992 examples to avoid logging too frequently
skipping logging after 13056 examples to avoid logging too frequently
skipping logging after 13120 examples to avoid logging too frequently
train stats after 13184 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.98', 'grad_norm': 'nan', 'counters/examples': 13184, 'counters/updates': 206}
skipping logging after 13248 examples to avoid logging too frequently
skipping logging after 13312 examples to avoid logging too frequently
skipping logging after 13376 examples to avoid logging too frequently
skipping logging after 13440 examples to avoid logging too frequently
skipping logging after 13504 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.68', 'grad_norm': 'nan', 'counters/examples': 13568, 'counters/updates': 212}
skipping logging after 13632 examples to avoid logging too frequently
skipping logging after 13696 examples to avoid logging too frequently
skipping logging after 13760 examples to avoid logging too frequently
skipping logging after 13824 examples to avoid logging too frequently
skipping logging after 13888 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '634.13', 'grad_norm': 'nan', 'counters/examples': 13952, 'counters/updates': 218}
skipping logging after 14016 examples to avoid logging too frequently
skipping logging after 14080 examples to avoid logging too frequently
skipping logging after 14144 examples to avoid logging too frequently
skipping logging after 14208 examples to avoid logging too frequently
skipping logging after 14272 examples to avoid logging too frequently
skipping logging after 14336 examples to avoid logging too frequently
train stats after 14400 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '579.56', 'grad_norm': 'nan', 'counters/examples': 14400, 'counters/updates': 225}
skipping logging after 14464 examples to avoid logging too frequently
skipping logging after 14528 examples to avoid logging too frequently
skipping logging after 14592 examples to avoid logging too frequently
skipping logging after 14656 examples to avoid logging too frequently
skipping logging after 14720 examples to avoid logging too frequently
train stats after 14784 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.84', 'grad_norm': 'nan', 'counters/examples': 14784, 'counters/updates': 231}
skipping logging after 14848 examples to avoid logging too frequently
skipping logging after 14912 examples to avoid logging too frequently
skipping logging after 14976 examples to avoid logging too frequently
skipping logging after 15040 examples to avoid logging too frequently
skipping logging after 15104 examples to avoid logging too frequently
train stats after 15168 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.34', 'grad_norm': 'nan', 'counters/examples': 15168, 'counters/updates': 237}
skipping logging after 15232 examples to avoid logging too frequently
skipping logging after 15296 examples to avoid logging too frequently
skipping logging after 15360 examples to avoid logging too frequently
skipping logging after 15424 examples to avoid logging too frequently
skipping logging after 15488 examples to avoid logging too frequently
train stats after 15552 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '616.05', 'grad_norm': 'nan', 'counters/examples': 15552, 'counters/updates': 243}
skipping logging after 15616 examples to avoid logging too frequently
skipping logging after 15680 examples to avoid logging too frequently
skipping logging after 15744 examples to avoid logging too frequently
skipping logging after 15808 examples to avoid logging too frequently
skipping logging after 15872 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '662.36', 'grad_norm': 'nan', 'counters/examples': 15936, 'counters/updates': 249}
skipping logging after 16000 examples to avoid logging too frequently
skipping logging after 16064 examples to avoid logging too frequently
skipping logging after 16128 examples to avoid logging too frequently
skipping logging after 16192 examples to avoid logging too frequently
skipping logging after 16256 examples to avoid logging too frequently
train stats after 16320 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '593.86', 'grad_norm': 'nan', 'counters/examples': 16320, 'counters/updates': 255}
skipping logging after 16384 examples to avoid logging too frequently
skipping logging after 16448 examples to avoid logging too frequently
skipping logging after 16512 examples to avoid logging too frequently
skipping logging after 16576 examples to avoid logging too frequently
skipping logging after 16640 examples to avoid logging too frequently
train stats after 16704 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '574.75', 'grad_norm': 'nan', 'counters/examples': 16704, 'counters/updates': 261}
skipping logging after 16768 examples to avoid logging too frequently
skipping logging after 16832 examples to avoid logging too frequently
skipping logging after 16896 examples to avoid logging too frequently
skipping logging after 16960 examples to avoid logging too frequently
skipping logging after 17024 examples to avoid logging too frequently
train stats after 17088 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.62', 'grad_norm': 'nan', 'counters/examples': 17088, 'counters/updates': 267}
skipping logging after 17152 examples to avoid logging too frequently
skipping logging after 17216 examples to avoid logging too frequently
skipping logging after 17280 examples to avoid logging too frequently
skipping logging after 17344 examples to avoid logging too frequently
skipping logging after 17408 examples to avoid logging too frequently
train stats after 17472 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '588.17', 'grad_norm': 'nan', 'counters/examples': 17472, 'counters/updates': 273}
skipping logging after 17536 examples to avoid logging too frequently
skipping logging after 17600 examples to avoid logging too frequently
skipping logging after 17664 examples to avoid logging too frequently
skipping logging after 17728 examples to avoid logging too frequently
skipping logging after 17792 examples to avoid logging too frequently
train stats after 17856 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '593.37', 'grad_norm': 'nan', 'counters/examples': 17856, 'counters/updates': 279}
skipping logging after 17920 examples to avoid logging too frequently
skipping logging after 17984 examples to avoid logging too frequently
skipping logging after 18048 examples to avoid logging too frequently
skipping logging after 18112 examples to avoid logging too frequently
skipping logging after 18176 examples to avoid logging too frequently
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18304 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '590.66', 'grad_norm': 'nan', 'counters/examples': 18304, 'counters/updates': 286}
skipping logging after 18368 examples to avoid logging too frequently
skipping logging after 18432 examples to avoid logging too frequently
skipping logging after 18496 examples to avoid logging too frequently
skipping logging after 18560 examples to avoid logging too frequently
skipping logging after 18624 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '606.52', 'grad_norm': 'nan', 'counters/examples': 18688, 'counters/updates': 292}
skipping logging after 18752 examples to avoid logging too frequently
skipping logging after 18816 examples to avoid logging too frequently
skipping logging after 18880 examples to avoid logging too frequently
skipping logging after 18944 examples to avoid logging too frequently
skipping logging after 19008 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '579.36', 'grad_norm': 'nan', 'counters/examples': 19072, 'counters/updates': 298}
skipping logging after 19136 examples to avoid logging too frequently
skipping logging after 19200 examples to avoid logging too frequently
skipping logging after 19264 examples to avoid logging too frequently
skipping logging after 19328 examples to avoid logging too frequently
skipping logging after 19392 examples to avoid logging too frequently
train stats after 19456 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.97', 'grad_norm': 'nan', 'counters/examples': 19456, 'counters/updates': 304}
skipping logging after 19520 examples to avoid logging too frequently
skipping logging after 19584 examples to avoid logging too frequently
skipping logging after 19648 examples to avoid logging too frequently
skipping logging after 19712 examples to avoid logging too frequently
skipping logging after 19776 examples to avoid logging too frequently
train stats after 19840 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.95', 'grad_norm': 'nan', 'counters/examples': 19840, 'counters/updates': 310}
skipping logging after 19904 examples to avoid logging too frequently
skipping logging after 19968 examples to avoid logging too frequently
skipping logging after 20032 examples to avoid logging too frequently
skipping logging after 20096 examples to avoid logging too frequently
skipping logging after 20160 examples to avoid logging too frequently
train stats after 20224 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.56', 'grad_norm': 'nan', 'counters/examples': 20224, 'counters/updates': 316}
skipping logging after 20288 examples to avoid logging too frequently
skipping logging after 20352 examples to avoid logging too frequently
skipping logging after 20416 examples to avoid logging too frequently
skipping logging after 20480 examples to avoid logging too frequently
skipping logging after 20544 examples to avoid logging too frequently
train stats after 20608 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.32', 'grad_norm': 'nan', 'counters/examples': 20608, 'counters/updates': 322}
skipping logging after 20672 examples to avoid logging too frequently
skipping logging after 20736 examples to avoid logging too frequently
skipping logging after 20800 examples to avoid logging too frequently
skipping logging after 20864 examples to avoid logging too frequently
skipping logging after 20928 examples to avoid logging too frequently
train stats after 20992 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.68', 'grad_norm': 'nan', 'counters/examples': 20992, 'counters/updates': 328}
skipping logging after 21056 examples to avoid logging too frequently
skipping logging after 21120 examples to avoid logging too frequently
skipping logging after 21184 examples to avoid logging too frequently
skipping logging after 21248 examples to avoid logging too frequently
skipping logging after 21312 examples to avoid logging too frequently
train stats after 21376 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '605.66', 'grad_norm': 'nan', 'counters/examples': 21376, 'counters/updates': 334}
skipping logging after 21440 examples to avoid logging too frequently
skipping logging after 21504 examples to avoid logging too frequently
skipping logging after 21568 examples to avoid logging too frequently
skipping logging after 21632 examples to avoid logging too frequently
skipping logging after 21696 examples to avoid logging too frequently
train stats after 21760 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '577.38', 'grad_norm': 'nan', 'counters/examples': 21760, 'counters/updates': 340}
skipping logging after 21824 examples to avoid logging too frequently
skipping logging after 21888 examples to avoid logging too frequently
skipping logging after 21952 examples to avoid logging too frequently
skipping logging after 22016 examples to avoid logging too frequently
skipping logging after 22080 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '594.42', 'grad_norm': 'nan', 'counters/examples': 22144, 'counters/updates': 346}
skipping logging after 22208 examples to avoid logging too frequently
skipping logging after 22272 examples to avoid logging too frequently
skipping logging after 22336 examples to avoid logging too frequently
skipping logging after 22400 examples to avoid logging too frequently
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22528 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.67', 'grad_norm': 'nan', 'counters/examples': 22528, 'counters/updates': 352}
skipping logging after 22592 examples to avoid logging too frequently
skipping logging after 22656 examples to avoid logging too frequently
skipping logging after 22720 examples to avoid logging too frequently
skipping logging after 22784 examples to avoid logging too frequently
skipping logging after 22848 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '671.53', 'grad_norm': 'nan', 'counters/examples': 22912, 'counters/updates': 358}
skipping logging after 22976 examples to avoid logging too frequently
skipping logging after 23040 examples to avoid logging too frequently
skipping logging after 23104 examples to avoid logging too frequently
skipping logging after 23168 examples to avoid logging too frequently
skipping logging after 23232 examples to avoid logging too frequently
train stats after 23296 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.53', 'grad_norm': 'nan', 'counters/examples': 23296, 'counters/updates': 364}
skipping logging after 23360 examples to avoid logging too frequently
skipping logging after 23424 examples to avoid logging too frequently
skipping logging after 23488 examples to avoid logging too frequently
skipping logging after 23552 examples to avoid logging too frequently
skipping logging after 23616 examples to avoid logging too frequently
train stats after 23680 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.81', 'grad_norm': 'nan', 'counters/examples': 23680, 'counters/updates': 370}
skipping logging after 23744 examples to avoid logging too frequently
skipping logging after 23808 examples to avoid logging too frequently
skipping logging after 23872 examples to avoid logging too frequently
skipping logging after 23936 examples to avoid logging too frequently
Running evaluation after 23936 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 36.17it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.03it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 36.34it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.31it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.23it/s]
eval after 23936: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-23936...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-23936/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-23936/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-23936/scheduler.pt...
train stats after 24000 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '517.06', 'grad_norm': 'nan', 'counters/examples': 24000, 'counters/updates': 375}
skipping logging after 24064 examples to avoid logging too frequently
skipping logging after 24128 examples to avoid logging too frequently
skipping logging after 24192 examples to avoid logging too frequently
skipping logging after 24256 examples to avoid logging too frequently
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24384 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.78', 'grad_norm': 'nan', 'counters/examples': 24384, 'counters/updates': 381}
skipping logging after 24448 examples to avoid logging too frequently
skipping logging after 24512 examples to avoid logging too frequently
skipping logging after 24576 examples to avoid logging too frequently
skipping logging after 24640 examples to avoid logging too frequently
skipping logging after 24704 examples to avoid logging too frequently
train stats after 24768 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '614.57', 'grad_norm': 'nan', 'counters/examples': 24768, 'counters/updates': 387}
skipping logging after 24832 examples to avoid logging too frequently
skipping logging after 24896 examples to avoid logging too frequently
skipping logging after 24960 examples to avoid logging too frequently
skipping logging after 25024 examples to avoid logging too frequently
skipping logging after 25088 examples to avoid logging too frequently
train stats after 25152 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.32', 'grad_norm': 'nan', 'counters/examples': 25152, 'counters/updates': 393}
skipping logging after 25216 examples to avoid logging too frequently
skipping logging after 25280 examples to avoid logging too frequently
skipping logging after 25344 examples to avoid logging too frequently
skipping logging after 25408 examples to avoid logging too frequently
skipping logging after 25472 examples to avoid logging too frequently
train stats after 25536 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.88', 'grad_norm': 'nan', 'counters/examples': 25536, 'counters/updates': 399}
skipping logging after 25600 examples to avoid logging too frequently
skipping logging after 25664 examples to avoid logging too frequently
skipping logging after 25728 examples to avoid logging too frequently
skipping logging after 25792 examples to avoid logging too frequently
skipping logging after 25856 examples to avoid logging too frequently
train stats after 25920 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.58', 'grad_norm': 'nan', 'counters/examples': 25920, 'counters/updates': 405}
skipping logging after 25984 examples to avoid logging too frequently
skipping logging after 26048 examples to avoid logging too frequently
skipping logging after 26112 examples to avoid logging too frequently
skipping logging after 26176 examples to avoid logging too frequently
skipping logging after 26240 examples to avoid logging too frequently
train stats after 26304 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '628.05', 'grad_norm': 'nan', 'counters/examples': 26304, 'counters/updates': 411}
skipping logging after 26368 examples to avoid logging too frequently
skipping logging after 26432 examples to avoid logging too frequently
skipping logging after 26496 examples to avoid logging too frequently
skipping logging after 26560 examples to avoid logging too frequently
skipping logging after 26624 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.69', 'grad_norm': 'nan', 'counters/examples': 26688, 'counters/updates': 417}
skipping logging after 26752 examples to avoid logging too frequently
skipping logging after 26816 examples to avoid logging too frequently
skipping logging after 26880 examples to avoid logging too frequently
skipping logging after 26944 examples to avoid logging too frequently
skipping logging after 27008 examples to avoid logging too frequently
train stats after 27072 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.89', 'grad_norm': 'nan', 'counters/examples': 27072, 'counters/updates': 423}
skipping logging after 27136 examples to avoid logging too frequently
skipping logging after 27200 examples to avoid logging too frequently
skipping logging after 27264 examples to avoid logging too frequently
skipping logging after 27328 examples to avoid logging too frequently
skipping logging after 27392 examples to avoid logging too frequently
train stats after 27456 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.89', 'grad_norm': 'nan', 'counters/examples': 27456, 'counters/updates': 429}
skipping logging after 27520 examples to avoid logging too frequently
skipping logging after 27584 examples to avoid logging too frequently
skipping logging after 27648 examples to avoid logging too frequently
skipping logging after 27712 examples to avoid logging too frequently
skipping logging after 27776 examples to avoid logging too frequently
train stats after 27840 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.76', 'grad_norm': 'nan', 'counters/examples': 27840, 'counters/updates': 435}
skipping logging after 27904 examples to avoid logging too frequently
skipping logging after 27968 examples to avoid logging too frequently
skipping logging after 28032 examples to avoid logging too frequently
skipping logging after 28096 examples to avoid logging too frequently
skipping logging after 28160 examples to avoid logging too frequently
train stats after 28224 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '582.56', 'grad_norm': 'nan', 'counters/examples': 28224, 'counters/updates': 441}
skipping logging after 28288 examples to avoid logging too frequently
skipping logging after 28352 examples to avoid logging too frequently
skipping logging after 28416 examples to avoid logging too frequently
skipping logging after 28480 examples to avoid logging too frequently
skipping logging after 28544 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '635.48', 'grad_norm': 'nan', 'counters/examples': 28608, 'counters/updates': 447}
skipping logging after 28672 examples to avoid logging too frequently
skipping logging after 28736 examples to avoid logging too frequently
skipping logging after 28800 examples to avoid logging too frequently
skipping logging after 28864 examples to avoid logging too frequently
skipping logging after 28928 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '612.66', 'grad_norm': 'nan', 'counters/examples': 28992, 'counters/updates': 453}
skipping logging after 29056 examples to avoid logging too frequently
skipping logging after 29120 examples to avoid logging too frequently
skipping logging after 29184 examples to avoid logging too frequently
skipping logging after 29248 examples to avoid logging too frequently
skipping logging after 29312 examples to avoid logging too frequently
train stats after 29376 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '590.64', 'grad_norm': 'nan', 'counters/examples': 29376, 'counters/updates': 459}
skipping logging after 29440 examples to avoid logging too frequently
skipping logging after 29504 examples to avoid logging too frequently
skipping logging after 29568 examples to avoid logging too frequently
skipping logging after 29632 examples to avoid logging too frequently
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29760 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '568.09', 'grad_norm': 'nan', 'counters/examples': 29760, 'counters/updates': 465}
skipping logging after 29824 examples to avoid logging too frequently
skipping logging after 29888 examples to avoid logging too frequently
skipping logging after 29952 examples to avoid logging too frequently
skipping logging after 30016 examples to avoid logging too frequently
skipping logging after 30080 examples to avoid logging too frequently
train stats after 30144 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '576.89', 'grad_norm': 'nan', 'counters/examples': 30144, 'counters/updates': 471}
skipping logging after 30208 examples to avoid logging too frequently
skipping logging after 30272 examples to avoid logging too frequently
skipping logging after 30336 examples to avoid logging too frequently
skipping logging after 30400 examples to avoid logging too frequently
skipping logging after 30464 examples to avoid logging too frequently
train stats after 30528 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.89', 'grad_norm': 'nan', 'counters/examples': 30528, 'counters/updates': 477}
skipping logging after 30592 examples to avoid logging too frequently
skipping logging after 30656 examples to avoid logging too frequently
skipping logging after 30720 examples to avoid logging too frequently
skipping logging after 30784 examples to avoid logging too frequently
skipping logging after 30848 examples to avoid logging too frequently
train stats after 30912 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.38', 'grad_norm': 'nan', 'counters/examples': 30912, 'counters/updates': 483}
skipping logging after 30976 examples to avoid logging too frequently
skipping logging after 31040 examples to avoid logging too frequently
skipping logging after 31104 examples to avoid logging too frequently
skipping logging after 31168 examples to avoid logging too frequently
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31296 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.14', 'grad_norm': 'nan', 'counters/examples': 31296, 'counters/updates': 489}
skipping logging after 31360 examples to avoid logging too frequently
skipping logging after 31424 examples to avoid logging too frequently
skipping logging after 31488 examples to avoid logging too frequently
skipping logging after 31552 examples to avoid logging too frequently
skipping logging after 31616 examples to avoid logging too frequently
train stats after 31680 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.58', 'grad_norm': 'nan', 'counters/examples': 31680, 'counters/updates': 495}
skipping logging after 31744 examples to avoid logging too frequently
skipping logging after 31808 examples to avoid logging too frequently
skipping logging after 31872 examples to avoid logging too frequently
skipping logging after 31936 examples to avoid logging too frequently
skipping logging after 32000 examples to avoid logging too frequently
train stats after 32064 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.38', 'grad_norm': 'nan', 'counters/examples': 32064, 'counters/updates': 501}
skipping logging after 32128 examples to avoid logging too frequently
skipping logging after 32192 examples to avoid logging too frequently
skipping logging after 32256 examples to avoid logging too frequently
skipping logging after 32320 examples to avoid logging too frequently
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32448 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '596.84', 'grad_norm': 'nan', 'counters/examples': 32448, 'counters/updates': 507}
skipping logging after 32512 examples to avoid logging too frequently
skipping logging after 32576 examples to avoid logging too frequently
skipping logging after 32640 examples to avoid logging too frequently
skipping logging after 32704 examples to avoid logging too frequently
skipping logging after 32768 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.04', 'grad_norm': 'nan', 'counters/examples': 32832, 'counters/updates': 513}
skipping logging after 32896 examples to avoid logging too frequently
skipping logging after 32960 examples to avoid logging too frequently
skipping logging after 33024 examples to avoid logging too frequently
skipping logging after 33088 examples to avoid logging too frequently
skipping logging after 33152 examples to avoid logging too frequently
train stats after 33216 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.45', 'grad_norm': 'nan', 'counters/examples': 33216, 'counters/updates': 519}
skipping logging after 33280 examples to avoid logging too frequently
skipping logging after 33344 examples to avoid logging too frequently
skipping logging after 33408 examples to avoid logging too frequently
skipping logging after 33472 examples to avoid logging too frequently
skipping logging after 33536 examples to avoid logging too frequently
train stats after 33600 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.69', 'grad_norm': 'nan', 'counters/examples': 33600, 'counters/updates': 525}
skipping logging after 33664 examples to avoid logging too frequently
skipping logging after 33728 examples to avoid logging too frequently
skipping logging after 33792 examples to avoid logging too frequently
skipping logging after 33856 examples to avoid logging too frequently
skipping logging after 33920 examples to avoid logging too frequently
train stats after 33984 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '594.49', 'grad_norm': 'nan', 'counters/examples': 33984, 'counters/updates': 531}
skipping logging after 34048 examples to avoid logging too frequently
skipping logging after 34112 examples to avoid logging too frequently
skipping logging after 34176 examples to avoid logging too frequently
skipping logging after 34240 examples to avoid logging too frequently
skipping logging after 34304 examples to avoid logging too frequently
train stats after 34368 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.08', 'grad_norm': 'nan', 'counters/examples': 34368, 'counters/updates': 537}
skipping logging after 34432 examples to avoid logging too frequently
skipping logging after 34496 examples to avoid logging too frequently
skipping logging after 34560 examples to avoid logging too frequently
skipping logging after 34624 examples to avoid logging too frequently
skipping logging after 34688 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '619.05', 'grad_norm': 'nan', 'counters/examples': 34752, 'counters/updates': 543}
skipping logging after 34816 examples to avoid logging too frequently
skipping logging after 34880 examples to avoid logging too frequently
skipping logging after 34944 examples to avoid logging too frequently
skipping logging after 35008 examples to avoid logging too frequently
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35136 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.16', 'grad_norm': 'nan', 'counters/examples': 35136, 'counters/updates': 549}
skipping logging after 35200 examples to avoid logging too frequently
skipping logging after 35264 examples to avoid logging too frequently
skipping logging after 35328 examples to avoid logging too frequently
skipping logging after 35392 examples to avoid logging too frequently
skipping logging after 35456 examples to avoid logging too frequently
train stats after 35520 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '573.13', 'grad_norm': 'nan', 'counters/examples': 35520, 'counters/updates': 555}
skipping logging after 35584 examples to avoid logging too frequently
skipping logging after 35648 examples to avoid logging too frequently
skipping logging after 35712 examples to avoid logging too frequently
skipping logging after 35776 examples to avoid logging too frequently
skipping logging after 35840 examples to avoid logging too frequently
train stats after 35904 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '637.71', 'grad_norm': 'nan', 'counters/examples': 35904, 'counters/updates': 561}
Running evaluation after 35904 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 36.16it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.30it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 36.32it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.38it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.31it/s]
eval after 35904: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-35904...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-35904/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-35904/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-35904/scheduler.pt...
train stats after 35968 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '503.1', 'grad_norm': 'nan', 'counters/examples': 35968, 'counters/updates': 562}
skipping logging after 36032 examples to avoid logging too frequently
skipping logging after 36096 examples to avoid logging too frequently
skipping logging after 36160 examples to avoid logging too frequently
skipping logging after 36224 examples to avoid logging too frequently
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36352 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '612.55', 'grad_norm': 'nan', 'counters/examples': 36352, 'counters/updates': 568}
skipping logging after 36416 examples to avoid logging too frequently
skipping logging after 36480 examples to avoid logging too frequently
skipping logging after 36544 examples to avoid logging too frequently
skipping logging after 36608 examples to avoid logging too frequently
skipping logging after 36672 examples to avoid logging too frequently
train stats after 36736 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.32', 'grad_norm': 'nan', 'counters/examples': 36736, 'counters/updates': 574}
skipping logging after 36800 examples to avoid logging too frequently
skipping logging after 36864 examples to avoid logging too frequently
skipping logging after 36928 examples to avoid logging too frequently
skipping logging after 36992 examples to avoid logging too frequently
skipping logging after 37056 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '655.29', 'grad_norm': 'nan', 'counters/examples': 37120, 'counters/updates': 580}
skipping logging after 37184 examples to avoid logging too frequently
skipping logging after 37248 examples to avoid logging too frequently
skipping logging after 37312 examples to avoid logging too frequently
skipping logging after 37376 examples to avoid logging too frequently
skipping logging after 37440 examples to avoid logging too frequently
train stats after 37504 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '587', 'grad_norm': 'nan', 'counters/examples': 37504, 'counters/updates': 586}
skipping logging after 37568 examples to avoid logging too frequently
skipping logging after 37632 examples to avoid logging too frequently
skipping logging after 37696 examples to avoid logging too frequently
skipping logging after 37760 examples to avoid logging too frequently
skipping logging after 37824 examples to avoid logging too frequently
train stats after 37888 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.22', 'grad_norm': 'nan', 'counters/examples': 37888, 'counters/updates': 592}
skipping logging after 37952 examples to avoid logging too frequently
skipping logging after 38016 examples to avoid logging too frequently
skipping logging after 38080 examples to avoid logging too frequently
skipping logging after 38144 examples to avoid logging too frequently
skipping logging after 38208 examples to avoid logging too frequently
train stats after 38272 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '571.77', 'grad_norm': 'nan', 'counters/examples': 38272, 'counters/updates': 598}
skipping logging after 38336 examples to avoid logging too frequently
skipping logging after 38400 examples to avoid logging too frequently
skipping logging after 38464 examples to avoid logging too frequently
skipping logging after 38528 examples to avoid logging too frequently
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38656 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.95', 'grad_norm': 'nan', 'counters/examples': 38656, 'counters/updates': 604}
skipping logging after 38720 examples to avoid logging too frequently
skipping logging after 38784 examples to avoid logging too frequently
skipping logging after 38848 examples to avoid logging too frequently
skipping logging after 38912 examples to avoid logging too frequently
skipping logging after 38976 examples to avoid logging too frequently
train stats after 39040 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.66', 'grad_norm': 'nan', 'counters/examples': 39040, 'counters/updates': 610}
skipping logging after 39104 examples to avoid logging too frequently
skipping logging after 39168 examples to avoid logging too frequently
skipping logging after 39232 examples to avoid logging too frequently
skipping logging after 39296 examples to avoid logging too frequently
skipping logging after 39360 examples to avoid logging too frequently
skipping logging after 39424 examples to avoid logging too frequently
train stats after 39488 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '572.55', 'grad_norm': 'nan', 'counters/examples': 39488, 'counters/updates': 617}
skipping logging after 39552 examples to avoid logging too frequently
skipping logging after 39616 examples to avoid logging too frequently
skipping logging after 39680 examples to avoid logging too frequently
skipping logging after 39744 examples to avoid logging too frequently
skipping logging after 39808 examples to avoid logging too frequently
train stats after 39872 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.95', 'grad_norm': 'nan', 'counters/examples': 39872, 'counters/updates': 623}
skipping logging after 39936 examples to avoid logging too frequently
skipping logging after 40000 examples to avoid logging too frequently
skipping logging after 40064 examples to avoid logging too frequently
skipping logging after 40128 examples to avoid logging too frequently
skipping logging after 40192 examples to avoid logging too frequently
train stats after 40256 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.5', 'grad_norm': 'nan', 'counters/examples': 40256, 'counters/updates': 629}
skipping logging after 40320 examples to avoid logging too frequently
skipping logging after 40384 examples to avoid logging too frequently
skipping logging after 40448 examples to avoid logging too frequently
skipping logging after 40512 examples to avoid logging too frequently
skipping logging after 40576 examples to avoid logging too frequently
train stats after 40640 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '580.7', 'grad_norm': 'nan', 'counters/examples': 40640, 'counters/updates': 635}
skipping logging after 40704 examples to avoid logging too frequently
skipping logging after 40768 examples to avoid logging too frequently
skipping logging after 40832 examples to avoid logging too frequently
skipping logging after 40896 examples to avoid logging too frequently
skipping logging after 40960 examples to avoid logging too frequently
train stats after 41024 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.07', 'grad_norm': 'nan', 'counters/examples': 41024, 'counters/updates': 641}
skipping logging after 41088 examples to avoid logging too frequently
skipping logging after 41152 examples to avoid logging too frequently
skipping logging after 41216 examples to avoid logging too frequently
skipping logging after 41280 examples to avoid logging too frequently
skipping logging after 41344 examples to avoid logging too frequently
train stats after 41408 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '593.21', 'grad_norm': 'nan', 'counters/examples': 41408, 'counters/updates': 647}
skipping logging after 41472 examples to avoid logging too frequently
skipping logging after 41536 examples to avoid logging too frequently
skipping logging after 41600 examples to avoid logging too frequently
skipping logging after 41664 examples to avoid logging too frequently
skipping logging after 41728 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.35', 'grad_norm': 'nan', 'counters/examples': 41792, 'counters/updates': 653}
skipping logging after 41856 examples to avoid logging too frequently
skipping logging after 41920 examples to avoid logging too frequently
skipping logging after 41984 examples to avoid logging too frequently
skipping logging after 42048 examples to avoid logging too frequently
skipping logging after 42112 examples to avoid logging too frequently
train stats after 42176 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603', 'grad_norm': 'nan', 'counters/examples': 42176, 'counters/updates': 659}
skipping logging after 42240 examples to avoid logging too frequently
skipping logging after 42304 examples to avoid logging too frequently
skipping logging after 42368 examples to avoid logging too frequently
skipping logging after 42432 examples to avoid logging too frequently
skipping logging after 42496 examples to avoid logging too frequently
train stats after 42560 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.42', 'grad_norm': 'nan', 'counters/examples': 42560, 'counters/updates': 665}
skipping logging after 42624 examples to avoid logging too frequently
skipping logging after 42688 examples to avoid logging too frequently
skipping logging after 42752 examples to avoid logging too frequently
skipping logging after 42816 examples to avoid logging too frequently
skipping logging after 42880 examples to avoid logging too frequently
train stats after 42944 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.55', 'grad_norm': 'nan', 'counters/examples': 42944, 'counters/updates': 671}
skipping logging after 43008 examples to avoid logging too frequently
skipping logging after 43072 examples to avoid logging too frequently
skipping logging after 43136 examples to avoid logging too frequently
skipping logging after 43200 examples to avoid logging too frequently
skipping logging after 43264 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.63', 'grad_norm': 'nan', 'counters/examples': 43328, 'counters/updates': 677}
skipping logging after 43392 examples to avoid logging too frequently
skipping logging after 43456 examples to avoid logging too frequently
skipping logging after 43520 examples to avoid logging too frequently
skipping logging after 43584 examples to avoid logging too frequently
skipping logging after 43648 examples to avoid logging too frequently
train stats after 43712 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.11', 'grad_norm': 'nan', 'counters/examples': 43712, 'counters/updates': 683}
skipping logging after 43776 examples to avoid logging too frequently
skipping logging after 43840 examples to avoid logging too frequently
skipping logging after 43904 examples to avoid logging too frequently
skipping logging after 43968 examples to avoid logging too frequently
skipping logging after 44032 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.47', 'grad_norm': 'nan', 'counters/examples': 44096, 'counters/updates': 689}
skipping logging after 44160 examples to avoid logging too frequently
skipping logging after 44224 examples to avoid logging too frequently
skipping logging after 44288 examples to avoid logging too frequently
skipping logging after 44352 examples to avoid logging too frequently
skipping logging after 44416 examples to avoid logging too frequently
train stats after 44480 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '613.55', 'grad_norm': 'nan', 'counters/examples': 44480, 'counters/updates': 695}
skipping logging after 44544 examples to avoid logging too frequently
skipping logging after 44608 examples to avoid logging too frequently
skipping logging after 44672 examples to avoid logging too frequently
skipping logging after 44736 examples to avoid logging too frequently
skipping logging after 44800 examples to avoid logging too frequently
train stats after 44864 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '585.96', 'grad_norm': 'nan', 'counters/examples': 44864, 'counters/updates': 701}
skipping logging after 44928 examples to avoid logging too frequently
skipping logging after 44992 examples to avoid logging too frequently
skipping logging after 45056 examples to avoid logging too frequently
skipping logging after 45120 examples to avoid logging too frequently
skipping logging after 45184 examples to avoid logging too frequently
train stats after 45248 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '549.9', 'grad_norm': 'nan', 'counters/examples': 45248, 'counters/updates': 707}
skipping logging after 45312 examples to avoid logging too frequently
skipping logging after 45376 examples to avoid logging too frequently
skipping logging after 45440 examples to avoid logging too frequently
skipping logging after 45504 examples to avoid logging too frequently
skipping logging after 45568 examples to avoid logging too frequently
train stats after 45632 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.06', 'grad_norm': 'nan', 'counters/examples': 45632, 'counters/updates': 713}
skipping logging after 45696 examples to avoid logging too frequently
skipping logging after 45760 examples to avoid logging too frequently
skipping logging after 45824 examples to avoid logging too frequently
skipping logging after 45888 examples to avoid logging too frequently
skipping logging after 45952 examples to avoid logging too frequently
train stats after 46016 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '534.49', 'grad_norm': 'nan', 'counters/examples': 46016, 'counters/updates': 719}
skipping logging after 46080 examples to avoid logging too frequently
skipping logging after 46144 examples to avoid logging too frequently
skipping logging after 46208 examples to avoid logging too frequently
skipping logging after 46272 examples to avoid logging too frequently
skipping logging after 46336 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '630.37', 'grad_norm': 'nan', 'counters/examples': 46400, 'counters/updates': 725}
skipping logging after 46464 examples to avoid logging too frequently
skipping logging after 46528 examples to avoid logging too frequently
skipping logging after 46592 examples to avoid logging too frequently
skipping logging after 46656 examples to avoid logging too frequently
skipping logging after 46720 examples to avoid logging too frequently
train stats after 46784 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.75', 'grad_norm': 'nan', 'counters/examples': 46784, 'counters/updates': 731}
skipping logging after 46848 examples to avoid logging too frequently
skipping logging after 46912 examples to avoid logging too frequently
skipping logging after 46976 examples to avoid logging too frequently
skipping logging after 47040 examples to avoid logging too frequently
skipping logging after 47104 examples to avoid logging too frequently
train stats after 47168 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.95', 'grad_norm': 'nan', 'counters/examples': 47168, 'counters/updates': 737}
skipping logging after 47232 examples to avoid logging too frequently
skipping logging after 47296 examples to avoid logging too frequently
skipping logging after 47360 examples to avoid logging too frequently
skipping logging after 47424 examples to avoid logging too frequently
skipping logging after 47488 examples to avoid logging too frequently
train stats after 47552 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '546.68', 'grad_norm': 'nan', 'counters/examples': 47552, 'counters/updates': 743}
skipping logging after 47616 examples to avoid logging too frequently
skipping logging after 47680 examples to avoid logging too frequently
skipping logging after 47744 examples to avoid logging too frequently
skipping logging after 47808 examples to avoid logging too frequently
skipping logging after 47872 examples to avoid logging too frequently
Running evaluation after 47872 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 35.26it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.14it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 36.19it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.20it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.08it/s]
eval after 47872: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-47872...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-47872/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-47872/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-47872/scheduler.pt...
train stats after 47936 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '508.52', 'grad_norm': 'nan', 'counters/examples': 47936, 'counters/updates': 749}
skipping logging after 48000 examples to avoid logging too frequently
skipping logging after 48064 examples to avoid logging too frequently
skipping logging after 48128 examples to avoid logging too frequently
skipping logging after 48192 examples to avoid logging too frequently
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48320 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.28', 'grad_norm': 'nan', 'counters/examples': 48320, 'counters/updates': 755}
skipping logging after 48384 examples to avoid logging too frequently
skipping logging after 48448 examples to avoid logging too frequently
skipping logging after 48512 examples to avoid logging too frequently
skipping logging after 48576 examples to avoid logging too frequently
skipping logging after 48640 examples to avoid logging too frequently
train stats after 48704 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.9', 'grad_norm': 'nan', 'counters/examples': 48704, 'counters/updates': 761}
skipping logging after 48768 examples to avoid logging too frequently
skipping logging after 48832 examples to avoid logging too frequently
skipping logging after 48896 examples to avoid logging too frequently
skipping logging after 48960 examples to avoid logging too frequently
skipping logging after 49024 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '610.92', 'grad_norm': 'nan', 'counters/examples': 49088, 'counters/updates': 767}
skipping logging after 49152 examples to avoid logging too frequently
skipping logging after 49216 examples to avoid logging too frequently
skipping logging after 49280 examples to avoid logging too frequently
skipping logging after 49344 examples to avoid logging too frequently
skipping logging after 49408 examples to avoid logging too frequently
train stats after 49472 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '633.7', 'grad_norm': 'nan', 'counters/examples': 49472, 'counters/updates': 773}
skipping logging after 49536 examples to avoid logging too frequently
skipping logging after 49600 examples to avoid logging too frequently
skipping logging after 49664 examples to avoid logging too frequently
skipping logging after 49728 examples to avoid logging too frequently
skipping logging after 49792 examples to avoid logging too frequently
train stats after 49856 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '596.98', 'grad_norm': 'nan', 'counters/examples': 49856, 'counters/updates': 779}
skipping logging after 49920 examples to avoid logging too frequently
skipping logging after 49984 examples to avoid logging too frequently
skipping logging after 50048 examples to avoid logging too frequently
skipping logging after 50112 examples to avoid logging too frequently
skipping logging after 50176 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '613.6', 'grad_norm': 'nan', 'counters/examples': 50240, 'counters/updates': 785}
skipping logging after 50304 examples to avoid logging too frequently
skipping logging after 50368 examples to avoid logging too frequently
skipping logging after 50432 examples to avoid logging too frequently
skipping logging after 50496 examples to avoid logging too frequently
skipping logging after 50560 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '635.5', 'grad_norm': 'nan', 'counters/examples': 50624, 'counters/updates': 791}
skipping logging after 50688 examples to avoid logging too frequently
skipping logging after 50752 examples to avoid logging too frequently
skipping logging after 50816 examples to avoid logging too frequently
skipping logging after 50880 examples to avoid logging too frequently
skipping logging after 50944 examples to avoid logging too frequently
train stats after 51008 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '614.01', 'grad_norm': 'nan', 'counters/examples': 51008, 'counters/updates': 797}
skipping logging after 51072 examples to avoid logging too frequently
skipping logging after 51136 examples to avoid logging too frequently
skipping logging after 51200 examples to avoid logging too frequently
skipping logging after 51264 examples to avoid logging too frequently
skipping logging after 51328 examples to avoid logging too frequently
train stats after 51392 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '523.58', 'grad_norm': 'nan', 'counters/examples': 51392, 'counters/updates': 803}
skipping logging after 51456 examples to avoid logging too frequently
skipping logging after 51520 examples to avoid logging too frequently
skipping logging after 51584 examples to avoid logging too frequently
skipping logging after 51648 examples to avoid logging too frequently
skipping logging after 51712 examples to avoid logging too frequently
train stats after 51776 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.17', 'grad_norm': 'nan', 'counters/examples': 51776, 'counters/updates': 809}
skipping logging after 51840 examples to avoid logging too frequently
skipping logging after 51904 examples to avoid logging too frequently
skipping logging after 51968 examples to avoid logging too frequently
skipping logging after 52032 examples to avoid logging too frequently
skipping logging after 52096 examples to avoid logging too frequently
train stats after 52160 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.78', 'grad_norm': 'nan', 'counters/examples': 52160, 'counters/updates': 815}
skipping logging after 52224 examples to avoid logging too frequently
skipping logging after 52288 examples to avoid logging too frequently
skipping logging after 52352 examples to avoid logging too frequently
skipping logging after 52416 examples to avoid logging too frequently
skipping logging after 52480 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '588.92', 'grad_norm': 'nan', 'counters/examples': 52544, 'counters/updates': 821}
skipping logging after 52608 examples to avoid logging too frequently
skipping logging after 52672 examples to avoid logging too frequently
skipping logging after 52736 examples to avoid logging too frequently
skipping logging after 52800 examples to avoid logging too frequently
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52928 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '581.53', 'grad_norm': 'nan', 'counters/examples': 52928, 'counters/updates': 827}
skipping logging after 52992 examples to avoid logging too frequently
skipping logging after 53056 examples to avoid logging too frequently
skipping logging after 53120 examples to avoid logging too frequently
skipping logging after 53184 examples to avoid logging too frequently
skipping logging after 53248 examples to avoid logging too frequently
train stats after 53312 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.2', 'grad_norm': 'nan', 'counters/examples': 53312, 'counters/updates': 833}
skipping logging after 53376 examples to avoid logging too frequently
skipping logging after 53440 examples to avoid logging too frequently
skipping logging after 53504 examples to avoid logging too frequently
skipping logging after 53568 examples to avoid logging too frequently
skipping logging after 53632 examples to avoid logging too frequently
train stats after 53696 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.21', 'grad_norm': 'nan', 'counters/examples': 53696, 'counters/updates': 839}
skipping logging after 53760 examples to avoid logging too frequently
skipping logging after 53824 examples to avoid logging too frequently
skipping logging after 53888 examples to avoid logging too frequently
skipping logging after 53952 examples to avoid logging too frequently
skipping logging after 54016 examples to avoid logging too frequently
train stats after 54080 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '577.11', 'grad_norm': 'nan', 'counters/examples': 54080, 'counters/updates': 845}
skipping logging after 54144 examples to avoid logging too frequently
skipping logging after 54208 examples to avoid logging too frequently
skipping logging after 54272 examples to avoid logging too frequently
skipping logging after 54336 examples to avoid logging too frequently
skipping logging after 54400 examples to avoid logging too frequently
train stats after 54464 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.78', 'grad_norm': 'nan', 'counters/examples': 54464, 'counters/updates': 851}
skipping logging after 54528 examples to avoid logging too frequently
skipping logging after 54592 examples to avoid logging too frequently
skipping logging after 54656 examples to avoid logging too frequently
skipping logging after 54720 examples to avoid logging too frequently
skipping logging after 54784 examples to avoid logging too frequently
train stats after 54848 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '590.47', 'grad_norm': 'nan', 'counters/examples': 54848, 'counters/updates': 857}
skipping logging after 54912 examples to avoid logging too frequently
skipping logging after 54976 examples to avoid logging too frequently
skipping logging after 55040 examples to avoid logging too frequently
skipping logging after 55104 examples to avoid logging too frequently
skipping logging after 55168 examples to avoid logging too frequently
train stats after 55232 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.28', 'grad_norm': 'nan', 'counters/examples': 55232, 'counters/updates': 863}
skipping logging after 55296 examples to avoid logging too frequently
skipping logging after 55360 examples to avoid logging too frequently
skipping logging after 55424 examples to avoid logging too frequently
skipping logging after 55488 examples to avoid logging too frequently
skipping logging after 55552 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.44', 'grad_norm': 'nan', 'counters/examples': 55616, 'counters/updates': 869}
skipping logging after 55680 examples to avoid logging too frequently
skipping logging after 55744 examples to avoid logging too frequently
skipping logging after 55808 examples to avoid logging too frequently
skipping logging after 55872 examples to avoid logging too frequently
skipping logging after 55936 examples to avoid logging too frequently
train stats after 56000 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.22', 'grad_norm': 'nan', 'counters/examples': 56000, 'counters/updates': 875}
skipping logging after 56064 examples to avoid logging too frequently
skipping logging after 56128 examples to avoid logging too frequently
skipping logging after 56192 examples to avoid logging too frequently
skipping logging after 56256 examples to avoid logging too frequently
skipping logging after 56320 examples to avoid logging too frequently
train stats after 56384 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.38', 'grad_norm': 'nan', 'counters/examples': 56384, 'counters/updates': 881}
skipping logging after 56448 examples to avoid logging too frequently
skipping logging after 56512 examples to avoid logging too frequently
skipping logging after 56576 examples to avoid logging too frequently
skipping logging after 56640 examples to avoid logging too frequently
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56768 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.44', 'grad_norm': 'nan', 'counters/examples': 56768, 'counters/updates': 887}
skipping logging after 56832 examples to avoid logging too frequently
skipping logging after 56896 examples to avoid logging too frequently
skipping logging after 56960 examples to avoid logging too frequently
skipping logging after 57024 examples to avoid logging too frequently
skipping logging after 57088 examples to avoid logging too frequently
train stats after 57152 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.46', 'grad_norm': 'nan', 'counters/examples': 57152, 'counters/updates': 893}
skipping logging after 57216 examples to avoid logging too frequently
skipping logging after 57280 examples to avoid logging too frequently
skipping logging after 57344 examples to avoid logging too frequently
skipping logging after 57408 examples to avoid logging too frequently
skipping logging after 57472 examples to avoid logging too frequently
train stats after 57536 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.57', 'grad_norm': 'nan', 'counters/examples': 57536, 'counters/updates': 899}
skipping logging after 57600 examples to avoid logging too frequently
skipping logging after 57664 examples to avoid logging too frequently
skipping logging after 57728 examples to avoid logging too frequently
skipping logging after 57792 examples to avoid logging too frequently
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57920 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '605.96', 'grad_norm': 'nan', 'counters/examples': 57920, 'counters/updates': 905}
skipping logging after 57984 examples to avoid logging too frequently
skipping logging after 58048 examples to avoid logging too frequently
skipping logging after 58112 examples to avoid logging too frequently
skipping logging after 58176 examples to avoid logging too frequently
skipping logging after 58240 examples to avoid logging too frequently
train stats after 58304 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.97', 'grad_norm': 'nan', 'counters/examples': 58304, 'counters/updates': 911}
skipping logging after 58368 examples to avoid logging too frequently
skipping logging after 58432 examples to avoid logging too frequently
skipping logging after 58496 examples to avoid logging too frequently
skipping logging after 58560 examples to avoid logging too frequently
skipping logging after 58624 examples to avoid logging too frequently
train stats after 58688 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '580.13', 'grad_norm': 'nan', 'counters/examples': 58688, 'counters/updates': 917}
skipping logging after 58752 examples to avoid logging too frequently
skipping logging after 58816 examples to avoid logging too frequently
skipping logging after 58880 examples to avoid logging too frequently
skipping logging after 58944 examples to avoid logging too frequently
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59072 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '588.12', 'grad_norm': 'nan', 'counters/examples': 59072, 'counters/updates': 923}
skipping logging after 59136 examples to avoid logging too frequently
skipping logging after 59200 examples to avoid logging too frequently
skipping logging after 59264 examples to avoid logging too frequently
skipping logging after 59328 examples to avoid logging too frequently
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59456 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.06', 'grad_norm': 'nan', 'counters/examples': 59456, 'counters/updates': 929}
skipping logging after 59520 examples to avoid logging too frequently
skipping logging after 59584 examples to avoid logging too frequently
skipping logging after 59648 examples to avoid logging too frequently
skipping logging after 59712 examples to avoid logging too frequently
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59840 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '621.79', 'grad_norm': 'nan', 'counters/examples': 59840, 'counters/updates': 935}
Running evaluation after 59840 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 35.91it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.25it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 36.28it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.31it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.23it/s]
eval after 59840: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-59840...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-59840/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-59840/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-59840/scheduler.pt...
train stats after 59904 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '533.78', 'grad_norm': 'nan', 'counters/examples': 59904, 'counters/updates': 936}
skipping logging after 59968 examples to avoid logging too frequently
skipping logging after 60032 examples to avoid logging too frequently
skipping logging after 60096 examples to avoid logging too frequently
skipping logging after 60160 examples to avoid logging too frequently
skipping logging after 60224 examples to avoid logging too frequently
train stats after 60288 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '563.69', 'grad_norm': 'nan', 'counters/examples': 60288, 'counters/updates': 942}
skipping logging after 60352 examples to avoid logging too frequently
skipping logging after 60416 examples to avoid logging too frequently
skipping logging after 60480 examples to avoid logging too frequently
skipping logging after 60544 examples to avoid logging too frequently
skipping logging after 60608 examples to avoid logging too frequently
train stats after 60672 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.49', 'grad_norm': 'nan', 'counters/examples': 60672, 'counters/updates': 948}
skipping logging after 60736 examples to avoid logging too frequently
skipping logging after 60800 examples to avoid logging too frequently
skipping logging after 60864 examples to avoid logging too frequently
skipping logging after 60928 examples to avoid logging too frequently
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61056 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '678.31', 'grad_norm': 'nan', 'counters/examples': 61056, 'counters/updates': 954}
skipping logging after 61120 examples to avoid logging too frequently
skipping logging after 61184 examples to avoid logging too frequently
skipping logging after 61248 examples to avoid logging too frequently
skipping logging after 61312 examples to avoid logging too frequently
skipping logging after 61376 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.26', 'grad_norm': 'nan', 'counters/examples': 61440, 'counters/updates': 960}
skipping logging after 61504 examples to avoid logging too frequently
skipping logging after 61568 examples to avoid logging too frequently
skipping logging after 61632 examples to avoid logging too frequently
skipping logging after 61696 examples to avoid logging too frequently
skipping logging after 61760 examples to avoid logging too frequently
train stats after 61824 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.11', 'grad_norm': 'nan', 'counters/examples': 61824, 'counters/updates': 966}
skipping logging after 61888 examples to avoid logging too frequently
skipping logging after 61952 examples to avoid logging too frequently
skipping logging after 62016 examples to avoid logging too frequently
skipping logging after 62080 examples to avoid logging too frequently
skipping logging after 62144 examples to avoid logging too frequently
train stats after 62208 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.16', 'grad_norm': 'nan', 'counters/examples': 62208, 'counters/updates': 972}
skipping logging after 62272 examples to avoid logging too frequently
skipping logging after 62336 examples to avoid logging too frequently
skipping logging after 62400 examples to avoid logging too frequently
skipping logging after 62464 examples to avoid logging too frequently
skipping logging after 62528 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.28', 'grad_norm': 'nan', 'counters/examples': 62592, 'counters/updates': 978}
skipping logging after 62656 examples to avoid logging too frequently
skipping logging after 62720 examples to avoid logging too frequently
skipping logging after 62784 examples to avoid logging too frequently
skipping logging after 62848 examples to avoid logging too frequently
skipping logging after 62912 examples to avoid logging too frequently
train stats after 62976 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '599.78', 'grad_norm': 'nan', 'counters/examples': 62976, 'counters/updates': 984}
skipping logging after 63040 examples to avoid logging too frequently
skipping logging after 63104 examples to avoid logging too frequently
skipping logging after 63168 examples to avoid logging too frequently
skipping logging after 63232 examples to avoid logging too frequently
skipping logging after 63296 examples to avoid logging too frequently
train stats after 63360 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '607.72', 'grad_norm': 'nan', 'counters/examples': 63360, 'counters/updates': 990}
skipping logging after 63424 examples to avoid logging too frequently
skipping logging after 63488 examples to avoid logging too frequently
skipping logging after 63552 examples to avoid logging too frequently
skipping logging after 63616 examples to avoid logging too frequently
skipping logging after 63680 examples to avoid logging too frequently
train stats after 63744 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '630.75', 'grad_norm': 'nan', 'counters/examples': 63744, 'counters/updates': 996}
skipping logging after 63808 examples to avoid logging too frequently
skipping logging after 63872 examples to avoid logging too frequently
skipping logging after 63936 examples to avoid logging too frequently
skipping logging after 64000 examples to avoid logging too frequently
skipping logging after 64064 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.31', 'grad_norm': 'nan', 'counters/examples': 64128, 'counters/updates': 1002}
skipping logging after 64192 examples to avoid logging too frequently
skipping logging after 64256 examples to avoid logging too frequently
skipping logging after 64320 examples to avoid logging too frequently
skipping logging after 64384 examples to avoid logging too frequently
skipping logging after 64448 examples to avoid logging too frequently
train stats after 64512 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.74', 'grad_norm': 'nan', 'counters/examples': 64512, 'counters/updates': 1008}
skipping logging after 64576 examples to avoid logging too frequently
skipping logging after 64640 examples to avoid logging too frequently
skipping logging after 64704 examples to avoid logging too frequently
skipping logging after 64768 examples to avoid logging too frequently
skipping logging after 64832 examples to avoid logging too frequently
train stats after 64896 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.16', 'grad_norm': 'nan', 'counters/examples': 64896, 'counters/updates': 1014}
skipping logging after 64960 examples to avoid logging too frequently
skipping logging after 65024 examples to avoid logging too frequently
skipping logging after 65088 examples to avoid logging too frequently
skipping logging after 65152 examples to avoid logging too frequently
skipping logging after 65216 examples to avoid logging too frequently
train stats after 65280 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '601.95', 'grad_norm': 'nan', 'counters/examples': 65280, 'counters/updates': 1020}
skipping logging after 65344 examples to avoid logging too frequently
skipping logging after 65408 examples to avoid logging too frequently
skipping logging after 65472 examples to avoid logging too frequently
skipping logging after 65536 examples to avoid logging too frequently
skipping logging after 65600 examples to avoid logging too frequently
train stats after 65664 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '610.65', 'grad_norm': 'nan', 'counters/examples': 65664, 'counters/updates': 1026}
skipping logging after 65728 examples to avoid logging too frequently
skipping logging after 65792 examples to avoid logging too frequently
skipping logging after 65856 examples to avoid logging too frequently
skipping logging after 65920 examples to avoid logging too frequently
skipping logging after 65984 examples to avoid logging too frequently
train stats after 66048 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '587.28', 'grad_norm': 'nan', 'counters/examples': 66048, 'counters/updates': 1032}
skipping logging after 66112 examples to avoid logging too frequently
skipping logging after 66176 examples to avoid logging too frequently
skipping logging after 66240 examples to avoid logging too frequently
skipping logging after 66304 examples to avoid logging too frequently
skipping logging after 66368 examples to avoid logging too frequently
train stats after 66432 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '579.06', 'grad_norm': 'nan', 'counters/examples': 66432, 'counters/updates': 1038}
skipping logging after 66496 examples to avoid logging too frequently
skipping logging after 66560 examples to avoid logging too frequently
skipping logging after 66624 examples to avoid logging too frequently
skipping logging after 66688 examples to avoid logging too frequently
skipping logging after 66752 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.93', 'grad_norm': 'nan', 'counters/examples': 66816, 'counters/updates': 1044}
skipping logging after 66880 examples to avoid logging too frequently
skipping logging after 66944 examples to avoid logging too frequently
skipping logging after 67008 examples to avoid logging too frequently
skipping logging after 67072 examples to avoid logging too frequently
skipping logging after 67136 examples to avoid logging too frequently
train stats after 67200 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.06', 'grad_norm': 'nan', 'counters/examples': 67200, 'counters/updates': 1050}
skipping logging after 67264 examples to avoid logging too frequently
skipping logging after 67328 examples to avoid logging too frequently
skipping logging after 67392 examples to avoid logging too frequently
skipping logging after 67456 examples to avoid logging too frequently
skipping logging after 67520 examples to avoid logging too frequently
train stats after 67584 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '591.14', 'grad_norm': 'nan', 'counters/examples': 67584, 'counters/updates': 1056}
skipping logging after 67648 examples to avoid logging too frequently
skipping logging after 67712 examples to avoid logging too frequently
skipping logging after 67776 examples to avoid logging too frequently
skipping logging after 67840 examples to avoid logging too frequently
skipping logging after 67904 examples to avoid logging too frequently
train stats after 67968 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.86', 'grad_norm': 'nan', 'counters/examples': 67968, 'counters/updates': 1062}
skipping logging after 68032 examples to avoid logging too frequently
skipping logging after 68096 examples to avoid logging too frequently
skipping logging after 68160 examples to avoid logging too frequently
skipping logging after 68224 examples to avoid logging too frequently
skipping logging after 68288 examples to avoid logging too frequently
train stats after 68352 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.48', 'grad_norm': 'nan', 'counters/examples': 68352, 'counters/updates': 1068}
skipping logging after 68416 examples to avoid logging too frequently
skipping logging after 68480 examples to avoid logging too frequently
skipping logging after 68544 examples to avoid logging too frequently
skipping logging after 68608 examples to avoid logging too frequently
skipping logging after 68672 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.42', 'grad_norm': 'nan', 'counters/examples': 68736, 'counters/updates': 1074}
skipping logging after 68800 examples to avoid logging too frequently
skipping logging after 68864 examples to avoid logging too frequently
skipping logging after 68928 examples to avoid logging too frequently
skipping logging after 68992 examples to avoid logging too frequently
skipping logging after 69056 examples to avoid logging too frequently
train stats after 69120 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '578.98', 'grad_norm': 'nan', 'counters/examples': 69120, 'counters/updates': 1080}
skipping logging after 69184 examples to avoid logging too frequently
skipping logging after 69248 examples to avoid logging too frequently
skipping logging after 69312 examples to avoid logging too frequently
skipping logging after 69376 examples to avoid logging too frequently
skipping logging after 69440 examples to avoid logging too frequently
train stats after 69504 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '562.76', 'grad_norm': 'nan', 'counters/examples': 69504, 'counters/updates': 1086}
skipping logging after 69568 examples to avoid logging too frequently
skipping logging after 69632 examples to avoid logging too frequently
skipping logging after 69696 examples to avoid logging too frequently
skipping logging after 69760 examples to avoid logging too frequently
skipping logging after 69824 examples to avoid logging too frequently
train stats after 69888 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600.83', 'grad_norm': 'nan', 'counters/examples': 69888, 'counters/updates': 1092}
skipping logging after 69952 examples to avoid logging too frequently
skipping logging after 70016 examples to avoid logging too frequently
skipping logging after 70080 examples to avoid logging too frequently
skipping logging after 70144 examples to avoid logging too frequently
skipping logging after 70208 examples to avoid logging too frequently
train stats after 70272 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '585.89', 'grad_norm': 'nan', 'counters/examples': 70272, 'counters/updates': 1098}
skipping logging after 70336 examples to avoid logging too frequently
skipping logging after 70400 examples to avoid logging too frequently
skipping logging after 70464 examples to avoid logging too frequently
skipping logging after 70528 examples to avoid logging too frequently
skipping logging after 70592 examples to avoid logging too frequently
train stats after 70656 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '600', 'grad_norm': 'nan', 'counters/examples': 70656, 'counters/updates': 1104}
skipping logging after 70720 examples to avoid logging too frequently
skipping logging after 70784 examples to avoid logging too frequently
skipping logging after 70848 examples to avoid logging too frequently
skipping logging after 70912 examples to avoid logging too frequently
skipping logging after 70976 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '593.77', 'grad_norm': 'nan', 'counters/examples': 71040, 'counters/updates': 1110}
skipping logging after 71104 examples to avoid logging too frequently
skipping logging after 71168 examples to avoid logging too frequently
skipping logging after 71232 examples to avoid logging too frequently
skipping logging after 71296 examples to avoid logging too frequently
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71424 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '581.8', 'grad_norm': 'nan', 'counters/examples': 71424, 'counters/updates': 1116}
skipping logging after 71488 examples to avoid logging too frequently
skipping logging after 71552 examples to avoid logging too frequently
skipping logging after 71616 examples to avoid logging too frequently
skipping logging after 71680 examples to avoid logging too frequently
skipping logging after 71744 examples to avoid logging too frequently
train stats after 71808 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '588.85', 'grad_norm': 'nan', 'counters/examples': 71808, 'counters/updates': 1122}
Running evaluation after 71808 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 35.90it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.14it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 35.99it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.12it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.05it/s]
eval after 71808: {'rewards_eval/chosen': 'nan', 'rewards_eval/rejected': 'nan', 'rewards_eval/accuracies': '0', 'rewards_eval/margins': 'nan', 'logps_eval/rejected': 'nan', 'logps_eval/chosen': 'nan', 'loss/eval': 'nan'}
creating checkpoint to write to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-71808...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-71808/policy.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-71808/optimizer.pt...
writing checkpoint to .cache/laura/pythia70m_dpo_seed0_2024-01-12_14-21-14_664090/step-71808/scheduler.pt...
train stats after 71872 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '502.4', 'grad_norm': 'nan', 'counters/examples': 71872, 'counters/updates': 1123}
skipping logging after 71936 examples to avoid logging too frequently
skipping logging after 72000 examples to avoid logging too frequently
skipping logging after 72064 examples to avoid logging too frequently
skipping logging after 72128 examples to avoid logging too frequently
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72256 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '603.4', 'grad_norm': 'nan', 'counters/examples': 72256, 'counters/updates': 1129}
skipping logging after 72320 examples to avoid logging too frequently
skipping logging after 72384 examples to avoid logging too frequently
skipping logging after 72448 examples to avoid logging too frequently
skipping logging after 72512 examples to avoid logging too frequently
skipping logging after 72576 examples to avoid logging too frequently
train stats after 72640 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '613.87', 'grad_norm': 'nan', 'counters/examples': 72640, 'counters/updates': 1135}
skipping logging after 72704 examples to avoid logging too frequently
skipping logging after 72768 examples to avoid logging too frequently
skipping logging after 72832 examples to avoid logging too frequently
skipping logging after 72896 examples to avoid logging too frequently
skipping logging after 72960 examples to avoid logging too frequently
train stats after 73024 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '602.7', 'grad_norm': 'nan', 'counters/examples': 73024, 'counters/updates': 1141}
skipping logging after 73088 examples to avoid logging too frequently
skipping logging after 73152 examples to avoid logging too frequently
skipping logging after 73216 examples to avoid logging too frequently
skipping logging after 73280 examples to avoid logging too frequently
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73408 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '596.63', 'grad_norm': 'nan', 'counters/examples': 73408, 'counters/updates': 1147}
skipping logging after 73472 examples to avoid logging too frequently
skipping logging after 73536 examples to avoid logging too frequently
skipping logging after 73600 examples to avoid logging too frequently
skipping logging after 73664 examples to avoid logging too frequently
skipping logging after 73728 examples to avoid logging too frequently
train stats after 73792 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '589.88', 'grad_norm': 'nan', 'counters/examples': 73792, 'counters/updates': 1153}
skipping logging after 73856 examples to avoid logging too frequently
skipping logging after 73920 examples to avoid logging too frequently
skipping logging after 73984 examples to avoid logging too frequently
skipping logging after 74048 examples to avoid logging too frequently
skipping logging after 74112 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '590.12', 'grad_norm': 'nan', 'counters/examples': 74176, 'counters/updates': 1159}
skipping logging after 74240 examples to avoid logging too frequently
skipping logging after 74304 examples to avoid logging too frequently
skipping logging after 74368 examples to avoid logging too frequently
skipping logging after 74432 examples to avoid logging too frequently
skipping logging after 74496 examples to avoid logging too frequently
train stats after 74560 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.21', 'grad_norm': 'nan', 'counters/examples': 74560, 'counters/updates': 1165}
skipping logging after 74624 examples to avoid logging too frequently
skipping logging after 74688 examples to avoid logging too frequently
skipping logging after 74752 examples to avoid logging too frequently
skipping logging after 74816 examples to avoid logging too frequently
skipping logging after 74880 examples to avoid logging too frequently
train stats after 74944 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '592.85', 'grad_norm': 'nan', 'counters/examples': 74944, 'counters/updates': 1171}
skipping logging after 75008 examples to avoid logging too frequently
skipping logging after 75072 examples to avoid logging too frequently
skipping logging after 75136 examples to avoid logging too frequently
skipping logging after 75200 examples to avoid logging too frequently
skipping logging after 75264 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.16', 'grad_norm': 'nan', 'counters/examples': 75328, 'counters/updates': 1177}
skipping logging after 75392 examples to avoid logging too frequently
skipping logging after 75456 examples to avoid logging too frequently
skipping logging after 75520 examples to avoid logging too frequently
skipping logging after 75584 examples to avoid logging too frequently
skipping logging after 75648 examples to avoid logging too frequently
train stats after 75712 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.73', 'grad_norm': 'nan', 'counters/examples': 75712, 'counters/updates': 1183}
skipping logging after 75776 examples to avoid logging too frequently
skipping logging after 75840 examples to avoid logging too frequently
skipping logging after 75904 examples to avoid logging too frequently
skipping logging after 75968 examples to avoid logging too frequently
skipping logging after 76032 examples to avoid logging too frequently
train stats after 76096 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.83', 'grad_norm': 'nan', 'counters/examples': 76096, 'counters/updates': 1189}
skipping logging after 76160 examples to avoid logging too frequently
skipping logging after 76224 examples to avoid logging too frequently
skipping logging after 76288 examples to avoid logging too frequently
skipping logging after 76352 examples to avoid logging too frequently
skipping logging after 76416 examples to avoid logging too frequently
train stats after 76480 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '586.83', 'grad_norm': 'nan', 'counters/examples': 76480, 'counters/updates': 1195}
skipping logging after 76544 examples to avoid logging too frequently
skipping logging after 76608 examples to avoid logging too frequently
skipping logging after 76672 examples to avoid logging too frequently
skipping logging after 76736 examples to avoid logging too frequently
skipping logging after 76800 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '607.35', 'grad_norm': 'nan', 'counters/examples': 76864, 'counters/updates': 1201}
skipping logging after 76928 examples to avoid logging too frequently
skipping logging after 76992 examples to avoid logging too frequently
skipping logging after 77056 examples to avoid logging too frequently
skipping logging after 77120 examples to avoid logging too frequently
skipping logging after 77184 examples to avoid logging too frequently
train stats after 77248 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '578.27', 'grad_norm': 'nan', 'counters/examples': 77248, 'counters/updates': 1207}
skipping logging after 77312 examples to avoid logging too frequently
skipping logging after 77376 examples to avoid logging too frequently
skipping logging after 77440 examples to avoid logging too frequently
skipping logging after 77504 examples to avoid logging too frequently
skipping logging after 77568 examples to avoid logging too frequently
train stats after 77632 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.61', 'grad_norm': 'nan', 'counters/examples': 77632, 'counters/updates': 1213}
skipping logging after 77696 examples to avoid logging too frequently
skipping logging after 77760 examples to avoid logging too frequently
skipping logging after 77824 examples to avoid logging too frequently
skipping logging after 77888 examples to avoid logging too frequently
skipping logging after 77952 examples to avoid logging too frequently
train stats after 78016 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '581.53', 'grad_norm': 'nan', 'counters/examples': 78016, 'counters/updates': 1219}
skipping logging after 78080 examples to avoid logging too frequently
skipping logging after 78144 examples to avoid logging too frequently
skipping logging after 78208 examples to avoid logging too frequently
skipping logging after 78272 examples to avoid logging too frequently
skipping logging after 78336 examples to avoid logging too frequently
train stats after 78400 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '595.89', 'grad_norm': 'nan', 'counters/examples': 78400, 'counters/updates': 1225}
skipping logging after 78464 examples to avoid logging too frequently
skipping logging after 78528 examples to avoid logging too frequently
skipping logging after 78592 examples to avoid logging too frequently
skipping logging after 78656 examples to avoid logging too frequently
skipping logging after 78720 examples to avoid logging too frequently
train stats after 78784 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '536.89', 'grad_norm': 'nan', 'counters/examples': 78784, 'counters/updates': 1231}
skipping logging after 78848 examples to avoid logging too frequently
skipping logging after 78912 examples to avoid logging too frequently
skipping logging after 78976 examples to avoid logging too frequently
skipping logging after 79040 examples to avoid logging too frequently
skipping logging after 79104 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '604.68', 'grad_norm': 'nan', 'counters/examples': 79168, 'counters/updates': 1237}
skipping logging after 79232 examples to avoid logging too frequently
skipping logging after 79296 examples to avoid logging too frequently
skipping logging after 79360 examples to avoid logging too frequently
skipping logging after 79424 examples to avoid logging too frequently
skipping logging after 79488 examples to avoid logging too frequently
train stats after 79552 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '597.59', 'grad_norm': 'nan', 'counters/examples': 79552, 'counters/updates': 1243}
skipping logging after 79616 examples to avoid logging too frequently
skipping logging after 79680 examples to avoid logging too frequently
skipping logging after 79744 examples to avoid logging too frequently
skipping logging after 79808 examples to avoid logging too frequently
skipping logging after 79872 examples to avoid logging too frequently
train stats after 79936 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '590.29', 'grad_norm': 'nan', 'counters/examples': 79936, 'counters/updates': 1249}
skipping logging after 80000 examples to avoid logging too frequently
skipping logging after 80064 examples to avoid logging too frequently
skipping logging after 80128 examples to avoid logging too frequently
skipping logging after 80192 examples to avoid logging too frequently
skipping logging after 80256 examples to avoid logging too frequently
train stats after 80320 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '584.43', 'grad_norm': 'nan', 'counters/examples': 80320, 'counters/updates': 1255}
skipping logging after 80384 examples to avoid logging too frequently
skipping logging after 80448 examples to avoid logging too frequently
skipping logging after 80512 examples to avoid logging too frequently
skipping logging after 80576 examples to avoid logging too frequently
skipping logging after 80640 examples to avoid logging too frequently
train stats after 80704 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '598.99', 'grad_norm': 'nan', 'counters/examples': 80704, 'counters/updates': 1261}
skipping logging after 80768 examples to avoid logging too frequently
skipping logging after 80832 examples to avoid logging too frequently
skipping logging after 80896 examples to avoid logging too frequently
skipping logging after 80960 examples to avoid logging too frequently
skipping logging after 81024 examples to avoid logging too frequently
train stats after 81088 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '594.06', 'grad_norm': 'nan', 'counters/examples': 81088, 'counters/updates': 1267}
skipping logging after 81152 examples to avoid logging too frequently
skipping logging after 81216 examples to avoid logging too frequently
skipping logging after 81280 examples to avoid logging too frequently
skipping logging after 81344 examples to avoid logging too frequently
skipping logging after 81408 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '578.35', 'grad_norm': 'nan', 'counters/examples': 81472, 'counters/updates': 1273}
skipping logging after 81536 examples to avoid logging too frequently
skipping logging after 81600 examples to avoid logging too frequently
skipping logging after 81664 examples to avoid logging too frequently
skipping logging after 81728 examples to avoid logging too frequently
skipping logging after 81792 examples to avoid logging too frequently
train stats after 81856 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '596.27', 'grad_norm': 'nan', 'counters/examples': 81856, 'counters/updates': 1279}
skipping logging after 81920 examples to avoid logging too frequently
skipping logging after 81984 examples to avoid logging too frequently
skipping logging after 82048 examples to avoid logging too frequently
skipping logging after 82112 examples to avoid logging too frequently
skipping logging after 82176 examples to avoid logging too frequently
train stats after 82240 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '617.83', 'grad_norm': 'nan', 'counters/examples': 82240, 'counters/updates': 1285}
skipping logging after 82304 examples to avoid logging too frequently
skipping logging after 82368 examples to avoid logging too frequently
skipping logging after 82432 examples to avoid logging too frequently
skipping logging after 82496 examples to avoid logging too frequently
skipping logging after 82560 examples to avoid logging too frequently
train stats after 82624 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '593.46', 'grad_norm': 'nan', 'counters/examples': 82624, 'counters/updates': 1291}
skipping logging after 82688 examples to avoid logging too frequently
skipping logging after 82752 examples to avoid logging too frequently
skipping logging after 82816 examples to avoid logging too frequently
skipping logging after 82880 examples to avoid logging too frequently
skipping logging after 82944 examples to avoid logging too frequently
train stats after 83008 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '589.99', 'grad_norm': 'nan', 'counters/examples': 83008, 'counters/updates': 1297}
skipping logging after 83072 examples to avoid logging too frequently
skipping logging after 83136 examples to avoid logging too frequently
skipping logging after 83200 examples to avoid logging too frequently
skipping logging after 83264 examples to avoid logging too frequently
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83392 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '587.76', 'grad_norm': 'nan', 'counters/examples': 83392, 'counters/updates': 1303}
skipping logging after 83456 examples to avoid logging too frequently
skipping logging after 83520 examples to avoid logging too frequently
skipping logging after 83584 examples to avoid logging too frequently
skipping logging after 83648 examples to avoid logging too frequently
skipping logging after 83712 examples to avoid logging too frequently
train stats after 83776 examples: {'rewards_train/chosen': 'nan', 'rewards_train/rejected': 'nan', 'rewards_train/accuracies': '0', 'rewards_train/margins': 'nan', 'logps_train/rejected': 'nan', 'logps_train/chosen': 'nan', 'loss/train': 'nan', 'examples_per_second': '583.85', 'grad_norm': 'nan', 'counters/examples': 83776, 'counters/updates': 1309}
Running evaluation after 83776 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:00<00:00, 35.92it/s]Computing eval metrics:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:00<00:00, 36.22it/s]Computing eval metrics:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [00:00<00:00, 36.35it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.21it/s]Computing eval metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 36.18it/s]
slurmstepd: error: *** REASON: burst_buffer/lua: Stage-out in progress ***
slurmstepd: error: *** JOB 52182 ON ip-10-0-222-166 CANCELLED AT 2024-01-12T14:27:08 ***
slurmstepd: error: *** REASON: burst_buffer/lua: Stage-out in progress ***
