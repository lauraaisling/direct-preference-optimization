no FSDP port specified; using open port for FSDP: 53501
seed: 0
exp_name: pythia2.8b_sfted3_dpo3_seed0
batch_size: 32
eval_batch_size: 16
debug: false
fsdp_port: 53501
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716
lr: 4.0e-07
gradient_accumulation_steps: 2
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 3
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 12000
minimum_log_interval_secs: 1.0
revision: main
model:
  name_or_path: lomahony/pythia-2.8b-helpful-sft-3epochs
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-193-57:.cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.08it/s]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
building reference model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 12.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.44s/it]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240319_005747-jlqomxa2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia2.8b_sfted3_dpo3_seed0
wandb: ⭐️ View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: 🚀 View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/jlqomxa2
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-2.8b-helpful-sft-3epochs
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  53%|█████▎    | 2712/5103 [00:00<00:00, 27113.11it/s]Processing HH static: 100%|██████████| 5103/5103 [00:00<00:00, 27401.40it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   0%|          | 275/96256 [00:00<00:35, 2738.77it/s]Processing HH static:   1%|          | 549/96256 [00:00<01:02, 1538.93it/s]Processing HH static:   3%|▎         | 3062/96256 [00:00<00:10, 9095.36it/s]Processing HH static:   6%|▌         | 5840/96256 [00:00<00:06, 14960.03it/s]Processing HH static:   9%|▉         | 8584/96256 [00:00<00:04, 18823.47it/s]Processing HH static:  12%|█▏        | 11368/96256 [00:00<00:03, 21587.49it/s]Processing HH static:  15%|█▍        | 14153/96256 [00:00<00:03, 23493.21it/s]Processing HH static:  18%|█▊        | 16928/96256 [00:00<00:03, 24783.16it/s]Processing HH static:  20%|██        | 19708/96256 [00:01<00:02, 25694.59it/s]Processing HH static:  23%|██▎       | 22464/96256 [00:01<00:02, 26255.47it/s]Processing HH static:  26%|██▌       | 25257/96256 [00:01<00:02, 26757.02it/s]Processing HH static:  29%|██▉       | 28031/96256 [00:01<00:02, 27051.24it/s]Processing HH static:  32%|███▏      | 30807/96256 [00:01<00:02, 27260.93it/s]Processing HH static:  35%|███▍      | 33554/96256 [00:01<00:03, 19022.56it/s]Processing HH static:  38%|███▊      | 36294/96256 [00:01<00:02, 20948.00it/s]Processing HH static:  41%|████      | 39005/96256 [00:01<00:02, 22469.65it/s]Processing HH static:  43%|████▎     | 41758/96256 [00:01<00:02, 23788.92it/s]Processing HH static:  46%|████▌     | 44411/96256 [00:02<00:02, 24531.43it/s]Processing HH static:  49%|████▉     | 47029/96256 [00:02<00:01, 24990.16it/s]Processing HH static:  52%|█████▏    | 49679/96256 [00:02<00:01, 25419.57it/s]Processing HH static:  54%|█████▍    | 52296/96256 [00:02<00:02, 17301.79it/s]Processing HH static:  57%|█████▋    | 54943/96256 [00:02<00:02, 19300.99it/s]Processing HH static:  60%|█████▉    | 57617/96256 [00:02<00:01, 21067.57it/s]Processing HH static:  63%|██████▎   | 60319/96256 [00:02<00:01, 22575.93it/s]Processing HH static:  65%|██████▌   | 63001/96256 [00:02<00:01, 23702.97it/s]Processing HH static:  68%|██████▊   | 65688/96256 [00:03<00:01, 24571.92it/s]Processing HH static:  71%|███████   | 68357/96256 [00:03<00:01, 25168.28it/s]Processing HH static:  74%|███████▎  | 70970/96256 [00:03<00:00, 25356.18it/s]Processing HH static:  76%|███████▋  | 73574/96256 [00:03<00:01, 15988.59it/s]Processing HH static:  79%|███████▊  | 75652/96256 [00:03<00:01, 15208.02it/s]Processing HH static:  81%|████████  | 77505/96256 [00:04<00:02, 6605.83it/s] Processing HH static:  82%|████████▏ | 78874/96256 [00:05<00:03, 5025.07it/s]Processing HH static:  83%|████████▎ | 79911/96256 [00:05<00:03, 4910.69it/s]Processing HH static:  84%|████████▍ | 80770/96256 [00:05<00:03, 4164.81it/s]Processing HH static:  85%|████████▍ | 81445/96256 [00:05<00:03, 3770.30it/s]Processing HH static:  85%|████████▌ | 81994/96256 [00:06<00:03, 3743.51it/s]Processing HH static:  86%|████████▌ | 82487/96256 [00:06<00:04, 3333.35it/s]Processing HH static:  86%|████████▌ | 82897/96256 [00:06<00:04, 3014.25it/s]Processing HH static:  86%|████████▋ | 83246/96256 [00:06<00:04, 2879.17it/s]Processing HH static:  87%|████████▋ | 83581/96256 [00:06<00:04, 2953.33it/s]Processing HH static:  87%|████████▋ | 83901/96256 [00:06<00:04, 2995.03it/s]Processing HH static:  87%|████████▋ | 84221/96256 [00:06<00:04, 2844.32it/s]Processing HH static:  88%|████████▊ | 84518/96256 [00:07<00:04, 2811.51it/s]Processing HH static:  88%|████████▊ | 84807/96256 [00:07<00:04, 2750.72it/s]Processing HH static:  88%|████████▊ | 85087/96256 [00:07<00:04, 2642.33it/s]Processing HH static:  89%|████████▊ | 85354/96256 [00:07<00:04, 2535.94it/s]Processing HH static:  89%|████████▉ | 85609/96256 [00:07<00:04, 2424.07it/s]Processing HH static:  89%|████████▉ | 85853/96256 [00:07<00:04, 2419.01it/s]Processing HH static:  89%|████████▉ | 86096/96256 [00:07<00:04, 2390.08it/s]Processing HH static:  90%|████████▉ | 86629/96256 [00:07<00:03, 3194.43it/s]Processing HH static:  90%|█████████ | 87082/96256 [00:07<00:02, 3567.02it/s]Processing HH static:  91%|█████████ | 87446/96256 [00:08<00:02, 3000.19it/s]Processing HH static:  91%|█████████▏| 87865/96256 [00:08<00:02, 3300.98it/s]Processing HH static:  92%|█████████▏| 88214/96256 [00:08<00:02, 3074.53it/s]Processing HH static:  92%|█████████▏| 88537/96256 [00:08<00:02, 2746.03it/s]Processing HH static:  92%|█████████▏| 88827/96256 [00:08<00:02, 2606.30it/s]Processing HH static:  93%|█████████▎| 89098/96256 [00:08<00:02, 2508.84it/s]Processing HH static:  93%|█████████▎| 89427/96256 [00:08<00:02, 2703.56it/s]Processing HH static:  93%|█████████▎| 89794/96256 [00:08<00:02, 2959.19it/s]Processing HH static:  94%|█████████▎| 90100/96256 [00:09<00:02, 2932.76it/s]Processing HH static:  94%|█████████▍| 90400/96256 [00:09<00:02, 2690.25it/s]Processing HH static:  94%|█████████▍| 90677/96256 [00:09<00:02, 2617.26it/s]Processing HH static:  95%|█████████▍| 91076/96256 [00:09<00:01, 2979.19it/s]Processing HH static:  95%|█████████▍| 91382/96256 [00:09<00:01, 2807.52it/s]Processing HH static:  95%|█████████▌| 91670/96256 [00:09<00:01, 2716.12it/s]Processing HH static:  96%|█████████▌| 91947/96256 [00:09<00:01, 2704.29it/s]Processing HH static:  96%|█████████▌| 92221/96256 [00:09<00:01, 2461.69it/s]Processing HH static:  96%|█████████▌| 92473/96256 [00:09<00:01, 2386.24it/s]Processing HH static:  96%|█████████▋| 92716/96256 [00:10<00:01, 2285.43it/s]Processing HH static:  97%|█████████▋| 92947/96256 [00:10<00:01, 2229.25it/s]Processing HH static:  97%|█████████▋| 93172/96256 [00:10<00:01, 2200.48it/s]Processing HH static:  97%|█████████▋| 93399/96256 [00:10<00:01, 2219.53it/s]Processing HH static:  97%|█████████▋| 93622/96256 [00:10<00:01, 2199.13it/s]Processing HH static:  98%|█████████▊| 93860/96256 [00:10<00:01, 2249.37it/s]Processing HH static:  98%|█████████▊| 94090/96256 [00:10<00:00, 2263.80it/s]Processing HH static:  98%|█████████▊| 94331/96256 [00:10<00:00, 2303.88it/s]Processing HH static:  98%|█████████▊| 94586/96256 [00:10<00:00, 2371.92it/s]Processing HH static:  99%|█████████▊| 94835/96256 [00:10<00:00, 2404.88it/s]Processing HH static:  99%|█████████▉| 95076/96256 [00:11<00:00, 2377.04it/s]Processing HH static:  99%|█████████▉| 95314/96256 [00:11<00:00, 2370.20it/s]Processing HH static:  99%|█████████▉| 95552/96256 [00:11<00:00, 2372.53it/s]Processing HH static: 100%|█████████▉| 95790/96256 [00:11<00:00, 2320.45it/s]Processing HH static: 100%|█████████▉| 96027/96256 [00:11<00:00, 2329.12it/s]Processing HH static: 100%|██████████| 96256/96256 [00:11<00:00, 8302.74it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:01<00:24,  1.64s/it]Computing eval metrics:  12%|█▎        | 2/16 [00:01<00:11,  1.17it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:02<00:07,  1.74it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:02<00:05,  2.23it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:02<00:04,  2.60it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:02<00:03,  3.06it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:03<00:02,  3.26it/s]Computing eval metrics:  50%|█████     | 8/16 [00:03<00:02,  3.43it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:03<00:01,  3.63it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:03<00:01,  3.66it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:04<00:01,  3.74it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:04<00:01,  3.78it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:04<00:00,  3.83it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:04<00:00,  3.78it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:05<00:00,  3.85it/s]Computing eval metrics: 100%|██████████| 16/16 [00:05<00:00,  3.78it/s]Computing eval metrics: 100%|██████████| 16/16 [00:05<00:00,  2.90it/s]
eval after 0: {'rewards_eval/chosen': '-0.016052', 'rewards_eval/rejected': '-0.030621', 'rewards_eval/accuracies': '0.50781', 'rewards_eval/margins': '0.014569', 'logps_eval/rejected': '-128.45', 'logps_eval/chosen': '-151.17', 'loss/eval': '0.70068'}
train stats after 32 examples: {'rewards_train/chosen': '-0.034942', 'rewards_train/rejected': '-0.057825', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022884', 'logps_train/rejected': '-118.03', 'logps_train/chosen': '-138.03', 'loss/train': '0.6954', 'examples_per_second': '25.196', 'grad_norm': '94.5', 'counters/examples': 32, 'counters/updates': 1}
train stats after 64 examples: {'rewards_train/chosen': '0.028233', 'rewards_train/rejected': '0.03573', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0074966', 'logps_train/rejected': '-170.95', 'logps_train/chosen': '-159.5', 'loss/train': '0.72256', 'examples_per_second': '29.108', 'grad_norm': '125.5', 'counters/examples': 64, 'counters/updates': 2}
train stats after 96 examples: {'rewards_train/chosen': '-0.021505', 'rewards_train/rejected': '0.033818', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.055323', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-125.44', 'loss/train': '0.7331', 'examples_per_second': '31.627', 'grad_norm': '87', 'counters/examples': 96, 'counters/updates': 3}
train stats after 128 examples: {'rewards_train/chosen': '-0.031941', 'rewards_train/rejected': '-0.0022252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029716', 'logps_train/rejected': '-161.14', 'logps_train/chosen': '-187.05', 'loss/train': '0.72902', 'examples_per_second': '31.567', 'grad_norm': '108.5', 'counters/examples': 128, 'counters/updates': 4}
train stats after 160 examples: {'rewards_train/chosen': '-0.011987', 'rewards_train/rejected': '-0.026726', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014738', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-165.26', 'loss/train': '0.69751', 'examples_per_second': '32.226', 'grad_norm': '81.5', 'counters/examples': 160, 'counters/updates': 5}
train stats after 192 examples: {'rewards_train/chosen': '-0.060344', 'rewards_train/rejected': '0.0044708', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.064815', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-133.27', 'loss/train': '0.74123', 'examples_per_second': '31.659', 'grad_norm': '106.5', 'counters/examples': 192, 'counters/updates': 6}
train stats after 224 examples: {'rewards_train/chosen': '0.055616', 'rewards_train/rejected': '-0.09557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15119', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-144.48', 'loss/train': '0.62786', 'examples_per_second': '31.303', 'grad_norm': '256', 'counters/examples': 224, 'counters/updates': 7}
train stats after 256 examples: {'rewards_train/chosen': '0.04277', 'rewards_train/rejected': '-0.015061', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057831', 'logps_train/rejected': '-167.51', 'logps_train/chosen': '-206.14', 'loss/train': '0.68507', 'examples_per_second': '31.624', 'grad_norm': '92.5', 'counters/examples': 256, 'counters/updates': 8}
train stats after 288 examples: {'rewards_train/chosen': '-0.050826', 'rewards_train/rejected': '-0.078961', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028135', 'logps_train/rejected': '-162.04', 'logps_train/chosen': '-137.01', 'loss/train': '0.69286', 'examples_per_second': '32.337', 'grad_norm': '73.5', 'counters/examples': 288, 'counters/updates': 9}
train stats after 320 examples: {'rewards_train/chosen': '-0.12223', 'rewards_train/rejected': '0.0056488', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12787', 'logps_train/rejected': '-147.18', 'logps_train/chosen': '-171.49', 'loss/train': '0.77106', 'examples_per_second': '30.086', 'grad_norm': '168', 'counters/examples': 320, 'counters/updates': 10}
train stats after 352 examples: {'rewards_train/chosen': '0.032698', 'rewards_train/rejected': '-0.041679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074377', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-138.67', 'loss/train': '0.66761', 'examples_per_second': '31.955', 'grad_norm': '99.5', 'counters/examples': 352, 'counters/updates': 11}
skipping logging after 384 examples to avoid logging too frequently
train stats after 416 examples: {'rewards_train/chosen': '-0.065178', 'rewards_train/rejected': '0.0082412', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.073419', 'logps_train/rejected': '-119.78', 'logps_train/chosen': '-170.32', 'loss/train': '0.74322', 'examples_per_second': '31.264', 'grad_norm': '85', 'counters/examples': 416, 'counters/updates': 13}
train stats after 448 examples: {'rewards_train/chosen': '-0.084264', 'rewards_train/rejected': '-0.02019', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.064074', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-163.66', 'loss/train': '0.73544', 'examples_per_second': '31.681', 'grad_norm': '127', 'counters/examples': 448, 'counters/updates': 14}
skipping logging after 480 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '-0.073026', 'rewards_train/rejected': '0.0044621', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077488', 'logps_train/rejected': '-163.77', 'logps_train/chosen': '-160.28', 'loss/train': '0.74199', 'examples_per_second': '34.188', 'grad_norm': '155', 'counters/examples': 512, 'counters/updates': 16}
train stats after 544 examples: {'rewards_train/chosen': '-0.070669', 'rewards_train/rejected': '-0.066222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0044471', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-163.52', 'loss/train': '0.70632', 'examples_per_second': '31.775', 'grad_norm': '148', 'counters/examples': 544, 'counters/updates': 17}
train stats after 576 examples: {'rewards_train/chosen': '-0.033542', 'rewards_train/rejected': '-0.036126', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0025832', 'logps_train/rejected': '-133.6', 'logps_train/chosen': '-185.92', 'loss/train': '0.70346', 'examples_per_second': '31.701', 'grad_norm': '71', 'counters/examples': 576, 'counters/updates': 18}
train stats after 608 examples: {'rewards_train/chosen': '0.023667', 'rewards_train/rejected': '0.00072201', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022945', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-145.69', 'loss/train': '0.69498', 'examples_per_second': '32.362', 'grad_norm': '78.5', 'counters/examples': 608, 'counters/updates': 19}
train stats after 640 examples: {'rewards_train/chosen': '-0.035051', 'rewards_train/rejected': '0.037547', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.072598', 'logps_train/rejected': '-159.53', 'logps_train/chosen': '-167.21', 'loss/train': '0.75155', 'examples_per_second': '30.724', 'grad_norm': '306', 'counters/examples': 640, 'counters/updates': 20}
train stats after 672 examples: {'rewards_train/chosen': '-0.057106', 'rewards_train/rejected': '-0.041005', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.016101', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-172.17', 'loss/train': '0.70883', 'examples_per_second': '33.211', 'grad_norm': '162', 'counters/examples': 672, 'counters/updates': 21}
skipping logging after 704 examples to avoid logging too frequently
train stats after 736 examples: {'rewards_train/chosen': '-0.0012159', 'rewards_train/rejected': '0.011899', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013115', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-131.29', 'loss/train': '0.70838', 'examples_per_second': '33.476', 'grad_norm': '53.75', 'counters/examples': 736, 'counters/updates': 23}
skipping logging after 768 examples to avoid logging too frequently
train stats after 800 examples: {'rewards_train/chosen': '0.054868', 'rewards_train/rejected': '-0.027251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082119', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-210.97', 'loss/train': '0.66811', 'examples_per_second': '30.617', 'grad_norm': '75.5', 'counters/examples': 800, 'counters/updates': 25}
train stats after 832 examples: {'rewards_train/chosen': '-0.081774', 'rewards_train/rejected': '0.019266', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10104', 'logps_train/rejected': '-115.62', 'logps_train/chosen': '-138.38', 'loss/train': '0.76567', 'examples_per_second': '31.713', 'grad_norm': '92.5', 'counters/examples': 832, 'counters/updates': 26}
train stats after 864 examples: {'rewards_train/chosen': '-0.018004', 'rewards_train/rejected': '-0.021021', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0030174', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-128.37', 'loss/train': '0.70136', 'examples_per_second': '31.274', 'grad_norm': '97.5', 'counters/examples': 864, 'counters/updates': 27}
skipping logging after 896 examples to avoid logging too frequently
train stats after 928 examples: {'rewards_train/chosen': '-0.046448', 'rewards_train/rejected': '0.042997', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.089445', 'logps_train/rejected': '-149.66', 'logps_train/chosen': '-187.72', 'loss/train': '0.76526', 'examples_per_second': '31.749', 'grad_norm': '109', 'counters/examples': 928, 'counters/updates': 29}
train stats after 960 examples: {'rewards_train/chosen': '-0.15032', 'rewards_train/rejected': '-0.094763', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.055556', 'logps_train/rejected': '-130.97', 'logps_train/chosen': '-137.27', 'loss/train': '0.75412', 'examples_per_second': '31.548', 'grad_norm': '171', 'counters/examples': 960, 'counters/updates': 30}
train stats after 992 examples: {'rewards_train/chosen': '0.044528', 'rewards_train/rejected': '-0.056759', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10129', 'logps_train/rejected': '-153', 'logps_train/chosen': '-169.83', 'loss/train': '0.66266', 'examples_per_second': '31.096', 'grad_norm': '61', 'counters/examples': 992, 'counters/updates': 31}
train stats after 1024 examples: {'rewards_train/chosen': '0.053082', 'rewards_train/rejected': '-0.018477', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071559', 'logps_train/rejected': '-160.58', 'logps_train/chosen': '-136.65', 'loss/train': '0.67945', 'examples_per_second': '32.275', 'grad_norm': '67.5', 'counters/examples': 1024, 'counters/updates': 32}
skipping logging after 1056 examples to avoid logging too frequently
train stats after 1088 examples: {'rewards_train/chosen': '-0.063884', 'rewards_train/rejected': '0.009591', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.073475', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-184.08', 'loss/train': '0.74529', 'examples_per_second': '30.196', 'grad_norm': '162', 'counters/examples': 1088, 'counters/updates': 34}
skipping logging after 1120 examples to avoid logging too frequently
train stats after 1152 examples: {'rewards_train/chosen': '-0.059381', 'rewards_train/rejected': '-0.059261', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00011973', 'logps_train/rejected': '-129.45', 'logps_train/chosen': '-154.86', 'loss/train': '0.70299', 'examples_per_second': '30.338', 'grad_norm': '105.5', 'counters/examples': 1152, 'counters/updates': 36}
train stats after 1184 examples: {'rewards_train/chosen': '-0.085815', 'rewards_train/rejected': '-0.0059663', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.079849', 'logps_train/rejected': '-171.97', 'logps_train/chosen': '-171.36', 'loss/train': '0.74471', 'examples_per_second': '30.639', 'grad_norm': '76.5', 'counters/examples': 1184, 'counters/updates': 37}
train stats after 1216 examples: {'rewards_train/chosen': '-0.025177', 'rewards_train/rejected': '-0.054833', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.029656', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-166.61', 'loss/train': '0.70122', 'examples_per_second': '30.853', 'grad_norm': '115', 'counters/examples': 1216, 'counters/updates': 38}
train stats after 1248 examples: {'rewards_train/chosen': '-0.10046', 'rewards_train/rejected': '0.007153', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10761', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-166.86', 'loss/train': '0.77365', 'examples_per_second': '31.729', 'grad_norm': '74.5', 'counters/examples': 1248, 'counters/updates': 39}
train stats after 1280 examples: {'rewards_train/chosen': '0.038803', 'rewards_train/rejected': '-0.025705', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064508', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-132.65', 'loss/train': '0.66904', 'examples_per_second': '30.764', 'grad_norm': '119.5', 'counters/examples': 1280, 'counters/updates': 40}
train stats after 1312 examples: {'rewards_train/chosen': '-0.062274', 'rewards_train/rejected': '-0.029762', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032511', 'logps_train/rejected': '-148.27', 'logps_train/chosen': '-157.12', 'loss/train': '0.73219', 'examples_per_second': '30.714', 'grad_norm': '101', 'counters/examples': 1312, 'counters/updates': 41}
train stats after 1344 examples: {'rewards_train/chosen': '-0.03781', 'rewards_train/rejected': '-0.034586', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0032233', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-241', 'loss/train': '0.70807', 'examples_per_second': '31.023', 'grad_norm': '141', 'counters/examples': 1344, 'counters/updates': 42}
train stats after 1376 examples: {'rewards_train/chosen': '-0.08226', 'rewards_train/rejected': '-0.072891', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0093691', 'logps_train/rejected': '-180.29', 'logps_train/chosen': '-188.37', 'loss/train': '0.71437', 'examples_per_second': '31.479', 'grad_norm': '84', 'counters/examples': 1376, 'counters/updates': 43}
skipping logging after 1408 examples to avoid logging too frequently
train stats after 1440 examples: {'rewards_train/chosen': '-0.071299', 'rewards_train/rejected': '0.044511', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11581', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-160.08', 'loss/train': '0.77411', 'examples_per_second': '31.464', 'grad_norm': '194', 'counters/examples': 1440, 'counters/updates': 45}
train stats after 1472 examples: {'rewards_train/chosen': '-0.049447', 'rewards_train/rejected': '-0.068304', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018857', 'logps_train/rejected': '-162.61', 'logps_train/chosen': '-153.85', 'loss/train': '0.69642', 'examples_per_second': '30.276', 'grad_norm': '72', 'counters/examples': 1472, 'counters/updates': 46}
train stats after 1504 examples: {'rewards_train/chosen': '0.014237', 'rewards_train/rejected': '0.02329', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0090531', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-139.37', 'loss/train': '0.7118', 'examples_per_second': '32.683', 'grad_norm': '75.5', 'counters/examples': 1504, 'counters/updates': 47}
train stats after 1536 examples: {'rewards_train/chosen': '-0.058235', 'rewards_train/rejected': '-0.00040024', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.057834', 'logps_train/rejected': '-132.71', 'logps_train/chosen': '-160.62', 'loss/train': '0.73468', 'examples_per_second': '30.897', 'grad_norm': '102', 'counters/examples': 1536, 'counters/updates': 48}
train stats after 1568 examples: {'rewards_train/chosen': '-0.034208', 'rewards_train/rejected': '-0.035153', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00094426', 'logps_train/rejected': '-137.39', 'logps_train/chosen': '-129.6', 'loss/train': '0.6995', 'examples_per_second': '31.262', 'grad_norm': '78', 'counters/examples': 1568, 'counters/updates': 49}
train stats after 1600 examples: {'rewards_train/chosen': '-0.097048', 'rewards_train/rejected': '-0.022383', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.074665', 'logps_train/rejected': '-182.99', 'logps_train/chosen': '-204.97', 'loss/train': '0.75427', 'examples_per_second': '30.194', 'grad_norm': '185', 'counters/examples': 1600, 'counters/updates': 50}
skipping logging after 1632 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '-0.011871', 'rewards_train/rejected': '0.054868', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.06674', 'logps_train/rejected': '-153.32', 'logps_train/chosen': '-135.66', 'loss/train': '0.7384', 'examples_per_second': '30.217', 'grad_norm': '140', 'counters/examples': 1664, 'counters/updates': 52}
skipping logging after 1696 examples to avoid logging too frequently
train stats after 1728 examples: {'rewards_train/chosen': '-0.02692', 'rewards_train/rejected': '-0.044221', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017301', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-167.68', 'loss/train': '0.69357', 'examples_per_second': '32.899', 'grad_norm': '79', 'counters/examples': 1728, 'counters/updates': 54}
train stats after 1760 examples: {'rewards_train/chosen': '0.045686', 'rewards_train/rejected': '-0.007847', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053533', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-200.2', 'loss/train': '0.68025', 'examples_per_second': '31.095', 'grad_norm': '143', 'counters/examples': 1760, 'counters/updates': 55}
skipping logging after 1792 examples to avoid logging too frequently
train stats after 1824 examples: {'rewards_train/chosen': '-0.13185', 'rewards_train/rejected': '-0.034305', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.097544', 'logps_train/rejected': '-159.78', 'logps_train/chosen': '-173.46', 'loss/train': '0.76505', 'examples_per_second': '31.657', 'grad_norm': '143', 'counters/examples': 1824, 'counters/updates': 57}
train stats after 1856 examples: {'rewards_train/chosen': '-0.065095', 'rewards_train/rejected': '-0.0028585', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.062237', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-133.93', 'loss/train': '0.73592', 'examples_per_second': '30.666', 'grad_norm': '73', 'counters/examples': 1856, 'counters/updates': 58}
train stats after 1888 examples: {'rewards_train/chosen': '-0.01091', 'rewards_train/rejected': '-0.017144', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0062344', 'logps_train/rejected': '-107.37', 'logps_train/chosen': '-120.11', 'loss/train': '0.70223', 'examples_per_second': '30.491', 'grad_norm': '90', 'counters/examples': 1888, 'counters/updates': 59}
train stats after 1920 examples: {'rewards_train/chosen': '-0.029797', 'rewards_train/rejected': '0.044838', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.074635', 'logps_train/rejected': '-181.01', 'logps_train/chosen': '-133.56', 'loss/train': '0.74579', 'examples_per_second': '32.752', 'grad_norm': '90', 'counters/examples': 1920, 'counters/updates': 60}
skipping logging after 1952 examples to avoid logging too frequently
train stats after 1984 examples: {'rewards_train/chosen': '-0.01169', 'rewards_train/rejected': '-0.04258', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03089', 'logps_train/rejected': '-122.86', 'logps_train/chosen': '-166.76', 'loss/train': '0.688', 'examples_per_second': '30.76', 'grad_norm': '127', 'counters/examples': 1984, 'counters/updates': 62}
train stats after 2016 examples: {'rewards_train/chosen': '0.028101', 'rewards_train/rejected': '0.01099', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017112', 'logps_train/rejected': '-172.81', 'logps_train/chosen': '-124.73', 'loss/train': '0.69487', 'examples_per_second': '31.048', 'grad_norm': '138', 'counters/examples': 2016, 'counters/updates': 63}
train stats after 2048 examples: {'rewards_train/chosen': '-0.0065452', 'rewards_train/rejected': '-0.052239', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045694', 'logps_train/rejected': '-161.19', 'logps_train/chosen': '-195.9', 'loss/train': '0.6881', 'examples_per_second': '31.857', 'grad_norm': '96', 'counters/examples': 2048, 'counters/updates': 64}
train stats after 2080 examples: {'rewards_train/chosen': '0.01138', 'rewards_train/rejected': '-0.02877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04015', 'logps_train/rejected': '-145.35', 'logps_train/chosen': '-138.08', 'loss/train': '0.68646', 'examples_per_second': '31.139', 'grad_norm': '122', 'counters/examples': 2080, 'counters/updates': 65}
train stats after 2112 examples: {'rewards_train/chosen': '0.023973', 'rewards_train/rejected': '-0.037287', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06126', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-113.56', 'loss/train': '0.67421', 'examples_per_second': '30.94', 'grad_norm': '53.25', 'counters/examples': 2112, 'counters/updates': 66}
train stats after 2144 examples: {'rewards_train/chosen': '-0.023127', 'rewards_train/rejected': '-0.015633', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0074941', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-124.94', 'loss/train': '0.70743', 'examples_per_second': '32.229', 'grad_norm': '68', 'counters/examples': 2144, 'counters/updates': 67}
skipping logging after 2176 examples to avoid logging too frequently
train stats after 2208 examples: {'rewards_train/chosen': '0.0064049', 'rewards_train/rejected': '0.013342', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0069366', 'logps_train/rejected': '-147.68', 'logps_train/chosen': '-161.08', 'loss/train': '0.70767', 'examples_per_second': '31.033', 'grad_norm': '134', 'counters/examples': 2208, 'counters/updates': 69}
train stats after 2240 examples: {'rewards_train/chosen': '-0.082905', 'rewards_train/rejected': '-0.0068556', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.07605', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-166.48', 'loss/train': '0.74596', 'examples_per_second': '31.35', 'grad_norm': '75', 'counters/examples': 2240, 'counters/updates': 70}
train stats after 2272 examples: {'rewards_train/chosen': '-0.00091559', 'rewards_train/rejected': '-0.075344', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074428', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-141.68', 'loss/train': '0.67148', 'examples_per_second': '31.985', 'grad_norm': '55.25', 'counters/examples': 2272, 'counters/updates': 71}
train stats after 2304 examples: {'rewards_train/chosen': '0.04299', 'rewards_train/rejected': '-0.021478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064468', 'logps_train/rejected': '-107.66', 'logps_train/chosen': '-137.33', 'loss/train': '0.67826', 'examples_per_second': '31.018', 'grad_norm': '95.5', 'counters/examples': 2304, 'counters/updates': 72}
train stats after 2336 examples: {'rewards_train/chosen': '0.014546', 'rewards_train/rejected': '0.028715', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014169', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-151.9', 'loss/train': '0.71049', 'examples_per_second': '31.671', 'grad_norm': '93.5', 'counters/examples': 2336, 'counters/updates': 73}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2400 examples: {'rewards_train/chosen': '-0.026582', 'rewards_train/rejected': '-0.075308', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.048727', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-171.33', 'loss/train': '0.69413', 'examples_per_second': '30.208', 'grad_norm': '102.5', 'counters/examples': 2400, 'counters/updates': 75}
train stats after 2432 examples: {'rewards_train/chosen': '0.0023623', 'rewards_train/rejected': '0.050967', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.048605', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-121.48', 'loss/train': '0.72886', 'examples_per_second': '30.611', 'grad_norm': '209', 'counters/examples': 2432, 'counters/updates': 76}
skipping logging after 2464 examples to avoid logging too frequently
train stats after 2496 examples: {'rewards_train/chosen': '-0.031069', 'rewards_train/rejected': '-0.075697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044628', 'logps_train/rejected': '-154.77', 'logps_train/chosen': '-153.49', 'loss/train': '0.6879', 'examples_per_second': '31.55', 'grad_norm': '108.5', 'counters/examples': 2496, 'counters/updates': 78}
train stats after 2528 examples: {'rewards_train/chosen': '-0.051419', 'rewards_train/rejected': '-0.051013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00040625', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-174.14', 'loss/train': '0.70407', 'examples_per_second': '30.545', 'grad_norm': '92.5', 'counters/examples': 2528, 'counters/updates': 79}
train stats after 2560 examples: {'rewards_train/chosen': '-0.070658', 'rewards_train/rejected': '0.043273', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.11393', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-133.64', 'loss/train': '0.77725', 'examples_per_second': '31.662', 'grad_norm': '116', 'counters/examples': 2560, 'counters/updates': 80}
skipping logging after 2592 examples to avoid logging too frequently
train stats after 2624 examples: {'rewards_train/chosen': '-0.092203', 'rewards_train/rejected': '-0.014414', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077789', 'logps_train/rejected': '-153.13', 'logps_train/chosen': '-183.29', 'loss/train': '0.7474', 'examples_per_second': '31.456', 'grad_norm': '153', 'counters/examples': 2624, 'counters/updates': 82}
train stats after 2656 examples: {'rewards_train/chosen': '-0.059195', 'rewards_train/rejected': '-0.069954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010758', 'logps_train/rejected': '-155.29', 'logps_train/chosen': '-151.74', 'loss/train': '0.70568', 'examples_per_second': '30.826', 'grad_norm': '149', 'counters/examples': 2656, 'counters/updates': 83}
skipping logging after 2688 examples to avoid logging too frequently
train stats after 2720 examples: {'rewards_train/chosen': '0.05104', 'rewards_train/rejected': '0.007826', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043214', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-173.22', 'loss/train': '0.70288', 'examples_per_second': '32.365', 'grad_norm': '82.5', 'counters/examples': 2720, 'counters/updates': 85}
train stats after 2752 examples: {'rewards_train/chosen': '-0.038788', 'rewards_train/rejected': '0.044271', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.083059', 'logps_train/rejected': '-128.24', 'logps_train/chosen': '-145.16', 'loss/train': '0.7476', 'examples_per_second': '31.969', 'grad_norm': '80.5', 'counters/examples': 2752, 'counters/updates': 86}
train stats after 2784 examples: {'rewards_train/chosen': '-0.065485', 'rewards_train/rejected': '0.043565', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.10905', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-147.52', 'loss/train': '0.75874', 'examples_per_second': '30.949', 'grad_norm': '134', 'counters/examples': 2784, 'counters/updates': 87}
train stats after 2816 examples: {'rewards_train/chosen': '-0.0023885', 'rewards_train/rejected': '-0.0012254', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0011631', 'logps_train/rejected': '-154.29', 'logps_train/chosen': '-150.61', 'loss/train': '0.70323', 'examples_per_second': '31.2', 'grad_norm': '97', 'counters/examples': 2816, 'counters/updates': 88}
train stats after 2848 examples: {'rewards_train/chosen': '0.048646', 'rewards_train/rejected': '-0.0017664', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050412', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-171.79', 'loss/train': '0.68015', 'examples_per_second': '31.414', 'grad_norm': '88.5', 'counters/examples': 2848, 'counters/updates': 89}
train stats after 2880 examples: {'rewards_train/chosen': '-0.095421', 'rewards_train/rejected': '-0.0019095', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.093511', 'logps_train/rejected': '-168.61', 'logps_train/chosen': '-175.13', 'loss/train': '0.7624', 'examples_per_second': '31.041', 'grad_norm': '135', 'counters/examples': 2880, 'counters/updates': 90}
skipping logging after 2912 examples to avoid logging too frequently
train stats after 2944 examples: {'rewards_train/chosen': '-0.038218', 'rewards_train/rejected': '-0.0025364', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035682', 'logps_train/rejected': '-161.83', 'logps_train/chosen': '-173.7', 'loss/train': '0.7258', 'examples_per_second': '32.232', 'grad_norm': '120.5', 'counters/examples': 2944, 'counters/updates': 92}
train stats after 2976 examples: {'rewards_train/chosen': '0.024433', 'rewards_train/rejected': '0.00098563', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023447', 'logps_train/rejected': '-142.83', 'logps_train/chosen': '-153.64', 'loss/train': '0.70476', 'examples_per_second': '31.885', 'grad_norm': '71.5', 'counters/examples': 2976, 'counters/updates': 93}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3040 examples: {'rewards_train/chosen': '0.018103', 'rewards_train/rejected': '-0.075385', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093488', 'logps_train/rejected': '-111.79', 'logps_train/chosen': '-128.27', 'loss/train': '0.65919', 'examples_per_second': '32.048', 'grad_norm': '56.25', 'counters/examples': 3040, 'counters/updates': 95}
train stats after 3072 examples: {'rewards_train/chosen': '-0.041057', 'rewards_train/rejected': '0.019715', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.060771', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-168.62', 'loss/train': '0.73795', 'examples_per_second': '32.524', 'grad_norm': '340', 'counters/examples': 3072, 'counters/updates': 96}
train stats after 3104 examples: {'rewards_train/chosen': '-0.013415', 'rewards_train/rejected': '-0.062819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049404', 'logps_train/rejected': '-131.46', 'logps_train/chosen': '-140.41', 'loss/train': '0.67466', 'examples_per_second': '32.752', 'grad_norm': '137', 'counters/examples': 3104, 'counters/updates': 97}
train stats after 3136 examples: {'rewards_train/chosen': '-0.020987', 'rewards_train/rejected': '-0.033974', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012987', 'logps_train/rejected': '-107.03', 'logps_train/chosen': '-137.72', 'loss/train': '0.7005', 'examples_per_second': '33.086', 'grad_norm': '108', 'counters/examples': 3136, 'counters/updates': 98}
train stats after 3168 examples: {'rewards_train/chosen': '-0.035127', 'rewards_train/rejected': '0.036699', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.071826', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-120.66', 'loss/train': '0.74074', 'examples_per_second': '31.707', 'grad_norm': '167', 'counters/examples': 3168, 'counters/updates': 99}
skipping logging after 3200 examples to avoid logging too frequently
train stats after 3232 examples: {'rewards_train/chosen': '0.052196', 'rewards_train/rejected': '-0.027008', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079205', 'logps_train/rejected': '-152.52', 'logps_train/chosen': '-178.84', 'loss/train': '0.68588', 'examples_per_second': '30.292', 'grad_norm': '98.5', 'counters/examples': 3232, 'counters/updates': 101}
train stats after 3264 examples: {'rewards_train/chosen': '-0.016076', 'rewards_train/rejected': '-0.012638', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.003438', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-157.13', 'loss/train': '0.70445', 'examples_per_second': '31.696', 'grad_norm': '129', 'counters/examples': 3264, 'counters/updates': 102}
train stats after 3296 examples: {'rewards_train/chosen': '0.070271', 'rewards_train/rejected': '-0.021234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091506', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-158.24', 'loss/train': '0.65599', 'examples_per_second': '31.129', 'grad_norm': '66.5', 'counters/examples': 3296, 'counters/updates': 103}
train stats after 3328 examples: {'rewards_train/chosen': '-0.0022173', 'rewards_train/rejected': '-0.019401', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017184', 'logps_train/rejected': '-142.88', 'logps_train/chosen': '-161.67', 'loss/train': '0.69648', 'examples_per_second': '30.782', 'grad_norm': '118', 'counters/examples': 3328, 'counters/updates': 104}
train stats after 3360 examples: {'rewards_train/chosen': '0.043178', 'rewards_train/rejected': '-0.0295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072677', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-136.44', 'loss/train': '0.66504', 'examples_per_second': '32.602', 'grad_norm': '42.25', 'counters/examples': 3360, 'counters/updates': 105}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3424 examples: {'rewards_train/chosen': '-0.028247', 'rewards_train/rejected': '-0.019218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0090294', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-173.02', 'loss/train': '0.70916', 'examples_per_second': '32.991', 'grad_norm': '91.5', 'counters/examples': 3424, 'counters/updates': 107}
train stats after 3456 examples: {'rewards_train/chosen': '-0.033638', 'rewards_train/rejected': '0.04715', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.080788', 'logps_train/rejected': '-170.76', 'logps_train/chosen': '-174.84', 'loss/train': '0.74685', 'examples_per_second': '30.209', 'grad_norm': '596', 'counters/examples': 3456, 'counters/updates': 108}
train stats after 3488 examples: {'rewards_train/chosen': '-0.033596', 'rewards_train/rejected': '-0.088112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054516', 'logps_train/rejected': '-170.86', 'logps_train/chosen': '-169.04', 'loss/train': '0.68163', 'examples_per_second': '29.608', 'grad_norm': '108', 'counters/examples': 3488, 'counters/updates': 109}
train stats after 3520 examples: {'rewards_train/chosen': '-0.031265', 'rewards_train/rejected': '-0.022534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0087315', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-158.81', 'loss/train': '0.70824', 'examples_per_second': '29.721', 'grad_norm': '140', 'counters/examples': 3520, 'counters/updates': 110}
train stats after 3552 examples: {'rewards_train/chosen': '-0.018985', 'rewards_train/rejected': '-0.028727', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0097419', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-159.44', 'loss/train': '0.70678', 'examples_per_second': '30.668', 'grad_norm': '148', 'counters/examples': 3552, 'counters/updates': 111}
train stats after 3584 examples: {'rewards_train/chosen': '-0.10095', 'rewards_train/rejected': '0.0048796', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.10583', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-205.89', 'loss/train': '0.75983', 'examples_per_second': '32.367', 'grad_norm': '126', 'counters/examples': 3584, 'counters/updates': 112}
train stats after 3616 examples: {'rewards_train/chosen': '-0.0067245', 'rewards_train/rejected': '-0.0016603', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0050642', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-157.77', 'loss/train': '0.70577', 'examples_per_second': '31.844', 'grad_norm': '100', 'counters/examples': 3616, 'counters/updates': 113}
train stats after 3648 examples: {'rewards_train/chosen': '-0.032931', 'rewards_train/rejected': '0.0029134', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035845', 'logps_train/rejected': '-136.29', 'logps_train/chosen': '-138.75', 'loss/train': '0.72064', 'examples_per_second': '29.973', 'grad_norm': '97.5', 'counters/examples': 3648, 'counters/updates': 114}
train stats after 3680 examples: {'rewards_train/chosen': '0.0044563', 'rewards_train/rejected': '0.026591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022135', 'logps_train/rejected': '-121.75', 'logps_train/chosen': '-176.35', 'loss/train': '0.72244', 'examples_per_second': '31.42', 'grad_norm': '121', 'counters/examples': 3680, 'counters/updates': 115}
train stats after 3712 examples: {'rewards_train/chosen': '0.062023', 'rewards_train/rejected': '-0.013701', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075724', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-132.53', 'loss/train': '0.67662', 'examples_per_second': '31.398', 'grad_norm': '258', 'counters/examples': 3712, 'counters/updates': 116}
train stats after 3744 examples: {'rewards_train/chosen': '-0.051199', 'rewards_train/rejected': '-0.015994', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035205', 'logps_train/rejected': '-158.9', 'logps_train/chosen': '-143.21', 'loss/train': '0.71609', 'examples_per_second': '31.482', 'grad_norm': '91', 'counters/examples': 3744, 'counters/updates': 117}
train stats after 3776 examples: {'rewards_train/chosen': '-0.03087', 'rewards_train/rejected': '-0.003892', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026978', 'logps_train/rejected': '-131.17', 'logps_train/chosen': '-186.86', 'loss/train': '0.71612', 'examples_per_second': '32.792', 'grad_norm': '85.5', 'counters/examples': 3776, 'counters/updates': 118}
train stats after 3808 examples: {'rewards_train/chosen': '-0.042894', 'rewards_train/rejected': '0.064969', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.10786', 'logps_train/rejected': '-120.77', 'logps_train/chosen': '-135.96', 'loss/train': '0.76292', 'examples_per_second': '31.729', 'grad_norm': '89.5', 'counters/examples': 3808, 'counters/updates': 119}
train stats after 3840 examples: {'rewards_train/chosen': '0.03727', 'rewards_train/rejected': '-0.039446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076716', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-146.97', 'loss/train': '0.6702', 'examples_per_second': '30.849', 'grad_norm': '75', 'counters/examples': 3840, 'counters/updates': 120}
train stats after 3872 examples: {'rewards_train/chosen': '0.065599', 'rewards_train/rejected': '-0.051536', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11714', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-144.65', 'loss/train': '0.64768', 'examples_per_second': '30.146', 'grad_norm': '75', 'counters/examples': 3872, 'counters/updates': 121}
train stats after 3904 examples: {'rewards_train/chosen': '0.10899', 'rewards_train/rejected': '0.066217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042774', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-166.35', 'loss/train': '0.69124', 'examples_per_second': '31.656', 'grad_norm': '119.5', 'counters/examples': 3904, 'counters/updates': 122}
train stats after 3936 examples: {'rewards_train/chosen': '0.07744', 'rewards_train/rejected': '0.001256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076184', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-163.8', 'loss/train': '0.66975', 'examples_per_second': '30.711', 'grad_norm': '92', 'counters/examples': 3936, 'counters/updates': 123}
skipping logging after 3968 examples to avoid logging too frequently
train stats after 4000 examples: {'rewards_train/chosen': '-0.03289', 'rewards_train/rejected': '-0.036688', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.003798', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-141.74', 'loss/train': '0.70277', 'examples_per_second': '34.493', 'grad_norm': '57.5', 'counters/examples': 4000, 'counters/updates': 125}
train stats after 4032 examples: {'rewards_train/chosen': '-0.01865', 'rewards_train/rejected': '-0.021345', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0026953', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-137.89', 'loss/train': '0.70029', 'examples_per_second': '31.684', 'grad_norm': '102', 'counters/examples': 4032, 'counters/updates': 126}
skipping logging after 4064 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '0.016222', 'rewards_train/rejected': '-0.056188', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072411', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-159.33', 'loss/train': '0.67161', 'examples_per_second': '32.351', 'grad_norm': '75.5', 'counters/examples': 4096, 'counters/updates': 128}
train stats after 4128 examples: {'rewards_train/chosen': '0.068141', 'rewards_train/rejected': '-0.031205', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.099345', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-187.82', 'loss/train': '0.66805', 'examples_per_second': '31.615', 'grad_norm': '98.5', 'counters/examples': 4128, 'counters/updates': 129}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4192 examples: {'rewards_train/chosen': '0.027591', 'rewards_train/rejected': '-0.013265', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.040856', 'logps_train/rejected': '-158.45', 'logps_train/chosen': '-121.28', 'loss/train': '0.68834', 'examples_per_second': '31.56', 'grad_norm': '78.5', 'counters/examples': 4192, 'counters/updates': 131}
train stats after 4224 examples: {'rewards_train/chosen': '-0.067842', 'rewards_train/rejected': '0.0084037', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.076246', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-134', 'loss/train': '0.74284', 'examples_per_second': '31.285', 'grad_norm': '71.5', 'counters/examples': 4224, 'counters/updates': 132}
train stats after 4256 examples: {'rewards_train/chosen': '0.019595', 'rewards_train/rejected': '-0.036154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055749', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-143.03', 'loss/train': '0.69679', 'examples_per_second': '30.155', 'grad_norm': '108.5', 'counters/examples': 4256, 'counters/updates': 133}
train stats after 4288 examples: {'rewards_train/chosen': '-0.09552', 'rewards_train/rejected': '-0.015527', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.079992', 'logps_train/rejected': '-136.71', 'logps_train/chosen': '-134.24', 'loss/train': '0.74742', 'examples_per_second': '30.641', 'grad_norm': '226', 'counters/examples': 4288, 'counters/updates': 134}
skipping logging after 4320 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '-0.064334', 'rewards_train/rejected': '-0.068733', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0043991', 'logps_train/rejected': '-150.95', 'logps_train/chosen': '-147.9', 'loss/train': '0.70989', 'examples_per_second': '32.995', 'grad_norm': '104', 'counters/examples': 4352, 'counters/updates': 136}
skipping logging after 4384 examples to avoid logging too frequently
train stats after 4416 examples: {'rewards_train/chosen': '0.0031586', 'rewards_train/rejected': '0.043214', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040055', 'logps_train/rejected': '-145.55', 'logps_train/chosen': '-147.64', 'loss/train': '0.72347', 'examples_per_second': '30.186', 'grad_norm': '100', 'counters/examples': 4416, 'counters/updates': 138}
train stats after 4448 examples: {'rewards_train/chosen': '-0.044968', 'rewards_train/rejected': '-0.047935', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0029668', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-161.64', 'loss/train': '0.69853', 'examples_per_second': '32.838', 'grad_norm': '93', 'counters/examples': 4448, 'counters/updates': 139}
skipping logging after 4480 examples to avoid logging too frequently
train stats after 4512 examples: {'rewards_train/chosen': '-0.093919', 'rewards_train/rejected': '0.0093665', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10329', 'logps_train/rejected': '-111.89', 'logps_train/chosen': '-171.69', 'loss/train': '0.75939', 'examples_per_second': '32.705', 'grad_norm': '108.5', 'counters/examples': 4512, 'counters/updates': 141}
train stats after 4544 examples: {'rewards_train/chosen': '0.011295', 'rewards_train/rejected': '-0.034121', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045415', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-124.03', 'loss/train': '0.68246', 'examples_per_second': '30.78', 'grad_norm': '93.5', 'counters/examples': 4544, 'counters/updates': 142}
train stats after 4576 examples: {'rewards_train/chosen': '-0.10058', 'rewards_train/rejected': '0.016119', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.1167', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-159.38', 'loss/train': '0.77422', 'examples_per_second': '31.795', 'grad_norm': '129', 'counters/examples': 4576, 'counters/updates': 143}
train stats after 4608 examples: {'rewards_train/chosen': '-0.026991', 'rewards_train/rejected': '-0.037428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010436', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-149.75', 'loss/train': '0.69805', 'examples_per_second': '31.615', 'grad_norm': '264', 'counters/examples': 4608, 'counters/updates': 144}
train stats after 4640 examples: {'rewards_train/chosen': '-0.036806', 'rewards_train/rejected': '-0.093552', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056745', 'logps_train/rejected': '-169.63', 'logps_train/chosen': '-184.63', 'loss/train': '0.68062', 'examples_per_second': '30.611', 'grad_norm': '148', 'counters/examples': 4640, 'counters/updates': 145}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4704 examples: {'rewards_train/chosen': '0.041864', 'rewards_train/rejected': '-0.036777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07864', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-173.32', 'loss/train': '0.66821', 'examples_per_second': '31.696', 'grad_norm': '62.25', 'counters/examples': 4704, 'counters/updates': 147}
train stats after 4736 examples: {'rewards_train/chosen': '-0.03575', 'rewards_train/rejected': '0.0024646', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.038215', 'logps_train/rejected': '-149.05', 'logps_train/chosen': '-171.35', 'loss/train': '0.72767', 'examples_per_second': '30.744', 'grad_norm': '114', 'counters/examples': 4736, 'counters/updates': 148}
skipping logging after 4768 examples to avoid logging too frequently
train stats after 4800 examples: {'rewards_train/chosen': '0.073958', 'rewards_train/rejected': '0.020857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053101', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-119.29', 'loss/train': '0.67921', 'examples_per_second': '31.689', 'grad_norm': '144', 'counters/examples': 4800, 'counters/updates': 150}
train stats after 4832 examples: {'rewards_train/chosen': '-0.019011', 'rewards_train/rejected': '0.015218', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034228', 'logps_train/rejected': '-129', 'logps_train/chosen': '-177.75', 'loss/train': '0.72174', 'examples_per_second': '32.172', 'grad_norm': '86.5', 'counters/examples': 4832, 'counters/updates': 151}
train stats after 4864 examples: {'rewards_train/chosen': '0.074866', 'rewards_train/rejected': '0.012647', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062219', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-163.7', 'loss/train': '0.67505', 'examples_per_second': '30.265', 'grad_norm': '67.5', 'counters/examples': 4864, 'counters/updates': 152}
train stats after 4896 examples: {'rewards_train/chosen': '-0.030614', 'rewards_train/rejected': '-0.076339', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.045725', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-154.25', 'loss/train': '0.69802', 'examples_per_second': '31.514', 'grad_norm': '127', 'counters/examples': 4896, 'counters/updates': 153}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4960 examples: {'rewards_train/chosen': '-0.11182', 'rewards_train/rejected': '0.017006', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.12882', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-175', 'loss/train': '0.77215', 'examples_per_second': '31.611', 'grad_norm': '114', 'counters/examples': 4960, 'counters/updates': 155}
train stats after 4992 examples: {'rewards_train/chosen': '-0.02913', 'rewards_train/rejected': '0.013732', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.042862', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-165.51', 'loss/train': '0.72588', 'examples_per_second': '30.72', 'grad_norm': '69', 'counters/examples': 4992, 'counters/updates': 156}
train stats after 5024 examples: {'rewards_train/chosen': '-0.014067', 'rewards_train/rejected': '-0.11778', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10371', 'logps_train/rejected': '-161.08', 'logps_train/chosen': '-131.88', 'loss/train': '0.66523', 'examples_per_second': '31.56', 'grad_norm': '116', 'counters/examples': 5024, 'counters/updates': 157}
train stats after 5056 examples: {'rewards_train/chosen': '-0.14307', 'rewards_train/rejected': '0.0017932', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.14486', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-139.17', 'loss/train': '0.77845', 'examples_per_second': '31.624', 'grad_norm': '81', 'counters/examples': 5056, 'counters/updates': 158}
skipping logging after 5088 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '0.0042795', 'rewards_train/rejected': '-0.079779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084059', 'logps_train/rejected': '-143.24', 'logps_train/chosen': '-127.74', 'loss/train': '0.65856', 'examples_per_second': '31.536', 'grad_norm': '79.5', 'counters/examples': 5120, 'counters/updates': 160}
train stats after 5152 examples: {'rewards_train/chosen': '0.013417', 'rewards_train/rejected': '-0.0087998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022216', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-195.56', 'loss/train': '0.69387', 'examples_per_second': '30.577', 'grad_norm': '82.5', 'counters/examples': 5152, 'counters/updates': 161}
train stats after 5184 examples: {'rewards_train/chosen': '0.023723', 'rewards_train/rejected': '-0.090675', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1144', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-141.79', 'loss/train': '0.64771', 'examples_per_second': '31.364', 'grad_norm': '69', 'counters/examples': 5184, 'counters/updates': 162}
train stats after 5216 examples: {'rewards_train/chosen': '-0.032465', 'rewards_train/rejected': '0.0076507', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.040115', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-157.47', 'loss/train': '0.72259', 'examples_per_second': '31.276', 'grad_norm': '88', 'counters/examples': 5216, 'counters/updates': 163}
train stats after 5248 examples: {'rewards_train/chosen': '0.031417', 'rewards_train/rejected': '-0.11169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1431', 'logps_train/rejected': '-129.56', 'logps_train/chosen': '-120.35', 'loss/train': '0.64045', 'examples_per_second': '31.494', 'grad_norm': '71.5', 'counters/examples': 5248, 'counters/updates': 164}
train stats after 5280 examples: {'rewards_train/chosen': '-0.042569', 'rewards_train/rejected': '-0.081189', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038619', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-159.97', 'loss/train': '0.69657', 'examples_per_second': '31.579', 'grad_norm': '84', 'counters/examples': 5280, 'counters/updates': 165}
train stats after 5312 examples: {'rewards_train/chosen': '-0.077543', 'rewards_train/rejected': '0.0058163', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.083359', 'logps_train/rejected': '-138.94', 'logps_train/chosen': '-140.88', 'loss/train': '0.74339', 'examples_per_second': '30.571', 'grad_norm': '146', 'counters/examples': 5312, 'counters/updates': 166}
train stats after 5344 examples: {'rewards_train/chosen': '-0.022207', 'rewards_train/rejected': '0.031151', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053359', 'logps_train/rejected': '-137.38', 'logps_train/chosen': '-203', 'loss/train': '0.73662', 'examples_per_second': '31.37', 'grad_norm': '135', 'counters/examples': 5344, 'counters/updates': 167}
train stats after 5376 examples: {'rewards_train/chosen': '0.04396', 'rewards_train/rejected': '0.0035449', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040415', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-150.35', 'loss/train': '0.68951', 'examples_per_second': '31.434', 'grad_norm': '82', 'counters/examples': 5376, 'counters/updates': 168}
train stats after 5408 examples: {'rewards_train/chosen': '-0.033455', 'rewards_train/rejected': '0.047511', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.080967', 'logps_train/rejected': '-162.52', 'logps_train/chosen': '-136.52', 'loss/train': '0.75427', 'examples_per_second': '31.723', 'grad_norm': '89', 'counters/examples': 5408, 'counters/updates': 169}
train stats after 5440 examples: {'rewards_train/chosen': '0.027533', 'rewards_train/rejected': '-0.003481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031014', 'logps_train/rejected': '-104.86', 'logps_train/chosen': '-128.49', 'loss/train': '0.68685', 'examples_per_second': '31.885', 'grad_norm': '64.5', 'counters/examples': 5440, 'counters/updates': 170}
train stats after 5472 examples: {'rewards_train/chosen': '-0.16264', 'rewards_train/rejected': '-0.11346', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.049178', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-178.54', 'loss/train': '0.73497', 'examples_per_second': '32.689', 'grad_norm': '91', 'counters/examples': 5472, 'counters/updates': 171}
skipping logging after 5504 examples to avoid logging too frequently
train stats after 5536 examples: {'rewards_train/chosen': '-0.037488', 'rewards_train/rejected': '-0.07367', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036181', 'logps_train/rejected': '-105.34', 'logps_train/chosen': '-100.13', 'loss/train': '0.68809', 'examples_per_second': '31.576', 'grad_norm': '113.5', 'counters/examples': 5536, 'counters/updates': 173}
train stats after 5568 examples: {'rewards_train/chosen': '0.027481', 'rewards_train/rejected': '0.071476', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.043995', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-153.52', 'loss/train': '0.72498', 'examples_per_second': '30.37', 'grad_norm': '70.5', 'counters/examples': 5568, 'counters/updates': 174}
skipping logging after 5600 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '0.018496', 'rewards_train/rejected': '-0.036783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055279', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-116.02', 'loss/train': '0.67322', 'examples_per_second': '35.902', 'grad_norm': '85', 'counters/examples': 5632, 'counters/updates': 176}
train stats after 5664 examples: {'rewards_train/chosen': '-0.080431', 'rewards_train/rejected': '-0.0022455', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.078186', 'logps_train/rejected': '-181.33', 'logps_train/chosen': '-139.57', 'loss/train': '0.75109', 'examples_per_second': '30.699', 'grad_norm': '148', 'counters/examples': 5664, 'counters/updates': 177}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5728 examples: {'rewards_train/chosen': '-0.0026017', 'rewards_train/rejected': '0.024614', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027216', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-169.23', 'loss/train': '0.72444', 'examples_per_second': '32.749', 'grad_norm': '244', 'counters/examples': 5728, 'counters/updates': 179}
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5792 examples: {'rewards_train/chosen': '0.003861', 'rewards_train/rejected': '-0.028832', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032693', 'logps_train/rejected': '-148.74', 'logps_train/chosen': '-190.72', 'loss/train': '0.69547', 'examples_per_second': '31.466', 'grad_norm': '107', 'counters/examples': 5792, 'counters/updates': 181}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5856 examples: {'rewards_train/chosen': '-0.07882', 'rewards_train/rejected': '0.052941', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.13176', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-146.85', 'loss/train': '0.78935', 'examples_per_second': '31.312', 'grad_norm': '146', 'counters/examples': 5856, 'counters/updates': 183}
train stats after 5888 examples: {'rewards_train/chosen': '0.0067627', 'rewards_train/rejected': '-0.017071', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023834', 'logps_train/rejected': '-107.62', 'logps_train/chosen': '-145.2', 'loss/train': '0.69152', 'examples_per_second': '31.585', 'grad_norm': '98', 'counters/examples': 5888, 'counters/updates': 184}
skipping logging after 5920 examples to avoid logging too frequently
train stats after 5952 examples: {'rewards_train/chosen': '-0.028747', 'rewards_train/rejected': '-0.025354', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0033935', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-124.92', 'loss/train': '0.70748', 'examples_per_second': '32.196', 'grad_norm': '79.5', 'counters/examples': 5952, 'counters/updates': 186}
train stats after 5984 examples: {'rewards_train/chosen': '-0.0039399', 'rewards_train/rejected': '-0.041503', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037563', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-112.93', 'loss/train': '0.68724', 'examples_per_second': '30.78', 'grad_norm': '107', 'counters/examples': 5984, 'counters/updates': 187}
train stats after 6016 examples: {'rewards_train/chosen': '0.020681', 'rewards_train/rejected': '0.022846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0021647', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-175.52', 'loss/train': '0.71395', 'examples_per_second': '31.143', 'grad_norm': '79', 'counters/examples': 6016, 'counters/updates': 188}
train stats after 6048 examples: {'rewards_train/chosen': '-0.13726', 'rewards_train/rejected': '-0.060986', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.076276', 'logps_train/rejected': '-145.03', 'logps_train/chosen': '-173.86', 'loss/train': '0.74208', 'examples_per_second': '31.703', 'grad_norm': '78.5', 'counters/examples': 6048, 'counters/updates': 189}
train stats after 6080 examples: {'rewards_train/chosen': '0.063245', 'rewards_train/rejected': '0.023575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03967', 'logps_train/rejected': '-156.56', 'logps_train/chosen': '-152.87', 'loss/train': '0.68625', 'examples_per_second': '31.573', 'grad_norm': '63', 'counters/examples': 6080, 'counters/updates': 190}
skipping logging after 6112 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '0.019066', 'rewards_train/rejected': '0.0050544', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014012', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-130.33', 'loss/train': '0.69418', 'examples_per_second': '31.577', 'grad_norm': '80', 'counters/examples': 6144, 'counters/updates': 192}
skipping logging after 6176 examples to avoid logging too frequently
train stats after 6208 examples: {'rewards_train/chosen': '0.014271', 'rewards_train/rejected': '-0.034461', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048731', 'logps_train/rejected': '-147.33', 'logps_train/chosen': '-173.9', 'loss/train': '0.68348', 'examples_per_second': '32.025', 'grad_norm': '84', 'counters/examples': 6208, 'counters/updates': 194}
skipping logging after 6240 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '0.044941', 'rewards_train/rejected': '-0.067969', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11291', 'logps_train/rejected': '-92.084', 'logps_train/chosen': '-157.65', 'loss/train': '0.65545', 'examples_per_second': '31.467', 'grad_norm': '93.5', 'counters/examples': 6272, 'counters/updates': 196}
train stats after 6304 examples: {'rewards_train/chosen': '-0.012664', 'rewards_train/rejected': '0.060117', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.072781', 'logps_train/rejected': '-147.19', 'logps_train/chosen': '-172.19', 'loss/train': '0.74142', 'examples_per_second': '31.537', 'grad_norm': '234', 'counters/examples': 6304, 'counters/updates': 197}
train stats after 6336 examples: {'rewards_train/chosen': '0.0081339', 'rewards_train/rejected': '-0.028972', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037106', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-149.43', 'loss/train': '0.69616', 'examples_per_second': '31.649', 'grad_norm': '103', 'counters/examples': 6336, 'counters/updates': 198}
train stats after 6368 examples: {'rewards_train/chosen': '0.1243', 'rewards_train/rejected': '-0.0014288', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.12573', 'logps_train/rejected': '-131.21', 'logps_train/chosen': '-184.41', 'loss/train': '0.65921', 'examples_per_second': '31.028', 'grad_norm': '89.5', 'counters/examples': 6368, 'counters/updates': 199}
train stats after 6400 examples: {'rewards_train/chosen': '-0.0274', 'rewards_train/rejected': '0.020182', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.047582', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-169.74', 'loss/train': '0.72849', 'examples_per_second': '32.524', 'grad_norm': '107', 'counters/examples': 6400, 'counters/updates': 200}
train stats after 6432 examples: {'rewards_train/chosen': '-0.0067245', 'rewards_train/rejected': '-0.01858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011855', 'logps_train/rejected': '-131.12', 'logps_train/chosen': '-166.66', 'loss/train': '0.69972', 'examples_per_second': '31.08', 'grad_norm': '65.5', 'counters/examples': 6432, 'counters/updates': 201}
train stats after 6464 examples: {'rewards_train/chosen': '-0.021135', 'rewards_train/rejected': '-0.010974', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010161', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-134.34', 'loss/train': '0.70922', 'examples_per_second': '32.97', 'grad_norm': '65', 'counters/examples': 6464, 'counters/updates': 202}
train stats after 6496 examples: {'rewards_train/chosen': '-0.020975', 'rewards_train/rejected': '0.0098763', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030852', 'logps_train/rejected': '-125.24', 'logps_train/chosen': '-146.57', 'loss/train': '0.73245', 'examples_per_second': '30.02', 'grad_norm': '100', 'counters/examples': 6496, 'counters/updates': 203}
train stats after 6528 examples: {'rewards_train/chosen': '0.033816', 'rewards_train/rejected': '-0.038402', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072218', 'logps_train/rejected': '-115.67', 'logps_train/chosen': '-115.66', 'loss/train': '0.66989', 'examples_per_second': '31.431', 'grad_norm': '101', 'counters/examples': 6528, 'counters/updates': 204}
train stats after 6560 examples: {'rewards_train/chosen': '-0.070481', 'rewards_train/rejected': '-0.064342', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.0061387', 'logps_train/rejected': '-124.45', 'logps_train/chosen': '-125.49', 'loss/train': '0.70518', 'examples_per_second': '25.127', 'grad_norm': '56.25', 'counters/examples': 6560, 'counters/updates': 205}
train stats after 6592 examples: {'rewards_train/chosen': '0.0098836', 'rewards_train/rejected': '-0.011387', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02127', 'logps_train/rejected': '-196.53', 'logps_train/chosen': '-170.05', 'loss/train': '0.69633', 'examples_per_second': '31.6', 'grad_norm': '91', 'counters/examples': 6592, 'counters/updates': 206}
skipping logging after 6624 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '-0.074596', 'rewards_train/rejected': '-0.059941', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014655', 'logps_train/rejected': '-116.61', 'logps_train/chosen': '-146.57', 'loss/train': '0.71855', 'examples_per_second': '25.296', 'grad_norm': '140', 'counters/examples': 6656, 'counters/updates': 208}
train stats after 6688 examples: {'rewards_train/chosen': '-0.00045726', 'rewards_train/rejected': '0.025685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026142', 'logps_train/rejected': '-146.3', 'logps_train/chosen': '-169.43', 'loss/train': '0.7251', 'examples_per_second': '31.343', 'grad_norm': '64', 'counters/examples': 6688, 'counters/updates': 209}
skipping logging after 6720 examples to avoid logging too frequently
train stats after 6752 examples: {'rewards_train/chosen': '-0.043486', 'rewards_train/rejected': '-0.011838', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031647', 'logps_train/rejected': '-134.98', 'logps_train/chosen': '-159.03', 'loss/train': '0.72486', 'examples_per_second': '30.637', 'grad_norm': '121', 'counters/examples': 6752, 'counters/updates': 211}
train stats after 6784 examples: {'rewards_train/chosen': '0.012968', 'rewards_train/rejected': '0.058396', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.045427', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-163.16', 'loss/train': '0.74341', 'examples_per_second': '30.816', 'grad_norm': '88.5', 'counters/examples': 6784, 'counters/updates': 212}
train stats after 6816 examples: {'rewards_train/chosen': '-0.066896', 'rewards_train/rejected': '-0.10408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037185', 'logps_train/rejected': '-129.14', 'logps_train/chosen': '-159.44', 'loss/train': '0.68829', 'examples_per_second': '31.63', 'grad_norm': '92.5', 'counters/examples': 6816, 'counters/updates': 213}
train stats after 6848 examples: {'rewards_train/chosen': '0.048444', 'rewards_train/rejected': '-0.03295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081394', 'logps_train/rejected': '-169.01', 'logps_train/chosen': '-164.7', 'loss/train': '0.66533', 'examples_per_second': '31.543', 'grad_norm': '117', 'counters/examples': 6848, 'counters/updates': 214}
train stats after 6880 examples: {'rewards_train/chosen': '-0.011373', 'rewards_train/rejected': '-0.059537', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048164', 'logps_train/rejected': '-162.52', 'logps_train/chosen': '-181.49', 'loss/train': '0.68681', 'examples_per_second': '30.09', 'grad_norm': '127', 'counters/examples': 6880, 'counters/updates': 215}
train stats after 6912 examples: {'rewards_train/chosen': '0.024501', 'rewards_train/rejected': '0.064215', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039714', 'logps_train/rejected': '-121.14', 'logps_train/chosen': '-147.06', 'loss/train': '0.73702', 'examples_per_second': '31.253', 'grad_norm': '76', 'counters/examples': 6912, 'counters/updates': 216}
skipping logging after 6944 examples to avoid logging too frequently
train stats after 6976 examples: {'rewards_train/chosen': '0.093733', 'rewards_train/rejected': '0.031828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061905', 'logps_train/rejected': '-139.2', 'logps_train/chosen': '-175.42', 'loss/train': '0.69217', 'examples_per_second': '30.458', 'grad_norm': '117.5', 'counters/examples': 6976, 'counters/updates': 218}
train stats after 7008 examples: {'rewards_train/chosen': '-0.029001', 'rewards_train/rejected': '-0.034897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0058958', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-175.53', 'loss/train': '0.71556', 'examples_per_second': '31.288', 'grad_norm': '165', 'counters/examples': 7008, 'counters/updates': 219}
train stats after 7040 examples: {'rewards_train/chosen': '0.0035515', 'rewards_train/rejected': '0.0228', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019248', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-158.71', 'loss/train': '0.72063', 'examples_per_second': '31.606', 'grad_norm': '97.5', 'counters/examples': 7040, 'counters/updates': 220}
train stats after 7072 examples: {'rewards_train/chosen': '-0.015577', 'rewards_train/rejected': '-0.0033891', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012188', 'logps_train/rejected': '-111.98', 'logps_train/chosen': '-160.52', 'loss/train': '0.72168', 'examples_per_second': '30.596', 'grad_norm': '90.5', 'counters/examples': 7072, 'counters/updates': 221}
train stats after 7104 examples: {'rewards_train/chosen': '0.019705', 'rewards_train/rejected': '0.068929', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049224', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-127.09', 'loss/train': '0.73919', 'examples_per_second': '30.961', 'grad_norm': '138', 'counters/examples': 7104, 'counters/updates': 222}
train stats after 7136 examples: {'rewards_train/chosen': '0.057347', 'rewards_train/rejected': '-0.044949', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1023', 'logps_train/rejected': '-148.03', 'logps_train/chosen': '-183.73', 'loss/train': '0.66117', 'examples_per_second': '24.769', 'grad_norm': '66', 'counters/examples': 7136, 'counters/updates': 223}
train stats after 7168 examples: {'rewards_train/chosen': '0.050105', 'rewards_train/rejected': '0.016521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033585', 'logps_train/rejected': '-113.18', 'logps_train/chosen': '-150.27', 'loss/train': '0.68318', 'examples_per_second': '30.249', 'grad_norm': '125.5', 'counters/examples': 7168, 'counters/updates': 224}
train stats after 7200 examples: {'rewards_train/chosen': '-0.01272', 'rewards_train/rejected': '0.022391', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035111', 'logps_train/rejected': '-97.487', 'logps_train/chosen': '-154.6', 'loss/train': '0.73062', 'examples_per_second': '32.736', 'grad_norm': '90', 'counters/examples': 7200, 'counters/updates': 225}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7264 examples: {'rewards_train/chosen': '0.012196', 'rewards_train/rejected': '0.048598', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.036402', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-164.55', 'loss/train': '0.728', 'examples_per_second': '31.64', 'grad_norm': '126.5', 'counters/examples': 7264, 'counters/updates': 227}
train stats after 7296 examples: {'rewards_train/chosen': '0.010342', 'rewards_train/rejected': '-0.038916', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049259', 'logps_train/rejected': '-149.5', 'logps_train/chosen': '-167.56', 'loss/train': '0.68138', 'examples_per_second': '32.044', 'grad_norm': '190', 'counters/examples': 7296, 'counters/updates': 228}
skipping logging after 7328 examples to avoid logging too frequently
train stats after 7360 examples: {'rewards_train/chosen': '-0.027882', 'rewards_train/rejected': '-0.040869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012987', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-117.63', 'loss/train': '0.69235', 'examples_per_second': '31.933', 'grad_norm': '65.5', 'counters/examples': 7360, 'counters/updates': 230}
skipping logging after 7392 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '0.034078', 'rewards_train/rejected': '-0.031715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065793', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-144.54', 'loss/train': '0.66879', 'examples_per_second': '31.611', 'grad_norm': '65', 'counters/examples': 7424, 'counters/updates': 232}
train stats after 7456 examples: {'rewards_train/chosen': '0.013367', 'rewards_train/rejected': '0.048019', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034652', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-162.16', 'loss/train': '0.72657', 'examples_per_second': '32.686', 'grad_norm': '154', 'counters/examples': 7456, 'counters/updates': 233}
train stats after 7488 examples: {'rewards_train/chosen': '-0.052455', 'rewards_train/rejected': '-0.062766', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010311', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-186.92', 'loss/train': '0.71235', 'examples_per_second': '30.116', 'grad_norm': '108.5', 'counters/examples': 7488, 'counters/updates': 234}
train stats after 7520 examples: {'rewards_train/chosen': '0.032373', 'rewards_train/rejected': '0.078779', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046406', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-114.21', 'loss/train': '0.72733', 'examples_per_second': '30.572', 'grad_norm': '84.5', 'counters/examples': 7520, 'counters/updates': 235}
train stats after 7552 examples: {'rewards_train/chosen': '-0.0062789', 'rewards_train/rejected': '-0.064342', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058063', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-158.16', 'loss/train': '0.67115', 'examples_per_second': '32.461', 'grad_norm': '82', 'counters/examples': 7552, 'counters/updates': 236}
train stats after 7584 examples: {'rewards_train/chosen': '-0.078878', 'rewards_train/rejected': '-0.038731', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040147', 'logps_train/rejected': '-158.96', 'logps_train/chosen': '-144.28', 'loss/train': '0.72882', 'examples_per_second': '31.614', 'grad_norm': '78', 'counters/examples': 7584, 'counters/updates': 237}
train stats after 7616 examples: {'rewards_train/chosen': '-0.032688', 'rewards_train/rejected': '-0.010331', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022357', 'logps_train/rejected': '-102.4', 'logps_train/chosen': '-136.54', 'loss/train': '0.71522', 'examples_per_second': '32.68', 'grad_norm': '67', 'counters/examples': 7616, 'counters/updates': 238}
train stats after 7648 examples: {'rewards_train/chosen': '-0.039284', 'rewards_train/rejected': '-0.041211', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0019276', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-157.68', 'loss/train': '0.7036', 'examples_per_second': '32.741', 'grad_norm': '122.5', 'counters/examples': 7648, 'counters/updates': 239}
skipping logging after 7680 examples to avoid logging too frequently
train stats after 7712 examples: {'rewards_train/chosen': '0.0032775', 'rewards_train/rejected': '0.078545', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.075267', 'logps_train/rejected': '-93.951', 'logps_train/chosen': '-148.16', 'loss/train': '0.74054', 'examples_per_second': '34.36', 'grad_norm': '103.5', 'counters/examples': 7712, 'counters/updates': 241}
train stats after 7744 examples: {'rewards_train/chosen': '0.011836', 'rewards_train/rejected': '-0.043586', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055422', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-183.68', 'loss/train': '0.67777', 'examples_per_second': '30.62', 'grad_norm': '101.5', 'counters/examples': 7744, 'counters/updates': 242}
skipping logging after 7776 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '0.10216', 'rewards_train/rejected': '0.063113', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.039046', 'logps_train/rejected': '-115.34', 'logps_train/chosen': '-178.28', 'loss/train': '0.70115', 'examples_per_second': '31.744', 'grad_norm': '69.5', 'counters/examples': 7808, 'counters/updates': 244}
train stats after 7840 examples: {'rewards_train/chosen': '-0.071491', 'rewards_train/rejected': '-0.0088152', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.062676', 'logps_train/rejected': '-166.2', 'logps_train/chosen': '-170.03', 'loss/train': '0.73658', 'examples_per_second': '31.695', 'grad_norm': '100.5', 'counters/examples': 7840, 'counters/updates': 245}
train stats after 7872 examples: {'rewards_train/chosen': '0.039631', 'rewards_train/rejected': '0.06122', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021589', 'logps_train/rejected': '-121.1', 'logps_train/chosen': '-132.3', 'loss/train': '0.71292', 'examples_per_second': '30.639', 'grad_norm': '50', 'counters/examples': 7872, 'counters/updates': 246}
train stats after 7904 examples: {'rewards_train/chosen': '0.031815', 'rewards_train/rejected': '-0.021847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053662', 'logps_train/rejected': '-117.31', 'logps_train/chosen': '-110.31', 'loss/train': '0.67244', 'examples_per_second': '30.222', 'grad_norm': '104.5', 'counters/examples': 7904, 'counters/updates': 247}
skipping logging after 7936 examples to avoid logging too frequently
train stats after 7968 examples: {'rewards_train/chosen': '-0.033752', 'rewards_train/rejected': '0.013131', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046884', 'logps_train/rejected': '-124.22', 'logps_train/chosen': '-153.37', 'loss/train': '0.72814', 'examples_per_second': '33.214', 'grad_norm': '95', 'counters/examples': 7968, 'counters/updates': 249}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8032 examples: {'rewards_train/chosen': '-0.009597', 'rewards_train/rejected': '-0.069335', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059738', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-158.23', 'loss/train': '0.68299', 'examples_per_second': '31.188', 'grad_norm': '132', 'counters/examples': 8032, 'counters/updates': 251}
train stats after 8064 examples: {'rewards_train/chosen': '0.025508', 'rewards_train/rejected': '-0.013748', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039256', 'logps_train/rejected': '-150.78', 'logps_train/chosen': '-174.72', 'loss/train': '0.70644', 'examples_per_second': '30.645', 'grad_norm': '107.5', 'counters/examples': 8064, 'counters/updates': 252}
skipping logging after 8096 examples to avoid logging too frequently
train stats after 8128 examples: {'rewards_train/chosen': '-0.033799', 'rewards_train/rejected': '0.027515', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.061314', 'logps_train/rejected': '-159.24', 'logps_train/chosen': '-164.05', 'loss/train': '0.74822', 'examples_per_second': '31.291', 'grad_norm': '89', 'counters/examples': 8128, 'counters/updates': 254}
train stats after 8160 examples: {'rewards_train/chosen': '9.4077e-05', 'rewards_train/rejected': '0.0025452', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024512', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-163.26', 'loss/train': '0.70362', 'examples_per_second': '30.166', 'grad_norm': '94.5', 'counters/examples': 8160, 'counters/updates': 255}
train stats after 8192 examples: {'rewards_train/chosen': '-0.11023', 'rewards_train/rejected': '-0.084123', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.026105', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-201.49', 'loss/train': '0.71907', 'examples_per_second': '30.372', 'grad_norm': '137', 'counters/examples': 8192, 'counters/updates': 256}
train stats after 8224 examples: {'rewards_train/chosen': '0.00094628', 'rewards_train/rejected': '-0.0056996', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0066459', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-148.63', 'loss/train': '0.69985', 'examples_per_second': '32.285', 'grad_norm': '79.5', 'counters/examples': 8224, 'counters/updates': 257}
train stats after 8256 examples: {'rewards_train/chosen': '-0.00074804', 'rewards_train/rejected': '-0.033848', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0331', 'logps_train/rejected': '-167.32', 'logps_train/chosen': '-163.26', 'loss/train': '0.70105', 'examples_per_second': '30.798', 'grad_norm': '83', 'counters/examples': 8256, 'counters/updates': 258}
train stats after 8288 examples: {'rewards_train/chosen': '0.088056', 'rewards_train/rejected': '0.02821', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059846', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-149.11', 'loss/train': '0.67495', 'examples_per_second': '32.218', 'grad_norm': '77', 'counters/examples': 8288, 'counters/updates': 259}
train stats after 8320 examples: {'rewards_train/chosen': '0.015062', 'rewards_train/rejected': '0.036671', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02161', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-142.24', 'loss/train': '0.71884', 'examples_per_second': '32.217', 'grad_norm': '86', 'counters/examples': 8320, 'counters/updates': 260}
skipping logging after 8352 examples to avoid logging too frequently
train stats after 8384 examples: {'rewards_train/chosen': '0.046725', 'rewards_train/rejected': '0.02893', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017795', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-154.12', 'loss/train': '0.69597', 'examples_per_second': '30.925', 'grad_norm': '90.5', 'counters/examples': 8384, 'counters/updates': 262}
train stats after 8416 examples: {'rewards_train/chosen': '-0.042532', 'rewards_train/rejected': '-0.074907', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032376', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-131.31', 'loss/train': '0.70753', 'examples_per_second': '31.572', 'grad_norm': '92.5', 'counters/examples': 8416, 'counters/updates': 263}
train stats after 8448 examples: {'rewards_train/chosen': '-0.12477', 'rewards_train/rejected': '-0.082494', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042273', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-191.4', 'loss/train': '0.72799', 'examples_per_second': '30.105', 'grad_norm': '97', 'counters/examples': 8448, 'counters/updates': 264}
train stats after 8480 examples: {'rewards_train/chosen': '-0.021072', 'rewards_train/rejected': '-0.090476', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069404', 'logps_train/rejected': '-151.08', 'logps_train/chosen': '-163.21', 'loss/train': '0.67418', 'examples_per_second': '31.557', 'grad_norm': '116.5', 'counters/examples': 8480, 'counters/updates': 265}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8544 examples: {'rewards_train/chosen': '0.0083365', 'rewards_train/rejected': '-0.020636', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028973', 'logps_train/rejected': '-149.11', 'logps_train/chosen': '-172.04', 'loss/train': '0.69037', 'examples_per_second': '30.538', 'grad_norm': '89', 'counters/examples': 8544, 'counters/updates': 267}
train stats after 8576 examples: {'rewards_train/chosen': '0.011962', 'rewards_train/rejected': '-0.04179', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053753', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-156.83', 'loss/train': '0.67229', 'examples_per_second': '31.537', 'grad_norm': '64', 'counters/examples': 8576, 'counters/updates': 268}
train stats after 8608 examples: {'rewards_train/chosen': '-0.028885', 'rewards_train/rejected': '-0.048587', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019702', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-139.55', 'loss/train': '0.69486', 'examples_per_second': '32.002', 'grad_norm': '72.5', 'counters/examples': 8608, 'counters/updates': 269}
train stats after 8640 examples: {'rewards_train/chosen': '0.03829', 'rewards_train/rejected': '0.020235', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018055', 'logps_train/rejected': '-102.72', 'logps_train/chosen': '-142.23', 'loss/train': '0.695', 'examples_per_second': '31.366', 'grad_norm': '79.5', 'counters/examples': 8640, 'counters/updates': 270}
train stats after 8672 examples: {'rewards_train/chosen': '-0.033083', 'rewards_train/rejected': '0.03024', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.063323', 'logps_train/rejected': '-143.04', 'logps_train/chosen': '-154.01', 'loss/train': '0.74072', 'examples_per_second': '31.607', 'grad_norm': '132', 'counters/examples': 8672, 'counters/updates': 271}
train stats after 8704 examples: {'rewards_train/chosen': '-0.040673', 'rewards_train/rejected': '0.022874', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.063547', 'logps_train/rejected': '-108.94', 'logps_train/chosen': '-118.01', 'loss/train': '0.73202', 'examples_per_second': '31.975', 'grad_norm': '76', 'counters/examples': 8704, 'counters/updates': 272}
skipping logging after 8736 examples to avoid logging too frequently
train stats after 8768 examples: {'rewards_train/chosen': '-0.06152', 'rewards_train/rejected': '-0.01611', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.045409', 'logps_train/rejected': '-153.2', 'logps_train/chosen': '-143.64', 'loss/train': '0.72698', 'examples_per_second': '30.566', 'grad_norm': '104', 'counters/examples': 8768, 'counters/updates': 274}
train stats after 8800 examples: {'rewards_train/chosen': '0.07761', 'rewards_train/rejected': '0.0014294', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076181', 'logps_train/rejected': '-153.62', 'logps_train/chosen': '-147.48', 'loss/train': '0.67585', 'examples_per_second': '31.559', 'grad_norm': '74.5', 'counters/examples': 8800, 'counters/updates': 275}
train stats after 8832 examples: {'rewards_train/chosen': '-0.0069018', 'rewards_train/rejected': '-0.022099', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.015197', 'logps_train/rejected': '-145.61', 'logps_train/chosen': '-169.7', 'loss/train': '0.7009', 'examples_per_second': '32.744', 'grad_norm': '61.25', 'counters/examples': 8832, 'counters/updates': 276}
skipping logging after 8864 examples to avoid logging too frequently
train stats after 8896 examples: {'rewards_train/chosen': '0.0017784', 'rewards_train/rejected': '0.0011406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00063771', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-165.16', 'loss/train': '0.70067', 'examples_per_second': '31.547', 'grad_norm': '157', 'counters/examples': 8896, 'counters/updates': 278}
train stats after 8928 examples: {'rewards_train/chosen': '0.039565', 'rewards_train/rejected': '0.0011478', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038417', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-147.66', 'loss/train': '0.68449', 'examples_per_second': '32.686', 'grad_norm': '80.5', 'counters/examples': 8928, 'counters/updates': 279}
train stats after 8960 examples: {'rewards_train/chosen': '0.07122', 'rewards_train/rejected': '-0.051007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12223', 'logps_train/rejected': '-107.64', 'logps_train/chosen': '-130.53', 'loss/train': '0.6409', 'examples_per_second': '30.559', 'grad_norm': '100.5', 'counters/examples': 8960, 'counters/updates': 280}
train stats after 8992 examples: {'rewards_train/chosen': '-0.028198', 'rewards_train/rejected': '-0.041272', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013074', 'logps_train/rejected': '-135.95', 'logps_train/chosen': '-160.99', 'loss/train': '0.70666', 'examples_per_second': '30.126', 'grad_norm': '92', 'counters/examples': 8992, 'counters/updates': 281}
train stats after 9024 examples: {'rewards_train/chosen': '-0.011359', 'rewards_train/rejected': '-0.092858', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081499', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-166.97', 'loss/train': '0.66991', 'examples_per_second': '30.634', 'grad_norm': '93', 'counters/examples': 9024, 'counters/updates': 282}
skipping logging after 9056 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '0.067192', 'rewards_train/rejected': '-0.050831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11802', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-131.26', 'loss/train': '0.64635', 'examples_per_second': '31.355', 'grad_norm': '114', 'counters/examples': 9088, 'counters/updates': 284}
train stats after 9120 examples: {'rewards_train/chosen': '0.013141', 'rewards_train/rejected': '0.0033196', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0098209', 'logps_train/rejected': '-109.33', 'logps_train/chosen': '-158.02', 'loss/train': '0.70257', 'examples_per_second': '31.345', 'grad_norm': '61.75', 'counters/examples': 9120, 'counters/updates': 285}
train stats after 9152 examples: {'rewards_train/chosen': '0.014483', 'rewards_train/rejected': '-0.0054276', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01991', 'logps_train/rejected': '-134.63', 'logps_train/chosen': '-136.04', 'loss/train': '0.69566', 'examples_per_second': '30.717', 'grad_norm': '102.5', 'counters/examples': 9152, 'counters/updates': 286}
train stats after 9184 examples: {'rewards_train/chosen': '0.045254', 'rewards_train/rejected': '0.054146', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0088927', 'logps_train/rejected': '-154.02', 'logps_train/chosen': '-160.87', 'loss/train': '0.72666', 'examples_per_second': '31.607', 'grad_norm': '266', 'counters/examples': 9184, 'counters/updates': 287}
train stats after 9216 examples: {'rewards_train/chosen': '-0.0021381', 'rewards_train/rejected': '0.031475', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033613', 'logps_train/rejected': '-142.04', 'logps_train/chosen': '-171.14', 'loss/train': '0.72454', 'examples_per_second': '31.482', 'grad_norm': '274', 'counters/examples': 9216, 'counters/updates': 288}
train stats after 9248 examples: {'rewards_train/chosen': '-0.057071', 'rewards_train/rejected': '-0.080989', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023918', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-194.57', 'loss/train': '0.69996', 'examples_per_second': '31.163', 'grad_norm': '89.5', 'counters/examples': 9248, 'counters/updates': 289}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9312 examples: {'rewards_train/chosen': '-0.039279', 'rewards_train/rejected': '-0.0086826', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.030596', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-144.99', 'loss/train': '0.7216', 'examples_per_second': '31.134', 'grad_norm': '87', 'counters/examples': 9312, 'counters/updates': 291}
train stats after 9344 examples: {'rewards_train/chosen': '0.14179', 'rewards_train/rejected': '-0.0263', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16809', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-163.66', 'loss/train': '0.62649', 'examples_per_second': '31.479', 'grad_norm': '153', 'counters/examples': 9344, 'counters/updates': 292}
skipping logging after 9376 examples to avoid logging too frequently
train stats after 9408 examples: {'rewards_train/chosen': '0.011633', 'rewards_train/rejected': '0.013895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0022624', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-135.13', 'loss/train': '0.70686', 'examples_per_second': '31.531', 'grad_norm': '203', 'counters/examples': 9408, 'counters/updates': 294}
train stats after 9440 examples: {'rewards_train/chosen': '-0.093011', 'rewards_train/rejected': '-0.0010946', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.091917', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-135.2', 'loss/train': '0.75234', 'examples_per_second': '30.644', 'grad_norm': '75.5', 'counters/examples': 9440, 'counters/updates': 295}
train stats after 9472 examples: {'rewards_train/chosen': '0.036089', 'rewards_train/rejected': '-0.018607', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054696', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-112.88', 'loss/train': '0.67575', 'examples_per_second': '31.528', 'grad_norm': '113.5', 'counters/examples': 9472, 'counters/updates': 296}
skipping logging after 9504 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': '0.013803', 'rewards_train/rejected': '0.011022', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0027815', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-140.86', 'loss/train': '0.70086', 'examples_per_second': '31.574', 'grad_norm': '108', 'counters/examples': 9536, 'counters/updates': 298}
train stats after 9568 examples: {'rewards_train/chosen': '0.020095', 'rewards_train/rejected': '-0.031283', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051378', 'logps_train/rejected': '-142.93', 'logps_train/chosen': '-117.28', 'loss/train': '0.67294', 'examples_per_second': '31.637', 'grad_norm': '63.75', 'counters/examples': 9568, 'counters/updates': 299}
train stats after 9600 examples: {'rewards_train/chosen': '-0.03632', 'rewards_train/rejected': '-0.060271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023951', 'logps_train/rejected': '-118.96', 'logps_train/chosen': '-183.99', 'loss/train': '0.69132', 'examples_per_second': '31.23', 'grad_norm': '89.5', 'counters/examples': 9600, 'counters/updates': 300}
train stats after 9632 examples: {'rewards_train/chosen': '0.018506', 'rewards_train/rejected': '0.030136', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01163', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-199.57', 'loss/train': '0.72742', 'examples_per_second': '31.475', 'grad_norm': '130', 'counters/examples': 9632, 'counters/updates': 301}
train stats after 9664 examples: {'rewards_train/chosen': '-0.040052', 'rewards_train/rejected': '0.067693', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10775', 'logps_train/rejected': '-139.2', 'logps_train/chosen': '-186.27', 'loss/train': '0.76353', 'examples_per_second': '30.845', 'grad_norm': '164', 'counters/examples': 9664, 'counters/updates': 302}
train stats after 9696 examples: {'rewards_train/chosen': '-0.052196', 'rewards_train/rejected': '-0.030133', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022062', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-150.68', 'loss/train': '0.71616', 'examples_per_second': '30.087', 'grad_norm': '90', 'counters/examples': 9696, 'counters/updates': 303}
train stats after 9728 examples: {'rewards_train/chosen': '0.044875', 'rewards_train/rejected': '0.10112', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056249', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-158.71', 'loss/train': '0.74178', 'examples_per_second': '31.384', 'grad_norm': '108', 'counters/examples': 9728, 'counters/updates': 304}
train stats after 9760 examples: {'rewards_train/chosen': '-0.030391', 'rewards_train/rejected': '-0.075098', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044707', 'logps_train/rejected': '-216.76', 'logps_train/chosen': '-184.3', 'loss/train': '0.69534', 'examples_per_second': '31.733', 'grad_norm': '86.5', 'counters/examples': 9760, 'counters/updates': 305}
train stats after 9792 examples: {'rewards_train/chosen': '-0.019097', 'rewards_train/rejected': '0.035209', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054306', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-176.34', 'loss/train': '0.73675', 'examples_per_second': '30.959', 'grad_norm': '147', 'counters/examples': 9792, 'counters/updates': 306}
train stats after 9824 examples: {'rewards_train/chosen': '-0.048074', 'rewards_train/rejected': '0.032846', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.08092', 'logps_train/rejected': '-94.655', 'logps_train/chosen': '-125.24', 'loss/train': '0.74271', 'examples_per_second': '32.489', 'grad_norm': '58.75', 'counters/examples': 9824, 'counters/updates': 307}
train stats after 9856 examples: {'rewards_train/chosen': '-0.043502', 'rewards_train/rejected': '-0.023597', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019906', 'logps_train/rejected': '-101.69', 'logps_train/chosen': '-116.59', 'loss/train': '0.71119', 'examples_per_second': '31.935', 'grad_norm': '532', 'counters/examples': 9856, 'counters/updates': 308}
train stats after 9888 examples: {'rewards_train/chosen': '-0.0064293', 'rewards_train/rejected': '0.0039844', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010414', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-136.25', 'loss/train': '0.71205', 'examples_per_second': '30.906', 'grad_norm': '50.75', 'counters/examples': 9888, 'counters/updates': 309}
train stats after 9920 examples: {'rewards_train/chosen': '-0.010384', 'rewards_train/rejected': '-0.018156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0077721', 'logps_train/rejected': '-124.07', 'logps_train/chosen': '-109.11', 'loss/train': '0.6931', 'examples_per_second': '32.024', 'grad_norm': '79', 'counters/examples': 9920, 'counters/updates': 310}
train stats after 9952 examples: {'rewards_train/chosen': '0.065819', 'rewards_train/rejected': '0.019372', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046448', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-170.55', 'loss/train': '0.68332', 'examples_per_second': '31.601', 'grad_norm': '58', 'counters/examples': 9952, 'counters/updates': 311}
train stats after 9984 examples: {'rewards_train/chosen': '-0.0024946', 'rewards_train/rejected': '-0.017801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015306', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-122.85', 'loss/train': '0.70135', 'examples_per_second': '30.866', 'grad_norm': '81.5', 'counters/examples': 9984, 'counters/updates': 312}
train stats after 10016 examples: {'rewards_train/chosen': '0.064847', 'rewards_train/rejected': '0.011746', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053101', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-135.13', 'loss/train': '0.67564', 'examples_per_second': '32.134', 'grad_norm': '71', 'counters/examples': 10016, 'counters/updates': 313}
train stats after 10048 examples: {'rewards_train/chosen': '0.067821', 'rewards_train/rejected': '0.0086065', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.059215', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-149.75', 'loss/train': '0.67591', 'examples_per_second': '30.213', 'grad_norm': '138', 'counters/examples': 10048, 'counters/updates': 314}
train stats after 10080 examples: {'rewards_train/chosen': '0.078692', 'rewards_train/rejected': '-0.019166', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097858', 'logps_train/rejected': '-130.87', 'logps_train/chosen': '-181.73', 'loss/train': '0.66377', 'examples_per_second': '31.598', 'grad_norm': '118', 'counters/examples': 10080, 'counters/updates': 315}
train stats after 10112 examples: {'rewards_train/chosen': '-0.031512', 'rewards_train/rejected': '-0.014554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016958', 'logps_train/rejected': '-140.16', 'logps_train/chosen': '-188.78', 'loss/train': '0.71123', 'examples_per_second': '31.523', 'grad_norm': '76', 'counters/examples': 10112, 'counters/updates': 316}
train stats after 10144 examples: {'rewards_train/chosen': '0.049151', 'rewards_train/rejected': '-0.0089814', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058133', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-169.66', 'loss/train': '0.67368', 'examples_per_second': '30.874', 'grad_norm': '57.5', 'counters/examples': 10144, 'counters/updates': 317}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10208 examples: {'rewards_train/chosen': '-0.07612', 'rewards_train/rejected': '0.0032369', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.079357', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-186.71', 'loss/train': '0.75193', 'examples_per_second': '30.592', 'grad_norm': '102.5', 'counters/examples': 10208, 'counters/updates': 319}
train stats after 10240 examples: {'rewards_train/chosen': '-0.06358', 'rewards_train/rejected': '-0.049935', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013645', 'logps_train/rejected': '-118.54', 'logps_train/chosen': '-166.14', 'loss/train': '0.71251', 'examples_per_second': '31.282', 'grad_norm': '65', 'counters/examples': 10240, 'counters/updates': 320}
train stats after 10272 examples: {'rewards_train/chosen': '-0.014963', 'rewards_train/rejected': '-0.021531', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0065679', 'logps_train/rejected': '-153.11', 'logps_train/chosen': '-188.62', 'loss/train': '0.70413', 'examples_per_second': '30.075', 'grad_norm': '71', 'counters/examples': 10272, 'counters/updates': 321}
train stats after 10304 examples: {'rewards_train/chosen': '0.038519', 'rewards_train/rejected': '0.031736', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0067837', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-129.96', 'loss/train': '0.69958', 'examples_per_second': '31.326', 'grad_norm': '82', 'counters/examples': 10304, 'counters/updates': 322}
train stats after 10336 examples: {'rewards_train/chosen': '-0.01771', 'rewards_train/rejected': '-0.030254', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012544', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-180.59', 'loss/train': '0.69674', 'examples_per_second': '31.648', 'grad_norm': '92.5', 'counters/examples': 10336, 'counters/updates': 323}
train stats after 10368 examples: {'rewards_train/chosen': '-0.032663', 'rewards_train/rejected': '-0.00044356', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032219', 'logps_train/rejected': '-101.02', 'logps_train/chosen': '-136.36', 'loss/train': '0.72136', 'examples_per_second': '32.662', 'grad_norm': '72', 'counters/examples': 10368, 'counters/updates': 324}
skipping logging after 10400 examples to avoid logging too frequently
train stats after 10432 examples: {'rewards_train/chosen': '0.053107', 'rewards_train/rejected': '0.0023869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05072', 'logps_train/rejected': '-167.74', 'logps_train/chosen': '-171.24', 'loss/train': '0.68711', 'examples_per_second': '31.65', 'grad_norm': '70.5', 'counters/examples': 10432, 'counters/updates': 326}
train stats after 10464 examples: {'rewards_train/chosen': '0.019513', 'rewards_train/rejected': '-0.0037225', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023236', 'logps_train/rejected': '-148.04', 'logps_train/chosen': '-122.56', 'loss/train': '0.70015', 'examples_per_second': '31.323', 'grad_norm': '124.5', 'counters/examples': 10464, 'counters/updates': 327}
skipping logging after 10496 examples to avoid logging too frequently
train stats after 10528 examples: {'rewards_train/chosen': '-0.010559', 'rewards_train/rejected': '-0.1445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13394', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-166.03', 'loss/train': '0.65032', 'examples_per_second': '31.37', 'grad_norm': '108', 'counters/examples': 10528, 'counters/updates': 329}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10592 examples: {'rewards_train/chosen': '-0.015399', 'rewards_train/rejected': '-0.062465', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047066', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-148.99', 'loss/train': '0.67998', 'examples_per_second': '30.279', 'grad_norm': '134', 'counters/examples': 10592, 'counters/updates': 331}
train stats after 10624 examples: {'rewards_train/chosen': '0.06439', 'rewards_train/rejected': '0.061589', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '0.0028016', 'logps_train/rejected': '-114.82', 'logps_train/chosen': '-126.03', 'loss/train': '0.69895', 'examples_per_second': '31.274', 'grad_norm': '129', 'counters/examples': 10624, 'counters/updates': 332}
train stats after 10656 examples: {'rewards_train/chosen': '-0.011951', 'rewards_train/rejected': '-0.018039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0060884', 'logps_train/rejected': '-98.685', 'logps_train/chosen': '-127.88', 'loss/train': '0.6979', 'examples_per_second': '30.063', 'grad_norm': '54', 'counters/examples': 10656, 'counters/updates': 333}
train stats after 10688 examples: {'rewards_train/chosen': '0.070082', 'rewards_train/rejected': '0.033412', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03667', 'logps_train/rejected': '-140.86', 'logps_train/chosen': '-153.46', 'loss/train': '0.69628', 'examples_per_second': '31.522', 'grad_norm': '116', 'counters/examples': 10688, 'counters/updates': 334}
train stats after 10720 examples: {'rewards_train/chosen': '0.013976', 'rewards_train/rejected': '0.027091', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013115', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-156.89', 'loss/train': '0.72025', 'examples_per_second': '30.048', 'grad_norm': '78', 'counters/examples': 10720, 'counters/updates': 335}
skipping logging after 10752 examples to avoid logging too frequently
train stats after 10784 examples: {'rewards_train/chosen': '0.030572', 'rewards_train/rejected': '0.06096', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030389', 'logps_train/rejected': '-156.67', 'logps_train/chosen': '-184.61', 'loss/train': '0.7224', 'examples_per_second': '31.541', 'grad_norm': '96', 'counters/examples': 10784, 'counters/updates': 337}
train stats after 10816 examples: {'rewards_train/chosen': '0.0090578', 'rewards_train/rejected': '0.0071318', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0019259', 'logps_train/rejected': '-208.22', 'logps_train/chosen': '-164.08', 'loss/train': '0.73434', 'examples_per_second': '30.517', 'grad_norm': '334', 'counters/examples': 10816, 'counters/updates': 338}
train stats after 10848 examples: {'rewards_train/chosen': '-0.0067134', 'rewards_train/rejected': '0.035109', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041823', 'logps_train/rejected': '-182.27', 'logps_train/chosen': '-146.72', 'loss/train': '0.73637', 'examples_per_second': '31.413', 'grad_norm': '226', 'counters/examples': 10848, 'counters/updates': 339}
train stats after 10880 examples: {'rewards_train/chosen': '-0.015846', 'rewards_train/rejected': '0.044115', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059961', 'logps_train/rejected': '-185.2', 'logps_train/chosen': '-200.16', 'loss/train': '0.75209', 'examples_per_second': '30.669', 'grad_norm': '190', 'counters/examples': 10880, 'counters/updates': 340}
train stats after 10912 examples: {'rewards_train/chosen': '-0.055157', 'rewards_train/rejected': '0.012502', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.067659', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-165.85', 'loss/train': '0.75157', 'examples_per_second': '31.552', 'grad_norm': '69', 'counters/examples': 10912, 'counters/updates': 341}
train stats after 10944 examples: {'rewards_train/chosen': '0.049371', 'rewards_train/rejected': '-0.062607', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11198', 'logps_train/rejected': '-131.64', 'logps_train/chosen': '-143.71', 'loss/train': '0.64666', 'examples_per_second': '31.621', 'grad_norm': '60.5', 'counters/examples': 10944, 'counters/updates': 342}
train stats after 10976 examples: {'rewards_train/chosen': '-0.10401', 'rewards_train/rejected': '-0.036272', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.067737', 'logps_train/rejected': '-138.06', 'logps_train/chosen': '-178.76', 'loss/train': '0.74038', 'examples_per_second': '30.647', 'grad_norm': '81', 'counters/examples': 10976, 'counters/updates': 343}
skipping logging after 11008 examples to avoid logging too frequently
train stats after 11040 examples: {'rewards_train/chosen': '0.015152', 'rewards_train/rejected': '-0.023539', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038691', 'logps_train/rejected': '-107.78', 'logps_train/chosen': '-151.85', 'loss/train': '0.69005', 'examples_per_second': '31.116', 'grad_norm': '64', 'counters/examples': 11040, 'counters/updates': 345}
train stats after 11072 examples: {'rewards_train/chosen': '0.0068717', 'rewards_train/rejected': '0.022448', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015576', 'logps_train/rejected': '-169.41', 'logps_train/chosen': '-146.85', 'loss/train': '0.71831', 'examples_per_second': '31.243', 'grad_norm': '80.5', 'counters/examples': 11072, 'counters/updates': 346}
train stats after 11104 examples: {'rewards_train/chosen': '0.011134', 'rewards_train/rejected': '0.0013072', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0098268', 'logps_train/rejected': '-150.55', 'logps_train/chosen': '-156.37', 'loss/train': '0.7076', 'examples_per_second': '31.498', 'grad_norm': '91', 'counters/examples': 11104, 'counters/updates': 347}
train stats after 11136 examples: {'rewards_train/chosen': '0.0042809', 'rewards_train/rejected': '-0.011902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016183', 'logps_train/rejected': '-169.22', 'logps_train/chosen': '-193.76', 'loss/train': '0.69936', 'examples_per_second': '30.85', 'grad_norm': '107', 'counters/examples': 11136, 'counters/updates': 348}
train stats after 11168 examples: {'rewards_train/chosen': '-0.035497', 'rewards_train/rejected': '-0.01767', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017827', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-141.29', 'loss/train': '0.71621', 'examples_per_second': '31.424', 'grad_norm': '47.75', 'counters/examples': 11168, 'counters/updates': 349}
train stats after 11200 examples: {'rewards_train/chosen': '0.012133', 'rewards_train/rejected': '-0.061494', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073627', 'logps_train/rejected': '-159.98', 'logps_train/chosen': '-169.48', 'loss/train': '0.6805', 'examples_per_second': '32.336', 'grad_norm': '141', 'counters/examples': 11200, 'counters/updates': 350}
train stats after 11232 examples: {'rewards_train/chosen': '0.0055324', 'rewards_train/rejected': '-0.085512', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091044', 'logps_train/rejected': '-173.6', 'logps_train/chosen': '-147.51', 'loss/train': '0.66053', 'examples_per_second': '31.084', 'grad_norm': '100', 'counters/examples': 11232, 'counters/updates': 351}
train stats after 11264 examples: {'rewards_train/chosen': '0.10193', 'rewards_train/rejected': '0.020545', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081389', 'logps_train/rejected': '-135.13', 'logps_train/chosen': '-154.11', 'loss/train': '0.66596', 'examples_per_second': '30.082', 'grad_norm': '79.5', 'counters/examples': 11264, 'counters/updates': 352}
skipping logging after 11296 examples to avoid logging too frequently
train stats after 11328 examples: {'rewards_train/chosen': '-0.09342', 'rewards_train/rejected': '-0.0054021', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.088018', 'logps_train/rejected': '-135.5', 'logps_train/chosen': '-159.38', 'loss/train': '0.75825', 'examples_per_second': '30.721', 'grad_norm': '112.5', 'counters/examples': 11328, 'counters/updates': 354}
train stats after 11360 examples: {'rewards_train/chosen': '-0.018833', 'rewards_train/rejected': '-0.023655', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0048217', 'logps_train/rejected': '-151.91', 'logps_train/chosen': '-128.16', 'loss/train': '0.69907', 'examples_per_second': '30.612', 'grad_norm': '84.5', 'counters/examples': 11360, 'counters/updates': 355}
train stats after 11392 examples: {'rewards_train/chosen': '-0.086716', 'rewards_train/rejected': '-0.064473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022243', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-174.03', 'loss/train': '0.71909', 'examples_per_second': '32.35', 'grad_norm': '108.5', 'counters/examples': 11392, 'counters/updates': 356}
train stats after 11424 examples: {'rewards_train/chosen': '-0.037447', 'rewards_train/rejected': '0.0051197', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042567', 'logps_train/rejected': '-137.92', 'logps_train/chosen': '-166.03', 'loss/train': '0.72281', 'examples_per_second': '30.167', 'grad_norm': '219', 'counters/examples': 11424, 'counters/updates': 357}
train stats after 11456 examples: {'rewards_train/chosen': '-0.012496', 'rewards_train/rejected': '0.043921', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.056416', 'logps_train/rejected': '-150.53', 'logps_train/chosen': '-137.85', 'loss/train': '0.73929', 'examples_per_second': '31.637', 'grad_norm': '126', 'counters/examples': 11456, 'counters/updates': 358}
train stats after 11488 examples: {'rewards_train/chosen': '-0.13126', 'rewards_train/rejected': '-0.068073', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.063182', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-156.23', 'loss/train': '0.74563', 'examples_per_second': '32.922', 'grad_norm': '64', 'counters/examples': 11488, 'counters/updates': 359}
train stats after 11520 examples: {'rewards_train/chosen': '0.00043569', 'rewards_train/rejected': '0.0055056', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0050699', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-193.48', 'loss/train': '0.71324', 'examples_per_second': '31.616', 'grad_norm': '114', 'counters/examples': 11520, 'counters/updates': 360}
train stats after 11552 examples: {'rewards_train/chosen': '-0.044827', 'rewards_train/rejected': '-0.026513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018314', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-143.37', 'loss/train': '0.71679', 'examples_per_second': '30.951', 'grad_norm': '175', 'counters/examples': 11552, 'counters/updates': 361}
train stats after 11584 examples: {'rewards_train/chosen': '-0.011124', 'rewards_train/rejected': '-0.040768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029644', 'logps_train/rejected': '-113.37', 'logps_train/chosen': '-147.6', 'loss/train': '0.69866', 'examples_per_second': '31.622', 'grad_norm': '191', 'counters/examples': 11584, 'counters/updates': 362}
skipping logging after 11616 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '0.019524', 'rewards_train/rejected': '-0.076265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095789', 'logps_train/rejected': '-137.13', 'logps_train/chosen': '-129.78', 'loss/train': '0.65268', 'examples_per_second': '30.6', 'grad_norm': '68.5', 'counters/examples': 11648, 'counters/updates': 364}
skipping logging after 11680 examples to avoid logging too frequently
train stats after 11712 examples: {'rewards_train/chosen': '-0.04086', 'rewards_train/rejected': '-0.10002', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059155', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-136.94', 'loss/train': '0.67723', 'examples_per_second': '31.484', 'grad_norm': '65.5', 'counters/examples': 11712, 'counters/updates': 366}
train stats after 11744 examples: {'rewards_train/chosen': '-0.071414', 'rewards_train/rejected': '-0.044642', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026773', 'logps_train/rejected': '-147.22', 'logps_train/chosen': '-201.22', 'loss/train': '0.72054', 'examples_per_second': '30.695', 'grad_norm': '137', 'counters/examples': 11744, 'counters/updates': 367}
train stats after 11776 examples: {'rewards_train/chosen': '0.028697', 'rewards_train/rejected': '-0.0063661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035063', 'logps_train/rejected': '-110.67', 'logps_train/chosen': '-165.64', 'loss/train': '0.68439', 'examples_per_second': '31.385', 'grad_norm': '61', 'counters/examples': 11776, 'counters/updates': 368}
train stats after 11808 examples: {'rewards_train/chosen': '0.075615', 'rewards_train/rejected': '-0.012434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088048', 'logps_train/rejected': '-135.56', 'logps_train/chosen': '-132.97', 'loss/train': '0.67258', 'examples_per_second': '31.43', 'grad_norm': '237', 'counters/examples': 11808, 'counters/updates': 369}
train stats after 11840 examples: {'rewards_train/chosen': '-0.097453', 'rewards_train/rejected': '-0.027866', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.069587', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-156.89', 'loss/train': '0.73956', 'examples_per_second': '31.276', 'grad_norm': '132', 'counters/examples': 11840, 'counters/updates': 370}
train stats after 11872 examples: {'rewards_train/chosen': '-0.0413', 'rewards_train/rejected': '0.099625', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.14092', 'logps_train/rejected': '-185.52', 'logps_train/chosen': '-163.33', 'loss/train': '0.79167', 'examples_per_second': '31.578', 'grad_norm': '176', 'counters/examples': 11872, 'counters/updates': 371}
train stats after 11904 examples: {'rewards_train/chosen': '0.027773', 'rewards_train/rejected': '0.004768', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023004', 'logps_train/rejected': '-153.38', 'logps_train/chosen': '-158.79', 'loss/train': '0.69634', 'examples_per_second': '30.233', 'grad_norm': '92', 'counters/examples': 11904, 'counters/updates': 372}
train stats after 11936 examples: {'rewards_train/chosen': '-0.013848', 'rewards_train/rejected': '-0.055758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04191', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-148.49', 'loss/train': '0.68202', 'examples_per_second': '31.849', 'grad_norm': '65.5', 'counters/examples': 11936, 'counters/updates': 373}
train stats after 11968 examples: {'rewards_train/chosen': '0.07098', 'rewards_train/rejected': '-0.018955', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089935', 'logps_train/rejected': '-155.85', 'logps_train/chosen': '-167.87', 'loss/train': '0.66613', 'examples_per_second': '31.571', 'grad_norm': '90.5', 'counters/examples': 11968, 'counters/updates': 374}
train stats after 12000 examples: {'rewards_train/chosen': '0.028683', 'rewards_train/rejected': '-0.0029497', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031633', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-126.03', 'loss/train': '0.6821', 'examples_per_second': '31.574', 'grad_norm': '52', 'counters/examples': 12000, 'counters/updates': 375}
Running evaluation after 12000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 12000: {'rewards_eval/chosen': '-0.0011414', 'rewards_eval/rejected': '0.0078759', 'rewards_eval/accuracies': '0.48047', 'rewards_eval/margins': '-0.0090173', 'logps_eval/rejected': '-128.06', 'logps_eval/chosen': '-151.02', 'loss/eval': '0.71163'}
skipping save for non epoch
train stats after 12032 examples: {'rewards_train/chosen': '0.0016658', 'rewards_train/rejected': '-0.042741', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044407', 'logps_train/rejected': '-117.56', 'logps_train/chosen': '-183.42', 'loss/train': '0.679', 'examples_per_second': '31.85', 'grad_norm': '87', 'counters/examples': 12032, 'counters/updates': 376}
skipping logging after 12064 examples to avoid logging too frequently
train stats after 12096 examples: {'rewards_train/chosen': '-0.027011', 'rewards_train/rejected': '0.068173', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.095184', 'logps_train/rejected': '-145.36', 'logps_train/chosen': '-131.01', 'loss/train': '0.75525', 'examples_per_second': '30.093', 'grad_norm': '112.5', 'counters/examples': 12096, 'counters/updates': 378}
skipping logging after 12128 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '-0.062816', 'rewards_train/rejected': '-0.051425', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01139', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-175.17', 'loss/train': '0.72306', 'examples_per_second': '32.447', 'grad_norm': '83', 'counters/examples': 12160, 'counters/updates': 380}
train stats after 12192 examples: {'rewards_train/chosen': '0.028093', 'rewards_train/rejected': '-0.0078742', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.035967', 'logps_train/rejected': '-148.91', 'logps_train/chosen': '-163.46', 'loss/train': '0.69315', 'examples_per_second': '30.483', 'grad_norm': '79', 'counters/examples': 12192, 'counters/updates': 381}
train stats after 12224 examples: {'rewards_train/chosen': '-0.044656', 'rewards_train/rejected': '-0.012893', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031763', 'logps_train/rejected': '-104.66', 'logps_train/chosen': '-120.56', 'loss/train': '0.71935', 'examples_per_second': '32.959', 'grad_norm': '82.5', 'counters/examples': 12224, 'counters/updates': 382}
train stats after 12256 examples: {'rewards_train/chosen': '0.068339', 'rewards_train/rejected': '-0.0054001', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073739', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-143.06', 'loss/train': '0.66982', 'examples_per_second': '31.461', 'grad_norm': '94.5', 'counters/examples': 12256, 'counters/updates': 383}
train stats after 12288 examples: {'rewards_train/chosen': '0.0018076', 'rewards_train/rejected': '-0.018638', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020445', 'logps_train/rejected': '-152.07', 'logps_train/chosen': '-164.08', 'loss/train': '0.69402', 'examples_per_second': '31.028', 'grad_norm': '96.5', 'counters/examples': 12288, 'counters/updates': 384}
skipping logging after 12320 examples to avoid logging too frequently
train stats after 12352 examples: {'rewards_train/chosen': '-0.01625', 'rewards_train/rejected': '-0.02702', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01077', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-145.79', 'loss/train': '0.69595', 'examples_per_second': '34.979', 'grad_norm': '86', 'counters/examples': 12352, 'counters/updates': 386}
train stats after 12384 examples: {'rewards_train/chosen': '-0.059866', 'rewards_train/rejected': '0.02949', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.089356', 'logps_train/rejected': '-104.59', 'logps_train/chosen': '-127.51', 'loss/train': '0.74568', 'examples_per_second': '31.657', 'grad_norm': '110', 'counters/examples': 12384, 'counters/updates': 387}
train stats after 12416 examples: {'rewards_train/chosen': '-0.022969', 'rewards_train/rejected': '0.02244', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045409', 'logps_train/rejected': '-143.51', 'logps_train/chosen': '-116.37', 'loss/train': '0.73078', 'examples_per_second': '31.512', 'grad_norm': '100', 'counters/examples': 12416, 'counters/updates': 388}
train stats after 12448 examples: {'rewards_train/chosen': '-0.035903', 'rewards_train/rejected': '0.081757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.11766', 'logps_train/rejected': '-172.92', 'logps_train/chosen': '-197.01', 'loss/train': '0.78266', 'examples_per_second': '29.965', 'grad_norm': '128', 'counters/examples': 12448, 'counters/updates': 389}
train stats after 12480 examples: {'rewards_train/chosen': '0.00041681', 'rewards_train/rejected': '-0.035404', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035821', 'logps_train/rejected': '-137.24', 'logps_train/chosen': '-188.72', 'loss/train': '0.68477', 'examples_per_second': '30.698', 'grad_norm': '556', 'counters/examples': 12480, 'counters/updates': 390}
train stats after 12512 examples: {'rewards_train/chosen': '-0.02822', 'rewards_train/rejected': '-0.037283', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0090629', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-158.56', 'loss/train': '0.70434', 'examples_per_second': '30.87', 'grad_norm': '84', 'counters/examples': 12512, 'counters/updates': 391}
train stats after 12544 examples: {'rewards_train/chosen': '0.018688', 'rewards_train/rejected': '0.055199', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.036511', 'logps_train/rejected': '-96.425', 'logps_train/chosen': '-170.33', 'loss/train': '0.73014', 'examples_per_second': '31.592', 'grad_norm': '119.5', 'counters/examples': 12544, 'counters/updates': 392}
train stats after 12576 examples: {'rewards_train/chosen': '-0.054458', 'rewards_train/rejected': '-0.03166', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.022798', 'logps_train/rejected': '-137.24', 'logps_train/chosen': '-135.35', 'loss/train': '0.70967', 'examples_per_second': '31.942', 'grad_norm': '92.5', 'counters/examples': 12576, 'counters/updates': 393}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12640 examples: {'rewards_train/chosen': '0.15493', 'rewards_train/rejected': '-0.060935', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21587', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-154.07', 'loss/train': '0.60807', 'examples_per_second': '30.285', 'grad_norm': '68.5', 'counters/examples': 12640, 'counters/updates': 395}
train stats after 12672 examples: {'rewards_train/chosen': '-0.035167', 'rewards_train/rejected': '-0.0041794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.030988', 'logps_train/rejected': '-147.89', 'logps_train/chosen': '-164.74', 'loss/train': '0.72973', 'examples_per_second': '30.561', 'grad_norm': '112.5', 'counters/examples': 12672, 'counters/updates': 396}
skipping logging after 12704 examples to avoid logging too frequently
train stats after 12736 examples: {'rewards_train/chosen': '-0.035936', 'rewards_train/rejected': '0.026588', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.062524', 'logps_train/rejected': '-144.25', 'logps_train/chosen': '-151.59', 'loss/train': '0.74373', 'examples_per_second': '25.267', 'grad_norm': '162', 'counters/examples': 12736, 'counters/updates': 398}
train stats after 12768 examples: {'rewards_train/chosen': '-0.072601', 'rewards_train/rejected': '-0.095962', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023361', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-152.45', 'loss/train': '0.69435', 'examples_per_second': '31.009', 'grad_norm': '77.5', 'counters/examples': 12768, 'counters/updates': 399}
train stats after 12800 examples: {'rewards_train/chosen': '-0.12382', 'rewards_train/rejected': '0.044269', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.16809', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-147.84', 'loss/train': '0.81036', 'examples_per_second': '31.62', 'grad_norm': '268', 'counters/examples': 12800, 'counters/updates': 400}
skipping logging after 12832 examples to avoid logging too frequently
train stats after 12864 examples: {'rewards_train/chosen': '-0.10823', 'rewards_train/rejected': '0.046017', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.15424', 'logps_train/rejected': '-144.83', 'logps_train/chosen': '-162.59', 'loss/train': '0.78773', 'examples_per_second': '31.608', 'grad_norm': '124.5', 'counters/examples': 12864, 'counters/updates': 402}
train stats after 12896 examples: {'rewards_train/chosen': '-0.048921', 'rewards_train/rejected': '-0.056145', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0072237', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-134.58', 'loss/train': '0.70003', 'examples_per_second': '31.61', 'grad_norm': '62.5', 'counters/examples': 12896, 'counters/updates': 403}
skipping logging after 12928 examples to avoid logging too frequently
train stats after 12960 examples: {'rewards_train/chosen': '0.007743', 'rewards_train/rejected': '0.045418', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037675', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-154.79', 'loss/train': '0.7206', 'examples_per_second': '33.956', 'grad_norm': '99', 'counters/examples': 12960, 'counters/updates': 405}
train stats after 12992 examples: {'rewards_train/chosen': '-0.013599', 'rewards_train/rejected': '-0.025961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012362', 'logps_train/rejected': '-160.17', 'logps_train/chosen': '-138.89', 'loss/train': '0.69912', 'examples_per_second': '31.064', 'grad_norm': '228', 'counters/examples': 12992, 'counters/updates': 406}
train stats after 13024 examples: {'rewards_train/chosen': '0.0068036', 'rewards_train/rejected': '-0.032411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039214', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-132.8', 'loss/train': '0.68236', 'examples_per_second': '31.05', 'grad_norm': '82.5', 'counters/examples': 13024, 'counters/updates': 407}
train stats after 13056 examples: {'rewards_train/chosen': '0.0082344', 'rewards_train/rejected': '-0.005007', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013241', 'logps_train/rejected': '-110.96', 'logps_train/chosen': '-118.52', 'loss/train': '0.69457', 'examples_per_second': '32.249', 'grad_norm': '59.75', 'counters/examples': 13056, 'counters/updates': 408}
skipping logging after 13088 examples to avoid logging too frequently
train stats after 13120 examples: {'rewards_train/chosen': '-0.048721', 'rewards_train/rejected': '0.058938', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.10766', 'logps_train/rejected': '-147.03', 'logps_train/chosen': '-189.48', 'loss/train': '0.77277', 'examples_per_second': '31.652', 'grad_norm': '400', 'counters/examples': 13120, 'counters/updates': 410}
train stats after 13152 examples: {'rewards_train/chosen': '-0.014619', 'rewards_train/rejected': '0.0031255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017744', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-154.6', 'loss/train': '0.71028', 'examples_per_second': '31.373', 'grad_norm': '102.5', 'counters/examples': 13152, 'counters/updates': 411}
train stats after 13184 examples: {'rewards_train/chosen': '0.083346', 'rewards_train/rejected': '-0.081898', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16524', 'logps_train/rejected': '-141.45', 'logps_train/chosen': '-148.27', 'loss/train': '0.62921', 'examples_per_second': '30.754', 'grad_norm': '88', 'counters/examples': 13184, 'counters/updates': 412}
skipping logging after 13216 examples to avoid logging too frequently
train stats after 13248 examples: {'rewards_train/chosen': '-0.020739', 'rewards_train/rejected': '-0.020057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00068182', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-168.58', 'loss/train': '0.71087', 'examples_per_second': '34.433', 'grad_norm': '197', 'counters/examples': 13248, 'counters/updates': 414}
train stats after 13280 examples: {'rewards_train/chosen': '0.017031', 'rewards_train/rejected': '0.12238', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10535', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-161.74', 'loss/train': '0.76021', 'examples_per_second': '31.559', 'grad_norm': '189', 'counters/examples': 13280, 'counters/updates': 415}
skipping logging after 13312 examples to avoid logging too frequently
train stats after 13344 examples: {'rewards_train/chosen': '0.0095283', 'rewards_train/rejected': '-0.0021593', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011688', 'logps_train/rejected': '-146.61', 'logps_train/chosen': '-158.51', 'loss/train': '0.69769', 'examples_per_second': '31.587', 'grad_norm': '131', 'counters/examples': 13344, 'counters/updates': 417}
train stats after 13376 examples: {'rewards_train/chosen': '0.049047', 'rewards_train/rejected': '0.057584', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0085378', 'logps_train/rejected': '-151.63', 'logps_train/chosen': '-150.51', 'loss/train': '0.70868', 'examples_per_second': '30.075', 'grad_norm': '130', 'counters/examples': 13376, 'counters/updates': 418}
train stats after 13408 examples: {'rewards_train/chosen': '0.038885', 'rewards_train/rejected': '-0.010986', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04987', 'logps_train/rejected': '-137.17', 'logps_train/chosen': '-163.89', 'loss/train': '0.68589', 'examples_per_second': '31.699', 'grad_norm': '206', 'counters/examples': 13408, 'counters/updates': 419}
train stats after 13440 examples: {'rewards_train/chosen': '0.0050561', 'rewards_train/rejected': '0.076102', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.071046', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-145.58', 'loss/train': '0.7372', 'examples_per_second': '31.52', 'grad_norm': '102', 'counters/examples': 13440, 'counters/updates': 420}
train stats after 13472 examples: {'rewards_train/chosen': '-0.0020032', 'rewards_train/rejected': '0.061434', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.063437', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-144.83', 'loss/train': '0.73357', 'examples_per_second': '25.445', 'grad_norm': '104.5', 'counters/examples': 13472, 'counters/updates': 421}
train stats after 13504 examples: {'rewards_train/chosen': '-0.015284', 'rewards_train/rejected': '-0.082412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067127', 'logps_train/rejected': '-162.08', 'logps_train/chosen': '-173.82', 'loss/train': '0.68799', 'examples_per_second': '31.605', 'grad_norm': '110', 'counters/examples': 13504, 'counters/updates': 422}
skipping logging after 13536 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '0.068378', 'rewards_train/rejected': '0.0063974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06198', 'logps_train/rejected': '-123.9', 'logps_train/chosen': '-152.48', 'loss/train': '0.67846', 'examples_per_second': '24.18', 'grad_norm': '73', 'counters/examples': 13568, 'counters/updates': 424}
train stats after 13600 examples: {'rewards_train/chosen': '-0.02336', 'rewards_train/rejected': '-0.067324', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043964', 'logps_train/rejected': '-150', 'logps_train/chosen': '-165.17', 'loss/train': '0.69316', 'examples_per_second': '30.996', 'grad_norm': '95.5', 'counters/examples': 13600, 'counters/updates': 425}
train stats after 13632 examples: {'rewards_train/chosen': '0.00104', 'rewards_train/rejected': '0.075641', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.074601', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-131.17', 'loss/train': '0.74365', 'examples_per_second': '30.933', 'grad_norm': '158', 'counters/examples': 13632, 'counters/updates': 426}
train stats after 13664 examples: {'rewards_train/chosen': '-0.016964', 'rewards_train/rejected': '0.021155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.038119', 'logps_train/rejected': '-137.29', 'logps_train/chosen': '-147.37', 'loss/train': '0.7264', 'examples_per_second': '33.016', 'grad_norm': '172', 'counters/examples': 13664, 'counters/updates': 427}
train stats after 13696 examples: {'rewards_train/chosen': '-0.0069311', 'rewards_train/rejected': '0.0078748', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014806', 'logps_train/rejected': '-161.79', 'logps_train/chosen': '-164.99', 'loss/train': '0.70876', 'examples_per_second': '32.137', 'grad_norm': '88.5', 'counters/examples': 13696, 'counters/updates': 428}
train stats after 13728 examples: {'rewards_train/chosen': '-0.011705', 'rewards_train/rejected': '0.0081398', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019845', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-177.74', 'loss/train': '0.71655', 'examples_per_second': '30.708', 'grad_norm': '75.5', 'counters/examples': 13728, 'counters/updates': 429}
train stats after 13760 examples: {'rewards_train/chosen': '-0.0006907', 'rewards_train/rejected': '-0.016575', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015884', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-180.35', 'loss/train': '0.70131', 'examples_per_second': '30.535', 'grad_norm': '134', 'counters/examples': 13760, 'counters/updates': 430}
train stats after 13792 examples: {'rewards_train/chosen': '-0.039939', 'rewards_train/rejected': '-0.0081632', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.031776', 'logps_train/rejected': '-151.41', 'logps_train/chosen': '-171.09', 'loss/train': '0.72479', 'examples_per_second': '31.06', 'grad_norm': '100', 'counters/examples': 13792, 'counters/updates': 431}
train stats after 13824 examples: {'rewards_train/chosen': '0.0037151', 'rewards_train/rejected': '-0.036238', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.039953', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-134.08', 'loss/train': '0.68288', 'examples_per_second': '31.725', 'grad_norm': '73', 'counters/examples': 13824, 'counters/updates': 432}
train stats after 13856 examples: {'rewards_train/chosen': '-0.10122', 'rewards_train/rejected': '-2.8608e-05', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.1012', 'logps_train/rejected': '-167.51', 'logps_train/chosen': '-160.31', 'loss/train': '0.76631', 'examples_per_second': '33.139', 'grad_norm': '110', 'counters/examples': 13856, 'counters/updates': 433}
train stats after 13888 examples: {'rewards_train/chosen': '-0.049843', 'rewards_train/rejected': '-0.02808', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021763', 'logps_train/rejected': '-168.53', 'logps_train/chosen': '-140.6', 'loss/train': '0.73167', 'examples_per_second': '31.702', 'grad_norm': '187', 'counters/examples': 13888, 'counters/updates': 434}
skipping logging after 13920 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '-0.0031304', 'rewards_train/rejected': '-0.015032', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011901', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-136.6', 'loss/train': '0.69746', 'examples_per_second': '33.397', 'grad_norm': '52', 'counters/examples': 13952, 'counters/updates': 436}
train stats after 13984 examples: {'rewards_train/chosen': '0.083446', 'rewards_train/rejected': '0.089829', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0063833', 'logps_train/rejected': '-103.86', 'logps_train/chosen': '-131.48', 'loss/train': '0.70342', 'examples_per_second': '32.833', 'grad_norm': '134', 'counters/examples': 13984, 'counters/updates': 437}
train stats after 14016 examples: {'rewards_train/chosen': '0.047061', 'rewards_train/rejected': '0.0089284', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038133', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-167.96', 'loss/train': '0.68642', 'examples_per_second': '32.603', 'grad_norm': '137', 'counters/examples': 14016, 'counters/updates': 438}
train stats after 14048 examples: {'rewards_train/chosen': '-0.02645', 'rewards_train/rejected': '0.01529', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041739', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-124.54', 'loss/train': '0.72428', 'examples_per_second': '32.441', 'grad_norm': '85.5', 'counters/examples': 14048, 'counters/updates': 439}
train stats after 14080 examples: {'rewards_train/chosen': '-0.039303', 'rewards_train/rejected': '-0.072371', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033067', 'logps_train/rejected': '-144.92', 'logps_train/chosen': '-138.24', 'loss/train': '0.68842', 'examples_per_second': '31.547', 'grad_norm': '85', 'counters/examples': 14080, 'counters/updates': 440}
train stats after 14112 examples: {'rewards_train/chosen': '0.022895', 'rewards_train/rejected': '-0.0070742', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029969', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-163.6', 'loss/train': '0.68916', 'examples_per_second': '33.02', 'grad_norm': '78.5', 'counters/examples': 14112, 'counters/updates': 441}
train stats after 14144 examples: {'rewards_train/chosen': '0.033846', 'rewards_train/rejected': '-0.0084749', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042321', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-134.15', 'loss/train': '0.68405', 'examples_per_second': '31.719', 'grad_norm': '142', 'counters/examples': 14144, 'counters/updates': 442}
train stats after 14176 examples: {'rewards_train/chosen': '-0.039621', 'rewards_train/rejected': '0.007569', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.04719', 'logps_train/rejected': '-147.33', 'logps_train/chosen': '-173.02', 'loss/train': '0.7452', 'examples_per_second': '33.139', 'grad_norm': '236', 'counters/examples': 14176, 'counters/updates': 443}
skipping logging after 14208 examples to avoid logging too frequently
train stats after 14240 examples: {'rewards_train/chosen': '-0.041614', 'rewards_train/rejected': '-0.00023692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.041377', 'logps_train/rejected': '-113', 'logps_train/chosen': '-148.29', 'loss/train': '0.73154', 'examples_per_second': '31.685', 'grad_norm': '144', 'counters/examples': 14240, 'counters/updates': 445}
train stats after 14272 examples: {'rewards_train/chosen': '0.031128', 'rewards_train/rejected': '-0.0058761', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037005', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-123.45', 'loss/train': '0.69075', 'examples_per_second': '32.445', 'grad_norm': '132', 'counters/examples': 14272, 'counters/updates': 446}
skipping logging after 14304 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '-0.064888', 'rewards_train/rejected': '0.061307', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12619', 'logps_train/rejected': '-154.33', 'logps_train/chosen': '-179.63', 'loss/train': '0.77662', 'examples_per_second': '30.706', 'grad_norm': '114', 'counters/examples': 14336, 'counters/updates': 448}
train stats after 14368 examples: {'rewards_train/chosen': '-0.0051469', 'rewards_train/rejected': '0.00022294', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0053698', 'logps_train/rejected': '-105.87', 'logps_train/chosen': '-149.47', 'loss/train': '0.70411', 'examples_per_second': '30.284', 'grad_norm': '127', 'counters/examples': 14368, 'counters/updates': 449}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14432 examples: {'rewards_train/chosen': '0.039077', 'rewards_train/rejected': '-0.053464', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092541', 'logps_train/rejected': '-153.79', 'logps_train/chosen': '-189.11', 'loss/train': '0.6713', 'examples_per_second': '31.986', 'grad_norm': '78.5', 'counters/examples': 14432, 'counters/updates': 451}
train stats after 14464 examples: {'rewards_train/chosen': '-0.0050024', 'rewards_train/rejected': '-0.034432', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02943', 'logps_train/rejected': '-146.42', 'logps_train/chosen': '-144.07', 'loss/train': '0.68965', 'examples_per_second': '31.843', 'grad_norm': '110', 'counters/examples': 14464, 'counters/updates': 452}
train stats after 14496 examples: {'rewards_train/chosen': '0.02387', 'rewards_train/rejected': '-0.042758', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066627', 'logps_train/rejected': '-161.22', 'logps_train/chosen': '-170.34', 'loss/train': '0.67636', 'examples_per_second': '30.91', 'grad_norm': '102', 'counters/examples': 14496, 'counters/updates': 453}
train stats after 14528 examples: {'rewards_train/chosen': '0.035869', 'rewards_train/rejected': '-0.019013', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054882', 'logps_train/rejected': '-146.12', 'logps_train/chosen': '-125.2', 'loss/train': '0.67698', 'examples_per_second': '32.544', 'grad_norm': '104', 'counters/examples': 14528, 'counters/updates': 454}
skipping logging after 14560 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '0.0017722', 'rewards_train/rejected': '-0.0014031', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0031753', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-143.58', 'loss/train': '0.71017', 'examples_per_second': '31.615', 'grad_norm': '87.5', 'counters/examples': 14592, 'counters/updates': 456}
train stats after 14624 examples: {'rewards_train/chosen': '-0.011818', 'rewards_train/rejected': '0.035815', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.047633', 'logps_train/rejected': '-91.447', 'logps_train/chosen': '-118.65', 'loss/train': '0.72772', 'examples_per_second': '31.719', 'grad_norm': '59.5', 'counters/examples': 14624, 'counters/updates': 457}
train stats after 14656 examples: {'rewards_train/chosen': '0.019022', 'rewards_train/rejected': '-0.025933', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044955', 'logps_train/rejected': '-120.19', 'logps_train/chosen': '-140.11', 'loss/train': '0.67677', 'examples_per_second': '31.738', 'grad_norm': '111', 'counters/examples': 14656, 'counters/updates': 458}
skipping logging after 14688 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '0.00094', 'rewards_train/rejected': '-0.037356', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038296', 'logps_train/rejected': '-165.81', 'logps_train/chosen': '-191.68', 'loss/train': '0.7039', 'examples_per_second': '31.772', 'grad_norm': '137', 'counters/examples': 14720, 'counters/updates': 460}
train stats after 14752 examples: {'rewards_train/chosen': '-0.0512', 'rewards_train/rejected': '0.017689', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.068888', 'logps_train/rejected': '-170.46', 'logps_train/chosen': '-173.48', 'loss/train': '0.75107', 'examples_per_second': '31.657', 'grad_norm': '60.25', 'counters/examples': 14752, 'counters/updates': 461}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14816 examples: {'rewards_train/chosen': '-0.03588', 'rewards_train/rejected': '-0.11145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075569', 'logps_train/rejected': '-157.99', 'logps_train/chosen': '-135.85', 'loss/train': '0.66846', 'examples_per_second': '31.685', 'grad_norm': '86', 'counters/examples': 14816, 'counters/updates': 463}
skipping logging after 14848 examples to avoid logging too frequently
train stats after 14880 examples: {'rewards_train/chosen': '0.086013', 'rewards_train/rejected': '-0.015022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10103', 'logps_train/rejected': '-143.61', 'logps_train/chosen': '-150.76', 'loss/train': '0.65447', 'examples_per_second': '32.785', 'grad_norm': '84.5', 'counters/examples': 14880, 'counters/updates': 465}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14944 examples: {'rewards_train/chosen': '-0.050676', 'rewards_train/rejected': '0.0070173', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.057693', 'logps_train/rejected': '-140.28', 'logps_train/chosen': '-138.99', 'loss/train': '0.73263', 'examples_per_second': '31.734', 'grad_norm': '65', 'counters/examples': 14944, 'counters/updates': 467}
train stats after 14976 examples: {'rewards_train/chosen': '-0.0037158', 'rewards_train/rejected': '0.0081031', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011819', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-128.14', 'loss/train': '0.71184', 'examples_per_second': '32.838', 'grad_norm': '66.5', 'counters/examples': 14976, 'counters/updates': 468}
train stats after 15008 examples: {'rewards_train/chosen': '-0.011759', 'rewards_train/rejected': '-0.0062241', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0055344', 'logps_train/rejected': '-103.74', 'logps_train/chosen': '-106.21', 'loss/train': '0.69922', 'examples_per_second': '31.018', 'grad_norm': '69', 'counters/examples': 15008, 'counters/updates': 469}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15072 examples: {'rewards_train/chosen': '-0.075058', 'rewards_train/rejected': '-0.0070773', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.067981', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-157.78', 'loss/train': '0.73657', 'examples_per_second': '30.416', 'grad_norm': '182', 'counters/examples': 15072, 'counters/updates': 471}
train stats after 15104 examples: {'rewards_train/chosen': '0.098458', 'rewards_train/rejected': '0.093363', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0050951', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-150.66', 'loss/train': '0.70253', 'examples_per_second': '30.97', 'grad_norm': '118.5', 'counters/examples': 15104, 'counters/updates': 472}
train stats after 15136 examples: {'rewards_train/chosen': '-0.028391', 'rewards_train/rejected': '0.02133', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.049721', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-156.05', 'loss/train': '0.74497', 'examples_per_second': '32.831', 'grad_norm': '245', 'counters/examples': 15136, 'counters/updates': 473}
train stats after 15168 examples: {'rewards_train/chosen': '0.0022816', 'rewards_train/rejected': '0.032776', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030494', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-129.16', 'loss/train': '0.71735', 'examples_per_second': '31.703', 'grad_norm': '138', 'counters/examples': 15168, 'counters/updates': 474}
train stats after 15200 examples: {'rewards_train/chosen': '0.062933', 'rewards_train/rejected': '0.013862', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.049071', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-177.71', 'loss/train': '0.6925', 'examples_per_second': '32.971', 'grad_norm': '69', 'counters/examples': 15200, 'counters/updates': 475}
train stats after 15232 examples: {'rewards_train/chosen': '-0.053266', 'rewards_train/rejected': '0.017297', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.070563', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-122.29', 'loss/train': '0.73802', 'examples_per_second': '31.271', 'grad_norm': '62.75', 'counters/examples': 15232, 'counters/updates': 476}
train stats after 15264 examples: {'rewards_train/chosen': '0.038397', 'rewards_train/rejected': '0.0061158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032281', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-165.02', 'loss/train': '0.68768', 'examples_per_second': '30.857', 'grad_norm': '153', 'counters/examples': 15264, 'counters/updates': 477}
train stats after 15296 examples: {'rewards_train/chosen': '-0.038331', 'rewards_train/rejected': '0.044002', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.082333', 'logps_train/rejected': '-106.81', 'logps_train/chosen': '-169.36', 'loss/train': '0.75408', 'examples_per_second': '30.257', 'grad_norm': '92.5', 'counters/examples': 15296, 'counters/updates': 478}
skipping logging after 15328 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '-0.0042012', 'rewards_train/rejected': '-0.023452', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019251', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-165.54', 'loss/train': '0.69435', 'examples_per_second': '30.339', 'grad_norm': '112', 'counters/examples': 15360, 'counters/updates': 480}
train stats after 15392 examples: {'rewards_train/chosen': '-0.02', 'rewards_train/rejected': '0.048275', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.068275', 'logps_train/rejected': '-116.6', 'logps_train/chosen': '-148.53', 'loss/train': '0.7406', 'examples_per_second': '31.887', 'grad_norm': '308', 'counters/examples': 15392, 'counters/updates': 481}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15456 examples: {'rewards_train/chosen': '-0.099326', 'rewards_train/rejected': '0.041851', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.14118', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-139.46', 'loss/train': '0.77725', 'examples_per_second': '31.688', 'grad_norm': '77.5', 'counters/examples': 15456, 'counters/updates': 483}
train stats after 15488 examples: {'rewards_train/chosen': '-0.12155', 'rewards_train/rejected': '-0.02838', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.093169', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-126.69', 'loss/train': '0.75942', 'examples_per_second': '31.858', 'grad_norm': '133', 'counters/examples': 15488, 'counters/updates': 484}
train stats after 15520 examples: {'rewards_train/chosen': '-0.15074', 'rewards_train/rejected': '-0.018481', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.13225', 'logps_train/rejected': '-188', 'logps_train/chosen': '-192.99', 'loss/train': '0.78857', 'examples_per_second': '32.707', 'grad_norm': '136', 'counters/examples': 15520, 'counters/updates': 485}
skipping logging after 15552 examples to avoid logging too frequently
train stats after 15584 examples: {'rewards_train/chosen': '-0.054649', 'rewards_train/rejected': '-0.0011688', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.053481', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-161.51', 'loss/train': '0.733', 'examples_per_second': '30.356', 'grad_norm': '117.5', 'counters/examples': 15584, 'counters/updates': 487}
train stats after 15616 examples: {'rewards_train/chosen': '0.06794', 'rewards_train/rejected': '0.025021', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042918', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-155.24', 'loss/train': '0.68276', 'examples_per_second': '31.58', 'grad_norm': '121', 'counters/examples': 15616, 'counters/updates': 488}
skipping logging after 15648 examples to avoid logging too frequently
train stats after 15680 examples: {'rewards_train/chosen': '-0.0016792', 'rewards_train/rejected': '0.12591', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.12758', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-122.63', 'loss/train': '0.76566', 'examples_per_second': '32.587', 'grad_norm': '76.5', 'counters/examples': 15680, 'counters/updates': 490}
train stats after 15712 examples: {'rewards_train/chosen': '-0.011275', 'rewards_train/rejected': '-0.046262', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034987', 'logps_train/rejected': '-155.98', 'logps_train/chosen': '-149.5', 'loss/train': '0.68419', 'examples_per_second': '32.784', 'grad_norm': '104', 'counters/examples': 15712, 'counters/updates': 491}
train stats after 15744 examples: {'rewards_train/chosen': '0.010735', 'rewards_train/rejected': '0.010116', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00061885', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-123.78', 'loss/train': '0.6998', 'examples_per_second': '31.435', 'grad_norm': '168', 'counters/examples': 15744, 'counters/updates': 492}
train stats after 15776 examples: {'rewards_train/chosen': '-0.0033793', 'rewards_train/rejected': '-0.04485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041471', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-150.21', 'loss/train': '0.68125', 'examples_per_second': '31.802', 'grad_norm': '85', 'counters/examples': 15776, 'counters/updates': 493}
train stats after 15808 examples: {'rewards_train/chosen': '-0.0074533', 'rewards_train/rejected': '-0.0095941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0021408', 'logps_train/rejected': '-158.72', 'logps_train/chosen': '-131.48', 'loss/train': '0.70652', 'examples_per_second': '31.696', 'grad_norm': '87', 'counters/examples': 15808, 'counters/updates': 494}
train stats after 15840 examples: {'rewards_train/chosen': '-0.042053', 'rewards_train/rejected': '0.023604', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.065657', 'logps_train/rejected': '-149.84', 'logps_train/chosen': '-127.59', 'loss/train': '0.74721', 'examples_per_second': '31.873', 'grad_norm': '68', 'counters/examples': 15840, 'counters/updates': 495}
train stats after 15872 examples: {'rewards_train/chosen': '0.013156', 'rewards_train/rejected': '0.020296', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0071396', 'logps_train/rejected': '-149.64', 'logps_train/chosen': '-149.03', 'loss/train': '0.71391', 'examples_per_second': '31.578', 'grad_norm': '108.5', 'counters/examples': 15872, 'counters/updates': 496}
skipping logging after 15904 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': '-0.029391', 'rewards_train/rejected': '-0.025689', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0037018', 'logps_train/rejected': '-179.56', 'logps_train/chosen': '-124.94', 'loss/train': '0.70539', 'examples_per_second': '35.779', 'grad_norm': '85.5', 'counters/examples': 15936, 'counters/updates': 498}
train stats after 15968 examples: {'rewards_train/chosen': '-0.0072377', 'rewards_train/rejected': '0.071035', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.078273', 'logps_train/rejected': '-163.12', 'logps_train/chosen': '-159.51', 'loss/train': '0.74273', 'examples_per_second': '30.844', 'grad_norm': '111', 'counters/examples': 15968, 'counters/updates': 499}
train stats after 16000 examples: {'rewards_train/chosen': '-0.026059', 'rewards_train/rejected': '-0.00064389', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025415', 'logps_train/rejected': '-127.22', 'logps_train/chosen': '-168.27', 'loss/train': '0.71605', 'examples_per_second': '31.652', 'grad_norm': '64.5', 'counters/examples': 16000, 'counters/updates': 500}
train stats after 16032 examples: {'rewards_train/chosen': '0.05478', 'rewards_train/rejected': '0.038809', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015971', 'logps_train/rejected': '-112.34', 'logps_train/chosen': '-174.04', 'loss/train': '0.70461', 'examples_per_second': '31.712', 'grad_norm': '100.5', 'counters/examples': 16032, 'counters/updates': 501}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16096 examples: {'rewards_train/chosen': '0.045949', 'rewards_train/rejected': '0.0043629', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041586', 'logps_train/rejected': '-180.79', 'logps_train/chosen': '-172.96', 'loss/train': '0.68232', 'examples_per_second': '31.415', 'grad_norm': '104', 'counters/examples': 16096, 'counters/updates': 503}
train stats after 16128 examples: {'rewards_train/chosen': '-0.016494', 'rewards_train/rejected': '-0.036414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01992', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-165.04', 'loss/train': '0.70177', 'examples_per_second': '31.696', 'grad_norm': '96.5', 'counters/examples': 16128, 'counters/updates': 504}
train stats after 16160 examples: {'rewards_train/chosen': '-0.061636', 'rewards_train/rejected': '0.025106', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.086742', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-134.15', 'loss/train': '0.7456', 'examples_per_second': '31.712', 'grad_norm': '100.5', 'counters/examples': 16160, 'counters/updates': 505}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16224 examples: {'rewards_train/chosen': '-0.053052', 'rewards_train/rejected': '-0.049398', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0036533', 'logps_train/rejected': '-129.76', 'logps_train/chosen': '-145.03', 'loss/train': '0.70554', 'examples_per_second': '31.895', 'grad_norm': '125.5', 'counters/examples': 16224, 'counters/updates': 507}
skipping logging after 16256 examples to avoid logging too frequently
train stats after 16288 examples: {'rewards_train/chosen': '-0.11125', 'rewards_train/rejected': '-0.04949', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06176', 'logps_train/rejected': '-100.18', 'logps_train/chosen': '-131.26', 'loss/train': '0.73313', 'examples_per_second': '30.871', 'grad_norm': '60.25', 'counters/examples': 16288, 'counters/updates': 509}
skipping logging after 16320 examples to avoid logging too frequently
train stats after 16352 examples: {'rewards_train/chosen': '-0.041517', 'rewards_train/rejected': '0.021775', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063292', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-143.8', 'loss/train': '0.73904', 'examples_per_second': '30.483', 'grad_norm': '90.5', 'counters/examples': 16352, 'counters/updates': 511}
train stats after 16384 examples: {'rewards_train/chosen': '0.0047359', 'rewards_train/rejected': '0.012703', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0079671', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-169.81', 'loss/train': '0.71182', 'examples_per_second': '31.714', 'grad_norm': '85', 'counters/examples': 16384, 'counters/updates': 512}
skipping logging after 16416 examples to avoid logging too frequently
train stats after 16448 examples: {'rewards_train/chosen': '0.028843', 'rewards_train/rejected': '-0.050451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079293', 'logps_train/rejected': '-145.7', 'logps_train/chosen': '-166.64', 'loss/train': '0.67118', 'examples_per_second': '31.703', 'grad_norm': '64', 'counters/examples': 16448, 'counters/updates': 514}
train stats after 16480 examples: {'rewards_train/chosen': '0.0066458', 'rewards_train/rejected': '-0.013414', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020059', 'logps_train/rejected': '-102.19', 'logps_train/chosen': '-130.53', 'loss/train': '0.68969', 'examples_per_second': '31.567', 'grad_norm': '71', 'counters/examples': 16480, 'counters/updates': 515}
skipping logging after 16512 examples to avoid logging too frequently
train stats after 16544 examples: {'rewards_train/chosen': '-0.027134', 'rewards_train/rejected': '0.035702', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.062837', 'logps_train/rejected': '-114.67', 'logps_train/chosen': '-184.13', 'loss/train': '0.73722', 'examples_per_second': '32.004', 'grad_norm': '62.5', 'counters/examples': 16544, 'counters/updates': 517}
train stats after 16576 examples: {'rewards_train/chosen': '-0.012475', 'rewards_train/rejected': '0.074051', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.086525', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-114.24', 'loss/train': '0.75818', 'examples_per_second': '30.846', 'grad_norm': '71.5', 'counters/examples': 16576, 'counters/updates': 518}
train stats after 16608 examples: {'rewards_train/chosen': '-0.026493', 'rewards_train/rejected': '-0.056254', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029761', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-124.09', 'loss/train': '0.68614', 'examples_per_second': '32.741', 'grad_norm': '85.5', 'counters/examples': 16608, 'counters/updates': 519}
train stats after 16640 examples: {'rewards_train/chosen': '-0.16936', 'rewards_train/rejected': '-0.036477', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.13289', 'logps_train/rejected': '-157.49', 'logps_train/chosen': '-175.04', 'loss/train': '0.80985', 'examples_per_second': '30.162', 'grad_norm': '136', 'counters/examples': 16640, 'counters/updates': 520}
train stats after 16672 examples: {'rewards_train/chosen': '0.0097753', 'rewards_train/rejected': '-0.012753', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022529', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-158.83', 'loss/train': '0.69137', 'examples_per_second': '30.786', 'grad_norm': '91', 'counters/examples': 16672, 'counters/updates': 521}
train stats after 16704 examples: {'rewards_train/chosen': '0.0047604', 'rewards_train/rejected': '0.014762', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010001', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-137.03', 'loss/train': '0.70757', 'examples_per_second': '30.811', 'grad_norm': '85.5', 'counters/examples': 16704, 'counters/updates': 522}
train stats after 16736 examples: {'rewards_train/chosen': '-0.0029035', 'rewards_train/rejected': '-0.018784', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015881', 'logps_train/rejected': '-144.01', 'logps_train/chosen': '-130.01', 'loss/train': '0.69994', 'examples_per_second': '31.6', 'grad_norm': '142', 'counters/examples': 16736, 'counters/updates': 523}
train stats after 16768 examples: {'rewards_train/chosen': '-0.011364', 'rewards_train/rejected': '-0.072049', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060686', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-155.9', 'loss/train': '0.66998', 'examples_per_second': '31.125', 'grad_norm': '102', 'counters/examples': 16768, 'counters/updates': 524}
train stats after 16800 examples: {'rewards_train/chosen': '0.033324', 'rewards_train/rejected': '0.03256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00076352', 'logps_train/rejected': '-188.28', 'logps_train/chosen': '-170.24', 'loss/train': '0.70869', 'examples_per_second': '31.646', 'grad_norm': '151', 'counters/examples': 16800, 'counters/updates': 525}
train stats after 16832 examples: {'rewards_train/chosen': '0.01129', 'rewards_train/rejected': '-0.096218', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10751', 'logps_train/rejected': '-163.59', 'logps_train/chosen': '-129.61', 'loss/train': '0.64852', 'examples_per_second': '30.518', 'grad_norm': '67', 'counters/examples': 16832, 'counters/updates': 526}
train stats after 16864 examples: {'rewards_train/chosen': '0.0051453', 'rewards_train/rejected': '0.047941', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042795', 'logps_train/rejected': '-202.64', 'logps_train/chosen': '-143.25', 'loss/train': '0.73163', 'examples_per_second': '31.581', 'grad_norm': '96.5', 'counters/examples': 16864, 'counters/updates': 527}
train stats after 16896 examples: {'rewards_train/chosen': '0.0019735', 'rewards_train/rejected': '-0.046534', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048507', 'logps_train/rejected': '-153.89', 'logps_train/chosen': '-183.59', 'loss/train': '0.68809', 'examples_per_second': '31.555', 'grad_norm': '90', 'counters/examples': 16896, 'counters/updates': 528}
train stats after 16928 examples: {'rewards_train/chosen': '0.014516', 'rewards_train/rejected': '0.01941', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0048942', 'logps_train/rejected': '-144.33', 'logps_train/chosen': '-166.96', 'loss/train': '0.70309', 'examples_per_second': '30.136', 'grad_norm': '99.5', 'counters/examples': 16928, 'counters/updates': 529}
train stats after 16960 examples: {'rewards_train/chosen': '0.038284', 'rewards_train/rejected': '-0.015859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054143', 'logps_train/rejected': '-83.074', 'logps_train/chosen': '-124.1', 'loss/train': '0.67389', 'examples_per_second': '30.74', 'grad_norm': '63.25', 'counters/examples': 16960, 'counters/updates': 530}
train stats after 16992 examples: {'rewards_train/chosen': '0.051701', 'rewards_train/rejected': '-0.0011264', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052827', 'logps_train/rejected': '-209.6', 'logps_train/chosen': '-166.69', 'loss/train': '0.68355', 'examples_per_second': '30.026', 'grad_norm': '91', 'counters/examples': 16992, 'counters/updates': 531}
train stats after 17024 examples: {'rewards_train/chosen': '0.033423', 'rewards_train/rejected': '0.019749', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013673', 'logps_train/rejected': '-144.56', 'logps_train/chosen': '-127.22', 'loss/train': '0.70272', 'examples_per_second': '31.551', 'grad_norm': '117', 'counters/examples': 17024, 'counters/updates': 532}
train stats after 17056 examples: {'rewards_train/chosen': '0.032909', 'rewards_train/rejected': '-0.022283', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055192', 'logps_train/rejected': '-156.4', 'logps_train/chosen': '-203.98', 'loss/train': '0.6829', 'examples_per_second': '31.764', 'grad_norm': '159', 'counters/examples': 17056, 'counters/updates': 533}
train stats after 17088 examples: {'rewards_train/chosen': '0.027279', 'rewards_train/rejected': '-0.017161', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04444', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-181.02', 'loss/train': '0.70453', 'examples_per_second': '30.236', 'grad_norm': '202', 'counters/examples': 17088, 'counters/updates': 534}
train stats after 17120 examples: {'rewards_train/chosen': '0.074052', 'rewards_train/rejected': '0.1051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031044', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-147.16', 'loss/train': '0.73182', 'examples_per_second': '29.459', 'grad_norm': '185', 'counters/examples': 17120, 'counters/updates': 535}
train stats after 17152 examples: {'rewards_train/chosen': '0.087236', 'rewards_train/rejected': '0.043001', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044236', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-164.61', 'loss/train': '0.6836', 'examples_per_second': '30.228', 'grad_norm': '92', 'counters/examples': 17152, 'counters/updates': 536}
skipping logging after 17184 examples to avoid logging too frequently
train stats after 17216 examples: {'rewards_train/chosen': '-0.040667', 'rewards_train/rejected': '-0.0027993', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037868', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-139.27', 'loss/train': '0.72125', 'examples_per_second': '30.636', 'grad_norm': '181', 'counters/examples': 17216, 'counters/updates': 538}
train stats after 17248 examples: {'rewards_train/chosen': '0.0012537', 'rewards_train/rejected': '-0.0066758', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0079295', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-174.53', 'loss/train': '0.7107', 'examples_per_second': '33.118', 'grad_norm': '120', 'counters/examples': 17248, 'counters/updates': 539}
train stats after 17280 examples: {'rewards_train/chosen': '-0.050356', 'rewards_train/rejected': '-0.016139', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034217', 'logps_train/rejected': '-145.58', 'logps_train/chosen': '-126.56', 'loss/train': '0.72735', 'examples_per_second': '31.677', 'grad_norm': '97', 'counters/examples': 17280, 'counters/updates': 540}
train stats after 17312 examples: {'rewards_train/chosen': '0.082689', 'rewards_train/rejected': '0.0069597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075729', 'logps_train/rejected': '-80.447', 'logps_train/chosen': '-158.62', 'loss/train': '0.66419', 'examples_per_second': '32.959', 'grad_norm': '65.5', 'counters/examples': 17312, 'counters/updates': 541}
train stats after 17344 examples: {'rewards_train/chosen': '0.016726', 'rewards_train/rejected': '0.016761', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-3.5212e-05', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-136.18', 'loss/train': '0.69665', 'examples_per_second': '32.716', 'grad_norm': '61.25', 'counters/examples': 17344, 'counters/updates': 542}
skipping logging after 17376 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '-0.027527', 'rewards_train/rejected': '-0.041228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013701', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-155.25', 'loss/train': '0.69581', 'examples_per_second': '30.455', 'grad_norm': '149', 'counters/examples': 17408, 'counters/updates': 544}
train stats after 17440 examples: {'rewards_train/chosen': '-0.027323', 'rewards_train/rejected': '0.096256', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12358', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-142.58', 'loss/train': '0.77595', 'examples_per_second': '32.046', 'grad_norm': '166', 'counters/examples': 17440, 'counters/updates': 545}
train stats after 17472 examples: {'rewards_train/chosen': '-0.0033948', 'rewards_train/rejected': '0.052992', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.056387', 'logps_train/rejected': '-117.18', 'logps_train/chosen': '-134.98', 'loss/train': '0.73368', 'examples_per_second': '30.351', 'grad_norm': '182', 'counters/examples': 17472, 'counters/updates': 546}
skipping logging after 17504 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '0.022091', 'rewards_train/rejected': '0.0072156', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014876', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-147.75', 'loss/train': '0.70363', 'examples_per_second': '32.734', 'grad_norm': '111.5', 'counters/examples': 17536, 'counters/updates': 548}
train stats after 17568 examples: {'rewards_train/chosen': '0.044804', 'rewards_train/rejected': '-0.023235', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068039', 'logps_train/rejected': '-160.15', 'logps_train/chosen': '-158.9', 'loss/train': '0.6767', 'examples_per_second': '30.551', 'grad_norm': '96', 'counters/examples': 17568, 'counters/updates': 549}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17632 examples: {'rewards_train/chosen': '0.057222', 'rewards_train/rejected': '0.028006', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029217', 'logps_train/rejected': '-155.95', 'logps_train/chosen': '-119.44', 'loss/train': '0.68804', 'examples_per_second': '34.016', 'grad_norm': '63', 'counters/examples': 17632, 'counters/updates': 551}
train stats after 17664 examples: {'rewards_train/chosen': '-0.0093347', 'rewards_train/rejected': '-0.018382', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0090476', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-144.08', 'loss/train': '0.69495', 'examples_per_second': '30.606', 'grad_norm': '108', 'counters/examples': 17664, 'counters/updates': 552}
skipping logging after 17696 examples to avoid logging too frequently
train stats after 17728 examples: {'rewards_train/chosen': '0.12623', 'rewards_train/rejected': '-0.00069905', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12693', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-152.35', 'loss/train': '0.64413', 'examples_per_second': '32.595', 'grad_norm': '88.5', 'counters/examples': 17728, 'counters/updates': 554}
train stats after 17760 examples: {'rewards_train/chosen': '0.0073751', 'rewards_train/rejected': '-0.03665', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044025', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-117.96', 'loss/train': '0.68608', 'examples_per_second': '31.566', 'grad_norm': '66.5', 'counters/examples': 17760, 'counters/updates': 555}
train stats after 17792 examples: {'rewards_train/chosen': '-0.046113', 'rewards_train/rejected': '0.053188', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.099301', 'logps_train/rejected': '-98.33', 'logps_train/chosen': '-109.81', 'loss/train': '0.75394', 'examples_per_second': '32.764', 'grad_norm': '55', 'counters/examples': 17792, 'counters/updates': 556}
train stats after 17824 examples: {'rewards_train/chosen': '0.026075', 'rewards_train/rejected': '-0.0015606', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027635', 'logps_train/rejected': '-102.86', 'logps_train/chosen': '-150.14', 'loss/train': '0.68897', 'examples_per_second': '31.671', 'grad_norm': '121.5', 'counters/examples': 17824, 'counters/updates': 557}
train stats after 17856 examples: {'rewards_train/chosen': '-0.029026', 'rewards_train/rejected': '0.086334', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11536', 'logps_train/rejected': '-151.01', 'logps_train/chosen': '-207.13', 'loss/train': '0.76881', 'examples_per_second': '31.663', 'grad_norm': '96', 'counters/examples': 17856, 'counters/updates': 558}
skipping logging after 17888 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '0.036035', 'rewards_train/rejected': '-0.025361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061396', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-123.42', 'loss/train': '0.67908', 'examples_per_second': '34.445', 'grad_norm': '66', 'counters/examples': 17920, 'counters/updates': 560}
train stats after 17952 examples: {'rewards_train/chosen': '0.015746', 'rewards_train/rejected': '-0.055588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071334', 'logps_train/rejected': '-95.653', 'logps_train/chosen': '-143.92', 'loss/train': '0.6658', 'examples_per_second': '32.571', 'grad_norm': '113.5', 'counters/examples': 17952, 'counters/updates': 561}
skipping logging after 17984 examples to avoid logging too frequently
train stats after 18016 examples: {'rewards_train/chosen': '-0.091011', 'rewards_train/rejected': '-0.074803', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016208', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-149.91', 'loss/train': '0.70915', 'examples_per_second': '32.746', 'grad_norm': '237', 'counters/examples': 18016, 'counters/updates': 563}
train stats after 18048 examples: {'rewards_train/chosen': '-0.073036', 'rewards_train/rejected': '-0.085607', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012571', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-153.95', 'loss/train': '0.69971', 'examples_per_second': '30.883', 'grad_norm': '126.5', 'counters/examples': 18048, 'counters/updates': 564}
train stats after 18080 examples: {'rewards_train/chosen': '-0.041919', 'rewards_train/rejected': '-0.01211', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029809', 'logps_train/rejected': '-167.92', 'logps_train/chosen': '-149.03', 'loss/train': '0.71826', 'examples_per_second': '30.956', 'grad_norm': '93', 'counters/examples': 18080, 'counters/updates': 565}
train stats after 18112 examples: {'rewards_train/chosen': '-0.01273', 'rewards_train/rejected': '0.00094518', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013676', 'logps_train/rejected': '-113.89', 'logps_train/chosen': '-119.42', 'loss/train': '0.70707', 'examples_per_second': '32.862', 'grad_norm': '72.5', 'counters/examples': 18112, 'counters/updates': 566}
train stats after 18144 examples: {'rewards_train/chosen': '-0.024117', 'rewards_train/rejected': '0.011344', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035461', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-118.55', 'loss/train': '0.7268', 'examples_per_second': '30.203', 'grad_norm': '106.5', 'counters/examples': 18144, 'counters/updates': 567}
skipping logging after 18176 examples to avoid logging too frequently
train stats after 18208 examples: {'rewards_train/chosen': '0.014268', 'rewards_train/rejected': '-0.025979', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040247', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-141.8', 'loss/train': '0.68418', 'examples_per_second': '37.49', 'grad_norm': '71', 'counters/examples': 18208, 'counters/updates': 569}
skipping logging after 18240 examples to avoid logging too frequently
train stats after 18272 examples: {'rewards_train/chosen': '-0.047061', 'rewards_train/rejected': '-0.0895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042438', 'logps_train/rejected': '-146.1', 'logps_train/chosen': '-175.46', 'loss/train': '0.69153', 'examples_per_second': '30.826', 'grad_norm': '85', 'counters/examples': 18272, 'counters/updates': 571}
train stats after 18304 examples: {'rewards_train/chosen': '-0.010366', 'rewards_train/rejected': '-0.028292', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017926', 'logps_train/rejected': '-149.91', 'logps_train/chosen': '-182.64', 'loss/train': '0.69747', 'examples_per_second': '17.75', 'grad_norm': '104.5', 'counters/examples': 18304, 'counters/updates': 572}
train stats after 18336 examples: {'rewards_train/chosen': '-0.068603', 'rewards_train/rejected': '-0.046701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.021902', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-151.29', 'loss/train': '0.71928', 'examples_per_second': '31.626', 'grad_norm': '102', 'counters/examples': 18336, 'counters/updates': 573}
train stats after 18368 examples: {'rewards_train/chosen': '0.014773', 'rewards_train/rejected': '-0.00059582', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.015369', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-143.58', 'loss/train': '0.69198', 'examples_per_second': '31.661', 'grad_norm': '82', 'counters/examples': 18368, 'counters/updates': 574}
skipping logging after 18400 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '0.043697', 'rewards_train/rejected': '-0.0084101', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052107', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-156.48', 'loss/train': '0.67543', 'examples_per_second': '30.566', 'grad_norm': '83', 'counters/examples': 18432, 'counters/updates': 576}
skipping logging after 18464 examples to avoid logging too frequently
train stats after 18496 examples: {'rewards_train/chosen': '-0.013126', 'rewards_train/rejected': '0.012693', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025819', 'logps_train/rejected': '-152.85', 'logps_train/chosen': '-137.72', 'loss/train': '0.71942', 'examples_per_second': '31.817', 'grad_norm': '102.5', 'counters/examples': 18496, 'counters/updates': 578}
train stats after 18528 examples: {'rewards_train/chosen': '0.030174', 'rewards_train/rejected': '0.0040044', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.026169', 'logps_train/rejected': '-138.02', 'logps_train/chosen': '-218.73', 'loss/train': '0.69145', 'examples_per_second': '31.644', 'grad_norm': '202', 'counters/examples': 18528, 'counters/updates': 579}
train stats after 18560 examples: {'rewards_train/chosen': '-0.067748', 'rewards_train/rejected': '-0.021726', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.046023', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-131.57', 'loss/train': '0.72849', 'examples_per_second': '30.815', 'grad_norm': '100.5', 'counters/examples': 18560, 'counters/updates': 580}
train stats after 18592 examples: {'rewards_train/chosen': '-0.040879', 'rewards_train/rejected': '-0.020289', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02059', 'logps_train/rejected': '-84.866', 'logps_train/chosen': '-128.48', 'loss/train': '0.71156', 'examples_per_second': '32.724', 'grad_norm': '46', 'counters/examples': 18592, 'counters/updates': 581}
train stats after 18624 examples: {'rewards_train/chosen': '0.055373', 'rewards_train/rejected': '-0.0077295', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063103', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-184.5', 'loss/train': '0.677', 'examples_per_second': '31.596', 'grad_norm': '72', 'counters/examples': 18624, 'counters/updates': 582}
skipping logging after 18656 examples to avoid logging too frequently
train stats after 18688 examples: {'rewards_train/chosen': '-0.12781', 'rewards_train/rejected': '-0.051618', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.076194', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-159.33', 'loss/train': '0.74884', 'examples_per_second': '28.108', 'grad_norm': '72', 'counters/examples': 18688, 'counters/updates': 584}
skipping logging after 18720 examples to avoid logging too frequently
train stats after 18752 examples: {'rewards_train/chosen': '-0.038112', 'rewards_train/rejected': '0.015442', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053554', 'logps_train/rejected': '-150.25', 'logps_train/chosen': '-164.71', 'loss/train': '0.73354', 'examples_per_second': '30.661', 'grad_norm': '111', 'counters/examples': 18752, 'counters/updates': 586}
train stats after 18784 examples: {'rewards_train/chosen': '-0.093039', 'rewards_train/rejected': '-0.058411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034628', 'logps_train/rejected': '-151.76', 'logps_train/chosen': '-154.69', 'loss/train': '0.71453', 'examples_per_second': '31.677', 'grad_norm': '70', 'counters/examples': 18784, 'counters/updates': 587}
train stats after 18816 examples: {'rewards_train/chosen': '0.034031', 'rewards_train/rejected': '-0.054848', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088879', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-159.71', 'loss/train': '0.66184', 'examples_per_second': '30.168', 'grad_norm': '77', 'counters/examples': 18816, 'counters/updates': 588}
skipping logging after 18848 examples to avoid logging too frequently
train stats after 18880 examples: {'rewards_train/chosen': '-0.012263', 'rewards_train/rejected': '-0.012625', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00036212', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-190.51', 'loss/train': '0.71715', 'examples_per_second': '33.03', 'grad_norm': '84.5', 'counters/examples': 18880, 'counters/updates': 590}
train stats after 18912 examples: {'rewards_train/chosen': '-0.00075073', 'rewards_train/rejected': '-0.028247', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.027497', 'logps_train/rejected': '-102.84', 'logps_train/chosen': '-140.53', 'loss/train': '0.68749', 'examples_per_second': '30.167', 'grad_norm': '69.5', 'counters/examples': 18912, 'counters/updates': 591}
train stats after 18944 examples: {'rewards_train/chosen': '-0.079101', 'rewards_train/rejected': '-0.0030722', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.076029', 'logps_train/rejected': '-156.92', 'logps_train/chosen': '-149.76', 'loss/train': '0.74217', 'examples_per_second': '31.64', 'grad_norm': '93.5', 'counters/examples': 18944, 'counters/updates': 592}
train stats after 18976 examples: {'rewards_train/chosen': '0.060301', 'rewards_train/rejected': '0.014619', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045683', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-152.14', 'loss/train': '0.68033', 'examples_per_second': '32.305', 'grad_norm': '63', 'counters/examples': 18976, 'counters/updates': 593}
train stats after 19008 examples: {'rewards_train/chosen': '0.012639', 'rewards_train/rejected': '0.012329', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00031033', 'logps_train/rejected': '-147.3', 'logps_train/chosen': '-171.95', 'loss/train': '0.71442', 'examples_per_second': '31.675', 'grad_norm': '99.5', 'counters/examples': 19008, 'counters/updates': 594}
skipping logging after 19040 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '0.00044285', 'rewards_train/rejected': '-0.094566', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095009', 'logps_train/rejected': '-148.89', 'logps_train/chosen': '-179.11', 'loss/train': '0.65912', 'examples_per_second': '23.627', 'grad_norm': '88.5', 'counters/examples': 19072, 'counters/updates': 596}
skipping logging after 19104 examples to avoid logging too frequently
train stats after 19136 examples: {'rewards_train/chosen': '-0.05143', 'rewards_train/rejected': '-0.081256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029826', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-144.41', 'loss/train': '0.69267', 'examples_per_second': '32.113', 'grad_norm': '77', 'counters/examples': 19136, 'counters/updates': 598}
train stats after 19168 examples: {'rewards_train/chosen': '0.03386', 'rewards_train/rejected': '-0.053092', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086952', 'logps_train/rejected': '-158.22', 'logps_train/chosen': '-162.04', 'loss/train': '0.6649', 'examples_per_second': '23.744', 'grad_norm': '61.25', 'counters/examples': 19168, 'counters/updates': 599}
train stats after 19200 examples: {'rewards_train/chosen': '0.024432', 'rewards_train/rejected': '-0.016219', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040651', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-187.15', 'loss/train': '0.69584', 'examples_per_second': '30.139', 'grad_norm': '163', 'counters/examples': 19200, 'counters/updates': 600}
skipping logging after 19232 examples to avoid logging too frequently
train stats after 19264 examples: {'rewards_train/chosen': '0.0032699', 'rewards_train/rejected': '-0.064188', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067458', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-155.9', 'loss/train': '0.67086', 'examples_per_second': '33.093', 'grad_norm': '112', 'counters/examples': 19264, 'counters/updates': 602}
train stats after 19296 examples: {'rewards_train/chosen': '0.023865', 'rewards_train/rejected': '-0.025503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049367', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-175.41', 'loss/train': '0.68101', 'examples_per_second': '31.602', 'grad_norm': '105', 'counters/examples': 19296, 'counters/updates': 603}
train stats after 19328 examples: {'rewards_train/chosen': '0.041018', 'rewards_train/rejected': '0.047203', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0061849', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-155.58', 'loss/train': '0.71553', 'examples_per_second': '32.332', 'grad_norm': '122.5', 'counters/examples': 19328, 'counters/updates': 604}
train stats after 19360 examples: {'rewards_train/chosen': '0.030618', 'rewards_train/rejected': '-0.035635', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066253', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-141.65', 'loss/train': '0.68057', 'examples_per_second': '30.203', 'grad_norm': '126', 'counters/examples': 19360, 'counters/updates': 605}
train stats after 19392 examples: {'rewards_train/chosen': '-0.012932', 'rewards_train/rejected': '0.034462', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047394', 'logps_train/rejected': '-161.79', 'logps_train/chosen': '-147.22', 'loss/train': '0.73028', 'examples_per_second': '31.572', 'grad_norm': '109', 'counters/examples': 19392, 'counters/updates': 606}
train stats after 19424 examples: {'rewards_train/chosen': '-0.052602', 'rewards_train/rejected': '-0.061807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0092047', 'logps_train/rejected': '-149.06', 'logps_train/chosen': '-149.83', 'loss/train': '0.71124', 'examples_per_second': '30.792', 'grad_norm': '111.5', 'counters/examples': 19424, 'counters/updates': 607}
skipping logging after 19456 examples to avoid logging too frequently
train stats after 19488 examples: {'rewards_train/chosen': '0.063553', 'rewards_train/rejected': '-0.064654', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12821', 'logps_train/rejected': '-175.55', 'logps_train/chosen': '-162.49', 'loss/train': '0.64395', 'examples_per_second': '33.265', 'grad_norm': '69', 'counters/examples': 19488, 'counters/updates': 609}
train stats after 19520 examples: {'rewards_train/chosen': '0.015414', 'rewards_train/rejected': '0.025887', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010474', 'logps_train/rejected': '-121.4', 'logps_train/chosen': '-143.36', 'loss/train': '0.70654', 'examples_per_second': '30.918', 'grad_norm': '138', 'counters/examples': 19520, 'counters/updates': 610}
train stats after 19552 examples: {'rewards_train/chosen': '-0.0034745', 'rewards_train/rejected': '-0.057849', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054375', 'logps_train/rejected': '-146.24', 'logps_train/chosen': '-169.73', 'loss/train': '0.67954', 'examples_per_second': '32.565', 'grad_norm': '108.5', 'counters/examples': 19552, 'counters/updates': 611}
train stats after 19584 examples: {'rewards_train/chosen': '-0.011755', 'rewards_train/rejected': '-0.0050714', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0066834', 'logps_train/rejected': '-173.24', 'logps_train/chosen': '-167.28', 'loss/train': '0.71479', 'examples_per_second': '31.556', 'grad_norm': '169', 'counters/examples': 19584, 'counters/updates': 612}
train stats after 19616 examples: {'rewards_train/chosen': '-0.040527', 'rewards_train/rejected': '0.0085381', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049065', 'logps_train/rejected': '-83.342', 'logps_train/chosen': '-120.15', 'loss/train': '0.72659', 'examples_per_second': '31.843', 'grad_norm': '75.5', 'counters/examples': 19616, 'counters/updates': 613}
train stats after 19648 examples: {'rewards_train/chosen': '-0.034946', 'rewards_train/rejected': '0.025362', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.060308', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-160.1', 'loss/train': '0.73764', 'examples_per_second': '30.581', 'grad_norm': '59', 'counters/examples': 19648, 'counters/updates': 614}
train stats after 19680 examples: {'rewards_train/chosen': '-0.074779', 'rewards_train/rejected': '-0.030139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.04464', 'logps_train/rejected': '-147.96', 'logps_train/chosen': '-172.81', 'loss/train': '0.73432', 'examples_per_second': '31.32', 'grad_norm': '123', 'counters/examples': 19680, 'counters/updates': 615}
train stats after 19712 examples: {'rewards_train/chosen': '0.0082766', 'rewards_train/rejected': '0.077811', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.069535', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-126.51', 'loss/train': '0.74434', 'examples_per_second': '31.609', 'grad_norm': '96', 'counters/examples': 19712, 'counters/updates': 616}
train stats after 19744 examples: {'rewards_train/chosen': '-0.062873', 'rewards_train/rejected': '0.016079', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.078952', 'logps_train/rejected': '-131.5', 'logps_train/chosen': '-133.88', 'loss/train': '0.74485', 'examples_per_second': '31.875', 'grad_norm': '106.5', 'counters/examples': 19744, 'counters/updates': 617}
train stats after 19776 examples: {'rewards_train/chosen': '0.084549', 'rewards_train/rejected': '0.0038018', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080747', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-145.86', 'loss/train': '0.66973', 'examples_per_second': '30.708', 'grad_norm': '66', 'counters/examples': 19776, 'counters/updates': 618}
train stats after 19808 examples: {'rewards_train/chosen': '0.0036352', 'rewards_train/rejected': '0.010884', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0072492', 'logps_train/rejected': '-138.85', 'logps_train/chosen': '-144.32', 'loss/train': '0.71065', 'examples_per_second': '31.567', 'grad_norm': '79', 'counters/examples': 19808, 'counters/updates': 619}
train stats after 19840 examples: {'rewards_train/chosen': '-0.03403', 'rewards_train/rejected': '-0.0061702', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02786', 'logps_train/rejected': '-145.86', 'logps_train/chosen': '-177.17', 'loss/train': '0.72768', 'examples_per_second': '32.738', 'grad_norm': '75', 'counters/examples': 19840, 'counters/updates': 620}
skipping logging after 19872 examples to avoid logging too frequently
train stats after 19904 examples: {'rewards_train/chosen': '0.010771', 'rewards_train/rejected': '-0.022668', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033439', 'logps_train/rejected': '-168.67', 'logps_train/chosen': '-178.32', 'loss/train': '0.70054', 'examples_per_second': '30.111', 'grad_norm': '100', 'counters/examples': 19904, 'counters/updates': 622}
train stats after 19936 examples: {'rewards_train/chosen': '0.018998', 'rewards_train/rejected': '0.0083359', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010662', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-180.36', 'loss/train': '0.70456', 'examples_per_second': '32.164', 'grad_norm': '82.5', 'counters/examples': 19936, 'counters/updates': 623}
skipping logging after 19968 examples to avoid logging too frequently
train stats after 20000 examples: {'rewards_train/chosen': '0.038839', 'rewards_train/rejected': '0.020831', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018008', 'logps_train/rejected': '-140.6', 'logps_train/chosen': '-199.58', 'loss/train': '0.69908', 'examples_per_second': '30.667', 'grad_norm': '78', 'counters/examples': 20000, 'counters/updates': 625}
train stats after 20032 examples: {'rewards_train/chosen': '0.024402', 'rewards_train/rejected': '-0.0056097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030011', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-158.21', 'loss/train': '0.68719', 'examples_per_second': '30.252', 'grad_norm': '79', 'counters/examples': 20032, 'counters/updates': 626}
skipping logging after 20064 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '0.012859', 'rewards_train/rejected': '-0.049505', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062364', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-177.52', 'loss/train': '0.67351', 'examples_per_second': '34.93', 'grad_norm': '53.75', 'counters/examples': 20096, 'counters/updates': 628}
train stats after 20128 examples: {'rewards_train/chosen': '0.074987', 'rewards_train/rejected': '0.018582', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056405', 'logps_train/rejected': '-167.22', 'logps_train/chosen': '-144.94', 'loss/train': '0.6739', 'examples_per_second': '31.671', 'grad_norm': '96.5', 'counters/examples': 20128, 'counters/updates': 629}
train stats after 20160 examples: {'rewards_train/chosen': '-0.040249', 'rewards_train/rejected': '-0.022837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017412', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-168.39', 'loss/train': '0.71382', 'examples_per_second': '31.811', 'grad_norm': '56.5', 'counters/examples': 20160, 'counters/updates': 630}
train stats after 20192 examples: {'rewards_train/chosen': '-0.065653', 'rewards_train/rejected': '0.060239', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12589', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-138.41', 'loss/train': '0.78524', 'examples_per_second': '30.982', 'grad_norm': '89.5', 'counters/examples': 20192, 'counters/updates': 631}
train stats after 20224 examples: {'rewards_train/chosen': '-0.026004', 'rewards_train/rejected': '0.021344', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047348', 'logps_train/rejected': '-148.46', 'logps_train/chosen': '-214.6', 'loss/train': '0.73629', 'examples_per_second': '31.667', 'grad_norm': '72', 'counters/examples': 20224, 'counters/updates': 632}
train stats after 20256 examples: {'rewards_train/chosen': '-0.051417', 'rewards_train/rejected': '0.0028254', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054243', 'logps_train/rejected': '-176.4', 'logps_train/chosen': '-205.76', 'loss/train': '0.74769', 'examples_per_second': '31.307', 'grad_norm': '139', 'counters/examples': 20256, 'counters/updates': 633}
train stats after 20288 examples: {'rewards_train/chosen': '0.011497', 'rewards_train/rejected': '-0.025984', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037481', 'logps_train/rejected': '-112.9', 'logps_train/chosen': '-140.43', 'loss/train': '0.68077', 'examples_per_second': '30.157', 'grad_norm': '56', 'counters/examples': 20288, 'counters/updates': 634}
train stats after 20320 examples: {'rewards_train/chosen': '0.11719', 'rewards_train/rejected': '0.01162', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10557', 'logps_train/rejected': '-99.302', 'logps_train/chosen': '-131.92', 'loss/train': '0.66297', 'examples_per_second': '30.861', 'grad_norm': '168', 'counters/examples': 20320, 'counters/updates': 635}
train stats after 20352 examples: {'rewards_train/chosen': '0.013268', 'rewards_train/rejected': '0.061977', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.048708', 'logps_train/rejected': '-148.05', 'logps_train/chosen': '-170.14', 'loss/train': '0.72516', 'examples_per_second': '31.424', 'grad_norm': '135', 'counters/examples': 20352, 'counters/updates': 636}
train stats after 20384 examples: {'rewards_train/chosen': '-0.062544', 'rewards_train/rejected': '0.047564', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.11011', 'logps_train/rejected': '-174.32', 'logps_train/chosen': '-164.87', 'loss/train': '0.7596', 'examples_per_second': '30.94', 'grad_norm': '93', 'counters/examples': 20384, 'counters/updates': 637}
train stats after 20416 examples: {'rewards_train/chosen': '-0.078339', 'rewards_train/rejected': '0.019856', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.098195', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-162.54', 'loss/train': '0.75389', 'examples_per_second': '31.035', 'grad_norm': '110', 'counters/examples': 20416, 'counters/updates': 638}
train stats after 20448 examples: {'rewards_train/chosen': '-0.034132', 'rewards_train/rejected': '-0.01108', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023052', 'logps_train/rejected': '-150.75', 'logps_train/chosen': '-147.76', 'loss/train': '0.72065', 'examples_per_second': '31.655', 'grad_norm': '78.5', 'counters/examples': 20448, 'counters/updates': 639}
train stats after 20480 examples: {'rewards_train/chosen': '0.062023', 'rewards_train/rejected': '0.019225', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042798', 'logps_train/rejected': '-192.35', 'logps_train/chosen': '-163.74', 'loss/train': '0.69055', 'examples_per_second': '31.661', 'grad_norm': '159', 'counters/examples': 20480, 'counters/updates': 640}
train stats after 20512 examples: {'rewards_train/chosen': '-0.046074', 'rewards_train/rejected': '0.010786', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05686', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-159.32', 'loss/train': '0.73467', 'examples_per_second': '31.677', 'grad_norm': '90.5', 'counters/examples': 20512, 'counters/updates': 641}
train stats after 20544 examples: {'rewards_train/chosen': '-0.017587', 'rewards_train/rejected': '0.050537', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068124', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-152.25', 'loss/train': '0.74266', 'examples_per_second': '30.981', 'grad_norm': '82', 'counters/examples': 20544, 'counters/updates': 642}
train stats after 20576 examples: {'rewards_train/chosen': '0.016569', 'rewards_train/rejected': '0.012871', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0036977', 'logps_train/rejected': '-164.07', 'logps_train/chosen': '-198.35', 'loss/train': '0.70509', 'examples_per_second': '31.588', 'grad_norm': '210', 'counters/examples': 20576, 'counters/updates': 643}
train stats after 20608 examples: {'rewards_train/chosen': '0.047821', 'rewards_train/rejected': '0.022291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02553', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-188.11', 'loss/train': '0.69331', 'examples_per_second': '33.018', 'grad_norm': '104.5', 'counters/examples': 20608, 'counters/updates': 644}
skipping logging after 20640 examples to avoid logging too frequently
train stats after 20672 examples: {'rewards_train/chosen': '0.033988', 'rewards_train/rejected': '-0.017196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051183', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-146.46', 'loss/train': '0.68098', 'examples_per_second': '30.235', 'grad_norm': '58', 'counters/examples': 20672, 'counters/updates': 646}
train stats after 20704 examples: {'rewards_train/chosen': '0.024701', 'rewards_train/rejected': '0.042286', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017585', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-161.87', 'loss/train': '0.70895', 'examples_per_second': '31.626', 'grad_norm': '86', 'counters/examples': 20704, 'counters/updates': 647}
train stats after 20736 examples: {'rewards_train/chosen': '0.11152', 'rewards_train/rejected': '-0.034906', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14642', 'logps_train/rejected': '-151.91', 'logps_train/chosen': '-209.64', 'loss/train': '0.64535', 'examples_per_second': '30.301', 'grad_norm': '136', 'counters/examples': 20736, 'counters/updates': 648}
train stats after 20768 examples: {'rewards_train/chosen': '-0.053846', 'rewards_train/rejected': '-0.028626', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.025221', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-135.1', 'loss/train': '0.70921', 'examples_per_second': '31.007', 'grad_norm': '290', 'counters/examples': 20768, 'counters/updates': 649}
train stats after 20800 examples: {'rewards_train/chosen': '0.0099579', 'rewards_train/rejected': '0.026915', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016957', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-121.3', 'loss/train': '0.71175', 'examples_per_second': '31.004', 'grad_norm': '102', 'counters/examples': 20800, 'counters/updates': 650}
train stats after 20832 examples: {'rewards_train/chosen': '0.0041071', 'rewards_train/rejected': '0.073231', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.069124', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-136.95', 'loss/train': '0.7338', 'examples_per_second': '30.131', 'grad_norm': '82', 'counters/examples': 20832, 'counters/updates': 651}
train stats after 20864 examples: {'rewards_train/chosen': '-0.055721', 'rewards_train/rejected': '-0.011533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.044188', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-169.38', 'loss/train': '0.73782', 'examples_per_second': '33.123', 'grad_norm': '64', 'counters/examples': 20864, 'counters/updates': 652}
train stats after 20896 examples: {'rewards_train/chosen': '0.009327', 'rewards_train/rejected': '0.013203', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0038763', 'logps_train/rejected': '-109.73', 'logps_train/chosen': '-170.89', 'loss/train': '0.7062', 'examples_per_second': '32.304', 'grad_norm': '76.5', 'counters/examples': 20896, 'counters/updates': 653}
train stats after 20928 examples: {'rewards_train/chosen': '0.058245', 'rewards_train/rejected': '0.0099336', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048312', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-149.38', 'loss/train': '0.68836', 'examples_per_second': '31.28', 'grad_norm': '183', 'counters/examples': 20928, 'counters/updates': 654}
train stats after 20960 examples: {'rewards_train/chosen': '0.027662', 'rewards_train/rejected': '0.0017406', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.025922', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-156.22', 'loss/train': '0.68822', 'examples_per_second': '31.188', 'grad_norm': '88', 'counters/examples': 20960, 'counters/updates': 655}
train stats after 20992 examples: {'rewards_train/chosen': '0.012944', 'rewards_train/rejected': '-0.042752', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055696', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-165.35', 'loss/train': '0.68575', 'examples_per_second': '31.593', 'grad_norm': '138', 'counters/examples': 20992, 'counters/updates': 656}
train stats after 21024 examples: {'rewards_train/chosen': '0.060108', 'rewards_train/rejected': '-0.030342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090451', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-129.57', 'loss/train': '0.66188', 'examples_per_second': '31.133', 'grad_norm': '79.5', 'counters/examples': 21024, 'counters/updates': 657}
train stats after 21056 examples: {'rewards_train/chosen': '-0.21771', 'rewards_train/rejected': '0.017232', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.23494', 'logps_train/rejected': '-144.02', 'logps_train/chosen': '-205.11', 'loss/train': '0.88417', 'examples_per_second': '31.429', 'grad_norm': '282', 'counters/examples': 21056, 'counters/updates': 658}
train stats after 21088 examples: {'rewards_train/chosen': '0.013852', 'rewards_train/rejected': '0.019629', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0057766', 'logps_train/rejected': '-140.48', 'logps_train/chosen': '-128.73', 'loss/train': '0.70527', 'examples_per_second': '31.532', 'grad_norm': '146', 'counters/examples': 21088, 'counters/updates': 659}
train stats after 21120 examples: {'rewards_train/chosen': '-0.040285', 'rewards_train/rejected': '0.033867', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.074152', 'logps_train/rejected': '-146.56', 'logps_train/chosen': '-152.09', 'loss/train': '0.75809', 'examples_per_second': '31.444', 'grad_norm': '167', 'counters/examples': 21120, 'counters/updates': 660}
train stats after 21152 examples: {'rewards_train/chosen': '0.0063935', 'rewards_train/rejected': '-0.019999', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026392', 'logps_train/rejected': '-123.78', 'logps_train/chosen': '-147.11', 'loss/train': '0.69089', 'examples_per_second': '31.796', 'grad_norm': '95.5', 'counters/examples': 21152, 'counters/updates': 661}
train stats after 21184 examples: {'rewards_train/chosen': '-0.033449', 'rewards_train/rejected': '-0.021954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.011496', 'logps_train/rejected': '-138.39', 'logps_train/chosen': '-143.57', 'loss/train': '0.71506', 'examples_per_second': '30.379', 'grad_norm': '121.5', 'counters/examples': 21184, 'counters/updates': 662}
skipping logging after 21216 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '-0.005751', 'rewards_train/rejected': '0.0087982', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014549', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-124.55', 'loss/train': '0.71179', 'examples_per_second': '32.081', 'grad_norm': '76', 'counters/examples': 21248, 'counters/updates': 664}
train stats after 21280 examples: {'rewards_train/chosen': '0.033737', 'rewards_train/rejected': '0.039994', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0062577', 'logps_train/rejected': '-143.04', 'logps_train/chosen': '-156.16', 'loss/train': '0.7093', 'examples_per_second': '30.755', 'grad_norm': '87', 'counters/examples': 21280, 'counters/updates': 665}
train stats after 21312 examples: {'rewards_train/chosen': '0.013062', 'rewards_train/rejected': '-0.051972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065034', 'logps_train/rejected': '-155.53', 'logps_train/chosen': '-149.82', 'loss/train': '0.676', 'examples_per_second': '32.79', 'grad_norm': '108.5', 'counters/examples': 21312, 'counters/updates': 666}
train stats after 21344 examples: {'rewards_train/chosen': '0.072291', 'rewards_train/rejected': '-0.029903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10219', 'logps_train/rejected': '-143.61', 'logps_train/chosen': '-151.53', 'loss/train': '0.65073', 'examples_per_second': '30.567', 'grad_norm': '75', 'counters/examples': 21344, 'counters/updates': 667}
train stats after 21376 examples: {'rewards_train/chosen': '0.0030934', 'rewards_train/rejected': '-0.0041387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0072321', 'logps_train/rejected': '-175.28', 'logps_train/chosen': '-144.37', 'loss/train': '0.70128', 'examples_per_second': '31.662', 'grad_norm': '72.5', 'counters/examples': 21376, 'counters/updates': 668}
train stats after 21408 examples: {'rewards_train/chosen': '-0.021545', 'rewards_train/rejected': '0.0093988', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030944', 'logps_train/rejected': '-105.38', 'logps_train/chosen': '-154.92', 'loss/train': '0.72173', 'examples_per_second': '31.136', 'grad_norm': '181', 'counters/examples': 21408, 'counters/updates': 669}
train stats after 21440 examples: {'rewards_train/chosen': '0.015965', 'rewards_train/rejected': '0.070487', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.054523', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-180.12', 'loss/train': '0.73442', 'examples_per_second': '31.452', 'grad_norm': '174', 'counters/examples': 21440, 'counters/updates': 670}
train stats after 21472 examples: {'rewards_train/chosen': '0.0037351', 'rewards_train/rejected': '0.00058849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0031466', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-157.92', 'loss/train': '0.70352', 'examples_per_second': '31.548', 'grad_norm': '73.5', 'counters/examples': 21472, 'counters/updates': 671}
train stats after 21504 examples: {'rewards_train/chosen': '0.0044177', 'rewards_train/rejected': '0.013719', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009301', 'logps_train/rejected': '-141.17', 'logps_train/chosen': '-183.6', 'loss/train': '0.71183', 'examples_per_second': '31.41', 'grad_norm': '96', 'counters/examples': 21504, 'counters/updates': 672}
train stats after 21536 examples: {'rewards_train/chosen': '0.0048315', 'rewards_train/rejected': '0.016387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.011556', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-170.73', 'loss/train': '0.70881', 'examples_per_second': '31.353', 'grad_norm': '84.5', 'counters/examples': 21536, 'counters/updates': 673}
train stats after 21568 examples: {'rewards_train/chosen': '-0.028808', 'rewards_train/rejected': '0.020995', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049804', 'logps_train/rejected': '-136.74', 'logps_train/chosen': '-152.27', 'loss/train': '0.72605', 'examples_per_second': '29.294', 'grad_norm': '74.5', 'counters/examples': 21568, 'counters/updates': 674}
train stats after 21600 examples: {'rewards_train/chosen': '0.010758', 'rewards_train/rejected': '0.013624', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0028663', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-165.95', 'loss/train': '0.70552', 'examples_per_second': '31.143', 'grad_norm': '130', 'counters/examples': 21600, 'counters/updates': 675}
train stats after 21632 examples: {'rewards_train/chosen': '-0.066338', 'rewards_train/rejected': '-0.047721', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018617', 'logps_train/rejected': '-102.94', 'logps_train/chosen': '-124.28', 'loss/train': '0.71121', 'examples_per_second': '31.346', 'grad_norm': '56.75', 'counters/examples': 21632, 'counters/updates': 676}
train stats after 21664 examples: {'rewards_train/chosen': '0.064417', 'rewards_train/rejected': '0.0030908', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061326', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-199.44', 'loss/train': '0.67884', 'examples_per_second': '31.532', 'grad_norm': '70', 'counters/examples': 21664, 'counters/updates': 677}
train stats after 21696 examples: {'rewards_train/chosen': '-0.054626', 'rewards_train/rejected': '-0.0071878', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.047438', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-151.67', 'loss/train': '0.73386', 'examples_per_second': '30.6', 'grad_norm': '85', 'counters/examples': 21696, 'counters/updates': 678}
train stats after 21728 examples: {'rewards_train/chosen': '0.16211', 'rewards_train/rejected': '0.10279', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059318', 'logps_train/rejected': '-167.88', 'logps_train/chosen': '-176.46', 'loss/train': '0.69806', 'examples_per_second': '31.943', 'grad_norm': '78.5', 'counters/examples': 21728, 'counters/updates': 679}
skipping logging after 21760 examples to avoid logging too frequently
train stats after 21792 examples: {'rewards_train/chosen': '-0.05581', 'rewards_train/rejected': '-0.048955', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0068551', 'logps_train/rejected': '-128', 'logps_train/chosen': '-176.83', 'loss/train': '0.70758', 'examples_per_second': '30.448', 'grad_norm': '78.5', 'counters/examples': 21792, 'counters/updates': 681}
train stats after 21824 examples: {'rewards_train/chosen': '0.035901', 'rewards_train/rejected': '0.046468', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010566', 'logps_train/rejected': '-125.76', 'logps_train/chosen': '-157.83', 'loss/train': '0.71552', 'examples_per_second': '30.807', 'grad_norm': '103.5', 'counters/examples': 21824, 'counters/updates': 682}
train stats after 21856 examples: {'rewards_train/chosen': '0.026791', 'rewards_train/rejected': '-0.0079636', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034755', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-134.95', 'loss/train': '0.68658', 'examples_per_second': '31.715', 'grad_norm': '54.25', 'counters/examples': 21856, 'counters/updates': 683}
train stats after 21888 examples: {'rewards_train/chosen': '0.018997', 'rewards_train/rejected': '0.019218', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.00022193', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-143.21', 'loss/train': '0.707', 'examples_per_second': '33.078', 'grad_norm': '86', 'counters/examples': 21888, 'counters/updates': 684}
train stats after 21920 examples: {'rewards_train/chosen': '0.058739', 'rewards_train/rejected': '0.022984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035755', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-162.18', 'loss/train': '0.68289', 'examples_per_second': '30.919', 'grad_norm': '61', 'counters/examples': 21920, 'counters/updates': 685}
train stats after 21952 examples: {'rewards_train/chosen': '0.071538', 'rewards_train/rejected': '0.038465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033073', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-160.91', 'loss/train': '0.69156', 'examples_per_second': '30.861', 'grad_norm': '103', 'counters/examples': 21952, 'counters/updates': 686}
train stats after 21984 examples: {'rewards_train/chosen': '-0.06524', 'rewards_train/rejected': '-0.10323', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037992', 'logps_train/rejected': '-165.38', 'logps_train/chosen': '-173.98', 'loss/train': '0.68298', 'examples_per_second': '31.3', 'grad_norm': '338', 'counters/examples': 21984, 'counters/updates': 687}
train stats after 22016 examples: {'rewards_train/chosen': '-0.058892', 'rewards_train/rejected': '-0.03673', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.022161', 'logps_train/rejected': '-155.69', 'logps_train/chosen': '-114.49', 'loss/train': '0.71505', 'examples_per_second': '30.962', 'grad_norm': '97', 'counters/examples': 22016, 'counters/updates': 688}
train stats after 22048 examples: {'rewards_train/chosen': '0.027086', 'rewards_train/rejected': '0.045212', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018126', 'logps_train/rejected': '-104.42', 'logps_train/chosen': '-134.53', 'loss/train': '0.71795', 'examples_per_second': '30.699', 'grad_norm': '65.5', 'counters/examples': 22048, 'counters/updates': 689}
train stats after 22080 examples: {'rewards_train/chosen': '-0.0052523', 'rewards_train/rejected': '0.015488', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.020741', 'logps_train/rejected': '-186.71', 'logps_train/chosen': '-183.42', 'loss/train': '0.72148', 'examples_per_second': '31.502', 'grad_norm': '118.5', 'counters/examples': 22080, 'counters/updates': 690}
skipping logging after 22112 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '-0.009307', 'rewards_train/rejected': '0.028522', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037829', 'logps_train/rejected': '-104.78', 'logps_train/chosen': '-122.98', 'loss/train': '0.72664', 'examples_per_second': '30.484', 'grad_norm': '58.25', 'counters/examples': 22144, 'counters/updates': 692}
train stats after 22176 examples: {'rewards_train/chosen': '-0.061493', 'rewards_train/rejected': '-0.007288', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054205', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-110.86', 'loss/train': '0.72615', 'examples_per_second': '31.653', 'grad_norm': '360', 'counters/examples': 22176, 'counters/updates': 693}
train stats after 22208 examples: {'rewards_train/chosen': '0.015515', 'rewards_train/rejected': '-0.096952', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11247', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-171.32', 'loss/train': '0.66312', 'examples_per_second': '31.945', 'grad_norm': '153', 'counters/examples': 22208, 'counters/updates': 694}
train stats after 22240 examples: {'rewards_train/chosen': '0.038107', 'rewards_train/rejected': '-0.034716', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072823', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-167.13', 'loss/train': '0.66791', 'examples_per_second': '31.922', 'grad_norm': '158', 'counters/examples': 22240, 'counters/updates': 695}
train stats after 22272 examples: {'rewards_train/chosen': '0.007918', 'rewards_train/rejected': '0.060746', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.052828', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-116.53', 'loss/train': '0.73429', 'examples_per_second': '30.139', 'grad_norm': '57.25', 'counters/examples': 22272, 'counters/updates': 696}
train stats after 22304 examples: {'rewards_train/chosen': '-0.095909', 'rewards_train/rejected': '-0.026695', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.069214', 'logps_train/rejected': '-159.39', 'logps_train/chosen': '-199.7', 'loss/train': '0.74809', 'examples_per_second': '31.613', 'grad_norm': '115.5', 'counters/examples': 22304, 'counters/updates': 697}
train stats after 22336 examples: {'rewards_train/chosen': '0.010197', 'rewards_train/rejected': '0.044909', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034712', 'logps_train/rejected': '-131.9', 'logps_train/chosen': '-179', 'loss/train': '0.72518', 'examples_per_second': '31.616', 'grad_norm': '90.5', 'counters/examples': 22336, 'counters/updates': 698}
train stats after 22368 examples: {'rewards_train/chosen': '0.041503', 'rewards_train/rejected': '0.046799', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0052958', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-183.12', 'loss/train': '0.71052', 'examples_per_second': '32.693', 'grad_norm': '120', 'counters/examples': 22368, 'counters/updates': 699}
train stats after 22400 examples: {'rewards_train/chosen': '-0.031992', 'rewards_train/rejected': '0.03281', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.064802', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-148.89', 'loss/train': '0.73878', 'examples_per_second': '31.591', 'grad_norm': '233', 'counters/examples': 22400, 'counters/updates': 700}
train stats after 22432 examples: {'rewards_train/chosen': '0.0015058', 'rewards_train/rejected': '0.055754', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054248', 'logps_train/rejected': '-164.92', 'logps_train/chosen': '-164.94', 'loss/train': '0.73487', 'examples_per_second': '31.595', 'grad_norm': '784', 'counters/examples': 22432, 'counters/updates': 701}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22496 examples: {'rewards_train/chosen': '0.023821', 'rewards_train/rejected': '-0.015132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038953', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-187.6', 'loss/train': '0.68968', 'examples_per_second': '31.595', 'grad_norm': '127.5', 'counters/examples': 22496, 'counters/updates': 703}
train stats after 22528 examples: {'rewards_train/chosen': '0.007193', 'rewards_train/rejected': '0.023757', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016564', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-144.97', 'loss/train': '0.71225', 'examples_per_second': '31.341', 'grad_norm': '78.5', 'counters/examples': 22528, 'counters/updates': 704}
skipping logging after 22560 examples to avoid logging too frequently
train stats after 22592 examples: {'rewards_train/chosen': '0.079698', 'rewards_train/rejected': '0.0061135', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073584', 'logps_train/rejected': '-161.42', 'logps_train/chosen': '-155.52', 'loss/train': '0.67932', 'examples_per_second': '30.421', 'grad_norm': '253', 'counters/examples': 22592, 'counters/updates': 706}
train stats after 22624 examples: {'rewards_train/chosen': '0.053688', 'rewards_train/rejected': '0.062804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.009116', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-176.26', 'loss/train': '0.70815', 'examples_per_second': '31.571', 'grad_norm': '100.5', 'counters/examples': 22624, 'counters/updates': 707}
train stats after 22656 examples: {'rewards_train/chosen': '0.0011553', 'rewards_train/rejected': '0.084217', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.083062', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-143.95', 'loss/train': '0.74611', 'examples_per_second': '31.602', 'grad_norm': '66', 'counters/examples': 22656, 'counters/updates': 708}
train stats after 22688 examples: {'rewards_train/chosen': '-0.076001', 'rewards_train/rejected': '0.026139', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10214', 'logps_train/rejected': '-147.21', 'logps_train/chosen': '-167.68', 'loss/train': '0.76792', 'examples_per_second': '31.982', 'grad_norm': '82.5', 'counters/examples': 22688, 'counters/updates': 709}
train stats after 22720 examples: {'rewards_train/chosen': '-0.02082', 'rewards_train/rejected': '0.039035', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.059855', 'logps_train/rejected': '-150.54', 'logps_train/chosen': '-130.15', 'loss/train': '0.73615', 'examples_per_second': '31.609', 'grad_norm': '80.5', 'counters/examples': 22720, 'counters/updates': 710}
skipping logging after 22752 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '0.016253', 'rewards_train/rejected': '-0.033815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050067', 'logps_train/rejected': '-149.2', 'logps_train/chosen': '-165.57', 'loss/train': '0.68156', 'examples_per_second': '32.416', 'grad_norm': '76.5', 'counters/examples': 22784, 'counters/updates': 712}
train stats after 22816 examples: {'rewards_train/chosen': '0.0047975', 'rewards_train/rejected': '-0.042034', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046831', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-105.96', 'loss/train': '0.68222', 'examples_per_second': '30.793', 'grad_norm': '72.5', 'counters/examples': 22816, 'counters/updates': 713}
train stats after 22848 examples: {'rewards_train/chosen': '-0.096339', 'rewards_train/rejected': '-0.030343', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.065997', 'logps_train/rejected': '-138.45', 'logps_train/chosen': '-140.06', 'loss/train': '0.74283', 'examples_per_second': '30.128', 'grad_norm': '110', 'counters/examples': 22848, 'counters/updates': 714}
skipping logging after 22880 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '-0.081073', 'rewards_train/rejected': '-0.061919', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019154', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-166.23', 'loss/train': '0.72429', 'examples_per_second': '34.01', 'grad_norm': '115.5', 'counters/examples': 22912, 'counters/updates': 716}
skipping logging after 22944 examples to avoid logging too frequently
train stats after 22976 examples: {'rewards_train/chosen': '0.03844', 'rewards_train/rejected': '0.0065795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031861', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-135.68', 'loss/train': '0.68951', 'examples_per_second': '33.323', 'grad_norm': '77.5', 'counters/examples': 22976, 'counters/updates': 718}
train stats after 23008 examples: {'rewards_train/chosen': '0.10053', 'rewards_train/rejected': '-0.00045681', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10099', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-124.51', 'loss/train': '0.65862', 'examples_per_second': '31.438', 'grad_norm': '72', 'counters/examples': 23008, 'counters/updates': 719}
train stats after 23040 examples: {'rewards_train/chosen': '0.058993', 'rewards_train/rejected': '-0.080709', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1397', 'logps_train/rejected': '-117.57', 'logps_train/chosen': '-141.46', 'loss/train': '0.64579', 'examples_per_second': '30.255', 'grad_norm': '68.5', 'counters/examples': 23040, 'counters/updates': 720}
skipping logging after 23072 examples to avoid logging too frequently
train stats after 23104 examples: {'rewards_train/chosen': '0.037125', 'rewards_train/rejected': '0.0083075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028817', 'logps_train/rejected': '-149.64', 'logps_train/chosen': '-153.23', 'loss/train': '0.68826', 'examples_per_second': '33.829', 'grad_norm': '120', 'counters/examples': 23104, 'counters/updates': 722}
skipping logging after 23136 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '-0.015217', 'rewards_train/rejected': '-0.017231', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0020131', 'logps_train/rejected': '-154.62', 'logps_train/chosen': '-194.25', 'loss/train': '0.70142', 'examples_per_second': '31.641', 'grad_norm': '156', 'counters/examples': 23168, 'counters/updates': 724}
train stats after 23200 examples: {'rewards_train/chosen': '0.036633', 'rewards_train/rejected': '0.031115', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0055183', 'logps_train/rejected': '-142.65', 'logps_train/chosen': '-170.71', 'loss/train': '0.70706', 'examples_per_second': '31.541', 'grad_norm': '92', 'counters/examples': 23200, 'counters/updates': 725}
train stats after 23232 examples: {'rewards_train/chosen': '0.086128', 'rewards_train/rejected': '-0.0025997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088727', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-133.44', 'loss/train': '0.66818', 'examples_per_second': '31.592', 'grad_norm': '126.5', 'counters/examples': 23232, 'counters/updates': 726}
train stats after 23264 examples: {'rewards_train/chosen': '0.034046', 'rewards_train/rejected': '0.020195', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013851', 'logps_train/rejected': '-166.76', 'logps_train/chosen': '-156.28', 'loss/train': '0.6985', 'examples_per_second': '31.663', 'grad_norm': '85.5', 'counters/examples': 23264, 'counters/updates': 727}
train stats after 23296 examples: {'rewards_train/chosen': '-0.013913', 'rewards_train/rejected': '-0.057556', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043643', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-179.22', 'loss/train': '0.68366', 'examples_per_second': '30.901', 'grad_norm': '100.5', 'counters/examples': 23296, 'counters/updates': 728}
train stats after 23328 examples: {'rewards_train/chosen': '0.059716', 'rewards_train/rejected': '-0.068457', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12817', 'logps_train/rejected': '-143.72', 'logps_train/chosen': '-182.85', 'loss/train': '0.64726', 'examples_per_second': '31.572', 'grad_norm': '64', 'counters/examples': 23328, 'counters/updates': 729}
train stats after 23360 examples: {'rewards_train/chosen': '-0.062426', 'rewards_train/rejected': '0.011521', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.073947', 'logps_train/rejected': '-153.83', 'logps_train/chosen': '-143.79', 'loss/train': '0.74366', 'examples_per_second': '31.871', 'grad_norm': '89', 'counters/examples': 23360, 'counters/updates': 730}
train stats after 23392 examples: {'rewards_train/chosen': '-0.068783', 'rewards_train/rejected': '-0.076494', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0077113', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-143.05', 'loss/train': '0.70448', 'examples_per_second': '31.526', 'grad_norm': '382', 'counters/examples': 23392, 'counters/updates': 731}
train stats after 23424 examples: {'rewards_train/chosen': '0.030801', 'rewards_train/rejected': '-0.019223', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.050024', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-144.94', 'loss/train': '0.67628', 'examples_per_second': '32.108', 'grad_norm': '135', 'counters/examples': 23424, 'counters/updates': 732}
train stats after 23456 examples: {'rewards_train/chosen': '0.019876', 'rewards_train/rejected': '-0.028724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0486', 'logps_train/rejected': '-165.36', 'logps_train/chosen': '-162.31', 'loss/train': '0.68463', 'examples_per_second': '30.599', 'grad_norm': '86.5', 'counters/examples': 23456, 'counters/updates': 733}
train stats after 23488 examples: {'rewards_train/chosen': '-0.030515', 'rewards_train/rejected': '0.022015', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.052529', 'logps_train/rejected': '-176.35', 'logps_train/chosen': '-154.99', 'loss/train': '0.73312', 'examples_per_second': '31.321', 'grad_norm': '121.5', 'counters/examples': 23488, 'counters/updates': 734}
train stats after 23520 examples: {'rewards_train/chosen': '0.029396', 'rewards_train/rejected': '0.0227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0066963', 'logps_train/rejected': '-177.68', 'logps_train/chosen': '-138.89', 'loss/train': '0.70535', 'examples_per_second': '31.522', 'grad_norm': '167', 'counters/examples': 23520, 'counters/updates': 735}
skipping logging after 23552 examples to avoid logging too frequently
train stats after 23584 examples: {'rewards_train/chosen': '0.11978', 'rewards_train/rejected': '0.021671', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098105', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-185.98', 'loss/train': '0.66059', 'examples_per_second': '30.065', 'grad_norm': '65', 'counters/examples': 23584, 'counters/updates': 737}
train stats after 23616 examples: {'rewards_train/chosen': '-0.098629', 'rewards_train/rejected': '-0.076534', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.022095', 'logps_train/rejected': '-183.34', 'logps_train/chosen': '-169.34', 'loss/train': '0.7286', 'examples_per_second': '30.104', 'grad_norm': '114.5', 'counters/examples': 23616, 'counters/updates': 738}
train stats after 23648 examples: {'rewards_train/chosen': '0.080038', 'rewards_train/rejected': '0.052542', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.027495', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-140.63', 'loss/train': '0.69921', 'examples_per_second': '32.394', 'grad_norm': '64.5', 'counters/examples': 23648, 'counters/updates': 739}
train stats after 23680 examples: {'rewards_train/chosen': '-0.022082', 'rewards_train/rejected': '0.022658', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04474', 'logps_train/rejected': '-105.55', 'logps_train/chosen': '-144.53', 'loss/train': '0.72434', 'examples_per_second': '30.044', 'grad_norm': '145', 'counters/examples': 23680, 'counters/updates': 740}
train stats after 23712 examples: {'rewards_train/chosen': '-0.019443', 'rewards_train/rejected': '-0.055392', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035949', 'logps_train/rejected': '-164.8', 'logps_train/chosen': '-159.98', 'loss/train': '0.69074', 'examples_per_second': '31.6', 'grad_norm': '63', 'counters/examples': 23712, 'counters/updates': 741}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23776 examples: {'rewards_train/chosen': '0.003843', 'rewards_train/rejected': '-0.05327', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057113', 'logps_train/rejected': '-149.58', 'logps_train/chosen': '-143.79', 'loss/train': '0.68232', 'examples_per_second': '31.121', 'grad_norm': '142', 'counters/examples': 23776, 'counters/updates': 743}
train stats after 23808 examples: {'rewards_train/chosen': '-0.030392', 'rewards_train/rejected': '0.041359', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.071751', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-164.08', 'loss/train': '0.74608', 'examples_per_second': '32.508', 'grad_norm': '90', 'counters/examples': 23808, 'counters/updates': 744}
skipping logging after 23840 examples to avoid logging too frequently
train stats after 23872 examples: {'rewards_train/chosen': '0.0002613', 'rewards_train/rejected': '0.029605', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029344', 'logps_train/rejected': '-102.71', 'logps_train/chosen': '-131', 'loss/train': '0.72139', 'examples_per_second': '24.151', 'grad_norm': '66', 'counters/examples': 23872, 'counters/updates': 746}
train stats after 23904 examples: {'rewards_train/chosen': '-0.050891', 'rewards_train/rejected': '-0.031569', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019322', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-155.84', 'loss/train': '0.71522', 'examples_per_second': '31.335', 'grad_norm': '82.5', 'counters/examples': 23904, 'counters/updates': 747}
skipping logging after 23936 examples to avoid logging too frequently
train stats after 23968 examples: {'rewards_train/chosen': '0.055279', 'rewards_train/rejected': '0.044517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010763', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-134.93', 'loss/train': '0.69654', 'examples_per_second': '34.353', 'grad_norm': '64', 'counters/examples': 23968, 'counters/updates': 749}
train stats after 24000 examples: {'rewards_train/chosen': '-0.036517', 'rewards_train/rejected': '-0.012041', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024476', 'logps_train/rejected': '-118.93', 'logps_train/chosen': '-156.65', 'loss/train': '0.7179', 'examples_per_second': '31.583', 'grad_norm': '69.5', 'counters/examples': 24000, 'counters/updates': 750}
Running evaluation after 24000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 24000: {'rewards_eval/chosen': '-0.0079744', 'rewards_eval/rejected': '-0.0095221', 'rewards_eval/accuracies': '0.48047', 'rewards_eval/margins': '0.0015478', 'logps_eval/rejected': '-128.24', 'logps_eval/chosen': '-151.09', 'loss/eval': '0.70735'}
skipping save for non epoch
train stats after 24032 examples: {'rewards_train/chosen': '0.080472', 'rewards_train/rejected': '0.18071', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10024', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-136.69', 'loss/train': '0.77213', 'examples_per_second': '32.084', 'grad_norm': '86.5', 'counters/examples': 24032, 'counters/updates': 751}
train stats after 24064 examples: {'rewards_train/chosen': '-0.035356', 'rewards_train/rejected': '-0.038949', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.003593', 'logps_train/rejected': '-109.14', 'logps_train/chosen': '-138.83', 'loss/train': '0.70051', 'examples_per_second': '30', 'grad_norm': '75.5', 'counters/examples': 24064, 'counters/updates': 752}
train stats after 24096 examples: {'rewards_train/chosen': '-0.045842', 'rewards_train/rejected': '-0.014991', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.030851', 'logps_train/rejected': '-114.91', 'logps_train/chosen': '-169.89', 'loss/train': '0.71647', 'examples_per_second': '31.708', 'grad_norm': '82', 'counters/examples': 24096, 'counters/updates': 753}
train stats after 24128 examples: {'rewards_train/chosen': '0.023729', 'rewards_train/rejected': '-0.083302', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10703', 'logps_train/rejected': '-125.2', 'logps_train/chosen': '-141.07', 'loss/train': '0.65094', 'examples_per_second': '31.534', 'grad_norm': '73.5', 'counters/examples': 24128, 'counters/updates': 754}
train stats after 24160 examples: {'rewards_train/chosen': '-0.0011169', 'rewards_train/rejected': '0.078172', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.079289', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-141.94', 'loss/train': '0.74704', 'examples_per_second': '32.354', 'grad_norm': '360', 'counters/examples': 24160, 'counters/updates': 755}
train stats after 24192 examples: {'rewards_train/chosen': '-0.013647', 'rewards_train/rejected': '-0.043232', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029585', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-143.02', 'loss/train': '0.68559', 'examples_per_second': '30.759', 'grad_norm': '110.5', 'counters/examples': 24192, 'counters/updates': 756}
train stats after 24224 examples: {'rewards_train/chosen': '-0.0062967', 'rewards_train/rejected': '-0.035894', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029597', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-196.27', 'loss/train': '0.6867', 'examples_per_second': '30.707', 'grad_norm': '96.5', 'counters/examples': 24224, 'counters/updates': 757}
train stats after 24256 examples: {'rewards_train/chosen': '0.0060559', 'rewards_train/rejected': '0.0030869', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.002969', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-179.49', 'loss/train': '0.70488', 'examples_per_second': '31.612', 'grad_norm': '116.5', 'counters/examples': 24256, 'counters/updates': 758}
train stats after 24288 examples: {'rewards_train/chosen': '-0.078089', 'rewards_train/rejected': '-0.061023', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017066', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-154.38', 'loss/train': '0.71057', 'examples_per_second': '32.664', 'grad_norm': '61.25', 'counters/examples': 24288, 'counters/updates': 759}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24352 examples: {'rewards_train/chosen': '-0.037508', 'rewards_train/rejected': '0.018812', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.05632', 'logps_train/rejected': '-142.79', 'logps_train/chosen': '-151.14', 'loss/train': '0.74313', 'examples_per_second': '30.966', 'grad_norm': '72', 'counters/examples': 24352, 'counters/updates': 761}
train stats after 24384 examples: {'rewards_train/chosen': '-0.056451', 'rewards_train/rejected': '0.035835', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.092285', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-160.51', 'loss/train': '0.75916', 'examples_per_second': '30.991', 'grad_norm': '157', 'counters/examples': 24384, 'counters/updates': 762}
train stats after 24416 examples: {'rewards_train/chosen': '-0.0020027', 'rewards_train/rejected': '0.072416', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.074418', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-137.38', 'loss/train': '0.74834', 'examples_per_second': '29.899', 'grad_norm': '62.75', 'counters/examples': 24416, 'counters/updates': 763}
train stats after 24448 examples: {'rewards_train/chosen': '-0.046776', 'rewards_train/rejected': '-0.0066585', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.040118', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-153.53', 'loss/train': '0.72432', 'examples_per_second': '31.552', 'grad_norm': '152', 'counters/examples': 24448, 'counters/updates': 764}
train stats after 24480 examples: {'rewards_train/chosen': '-0.12232', 'rewards_train/rejected': '-0.0034678', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11885', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-159.14', 'loss/train': '0.7654', 'examples_per_second': '30.149', 'grad_norm': '56.5', 'counters/examples': 24480, 'counters/updates': 765}
train stats after 24512 examples: {'rewards_train/chosen': '-0.01033', 'rewards_train/rejected': '-0.023112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012782', 'logps_train/rejected': '-118.09', 'logps_train/chosen': '-148.66', 'loss/train': '0.69976', 'examples_per_second': '31.807', 'grad_norm': '77.5', 'counters/examples': 24512, 'counters/updates': 766}
train stats after 24544 examples: {'rewards_train/chosen': '-0.045827', 'rewards_train/rejected': '0.004792', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.050619', 'logps_train/rejected': '-159.1', 'logps_train/chosen': '-167.64', 'loss/train': '0.73185', 'examples_per_second': '24.291', 'grad_norm': '117.5', 'counters/examples': 24544, 'counters/updates': 767}
train stats after 24576 examples: {'rewards_train/chosen': '0.0016646', 'rewards_train/rejected': '0.0044292', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0027646', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-145.94', 'loss/train': '0.70706', 'examples_per_second': '31.577', 'grad_norm': '79', 'counters/examples': 24576, 'counters/updates': 768}
train stats after 24608 examples: {'rewards_train/chosen': '-0.010945', 'rewards_train/rejected': '0.003077', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.014022', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-143.58', 'loss/train': '0.71405', 'examples_per_second': '32.023', 'grad_norm': '120', 'counters/examples': 24608, 'counters/updates': 769}
train stats after 24640 examples: {'rewards_train/chosen': '-0.072769', 'rewards_train/rejected': '0.029411', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.10218', 'logps_train/rejected': '-88.117', 'logps_train/chosen': '-171.32', 'loss/train': '0.75882', 'examples_per_second': '24.843', 'grad_norm': '99.5', 'counters/examples': 24640, 'counters/updates': 770}
skipping logging after 24672 examples to avoid logging too frequently
train stats after 24704 examples: {'rewards_train/chosen': '0.024944', 'rewards_train/rejected': '0.022989', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0019558', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-137.65', 'loss/train': '0.70178', 'examples_per_second': '32.189', 'grad_norm': '87', 'counters/examples': 24704, 'counters/updates': 772}
train stats after 24736 examples: {'rewards_train/chosen': '-0.067744', 'rewards_train/rejected': '0.12426', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.192', 'logps_train/rejected': '-167.94', 'logps_train/chosen': '-145.38', 'loss/train': '0.81646', 'examples_per_second': '31.325', 'grad_norm': '102', 'counters/examples': 24736, 'counters/updates': 773}
train stats after 24768 examples: {'rewards_train/chosen': '-0.00262', 'rewards_train/rejected': '-0.012562', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0099418', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-148.63', 'loss/train': '0.69875', 'examples_per_second': '32.938', 'grad_norm': '89', 'counters/examples': 24768, 'counters/updates': 774}
train stats after 24800 examples: {'rewards_train/chosen': '0.02567', 'rewards_train/rejected': '-0.020898', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046568', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-158.87', 'loss/train': '0.6911', 'examples_per_second': '31.312', 'grad_norm': '80.5', 'counters/examples': 24800, 'counters/updates': 775}
train stats after 24832 examples: {'rewards_train/chosen': '-0.063372', 'rewards_train/rejected': '0.01141', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.074783', 'logps_train/rejected': '-105.14', 'logps_train/chosen': '-159.04', 'loss/train': '0.74428', 'examples_per_second': '32.207', 'grad_norm': '73.5', 'counters/examples': 24832, 'counters/updates': 776}
train stats after 24864 examples: {'rewards_train/chosen': '0.017999', 'rewards_train/rejected': '0.0063055', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011694', 'logps_train/rejected': '-156.07', 'logps_train/chosen': '-171.03', 'loss/train': '0.69669', 'examples_per_second': '30.665', 'grad_norm': '71', 'counters/examples': 24864, 'counters/updates': 777}
train stats after 24896 examples: {'rewards_train/chosen': '-0.039989', 'rewards_train/rejected': '0.010874', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050864', 'logps_train/rejected': '-194.99', 'logps_train/chosen': '-176.24', 'loss/train': '0.7329', 'examples_per_second': '31.162', 'grad_norm': '104.5', 'counters/examples': 24896, 'counters/updates': 778}
skipping logging after 24928 examples to avoid logging too frequently
train stats after 24960 examples: {'rewards_train/chosen': '0.12169', 'rewards_train/rejected': '0.00038617', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.12131', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-131.18', 'loss/train': '0.65517', 'examples_per_second': '30.605', 'grad_norm': '86.5', 'counters/examples': 24960, 'counters/updates': 780}
train stats after 24992 examples: {'rewards_train/chosen': '-0.0097067', 'rewards_train/rejected': '-0.014435', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0047281', 'logps_train/rejected': '-167.27', 'logps_train/chosen': '-179.78', 'loss/train': '0.7112', 'examples_per_second': '30.945', 'grad_norm': '151', 'counters/examples': 24992, 'counters/updates': 781}
train stats after 25024 examples: {'rewards_train/chosen': '-0.023778', 'rewards_train/rejected': '0.0004987', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.024276', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-144.29', 'loss/train': '0.71445', 'examples_per_second': '32.523', 'grad_norm': '109.5', 'counters/examples': 25024, 'counters/updates': 782}
train stats after 25056 examples: {'rewards_train/chosen': '0.01503', 'rewards_train/rejected': '-0.0034828', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018513', 'logps_train/rejected': '-165.67', 'logps_train/chosen': '-157.76', 'loss/train': '0.70309', 'examples_per_second': '31.605', 'grad_norm': '74', 'counters/examples': 25056, 'counters/updates': 783}
train stats after 25088 examples: {'rewards_train/chosen': '0.030713', 'rewards_train/rejected': '-0.0073756', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038088', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-163.22', 'loss/train': '0.6884', 'examples_per_second': '31.517', 'grad_norm': '94.5', 'counters/examples': 25088, 'counters/updates': 784}
train stats after 25120 examples: {'rewards_train/chosen': '-0.013677', 'rewards_train/rejected': '0.12915', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.14283', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-147.08', 'loss/train': '0.79214', 'examples_per_second': '30.764', 'grad_norm': '219', 'counters/examples': 25120, 'counters/updates': 785}
train stats after 25152 examples: {'rewards_train/chosen': '0.07037', 'rewards_train/rejected': '-0.043972', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11434', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-135.02', 'loss/train': '0.65211', 'examples_per_second': '31.618', 'grad_norm': '79.5', 'counters/examples': 25152, 'counters/updates': 786}
skipping logging after 25184 examples to avoid logging too frequently
train stats after 25216 examples: {'rewards_train/chosen': '-0.0079506', 'rewards_train/rejected': '-0.0045554', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0033953', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-147.73', 'loss/train': '0.70973', 'examples_per_second': '32.923', 'grad_norm': '78', 'counters/examples': 25216, 'counters/updates': 788}
train stats after 25248 examples: {'rewards_train/chosen': '0.0021132', 'rewards_train/rejected': '0.0079434', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0058302', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-132.44', 'loss/train': '0.70867', 'examples_per_second': '32.119', 'grad_norm': '100.5', 'counters/examples': 25248, 'counters/updates': 789}
train stats after 25280 examples: {'rewards_train/chosen': '-0.0070679', 'rewards_train/rejected': '0.014615', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021683', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-133.58', 'loss/train': '0.71199', 'examples_per_second': '31.568', 'grad_norm': '63', 'counters/examples': 25280, 'counters/updates': 790}
skipping logging after 25312 examples to avoid logging too frequently
train stats after 25344 examples: {'rewards_train/chosen': '0.086139', 'rewards_train/rejected': '0.026046', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060093', 'logps_train/rejected': '-141.03', 'logps_train/chosen': '-158.66', 'loss/train': '0.6783', 'examples_per_second': '30.531', 'grad_norm': '112.5', 'counters/examples': 25344, 'counters/updates': 792}
skipping logging after 25376 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '0.023638', 'rewards_train/rejected': '0.0041463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019491', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-158.6', 'loss/train': '0.68974', 'examples_per_second': '31.595', 'grad_norm': '130', 'counters/examples': 25408, 'counters/updates': 794}
train stats after 25440 examples: {'rewards_train/chosen': '-0.064432', 'rewards_train/rejected': '-0.0494', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.015033', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-155.11', 'loss/train': '0.73566', 'examples_per_second': '31.551', 'grad_norm': '108.5', 'counters/examples': 25440, 'counters/updates': 795}
train stats after 25472 examples: {'rewards_train/chosen': '-0.022862', 'rewards_train/rejected': '0.011683', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034545', 'logps_train/rejected': '-128.18', 'logps_train/chosen': '-110.2', 'loss/train': '0.72143', 'examples_per_second': '31.805', 'grad_norm': '113.5', 'counters/examples': 25472, 'counters/updates': 796}
train stats after 25504 examples: {'rewards_train/chosen': '0.049258', 'rewards_train/rejected': '-0.05425', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10351', 'logps_train/rejected': '-125.91', 'logps_train/chosen': '-160.41', 'loss/train': '0.66737', 'examples_per_second': '31.587', 'grad_norm': '79', 'counters/examples': 25504, 'counters/updates': 797}
train stats after 25536 examples: {'rewards_train/chosen': '-0.035545', 'rewards_train/rejected': '0.016188', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051733', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-134.03', 'loss/train': '0.7332', 'examples_per_second': '31.593', 'grad_norm': '80', 'counters/examples': 25536, 'counters/updates': 798}
train stats after 25568 examples: {'rewards_train/chosen': '-0.018097', 'rewards_train/rejected': '0.041528', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059625', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-152.38', 'loss/train': '0.73406', 'examples_per_second': '30.772', 'grad_norm': '87.5', 'counters/examples': 25568, 'counters/updates': 799}
train stats after 25600 examples: {'rewards_train/chosen': '0.03009', 'rewards_train/rejected': '-0.033384', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063474', 'logps_train/rejected': '-143.99', 'logps_train/chosen': '-162.08', 'loss/train': '0.67791', 'examples_per_second': '31.626', 'grad_norm': '73', 'counters/examples': 25600, 'counters/updates': 800}
train stats after 25632 examples: {'rewards_train/chosen': '-0.010704', 'rewards_train/rejected': '-0.074065', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063361', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-139.49', 'loss/train': '0.67191', 'examples_per_second': '31.294', 'grad_norm': '88.5', 'counters/examples': 25632, 'counters/updates': 801}
train stats after 25664 examples: {'rewards_train/chosen': '0.11549', 'rewards_train/rejected': '0.02028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095213', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-145.32', 'loss/train': '0.66403', 'examples_per_second': '30.488', 'grad_norm': '55.75', 'counters/examples': 25664, 'counters/updates': 802}
train stats after 25696 examples: {'rewards_train/chosen': '-0.022306', 'rewards_train/rejected': '-0.054245', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031939', 'logps_train/rejected': '-146.12', 'logps_train/chosen': '-208.11', 'loss/train': '0.69645', 'examples_per_second': '30.416', 'grad_norm': '77.5', 'counters/examples': 25696, 'counters/updates': 803}
train stats after 25728 examples: {'rewards_train/chosen': '0.049264', 'rewards_train/rejected': '0.046173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0030909', 'logps_train/rejected': '-141.04', 'logps_train/chosen': '-175.75', 'loss/train': '0.70418', 'examples_per_second': '32.198', 'grad_norm': '82', 'counters/examples': 25728, 'counters/updates': 804}
train stats after 25760 examples: {'rewards_train/chosen': '0.035222', 'rewards_train/rejected': '-0.019898', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05512', 'logps_train/rejected': '-146.55', 'logps_train/chosen': '-145.85', 'loss/train': '0.67183', 'examples_per_second': '30.571', 'grad_norm': '60', 'counters/examples': 25760, 'counters/updates': 805}
skipping logging after 25792 examples to avoid logging too frequently
train stats after 25824 examples: {'rewards_train/chosen': '0.08004', 'rewards_train/rejected': '-0.033275', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11332', 'logps_train/rejected': '-159.4', 'logps_train/chosen': '-148.48', 'loss/train': '0.6625', 'examples_per_second': '30.069', 'grad_norm': '114', 'counters/examples': 25824, 'counters/updates': 807}
train stats after 25856 examples: {'rewards_train/chosen': '-0.0057561', 'rewards_train/rejected': '0.049187', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054943', 'logps_train/rejected': '-110.43', 'logps_train/chosen': '-169.14', 'loss/train': '0.73458', 'examples_per_second': '31.739', 'grad_norm': '97.5', 'counters/examples': 25856, 'counters/updates': 808}
train stats after 25888 examples: {'rewards_train/chosen': '-0.020109', 'rewards_train/rejected': '0.055179', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.075289', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-161.9', 'loss/train': '0.74735', 'examples_per_second': '31.881', 'grad_norm': '75', 'counters/examples': 25888, 'counters/updates': 809}
train stats after 25920 examples: {'rewards_train/chosen': '0.091667', 'rewards_train/rejected': '-0.0065042', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098171', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-176.16', 'loss/train': '0.65506', 'examples_per_second': '31.581', 'grad_norm': '104', 'counters/examples': 25920, 'counters/updates': 810}
train stats after 25952 examples: {'rewards_train/chosen': '-0.040204', 'rewards_train/rejected': '0.00070313', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040907', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-139.58', 'loss/train': '0.72918', 'examples_per_second': '31.582', 'grad_norm': '54', 'counters/examples': 25952, 'counters/updates': 811}
train stats after 25984 examples: {'rewards_train/chosen': '0.058182', 'rewards_train/rejected': '-0.035325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093507', 'logps_train/rejected': '-147.97', 'logps_train/chosen': '-136.53', 'loss/train': '0.65404', 'examples_per_second': '32.354', 'grad_norm': '92.5', 'counters/examples': 25984, 'counters/updates': 812}
train stats after 26016 examples: {'rewards_train/chosen': '0.0030762', 'rewards_train/rejected': '-0.0035481', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0066243', 'logps_train/rejected': '-132.19', 'logps_train/chosen': '-157.73', 'loss/train': '0.69949', 'examples_per_second': '32.331', 'grad_norm': '84', 'counters/examples': 26016, 'counters/updates': 813}
train stats after 26048 examples: {'rewards_train/chosen': '-0.046424', 'rewards_train/rejected': '-0.015544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.03088', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-196.89', 'loss/train': '0.72524', 'examples_per_second': '30.537', 'grad_norm': '101.5', 'counters/examples': 26048, 'counters/updates': 814}
skipping logging after 26080 examples to avoid logging too frequently
train stats after 26112 examples: {'rewards_train/chosen': '0.07198', 'rewards_train/rejected': '0.023064', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048916', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-141.43', 'loss/train': '0.67623', 'examples_per_second': '36.556', 'grad_norm': '79.5', 'counters/examples': 26112, 'counters/updates': 816}
train stats after 26144 examples: {'rewards_train/chosen': '-0.022473', 'rewards_train/rejected': '0.011522', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033995', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-147.55', 'loss/train': '0.72256', 'examples_per_second': '32.056', 'grad_norm': '91.5', 'counters/examples': 26144, 'counters/updates': 817}
train stats after 26176 examples: {'rewards_train/chosen': '-0.030459', 'rewards_train/rejected': '-0.024008', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.006451', 'logps_train/rejected': '-109.63', 'logps_train/chosen': '-120.03', 'loss/train': '0.70412', 'examples_per_second': '30.299', 'grad_norm': '54', 'counters/examples': 26176, 'counters/updates': 818}
train stats after 26208 examples: {'rewards_train/chosen': '0.0086695', 'rewards_train/rejected': '0.032712', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024042', 'logps_train/rejected': '-151.67', 'logps_train/chosen': '-132.97', 'loss/train': '0.71907', 'examples_per_second': '30.664', 'grad_norm': '73.5', 'counters/examples': 26208, 'counters/updates': 819}
train stats after 26240 examples: {'rewards_train/chosen': '0.010217', 'rewards_train/rejected': '-0.11186', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12208', 'logps_train/rejected': '-99.273', 'logps_train/chosen': '-135.21', 'loss/train': '0.65386', 'examples_per_second': '30.627', 'grad_norm': '79', 'counters/examples': 26240, 'counters/updates': 820}
train stats after 26272 examples: {'rewards_train/chosen': '-0.024857', 'rewards_train/rejected': '-0.091479', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066623', 'logps_train/rejected': '-114.68', 'logps_train/chosen': '-157.14', 'loss/train': '0.67239', 'examples_per_second': '32.145', 'grad_norm': '134', 'counters/examples': 26272, 'counters/updates': 821}
train stats after 26304 examples: {'rewards_train/chosen': '-0.033129', 'rewards_train/rejected': '-0.046317', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013188', 'logps_train/rejected': '-125.16', 'logps_train/chosen': '-169.96', 'loss/train': '0.69843', 'examples_per_second': '32.322', 'grad_norm': '129', 'counters/examples': 26304, 'counters/updates': 822}
train stats after 26336 examples: {'rewards_train/chosen': '0.0010258', 'rewards_train/rejected': '0.014242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013216', 'logps_train/rejected': '-136.29', 'logps_train/chosen': '-173.43', 'loss/train': '0.72232', 'examples_per_second': '32.4', 'grad_norm': '80', 'counters/examples': 26336, 'counters/updates': 823}
train stats after 26368 examples: {'rewards_train/chosen': '0.044191', 'rewards_train/rejected': '0.046563', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0023719', 'logps_train/rejected': '-146.43', 'logps_train/chosen': '-154.24', 'loss/train': '0.71979', 'examples_per_second': '30.91', 'grad_norm': '174', 'counters/examples': 26368, 'counters/updates': 824}
train stats after 26400 examples: {'rewards_train/chosen': '-0.012275', 'rewards_train/rejected': '-0.028495', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01622', 'logps_train/rejected': '-184.67', 'logps_train/chosen': '-196.11', 'loss/train': '0.70332', 'examples_per_second': '30.212', 'grad_norm': '130', 'counters/examples': 26400, 'counters/updates': 825}
skipping logging after 26432 examples to avoid logging too frequently
train stats after 26464 examples: {'rewards_train/chosen': '0.057562', 'rewards_train/rejected': '0.052065', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0054974', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-165.42', 'loss/train': '0.71346', 'examples_per_second': '30.188', 'grad_norm': '125', 'counters/examples': 26464, 'counters/updates': 827}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26528 examples: {'rewards_train/chosen': '0.017889', 'rewards_train/rejected': '-0.0064335', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024323', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-139.45', 'loss/train': '0.69147', 'examples_per_second': '32.748', 'grad_norm': '65.5', 'counters/examples': 26528, 'counters/updates': 829}
train stats after 26560 examples: {'rewards_train/chosen': '-0.0033328', 'rewards_train/rejected': '-0.024032', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020699', 'logps_train/rejected': '-118.71', 'logps_train/chosen': '-122.16', 'loss/train': '0.69387', 'examples_per_second': '31.854', 'grad_norm': '66', 'counters/examples': 26560, 'counters/updates': 830}
train stats after 26592 examples: {'rewards_train/chosen': '-0.035301', 'rewards_train/rejected': '0.0094162', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.044717', 'logps_train/rejected': '-108.97', 'logps_train/chosen': '-129.53', 'loss/train': '0.72719', 'examples_per_second': '32.693', 'grad_norm': '110.5', 'counters/examples': 26592, 'counters/updates': 831}
train stats after 26624 examples: {'rewards_train/chosen': '-0.014709', 'rewards_train/rejected': '-0.12931', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11461', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-142.21', 'loss/train': '0.65126', 'examples_per_second': '31.902', 'grad_norm': '122.5', 'counters/examples': 26624, 'counters/updates': 832}
skipping logging after 26656 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '0.061104', 'rewards_train/rejected': '-0.0066525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067756', 'logps_train/rejected': '-171.05', 'logps_train/chosen': '-187.62', 'loss/train': '0.67176', 'examples_per_second': '31.532', 'grad_norm': '79', 'counters/examples': 26688, 'counters/updates': 834}
train stats after 26720 examples: {'rewards_train/chosen': '0.01129', 'rewards_train/rejected': '-0.0056068', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016897', 'logps_train/rejected': '-131.13', 'logps_train/chosen': '-140.06', 'loss/train': '0.6964', 'examples_per_second': '30.421', 'grad_norm': '944', 'counters/examples': 26720, 'counters/updates': 835}
train stats after 26752 examples: {'rewards_train/chosen': '0.000637', 'rewards_train/rejected': '0.010564', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0099269', 'logps_train/rejected': '-107.57', 'logps_train/chosen': '-129.44', 'loss/train': '0.70912', 'examples_per_second': '30.761', 'grad_norm': '80', 'counters/examples': 26752, 'counters/updates': 836}
skipping logging after 26784 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.046981', 'rewards_train/rejected': '-0.081592', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034611', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-181.55', 'loss/train': '0.70097', 'examples_per_second': '31.496', 'grad_norm': '84.5', 'counters/examples': 26816, 'counters/updates': 838}
train stats after 26848 examples: {'rewards_train/chosen': '-0.018683', 'rewards_train/rejected': '0.044735', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.063418', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-201.96', 'loss/train': '0.7323', 'examples_per_second': '31.585', 'grad_norm': '112', 'counters/examples': 26848, 'counters/updates': 839}
train stats after 26880 examples: {'rewards_train/chosen': '0.033424', 'rewards_train/rejected': '-0.049206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082631', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-173.19', 'loss/train': '0.66191', 'examples_per_second': '31.572', 'grad_norm': '54.5', 'counters/examples': 26880, 'counters/updates': 840}
train stats after 26912 examples: {'rewards_train/chosen': '-0.082922', 'rewards_train/rejected': '-0.035954', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.046968', 'logps_train/rejected': '-166.48', 'logps_train/chosen': '-168.24', 'loss/train': '0.72741', 'examples_per_second': '32.43', 'grad_norm': '125.5', 'counters/examples': 26912, 'counters/updates': 841}
train stats after 26944 examples: {'rewards_train/chosen': '0.049043', 'rewards_train/rejected': '-0.060759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1098', 'logps_train/rejected': '-183.58', 'logps_train/chosen': '-156.57', 'loss/train': '0.65972', 'examples_per_second': '33.105', 'grad_norm': '314', 'counters/examples': 26944, 'counters/updates': 842}
skipping logging after 26976 examples to avoid logging too frequently
train stats after 27008 examples: {'rewards_train/chosen': '-0.024397', 'rewards_train/rejected': '-0.030054', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0056564', 'logps_train/rejected': '-162.59', 'logps_train/chosen': '-146.78', 'loss/train': '0.70336', 'examples_per_second': '33.456', 'grad_norm': '94.5', 'counters/examples': 27008, 'counters/updates': 844}
train stats after 27040 examples: {'rewards_train/chosen': '0.025141', 'rewards_train/rejected': '-0.045186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070327', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-139.32', 'loss/train': '0.67026', 'examples_per_second': '31.079', 'grad_norm': '69.5', 'counters/examples': 27040, 'counters/updates': 845}
train stats after 27072 examples: {'rewards_train/chosen': '-0.039655', 'rewards_train/rejected': '-0.047154', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.007499', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-164.06', 'loss/train': '0.6998', 'examples_per_second': '31.192', 'grad_norm': '75.5', 'counters/examples': 27072, 'counters/updates': 846}
train stats after 27104 examples: {'rewards_train/chosen': '0.032503', 'rewards_train/rejected': '-0.041938', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.074441', 'logps_train/rejected': '-154.49', 'logps_train/chosen': '-168.81', 'loss/train': '0.67275', 'examples_per_second': '31.603', 'grad_norm': '101', 'counters/examples': 27104, 'counters/updates': 847}
train stats after 27136 examples: {'rewards_train/chosen': '-0.0077081', 'rewards_train/rejected': '-0.0083542', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00064609', 'logps_train/rejected': '-109.99', 'logps_train/chosen': '-127.73', 'loss/train': '0.70446', 'examples_per_second': '31.289', 'grad_norm': '60.75', 'counters/examples': 27136, 'counters/updates': 848}
skipping logging after 27168 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '0.029891', 'rewards_train/rejected': '-0.0081334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038024', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-119.09', 'loss/train': '0.68169', 'examples_per_second': '30.05', 'grad_norm': '79.5', 'counters/examples': 27200, 'counters/updates': 850}
train stats after 27232 examples: {'rewards_train/chosen': '-0.085962', 'rewards_train/rejected': '0.032039', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.118', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-174.7', 'loss/train': '0.76193', 'examples_per_second': '31.578', 'grad_norm': '125', 'counters/examples': 27232, 'counters/updates': 851}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27296 examples: {'rewards_train/chosen': '0.027631', 'rewards_train/rejected': '-0.025401', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053032', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-112.26', 'loss/train': '0.67521', 'examples_per_second': '34.687', 'grad_norm': '47.75', 'counters/examples': 27296, 'counters/updates': 853}
train stats after 27328 examples: {'rewards_train/chosen': '0.041777', 'rewards_train/rejected': '-0.0031936', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04497', 'logps_train/rejected': '-183.82', 'logps_train/chosen': '-165.21', 'loss/train': '0.69179', 'examples_per_second': '31.335', 'grad_norm': '113', 'counters/examples': 27328, 'counters/updates': 854}
train stats after 27360 examples: {'rewards_train/chosen': '-0.013034', 'rewards_train/rejected': '-0.014353', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0013188', 'logps_train/rejected': '-136.18', 'logps_train/chosen': '-143.57', 'loss/train': '0.70172', 'examples_per_second': '32.317', 'grad_norm': '75', 'counters/examples': 27360, 'counters/updates': 855}
train stats after 27392 examples: {'rewards_train/chosen': '-0.043403', 'rewards_train/rejected': '-0.094918', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051515', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-157.45', 'loss/train': '0.67634', 'examples_per_second': '31.614', 'grad_norm': '205', 'counters/examples': 27392, 'counters/updates': 856}
train stats after 27424 examples: {'rewards_train/chosen': '0.063201', 'rewards_train/rejected': '0.012447', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050754', 'logps_train/rejected': '-171.04', 'logps_train/chosen': '-187.84', 'loss/train': '0.68451', 'examples_per_second': '30.841', 'grad_norm': '135', 'counters/examples': 27424, 'counters/updates': 857}
train stats after 27456 examples: {'rewards_train/chosen': '-0.070709', 'rewards_train/rejected': '0.028731', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.09944', 'logps_train/rejected': '-161.65', 'logps_train/chosen': '-201.77', 'loss/train': '0.76814', 'examples_per_second': '31.569', 'grad_norm': '294', 'counters/examples': 27456, 'counters/updates': 858}
train stats after 27488 examples: {'rewards_train/chosen': '0.064237', 'rewards_train/rejected': '0.033912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030325', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-134.41', 'loss/train': '0.69093', 'examples_per_second': '32.318', 'grad_norm': '115', 'counters/examples': 27488, 'counters/updates': 859}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27552 examples: {'rewards_train/chosen': '-0.082877', 'rewards_train/rejected': '0.037855', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.12073', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-167.61', 'loss/train': '0.77355', 'examples_per_second': '30.748', 'grad_norm': '90', 'counters/examples': 27552, 'counters/updates': 861}
train stats after 27584 examples: {'rewards_train/chosen': '-0.037589', 'rewards_train/rejected': '-0.06008', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022491', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-141.01', 'loss/train': '0.69488', 'examples_per_second': '30.377', 'grad_norm': '64.5', 'counters/examples': 27584, 'counters/updates': 862}
train stats after 27616 examples: {'rewards_train/chosen': '0.016496', 'rewards_train/rejected': '-0.0096033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026099', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-166.14', 'loss/train': '0.70209', 'examples_per_second': '31.287', 'grad_norm': '111', 'counters/examples': 27616, 'counters/updates': 863}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27680 examples: {'rewards_train/chosen': '0.021924', 'rewards_train/rejected': '0.014942', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0069827', 'logps_train/rejected': '-101.89', 'logps_train/chosen': '-133.58', 'loss/train': '0.69659', 'examples_per_second': '31.546', 'grad_norm': '330', 'counters/examples': 27680, 'counters/updates': 865}
train stats after 27712 examples: {'rewards_train/chosen': '0.032683', 'rewards_train/rejected': '0.1491', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11642', 'logps_train/rejected': '-161.52', 'logps_train/chosen': '-158.88', 'loss/train': '0.7657', 'examples_per_second': '30.482', 'grad_norm': '115', 'counters/examples': 27712, 'counters/updates': 866}
skipping logging after 27744 examples to avoid logging too frequently
train stats after 27776 examples: {'rewards_train/chosen': '-0.025007', 'rewards_train/rejected': '0.028401', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053408', 'logps_train/rejected': '-143.04', 'logps_train/chosen': '-155.63', 'loss/train': '0.72433', 'examples_per_second': '37.309', 'grad_norm': '57', 'counters/examples': 27776, 'counters/updates': 868}
train stats after 27808 examples: {'rewards_train/chosen': '0.10745', 'rewards_train/rejected': '0.039827', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067621', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-142.31', 'loss/train': '0.67649', 'examples_per_second': '31.608', 'grad_norm': '104.5', 'counters/examples': 27808, 'counters/updates': 869}
train stats after 27840 examples: {'rewards_train/chosen': '0.0053888', 'rewards_train/rejected': '-0.004607', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0099957', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-149', 'loss/train': '0.69989', 'examples_per_second': '31.826', 'grad_norm': '123', 'counters/examples': 27840, 'counters/updates': 870}
train stats after 27872 examples: {'rewards_train/chosen': '-0.049892', 'rewards_train/rejected': '-0.00038623', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.049506', 'logps_train/rejected': '-86.031', 'logps_train/chosen': '-145.16', 'loss/train': '0.73524', 'examples_per_second': '31.147', 'grad_norm': '59.75', 'counters/examples': 27872, 'counters/updates': 871}
train stats after 27904 examples: {'rewards_train/chosen': '-0.045659', 'rewards_train/rejected': '-0.062365', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.016706', 'logps_train/rejected': '-160.53', 'logps_train/chosen': '-161.45', 'loss/train': '0.70248', 'examples_per_second': '31.6', 'grad_norm': '93', 'counters/examples': 27904, 'counters/updates': 872}
train stats after 27936 examples: {'rewards_train/chosen': '0.056186', 'rewards_train/rejected': '-0.028103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084288', 'logps_train/rejected': '-102.07', 'logps_train/chosen': '-123.6', 'loss/train': '0.6618', 'examples_per_second': '31.65', 'grad_norm': '62.25', 'counters/examples': 27936, 'counters/updates': 873}
train stats after 27968 examples: {'rewards_train/chosen': '0.042977', 'rewards_train/rejected': '0.072573', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029596', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-146.42', 'loss/train': '0.72043', 'examples_per_second': '30.521', 'grad_norm': '88', 'counters/examples': 27968, 'counters/updates': 874}
train stats after 28000 examples: {'rewards_train/chosen': '0.038198', 'rewards_train/rejected': '-0.10298', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14118', 'logps_train/rejected': '-193.98', 'logps_train/chosen': '-197.35', 'loss/train': '0.64391', 'examples_per_second': '31.566', 'grad_norm': '146', 'counters/examples': 28000, 'counters/updates': 875}
train stats after 28032 examples: {'rewards_train/chosen': '-0.11184', 'rewards_train/rejected': '0.079453', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.19129', 'logps_train/rejected': '-197.91', 'logps_train/chosen': '-155.97', 'loss/train': '0.82059', 'examples_per_second': '30.828', 'grad_norm': '211', 'counters/examples': 28032, 'counters/updates': 876}
train stats after 28064 examples: {'rewards_train/chosen': '0.015517', 'rewards_train/rejected': '0.042509', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026992', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-198.83', 'loss/train': '0.71366', 'examples_per_second': '31.475', 'grad_norm': '226', 'counters/examples': 28064, 'counters/updates': 877}
train stats after 28096 examples: {'rewards_train/chosen': '0.014866', 'rewards_train/rejected': '-0.046827', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061693', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-156.94', 'loss/train': '0.6787', 'examples_per_second': '31.588', 'grad_norm': '62.5', 'counters/examples': 28096, 'counters/updates': 878}
skipping logging after 28128 examples to avoid logging too frequently
train stats after 28160 examples: {'rewards_train/chosen': '-0.066185', 'rewards_train/rejected': '-0.011743', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.054441', 'logps_train/rejected': '-141.2', 'logps_train/chosen': '-146.55', 'loss/train': '0.74142', 'examples_per_second': '30.114', 'grad_norm': '93', 'counters/examples': 28160, 'counters/updates': 880}
train stats after 28192 examples: {'rewards_train/chosen': '-0.021617', 'rewards_train/rejected': '-0.092124', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070507', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-157.76', 'loss/train': '0.66907', 'examples_per_second': '30.155', 'grad_norm': '57', 'counters/examples': 28192, 'counters/updates': 881}
skipping logging after 28224 examples to avoid logging too frequently
train stats after 28256 examples: {'rewards_train/chosen': '-0.045606', 'rewards_train/rejected': '-0.033019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012588', 'logps_train/rejected': '-158.08', 'logps_train/chosen': '-147.12', 'loss/train': '0.70926', 'examples_per_second': '31.639', 'grad_norm': '117', 'counters/examples': 28256, 'counters/updates': 883}
train stats after 28288 examples: {'rewards_train/chosen': '0.01297', 'rewards_train/rejected': '0.035769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022799', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-134.22', 'loss/train': '0.71863', 'examples_per_second': '31.682', 'grad_norm': '91.5', 'counters/examples': 28288, 'counters/updates': 884}
train stats after 28320 examples: {'rewards_train/chosen': '0.11581', 'rewards_train/rejected': '-0.080724', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19653', 'logps_train/rejected': '-164.39', 'logps_train/chosen': '-151.47', 'loss/train': '0.61347', 'examples_per_second': '32.786', 'grad_norm': '78', 'counters/examples': 28320, 'counters/updates': 885}
train stats after 28352 examples: {'rewards_train/chosen': '0.0045187', 'rewards_train/rejected': '0.07495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.070431', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-127.41', 'loss/train': '0.741', 'examples_per_second': '32.917', 'grad_norm': '73', 'counters/examples': 28352, 'counters/updates': 886}
train stats after 28384 examples: {'rewards_train/chosen': '-0.10776', 'rewards_train/rejected': '-0.038651', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.069106', 'logps_train/rejected': '-148.51', 'logps_train/chosen': '-183', 'loss/train': '0.75339', 'examples_per_second': '31.574', 'grad_norm': '108', 'counters/examples': 28384, 'counters/updates': 887}
train stats after 28416 examples: {'rewards_train/chosen': '-0.013283', 'rewards_train/rejected': '-0.015719', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0024363', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-159.58', 'loss/train': '0.7107', 'examples_per_second': '31.066', 'grad_norm': '108', 'counters/examples': 28416, 'counters/updates': 888}
train stats after 28448 examples: {'rewards_train/chosen': '0.031039', 'rewards_train/rejected': '-0.048377', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079416', 'logps_train/rejected': '-153.6', 'logps_train/chosen': '-181.45', 'loss/train': '0.66618', 'examples_per_second': '31.348', 'grad_norm': '71', 'counters/examples': 28448, 'counters/updates': 889}
train stats after 28480 examples: {'rewards_train/chosen': '0.046272', 'rewards_train/rejected': '-0.0068015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.053073', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-127.99', 'loss/train': '0.67179', 'examples_per_second': '30.674', 'grad_norm': '90', 'counters/examples': 28480, 'counters/updates': 890}
train stats after 28512 examples: {'rewards_train/chosen': '-0.093516', 'rewards_train/rejected': '-0.025048', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.068468', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-136.2', 'loss/train': '0.73935', 'examples_per_second': '31.685', 'grad_norm': '149', 'counters/examples': 28512, 'counters/updates': 891}
train stats after 28544 examples: {'rewards_train/chosen': '0.095709', 'rewards_train/rejected': '-0.022191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1179', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-170.05', 'loss/train': '0.64743', 'examples_per_second': '30.155', 'grad_norm': '63.25', 'counters/examples': 28544, 'counters/updates': 892}
skipping logging after 28576 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '-0.048488', 'rewards_train/rejected': '0.014973', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.063462', 'logps_train/rejected': '-135.59', 'logps_train/chosen': '-179.07', 'loss/train': '0.73521', 'examples_per_second': '32.669', 'grad_norm': '70.5', 'counters/examples': 28608, 'counters/updates': 894}
train stats after 28640 examples: {'rewards_train/chosen': '-0.02775', 'rewards_train/rejected': '-0.0047101', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02304', 'logps_train/rejected': '-146.81', 'logps_train/chosen': '-154.91', 'loss/train': '0.72144', 'examples_per_second': '32.474', 'grad_norm': '147', 'counters/examples': 28640, 'counters/updates': 895}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28704 examples: {'rewards_train/chosen': '-0.034186', 'rewards_train/rejected': '-0.061766', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02758', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-178.54', 'loss/train': '0.69611', 'examples_per_second': '31.68', 'grad_norm': '89', 'counters/examples': 28704, 'counters/updates': 897}
train stats after 28736 examples: {'rewards_train/chosen': '-0.092202', 'rewards_train/rejected': '0.015989', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10819', 'logps_train/rejected': '-142.98', 'logps_train/chosen': '-158.83', 'loss/train': '0.76354', 'examples_per_second': '30.618', 'grad_norm': '130', 'counters/examples': 28736, 'counters/updates': 898}
train stats after 28768 examples: {'rewards_train/chosen': '-0.075395', 'rewards_train/rejected': '-0.0088278', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.066567', 'logps_train/rejected': '-131', 'logps_train/chosen': '-191.71', 'loss/train': '0.74045', 'examples_per_second': '30.304', 'grad_norm': '161', 'counters/examples': 28768, 'counters/updates': 899}
train stats after 28800 examples: {'rewards_train/chosen': '-0.073367', 'rewards_train/rejected': '0.022838', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.096205', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-139.59', 'loss/train': '0.75183', 'examples_per_second': '31.707', 'grad_norm': '58.75', 'counters/examples': 28800, 'counters/updates': 900}
train stats after 28832 examples: {'rewards_train/chosen': '-0.029894', 'rewards_train/rejected': '0.013993', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.043888', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-134.97', 'loss/train': '0.7278', 'examples_per_second': '30.994', 'grad_norm': '68', 'counters/examples': 28832, 'counters/updates': 901}
train stats after 28864 examples: {'rewards_train/chosen': '-0.050177', 'rewards_train/rejected': '-0.00068897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.049488', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-132.01', 'loss/train': '0.74838', 'examples_per_second': '32.7', 'grad_norm': '105', 'counters/examples': 28864, 'counters/updates': 902}
train stats after 28896 examples: {'rewards_train/chosen': '0.032525', 'rewards_train/rejected': '0.0096851', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02284', 'logps_train/rejected': '-100.94', 'logps_train/chosen': '-140.25', 'loss/train': '0.70971', 'examples_per_second': '30.761', 'grad_norm': '250', 'counters/examples': 28896, 'counters/updates': 903}
train stats after 28928 examples: {'rewards_train/chosen': '0.018192', 'rewards_train/rejected': '-0.011484', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029676', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-145.25', 'loss/train': '0.68798', 'examples_per_second': '30.645', 'grad_norm': '121.5', 'counters/examples': 28928, 'counters/updates': 904}
skipping logging after 28960 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '-0.041142', 'rewards_train/rejected': '-0.13111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089965', 'logps_train/rejected': '-111.34', 'logps_train/chosen': '-123.99', 'loss/train': '0.66206', 'examples_per_second': '32.451', 'grad_norm': '51.75', 'counters/examples': 28992, 'counters/updates': 906}
train stats after 29024 examples: {'rewards_train/chosen': '0.0068621', 'rewards_train/rejected': '-0.11944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1263', 'logps_train/rejected': '-147.62', 'logps_train/chosen': '-165.59', 'loss/train': '0.64815', 'examples_per_second': '30.704', 'grad_norm': '136', 'counters/examples': 29024, 'counters/updates': 907}
train stats after 29056 examples: {'rewards_train/chosen': '0.082129', 'rewards_train/rejected': '0.042031', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040098', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-156.9', 'loss/train': '0.68266', 'examples_per_second': '31.385', 'grad_norm': '59.75', 'counters/examples': 29056, 'counters/updates': 908}
train stats after 29088 examples: {'rewards_train/chosen': '0.0049536', 'rewards_train/rejected': '-0.0057704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010724', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-163.71', 'loss/train': '0.69953', 'examples_per_second': '31.457', 'grad_norm': '208', 'counters/examples': 29088, 'counters/updates': 909}
train stats after 29120 examples: {'rewards_train/chosen': '-0.024124', 'rewards_train/rejected': '-0.0073117', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016813', 'logps_train/rejected': '-128.41', 'logps_train/chosen': '-174.3', 'loss/train': '0.71234', 'examples_per_second': '30.219', 'grad_norm': '74', 'counters/examples': 29120, 'counters/updates': 910}
train stats after 29152 examples: {'rewards_train/chosen': '0.056044', 'rewards_train/rejected': '-0.056045', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11209', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-159.41', 'loss/train': '0.65541', 'examples_per_second': '31.697', 'grad_norm': '84.5', 'counters/examples': 29152, 'counters/updates': 911}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29216 examples: {'rewards_train/chosen': '-0.029356', 'rewards_train/rejected': '-0.074378', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045021', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-155.43', 'loss/train': '0.68203', 'examples_per_second': '30.327', 'grad_norm': '84', 'counters/examples': 29216, 'counters/updates': 913}
train stats after 29248 examples: {'rewards_train/chosen': '0.031422', 'rewards_train/rejected': '-0.09615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12757', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-170.44', 'loss/train': '0.65869', 'examples_per_second': '31.676', 'grad_norm': '81', 'counters/examples': 29248, 'counters/updates': 914}
train stats after 29280 examples: {'rewards_train/chosen': '-0.00029745', 'rewards_train/rejected': '-0.037429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.037132', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-152.97', 'loss/train': '0.68407', 'examples_per_second': '31.541', 'grad_norm': '140', 'counters/examples': 29280, 'counters/updates': 915}
train stats after 29312 examples: {'rewards_train/chosen': '0.017732', 'rewards_train/rejected': '-0.017726', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.035459', 'logps_train/rejected': '-136.74', 'logps_train/chosen': '-156.33', 'loss/train': '0.68367', 'examples_per_second': '30.568', 'grad_norm': '88.5', 'counters/examples': 29312, 'counters/updates': 916}
train stats after 29344 examples: {'rewards_train/chosen': '0.032005', 'rewards_train/rejected': '-0.024378', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056383', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-189.45', 'loss/train': '0.67882', 'examples_per_second': '30.552', 'grad_norm': '97.5', 'counters/examples': 29344, 'counters/updates': 917}
train stats after 29376 examples: {'rewards_train/chosen': '-0.01048', 'rewards_train/rejected': '-0.008783', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0016968', 'logps_train/rejected': '-122', 'logps_train/chosen': '-147.2', 'loss/train': '0.70024', 'examples_per_second': '30.279', 'grad_norm': '132', 'counters/examples': 29376, 'counters/updates': 918}
train stats after 29408 examples: {'rewards_train/chosen': '0.0025195', 'rewards_train/rejected': '-0.010784', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013304', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-158.44', 'loss/train': '0.6966', 'examples_per_second': '32.512', 'grad_norm': '72', 'counters/examples': 29408, 'counters/updates': 919}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29472 examples: {'rewards_train/chosen': '-0.024808', 'rewards_train/rejected': '-0.02186', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0029474', 'logps_train/rejected': '-105.01', 'logps_train/chosen': '-155.74', 'loss/train': '0.70378', 'examples_per_second': '32.462', 'grad_norm': '58.25', 'counters/examples': 29472, 'counters/updates': 921}
train stats after 29504 examples: {'rewards_train/chosen': '0.0074686', 'rewards_train/rejected': '-0.015657', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023125', 'logps_train/rejected': '-95.289', 'logps_train/chosen': '-111.21', 'loss/train': '0.69233', 'examples_per_second': '31.347', 'grad_norm': '43.25', 'counters/examples': 29504, 'counters/updates': 922}
train stats after 29536 examples: {'rewards_train/chosen': '0.098049', 'rewards_train/rejected': '-0.029585', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12763', 'logps_train/rejected': '-148.85', 'logps_train/chosen': '-168.7', 'loss/train': '0.64142', 'examples_per_second': '30.694', 'grad_norm': '150', 'counters/examples': 29536, 'counters/updates': 923}
train stats after 29568 examples: {'rewards_train/chosen': '-0.038275', 'rewards_train/rejected': '-0.056341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018067', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-137.44', 'loss/train': '0.70146', 'examples_per_second': '31.407', 'grad_norm': '84', 'counters/examples': 29568, 'counters/updates': 924}
skipping logging after 29600 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '-0.0070215', 'rewards_train/rejected': '-0.0068182', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00020324', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-134.87', 'loss/train': '0.71352', 'examples_per_second': '31.682', 'grad_norm': '85.5', 'counters/examples': 29632, 'counters/updates': 926}
train stats after 29664 examples: {'rewards_train/chosen': '0.025931', 'rewards_train/rejected': '0.07828', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.052349', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-139.94', 'loss/train': '0.7422', 'examples_per_second': '31.975', 'grad_norm': '83.5', 'counters/examples': 29664, 'counters/updates': 927}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29728 examples: {'rewards_train/chosen': '0.00076594', 'rewards_train/rejected': '0.037417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036651', 'logps_train/rejected': '-162.65', 'logps_train/chosen': '-159.49', 'loss/train': '0.72788', 'examples_per_second': '31.407', 'grad_norm': '125.5', 'counters/examples': 29728, 'counters/updates': 929}
train stats after 29760 examples: {'rewards_train/chosen': '0.032935', 'rewards_train/rejected': '0.02569', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0072455', 'logps_train/rejected': '-154.16', 'logps_train/chosen': '-140.34', 'loss/train': '0.69974', 'examples_per_second': '30.845', 'grad_norm': '73.5', 'counters/examples': 29760, 'counters/updates': 930}
train stats after 29792 examples: {'rewards_train/chosen': '0.0067325', 'rewards_train/rejected': '-0.034896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041629', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-155.2', 'loss/train': '0.68753', 'examples_per_second': '30.555', 'grad_norm': '82.5', 'counters/examples': 29792, 'counters/updates': 931}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29856 examples: {'rewards_train/chosen': '-0.026029', 'rewards_train/rejected': '0.0082578', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034287', 'logps_train/rejected': '-134.04', 'logps_train/chosen': '-138.09', 'loss/train': '0.72106', 'examples_per_second': '34.795', 'grad_norm': '76', 'counters/examples': 29856, 'counters/updates': 933}
skipping logging after 29888 examples to avoid logging too frequently
train stats after 29920 examples: {'rewards_train/chosen': '-0.065256', 'rewards_train/rejected': '0.0092465', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.074503', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-120.84', 'loss/train': '0.74639', 'examples_per_second': '31.86', 'grad_norm': '140', 'counters/examples': 29920, 'counters/updates': 935}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 29984 examples: {'rewards_train/chosen': '0.0042089', 'rewards_train/rejected': '0.078179', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.07397', 'logps_train/rejected': '-108.63', 'logps_train/chosen': '-121.98', 'loss/train': '0.7509', 'examples_per_second': '35.928', 'grad_norm': '84.5', 'counters/examples': 29984, 'counters/updates': 937}
train stats after 30016 examples: {'rewards_train/chosen': '-0.022921', 'rewards_train/rejected': '0.017049', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03997', 'logps_train/rejected': '-136', 'logps_train/chosen': '-147.06', 'loss/train': '0.72404', 'examples_per_second': '25.943', 'grad_norm': '105', 'counters/examples': 30016, 'counters/updates': 938}
skipping logging after 30048 examples to avoid logging too frequently
train stats after 30080 examples: {'rewards_train/chosen': '-0.0072563', 'rewards_train/rejected': '-0.080572', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073316', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-157.26', 'loss/train': '0.66987', 'examples_per_second': '30.694', 'grad_norm': '92.5', 'counters/examples': 30080, 'counters/updates': 940}
train stats after 30112 examples: {'rewards_train/chosen': '0.062039', 'rewards_train/rejected': '0.018458', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04358', 'logps_train/rejected': '-140.19', 'logps_train/chosen': '-147.23', 'loss/train': '0.69288', 'examples_per_second': '24.067', 'grad_norm': '149', 'counters/examples': 30112, 'counters/updates': 941}
train stats after 30144 examples: {'rewards_train/chosen': '-0.11924', 'rewards_train/rejected': '-0.10385', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015387', 'logps_train/rejected': '-131.73', 'logps_train/chosen': '-142.57', 'loss/train': '0.71049', 'examples_per_second': '32.441', 'grad_norm': '123', 'counters/examples': 30144, 'counters/updates': 942}
train stats after 30176 examples: {'rewards_train/chosen': '0.022501', 'rewards_train/rejected': '0.019384', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0031166', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-123.8', 'loss/train': '0.69972', 'examples_per_second': '32.561', 'grad_norm': '118.5', 'counters/examples': 30176, 'counters/updates': 943}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30240 examples: {'rewards_train/chosen': '-0.054744', 'rewards_train/rejected': '0.040055', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.094798', 'logps_train/rejected': '-169', 'logps_train/chosen': '-162.03', 'loss/train': '0.74924', 'examples_per_second': '33.408', 'grad_norm': '100', 'counters/examples': 30240, 'counters/updates': 945}
train stats after 30272 examples: {'rewards_train/chosen': '0.072647', 'rewards_train/rejected': '0.073465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0008182', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-152.96', 'loss/train': '0.72387', 'examples_per_second': '31.708', 'grad_norm': '60.25', 'counters/examples': 30272, 'counters/updates': 946}
train stats after 30304 examples: {'rewards_train/chosen': '0.016069', 'rewards_train/rejected': '0.055019', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03895', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-150.22', 'loss/train': '0.73407', 'examples_per_second': '30.945', 'grad_norm': '132', 'counters/examples': 30304, 'counters/updates': 947}
train stats after 30336 examples: {'rewards_train/chosen': '-0.002645', 'rewards_train/rejected': '-0.068819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066174', 'logps_train/rejected': '-165.82', 'logps_train/chosen': '-166.94', 'loss/train': '0.69018', 'examples_per_second': '31.697', 'grad_norm': '96.5', 'counters/examples': 30336, 'counters/updates': 948}
train stats after 30368 examples: {'rewards_train/chosen': '-0.057943', 'rewards_train/rejected': '0.064478', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.12242', 'logps_train/rejected': '-138.96', 'logps_train/chosen': '-153.38', 'loss/train': '0.7727', 'examples_per_second': '32.336', 'grad_norm': '98', 'counters/examples': 30368, 'counters/updates': 949}
train stats after 30400 examples: {'rewards_train/chosen': '0.024637', 'rewards_train/rejected': '-0.068144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092781', 'logps_train/rejected': '-163.84', 'logps_train/chosen': '-155.08', 'loss/train': '0.65612', 'examples_per_second': '31.68', 'grad_norm': '66.5', 'counters/examples': 30400, 'counters/updates': 950}
skipping logging after 30432 examples to avoid logging too frequently
train stats after 30464 examples: {'rewards_train/chosen': '0.024818', 'rewards_train/rejected': '-0.027486', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052305', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-151.45', 'loss/train': '0.6819', 'examples_per_second': '30.219', 'grad_norm': '57.5', 'counters/examples': 30464, 'counters/updates': 952}
train stats after 30496 examples: {'rewards_train/chosen': '0.0015505', 'rewards_train/rejected': '0.041559', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.040009', 'logps_train/rejected': '-157.88', 'logps_train/chosen': '-146.05', 'loss/train': '0.7325', 'examples_per_second': '31.705', 'grad_norm': '122', 'counters/examples': 30496, 'counters/updates': 953}
skipping logging after 30528 examples to avoid logging too frequently
train stats after 30560 examples: {'rewards_train/chosen': '0.0064277', 'rewards_train/rejected': '0.031056', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.024628', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-132.09', 'loss/train': '0.71929', 'examples_per_second': '30.785', 'grad_norm': '94', 'counters/examples': 30560, 'counters/updates': 955}
train stats after 30592 examples: {'rewards_train/chosen': '0.076148', 'rewards_train/rejected': '-0.034597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11075', 'logps_train/rejected': '-126.2', 'logps_train/chosen': '-158.95', 'loss/train': '0.65348', 'examples_per_second': '30.874', 'grad_norm': '123', 'counters/examples': 30592, 'counters/updates': 956}
train stats after 30624 examples: {'rewards_train/chosen': '-0.025565', 'rewards_train/rejected': '0.010206', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035771', 'logps_train/rejected': '-162.49', 'logps_train/chosen': '-153.19', 'loss/train': '0.7306', 'examples_per_second': '32.166', 'grad_norm': '221', 'counters/examples': 30624, 'counters/updates': 957}
train stats after 30656 examples: {'rewards_train/chosen': '0.038996', 'rewards_train/rejected': '0.013259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025737', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-147.2', 'loss/train': '0.69167', 'examples_per_second': '30.353', 'grad_norm': '57.75', 'counters/examples': 30656, 'counters/updates': 958}
skipping logging after 30688 examples to avoid logging too frequently
train stats after 30720 examples: {'rewards_train/chosen': '-0.0032937', 'rewards_train/rejected': '-0.01026', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0069661', 'logps_train/rejected': '-126', 'logps_train/chosen': '-131.27', 'loss/train': '0.69617', 'examples_per_second': '35.426', 'grad_norm': '80', 'counters/examples': 30720, 'counters/updates': 960}
train stats after 30752 examples: {'rewards_train/chosen': '0.071453', 'rewards_train/rejected': '0.022174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049279', 'logps_train/rejected': '-112.28', 'logps_train/chosen': '-146.48', 'loss/train': '0.67924', 'examples_per_second': '25.767', 'grad_norm': '74', 'counters/examples': 30752, 'counters/updates': 961}
train stats after 30784 examples: {'rewards_train/chosen': '-0.088712', 'rewards_train/rejected': '-0.093096', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0043835', 'logps_train/rejected': '-125.56', 'logps_train/chosen': '-126.31', 'loss/train': '0.70301', 'examples_per_second': '31.683', 'grad_norm': '111', 'counters/examples': 30784, 'counters/updates': 962}
train stats after 30816 examples: {'rewards_train/chosen': '0.022253', 'rewards_train/rejected': '0.033433', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01118', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-180.03', 'loss/train': '0.70894', 'examples_per_second': '31.342', 'grad_norm': '126', 'counters/examples': 30816, 'counters/updates': 963}
train stats after 30848 examples: {'rewards_train/chosen': '0.0034857', 'rewards_train/rejected': '0.090854', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.087368', 'logps_train/rejected': '-151.44', 'logps_train/chosen': '-177.19', 'loss/train': '0.74901', 'examples_per_second': '31.632', 'grad_norm': '76.5', 'counters/examples': 30848, 'counters/updates': 964}
train stats after 30880 examples: {'rewards_train/chosen': '-0.10522', 'rewards_train/rejected': '0.038218', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.14344', 'logps_train/rejected': '-104', 'logps_train/chosen': '-155.52', 'loss/train': '0.7782', 'examples_per_second': '31.508', 'grad_norm': '140', 'counters/examples': 30880, 'counters/updates': 965}
train stats after 30912 examples: {'rewards_train/chosen': '-0.027661', 'rewards_train/rejected': '0.0024942', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030155', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-175.06', 'loss/train': '0.72104', 'examples_per_second': '30.221', 'grad_norm': '191', 'counters/examples': 30912, 'counters/updates': 966}
train stats after 30944 examples: {'rewards_train/chosen': '0.017289', 'rewards_train/rejected': '-0.0056927', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.022982', 'logps_train/rejected': '-151.88', 'logps_train/chosen': '-139.42', 'loss/train': '0.69771', 'examples_per_second': '31.675', 'grad_norm': '202', 'counters/examples': 30944, 'counters/updates': 967}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31008 examples: {'rewards_train/chosen': '-0.030425', 'rewards_train/rejected': '0.029363', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.059788', 'logps_train/rejected': '-125.19', 'logps_train/chosen': '-156.55', 'loss/train': '0.73533', 'examples_per_second': '34.441', 'grad_norm': '99.5', 'counters/examples': 31008, 'counters/updates': 969}
train stats after 31040 examples: {'rewards_train/chosen': '-0.0081817', 'rewards_train/rejected': '-0.048728', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040546', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-110.02', 'loss/train': '0.68416', 'examples_per_second': '31.444', 'grad_norm': '62.5', 'counters/examples': 31040, 'counters/updates': 970}
skipping logging after 31072 examples to avoid logging too frequently
train stats after 31104 examples: {'rewards_train/chosen': '0.019091', 'rewards_train/rejected': '0.018826', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00026495', 'logps_train/rejected': '-169.38', 'logps_train/chosen': '-129.89', 'loss/train': '0.71459', 'examples_per_second': '32.981', 'grad_norm': '125.5', 'counters/examples': 31104, 'counters/updates': 972}
train stats after 31136 examples: {'rewards_train/chosen': '0.075496', 'rewards_train/rejected': '0.018467', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057029', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-159.19', 'loss/train': '0.67864', 'examples_per_second': '30.709', 'grad_norm': '78.5', 'counters/examples': 31136, 'counters/updates': 973}
train stats after 31168 examples: {'rewards_train/chosen': '-0.019209', 'rewards_train/rejected': '0.037016', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.056226', 'logps_train/rejected': '-182.16', 'logps_train/chosen': '-151.37', 'loss/train': '0.73839', 'examples_per_second': '31.581', 'grad_norm': '114', 'counters/examples': 31168, 'counters/updates': 974}
train stats after 31200 examples: {'rewards_train/chosen': '-0.044755', 'rewards_train/rejected': '-0.036682', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0080724', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-171.83', 'loss/train': '0.70573', 'examples_per_second': '32.85', 'grad_norm': '70.5', 'counters/examples': 31200, 'counters/updates': 975}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31264 examples: {'rewards_train/chosen': '0.033041', 'rewards_train/rejected': '0.019806', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013235', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-180.22', 'loss/train': '0.70168', 'examples_per_second': '31.516', 'grad_norm': '69.5', 'counters/examples': 31264, 'counters/updates': 977}
train stats after 31296 examples: {'rewards_train/chosen': '0.0085866', 'rewards_train/rejected': '0.0020588', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0065278', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-141', 'loss/train': '0.69764', 'examples_per_second': '30.705', 'grad_norm': '68.5', 'counters/examples': 31296, 'counters/updates': 978}
train stats after 31328 examples: {'rewards_train/chosen': '0.036905', 'rewards_train/rejected': '0.022818', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014087', 'logps_train/rejected': '-100.88', 'logps_train/chosen': '-158.66', 'loss/train': '0.69308', 'examples_per_second': '29.647', 'grad_norm': '94', 'counters/examples': 31328, 'counters/updates': 979}
train stats after 31360 examples: {'rewards_train/chosen': '0.031266', 'rewards_train/rejected': '-0.031383', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062649', 'logps_train/rejected': '-174.43', 'logps_train/chosen': '-175.71', 'loss/train': '0.68483', 'examples_per_second': '31.863', 'grad_norm': '137', 'counters/examples': 31360, 'counters/updates': 980}
train stats after 31392 examples: {'rewards_train/chosen': '0.029272', 'rewards_train/rejected': '-0.030206', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059478', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-169.87', 'loss/train': '0.67337', 'examples_per_second': '30.816', 'grad_norm': '55.75', 'counters/examples': 31392, 'counters/updates': 981}
train stats after 31424 examples: {'rewards_train/chosen': '0.011926', 'rewards_train/rejected': '-0.019509', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031435', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-145.75', 'loss/train': '0.68788', 'examples_per_second': '30.696', 'grad_norm': '86.5', 'counters/examples': 31424, 'counters/updates': 982}
train stats after 31456 examples: {'rewards_train/chosen': '-0.0091129', 'rewards_train/rejected': '-0.022673', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013561', 'logps_train/rejected': '-142.34', 'logps_train/chosen': '-178.56', 'loss/train': '0.69662', 'examples_per_second': '31.639', 'grad_norm': '125', 'counters/examples': 31456, 'counters/updates': 983}
train stats after 31488 examples: {'rewards_train/chosen': '0.01967', 'rewards_train/rejected': '-0.099889', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11956', 'logps_train/rejected': '-147.06', 'logps_train/chosen': '-167.47', 'loss/train': '0.64885', 'examples_per_second': '31.691', 'grad_norm': '150', 'counters/examples': 31488, 'counters/updates': 984}
skipping logging after 31520 examples to avoid logging too frequently
train stats after 31552 examples: {'rewards_train/chosen': '0.074443', 'rewards_train/rejected': '0.036333', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03811', 'logps_train/rejected': '-139.27', 'logps_train/chosen': '-160.26', 'loss/train': '0.6831', 'examples_per_second': '31.625', 'grad_norm': '83.5', 'counters/examples': 31552, 'counters/updates': 986}
train stats after 31584 examples: {'rewards_train/chosen': '-0.045897', 'rewards_train/rejected': '0.0027518', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.048649', 'logps_train/rejected': '-154.85', 'logps_train/chosen': '-154.92', 'loss/train': '0.73166', 'examples_per_second': '31.629', 'grad_norm': '141', 'counters/examples': 31584, 'counters/updates': 987}
train stats after 31616 examples: {'rewards_train/chosen': '0.062502', 'rewards_train/rejected': '-0.049356', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11186', 'logps_train/rejected': '-127.75', 'logps_train/chosen': '-167.66', 'loss/train': '0.652', 'examples_per_second': '30.884', 'grad_norm': '241', 'counters/examples': 31616, 'counters/updates': 988}
train stats after 31648 examples: {'rewards_train/chosen': '-0.019141', 'rewards_train/rejected': '0.040715', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059856', 'logps_train/rejected': '-146.62', 'logps_train/chosen': '-152.05', 'loss/train': '0.74315', 'examples_per_second': '31.098', 'grad_norm': '290', 'counters/examples': 31648, 'counters/updates': 989}
train stats after 31680 examples: {'rewards_train/chosen': '0.019834', 'rewards_train/rejected': '-0.030841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050675', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-175.26', 'loss/train': '0.68077', 'examples_per_second': '31.636', 'grad_norm': '143', 'counters/examples': 31680, 'counters/updates': 990}
train stats after 31712 examples: {'rewards_train/chosen': '-0.12276', 'rewards_train/rejected': '-0.0361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.086662', 'logps_train/rejected': '-120.61', 'logps_train/chosen': '-163.98', 'loss/train': '0.76495', 'examples_per_second': '31.082', 'grad_norm': '472', 'counters/examples': 31712, 'counters/updates': 991}
train stats after 31744 examples: {'rewards_train/chosen': '-0.086429', 'rewards_train/rejected': '0.023621', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.11005', 'logps_train/rejected': '-168.2', 'logps_train/chosen': '-179.24', 'loss/train': '0.77808', 'examples_per_second': '31.401', 'grad_norm': '124.5', 'counters/examples': 31744, 'counters/updates': 992}
skipping logging after 31776 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '0.039686', 'rewards_train/rejected': '-0.0050289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044715', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-190.73', 'loss/train': '0.68804', 'examples_per_second': '31.674', 'grad_norm': '175', 'counters/examples': 31808, 'counters/updates': 994}
train stats after 31840 examples: {'rewards_train/chosen': '0.02785', 'rewards_train/rejected': '0.03331', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0054602', 'logps_train/rejected': '-98.72', 'logps_train/chosen': '-111.07', 'loss/train': '0.70712', 'examples_per_second': '31.677', 'grad_norm': '69', 'counters/examples': 31840, 'counters/updates': 995}
train stats after 31872 examples: {'rewards_train/chosen': '0.047678', 'rewards_train/rejected': '-0.0061013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05378', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-148.02', 'loss/train': '0.67387', 'examples_per_second': '32.224', 'grad_norm': '142', 'counters/examples': 31872, 'counters/updates': 996}
train stats after 31904 examples: {'rewards_train/chosen': '-0.00049251', 'rewards_train/rejected': '0.063908', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0644', 'logps_train/rejected': '-174.01', 'logps_train/chosen': '-132.24', 'loss/train': '0.74558', 'examples_per_second': '30.69', 'grad_norm': '104', 'counters/examples': 31904, 'counters/updates': 997}
skipping logging after 31936 examples to avoid logging too frequently
train stats after 31968 examples: {'rewards_train/chosen': '0.14162', 'rewards_train/rejected': '-0.00835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14997', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-162.74', 'loss/train': '0.65446', 'examples_per_second': '32.239', 'grad_norm': '125.5', 'counters/examples': 31968, 'counters/updates': 999}
train stats after 32000 examples: {'rewards_train/chosen': '0.041061', 'rewards_train/rejected': '-0.042738', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083799', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-158.59', 'loss/train': '0.67072', 'examples_per_second': '31.351', 'grad_norm': '256', 'counters/examples': 32000, 'counters/updates': 1000}
train stats after 32032 examples: {'rewards_train/chosen': '0.0064882', 'rewards_train/rejected': '0.011798', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0053094', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-173.32', 'loss/train': '0.7082', 'examples_per_second': '31.683', 'grad_norm': '125', 'counters/examples': 32032, 'counters/updates': 1001}
train stats after 32064 examples: {'rewards_train/chosen': '0.035801', 'rewards_train/rejected': '-0.028229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06403', 'logps_train/rejected': '-142.76', 'logps_train/chosen': '-143.27', 'loss/train': '0.67072', 'examples_per_second': '31.642', 'grad_norm': '94.5', 'counters/examples': 32064, 'counters/updates': 1002}
train stats after 32096 examples: {'rewards_train/chosen': '0.033214', 'rewards_train/rejected': '-0.054092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087306', 'logps_train/rejected': '-135.26', 'logps_train/chosen': '-109.39', 'loss/train': '0.66071', 'examples_per_second': '31.878', 'grad_norm': '104.5', 'counters/examples': 32096, 'counters/updates': 1003}
train stats after 32128 examples: {'rewards_train/chosen': '-0.019949', 'rewards_train/rejected': '-0.011298', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0086508', 'logps_train/rejected': '-112.24', 'logps_train/chosen': '-131.13', 'loss/train': '0.7052', 'examples_per_second': '31.769', 'grad_norm': '72', 'counters/examples': 32128, 'counters/updates': 1004}
skipping logging after 32160 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '0.082822', 'rewards_train/rejected': '-0.0042786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087101', 'logps_train/rejected': '-160.83', 'logps_train/chosen': '-164.7', 'loss/train': '0.66415', 'examples_per_second': '30.255', 'grad_norm': '129', 'counters/examples': 32192, 'counters/updates': 1006}
train stats after 32224 examples: {'rewards_train/chosen': '-0.055816', 'rewards_train/rejected': '-0.014863', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040953', 'logps_train/rejected': '-121.83', 'logps_train/chosen': '-128.13', 'loss/train': '0.72225', 'examples_per_second': '30.624', 'grad_norm': '80', 'counters/examples': 32224, 'counters/updates': 1007}
train stats after 32256 examples: {'rewards_train/chosen': '0.048605', 'rewards_train/rejected': '0.011801', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036804', 'logps_train/rejected': '-108.51', 'logps_train/chosen': '-131.8', 'loss/train': '0.68783', 'examples_per_second': '30.702', 'grad_norm': '71.5', 'counters/examples': 32256, 'counters/updates': 1008}
skipping logging after 32288 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '-0.0040356', 'rewards_train/rejected': '0.00067563', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0047112', 'logps_train/rejected': '-107.91', 'logps_train/chosen': '-159.25', 'loss/train': '0.70178', 'examples_per_second': '32.86', 'grad_norm': '49.75', 'counters/examples': 32320, 'counters/updates': 1010}
train stats after 32352 examples: {'rewards_train/chosen': '-0.047755', 'rewards_train/rejected': '-0.052803', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0050478', 'logps_train/rejected': '-139.95', 'logps_train/chosen': '-147.28', 'loss/train': '0.70079', 'examples_per_second': '31.255', 'grad_norm': '100.5', 'counters/examples': 32352, 'counters/updates': 1011}
skipping logging after 32384 examples to avoid logging too frequently
train stats after 32416 examples: {'rewards_train/chosen': '-0.0021729', 'rewards_train/rejected': '0.0033224', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0054953', 'logps_train/rejected': '-134.04', 'logps_train/chosen': '-142.32', 'loss/train': '0.71143', 'examples_per_second': '31.696', 'grad_norm': '79.5', 'counters/examples': 32416, 'counters/updates': 1013}
train stats after 32448 examples: {'rewards_train/chosen': '-0.049566', 'rewards_train/rejected': '-0.022108', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.027458', 'logps_train/rejected': '-154.8', 'logps_train/chosen': '-176.46', 'loss/train': '0.72601', 'examples_per_second': '31.725', 'grad_norm': '78.5', 'counters/examples': 32448, 'counters/updates': 1014}
train stats after 32480 examples: {'rewards_train/chosen': '-0.076338', 'rewards_train/rejected': '-0.037858', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03848', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-119.81', 'loss/train': '0.71787', 'examples_per_second': '31.677', 'grad_norm': '119', 'counters/examples': 32480, 'counters/updates': 1015}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32544 examples: {'rewards_train/chosen': '0.0046909', 'rewards_train/rejected': '-0.099', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10369', 'logps_train/rejected': '-118.14', 'logps_train/chosen': '-151.3', 'loss/train': '0.65755', 'examples_per_second': '33.708', 'grad_norm': '63.75', 'counters/examples': 32544, 'counters/updates': 1017}
train stats after 32576 examples: {'rewards_train/chosen': '0.090725', 'rewards_train/rejected': '0.029696', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061029', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-143.72', 'loss/train': '0.67308', 'examples_per_second': '31.678', 'grad_norm': '120.5', 'counters/examples': 32576, 'counters/updates': 1018}
train stats after 32608 examples: {'rewards_train/chosen': '0.11951', 'rewards_train/rejected': '-0.001147', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12065', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-137.47', 'loss/train': '0.65239', 'examples_per_second': '32.777', 'grad_norm': '93.5', 'counters/examples': 32608, 'counters/updates': 1019}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32672 examples: {'rewards_train/chosen': '-0.042947', 'rewards_train/rejected': '-0.052551', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096036', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-171.07', 'loss/train': '0.70818', 'examples_per_second': '31.694', 'grad_norm': '84', 'counters/examples': 32672, 'counters/updates': 1021}
skipping logging after 32704 examples to avoid logging too frequently
train stats after 32736 examples: {'rewards_train/chosen': '-0.070455', 'rewards_train/rejected': '-0.0017367', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068718', 'logps_train/rejected': '-144.71', 'logps_train/chosen': '-175.77', 'loss/train': '0.7383', 'examples_per_second': '30.12', 'grad_norm': '121.5', 'counters/examples': 32736, 'counters/updates': 1023}
train stats after 32768 examples: {'rewards_train/chosen': '-0.009737', 'rewards_train/rejected': '-0.010149', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00041155', 'logps_train/rejected': '-179.23', 'logps_train/chosen': '-171.59', 'loss/train': '0.7019', 'examples_per_second': '31.578', 'grad_norm': '133', 'counters/examples': 32768, 'counters/updates': 1024}
skipping logging after 32800 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '-0.02028', 'rewards_train/rejected': '0.052476', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.072757', 'logps_train/rejected': '-137.23', 'logps_train/chosen': '-131.59', 'loss/train': '0.73763', 'examples_per_second': '31.661', 'grad_norm': '98', 'counters/examples': 32832, 'counters/updates': 1026}
train stats after 32864 examples: {'rewards_train/chosen': '0.069056', 'rewards_train/rejected': '0.035432', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033625', 'logps_train/rejected': '-147.06', 'logps_train/chosen': '-161.97', 'loss/train': '0.69186', 'examples_per_second': '31.193', 'grad_norm': '130', 'counters/examples': 32864, 'counters/updates': 1027}
train stats after 32896 examples: {'rewards_train/chosen': '-0.075037', 'rewards_train/rejected': '-0.0012262', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.073811', 'logps_train/rejected': '-142.53', 'logps_train/chosen': '-143.69', 'loss/train': '0.7362', 'examples_per_second': '31.867', 'grad_norm': '100', 'counters/examples': 32896, 'counters/updates': 1028}
skipping logging after 32928 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '-0.1041', 'rewards_train/rejected': '0.10307', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.20717', 'logps_train/rejected': '-172.53', 'logps_train/chosen': '-155.48', 'loss/train': '0.84587', 'examples_per_second': '30.664', 'grad_norm': '268', 'counters/examples': 32960, 'counters/updates': 1030}
train stats after 32992 examples: {'rewards_train/chosen': '-0.033457', 'rewards_train/rejected': '-0.022885', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010573', 'logps_train/rejected': '-139.12', 'logps_train/chosen': '-132.42', 'loss/train': '0.70838', 'examples_per_second': '31.276', 'grad_norm': '65', 'counters/examples': 32992, 'counters/updates': 1031}
train stats after 33024 examples: {'rewards_train/chosen': '0.015487', 'rewards_train/rejected': '-0.037731', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053218', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-118.94', 'loss/train': '0.67388', 'examples_per_second': '31.666', 'grad_norm': '73.5', 'counters/examples': 33024, 'counters/updates': 1032}
train stats after 33056 examples: {'rewards_train/chosen': '-0.013005', 'rewards_train/rejected': '-0.031567', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018561', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-162.75', 'loss/train': '0.69286', 'examples_per_second': '31.829', 'grad_norm': '89.5', 'counters/examples': 33056, 'counters/updates': 1033}
train stats after 33088 examples: {'rewards_train/chosen': '-0.015244', 'rewards_train/rejected': '0.072022', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.087265', 'logps_train/rejected': '-202.3', 'logps_train/chosen': '-180.64', 'loss/train': '0.74758', 'examples_per_second': '31.709', 'grad_norm': '140', 'counters/examples': 33088, 'counters/updates': 1034}
skipping logging after 33120 examples to avoid logging too frequently
train stats after 33152 examples: {'rewards_train/chosen': '0.010097', 'rewards_train/rejected': '0.030506', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020409', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-153.37', 'loss/train': '0.72293', 'examples_per_second': '30.684', 'grad_norm': '77.5', 'counters/examples': 33152, 'counters/updates': 1036}
train stats after 33184 examples: {'rewards_train/chosen': '0.016106', 'rewards_train/rejected': '0.0015956', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.01451', 'logps_train/rejected': '-151', 'logps_train/chosen': '-148.74', 'loss/train': '0.70492', 'examples_per_second': '31.441', 'grad_norm': '102', 'counters/examples': 33184, 'counters/updates': 1037}
train stats after 33216 examples: {'rewards_train/chosen': '-0.063655', 'rewards_train/rejected': '-0.024844', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038811', 'logps_train/rejected': '-135.54', 'logps_train/chosen': '-148.11', 'loss/train': '0.72829', 'examples_per_second': '30.614', 'grad_norm': '117.5', 'counters/examples': 33216, 'counters/updates': 1038}
train stats after 33248 examples: {'rewards_train/chosen': '0.021829', 'rewards_train/rejected': '0.0044484', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01738', 'logps_train/rejected': '-190.42', 'logps_train/chosen': '-192.71', 'loss/train': '0.69767', 'examples_per_second': '30.158', 'grad_norm': '149', 'counters/examples': 33248, 'counters/updates': 1039}
train stats after 33280 examples: {'rewards_train/chosen': '0.010057', 'rewards_train/rejected': '-0.088286', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098343', 'logps_train/rejected': '-167.93', 'logps_train/chosen': '-142.34', 'loss/train': '0.66336', 'examples_per_second': '32.59', 'grad_norm': '76', 'counters/examples': 33280, 'counters/updates': 1040}
train stats after 33312 examples: {'rewards_train/chosen': '0.014631', 'rewards_train/rejected': '-0.027246', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041877', 'logps_train/rejected': '-147.04', 'logps_train/chosen': '-170.23', 'loss/train': '0.68811', 'examples_per_second': '32.011', 'grad_norm': '98', 'counters/examples': 33312, 'counters/updates': 1041}
train stats after 33344 examples: {'rewards_train/chosen': '-0.056726', 'rewards_train/rejected': '-0.052876', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00385', 'logps_train/rejected': '-167.51', 'logps_train/chosen': '-160.7', 'loss/train': '0.71207', 'examples_per_second': '33.03', 'grad_norm': '93', 'counters/examples': 33344, 'counters/updates': 1042}
train stats after 33376 examples: {'rewards_train/chosen': '-0.031753', 'rewards_train/rejected': '-0.0073005', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024452', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-186.35', 'loss/train': '0.73041', 'examples_per_second': '32.62', 'grad_norm': '95', 'counters/examples': 33376, 'counters/updates': 1043}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33440 examples: {'rewards_train/chosen': '-0.038217', 'rewards_train/rejected': '-0.0080726', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030144', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-173.01', 'loss/train': '0.71615', 'examples_per_second': '33.45', 'grad_norm': '106.5', 'counters/examples': 33440, 'counters/updates': 1045}
train stats after 33472 examples: {'rewards_train/chosen': '-0.020845', 'rewards_train/rejected': '-0.03436', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013515', 'logps_train/rejected': '-149.74', 'logps_train/chosen': '-157.71', 'loss/train': '0.69266', 'examples_per_second': '31.651', 'grad_norm': '131', 'counters/examples': 33472, 'counters/updates': 1046}
skipping logging after 33504 examples to avoid logging too frequently
train stats after 33536 examples: {'rewards_train/chosen': '0.032616', 'rewards_train/rejected': '0.0048288', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027787', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-167.08', 'loss/train': '0.70581', 'examples_per_second': '33.112', 'grad_norm': '102.5', 'counters/examples': 33536, 'counters/updates': 1048}
train stats after 33568 examples: {'rewards_train/chosen': '0.038274', 'rewards_train/rejected': '0.05048', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012205', 'logps_train/rejected': '-115.28', 'logps_train/chosen': '-140.6', 'loss/train': '0.71122', 'examples_per_second': '31.539', 'grad_norm': '70', 'counters/examples': 33568, 'counters/updates': 1049}
train stats after 33600 examples: {'rewards_train/chosen': '-0.019798', 'rewards_train/rejected': '0.06773', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.087528', 'logps_train/rejected': '-121.71', 'logps_train/chosen': '-203.5', 'loss/train': '0.76029', 'examples_per_second': '31.703', 'grad_norm': '118.5', 'counters/examples': 33600, 'counters/updates': 1050}
train stats after 33632 examples: {'rewards_train/chosen': '0.0069618', 'rewards_train/rejected': '-0.075844', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082805', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-167.61', 'loss/train': '0.6627', 'examples_per_second': '31.636', 'grad_norm': '103', 'counters/examples': 33632, 'counters/updates': 1051}
train stats after 33664 examples: {'rewards_train/chosen': '0.042519', 'rewards_train/rejected': '0.026308', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016211', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-140', 'loss/train': '0.69315', 'examples_per_second': '30.201', 'grad_norm': '133', 'counters/examples': 33664, 'counters/updates': 1052}
train stats after 33696 examples: {'rewards_train/chosen': '-0.0052318', 'rewards_train/rejected': '-0.035295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030063', 'logps_train/rejected': '-121.93', 'logps_train/chosen': '-165.67', 'loss/train': '0.68719', 'examples_per_second': '30.203', 'grad_norm': '81.5', 'counters/examples': 33696, 'counters/updates': 1053}
train stats after 33728 examples: {'rewards_train/chosen': '-0.051602', 'rewards_train/rejected': '0.044523', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.096125', 'logps_train/rejected': '-153.64', 'logps_train/chosen': '-145.72', 'loss/train': '0.75225', 'examples_per_second': '30.5', 'grad_norm': '85.5', 'counters/examples': 33728, 'counters/updates': 1054}
train stats after 33760 examples: {'rewards_train/chosen': '0.05037', 'rewards_train/rejected': '-0.029133', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079503', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-165.28', 'loss/train': '0.66621', 'examples_per_second': '30.63', 'grad_norm': '71', 'counters/examples': 33760, 'counters/updates': 1055}
train stats after 33792 examples: {'rewards_train/chosen': '0.029441', 'rewards_train/rejected': '0.075449', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.046007', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-163.7', 'loss/train': '0.73112', 'examples_per_second': '31.611', 'grad_norm': '139', 'counters/examples': 33792, 'counters/updates': 1056}
train stats after 33824 examples: {'rewards_train/chosen': '0.064247', 'rewards_train/rejected': '-0.0027142', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066961', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-168.87', 'loss/train': '0.66574', 'examples_per_second': '30.357', 'grad_norm': '89', 'counters/examples': 33824, 'counters/updates': 1057}
train stats after 33856 examples: {'rewards_train/chosen': '3.0709e-05', 'rewards_train/rejected': '-0.0098312', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.009862', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-134.21', 'loss/train': '0.70032', 'examples_per_second': '31.641', 'grad_norm': '78', 'counters/examples': 33856, 'counters/updates': 1058}
train stats after 33888 examples: {'rewards_train/chosen': '-0.036456', 'rewards_train/rejected': '0.030852', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.067308', 'logps_train/rejected': '-132.02', 'logps_train/chosen': '-164.02', 'loss/train': '0.74115', 'examples_per_second': '31.673', 'grad_norm': '79', 'counters/examples': 33888, 'counters/updates': 1059}
train stats after 33920 examples: {'rewards_train/chosen': '-0.074676', 'rewards_train/rejected': '-0.010918', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063758', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-158.23', 'loss/train': '0.7495', 'examples_per_second': '31.655', 'grad_norm': '74', 'counters/examples': 33920, 'counters/updates': 1060}
train stats after 33952 examples: {'rewards_train/chosen': '0.0075785', 'rewards_train/rejected': '-0.012787', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020366', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-146.84', 'loss/train': '0.71056', 'examples_per_second': '32.128', 'grad_norm': '272', 'counters/examples': 33952, 'counters/updates': 1061}
train stats after 33984 examples: {'rewards_train/chosen': '0.036745', 'rewards_train/rejected': '-0.072465', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10921', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-173.98', 'loss/train': '0.66598', 'examples_per_second': '31.601', 'grad_norm': '95', 'counters/examples': 33984, 'counters/updates': 1062}
train stats after 34016 examples: {'rewards_train/chosen': '0.12257', 'rewards_train/rejected': '-0.010634', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13321', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-147.06', 'loss/train': '0.64386', 'examples_per_second': '30.556', 'grad_norm': '139', 'counters/examples': 34016, 'counters/updates': 1063}
train stats after 34048 examples: {'rewards_train/chosen': '0.027504', 'rewards_train/rejected': '0.050411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022907', 'logps_train/rejected': '-130.29', 'logps_train/chosen': '-151.03', 'loss/train': '0.71316', 'examples_per_second': '30.624', 'grad_norm': '75', 'counters/examples': 34048, 'counters/updates': 1064}
train stats after 34080 examples: {'rewards_train/chosen': '0.0037025', 'rewards_train/rejected': '-0.022454', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026157', 'logps_train/rejected': '-96.121', 'logps_train/chosen': '-167.94', 'loss/train': '0.69214', 'examples_per_second': '30.236', 'grad_norm': '73.5', 'counters/examples': 34080, 'counters/updates': 1065}
skipping logging after 34112 examples to avoid logging too frequently
train stats after 34144 examples: {'rewards_train/chosen': '0.050341', 'rewards_train/rejected': '-0.044732', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095074', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-140.46', 'loss/train': '0.65735', 'examples_per_second': '34.136', 'grad_norm': '95.5', 'counters/examples': 34144, 'counters/updates': 1067}
train stats after 34176 examples: {'rewards_train/chosen': '0.059875', 'rewards_train/rejected': '0.043488', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016387', 'logps_train/rejected': '-145.73', 'logps_train/chosen': '-172.68', 'loss/train': '0.69079', 'examples_per_second': '31.697', 'grad_norm': '116.5', 'counters/examples': 34176, 'counters/updates': 1068}
train stats after 34208 examples: {'rewards_train/chosen': '-0.031118', 'rewards_train/rejected': '-0.10476', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.07364', 'logps_train/rejected': '-105.85', 'logps_train/chosen': '-138.25', 'loss/train': '0.669', 'examples_per_second': '32.84', 'grad_norm': '217', 'counters/examples': 34208, 'counters/updates': 1069}
train stats after 34240 examples: {'rewards_train/chosen': '0.056276', 'rewards_train/rejected': '0.087518', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.031242', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-159.56', 'loss/train': '0.72077', 'examples_per_second': '31.868', 'grad_norm': '63.25', 'counters/examples': 34240, 'counters/updates': 1070}
train stats after 34272 examples: {'rewards_train/chosen': '0.018423', 'rewards_train/rejected': '-0.0067103', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025133', 'logps_train/rejected': '-152.47', 'logps_train/chosen': '-198.6', 'loss/train': '0.69803', 'examples_per_second': '31.602', 'grad_norm': '97.5', 'counters/examples': 34272, 'counters/updates': 1071}
train stats after 34304 examples: {'rewards_train/chosen': '0.058571', 'rewards_train/rejected': '0.061908', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0033363', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-158.76', 'loss/train': '0.70542', 'examples_per_second': '31.811', 'grad_norm': '81.5', 'counters/examples': 34304, 'counters/updates': 1072}
train stats after 34336 examples: {'rewards_train/chosen': '0.021218', 'rewards_train/rejected': '0.00871', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.012508', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-100.92', 'loss/train': '0.70704', 'examples_per_second': '31.657', 'grad_norm': '82.5', 'counters/examples': 34336, 'counters/updates': 1073}
skipping logging after 34368 examples to avoid logging too frequently
train stats after 34400 examples: {'rewards_train/chosen': '-0.076903', 'rewards_train/rejected': '-0.0054663', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.071437', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-188.74', 'loss/train': '0.73977', 'examples_per_second': '30.439', 'grad_norm': '89', 'counters/examples': 34400, 'counters/updates': 1075}
train stats after 34432 examples: {'rewards_train/chosen': '0.017785', 'rewards_train/rejected': '0.012254', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0055306', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-150.44', 'loss/train': '0.70436', 'examples_per_second': '31.088', 'grad_norm': '77.5', 'counters/examples': 34432, 'counters/updates': 1076}
train stats after 34464 examples: {'rewards_train/chosen': '-0.050048', 'rewards_train/rejected': '0.015019', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.065066', 'logps_train/rejected': '-162.57', 'logps_train/chosen': '-142.65', 'loss/train': '0.75125', 'examples_per_second': '31.63', 'grad_norm': '132', 'counters/examples': 34464, 'counters/updates': 1077}
train stats after 34496 examples: {'rewards_train/chosen': '-0.070711', 'rewards_train/rejected': '-0.029181', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041529', 'logps_train/rejected': '-110.87', 'logps_train/chosen': '-148.72', 'loss/train': '0.72129', 'examples_per_second': '31.479', 'grad_norm': '107.5', 'counters/examples': 34496, 'counters/updates': 1078}
train stats after 34528 examples: {'rewards_train/chosen': '-0.062406', 'rewards_train/rejected': '-0.041672', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020734', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-125.38', 'loss/train': '0.71756', 'examples_per_second': '32.882', 'grad_norm': '91', 'counters/examples': 34528, 'counters/updates': 1079}
train stats after 34560 examples: {'rewards_train/chosen': '-0.042856', 'rewards_train/rejected': '0.0047128', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047569', 'logps_train/rejected': '-128.85', 'logps_train/chosen': '-153.26', 'loss/train': '0.73421', 'examples_per_second': '30.713', 'grad_norm': '100.5', 'counters/examples': 34560, 'counters/updates': 1080}
train stats after 34592 examples: {'rewards_train/chosen': '0.030218', 'rewards_train/rejected': '-0.042213', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.072431', 'logps_train/rejected': '-170.63', 'logps_train/chosen': '-163.05', 'loss/train': '0.68809', 'examples_per_second': '31.203', 'grad_norm': '169', 'counters/examples': 34592, 'counters/updates': 1081}
skipping logging after 34624 examples to avoid logging too frequently
train stats after 34656 examples: {'rewards_train/chosen': '0.082879', 'rewards_train/rejected': '-0.053266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13614', 'logps_train/rejected': '-108.21', 'logps_train/chosen': '-165.42', 'loss/train': '0.64061', 'examples_per_second': '31.719', 'grad_norm': '63.5', 'counters/examples': 34656, 'counters/updates': 1083}
train stats after 34688 examples: {'rewards_train/chosen': '0.027897', 'rewards_train/rejected': '0.022334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0055626', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-125.89', 'loss/train': '0.70004', 'examples_per_second': '30.848', 'grad_norm': '113', 'counters/examples': 34688, 'counters/updates': 1084}
skipping logging after 34720 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '-0.0049395', 'rewards_train/rejected': '0.062769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.067708', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-129.77', 'loss/train': '0.74241', 'examples_per_second': '31.851', 'grad_norm': '208', 'counters/examples': 34752, 'counters/updates': 1086}
train stats after 34784 examples: {'rewards_train/chosen': '-0.0099768', 'rewards_train/rejected': '0.019543', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02952', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-133.03', 'loss/train': '0.73037', 'examples_per_second': '30.328', 'grad_norm': '112.5', 'counters/examples': 34784, 'counters/updates': 1087}
train stats after 34816 examples: {'rewards_train/chosen': '-0.043219', 'rewards_train/rejected': '-0.06267', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019451', 'logps_train/rejected': '-169.83', 'logps_train/chosen': '-116.63', 'loss/train': '0.70337', 'examples_per_second': '30.905', 'grad_norm': '99.5', 'counters/examples': 34816, 'counters/updates': 1088}
train stats after 34848 examples: {'rewards_train/chosen': '0.038231', 'rewards_train/rejected': '-0.10164', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13987', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-166.51', 'loss/train': '0.64292', 'examples_per_second': '31.57', 'grad_norm': '118.5', 'counters/examples': 34848, 'counters/updates': 1089}
skipping logging after 34880 examples to avoid logging too frequently
train stats after 34912 examples: {'rewards_train/chosen': '0.0361', 'rewards_train/rejected': '-0.038996', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075096', 'logps_train/rejected': '-153.67', 'logps_train/chosen': '-170.79', 'loss/train': '0.68007', 'examples_per_second': '30.704', 'grad_norm': '320', 'counters/examples': 34912, 'counters/updates': 1091}
train stats after 34944 examples: {'rewards_train/chosen': '0.036978', 'rewards_train/rejected': '0.019428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01755', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-156.42', 'loss/train': '0.69177', 'examples_per_second': '30.529', 'grad_norm': '84', 'counters/examples': 34944, 'counters/updates': 1092}
train stats after 34976 examples: {'rewards_train/chosen': '-0.054507', 'rewards_train/rejected': '-0.069338', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014831', 'logps_train/rejected': '-112.35', 'logps_train/chosen': '-163.19', 'loss/train': '0.70093', 'examples_per_second': '32.239', 'grad_norm': '65.5', 'counters/examples': 34976, 'counters/updates': 1093}
train stats after 35008 examples: {'rewards_train/chosen': '0.011598', 'rewards_train/rejected': '0.10048', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.088886', 'logps_train/rejected': '-135.86', 'logps_train/chosen': '-140.1', 'loss/train': '0.75205', 'examples_per_second': '31.898', 'grad_norm': '102', 'counters/examples': 35008, 'counters/updates': 1094}
train stats after 35040 examples: {'rewards_train/chosen': '-0.095224', 'rewards_train/rejected': '-0.015378', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.079846', 'logps_train/rejected': '-154.23', 'logps_train/chosen': '-138.35', 'loss/train': '0.74683', 'examples_per_second': '31.643', 'grad_norm': '91.5', 'counters/examples': 35040, 'counters/updates': 1095}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35104 examples: {'rewards_train/chosen': '-0.044213', 'rewards_train/rejected': '-0.0025494', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041663', 'logps_train/rejected': '-107.44', 'logps_train/chosen': '-152.21', 'loss/train': '0.7271', 'examples_per_second': '31.592', 'grad_norm': '79', 'counters/examples': 35104, 'counters/updates': 1097}
train stats after 35136 examples: {'rewards_train/chosen': '0.0066568', 'rewards_train/rejected': '0.011395', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0047382', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-125.87', 'loss/train': '0.70919', 'examples_per_second': '32.62', 'grad_norm': '57', 'counters/examples': 35136, 'counters/updates': 1098}
train stats after 35168 examples: {'rewards_train/chosen': '-0.040702', 'rewards_train/rejected': '0.056468', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.097169', 'logps_train/rejected': '-170.68', 'logps_train/chosen': '-151.35', 'loss/train': '0.76424', 'examples_per_second': '30.727', 'grad_norm': '174', 'counters/examples': 35168, 'counters/updates': 1099}
train stats after 35200 examples: {'rewards_train/chosen': '-0.092744', 'rewards_train/rejected': '0.0026807', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.095425', 'logps_train/rejected': '-140.28', 'logps_train/chosen': '-173.68', 'loss/train': '0.76339', 'examples_per_second': '31.612', 'grad_norm': '111.5', 'counters/examples': 35200, 'counters/updates': 1100}
train stats after 35232 examples: {'rewards_train/chosen': '-0.0024205', 'rewards_train/rejected': '0.0039106', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0063311', 'logps_train/rejected': '-128.84', 'logps_train/chosen': '-169.03', 'loss/train': '0.70943', 'examples_per_second': '31.452', 'grad_norm': '83', 'counters/examples': 35232, 'counters/updates': 1101}
train stats after 35264 examples: {'rewards_train/chosen': '0.016321', 'rewards_train/rejected': '0.062216', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045895', 'logps_train/rejected': '-113.58', 'logps_train/chosen': '-117.33', 'loss/train': '0.72697', 'examples_per_second': '31.335', 'grad_norm': '79', 'counters/examples': 35264, 'counters/updates': 1102}
train stats after 35296 examples: {'rewards_train/chosen': '0.028762', 'rewards_train/rejected': '-0.012557', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041319', 'logps_train/rejected': '-101.58', 'logps_train/chosen': '-128.75', 'loss/train': '0.69029', 'examples_per_second': '31.805', 'grad_norm': '82', 'counters/examples': 35296, 'counters/updates': 1103}
train stats after 35328 examples: {'rewards_train/chosen': '-0.0081178', 'rewards_train/rejected': '-0.058066', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049948', 'logps_train/rejected': '-132.31', 'logps_train/chosen': '-146.46', 'loss/train': '0.68052', 'examples_per_second': '30.5', 'grad_norm': '114.5', 'counters/examples': 35328, 'counters/updates': 1104}
train stats after 35360 examples: {'rewards_train/chosen': '0.13097', 'rewards_train/rejected': '-0.035166', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16613', 'logps_train/rejected': '-132.66', 'logps_train/chosen': '-166.13', 'loss/train': '0.6344', 'examples_per_second': '31.704', 'grad_norm': '81', 'counters/examples': 35360, 'counters/updates': 1105}
train stats after 35392 examples: {'rewards_train/chosen': '-0.00052095', 'rewards_train/rejected': '0.0049477', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0054686', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-162.97', 'loss/train': '0.70523', 'examples_per_second': '30.206', 'grad_norm': '82', 'counters/examples': 35392, 'counters/updates': 1106}
train stats after 35424 examples: {'rewards_train/chosen': '-0.022686', 'rewards_train/rejected': '0.03369', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.056377', 'logps_train/rejected': '-154.95', 'logps_train/chosen': '-126.54', 'loss/train': '0.73401', 'examples_per_second': '31.655', 'grad_norm': '110.5', 'counters/examples': 35424, 'counters/updates': 1107}
train stats after 35456 examples: {'rewards_train/chosen': '0.028246', 'rewards_train/rejected': '-0.049028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077274', 'logps_train/rejected': '-156.04', 'logps_train/chosen': '-143.66', 'loss/train': '0.67371', 'examples_per_second': '31.262', 'grad_norm': '59.75', 'counters/examples': 35456, 'counters/updates': 1108}
train stats after 35488 examples: {'rewards_train/chosen': '0.00061987', 'rewards_train/rejected': '-0.021052', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021672', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-140.93', 'loss/train': '0.69533', 'examples_per_second': '24.452', 'grad_norm': '120', 'counters/examples': 35488, 'counters/updates': 1109}
train stats after 35520 examples: {'rewards_train/chosen': '-0.01862', 'rewards_train/rejected': '-0.048571', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029952', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-185.65', 'loss/train': '0.69566', 'examples_per_second': '31.611', 'grad_norm': '121', 'counters/examples': 35520, 'counters/updates': 1110}
train stats after 35552 examples: {'rewards_train/chosen': '-0.08367', 'rewards_train/rejected': '0.0034678', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.087137', 'logps_train/rejected': '-153.98', 'logps_train/chosen': '-136.67', 'loss/train': '0.74416', 'examples_per_second': '30.458', 'grad_norm': '129', 'counters/examples': 35552, 'counters/updates': 1111}
train stats after 35584 examples: {'rewards_train/chosen': '0.0068136', 'rewards_train/rejected': '-0.023364', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030178', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-133.24', 'loss/train': '0.68665', 'examples_per_second': '24.48', 'grad_norm': '155', 'counters/examples': 35584, 'counters/updates': 1112}
skipping logging after 35616 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '0.017767', 'rewards_train/rejected': '0.031234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013468', 'logps_train/rejected': '-138.8', 'logps_train/chosen': '-164.94', 'loss/train': '0.72457', 'examples_per_second': '31.676', 'grad_norm': '117', 'counters/examples': 35648, 'counters/updates': 1114}
train stats after 35680 examples: {'rewards_train/chosen': '-0.037745', 'rewards_train/rejected': '0.053984', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.091728', 'logps_train/rejected': '-143.6', 'logps_train/chosen': '-164.82', 'loss/train': '0.7503', 'examples_per_second': '32.161', 'grad_norm': '340', 'counters/examples': 35680, 'counters/updates': 1115}
train stats after 35712 examples: {'rewards_train/chosen': '-0.026579', 'rewards_train/rejected': '-0.069251', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042672', 'logps_train/rejected': '-162.98', 'logps_train/chosen': '-140.6', 'loss/train': '0.69137', 'examples_per_second': '30.796', 'grad_norm': '218', 'counters/examples': 35712, 'counters/updates': 1116}
train stats after 35744 examples: {'rewards_train/chosen': '-0.039508', 'rewards_train/rejected': '-0.014954', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024554', 'logps_train/rejected': '-179.47', 'logps_train/chosen': '-166.86', 'loss/train': '0.71655', 'examples_per_second': '32.519', 'grad_norm': '85', 'counters/examples': 35744, 'counters/updates': 1117}
train stats after 35776 examples: {'rewards_train/chosen': '0.065684', 'rewards_train/rejected': '-0.047146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11283', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-153.01', 'loss/train': '0.65013', 'examples_per_second': '30.608', 'grad_norm': '112.5', 'counters/examples': 35776, 'counters/updates': 1118}
train stats after 35808 examples: {'rewards_train/chosen': '-0.022456', 'rewards_train/rejected': '0.044461', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.066917', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-171.13', 'loss/train': '0.73397', 'examples_per_second': '31.699', 'grad_norm': '102', 'counters/examples': 35808, 'counters/updates': 1119}
train stats after 35840 examples: {'rewards_train/chosen': '-0.022171', 'rewards_train/rejected': '0.037626', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059797', 'logps_train/rejected': '-157.21', 'logps_train/chosen': '-133.92', 'loss/train': '0.75159', 'examples_per_second': '31.693', 'grad_norm': '118.5', 'counters/examples': 35840, 'counters/updates': 1120}
train stats after 35872 examples: {'rewards_train/chosen': '0.026242', 'rewards_train/rejected': '0.084186', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.057944', 'logps_train/rejected': '-157.2', 'logps_train/chosen': '-153.41', 'loss/train': '0.73001', 'examples_per_second': '33.155', 'grad_norm': '202', 'counters/examples': 35872, 'counters/updates': 1121}
train stats after 35904 examples: {'rewards_train/chosen': '-0.005718', 'rewards_train/rejected': '0.01975', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025468', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-156.77', 'loss/train': '0.7208', 'examples_per_second': '32.227', 'grad_norm': '115', 'counters/examples': 35904, 'counters/updates': 1122}
train stats after 35936 examples: {'rewards_train/chosen': '-0.0041617', 'rewards_train/rejected': '0.0049225', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0090842', 'logps_train/rejected': '-110.06', 'logps_train/chosen': '-138.79', 'loss/train': '0.70428', 'examples_per_second': '30.299', 'grad_norm': '63.75', 'counters/examples': 35936, 'counters/updates': 1123}
train stats after 35968 examples: {'rewards_train/chosen': '-0.078684', 'rewards_train/rejected': '-0.040784', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0379', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-125.06', 'loss/train': '0.72491', 'examples_per_second': '30.989', 'grad_norm': '83', 'counters/examples': 35968, 'counters/updates': 1124}
train stats after 36000 examples: {'rewards_train/chosen': '-0.037725', 'rewards_train/rejected': '-0.034221', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0035043', 'logps_train/rejected': '-172.06', 'logps_train/chosen': '-162.28', 'loss/train': '0.70486', 'examples_per_second': '31.737', 'grad_norm': '136', 'counters/examples': 36000, 'counters/updates': 1125}
Running evaluation after 36000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.03it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.94it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.90it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]
eval after 36000: {'rewards_eval/chosen': '0.0031726', 'rewards_eval/rejected': '0.0072962', 'rewards_eval/accuracies': '0.51953', 'rewards_eval/margins': '-0.0041236', 'logps_eval/rejected': '-128.07', 'logps_eval/chosen': '-150.97', 'loss/eval': '0.71351'}
skipping save for non epoch
train stats after 36032 examples: {'rewards_train/chosen': '0.056169', 'rewards_train/rejected': '-0.01652', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072689', 'logps_train/rejected': '-107.68', 'logps_train/chosen': '-175.45', 'loss/train': '0.68297', 'examples_per_second': '33.322', 'grad_norm': '119', 'counters/examples': 36032, 'counters/updates': 1126}
train stats after 36064 examples: {'rewards_train/chosen': '0.050998', 'rewards_train/rejected': '0.10566', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.054666', 'logps_train/rejected': '-176.33', 'logps_train/chosen': '-149.22', 'loss/train': '0.73129', 'examples_per_second': '31.666', 'grad_norm': '116', 'counters/examples': 36064, 'counters/updates': 1127}
train stats after 36096 examples: {'rewards_train/chosen': '0.033619', 'rewards_train/rejected': '-0.023238', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056856', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-130.05', 'loss/train': '0.67299', 'examples_per_second': '31.714', 'grad_norm': '116', 'counters/examples': 36096, 'counters/updates': 1128}
train stats after 36128 examples: {'rewards_train/chosen': '0.02959', 'rewards_train/rejected': '0.027463', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0021272', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-153.26', 'loss/train': '0.71131', 'examples_per_second': '30.72', 'grad_norm': '77.5', 'counters/examples': 36128, 'counters/updates': 1129}
train stats after 36160 examples: {'rewards_train/chosen': '0.001913', 'rewards_train/rejected': '-0.042505', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044418', 'logps_train/rejected': '-117.92', 'logps_train/chosen': '-145.8', 'loss/train': '0.68456', 'examples_per_second': '30.816', 'grad_norm': '73', 'counters/examples': 36160, 'counters/updates': 1130}
train stats after 36192 examples: {'rewards_train/chosen': '-0.031032', 'rewards_train/rejected': '-0.038992', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0079605', 'logps_train/rejected': '-153.13', 'logps_train/chosen': '-158.32', 'loss/train': '0.71138', 'examples_per_second': '32.256', 'grad_norm': '111', 'counters/examples': 36192, 'counters/updates': 1131}
train stats after 36224 examples: {'rewards_train/chosen': '-0.026363', 'rewards_train/rejected': '0.024208', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.050571', 'logps_train/rejected': '-155.2', 'logps_train/chosen': '-157.5', 'loss/train': '0.73234', 'examples_per_second': '31.283', 'grad_norm': '129', 'counters/examples': 36224, 'counters/updates': 1132}
train stats after 36256 examples: {'rewards_train/chosen': '-0.046378', 'rewards_train/rejected': '-0.0035503', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042828', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-177.57', 'loss/train': '0.7283', 'examples_per_second': '31.366', 'grad_norm': '84.5', 'counters/examples': 36256, 'counters/updates': 1133}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36320 examples: {'rewards_train/chosen': '0.16615', 'rewards_train/rejected': '0.099108', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067039', 'logps_train/rejected': '-164.92', 'logps_train/chosen': '-181.69', 'loss/train': '0.67396', 'examples_per_second': '32.593', 'grad_norm': '322', 'counters/examples': 36320, 'counters/updates': 1135}
train stats after 36352 examples: {'rewards_train/chosen': '0.011404', 'rewards_train/rejected': '0.019729', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.008325', 'logps_train/rejected': '-101.68', 'logps_train/chosen': '-139.71', 'loss/train': '0.70105', 'examples_per_second': '25.041', 'grad_norm': '64.5', 'counters/examples': 36352, 'counters/updates': 1136}
skipping logging after 36384 examples to avoid logging too frequently
train stats after 36416 examples: {'rewards_train/chosen': '-0.0038524', 'rewards_train/rejected': '-0.051816', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047964', 'logps_train/rejected': '-112.89', 'logps_train/chosen': '-153.88', 'loss/train': '0.69281', 'examples_per_second': '30.16', 'grad_norm': '103.5', 'counters/examples': 36416, 'counters/updates': 1138}
train stats after 36448 examples: {'rewards_train/chosen': '-0.03568', 'rewards_train/rejected': '-0.088946', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053266', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-163.23', 'loss/train': '0.68231', 'examples_per_second': '32.558', 'grad_norm': '94.5', 'counters/examples': 36448, 'counters/updates': 1139}
train stats after 36480 examples: {'rewards_train/chosen': '0.0049585', 'rewards_train/rejected': '0.034104', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.029145', 'logps_train/rejected': '-100.86', 'logps_train/chosen': '-136.95', 'loss/train': '0.71546', 'examples_per_second': '32.197', 'grad_norm': '55.75', 'counters/examples': 36480, 'counters/updates': 1140}
train stats after 36512 examples: {'rewards_train/chosen': '-0.022895', 'rewards_train/rejected': '-0.034492', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011597', 'logps_train/rejected': '-124.16', 'logps_train/chosen': '-129.56', 'loss/train': '0.69885', 'examples_per_second': '32.831', 'grad_norm': '62.75', 'counters/examples': 36512, 'counters/updates': 1141}
train stats after 36544 examples: {'rewards_train/chosen': '-0.0076888', 'rewards_train/rejected': '-0.0038235', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0038653', 'logps_train/rejected': '-155.63', 'logps_train/chosen': '-185.24', 'loss/train': '0.70909', 'examples_per_second': '30.662', 'grad_norm': '79.5', 'counters/examples': 36544, 'counters/updates': 1142}
skipping logging after 36576 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '0.046975', 'rewards_train/rejected': '-0.044629', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091604', 'logps_train/rejected': '-135.44', 'logps_train/chosen': '-128.29', 'loss/train': '0.65812', 'examples_per_second': '30.662', 'grad_norm': '120', 'counters/examples': 36608, 'counters/updates': 1144}
train stats after 36640 examples: {'rewards_train/chosen': '0.02062', 'rewards_train/rejected': '0.0017385', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018882', 'logps_train/rejected': '-151.18', 'logps_train/chosen': '-123.65', 'loss/train': '0.69349', 'examples_per_second': '31.663', 'grad_norm': '102.5', 'counters/examples': 36640, 'counters/updates': 1145}
train stats after 36672 examples: {'rewards_train/chosen': '0.041082', 'rewards_train/rejected': '0.032619', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0084626', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-174.61', 'loss/train': '0.7029', 'examples_per_second': '31.165', 'grad_norm': '103.5', 'counters/examples': 36672, 'counters/updates': 1146}
train stats after 36704 examples: {'rewards_train/chosen': '0.020512', 'rewards_train/rejected': '0.040767', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020255', 'logps_train/rejected': '-79.405', 'logps_train/chosen': '-166.47', 'loss/train': '0.70899', 'examples_per_second': '30.665', 'grad_norm': '64.5', 'counters/examples': 36704, 'counters/updates': 1147}
train stats after 36736 examples: {'rewards_train/chosen': '0.0064644', 'rewards_train/rejected': '0.021985', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.01552', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-151.4', 'loss/train': '0.71411', 'examples_per_second': '31.671', 'grad_norm': '107', 'counters/examples': 36736, 'counters/updates': 1148}
skipping logging after 36768 examples to avoid logging too frequently
train stats after 36800 examples: {'rewards_train/chosen': '-0.073855', 'rewards_train/rejected': '-0.020636', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.053219', 'logps_train/rejected': '-97.843', 'logps_train/chosen': '-165.48', 'loss/train': '0.73787', 'examples_per_second': '31.034', 'grad_norm': '63', 'counters/examples': 36800, 'counters/updates': 1150}
skipping logging after 36832 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '0.072497', 'rewards_train/rejected': '-0.01159', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084087', 'logps_train/rejected': '-152.51', 'logps_train/chosen': '-178.88', 'loss/train': '0.65832', 'examples_per_second': '30.647', 'grad_norm': '75.5', 'counters/examples': 36864, 'counters/updates': 1152}
train stats after 36896 examples: {'rewards_train/chosen': '0.017482', 'rewards_train/rejected': '-0.048765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066246', 'logps_train/rejected': '-150.14', 'logps_train/chosen': '-141.08', 'loss/train': '0.67734', 'examples_per_second': '31.022', 'grad_norm': '63.5', 'counters/examples': 36896, 'counters/updates': 1153}
train stats after 36928 examples: {'rewards_train/chosen': '0.03385', 'rewards_train/rejected': '-0.025135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058985', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-151.44', 'loss/train': '0.66894', 'examples_per_second': '31.048', 'grad_norm': '59', 'counters/examples': 36928, 'counters/updates': 1154}
train stats after 36960 examples: {'rewards_train/chosen': '-0.0091035', 'rewards_train/rejected': '-0.017827', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0087239', 'logps_train/rejected': '-157.71', 'logps_train/chosen': '-121.26', 'loss/train': '0.7001', 'examples_per_second': '32.037', 'grad_norm': '69', 'counters/examples': 36960, 'counters/updates': 1155}
train stats after 36992 examples: {'rewards_train/chosen': '-0.047437', 'rewards_train/rejected': '-0.020198', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027239', 'logps_train/rejected': '-149.51', 'logps_train/chosen': '-167.4', 'loss/train': '0.71672', 'examples_per_second': '31.88', 'grad_norm': '55.75', 'counters/examples': 36992, 'counters/updates': 1156}
train stats after 37024 examples: {'rewards_train/chosen': '-0.0058249', 'rewards_train/rejected': '-0.099237', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093412', 'logps_train/rejected': '-165.39', 'logps_train/chosen': '-188.59', 'loss/train': '0.6791', 'examples_per_second': '31.835', 'grad_norm': '117', 'counters/examples': 37024, 'counters/updates': 1157}
train stats after 37056 examples: {'rewards_train/chosen': '0.010082', 'rewards_train/rejected': '0.0086964', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0013855', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-151.84', 'loss/train': '0.70391', 'examples_per_second': '31.475', 'grad_norm': '72', 'counters/examples': 37056, 'counters/updates': 1158}
skipping logging after 37088 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '0.019435', 'rewards_train/rejected': '-0.018599', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038034', 'logps_train/rejected': '-132.8', 'logps_train/chosen': '-171.13', 'loss/train': '0.68986', 'examples_per_second': '36.657', 'grad_norm': '102.5', 'counters/examples': 37120, 'counters/updates': 1160}
skipping logging after 37152 examples to avoid logging too frequently
train stats after 37184 examples: {'rewards_train/chosen': '0.13752', 'rewards_train/rejected': '-0.039614', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17714', 'logps_train/rejected': '-107.95', 'logps_train/chosen': '-183.36', 'loss/train': '0.62364', 'examples_per_second': '31.292', 'grad_norm': '83', 'counters/examples': 37184, 'counters/updates': 1162}
train stats after 37216 examples: {'rewards_train/chosen': '-0.013826', 'rewards_train/rejected': '-0.0061078', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.007718', 'logps_train/rejected': '-91.463', 'logps_train/chosen': '-120.43', 'loss/train': '0.70687', 'examples_per_second': '31.677', 'grad_norm': '95', 'counters/examples': 37216, 'counters/updates': 1163}
skipping logging after 37248 examples to avoid logging too frequently
train stats after 37280 examples: {'rewards_train/chosen': '0.033057', 'rewards_train/rejected': '-0.03579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068848', 'logps_train/rejected': '-100.32', 'logps_train/chosen': '-176.6', 'loss/train': '0.67254', 'examples_per_second': '30.863', 'grad_norm': '107.5', 'counters/examples': 37280, 'counters/updates': 1165}
train stats after 37312 examples: {'rewards_train/chosen': '0.09855', 'rewards_train/rejected': '0.071982', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026568', 'logps_train/rejected': '-181.93', 'logps_train/chosen': '-181.4', 'loss/train': '0.69842', 'examples_per_second': '31.602', 'grad_norm': '188', 'counters/examples': 37312, 'counters/updates': 1166}
skipping logging after 37344 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '-0.0053597', 'rewards_train/rejected': '-0.025856', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.020496', 'logps_train/rejected': '-113.29', 'logps_train/chosen': '-166.3', 'loss/train': '0.69534', 'examples_per_second': '31.139', 'grad_norm': '118', 'counters/examples': 37376, 'counters/updates': 1168}
train stats after 37408 examples: {'rewards_train/chosen': '-0.0039968', 'rewards_train/rejected': '0.1028', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.1068', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-178.19', 'loss/train': '0.76613', 'examples_per_second': '30.082', 'grad_norm': '1016', 'counters/examples': 37408, 'counters/updates': 1169}
train stats after 37440 examples: {'rewards_train/chosen': '-0.0096195', 'rewards_train/rejected': '-0.06114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05152', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-182.82', 'loss/train': '0.67776', 'examples_per_second': '31.317', 'grad_norm': '99', 'counters/examples': 37440, 'counters/updates': 1170}
train stats after 37472 examples: {'rewards_train/chosen': '-0.062279', 'rewards_train/rejected': '0.069884', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.13216', 'logps_train/rejected': '-147.74', 'logps_train/chosen': '-149.8', 'loss/train': '0.77768', 'examples_per_second': '30.551', 'grad_norm': '108', 'counters/examples': 37472, 'counters/updates': 1171}
skipping logging after 37504 examples to avoid logging too frequently
train stats after 37536 examples: {'rewards_train/chosen': '-0.046852', 'rewards_train/rejected': '-0.019037', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027815', 'logps_train/rejected': '-163.72', 'logps_train/chosen': '-155.89', 'loss/train': '0.73414', 'examples_per_second': '32.256', 'grad_norm': '77', 'counters/examples': 37536, 'counters/updates': 1173}
train stats after 37568 examples: {'rewards_train/chosen': '-0.041094', 'rewards_train/rejected': '-0.097503', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05641', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-117.45', 'loss/train': '0.67901', 'examples_per_second': '30.606', 'grad_norm': '80', 'counters/examples': 37568, 'counters/updates': 1174}
train stats after 37600 examples: {'rewards_train/chosen': '-0.023166', 'rewards_train/rejected': '-0.073078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049912', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-183.98', 'loss/train': '0.67871', 'examples_per_second': '30.337', 'grad_norm': '75', 'counters/examples': 37600, 'counters/updates': 1175}
train stats after 37632 examples: {'rewards_train/chosen': '0.012217', 'rewards_train/rejected': '0.01777', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.005553', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-123.65', 'loss/train': '0.71224', 'examples_per_second': '30.319', 'grad_norm': '214', 'counters/examples': 37632, 'counters/updates': 1176}
skipping logging after 37664 examples to avoid logging too frequently
train stats after 37696 examples: {'rewards_train/chosen': '0.00097981', 'rewards_train/rejected': '-0.0041666', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0051464', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-180.85', 'loss/train': '0.69992', 'examples_per_second': '34.362', 'grad_norm': '106', 'counters/examples': 37696, 'counters/updates': 1178}
skipping logging after 37728 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '0.016862', 'rewards_train/rejected': '-0.095424', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11229', 'logps_train/rejected': '-123.16', 'logps_train/chosen': '-203.82', 'loss/train': '0.64671', 'examples_per_second': '31.444', 'grad_norm': '181', 'counters/examples': 37760, 'counters/updates': 1180}
train stats after 37792 examples: {'rewards_train/chosen': '-0.049924', 'rewards_train/rejected': '-0.039251', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010673', 'logps_train/rejected': '-164.49', 'logps_train/chosen': '-128.32', 'loss/train': '0.71419', 'examples_per_second': '31.612', 'grad_norm': '92.5', 'counters/examples': 37792, 'counters/updates': 1181}
train stats after 37824 examples: {'rewards_train/chosen': '0.10848', 'rewards_train/rejected': '-0.0037373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11222', 'logps_train/rejected': '-173.49', 'logps_train/chosen': '-174.01', 'loss/train': '0.6565', 'examples_per_second': '31.701', 'grad_norm': '76', 'counters/examples': 37824, 'counters/updates': 1182}
train stats after 37856 examples: {'rewards_train/chosen': '0.05331', 'rewards_train/rejected': '0.050063', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0032471', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-194.93', 'loss/train': '0.707', 'examples_per_second': '31.647', 'grad_norm': '112', 'counters/examples': 37856, 'counters/updates': 1183}
skipping logging after 37888 examples to avoid logging too frequently
train stats after 37920 examples: {'rewards_train/chosen': '0.017046', 'rewards_train/rejected': '0.01426', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0027861', 'logps_train/rejected': '-161.78', 'logps_train/chosen': '-153.25', 'loss/train': '0.70949', 'examples_per_second': '32.295', 'grad_norm': '72.5', 'counters/examples': 37920, 'counters/updates': 1185}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 37984 examples: {'rewards_train/chosen': '0.027279', 'rewards_train/rejected': '-0.0067044', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.033983', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-155.94', 'loss/train': '0.68624', 'examples_per_second': '31.616', 'grad_norm': '122', 'counters/examples': 37984, 'counters/updates': 1187}
train stats after 38016 examples: {'rewards_train/chosen': '0.1476', 'rewards_train/rejected': '-0.0074901', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15509', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-182.38', 'loss/train': '0.63664', 'examples_per_second': '31.644', 'grad_norm': '87', 'counters/examples': 38016, 'counters/updates': 1188}
train stats after 38048 examples: {'rewards_train/chosen': '-0.076415', 'rewards_train/rejected': '0.078103', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.15452', 'logps_train/rejected': '-164.33', 'logps_train/chosen': '-139.44', 'loss/train': '0.78975', 'examples_per_second': '31.645', 'grad_norm': '99.5', 'counters/examples': 38048, 'counters/updates': 1189}
train stats after 38080 examples: {'rewards_train/chosen': '0.059073', 'rewards_train/rejected': '0.016182', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042891', 'logps_train/rejected': '-96.031', 'logps_train/chosen': '-168.92', 'loss/train': '0.68641', 'examples_per_second': '30.199', 'grad_norm': '71.5', 'counters/examples': 38080, 'counters/updates': 1190}
train stats after 38112 examples: {'rewards_train/chosen': '-0.001343', 'rewards_train/rejected': '-0.003448', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.002105', 'logps_train/rejected': '-143.21', 'logps_train/chosen': '-156.96', 'loss/train': '0.70422', 'examples_per_second': '32.163', 'grad_norm': '75.5', 'counters/examples': 38112, 'counters/updates': 1191}
train stats after 38144 examples: {'rewards_train/chosen': '-0.067228', 'rewards_train/rejected': '0.011848', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.079076', 'logps_train/rejected': '-133.11', 'logps_train/chosen': '-132.68', 'loss/train': '0.74534', 'examples_per_second': '31.655', 'grad_norm': '74', 'counters/examples': 38144, 'counters/updates': 1192}
train stats after 38176 examples: {'rewards_train/chosen': '0.07807', 'rewards_train/rejected': '0.078875', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0008049', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-131.33', 'loss/train': '0.69953', 'examples_per_second': '32.189', 'grad_norm': '52.75', 'counters/examples': 38176, 'counters/updates': 1193}
train stats after 38208 examples: {'rewards_train/chosen': '0.06565', 'rewards_train/rejected': '0.027261', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038389', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-188.33', 'loss/train': '0.68186', 'examples_per_second': '31.831', 'grad_norm': '87', 'counters/examples': 38208, 'counters/updates': 1194}
train stats after 38240 examples: {'rewards_train/chosen': '-0.046668', 'rewards_train/rejected': '-0.064459', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.017791', 'logps_train/rejected': '-114.2', 'logps_train/chosen': '-143.57', 'loss/train': '0.69874', 'examples_per_second': '30.167', 'grad_norm': '54.75', 'counters/examples': 38240, 'counters/updates': 1195}
train stats after 38272 examples: {'rewards_train/chosen': '0.027434', 'rewards_train/rejected': '0.024613', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0028213', 'logps_train/rejected': '-141.03', 'logps_train/chosen': '-163.78', 'loss/train': '0.70649', 'examples_per_second': '30.673', 'grad_norm': '58.75', 'counters/examples': 38272, 'counters/updates': 1196}
train stats after 38304 examples: {'rewards_train/chosen': '0.077092', 'rewards_train/rejected': '0.026046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051046', 'logps_train/rejected': '-173.32', 'logps_train/chosen': '-164.75', 'loss/train': '0.68607', 'examples_per_second': '31.585', 'grad_norm': '63.75', 'counters/examples': 38304, 'counters/updates': 1197}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38368 examples: {'rewards_train/chosen': '-0.0030819', 'rewards_train/rejected': '0.046945', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.050026', 'logps_train/rejected': '-146.85', 'logps_train/chosen': '-176.45', 'loss/train': '0.744', 'examples_per_second': '31.521', 'grad_norm': '163', 'counters/examples': 38368, 'counters/updates': 1199}
train stats after 38400 examples: {'rewards_train/chosen': '-0.010774', 'rewards_train/rejected': '0.040003', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.050777', 'logps_train/rejected': '-140.59', 'logps_train/chosen': '-154.82', 'loss/train': '0.73591', 'examples_per_second': '32.086', 'grad_norm': '92.5', 'counters/examples': 38400, 'counters/updates': 1200}
train stats after 38432 examples: {'rewards_train/chosen': '-0.046889', 'rewards_train/rejected': '-0.045265', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0016238', 'logps_train/rejected': '-153.22', 'logps_train/chosen': '-166.87', 'loss/train': '0.7044', 'examples_per_second': '31.629', 'grad_norm': '149', 'counters/examples': 38432, 'counters/updates': 1201}
train stats after 38464 examples: {'rewards_train/chosen': '0.017553', 'rewards_train/rejected': '-0.005185', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022738', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-184.81', 'loss/train': '0.70771', 'examples_per_second': '31.567', 'grad_norm': '99.5', 'counters/examples': 38464, 'counters/updates': 1202}
train stats after 38496 examples: {'rewards_train/chosen': '-0.017234', 'rewards_train/rejected': '-0.10657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089335', 'logps_train/rejected': '-176.48', 'logps_train/chosen': '-176.13', 'loss/train': '0.65765', 'examples_per_second': '30.587', 'grad_norm': '121.5', 'counters/examples': 38496, 'counters/updates': 1203}
train stats after 38528 examples: {'rewards_train/chosen': '0.012687', 'rewards_train/rejected': '0.022262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0095753', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-146.54', 'loss/train': '0.70806', 'examples_per_second': '31.062', 'grad_norm': '74', 'counters/examples': 38528, 'counters/updates': 1204}
train stats after 38560 examples: {'rewards_train/chosen': '0.0067615', 'rewards_train/rejected': '0.013773', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0070119', 'logps_train/rejected': '-126.41', 'logps_train/chosen': '-122.69', 'loss/train': '0.70929', 'examples_per_second': '31.152', 'grad_norm': '92.5', 'counters/examples': 38560, 'counters/updates': 1205}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38624 examples: {'rewards_train/chosen': '0.01944', 'rewards_train/rejected': '-0.0071205', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02656', 'logps_train/rejected': '-143.37', 'logps_train/chosen': '-145.38', 'loss/train': '0.69587', 'examples_per_second': '30.272', 'grad_norm': '99', 'counters/examples': 38624, 'counters/updates': 1207}
train stats after 38656 examples: {'rewards_train/chosen': '0.0083692', 'rewards_train/rejected': '0.057345', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.048976', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-168.28', 'loss/train': '0.72381', 'examples_per_second': '31.904', 'grad_norm': '64.5', 'counters/examples': 38656, 'counters/updates': 1208}
train stats after 38688 examples: {'rewards_train/chosen': '-0.022992', 'rewards_train/rejected': '-0.01584', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0071516', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-104.43', 'loss/train': '0.70398', 'examples_per_second': '33.02', 'grad_norm': '149', 'counters/examples': 38688, 'counters/updates': 1209}
train stats after 38720 examples: {'rewards_train/chosen': '0.014722', 'rewards_train/rejected': '0.0035949', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011127', 'logps_train/rejected': '-154.74', 'logps_train/chosen': '-171.5', 'loss/train': '0.69753', 'examples_per_second': '30.854', 'grad_norm': '78.5', 'counters/examples': 38720, 'counters/updates': 1210}
train stats after 38752 examples: {'rewards_train/chosen': '0.01826', 'rewards_train/rejected': '-0.052875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071134', 'logps_train/rejected': '-136.43', 'logps_train/chosen': '-164.53', 'loss/train': '0.68053', 'examples_per_second': '30.605', 'grad_norm': '94', 'counters/examples': 38752, 'counters/updates': 1211}
train stats after 38784 examples: {'rewards_train/chosen': '-0.016669', 'rewards_train/rejected': '0.035601', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05227', 'logps_train/rejected': '-162.53', 'logps_train/chosen': '-161.74', 'loss/train': '0.72953', 'examples_per_second': '30.123', 'grad_norm': '121', 'counters/examples': 38784, 'counters/updates': 1212}
train stats after 38816 examples: {'rewards_train/chosen': '-0.090207', 'rewards_train/rejected': '-0.0056181', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.084589', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-182.61', 'loss/train': '0.75606', 'examples_per_second': '31.662', 'grad_norm': '110', 'counters/examples': 38816, 'counters/updates': 1213}
train stats after 38848 examples: {'rewards_train/chosen': '0.012579', 'rewards_train/rejected': '-0.15741', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.16998', 'logps_train/rejected': '-184.63', 'logps_train/chosen': '-196.42', 'loss/train': '0.70323', 'examples_per_second': '30.649', 'grad_norm': '99.5', 'counters/examples': 38848, 'counters/updates': 1214}
train stats after 38880 examples: {'rewards_train/chosen': '0.041088', 'rewards_train/rejected': '0.047892', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.006804', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-169.86', 'loss/train': '0.71953', 'examples_per_second': '31.639', 'grad_norm': '88.5', 'counters/examples': 38880, 'counters/updates': 1215}
train stats after 38912 examples: {'rewards_train/chosen': '-0.0062034', 'rewards_train/rejected': '-0.028143', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021939', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-132.42', 'loss/train': '0.69219', 'examples_per_second': '31.678', 'grad_norm': '74', 'counters/examples': 38912, 'counters/updates': 1216}
train stats after 38944 examples: {'rewards_train/chosen': '-0.0073812', 'rewards_train/rejected': '-0.074075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066694', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-165.43', 'loss/train': '0.6738', 'examples_per_second': '30.977', 'grad_norm': '98.5', 'counters/examples': 38944, 'counters/updates': 1217}
train stats after 38976 examples: {'rewards_train/chosen': '0.029604', 'rewards_train/rejected': '-0.019513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049117', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-194.1', 'loss/train': '0.67944', 'examples_per_second': '31.672', 'grad_norm': '84.5', 'counters/examples': 38976, 'counters/updates': 1218}
train stats after 39008 examples: {'rewards_train/chosen': '0.023789', 'rewards_train/rejected': '-0.0034899', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027278', 'logps_train/rejected': '-155.62', 'logps_train/chosen': '-146.32', 'loss/train': '0.68642', 'examples_per_second': '32.715', 'grad_norm': '92.5', 'counters/examples': 39008, 'counters/updates': 1219}
train stats after 39040 examples: {'rewards_train/chosen': '-0.021963', 'rewards_train/rejected': '-0.034219', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012256', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-135.88', 'loss/train': '0.70864', 'examples_per_second': '31.669', 'grad_norm': '59', 'counters/examples': 39040, 'counters/updates': 1220}
skipping logging after 39072 examples to avoid logging too frequently
train stats after 39104 examples: {'rewards_train/chosen': '0.039342', 'rewards_train/rejected': '0.031868', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0074746', 'logps_train/rejected': '-102.17', 'logps_train/chosen': '-211.52', 'loss/train': '0.69952', 'examples_per_second': '32.457', 'grad_norm': '159', 'counters/examples': 39104, 'counters/updates': 1222}
train stats after 39136 examples: {'rewards_train/chosen': '0.00061523', 'rewards_train/rejected': '-0.0084325', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0090477', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-165.35', 'loss/train': '0.69486', 'examples_per_second': '31.186', 'grad_norm': '75', 'counters/examples': 39136, 'counters/updates': 1223}
skipping logging after 39168 examples to avoid logging too frequently
train stats after 39200 examples: {'rewards_train/chosen': '0.017388', 'rewards_train/rejected': '0.0077536', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096346', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-144.15', 'loss/train': '0.69598', 'examples_per_second': '36.456', 'grad_norm': '81.5', 'counters/examples': 39200, 'counters/updates': 1225}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39264 examples: {'rewards_train/chosen': '0.024336', 'rewards_train/rejected': '-0.028371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052707', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-149.88', 'loss/train': '0.67412', 'examples_per_second': '31.797', 'grad_norm': '95', 'counters/examples': 39264, 'counters/updates': 1227}
skipping logging after 39296 examples to avoid logging too frequently
train stats after 39328 examples: {'rewards_train/chosen': '-0.024248', 'rewards_train/rejected': '0.022856', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047104', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-157.16', 'loss/train': '0.72884', 'examples_per_second': '32.25', 'grad_norm': '77', 'counters/examples': 39328, 'counters/updates': 1229}
train stats after 39360 examples: {'rewards_train/chosen': '0.016207', 'rewards_train/rejected': '-0.034567', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050775', 'logps_train/rejected': '-152.09', 'logps_train/chosen': '-134.3', 'loss/train': '0.68506', 'examples_per_second': '32.864', 'grad_norm': '100', 'counters/examples': 39360, 'counters/updates': 1230}
skipping logging after 39392 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '0.083645', 'rewards_train/rejected': '0.049617', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034028', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-167.84', 'loss/train': '0.70128', 'examples_per_second': '31.294', 'grad_norm': '91.5', 'counters/examples': 39424, 'counters/updates': 1232}
train stats after 39456 examples: {'rewards_train/chosen': '-0.016956', 'rewards_train/rejected': '-0.053097', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036141', 'logps_train/rejected': '-95.118', 'logps_train/chosen': '-148.19', 'loss/train': '0.68352', 'examples_per_second': '31.632', 'grad_norm': '85.5', 'counters/examples': 39456, 'counters/updates': 1233}
train stats after 39488 examples: {'rewards_train/chosen': '0.088314', 'rewards_train/rejected': '0.031305', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057009', 'logps_train/rejected': '-148.02', 'logps_train/chosen': '-127.46', 'loss/train': '0.68457', 'examples_per_second': '30.079', 'grad_norm': '65', 'counters/examples': 39488, 'counters/updates': 1234}
train stats after 39520 examples: {'rewards_train/chosen': '0.016155', 'rewards_train/rejected': '0.095132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.078976', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-155.41', 'loss/train': '0.75658', 'examples_per_second': '30.118', 'grad_norm': '91.5', 'counters/examples': 39520, 'counters/updates': 1235}
train stats after 39552 examples: {'rewards_train/chosen': '-0.077528', 'rewards_train/rejected': '-0.0066334', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.070895', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-119.21', 'loss/train': '0.73848', 'examples_per_second': '30.961', 'grad_norm': '95', 'counters/examples': 39552, 'counters/updates': 1236}
train stats after 39584 examples: {'rewards_train/chosen': '-0.039317', 'rewards_train/rejected': '-0.029762', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0095547', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-144.52', 'loss/train': '0.70944', 'examples_per_second': '31.473', 'grad_norm': '103', 'counters/examples': 39584, 'counters/updates': 1237}
train stats after 39616 examples: {'rewards_train/chosen': '0.061048', 'rewards_train/rejected': '-0.012229', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073277', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-147.12', 'loss/train': '0.66451', 'examples_per_second': '33.093', 'grad_norm': '62.25', 'counters/examples': 39616, 'counters/updates': 1238}
train stats after 39648 examples: {'rewards_train/chosen': '0.10807', 'rewards_train/rejected': '0.038057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070012', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-146.46', 'loss/train': '0.66475', 'examples_per_second': '31.557', 'grad_norm': '82.5', 'counters/examples': 39648, 'counters/updates': 1239}
train stats after 39680 examples: {'rewards_train/chosen': '-0.023078', 'rewards_train/rejected': '0.018481', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041559', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-165.81', 'loss/train': '0.72571', 'examples_per_second': '32.588', 'grad_norm': '103.5', 'counters/examples': 39680, 'counters/updates': 1240}
train stats after 39712 examples: {'rewards_train/chosen': '0.047039', 'rewards_train/rejected': '-0.0056358', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.052675', 'logps_train/rejected': '-147.97', 'logps_train/chosen': '-166.21', 'loss/train': '0.68058', 'examples_per_second': '32.489', 'grad_norm': '150', 'counters/examples': 39712, 'counters/updates': 1241}
train stats after 39744 examples: {'rewards_train/chosen': '-0.0024609', 'rewards_train/rejected': '0.014478', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016939', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-178.59', 'loss/train': '0.71454', 'examples_per_second': '31.875', 'grad_norm': '178', 'counters/examples': 39744, 'counters/updates': 1242}
train stats after 39776 examples: {'rewards_train/chosen': '0.02662', 'rewards_train/rejected': '-0.017631', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044251', 'logps_train/rejected': '-162.55', 'logps_train/chosen': '-167.42', 'loss/train': '0.68238', 'examples_per_second': '32.093', 'grad_norm': '100.5', 'counters/examples': 39776, 'counters/updates': 1243}
train stats after 39808 examples: {'rewards_train/chosen': '-0.021184', 'rewards_train/rejected': '-0.013003', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.008182', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-134.61', 'loss/train': '0.70407', 'examples_per_second': '32.563', 'grad_norm': '77.5', 'counters/examples': 39808, 'counters/updates': 1244}
train stats after 39840 examples: {'rewards_train/chosen': '0.04522', 'rewards_train/rejected': '0.034753', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010467', 'logps_train/rejected': '-131.64', 'logps_train/chosen': '-125.6', 'loss/train': '0.70063', 'examples_per_second': '31.927', 'grad_norm': '69.5', 'counters/examples': 39840, 'counters/updates': 1245}
train stats after 39872 examples: {'rewards_train/chosen': '0.021882', 'rewards_train/rejected': '-0.05006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071943', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-121.42', 'loss/train': '0.66902', 'examples_per_second': '30.264', 'grad_norm': '38.25', 'counters/examples': 39872, 'counters/updates': 1246}
train stats after 39904 examples: {'rewards_train/chosen': '0.046137', 'rewards_train/rejected': '-0.07481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12095', 'logps_train/rejected': '-168.44', 'logps_train/chosen': '-190.08', 'loss/train': '0.65541', 'examples_per_second': '31.924', 'grad_norm': '148', 'counters/examples': 39904, 'counters/updates': 1247}
train stats after 39936 examples: {'rewards_train/chosen': '0.021438', 'rewards_train/rejected': '-0.04449', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065929', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-152.16', 'loss/train': '0.66952', 'examples_per_second': '30.357', 'grad_norm': '81.5', 'counters/examples': 39936, 'counters/updates': 1248}
train stats after 39968 examples: {'rewards_train/chosen': '0.036022', 'rewards_train/rejected': '0.11395', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.077925', 'logps_train/rejected': '-126.32', 'logps_train/chosen': '-120.33', 'loss/train': '0.75291', 'examples_per_second': '30.499', 'grad_norm': '97.5', 'counters/examples': 39968, 'counters/updates': 1249}
train stats after 40000 examples: {'rewards_train/chosen': '0.082102', 'rewards_train/rejected': '0.083438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0013354', 'logps_train/rejected': '-153.75', 'logps_train/chosen': '-133.4', 'loss/train': '0.70572', 'examples_per_second': '30.235', 'grad_norm': '110', 'counters/examples': 40000, 'counters/updates': 1250}
train stats after 40032 examples: {'rewards_train/chosen': '-0.0025189', 'rewards_train/rejected': '-0.12461', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12209', 'logps_train/rejected': '-120.81', 'logps_train/chosen': '-157.47', 'loss/train': '0.65201', 'examples_per_second': '32.627', 'grad_norm': '63.75', 'counters/examples': 40032, 'counters/updates': 1251}
train stats after 40064 examples: {'rewards_train/chosen': '0.11317', 'rewards_train/rejected': '0.063198', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049968', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-154.54', 'loss/train': '0.68002', 'examples_per_second': '30.077', 'grad_norm': '68.5', 'counters/examples': 40064, 'counters/updates': 1252}
skipping logging after 40096 examples to avoid logging too frequently
train stats after 40128 examples: {'rewards_train/chosen': '-0.015878', 'rewards_train/rejected': '0.013213', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029091', 'logps_train/rejected': '-147.08', 'logps_train/chosen': '-170.3', 'loss/train': '0.72664', 'examples_per_second': '31.479', 'grad_norm': '75', 'counters/examples': 40128, 'counters/updates': 1254}
skipping logging after 40160 examples to avoid logging too frequently
train stats after 40192 examples: {'rewards_train/chosen': '0.058821', 'rewards_train/rejected': '0.075298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016477', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-168.12', 'loss/train': '0.71955', 'examples_per_second': '32.502', 'grad_norm': '117', 'counters/examples': 40192, 'counters/updates': 1256}
train stats after 40224 examples: {'rewards_train/chosen': '-0.026983', 'rewards_train/rejected': '-0.02772', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00073661', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-169.8', 'loss/train': '0.70653', 'examples_per_second': '31.607', 'grad_norm': '103.5', 'counters/examples': 40224, 'counters/updates': 1257}
train stats after 40256 examples: {'rewards_train/chosen': '0.0364', 'rewards_train/rejected': '0.091998', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.055598', 'logps_train/rejected': '-150.32', 'logps_train/chosen': '-174.03', 'loss/train': '0.73822', 'examples_per_second': '30.674', 'grad_norm': '175', 'counters/examples': 40256, 'counters/updates': 1258}
train stats after 40288 examples: {'rewards_train/chosen': '0.0096693', 'rewards_train/rejected': '-0.037399', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.047068', 'logps_train/rejected': '-153.64', 'logps_train/chosen': '-153.9', 'loss/train': '0.68289', 'examples_per_second': '31.549', 'grad_norm': '110', 'counters/examples': 40288, 'counters/updates': 1259}
train stats after 40320 examples: {'rewards_train/chosen': '0.053446', 'rewards_train/rejected': '0.003475', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049971', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-138.58', 'loss/train': '0.68038', 'examples_per_second': '32.13', 'grad_norm': '100', 'counters/examples': 40320, 'counters/updates': 1260}
skipping logging after 40352 examples to avoid logging too frequently
train stats after 40384 examples: {'rewards_train/chosen': '-0.0077044', 'rewards_train/rejected': '0.017055', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024759', 'logps_train/rejected': '-114.42', 'logps_train/chosen': '-138.98', 'loss/train': '0.71463', 'examples_per_second': '32.849', 'grad_norm': '166', 'counters/examples': 40384, 'counters/updates': 1262}
train stats after 40416 examples: {'rewards_train/chosen': '-0.073581', 'rewards_train/rejected': '0.00079086', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.074372', 'logps_train/rejected': '-145.42', 'logps_train/chosen': '-136.53', 'loss/train': '0.74874', 'examples_per_second': '30.397', 'grad_norm': '71.5', 'counters/examples': 40416, 'counters/updates': 1263}
train stats after 40448 examples: {'rewards_train/chosen': '-0.0024685', 'rewards_train/rejected': '0.03638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.038848', 'logps_train/rejected': '-173.87', 'logps_train/chosen': '-143.48', 'loss/train': '0.72594', 'examples_per_second': '32.402', 'grad_norm': '80', 'counters/examples': 40448, 'counters/updates': 1264}
train stats after 40480 examples: {'rewards_train/chosen': '-0.025249', 'rewards_train/rejected': '0.024457', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049706', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-148.6', 'loss/train': '0.73667', 'examples_per_second': '31.601', 'grad_norm': '98.5', 'counters/examples': 40480, 'counters/updates': 1265}
train stats after 40512 examples: {'rewards_train/chosen': '0.07981', 'rewards_train/rejected': '0.022657', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057153', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-146.53', 'loss/train': '0.67104', 'examples_per_second': '32.497', 'grad_norm': '233', 'counters/examples': 40512, 'counters/updates': 1266}
train stats after 40544 examples: {'rewards_train/chosen': '0.099919', 'rewards_train/rejected': '-0.015929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11585', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-160.57', 'loss/train': '0.64775', 'examples_per_second': '31.539', 'grad_norm': '159', 'counters/examples': 40544, 'counters/updates': 1267}
train stats after 40576 examples: {'rewards_train/chosen': '-0.059275', 'rewards_train/rejected': '0.02366', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.082935', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-152.72', 'loss/train': '0.75367', 'examples_per_second': '30.216', 'grad_norm': '225', 'counters/examples': 40576, 'counters/updates': 1268}
train stats after 40608 examples: {'rewards_train/chosen': '0.034039', 'rewards_train/rejected': '-0.0079349', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041974', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-189.04', 'loss/train': '0.68459', 'examples_per_second': '30.082', 'grad_norm': '81', 'counters/examples': 40608, 'counters/updates': 1269}
train stats after 40640 examples: {'rewards_train/chosen': '-0.014425', 'rewards_train/rejected': '0.059864', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.074289', 'logps_train/rejected': '-105.04', 'logps_train/chosen': '-117.12', 'loss/train': '0.73783', 'examples_per_second': '32.224', 'grad_norm': '101', 'counters/examples': 40640, 'counters/updates': 1270}
train stats after 40672 examples: {'rewards_train/chosen': '0.075978', 'rewards_train/rejected': '-0.050834', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12681', 'logps_train/rejected': '-163.79', 'logps_train/chosen': '-140.81', 'loss/train': '0.6478', 'examples_per_second': '31.457', 'grad_norm': '111', 'counters/examples': 40672, 'counters/updates': 1271}
train stats after 40704 examples: {'rewards_train/chosen': '0.033257', 'rewards_train/rejected': '-0.020313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05357', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-139.35', 'loss/train': '0.67648', 'examples_per_second': '32.926', 'grad_norm': '74', 'counters/examples': 40704, 'counters/updates': 1272}
skipping logging after 40736 examples to avoid logging too frequently
train stats after 40768 examples: {'rewards_train/chosen': '0.016456', 'rewards_train/rejected': '-0.021841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038298', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-166.58', 'loss/train': '0.68561', 'examples_per_second': '32.203', 'grad_norm': '98.5', 'counters/examples': 40768, 'counters/updates': 1274}
skipping logging after 40800 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.052673', 'rewards_train/rejected': '-0.036404', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016269', 'logps_train/rejected': '-96.558', 'logps_train/chosen': '-128.64', 'loss/train': '0.70591', 'examples_per_second': '35.757', 'grad_norm': '85.5', 'counters/examples': 40832, 'counters/updates': 1276}
skipping logging after 40864 examples to avoid logging too frequently
train stats after 40896 examples: {'rewards_train/chosen': '-0.018518', 'rewards_train/rejected': '-0.0045523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013966', 'logps_train/rejected': '-148.65', 'logps_train/chosen': '-170.99', 'loss/train': '0.7175', 'examples_per_second': '31.642', 'grad_norm': '57', 'counters/examples': 40896, 'counters/updates': 1278}
train stats after 40928 examples: {'rewards_train/chosen': '0.023891', 'rewards_train/rejected': '-0.11264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-150.69', 'logps_train/chosen': '-135.3', 'loss/train': '0.63877', 'examples_per_second': '31.865', 'grad_norm': '173', 'counters/examples': 40928, 'counters/updates': 1279}
train stats after 40960 examples: {'rewards_train/chosen': '0.094116', 'rewards_train/rejected': '0.0076837', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086433', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-132.33', 'loss/train': '0.66968', 'examples_per_second': '24.477', 'grad_norm': '101.5', 'counters/examples': 40960, 'counters/updates': 1280}
train stats after 40992 examples: {'rewards_train/chosen': '0.019164', 'rewards_train/rejected': '-0.040266', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05943', 'logps_train/rejected': '-107.7', 'logps_train/chosen': '-130.21', 'loss/train': '0.6756', 'examples_per_second': '31.631', 'grad_norm': '50', 'counters/examples': 40992, 'counters/updates': 1281}
train stats after 41024 examples: {'rewards_train/chosen': '0.073948', 'rewards_train/rejected': '-0.0090299', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082977', 'logps_train/rejected': '-129.25', 'logps_train/chosen': '-148.82', 'loss/train': '0.68651', 'examples_per_second': '31.664', 'grad_norm': '111', 'counters/examples': 41024, 'counters/updates': 1282}
train stats after 41056 examples: {'rewards_train/chosen': '-0.0032762', 'rewards_train/rejected': '0.0063883', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0096645', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-185.92', 'loss/train': '0.70856', 'examples_per_second': '24.398', 'grad_norm': '107', 'counters/examples': 41056, 'counters/updates': 1283}
train stats after 41088 examples: {'rewards_train/chosen': '0.028348', 'rewards_train/rejected': '-0.012171', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040519', 'logps_train/rejected': '-154.56', 'logps_train/chosen': '-156.87', 'loss/train': '0.69778', 'examples_per_second': '30.619', 'grad_norm': '116.5', 'counters/examples': 41088, 'counters/updates': 1284}
skipping logging after 41120 examples to avoid logging too frequently
train stats after 41152 examples: {'rewards_train/chosen': '-0.0046982', 'rewards_train/rejected': '-0.0019004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0027978', 'logps_train/rejected': '-110.24', 'logps_train/chosen': '-151.79', 'loss/train': '0.70419', 'examples_per_second': '34.878', 'grad_norm': '57.5', 'counters/examples': 41152, 'counters/updates': 1286}
skipping logging after 41184 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '0.038415', 'rewards_train/rejected': '0.01924', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.019176', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-132.68', 'loss/train': '0.69514', 'examples_per_second': '37.318', 'grad_norm': '90', 'counters/examples': 41216, 'counters/updates': 1288}
train stats after 41248 examples: {'rewards_train/chosen': '0.072071', 'rewards_train/rejected': '0.017218', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.054853', 'logps_train/rejected': '-137.93', 'logps_train/chosen': '-164.69', 'loss/train': '0.6831', 'examples_per_second': '33.063', 'grad_norm': '81.5', 'counters/examples': 41248, 'counters/updates': 1289}
skipping logging after 41280 examples to avoid logging too frequently
train stats after 41312 examples: {'rewards_train/chosen': '-0.025344', 'rewards_train/rejected': '-0.018857', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0064874', 'logps_train/rejected': '-132.15', 'logps_train/chosen': '-151.48', 'loss/train': '0.70305', 'examples_per_second': '32.817', 'grad_norm': '192', 'counters/examples': 41312, 'counters/updates': 1291}
skipping logging after 41344 examples to avoid logging too frequently
train stats after 41376 examples: {'rewards_train/chosen': '0.04618', 'rewards_train/rejected': '-0.017008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063188', 'logps_train/rejected': '-132.63', 'logps_train/chosen': '-119.19', 'loss/train': '0.67441', 'examples_per_second': '31.19', 'grad_norm': '86.5', 'counters/examples': 41376, 'counters/updates': 1293}
train stats after 41408 examples: {'rewards_train/chosen': '-0.073206', 'rewards_train/rejected': '-0.015031', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.058175', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-137.84', 'loss/train': '0.73425', 'examples_per_second': '32.181', 'grad_norm': '119', 'counters/examples': 41408, 'counters/updates': 1294}
train stats after 41440 examples: {'rewards_train/chosen': '0.040487', 'rewards_train/rejected': '0.03483', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056572', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-144.83', 'loss/train': '0.70508', 'examples_per_second': '31.677', 'grad_norm': '105', 'counters/examples': 41440, 'counters/updates': 1295}
train stats after 41472 examples: {'rewards_train/chosen': '0.0085583', 'rewards_train/rejected': '-0.010467', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.019026', 'logps_train/rejected': '-140.87', 'logps_train/chosen': '-190.34', 'loss/train': '0.70117', 'examples_per_second': '30.969', 'grad_norm': '136', 'counters/examples': 41472, 'counters/updates': 1296}
skipping logging after 41504 examples to avoid logging too frequently
train stats after 41536 examples: {'rewards_train/chosen': '0.060157', 'rewards_train/rejected': '-0.0093875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069545', 'logps_train/rejected': '-162.76', 'logps_train/chosen': '-156.14', 'loss/train': '0.67347', 'examples_per_second': '32.882', 'grad_norm': '95.5', 'counters/examples': 41536, 'counters/updates': 1298}
train stats after 41568 examples: {'rewards_train/chosen': '0.011537', 'rewards_train/rejected': '-0.075584', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087121', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-131.01', 'loss/train': '0.66546', 'examples_per_second': '30.542', 'grad_norm': '137', 'counters/examples': 41568, 'counters/updates': 1299}
train stats after 41600 examples: {'rewards_train/chosen': '0.012534', 'rewards_train/rejected': '0.015509', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0029757', 'logps_train/rejected': '-157.38', 'logps_train/chosen': '-141.54', 'loss/train': '0.71359', 'examples_per_second': '30.932', 'grad_norm': '89', 'counters/examples': 41600, 'counters/updates': 1300}
train stats after 41632 examples: {'rewards_train/chosen': '0.044979', 'rewards_train/rejected': '0.036509', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0084693', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-151.16', 'loss/train': '0.70596', 'examples_per_second': '30.632', 'grad_norm': '141', 'counters/examples': 41632, 'counters/updates': 1301}
train stats after 41664 examples: {'rewards_train/chosen': '0.023465', 'rewards_train/rejected': '-0.084255', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10772', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-132.61', 'loss/train': '0.65065', 'examples_per_second': '31.726', 'grad_norm': '126.5', 'counters/examples': 41664, 'counters/updates': 1302}
train stats after 41696 examples: {'rewards_train/chosen': '-0.026774', 'rewards_train/rejected': '-0.071097', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044323', 'logps_train/rejected': '-136.31', 'logps_train/chosen': '-166.4', 'loss/train': '0.68142', 'examples_per_second': '31.699', 'grad_norm': '107', 'counters/examples': 41696, 'counters/updates': 1303}
train stats after 41728 examples: {'rewards_train/chosen': '-0.013963', 'rewards_train/rejected': '-0.003846', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010117', 'logps_train/rejected': '-115.39', 'logps_train/chosen': '-139.77', 'loss/train': '0.71616', 'examples_per_second': '30.297', 'grad_norm': '106.5', 'counters/examples': 41728, 'counters/updates': 1304}
skipping logging after 41760 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': '0.0039568', 'rewards_train/rejected': '0.012048', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0080914', 'logps_train/rejected': '-168.39', 'logps_train/chosen': '-147.16', 'loss/train': '0.71727', 'examples_per_second': '31.644', 'grad_norm': '112', 'counters/examples': 41792, 'counters/updates': 1306}
train stats after 41824 examples: {'rewards_train/chosen': '-0.017265', 'rewards_train/rejected': '-0.010323', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0069426', 'logps_train/rejected': '-112.41', 'logps_train/chosen': '-134.78', 'loss/train': '0.70763', 'examples_per_second': '32.713', 'grad_norm': '64', 'counters/examples': 41824, 'counters/updates': 1307}
train stats after 41856 examples: {'rewards_train/chosen': '-0.016189', 'rewards_train/rejected': '-0.021568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0053796', 'logps_train/rejected': '-134.5', 'logps_train/chosen': '-165.51', 'loss/train': '0.70554', 'examples_per_second': '31.528', 'grad_norm': '115', 'counters/examples': 41856, 'counters/updates': 1308}
train stats after 41888 examples: {'rewards_train/chosen': '-0.0043151', 'rewards_train/rejected': '-0.015917', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011602', 'logps_train/rejected': '-97.659', 'logps_train/chosen': '-132.16', 'loss/train': '0.69904', 'examples_per_second': '32.252', 'grad_norm': '217', 'counters/examples': 41888, 'counters/updates': 1309}
train stats after 41920 examples: {'rewards_train/chosen': '0.0055236', 'rewards_train/rejected': '0.027109', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021585', 'logps_train/rejected': '-148.88', 'logps_train/chosen': '-148.77', 'loss/train': '0.7203', 'examples_per_second': '24.458', 'grad_norm': '72', 'counters/examples': 41920, 'counters/updates': 1310}
train stats after 41952 examples: {'rewards_train/chosen': '-0.032803', 'rewards_train/rejected': '0.1448', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.1776', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-157.27', 'loss/train': '0.80487', 'examples_per_second': '31.225', 'grad_norm': '180', 'counters/examples': 41952, 'counters/updates': 1311}
skipping logging after 41984 examples to avoid logging too frequently
train stats after 42016 examples: {'rewards_train/chosen': '0.013932', 'rewards_train/rejected': '0.078409', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.064477', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-143.76', 'loss/train': '0.74775', 'examples_per_second': '30.487', 'grad_norm': '61.25', 'counters/examples': 42016, 'counters/updates': 1313}
train stats after 42048 examples: {'rewards_train/chosen': '-0.031409', 'rewards_train/rejected': '0.036045', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.067455', 'logps_train/rejected': '-146.29', 'logps_train/chosen': '-151.01', 'loss/train': '0.74011', 'examples_per_second': '33.136', 'grad_norm': '117', 'counters/examples': 42048, 'counters/updates': 1314}
train stats after 42080 examples: {'rewards_train/chosen': '0.02993', 'rewards_train/rejected': '0.038201', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0082713', 'logps_train/rejected': '-153.28', 'logps_train/chosen': '-122.44', 'loss/train': '0.70944', 'examples_per_second': '31.723', 'grad_norm': '74', 'counters/examples': 42080, 'counters/updates': 1315}
train stats after 42112 examples: {'rewards_train/chosen': '-0.03691', 'rewards_train/rejected': '0.015498', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052408', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-135.96', 'loss/train': '0.72525', 'examples_per_second': '31.728', 'grad_norm': '130', 'counters/examples': 42112, 'counters/updates': 1316}
train stats after 42144 examples: {'rewards_train/chosen': '0.081298', 'rewards_train/rejected': '0.0052952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076003', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-155.95', 'loss/train': '0.665', 'examples_per_second': '31.72', 'grad_norm': '78.5', 'counters/examples': 42144, 'counters/updates': 1317}
train stats after 42176 examples: {'rewards_train/chosen': '0.075228', 'rewards_train/rejected': '0.0091161', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066112', 'logps_train/rejected': '-187.12', 'logps_train/chosen': '-180.48', 'loss/train': '0.67609', 'examples_per_second': '31.697', 'grad_norm': '122.5', 'counters/examples': 42176, 'counters/updates': 1318}
train stats after 42208 examples: {'rewards_train/chosen': '0.07954', 'rewards_train/rejected': '0.021', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05854', 'logps_train/rejected': '-105.17', 'logps_train/chosen': '-159.55', 'loss/train': '0.67122', 'examples_per_second': '30.338', 'grad_norm': '54', 'counters/examples': 42208, 'counters/updates': 1319}
train stats after 42240 examples: {'rewards_train/chosen': '0.067484', 'rewards_train/rejected': '0.0061319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061352', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-159.54', 'loss/train': '0.67664', 'examples_per_second': '30.747', 'grad_norm': '60.75', 'counters/examples': 42240, 'counters/updates': 1320}
train stats after 42272 examples: {'rewards_train/chosen': '0.0042042', 'rewards_train/rejected': '-0.050956', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05516', 'logps_train/rejected': '-130.2', 'logps_train/chosen': '-141.33', 'loss/train': '0.68251', 'examples_per_second': '30.885', 'grad_norm': '74', 'counters/examples': 42272, 'counters/updates': 1321}
train stats after 42304 examples: {'rewards_train/chosen': '0.00096257', 'rewards_train/rejected': '0.024026', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023063', 'logps_train/rejected': '-148.84', 'logps_train/chosen': '-134.07', 'loss/train': '0.71828', 'examples_per_second': '32.711', 'grad_norm': '96', 'counters/examples': 42304, 'counters/updates': 1322}
skipping logging after 42336 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '-0.036023', 'rewards_train/rejected': '-0.022314', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013708', 'logps_train/rejected': '-144.29', 'logps_train/chosen': '-137.61', 'loss/train': '0.70488', 'examples_per_second': '36.32', 'grad_norm': '71', 'counters/examples': 42368, 'counters/updates': 1324}
train stats after 42400 examples: {'rewards_train/chosen': '0.025557', 'rewards_train/rejected': '-0.040732', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066289', 'logps_train/rejected': '-154.02', 'logps_train/chosen': '-154.33', 'loss/train': '0.67498', 'examples_per_second': '32.01', 'grad_norm': '105.5', 'counters/examples': 42400, 'counters/updates': 1325}
train stats after 42432 examples: {'rewards_train/chosen': '0.019359', 'rewards_train/rejected': '0.056031', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036672', 'logps_train/rejected': '-95.687', 'logps_train/chosen': '-124.87', 'loss/train': '0.72186', 'examples_per_second': '32.526', 'grad_norm': '153', 'counters/examples': 42432, 'counters/updates': 1326}
train stats after 42464 examples: {'rewards_train/chosen': '0.032348', 'rewards_train/rejected': '0.034798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.00245', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-158.73', 'loss/train': '0.72982', 'examples_per_second': '31.549', 'grad_norm': '118', 'counters/examples': 42464, 'counters/updates': 1327}
train stats after 42496 examples: {'rewards_train/chosen': '-0.006175', 'rewards_train/rejected': '-0.05366', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047485', 'logps_train/rejected': '-105.88', 'logps_train/chosen': '-134.6', 'loss/train': '0.67821', 'examples_per_second': '31.427', 'grad_norm': '85.5', 'counters/examples': 42496, 'counters/updates': 1328}
train stats after 42528 examples: {'rewards_train/chosen': '0.033157', 'rewards_train/rejected': '-0.079286', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11244', 'logps_train/rejected': '-118.4', 'logps_train/chosen': '-155.62', 'loss/train': '0.6492', 'examples_per_second': '32.286', 'grad_norm': '81', 'counters/examples': 42528, 'counters/updates': 1329}
train stats after 42560 examples: {'rewards_train/chosen': '0.020086', 'rewards_train/rejected': '-0.076037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096123', 'logps_train/rejected': '-151.14', 'logps_train/chosen': '-142.65', 'loss/train': '0.66727', 'examples_per_second': '31.707', 'grad_norm': '159', 'counters/examples': 42560, 'counters/updates': 1330}
skipping logging after 42592 examples to avoid logging too frequently
train stats after 42624 examples: {'rewards_train/chosen': '0.031023', 'rewards_train/rejected': '-0.0062558', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037278', 'logps_train/rejected': '-114.81', 'logps_train/chosen': '-142.83', 'loss/train': '0.6883', 'examples_per_second': '30.751', 'grad_norm': '79.5', 'counters/examples': 42624, 'counters/updates': 1332}
skipping logging after 42656 examples to avoid logging too frequently
train stats after 42688 examples: {'rewards_train/chosen': '0.088718', 'rewards_train/rejected': '-0.04168', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1304', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-154.54', 'loss/train': '0.65107', 'examples_per_second': '33.545', 'grad_norm': '80', 'counters/examples': 42688, 'counters/updates': 1334}
skipping logging after 42720 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '-0.00029165', 'rewards_train/rejected': '0.01243', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012722', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-171.77', 'loss/train': '0.70629', 'examples_per_second': '31.704', 'grad_norm': '51.25', 'counters/examples': 42752, 'counters/updates': 1336}
train stats after 42784 examples: {'rewards_train/chosen': '-0.0056202', 'rewards_train/rejected': '-0.021338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015717', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-128.72', 'loss/train': '0.69399', 'examples_per_second': '30.222', 'grad_norm': '84.5', 'counters/examples': 42784, 'counters/updates': 1337}
train stats after 42816 examples: {'rewards_train/chosen': '-0.044201', 'rewards_train/rejected': '0.00025483', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.044456', 'logps_train/rejected': '-160.4', 'logps_train/chosen': '-176.57', 'loss/train': '0.73112', 'examples_per_second': '32.632', 'grad_norm': '153', 'counters/examples': 42816, 'counters/updates': 1338}
train stats after 42848 examples: {'rewards_train/chosen': '0.029229', 'rewards_train/rejected': '-0.074778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10401', 'logps_train/rejected': '-123.06', 'logps_train/chosen': '-108.77', 'loss/train': '0.65775', 'examples_per_second': '32.531', 'grad_norm': '46.5', 'counters/examples': 42848, 'counters/updates': 1339}
skipping logging after 42880 examples to avoid logging too frequently
train stats after 42912 examples: {'rewards_train/chosen': '0.036803', 'rewards_train/rejected': '0.070563', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03376', 'logps_train/rejected': '-126.85', 'logps_train/chosen': '-144.8', 'loss/train': '0.72246', 'examples_per_second': '32.022', 'grad_norm': '87', 'counters/examples': 42912, 'counters/updates': 1341}
train stats after 42944 examples: {'rewards_train/chosen': '0.044568', 'rewards_train/rejected': '-0.0010196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045587', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-145.43', 'loss/train': '0.67873', 'examples_per_second': '31.7', 'grad_norm': '115', 'counters/examples': 42944, 'counters/updates': 1342}
train stats after 42976 examples: {'rewards_train/chosen': '-0.025707', 'rewards_train/rejected': '-0.036325', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010618', 'logps_train/rejected': '-137.54', 'logps_train/chosen': '-159.18', 'loss/train': '0.70436', 'examples_per_second': '31.698', 'grad_norm': '68.5', 'counters/examples': 42976, 'counters/updates': 1343}
train stats after 43008 examples: {'rewards_train/chosen': '0.0044404', 'rewards_train/rejected': '0.014427', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0099863', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-189.7', 'loss/train': '0.71582', 'examples_per_second': '31.601', 'grad_norm': '270', 'counters/examples': 43008, 'counters/updates': 1344}
train stats after 43040 examples: {'rewards_train/chosen': '0.012551', 'rewards_train/rejected': '-0.0099208', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022472', 'logps_train/rejected': '-152.09', 'logps_train/chosen': '-154.97', 'loss/train': '0.69113', 'examples_per_second': '31.722', 'grad_norm': '95', 'counters/examples': 43040, 'counters/updates': 1345}
train stats after 43072 examples: {'rewards_train/chosen': '0.035364', 'rewards_train/rejected': '-0.024087', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059451', 'logps_train/rejected': '-137.02', 'logps_train/chosen': '-167.95', 'loss/train': '0.67463', 'examples_per_second': '32.295', 'grad_norm': '152', 'counters/examples': 43072, 'counters/updates': 1346}
train stats after 43104 examples: {'rewards_train/chosen': '0.028099', 'rewards_train/rejected': '-0.058315', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086414', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-129.99', 'loss/train': '0.66442', 'examples_per_second': '31.129', 'grad_norm': '81.5', 'counters/examples': 43104, 'counters/updates': 1347}
train stats after 43136 examples: {'rewards_train/chosen': '0.15655', 'rewards_train/rejected': '0.084665', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071883', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-171.28', 'loss/train': '0.67716', 'examples_per_second': '32.956', 'grad_norm': '131', 'counters/examples': 43136, 'counters/updates': 1348}
skipping logging after 43168 examples to avoid logging too frequently
train stats after 43200 examples: {'rewards_train/chosen': '0.038206', 'rewards_train/rejected': '0.054908', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016701', 'logps_train/rejected': '-132.43', 'logps_train/chosen': '-162.6', 'loss/train': '0.7298', 'examples_per_second': '31.633', 'grad_norm': '68.5', 'counters/examples': 43200, 'counters/updates': 1350}
train stats after 43232 examples: {'rewards_train/chosen': '0.04591', 'rewards_train/rejected': '-0.052663', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098572', 'logps_train/rejected': '-96.099', 'logps_train/chosen': '-146.22', 'loss/train': '0.65307', 'examples_per_second': '31.067', 'grad_norm': '74', 'counters/examples': 43232, 'counters/updates': 1351}
train stats after 43264 examples: {'rewards_train/chosen': '0.011039', 'rewards_train/rejected': '-0.068249', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079288', 'logps_train/rejected': '-161.43', 'logps_train/chosen': '-171.37', 'loss/train': '0.66449', 'examples_per_second': '30.715', 'grad_norm': '114.5', 'counters/examples': 43264, 'counters/updates': 1352}
skipping logging after 43296 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': '0.163', 'rewards_train/rejected': '-0.034398', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1974', 'logps_train/rejected': '-149.35', 'logps_train/chosen': '-178.62', 'loss/train': '0.63383', 'examples_per_second': '30.517', 'grad_norm': '152', 'counters/examples': 43328, 'counters/updates': 1354}
skipping logging after 43360 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '0.02796', 'rewards_train/rejected': '0.025777', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0021825', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-168.1', 'loss/train': '0.70401', 'examples_per_second': '30.614', 'grad_norm': '74', 'counters/examples': 43392, 'counters/updates': 1356}
train stats after 43424 examples: {'rewards_train/chosen': '-0.059512', 'rewards_train/rejected': '-0.023496', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036016', 'logps_train/rejected': '-149.38', 'logps_train/chosen': '-138.05', 'loss/train': '0.72713', 'examples_per_second': '32.184', 'grad_norm': '247', 'counters/examples': 43424, 'counters/updates': 1357}
train stats after 43456 examples: {'rewards_train/chosen': '-0.076847', 'rewards_train/rejected': '-0.062425', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014421', 'logps_train/rejected': '-162.04', 'logps_train/chosen': '-137.36', 'loss/train': '0.72078', 'examples_per_second': '31.707', 'grad_norm': '118', 'counters/examples': 43456, 'counters/updates': 1358}
train stats after 43488 examples: {'rewards_train/chosen': '0.029591', 'rewards_train/rejected': '0.039074', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0094833', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-121.71', 'loss/train': '0.70418', 'examples_per_second': '30.282', 'grad_norm': '77', 'counters/examples': 43488, 'counters/updates': 1359}
train stats after 43520 examples: {'rewards_train/chosen': '0.058363', 'rewards_train/rejected': '-0.02401', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.082373', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-167.53', 'loss/train': '0.67529', 'examples_per_second': '31.409', 'grad_norm': '94', 'counters/examples': 43520, 'counters/updates': 1360}
train stats after 43552 examples: {'rewards_train/chosen': '0.051001', 'rewards_train/rejected': '0.0061701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044831', 'logps_train/rejected': '-172.01', 'logps_train/chosen': '-128.65', 'loss/train': '0.68589', 'examples_per_second': '31.693', 'grad_norm': '88.5', 'counters/examples': 43552, 'counters/updates': 1361}
train stats after 43584 examples: {'rewards_train/chosen': '0.078945', 'rewards_train/rejected': '0.0099686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068976', 'logps_train/rejected': '-130.81', 'logps_train/chosen': '-159.73', 'loss/train': '0.67304', 'examples_per_second': '31.693', 'grad_norm': '74.5', 'counters/examples': 43584, 'counters/updates': 1362}
train stats after 43616 examples: {'rewards_train/chosen': '-0.044372', 'rewards_train/rejected': '-0.023062', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02131', 'logps_train/rejected': '-82.441', 'logps_train/chosen': '-110.04', 'loss/train': '0.71061', 'examples_per_second': '30.949', 'grad_norm': '93.5', 'counters/examples': 43616, 'counters/updates': 1363}
skipping logging after 43648 examples to avoid logging too frequently
train stats after 43680 examples: {'rewards_train/chosen': '0.0089484', 'rewards_train/rejected': '0.036305', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027357', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-129.08', 'loss/train': '0.72003', 'examples_per_second': '35.964', 'grad_norm': '66', 'counters/examples': 43680, 'counters/updates': 1365}
train stats after 43712 examples: {'rewards_train/chosen': '0.060353', 'rewards_train/rejected': '0.075165', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014812', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-184.52', 'loss/train': '0.71385', 'examples_per_second': '31.601', 'grad_norm': '96.5', 'counters/examples': 43712, 'counters/updates': 1366}
train stats after 43744 examples: {'rewards_train/chosen': '0.031606', 'rewards_train/rejected': '-0.033468', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065074', 'logps_train/rejected': '-98.276', 'logps_train/chosen': '-145.37', 'loss/train': '0.67404', 'examples_per_second': '31.693', 'grad_norm': '84.5', 'counters/examples': 43744, 'counters/updates': 1367}
skipping logging after 43776 examples to avoid logging too frequently
train stats after 43808 examples: {'rewards_train/chosen': '-0.086313', 'rewards_train/rejected': '-0.063714', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022599', 'logps_train/rejected': '-91.181', 'logps_train/chosen': '-118.99', 'loss/train': '0.72035', 'examples_per_second': '34.264', 'grad_norm': '70', 'counters/examples': 43808, 'counters/updates': 1369}
train stats after 43840 examples: {'rewards_train/chosen': '0.050284', 'rewards_train/rejected': '0.056697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0064126', 'logps_train/rejected': '-173.94', 'logps_train/chosen': '-163.51', 'loss/train': '0.71612', 'examples_per_second': '31.418', 'grad_norm': '74', 'counters/examples': 43840, 'counters/updates': 1370}
skipping logging after 43872 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '-0.0077603', 'rewards_train/rejected': '0.053937', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.061698', 'logps_train/rejected': '-98.869', 'logps_train/chosen': '-124.63', 'loss/train': '0.73394', 'examples_per_second': '34.296', 'grad_norm': '113', 'counters/examples': 43904, 'counters/updates': 1372}
train stats after 43936 examples: {'rewards_train/chosen': '-0.0227', 'rewards_train/rejected': '-0.03192', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0092202', 'logps_train/rejected': '-168.59', 'logps_train/chosen': '-202.04', 'loss/train': '0.70222', 'examples_per_second': '30.882', 'grad_norm': '207', 'counters/examples': 43936, 'counters/updates': 1373}
train stats after 43968 examples: {'rewards_train/chosen': '0.0045225', 'rewards_train/rejected': '-0.04892', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053443', 'logps_train/rejected': '-134.73', 'logps_train/chosen': '-135.42', 'loss/train': '0.68264', 'examples_per_second': '33.13', 'grad_norm': '81', 'counters/examples': 43968, 'counters/updates': 1374}
train stats after 44000 examples: {'rewards_train/chosen': '-0.0060588', 'rewards_train/rejected': '0.014454', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020513', 'logps_train/rejected': '-155.61', 'logps_train/chosen': '-180.1', 'loss/train': '0.71891', 'examples_per_second': '31.532', 'grad_norm': '156', 'counters/examples': 44000, 'counters/updates': 1375}
train stats after 44032 examples: {'rewards_train/chosen': '0.023053', 'rewards_train/rejected': '-0.029893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052946', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-144.04', 'loss/train': '0.67619', 'examples_per_second': '31.771', 'grad_norm': '79.5', 'counters/examples': 44032, 'counters/updates': 1376}
skipping logging after 44064 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': '-0.034861', 'rewards_train/rejected': '0.062375', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.097236', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-111.58', 'loss/train': '0.75502', 'examples_per_second': '31.047', 'grad_norm': '102', 'counters/examples': 44096, 'counters/updates': 1378}
train stats after 44128 examples: {'rewards_train/chosen': '0.0064056', 'rewards_train/rejected': '-0.022162', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028568', 'logps_train/rejected': '-152.15', 'logps_train/chosen': '-201.25', 'loss/train': '0.69441', 'examples_per_second': '31.689', 'grad_norm': '125.5', 'counters/examples': 44128, 'counters/updates': 1379}
train stats after 44160 examples: {'rewards_train/chosen': '-0.0072371', 'rewards_train/rejected': '-0.019154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011917', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-129.37', 'loss/train': '0.69326', 'examples_per_second': '31.741', 'grad_norm': '132', 'counters/examples': 44160, 'counters/updates': 1380}
train stats after 44192 examples: {'rewards_train/chosen': '-0.0041897', 'rewards_train/rejected': '-0.021198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017009', 'logps_train/rejected': '-93.846', 'logps_train/chosen': '-152.41', 'loss/train': '0.69332', 'examples_per_second': '31.499', 'grad_norm': '107', 'counters/examples': 44192, 'counters/updates': 1381}
train stats after 44224 examples: {'rewards_train/chosen': '-0.048731', 'rewards_train/rejected': '0.036659', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.08539', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-183.05', 'loss/train': '0.74855', 'examples_per_second': '31.725', 'grad_norm': '101', 'counters/examples': 44224, 'counters/updates': 1382}
train stats after 44256 examples: {'rewards_train/chosen': '-0.036488', 'rewards_train/rejected': '0.0050721', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.04156', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-153.41', 'loss/train': '0.72329', 'examples_per_second': '32.706', 'grad_norm': '123.5', 'counters/examples': 44256, 'counters/updates': 1383}
train stats after 44288 examples: {'rewards_train/chosen': '-0.074891', 'rewards_train/rejected': '0.027429', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.10232', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-131.41', 'loss/train': '0.75418', 'examples_per_second': '32.055', 'grad_norm': '68.5', 'counters/examples': 44288, 'counters/updates': 1384}
skipping logging after 44320 examples to avoid logging too frequently
train stats after 44352 examples: {'rewards_train/chosen': '0.041445', 'rewards_train/rejected': '-0.027851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069296', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-135.02', 'loss/train': '0.67575', 'examples_per_second': '30.232', 'grad_norm': '78', 'counters/examples': 44352, 'counters/updates': 1386}
train stats after 44384 examples: {'rewards_train/chosen': '0.02227', 'rewards_train/rejected': '0.010534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011735', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-136.31', 'loss/train': '0.69358', 'examples_per_second': '31.646', 'grad_norm': '167', 'counters/examples': 44384, 'counters/updates': 1387}
train stats after 44416 examples: {'rewards_train/chosen': '0.044139', 'rewards_train/rejected': '-0.08302', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12716', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-159.63', 'loss/train': '0.64706', 'examples_per_second': '30.175', 'grad_norm': '69', 'counters/examples': 44416, 'counters/updates': 1388}
train stats after 44448 examples: {'rewards_train/chosen': '0.059086', 'rewards_train/rejected': '0.013638', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.045448', 'logps_train/rejected': '-124.88', 'logps_train/chosen': '-150.33', 'loss/train': '0.68082', 'examples_per_second': '32.432', 'grad_norm': '80', 'counters/examples': 44448, 'counters/updates': 1389}
skipping logging after 44480 examples to avoid logging too frequently
train stats after 44512 examples: {'rewards_train/chosen': '0.048482', 'rewards_train/rejected': '-0.052966', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10145', 'logps_train/rejected': '-101.03', 'logps_train/chosen': '-141.33', 'loss/train': '0.64829', 'examples_per_second': '40.392', 'grad_norm': '55.5', 'counters/examples': 44512, 'counters/updates': 1391}
train stats after 44544 examples: {'rewards_train/chosen': '0.069226', 'rewards_train/rejected': '-0.0073598', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076585', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-128.83', 'loss/train': '0.66364', 'examples_per_second': '31.753', 'grad_norm': '69', 'counters/examples': 44544, 'counters/updates': 1392}
train stats after 44576 examples: {'rewards_train/chosen': '0.01363', 'rewards_train/rejected': '-0.0017681', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015398', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-182.62', 'loss/train': '0.7044', 'examples_per_second': '31.688', 'grad_norm': '376', 'counters/examples': 44576, 'counters/updates': 1393}
train stats after 44608 examples: {'rewards_train/chosen': '0.060827', 'rewards_train/rejected': '-0.037932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098759', 'logps_train/rejected': '-112.4', 'logps_train/chosen': '-185.35', 'loss/train': '0.65813', 'examples_per_second': '30.82', 'grad_norm': '75', 'counters/examples': 44608, 'counters/updates': 1394}
skipping logging after 44640 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '-0.004349', 'rewards_train/rejected': '0.1431', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.14745', 'logps_train/rejected': '-134.15', 'logps_train/chosen': '-157.93', 'loss/train': '0.79536', 'examples_per_second': '31.605', 'grad_norm': '110', 'counters/examples': 44672, 'counters/updates': 1396}
skipping logging after 44704 examples to avoid logging too frequently
train stats after 44736 examples: {'rewards_train/chosen': '0.056226', 'rewards_train/rejected': '0.049258', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0069679', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-150.78', 'loss/train': '0.69821', 'examples_per_second': '30.664', 'grad_norm': '91', 'counters/examples': 44736, 'counters/updates': 1398}
train stats after 44768 examples: {'rewards_train/chosen': '0.062619', 'rewards_train/rejected': '0.08532', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0227', 'logps_train/rejected': '-145.26', 'logps_train/chosen': '-143.58', 'loss/train': '0.71045', 'examples_per_second': '31.651', 'grad_norm': '90.5', 'counters/examples': 44768, 'counters/updates': 1399}
train stats after 44800 examples: {'rewards_train/chosen': '-0.031132', 'rewards_train/rejected': '-0.0050672', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026065', 'logps_train/rejected': '-157.52', 'logps_train/chosen': '-205.68', 'loss/train': '0.72081', 'examples_per_second': '31.712', 'grad_norm': '158', 'counters/examples': 44800, 'counters/updates': 1400}
train stats after 44832 examples: {'rewards_train/chosen': '-0.0080854', 'rewards_train/rejected': '0.0045457', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012631', 'logps_train/rejected': '-160.33', 'logps_train/chosen': '-150.01', 'loss/train': '0.7076', 'examples_per_second': '30.316', 'grad_norm': '61.25', 'counters/examples': 44832, 'counters/updates': 1401}
train stats after 44864 examples: {'rewards_train/chosen': '-0.081666', 'rewards_train/rejected': '0.020194', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.10186', 'logps_train/rejected': '-162.77', 'logps_train/chosen': '-158.22', 'loss/train': '0.75975', 'examples_per_second': '32.263', 'grad_norm': '76', 'counters/examples': 44864, 'counters/updates': 1402}
train stats after 44896 examples: {'rewards_train/chosen': '0.070071', 'rewards_train/rejected': '-0.050762', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12083', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-152.07', 'loss/train': '0.6591', 'examples_per_second': '31.66', 'grad_norm': '218', 'counters/examples': 44896, 'counters/updates': 1403}
train stats after 44928 examples: {'rewards_train/chosen': '-0.038462', 'rewards_train/rejected': '-0.0052201', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033241', 'logps_train/rejected': '-195.41', 'logps_train/chosen': '-166.26', 'loss/train': '0.72004', 'examples_per_second': '31.336', 'grad_norm': '125', 'counters/examples': 44928, 'counters/updates': 1404}
train stats after 44960 examples: {'rewards_train/chosen': '0.0025546', 'rewards_train/rejected': '-0.039183', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041737', 'logps_train/rejected': '-152.64', 'logps_train/chosen': '-141.85', 'loss/train': '0.68286', 'examples_per_second': '31.898', 'grad_norm': '264', 'counters/examples': 44960, 'counters/updates': 1405}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45024 examples: {'rewards_train/chosen': '0.098973', 'rewards_train/rejected': '-0.014665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11364', 'logps_train/rejected': '-150.53', 'logps_train/chosen': '-178.7', 'loss/train': '0.64938', 'examples_per_second': '31.729', 'grad_norm': '67.5', 'counters/examples': 45024, 'counters/updates': 1407}
train stats after 45056 examples: {'rewards_train/chosen': '0.062906', 'rewards_train/rejected': '-0.0085353', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071441', 'logps_train/rejected': '-138.83', 'logps_train/chosen': '-136.67', 'loss/train': '0.66744', 'examples_per_second': '32.463', 'grad_norm': '67', 'counters/examples': 45056, 'counters/updates': 1408}
train stats after 45088 examples: {'rewards_train/chosen': '-0.044632', 'rewards_train/rejected': '-0.028987', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015645', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-147', 'loss/train': '0.71814', 'examples_per_second': '30.207', 'grad_norm': '157', 'counters/examples': 45088, 'counters/updates': 1409}
train stats after 45120 examples: {'rewards_train/chosen': '-0.040178', 'rewards_train/rejected': '0.056598', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.096776', 'logps_train/rejected': '-107.06', 'logps_train/chosen': '-123.73', 'loss/train': '0.75908', 'examples_per_second': '33.063', 'grad_norm': '87.5', 'counters/examples': 45120, 'counters/updates': 1410}
train stats after 45152 examples: {'rewards_train/chosen': '0.050334', 'rewards_train/rejected': '0.0026643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04767', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-124.45', 'loss/train': '0.68027', 'examples_per_second': '32.647', 'grad_norm': '182', 'counters/examples': 45152, 'counters/updates': 1411}
train stats after 45184 examples: {'rewards_train/chosen': '-0.039653', 'rewards_train/rejected': '0.01403', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053683', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-148.51', 'loss/train': '0.73289', 'examples_per_second': '30.878', 'grad_norm': '123.5', 'counters/examples': 45184, 'counters/updates': 1412}
train stats after 45216 examples: {'rewards_train/chosen': '0.044451', 'rewards_train/rejected': '-0.003834', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.048285', 'logps_train/rejected': '-144.64', 'logps_train/chosen': '-142.26', 'loss/train': '0.68511', 'examples_per_second': '30.706', 'grad_norm': '53.5', 'counters/examples': 45216, 'counters/updates': 1413}
train stats after 45248 examples: {'rewards_train/chosen': '0.015874', 'rewards_train/rejected': '-0.0021139', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017988', 'logps_train/rejected': '-145.95', 'logps_train/chosen': '-125.89', 'loss/train': '0.70311', 'examples_per_second': '32.594', 'grad_norm': '125.5', 'counters/examples': 45248, 'counters/updates': 1414}
train stats after 45280 examples: {'rewards_train/chosen': '0.064263', 'rewards_train/rejected': '-0.016193', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080456', 'logps_train/rejected': '-107.08', 'logps_train/chosen': '-132.49', 'loss/train': '0.67667', 'examples_per_second': '31.27', 'grad_norm': '78.5', 'counters/examples': 45280, 'counters/updates': 1415}
train stats after 45312 examples: {'rewards_train/chosen': '0.01041', 'rewards_train/rejected': '0.013106', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0026951', 'logps_train/rejected': '-153.88', 'logps_train/chosen': '-171.65', 'loss/train': '0.70591', 'examples_per_second': '32.192', 'grad_norm': '91', 'counters/examples': 45312, 'counters/updates': 1416}
skipping logging after 45344 examples to avoid logging too frequently
train stats after 45376 examples: {'rewards_train/chosen': '-0.062731', 'rewards_train/rejected': '0.11037', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.1731', 'logps_train/rejected': '-109.82', 'logps_train/chosen': '-141.86', 'loss/train': '0.81132', 'examples_per_second': '31.662', 'grad_norm': '139', 'counters/examples': 45376, 'counters/updates': 1418}
train stats after 45408 examples: {'rewards_train/chosen': '0.01288', 'rewards_train/rejected': '0.055384', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042504', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-142.36', 'loss/train': '0.72841', 'examples_per_second': '32.384', 'grad_norm': '236', 'counters/examples': 45408, 'counters/updates': 1419}
train stats after 45440 examples: {'rewards_train/chosen': '-0.023032', 'rewards_train/rejected': '0.067142', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.090174', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-124.15', 'loss/train': '0.74939', 'examples_per_second': '31.284', 'grad_norm': '122', 'counters/examples': 45440, 'counters/updates': 1420}
train stats after 45472 examples: {'rewards_train/chosen': '-0.0097687', 'rewards_train/rejected': '0.05856', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.068329', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-119.21', 'loss/train': '0.73696', 'examples_per_second': '30.636', 'grad_norm': '181', 'counters/examples': 45472, 'counters/updates': 1421}
train stats after 45504 examples: {'rewards_train/chosen': '0.034634', 'rewards_train/rejected': '0.015351', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019283', 'logps_train/rejected': '-142.45', 'logps_train/chosen': '-200.76', 'loss/train': '0.69509', 'examples_per_second': '32.959', 'grad_norm': '80.5', 'counters/examples': 45504, 'counters/updates': 1422}
skipping logging after 45536 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '0.020353', 'rewards_train/rejected': '0.0019272', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018426', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-119.41', 'loss/train': '0.69186', 'examples_per_second': '33.129', 'grad_norm': '55.25', 'counters/examples': 45568, 'counters/updates': 1424}
train stats after 45600 examples: {'rewards_train/chosen': '0.048338', 'rewards_train/rejected': '-0.011193', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059531', 'logps_train/rejected': '-108.53', 'logps_train/chosen': '-151.99', 'loss/train': '0.67981', 'examples_per_second': '31.687', 'grad_norm': '79', 'counters/examples': 45600, 'counters/updates': 1425}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45664 examples: {'rewards_train/chosen': '-0.015734', 'rewards_train/rejected': '-0.021396', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0056622', 'logps_train/rejected': '-134.87', 'logps_train/chosen': '-145.28', 'loss/train': '0.70308', 'examples_per_second': '31.648', 'grad_norm': '84.5', 'counters/examples': 45664, 'counters/updates': 1427}
train stats after 45696 examples: {'rewards_train/chosen': '0.15738', 'rewards_train/rejected': '-0.029708', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18709', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-132.44', 'loss/train': '0.61693', 'examples_per_second': '32.8', 'grad_norm': '48.25', 'counters/examples': 45696, 'counters/updates': 1428}
train stats after 45728 examples: {'rewards_train/chosen': '0.057128', 'rewards_train/rejected': '0.073196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016068', 'logps_train/rejected': '-137.29', 'logps_train/chosen': '-178.94', 'loss/train': '0.7107', 'examples_per_second': '32.111', 'grad_norm': '91', 'counters/examples': 45728, 'counters/updates': 1429}
train stats after 45760 examples: {'rewards_train/chosen': '-0.042167', 'rewards_train/rejected': '-0.023478', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.018689', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-167.32', 'loss/train': '0.71156', 'examples_per_second': '30.845', 'grad_norm': '282', 'counters/examples': 45760, 'counters/updates': 1430}
train stats after 45792 examples: {'rewards_train/chosen': '-0.090628', 'rewards_train/rejected': '-0.024145', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066483', 'logps_train/rejected': '-96.291', 'logps_train/chosen': '-136.09', 'loss/train': '0.74224', 'examples_per_second': '31.431', 'grad_norm': '82.5', 'counters/examples': 45792, 'counters/updates': 1431}
train stats after 45824 examples: {'rewards_train/chosen': '-0.012252', 'rewards_train/rejected': '-0.042722', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03047', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-147.24', 'loss/train': '0.6913', 'examples_per_second': '31.627', 'grad_norm': '104', 'counters/examples': 45824, 'counters/updates': 1432}
train stats after 45856 examples: {'rewards_train/chosen': '0.057962', 'rewards_train/rejected': '-0.0041196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062081', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-168.8', 'loss/train': '0.67452', 'examples_per_second': '31.665', 'grad_norm': '260', 'counters/examples': 45856, 'counters/updates': 1433}
train stats after 45888 examples: {'rewards_train/chosen': '0.13768', 'rewards_train/rejected': '0.044071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093607', 'logps_train/rejected': '-143.17', 'logps_train/chosen': '-166.71', 'loss/train': '0.68688', 'examples_per_second': '30.24', 'grad_norm': '126.5', 'counters/examples': 45888, 'counters/updates': 1434}
train stats after 45920 examples: {'rewards_train/chosen': '0.049807', 'rewards_train/rejected': '-0.0081719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057979', 'logps_train/rejected': '-145.3', 'logps_train/chosen': '-186.93', 'loss/train': '0.68321', 'examples_per_second': '32.399', 'grad_norm': '90', 'counters/examples': 45920, 'counters/updates': 1435}
train stats after 45952 examples: {'rewards_train/chosen': '0.03343', 'rewards_train/rejected': '-0.0089504', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04238', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-113.83', 'loss/train': '0.67858', 'examples_per_second': '32.928', 'grad_norm': '59.25', 'counters/examples': 45952, 'counters/updates': 1436}
train stats after 45984 examples: {'rewards_train/chosen': '-0.0062032', 'rewards_train/rejected': '0.097273', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10348', 'logps_train/rejected': '-153.34', 'logps_train/chosen': '-192.95', 'loss/train': '0.7651', 'examples_per_second': '31.663', 'grad_norm': '143', 'counters/examples': 45984, 'counters/updates': 1437}
train stats after 46016 examples: {'rewards_train/chosen': '0.041977', 'rewards_train/rejected': '0.22884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.18686', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-171.56', 'loss/train': '0.85765', 'examples_per_second': '31.823', 'grad_norm': '153', 'counters/examples': 46016, 'counters/updates': 1438}
train stats after 46048 examples: {'rewards_train/chosen': '-0.045757', 'rewards_train/rejected': '-0.055952', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010195', 'logps_train/rejected': '-154.46', 'logps_train/chosen': '-134.34', 'loss/train': '0.70351', 'examples_per_second': '30.251', 'grad_norm': '93', 'counters/examples': 46048, 'counters/updates': 1439}
train stats after 46080 examples: {'rewards_train/chosen': '-0.00053214', 'rewards_train/rejected': '0.037268', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0378', 'logps_train/rejected': '-150.85', 'logps_train/chosen': '-149.49', 'loss/train': '0.72775', 'examples_per_second': '32.705', 'grad_norm': '106.5', 'counters/examples': 46080, 'counters/updates': 1440}
skipping logging after 46112 examples to avoid logging too frequently
train stats after 46144 examples: {'rewards_train/chosen': '0.037947', 'rewards_train/rejected': '0.12155', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0836', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-191.69', 'loss/train': '0.75089', 'examples_per_second': '31.679', 'grad_norm': '108.5', 'counters/examples': 46144, 'counters/updates': 1442}
train stats after 46176 examples: {'rewards_train/chosen': '0.019221', 'rewards_train/rejected': '0.03217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.012949', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-161.69', 'loss/train': '0.7168', 'examples_per_second': '32.815', 'grad_norm': '89', 'counters/examples': 46176, 'counters/updates': 1443}
train stats after 46208 examples: {'rewards_train/chosen': '0.016274', 'rewards_train/rejected': '0.04519', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028916', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-165', 'loss/train': '0.71964', 'examples_per_second': '31.693', 'grad_norm': '508', 'counters/examples': 46208, 'counters/updates': 1444}
train stats after 46240 examples: {'rewards_train/chosen': '-0.054407', 'rewards_train/rejected': '0.019396', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.073803', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-131.29', 'loss/train': '0.73897', 'examples_per_second': '31.514', 'grad_norm': '94', 'counters/examples': 46240, 'counters/updates': 1445}
train stats after 46272 examples: {'rewards_train/chosen': '0.09479', 'rewards_train/rejected': '0.0064754', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088314', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-169.33', 'loss/train': '0.65916', 'examples_per_second': '31.386', 'grad_norm': '95', 'counters/examples': 46272, 'counters/updates': 1446}
skipping logging after 46304 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '0.07687', 'rewards_train/rejected': '0.045562', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.031308', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-158.91', 'loss/train': '0.70247', 'examples_per_second': '31.844', 'grad_norm': '97', 'counters/examples': 46336, 'counters/updates': 1448}
skipping logging after 46368 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': '0.045838', 'rewards_train/rejected': '0.017692', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028146', 'logps_train/rejected': '-200.26', 'logps_train/chosen': '-158.27', 'loss/train': '0.69353', 'examples_per_second': '32.928', 'grad_norm': '120.5', 'counters/examples': 46400, 'counters/updates': 1450}
train stats after 46432 examples: {'rewards_train/chosen': '-0.028903', 'rewards_train/rejected': '0.065428', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.094331', 'logps_train/rejected': '-153.75', 'logps_train/chosen': '-131.34', 'loss/train': '0.75281', 'examples_per_second': '24.395', 'grad_norm': '97.5', 'counters/examples': 46432, 'counters/updates': 1451}
train stats after 46464 examples: {'rewards_train/chosen': '0.028354', 'rewards_train/rejected': '0.02104', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.007314', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-121.76', 'loss/train': '0.70241', 'examples_per_second': '32.298', 'grad_norm': '108', 'counters/examples': 46464, 'counters/updates': 1452}
train stats after 46496 examples: {'rewards_train/chosen': '0.050848', 'rewards_train/rejected': '-0.091616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14246', 'logps_train/rejected': '-166.9', 'logps_train/chosen': '-177.74', 'loss/train': '0.64058', 'examples_per_second': '31.619', 'grad_norm': '82.5', 'counters/examples': 46496, 'counters/updates': 1453}
train stats after 46528 examples: {'rewards_train/chosen': '0.018533', 'rewards_train/rejected': '0.058905', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.040372', 'logps_train/rejected': '-98.874', 'logps_train/chosen': '-156.15', 'loss/train': '0.72129', 'examples_per_second': '27.747', 'grad_norm': '66.5', 'counters/examples': 46528, 'counters/updates': 1454}
train stats after 46560 examples: {'rewards_train/chosen': '-0.021582', 'rewards_train/rejected': '0.011206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.032788', 'logps_train/rejected': '-163.17', 'logps_train/chosen': '-134.56', 'loss/train': '0.73607', 'examples_per_second': '30.167', 'grad_norm': '112', 'counters/examples': 46560, 'counters/updates': 1455}
train stats after 46592 examples: {'rewards_train/chosen': '0.017732', 'rewards_train/rejected': '-0.0085923', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026324', 'logps_train/rejected': '-120.52', 'logps_train/chosen': '-158.94', 'loss/train': '0.68577', 'examples_per_second': '31.658', 'grad_norm': '84.5', 'counters/examples': 46592, 'counters/updates': 1456}
train stats after 46624 examples: {'rewards_train/chosen': '0.071399', 'rewards_train/rejected': '0.014587', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056812', 'logps_train/rejected': '-112.63', 'logps_train/chosen': '-163.63', 'loss/train': '0.6768', 'examples_per_second': '31.652', 'grad_norm': '79.5', 'counters/examples': 46624, 'counters/updates': 1457}
train stats after 46656 examples: {'rewards_train/chosen': '0.021142', 'rewards_train/rejected': '-0.015112', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036254', 'logps_train/rejected': '-97.27', 'logps_train/chosen': '-123.92', 'loss/train': '0.68036', 'examples_per_second': '31.196', 'grad_norm': '92.5', 'counters/examples': 46656, 'counters/updates': 1458}
skipping logging after 46688 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '0.1074', 'rewards_train/rejected': '0.073628', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033768', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-199.14', 'loss/train': '0.69775', 'examples_per_second': '30.655', 'grad_norm': '94', 'counters/examples': 46720, 'counters/updates': 1460}
train stats after 46752 examples: {'rewards_train/chosen': '0.08502', 'rewards_train/rejected': '-0.01411', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09913', 'logps_train/rejected': '-113.54', 'logps_train/chosen': '-161.06', 'loss/train': '0.66067', 'examples_per_second': '31.652', 'grad_norm': '112.5', 'counters/examples': 46752, 'counters/updates': 1461}
train stats after 46784 examples: {'rewards_train/chosen': '-0.007376', 'rewards_train/rejected': '0.064054', 'rewards_train/accuracies': '0.21875', 'rewards_train/margins': '-0.07143', 'logps_train/rejected': '-172.12', 'logps_train/chosen': '-161.3', 'loss/train': '0.74392', 'examples_per_second': '32.623', 'grad_norm': '247', 'counters/examples': 46784, 'counters/updates': 1462}
train stats after 46816 examples: {'rewards_train/chosen': '-0.056647', 'rewards_train/rejected': '-0.031308', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.025339', 'logps_train/rejected': '-103.92', 'logps_train/chosen': '-153.6', 'loss/train': '0.71544', 'examples_per_second': '30.278', 'grad_norm': '78', 'counters/examples': 46816, 'counters/updates': 1463}
skipping logging after 46848 examples to avoid logging too frequently
train stats after 46880 examples: {'rewards_train/chosen': '0.0078214', 'rewards_train/rejected': '0.076608', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.068787', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-158.67', 'loss/train': '0.74351', 'examples_per_second': '30.199', 'grad_norm': '180', 'counters/examples': 46880, 'counters/updates': 1465}
train stats after 46912 examples: {'rewards_train/chosen': '0.091061', 'rewards_train/rejected': '0.051758', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039303', 'logps_train/rejected': '-151.4', 'logps_train/chosen': '-201.24', 'loss/train': '0.68174', 'examples_per_second': '31.628', 'grad_norm': '99', 'counters/examples': 46912, 'counters/updates': 1466}
train stats after 46944 examples: {'rewards_train/chosen': '0.0222', 'rewards_train/rejected': '0.034868', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012669', 'logps_train/rejected': '-110.5', 'logps_train/chosen': '-150.96', 'loss/train': '0.713', 'examples_per_second': '30.543', 'grad_norm': '98.5', 'counters/examples': 46944, 'counters/updates': 1467}
train stats after 46976 examples: {'rewards_train/chosen': '0.06467', 'rewards_train/rejected': '0.034491', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030179', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-160', 'loss/train': '0.68956', 'examples_per_second': '32.653', 'grad_norm': '102', 'counters/examples': 46976, 'counters/updates': 1468}
train stats after 47008 examples: {'rewards_train/chosen': '0.018119', 'rewards_train/rejected': '0.025373', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.007254', 'logps_train/rejected': '-162.59', 'logps_train/chosen': '-168.47', 'loss/train': '0.70478', 'examples_per_second': '31.866', 'grad_norm': '73.5', 'counters/examples': 47008, 'counters/updates': 1469}
train stats after 47040 examples: {'rewards_train/chosen': '0.04468', 'rewards_train/rejected': '0.0025457', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042134', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-145.74', 'loss/train': '0.68509', 'examples_per_second': '30.065', 'grad_norm': '117.5', 'counters/examples': 47040, 'counters/updates': 1470}
train stats after 47072 examples: {'rewards_train/chosen': '0.1121', 'rewards_train/rejected': '-0.0013157', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11341', 'logps_train/rejected': '-124.23', 'logps_train/chosen': '-153.95', 'loss/train': '0.65298', 'examples_per_second': '31.828', 'grad_norm': '67.5', 'counters/examples': 47072, 'counters/updates': 1471}
train stats after 47104 examples: {'rewards_train/chosen': '0.006776', 'rewards_train/rejected': '-0.031136', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037912', 'logps_train/rejected': '-139.22', 'logps_train/chosen': '-167.32', 'loss/train': '0.69227', 'examples_per_second': '31.678', 'grad_norm': '82.5', 'counters/examples': 47104, 'counters/updates': 1472}
train stats after 47136 examples: {'rewards_train/chosen': '0.034813', 'rewards_train/rejected': '0.026709', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0081044', 'logps_train/rejected': '-119.27', 'logps_train/chosen': '-153.59', 'loss/train': '0.69998', 'examples_per_second': '30.958', 'grad_norm': '77.5', 'counters/examples': 47136, 'counters/updates': 1473}
train stats after 47168 examples: {'rewards_train/chosen': '-0.028914', 'rewards_train/rejected': '0.01685', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.045764', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-138.35', 'loss/train': '0.72815', 'examples_per_second': '32.705', 'grad_norm': '76.5', 'counters/examples': 47168, 'counters/updates': 1474}
train stats after 47200 examples: {'rewards_train/chosen': '-0.010447', 'rewards_train/rejected': '0.059551', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.069998', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-114.9', 'loss/train': '0.73986', 'examples_per_second': '31.601', 'grad_norm': '163', 'counters/examples': 47200, 'counters/updates': 1475}
skipping logging after 47232 examples to avoid logging too frequently
train stats after 47264 examples: {'rewards_train/chosen': '0.086269', 'rewards_train/rejected': '0.010699', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07557', 'logps_train/rejected': '-148.24', 'logps_train/chosen': '-175.01', 'loss/train': '0.67576', 'examples_per_second': '30.63', 'grad_norm': '80', 'counters/examples': 47264, 'counters/updates': 1477}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47328 examples: {'rewards_train/chosen': '-0.046513', 'rewards_train/rejected': '0.005521', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.052034', 'logps_train/rejected': '-138.57', 'logps_train/chosen': '-182.63', 'loss/train': '0.73553', 'examples_per_second': '31.754', 'grad_norm': '126', 'counters/examples': 47328, 'counters/updates': 1479}
skipping logging after 47360 examples to avoid logging too frequently
train stats after 47392 examples: {'rewards_train/chosen': '-0.060213', 'rewards_train/rejected': '-0.064907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0046938', 'logps_train/rejected': '-117.66', 'logps_train/chosen': '-161.22', 'loss/train': '0.70092', 'examples_per_second': '31.321', 'grad_norm': '107.5', 'counters/examples': 47392, 'counters/updates': 1481}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47456 examples: {'rewards_train/chosen': '-0.010367', 'rewards_train/rejected': '0.008761', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019128', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-153.86', 'loss/train': '0.71089', 'examples_per_second': '31.65', 'grad_norm': '90', 'counters/examples': 47456, 'counters/updates': 1483}
train stats after 47488 examples: {'rewards_train/chosen': '-0.039177', 'rewards_train/rejected': '-0.037443', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0017334', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-185.34', 'loss/train': '0.70473', 'examples_per_second': '24.836', 'grad_norm': '112', 'counters/examples': 47488, 'counters/updates': 1484}
train stats after 47520 examples: {'rewards_train/chosen': '0.0198', 'rewards_train/rejected': '-0.020849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040649', 'logps_train/rejected': '-114.45', 'logps_train/chosen': '-143.11', 'loss/train': '0.69649', 'examples_per_second': '31.77', 'grad_norm': '74', 'counters/examples': 47520, 'counters/updates': 1485}
train stats after 47552 examples: {'rewards_train/chosen': '-0.040334', 'rewards_train/rejected': '0.050109', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.090444', 'logps_train/rejected': '-132.44', 'logps_train/chosen': '-157.06', 'loss/train': '0.75349', 'examples_per_second': '31.622', 'grad_norm': '96.5', 'counters/examples': 47552, 'counters/updates': 1486}
train stats after 47584 examples: {'rewards_train/chosen': '0.058303', 'rewards_train/rejected': '-0.025451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083754', 'logps_train/rejected': '-145.86', 'logps_train/chosen': '-165.69', 'loss/train': '0.67265', 'examples_per_second': '30.118', 'grad_norm': '181', 'counters/examples': 47584, 'counters/updates': 1487}
skipping logging after 47616 examples to avoid logging too frequently
train stats after 47648 examples: {'rewards_train/chosen': '0.063225', 'rewards_train/rejected': '-0.0044759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067701', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-122.31', 'loss/train': '0.66744', 'examples_per_second': '32.682', 'grad_norm': '105.5', 'counters/examples': 47648, 'counters/updates': 1489}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47712 examples: {'rewards_train/chosen': '0.012414', 'rewards_train/rejected': '-0.045765', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05818', 'logps_train/rejected': '-120.69', 'logps_train/chosen': '-143.5', 'loss/train': '0.68044', 'examples_per_second': '38.494', 'grad_norm': '86.5', 'counters/examples': 47712, 'counters/updates': 1491}
train stats after 47744 examples: {'rewards_train/chosen': '0.11272', 'rewards_train/rejected': '0.037972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074752', 'logps_train/rejected': '-130.89', 'logps_train/chosen': '-129.7', 'loss/train': '0.67219', 'examples_per_second': '31.75', 'grad_norm': '89', 'counters/examples': 47744, 'counters/updates': 1492}
skipping logging after 47776 examples to avoid logging too frequently
train stats after 47808 examples: {'rewards_train/chosen': '0.032477', 'rewards_train/rejected': '0.05515', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.022673', 'logps_train/rejected': '-182.78', 'logps_train/chosen': '-160.39', 'loss/train': '0.72174', 'examples_per_second': '30.418', 'grad_norm': '221', 'counters/examples': 47808, 'counters/updates': 1494}
train stats after 47840 examples: {'rewards_train/chosen': '0.0032164', 'rewards_train/rejected': '-0.049427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052643', 'logps_train/rejected': '-150.9', 'logps_train/chosen': '-126.97', 'loss/train': '0.6841', 'examples_per_second': '29.96', 'grad_norm': '148', 'counters/examples': 47840, 'counters/updates': 1495}
skipping logging after 47872 examples to avoid logging too frequently
train stats after 47904 examples: {'rewards_train/chosen': '0.13167', 'rewards_train/rejected': '0.012958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11871', 'logps_train/rejected': '-155.59', 'logps_train/chosen': '-172.23', 'loss/train': '0.64805', 'examples_per_second': '30.241', 'grad_norm': '86.5', 'counters/examples': 47904, 'counters/updates': 1497}
train stats after 47936 examples: {'rewards_train/chosen': '0.042698', 'rewards_train/rejected': '-0.034982', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07768', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-147.21', 'loss/train': '0.66569', 'examples_per_second': '31.613', 'grad_norm': '71.5', 'counters/examples': 47936, 'counters/updates': 1498}
train stats after 47968 examples: {'rewards_train/chosen': '-0.016806', 'rewards_train/rejected': '-0.043052', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026247', 'logps_train/rejected': '-144.92', 'logps_train/chosen': '-170.75', 'loss/train': '0.69', 'examples_per_second': '30.545', 'grad_norm': '132', 'counters/examples': 47968, 'counters/updates': 1499}
skipping logging after 48000 examples to avoid logging too frequently
Running evaluation after 48000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.89it/s]
eval after 48000: {'rewards_eval/chosen': '0.032293', 'rewards_eval/rejected': '-0.0064918', 'rewards_eval/accuracies': '0.55078', 'rewards_eval/margins': '0.038784', 'logps_eval/rejected': '-128.21', 'logps_eval/chosen': '-150.68', 'loss/eval': '0.68599'}
skipping save for non epoch
train stats after 48032 examples: {'rewards_train/chosen': '-0.048266', 'rewards_train/rejected': '-0.042627', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0056393', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-141.8', 'loss/train': '0.70575', 'examples_per_second': '38.325', 'grad_norm': '78', 'counters/examples': 48032, 'counters/updates': 1501}
train stats after 48064 examples: {'rewards_train/chosen': '0.045511', 'rewards_train/rejected': '0.015969', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029542', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-183.35', 'loss/train': '0.69586', 'examples_per_second': '32.244', 'grad_norm': '243', 'counters/examples': 48064, 'counters/updates': 1502}
skipping logging after 48096 examples to avoid logging too frequently
train stats after 48128 examples: {'rewards_train/chosen': '0.041662', 'rewards_train/rejected': '0.20082', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.15916', 'logps_train/rejected': '-109.27', 'logps_train/chosen': '-140.65', 'loss/train': '0.79754', 'examples_per_second': '31.743', 'grad_norm': '154', 'counters/examples': 48128, 'counters/updates': 1504}
train stats after 48160 examples: {'rewards_train/chosen': '-0.0089757', 'rewards_train/rejected': '-0.031484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022508', 'logps_train/rejected': '-110.32', 'logps_train/chosen': '-143.8', 'loss/train': '0.68741', 'examples_per_second': '30.979', 'grad_norm': '60.5', 'counters/examples': 48160, 'counters/updates': 1505}
train stats after 48192 examples: {'rewards_train/chosen': '-0.028006', 'rewards_train/rejected': '-0.074005', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045999', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-121.5', 'loss/train': '0.68266', 'examples_per_second': '31.154', 'grad_norm': '62.75', 'counters/examples': 48192, 'counters/updates': 1506}
train stats after 48224 examples: {'rewards_train/chosen': '-0.019165', 'rewards_train/rejected': '0.049356', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.068521', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-117.44', 'loss/train': '0.74065', 'examples_per_second': '31.007', 'grad_norm': '74.5', 'counters/examples': 48224, 'counters/updates': 1507}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48288 examples: {'rewards_train/chosen': '-0.024236', 'rewards_train/rejected': '-0.013494', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010742', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-135.76', 'loss/train': '0.71208', 'examples_per_second': '31.174', 'grad_norm': '85.5', 'counters/examples': 48288, 'counters/updates': 1509}
train stats after 48320 examples: {'rewards_train/chosen': '0.012969', 'rewards_train/rejected': '0.03201', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019041', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-143.47', 'loss/train': '0.71338', 'examples_per_second': '30.62', 'grad_norm': '87', 'counters/examples': 48320, 'counters/updates': 1510}
train stats after 48352 examples: {'rewards_train/chosen': '0.092145', 'rewards_train/rejected': '0.040097', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.052048', 'logps_train/rejected': '-80.795', 'logps_train/chosen': '-147.88', 'loss/train': '0.68061', 'examples_per_second': '31.275', 'grad_norm': '278', 'counters/examples': 48352, 'counters/updates': 1511}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48416 examples: {'rewards_train/chosen': '-0.0083483', 'rewards_train/rejected': '-0.031475', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023127', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-134.03', 'loss/train': '0.68999', 'examples_per_second': '32.363', 'grad_norm': '81', 'counters/examples': 48416, 'counters/updates': 1513}
train stats after 48448 examples: {'rewards_train/chosen': '-0.063933', 'rewards_train/rejected': '0.0063029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.070236', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-169.39', 'loss/train': '0.77831', 'examples_per_second': '31.583', 'grad_norm': '133', 'counters/examples': 48448, 'counters/updates': 1514}
train stats after 48480 examples: {'rewards_train/chosen': '-0.0064854', 'rewards_train/rejected': '-0.068147', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061661', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-176.56', 'loss/train': '0.67363', 'examples_per_second': '31.578', 'grad_norm': '181', 'counters/examples': 48480, 'counters/updates': 1515}
train stats after 48512 examples: {'rewards_train/chosen': '-0.034457', 'rewards_train/rejected': '-0.0082492', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026208', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-133.71', 'loss/train': '0.71415', 'examples_per_second': '31.318', 'grad_norm': '158', 'counters/examples': 48512, 'counters/updates': 1516}
train stats after 48544 examples: {'rewards_train/chosen': '-0.0043806', 'rewards_train/rejected': '-0.035246', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030865', 'logps_train/rejected': '-106.86', 'logps_train/chosen': '-145.17', 'loss/train': '0.68693', 'examples_per_second': '33.003', 'grad_norm': '108', 'counters/examples': 48544, 'counters/updates': 1517}
train stats after 48576 examples: {'rewards_train/chosen': '-0.025781', 'rewards_train/rejected': '-0.11797', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092194', 'logps_train/rejected': '-164.68', 'logps_train/chosen': '-159.41', 'loss/train': '0.65377', 'examples_per_second': '31.57', 'grad_norm': '79.5', 'counters/examples': 48576, 'counters/updates': 1518}
skipping logging after 48608 examples to avoid logging too frequently
train stats after 48640 examples: {'rewards_train/chosen': '-0.029371', 'rewards_train/rejected': '0.048176', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077547', 'logps_train/rejected': '-148', 'logps_train/chosen': '-128.62', 'loss/train': '0.75264', 'examples_per_second': '32.066', 'grad_norm': '792', 'counters/examples': 48640, 'counters/updates': 1520}
train stats after 48672 examples: {'rewards_train/chosen': '0.017708', 'rewards_train/rejected': '-0.017601', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035309', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-144.53', 'loss/train': '0.6914', 'examples_per_second': '31.395', 'grad_norm': '105.5', 'counters/examples': 48672, 'counters/updates': 1521}
train stats after 48704 examples: {'rewards_train/chosen': '0.039287', 'rewards_train/rejected': '0.013878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025409', 'logps_train/rejected': '-98.103', 'logps_train/chosen': '-136.92', 'loss/train': '0.68965', 'examples_per_second': '31.568', 'grad_norm': '98', 'counters/examples': 48704, 'counters/updates': 1522}
skipping logging after 48736 examples to avoid logging too frequently
train stats after 48768 examples: {'rewards_train/chosen': '-0.013026', 'rewards_train/rejected': '0.019338', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032364', 'logps_train/rejected': '-155.08', 'logps_train/chosen': '-172.01', 'loss/train': '0.74478', 'examples_per_second': '32.274', 'grad_norm': '167', 'counters/examples': 48768, 'counters/updates': 1524}
train stats after 48800 examples: {'rewards_train/chosen': '-0.054832', 'rewards_train/rejected': '0.043167', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.097998', 'logps_train/rejected': '-152.42', 'logps_train/chosen': '-179.21', 'loss/train': '0.75464', 'examples_per_second': '31.59', 'grad_norm': '100.5', 'counters/examples': 48800, 'counters/updates': 1525}
train stats after 48832 examples: {'rewards_train/chosen': '-0.01532', 'rewards_train/rejected': '-0.012963', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0023562', 'logps_train/rejected': '-165.32', 'logps_train/chosen': '-175.5', 'loss/train': '0.71687', 'examples_per_second': '31.503', 'grad_norm': '80', 'counters/examples': 48832, 'counters/updates': 1526}
train stats after 48864 examples: {'rewards_train/chosen': '0.029525', 'rewards_train/rejected': '-0.045972', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075497', 'logps_train/rejected': '-146.68', 'logps_train/chosen': '-145.36', 'loss/train': '0.66507', 'examples_per_second': '31.684', 'grad_norm': '72', 'counters/examples': 48864, 'counters/updates': 1527}
train stats after 48896 examples: {'rewards_train/chosen': '0.069278', 'rewards_train/rejected': '-0.002567', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071845', 'logps_train/rejected': '-155.54', 'logps_train/chosen': '-166.22', 'loss/train': '0.67643', 'examples_per_second': '31.563', 'grad_norm': '102', 'counters/examples': 48896, 'counters/updates': 1528}
train stats after 48928 examples: {'rewards_train/chosen': '-0.0082098', 'rewards_train/rejected': '0.0037951', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012005', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-191.48', 'loss/train': '0.7134', 'examples_per_second': '31.5', 'grad_norm': '205', 'counters/examples': 48928, 'counters/updates': 1529}
train stats after 48960 examples: {'rewards_train/chosen': '-0.019954', 'rewards_train/rejected': '8.4143e-06', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019963', 'logps_train/rejected': '-103.18', 'logps_train/chosen': '-162.94', 'loss/train': '0.71367', 'examples_per_second': '32.599', 'grad_norm': '185', 'counters/examples': 48960, 'counters/updates': 1530}
train stats after 48992 examples: {'rewards_train/chosen': '-0.025719', 'rewards_train/rejected': '0.034796', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.060515', 'logps_train/rejected': '-153.99', 'logps_train/chosen': '-115.93', 'loss/train': '0.74295', 'examples_per_second': '31.52', 'grad_norm': '126.5', 'counters/examples': 48992, 'counters/updates': 1531}
train stats after 49024 examples: {'rewards_train/chosen': '0.055053', 'rewards_train/rejected': '0.029165', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025888', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-137.27', 'loss/train': '0.68889', 'examples_per_second': '31.831', 'grad_norm': '74', 'counters/examples': 49024, 'counters/updates': 1532}
skipping logging after 49056 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '0.0064483', 'rewards_train/rejected': '0.0012328', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0052155', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-137.88', 'loss/train': '0.69951', 'examples_per_second': '31.557', 'grad_norm': '66.5', 'counters/examples': 49088, 'counters/updates': 1534}
train stats after 49120 examples: {'rewards_train/chosen': '0.041698', 'rewards_train/rejected': '0.029557', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012141', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-154.99', 'loss/train': '0.71719', 'examples_per_second': '31.827', 'grad_norm': '264', 'counters/examples': 49120, 'counters/updates': 1535}
train stats after 49152 examples: {'rewards_train/chosen': '-0.025755', 'rewards_train/rejected': '-0.024044', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '-0.0017115', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-163.88', 'loss/train': '0.72621', 'examples_per_second': '31.425', 'grad_norm': '148', 'counters/examples': 49152, 'counters/updates': 1536}
train stats after 49184 examples: {'rewards_train/chosen': '0.058422', 'rewards_train/rejected': '0.06253', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0041078', 'logps_train/rejected': '-171.04', 'logps_train/chosen': '-189.46', 'loss/train': '0.72806', 'examples_per_second': '31.54', 'grad_norm': '104', 'counters/examples': 49184, 'counters/updates': 1537}
train stats after 49216 examples: {'rewards_train/chosen': '-0.02206', 'rewards_train/rejected': '-0.021844', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00021572', 'logps_train/rejected': '-143.41', 'logps_train/chosen': '-189.86', 'loss/train': '0.71006', 'examples_per_second': '31.199', 'grad_norm': '139', 'counters/examples': 49216, 'counters/updates': 1538}
train stats after 49248 examples: {'rewards_train/chosen': '0.015386', 'rewards_train/rejected': '0.0081746', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.007211', 'logps_train/rejected': '-101.73', 'logps_train/chosen': '-172.95', 'loss/train': '0.69687', 'examples_per_second': '32.962', 'grad_norm': '153', 'counters/examples': 49248, 'counters/updates': 1539}
train stats after 49280 examples: {'rewards_train/chosen': '0.0099179', 'rewards_train/rejected': '-0.010326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020243', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-137.5', 'loss/train': '0.69598', 'examples_per_second': '31.217', 'grad_norm': '95.5', 'counters/examples': 49280, 'counters/updates': 1540}
train stats after 49312 examples: {'rewards_train/chosen': '0.01265', 'rewards_train/rejected': '-0.032036', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044687', 'logps_train/rejected': '-138.84', 'logps_train/chosen': '-154.51', 'loss/train': '0.67771', 'examples_per_second': '31.579', 'grad_norm': '86', 'counters/examples': 49312, 'counters/updates': 1541}
train stats after 49344 examples: {'rewards_train/chosen': '0.031394', 'rewards_train/rejected': '0.013001', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018393', 'logps_train/rejected': '-111.06', 'logps_train/chosen': '-160.39', 'loss/train': '0.69833', 'examples_per_second': '30.441', 'grad_norm': '160', 'counters/examples': 49344, 'counters/updates': 1542}
train stats after 49376 examples: {'rewards_train/chosen': '0.072214', 'rewards_train/rejected': '-0.044548', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11676', 'logps_train/rejected': '-164.66', 'logps_train/chosen': '-181.99', 'loss/train': '0.65695', 'examples_per_second': '32.433', 'grad_norm': '68', 'counters/examples': 49376, 'counters/updates': 1543}
train stats after 49408 examples: {'rewards_train/chosen': '-0.020907', 'rewards_train/rejected': '-0.050148', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029241', 'logps_train/rejected': '-114.3', 'logps_train/chosen': '-160.89', 'loss/train': '0.6905', 'examples_per_second': '31.466', 'grad_norm': '84', 'counters/examples': 49408, 'counters/updates': 1544}
train stats after 49440 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '-0.014527', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15031', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-147.76', 'loss/train': '0.62807', 'examples_per_second': '32.505', 'grad_norm': '49.5', 'counters/examples': 49440, 'counters/updates': 1545}
skipping logging after 49472 examples to avoid logging too frequently
train stats after 49504 examples: {'rewards_train/chosen': '0.059392', 'rewards_train/rejected': '-0.044714', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10411', 'logps_train/rejected': '-190.19', 'logps_train/chosen': '-138.92', 'loss/train': '0.66239', 'examples_per_second': '31.574', 'grad_norm': '116.5', 'counters/examples': 49504, 'counters/updates': 1547}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49568 examples: {'rewards_train/chosen': '-0.011302', 'rewards_train/rejected': '-0.0079044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0033973', 'logps_train/rejected': '-156.97', 'logps_train/chosen': '-167.75', 'loss/train': '0.70449', 'examples_per_second': '31.544', 'grad_norm': '78', 'counters/examples': 49568, 'counters/updates': 1549}
train stats after 49600 examples: {'rewards_train/chosen': '0.018917', 'rewards_train/rejected': '0.031955', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013038', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-148.62', 'loss/train': '0.7216', 'examples_per_second': '30.178', 'grad_norm': '51', 'counters/examples': 49600, 'counters/updates': 1550}
skipping logging after 49632 examples to avoid logging too frequently
train stats after 49664 examples: {'rewards_train/chosen': '0.020034', 'rewards_train/rejected': '-0.043074', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063108', 'logps_train/rejected': '-114.88', 'logps_train/chosen': '-135.67', 'loss/train': '0.67008', 'examples_per_second': '34.371', 'grad_norm': '83.5', 'counters/examples': 49664, 'counters/updates': 1552}
skipping logging after 49696 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '-0.02377', 'rewards_train/rejected': '0.0034347', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027205', 'logps_train/rejected': '-152.48', 'logps_train/chosen': '-133.7', 'loss/train': '0.72145', 'examples_per_second': '31.614', 'grad_norm': '101', 'counters/examples': 49728, 'counters/updates': 1554}
skipping logging after 49760 examples to avoid logging too frequently
train stats after 49792 examples: {'rewards_train/chosen': '-0.026333', 'rewards_train/rejected': '-0.0062724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020061', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-176.42', 'loss/train': '0.72039', 'examples_per_second': '31.577', 'grad_norm': '126', 'counters/examples': 49792, 'counters/updates': 1556}
train stats after 49824 examples: {'rewards_train/chosen': '0.015523', 'rewards_train/rejected': '-0.034791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050314', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-143.24', 'loss/train': '0.6815', 'examples_per_second': '31.005', 'grad_norm': '77.5', 'counters/examples': 49824, 'counters/updates': 1557}
skipping logging after 49856 examples to avoid logging too frequently
train stats after 49888 examples: {'rewards_train/chosen': '0.00038855', 'rewards_train/rejected': '0.010398', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01001', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-144.62', 'loss/train': '0.70773', 'examples_per_second': '32.102', 'grad_norm': '85', 'counters/examples': 49888, 'counters/updates': 1559}
train stats after 49920 examples: {'rewards_train/chosen': '0.00202', 'rewards_train/rejected': '-0.048391', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050411', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-165.44', 'loss/train': '0.68552', 'examples_per_second': '30.019', 'grad_norm': '75.5', 'counters/examples': 49920, 'counters/updates': 1560}
train stats after 49952 examples: {'rewards_train/chosen': '0.044717', 'rewards_train/rejected': '0.081823', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037106', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-137.87', 'loss/train': '0.72394', 'examples_per_second': '31.743', 'grad_norm': '71', 'counters/examples': 49952, 'counters/updates': 1561}
train stats after 49984 examples: {'rewards_train/chosen': '0.0094422', 'rewards_train/rejected': '-0.0040897', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013532', 'logps_train/rejected': '-106.3', 'logps_train/chosen': '-142.61', 'loss/train': '0.69853', 'examples_per_second': '31.523', 'grad_norm': '74.5', 'counters/examples': 49984, 'counters/updates': 1562}
train stats after 50016 examples: {'rewards_train/chosen': '0.040821', 'rewards_train/rejected': '0.053867', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013047', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-117.7', 'loss/train': '0.71288', 'examples_per_second': '31.259', 'grad_norm': '83', 'counters/examples': 50016, 'counters/updates': 1563}
train stats after 50048 examples: {'rewards_train/chosen': '-0.011528', 'rewards_train/rejected': '0.010798', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022326', 'logps_train/rejected': '-147.46', 'logps_train/chosen': '-157.37', 'loss/train': '0.71726', 'examples_per_second': '31.569', 'grad_norm': '96', 'counters/examples': 50048, 'counters/updates': 1564}
train stats after 50080 examples: {'rewards_train/chosen': '-0.015989', 'rewards_train/rejected': '0.0046796', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020668', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-164.07', 'loss/train': '0.72201', 'examples_per_second': '31.554', 'grad_norm': '94.5', 'counters/examples': 50080, 'counters/updates': 1565}
train stats after 50112 examples: {'rewards_train/chosen': '0.058275', 'rewards_train/rejected': '0.042322', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015952', 'logps_train/rejected': '-149.26', 'logps_train/chosen': '-174.61', 'loss/train': '0.69753', 'examples_per_second': '30.054', 'grad_norm': '132', 'counters/examples': 50112, 'counters/updates': 1566}
train stats after 50144 examples: {'rewards_train/chosen': '0.047577', 'rewards_train/rejected': '0.030162', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017416', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-126.62', 'loss/train': '0.69784', 'examples_per_second': '31.561', 'grad_norm': '230', 'counters/examples': 50144, 'counters/updates': 1567}
train stats after 50176 examples: {'rewards_train/chosen': '0.046488', 'rewards_train/rejected': '-0.053079', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099566', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-138.42', 'loss/train': '0.65057', 'examples_per_second': '32.695', 'grad_norm': '93', 'counters/examples': 50176, 'counters/updates': 1568}
skipping logging after 50208 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '-0.0065288', 'rewards_train/rejected': '0.037977', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044506', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-132.96', 'loss/train': '0.72804', 'examples_per_second': '31.771', 'grad_norm': '87', 'counters/examples': 50240, 'counters/updates': 1570}
skipping logging after 50272 examples to avoid logging too frequently
train stats after 50304 examples: {'rewards_train/chosen': '0.025875', 'rewards_train/rejected': '0.05361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027735', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-166.21', 'loss/train': '0.7167', 'examples_per_second': '31.226', 'grad_norm': '90', 'counters/examples': 50304, 'counters/updates': 1572}
train stats after 50336 examples: {'rewards_train/chosen': '0.04349', 'rewards_train/rejected': '0.013464', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030026', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-176.65', 'loss/train': '0.69195', 'examples_per_second': '31.245', 'grad_norm': '110', 'counters/examples': 50336, 'counters/updates': 1573}
skipping logging after 50368 examples to avoid logging too frequently
train stats after 50400 examples: {'rewards_train/chosen': '-0.071051', 'rewards_train/rejected': '-0.067942', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0031091', 'logps_train/rejected': '-112.31', 'logps_train/chosen': '-128.21', 'loss/train': '0.71182', 'examples_per_second': '30.993', 'grad_norm': '99', 'counters/examples': 50400, 'counters/updates': 1575}
train stats after 50432 examples: {'rewards_train/chosen': '0.0075541', 'rewards_train/rejected': '-0.0087489', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016303', 'logps_train/rejected': '-137.51', 'logps_train/chosen': '-162.06', 'loss/train': '0.6902', 'examples_per_second': '31.597', 'grad_norm': '62.75', 'counters/examples': 50432, 'counters/updates': 1576}
train stats after 50464 examples: {'rewards_train/chosen': '0.029948', 'rewards_train/rejected': '0.10363', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.073683', 'logps_train/rejected': '-155', 'logps_train/chosen': '-145.26', 'loss/train': '0.75142', 'examples_per_second': '31.314', 'grad_norm': '87.5', 'counters/examples': 50464, 'counters/updates': 1577}
train stats after 50496 examples: {'rewards_train/chosen': '0.060803', 'rewards_train/rejected': '0.034187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026616', 'logps_train/rejected': '-177.01', 'logps_train/chosen': '-182.33', 'loss/train': '0.69544', 'examples_per_second': '31.505', 'grad_norm': '78.5', 'counters/examples': 50496, 'counters/updates': 1578}
train stats after 50528 examples: {'rewards_train/chosen': '0.10844', 'rewards_train/rejected': '0.036118', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072319', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-141.95', 'loss/train': '0.66658', 'examples_per_second': '32.674', 'grad_norm': '68', 'counters/examples': 50528, 'counters/updates': 1579}
train stats after 50560 examples: {'rewards_train/chosen': '0.039076', 'rewards_train/rejected': '0.034182', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0048944', 'logps_train/rejected': '-104.98', 'logps_train/chosen': '-112.45', 'loss/train': '0.6983', 'examples_per_second': '32.542', 'grad_norm': '69.5', 'counters/examples': 50560, 'counters/updates': 1580}
skipping logging after 50592 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '0.071108', 'rewards_train/rejected': '0.019755', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051353', 'logps_train/rejected': '-123', 'logps_train/chosen': '-131.61', 'loss/train': '0.68453', 'examples_per_second': '32.446', 'grad_norm': '52', 'counters/examples': 50624, 'counters/updates': 1582}
train stats after 50656 examples: {'rewards_train/chosen': '0.059841', 'rewards_train/rejected': '0.064575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0047344', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-135.69', 'loss/train': '0.70417', 'examples_per_second': '30.617', 'grad_norm': '64.5', 'counters/examples': 50656, 'counters/updates': 1583}
train stats after 50688 examples: {'rewards_train/chosen': '0.11006', 'rewards_train/rejected': '0.0040328', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10603', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-150.71', 'loss/train': '0.65383', 'examples_per_second': '31.614', 'grad_norm': '84', 'counters/examples': 50688, 'counters/updates': 1584}
train stats after 50720 examples: {'rewards_train/chosen': '-0.023829', 'rewards_train/rejected': '-0.028915', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0050856', 'logps_train/rejected': '-160.72', 'logps_train/chosen': '-156.21', 'loss/train': '0.7147', 'examples_per_second': '31.682', 'grad_norm': '66.5', 'counters/examples': 50720, 'counters/updates': 1585}
train stats after 50752 examples: {'rewards_train/chosen': '0.021677', 'rewards_train/rejected': '0.012466', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0092114', 'logps_train/rejected': '-142.84', 'logps_train/chosen': '-135.27', 'loss/train': '0.69823', 'examples_per_second': '31.463', 'grad_norm': '87', 'counters/examples': 50752, 'counters/updates': 1586}
train stats after 50784 examples: {'rewards_train/chosen': '0.0022108', 'rewards_train/rejected': '-0.02618', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028391', 'logps_train/rejected': '-148.73', 'logps_train/chosen': '-146.69', 'loss/train': '0.70832', 'examples_per_second': '31.205', 'grad_norm': '248', 'counters/examples': 50784, 'counters/updates': 1587}
train stats after 50816 examples: {'rewards_train/chosen': '0.034816', 'rewards_train/rejected': '-0.013778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048594', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-166.13', 'loss/train': '0.68773', 'examples_per_second': '31.207', 'grad_norm': '148', 'counters/examples': 50816, 'counters/updates': 1588}
train stats after 50848 examples: {'rewards_train/chosen': '0.023022', 'rewards_train/rejected': '-0.044131', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067153', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-174.03', 'loss/train': '0.66937', 'examples_per_second': '31.585', 'grad_norm': '242', 'counters/examples': 50848, 'counters/updates': 1589}
train stats after 50880 examples: {'rewards_train/chosen': '0.090327', 'rewards_train/rejected': '0.0039989', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086328', 'logps_train/rejected': '-145.77', 'logps_train/chosen': '-150.68', 'loss/train': '0.66434', 'examples_per_second': '30.785', 'grad_norm': '69', 'counters/examples': 50880, 'counters/updates': 1590}
train stats after 50912 examples: {'rewards_train/chosen': '0.044926', 'rewards_train/rejected': '-0.0069569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051883', 'logps_train/rejected': '-146.41', 'logps_train/chosen': '-141.36', 'loss/train': '0.67634', 'examples_per_second': '30.444', 'grad_norm': '67.5', 'counters/examples': 50912, 'counters/updates': 1591}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 50976 examples: {'rewards_train/chosen': '-0.047631', 'rewards_train/rejected': '0.046989', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.09462', 'logps_train/rejected': '-118.8', 'logps_train/chosen': '-202.07', 'loss/train': '0.7514', 'examples_per_second': '31.697', 'grad_norm': '83.5', 'counters/examples': 50976, 'counters/updates': 1593}
skipping logging after 51008 examples to avoid logging too frequently
train stats after 51040 examples: {'rewards_train/chosen': '0.053784', 'rewards_train/rejected': '0.12023', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066444', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-164.38', 'loss/train': '0.74571', 'examples_per_second': '31.599', 'grad_norm': '81', 'counters/examples': 51040, 'counters/updates': 1595}
train stats after 51072 examples: {'rewards_train/chosen': '-0.0081694', 'rewards_train/rejected': '0.043055', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.051225', 'logps_train/rejected': '-150.73', 'logps_train/chosen': '-159.06', 'loss/train': '0.72889', 'examples_per_second': '31.166', 'grad_norm': '180', 'counters/examples': 51072, 'counters/updates': 1596}
train stats after 51104 examples: {'rewards_train/chosen': '0.04172', 'rewards_train/rejected': '-0.062744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10446', 'logps_train/rejected': '-162.09', 'logps_train/chosen': '-198.63', 'loss/train': '0.65917', 'examples_per_second': '30.664', 'grad_norm': '78.5', 'counters/examples': 51104, 'counters/updates': 1597}
train stats after 51136 examples: {'rewards_train/chosen': '0.043692', 'rewards_train/rejected': '0.039912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0037794', 'logps_train/rejected': '-100.49', 'logps_train/chosen': '-122.7', 'loss/train': '0.69956', 'examples_per_second': '31.634', 'grad_norm': '64', 'counters/examples': 51136, 'counters/updates': 1598}
train stats after 51168 examples: {'rewards_train/chosen': '-0.060132', 'rewards_train/rejected': '0.12466', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.1848', 'logps_train/rejected': '-153.06', 'logps_train/chosen': '-172.01', 'loss/train': '0.89343', 'examples_per_second': '31.005', 'grad_norm': '234', 'counters/examples': 51168, 'counters/updates': 1599}
train stats after 51200 examples: {'rewards_train/chosen': '0.0034257', 'rewards_train/rejected': '-0.060697', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064123', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-148.89', 'loss/train': '0.67056', 'examples_per_second': '30.567', 'grad_norm': '101.5', 'counters/examples': 51200, 'counters/updates': 1600}
train stats after 51232 examples: {'rewards_train/chosen': '0.017924', 'rewards_train/rejected': '0.040007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022083', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-160.69', 'loss/train': '0.72927', 'examples_per_second': '31.565', 'grad_norm': '144', 'counters/examples': 51232, 'counters/updates': 1601}
skipping logging after 51264 examples to avoid logging too frequently
train stats after 51296 examples: {'rewards_train/chosen': '-0.018604', 'rewards_train/rejected': '-0.04584', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027236', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-149.68', 'loss/train': '0.69947', 'examples_per_second': '31.614', 'grad_norm': '68', 'counters/examples': 51296, 'counters/updates': 1603}
train stats after 51328 examples: {'rewards_train/chosen': '0.0012019', 'rewards_train/rejected': '0.015933', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014732', 'logps_train/rejected': '-151.78', 'logps_train/chosen': '-152.11', 'loss/train': '0.71388', 'examples_per_second': '30.214', 'grad_norm': '83.5', 'counters/examples': 51328, 'counters/updates': 1604}
train stats after 51360 examples: {'rewards_train/chosen': '0.10545', 'rewards_train/rejected': '-0.040272', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14573', 'logps_train/rejected': '-144.4', 'logps_train/chosen': '-184.72', 'loss/train': '0.6351', 'examples_per_second': '30.085', 'grad_norm': '90', 'counters/examples': 51360, 'counters/updates': 1605}
train stats after 51392 examples: {'rewards_train/chosen': '0.014604', 'rewards_train/rejected': '0.017158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0025533', 'logps_train/rejected': '-106.84', 'logps_train/chosen': '-140.51', 'loss/train': '0.70499', 'examples_per_second': '30.259', 'grad_norm': '78.5', 'counters/examples': 51392, 'counters/updates': 1606}
train stats after 51424 examples: {'rewards_train/chosen': '0.046418', 'rewards_train/rejected': '0.015006', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031412', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-124.53', 'loss/train': '0.6882', 'examples_per_second': '32.416', 'grad_norm': '87', 'counters/examples': 51424, 'counters/updates': 1607}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51488 examples: {'rewards_train/chosen': '-0.049879', 'rewards_train/rejected': '-0.030743', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019136', 'logps_train/rejected': '-86.233', 'logps_train/chosen': '-173.93', 'loss/train': '0.70773', 'examples_per_second': '30.42', 'grad_norm': '134', 'counters/examples': 51488, 'counters/updates': 1609}
train stats after 51520 examples: {'rewards_train/chosen': '0.05475', 'rewards_train/rejected': '-0.056543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11129', 'logps_train/rejected': '-167.47', 'logps_train/chosen': '-144.35', 'loss/train': '0.64723', 'examples_per_second': '31.441', 'grad_norm': '59.25', 'counters/examples': 51520, 'counters/updates': 1610}
train stats after 51552 examples: {'rewards_train/chosen': '-0.016815', 'rewards_train/rejected': '0.047579', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.064393', 'logps_train/rejected': '-137.46', 'logps_train/chosen': '-128.17', 'loss/train': '0.73582', 'examples_per_second': '30.104', 'grad_norm': '92', 'counters/examples': 51552, 'counters/updates': 1611}
train stats after 51584 examples: {'rewards_train/chosen': '0.010418', 'rewards_train/rejected': '-0.023363', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033781', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-149.52', 'loss/train': '0.68681', 'examples_per_second': '31.742', 'grad_norm': '67', 'counters/examples': 51584, 'counters/updates': 1612}
train stats after 51616 examples: {'rewards_train/chosen': '0.083726', 'rewards_train/rejected': '-0.044464', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12819', 'logps_train/rejected': '-111.92', 'logps_train/chosen': '-149.8', 'loss/train': '0.64315', 'examples_per_second': '32.105', 'grad_norm': '68.5', 'counters/examples': 51616, 'counters/updates': 1613}
train stats after 51648 examples: {'rewards_train/chosen': '-0.044544', 'rewards_train/rejected': '0.045716', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.09026', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-163.96', 'loss/train': '0.74991', 'examples_per_second': '31.542', 'grad_norm': '131', 'counters/examples': 51648, 'counters/updates': 1614}
train stats after 51680 examples: {'rewards_train/chosen': '-0.027415', 'rewards_train/rejected': '-0.042989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015574', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-173.09', 'loss/train': '0.70551', 'examples_per_second': '30.954', 'grad_norm': '115', 'counters/examples': 51680, 'counters/updates': 1615}
train stats after 51712 examples: {'rewards_train/chosen': '-0.036622', 'rewards_train/rejected': '-0.038324', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0017022', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-185', 'loss/train': '0.7009', 'examples_per_second': '31.069', 'grad_norm': '87.5', 'counters/examples': 51712, 'counters/updates': 1616}
train stats after 51744 examples: {'rewards_train/chosen': '0.038653', 'rewards_train/rejected': '-0.016733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055385', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-136.63', 'loss/train': '0.67001', 'examples_per_second': '31.513', 'grad_norm': '105', 'counters/examples': 51744, 'counters/updates': 1617}
train stats after 51776 examples: {'rewards_train/chosen': '0.088824', 'rewards_train/rejected': '-0.0034004', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.092224', 'logps_train/rejected': '-148.53', 'logps_train/chosen': '-131.39', 'loss/train': '0.65696', 'examples_per_second': '30.76', 'grad_norm': '101.5', 'counters/examples': 51776, 'counters/updates': 1618}
train stats after 51808 examples: {'rewards_train/chosen': '0.047339', 'rewards_train/rejected': '0.072579', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.02524', 'logps_train/rejected': '-202.57', 'logps_train/chosen': '-188.8', 'loss/train': '0.72866', 'examples_per_second': '30.601', 'grad_norm': '410', 'counters/examples': 51808, 'counters/updates': 1619}
train stats after 51840 examples: {'rewards_train/chosen': '0.03203', 'rewards_train/rejected': '-0.073102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10513', 'logps_train/rejected': '-106.99', 'logps_train/chosen': '-160.08', 'loss/train': '0.66209', 'examples_per_second': '31.553', 'grad_norm': '71', 'counters/examples': 51840, 'counters/updates': 1620}
train stats after 51872 examples: {'rewards_train/chosen': '-0.0045316', 'rewards_train/rejected': '0.011799', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.01633', 'logps_train/rejected': '-129.22', 'logps_train/chosen': '-160.82', 'loss/train': '0.71846', 'examples_per_second': '32.358', 'grad_norm': '58.75', 'counters/examples': 51872, 'counters/updates': 1621}
train stats after 51904 examples: {'rewards_train/chosen': '0.032585', 'rewards_train/rejected': '0.024135', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0084506', 'logps_train/rejected': '-114.94', 'logps_train/chosen': '-136.73', 'loss/train': '0.70184', 'examples_per_second': '25.791', 'grad_norm': '150', 'counters/examples': 51904, 'counters/updates': 1622}
train stats after 51936 examples: {'rewards_train/chosen': '0.055501', 'rewards_train/rejected': '-0.058606', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11411', 'logps_train/rejected': '-119.68', 'logps_train/chosen': '-141.62', 'loss/train': '0.64553', 'examples_per_second': '30.47', 'grad_norm': '113', 'counters/examples': 51936, 'counters/updates': 1623}
train stats after 51968 examples: {'rewards_train/chosen': '-0.02877', 'rewards_train/rejected': '0.032837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.061607', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-166.02', 'loss/train': '0.74807', 'examples_per_second': '30.111', 'grad_norm': '105', 'counters/examples': 51968, 'counters/updates': 1624}
train stats after 52000 examples: {'rewards_train/chosen': '0.028566', 'rewards_train/rejected': '-0.078802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10737', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-161.57', 'loss/train': '0.64964', 'examples_per_second': '25.059', 'grad_norm': '86', 'counters/examples': 52000, 'counters/updates': 1625}
skipping logging after 52032 examples to avoid logging too frequently
train stats after 52064 examples: {'rewards_train/chosen': '0.053516', 'rewards_train/rejected': '-0.0018473', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055363', 'logps_train/rejected': '-116.54', 'logps_train/chosen': '-137.07', 'loss/train': '0.69094', 'examples_per_second': '30.887', 'grad_norm': '89.5', 'counters/examples': 52064, 'counters/updates': 1627}
train stats after 52096 examples: {'rewards_train/chosen': '0.064239', 'rewards_train/rejected': '-0.012594', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076832', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-184.59', 'loss/train': '0.66891', 'examples_per_second': '31.473', 'grad_norm': '93.5', 'counters/examples': 52096, 'counters/updates': 1628}
train stats after 52128 examples: {'rewards_train/chosen': '0.03331', 'rewards_train/rejected': '0.0018492', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031461', 'logps_train/rejected': '-157.1', 'logps_train/chosen': '-153.65', 'loss/train': '0.68933', 'examples_per_second': '31.082', 'grad_norm': '229', 'counters/examples': 52128, 'counters/updates': 1629}
train stats after 52160 examples: {'rewards_train/chosen': '0.036006', 'rewards_train/rejected': '0.044409', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0084026', 'logps_train/rejected': '-152.34', 'logps_train/chosen': '-147.74', 'loss/train': '0.70802', 'examples_per_second': '33.076', 'grad_norm': '102', 'counters/examples': 52160, 'counters/updates': 1630}
train stats after 52192 examples: {'rewards_train/chosen': '-0.014983', 'rewards_train/rejected': '-0.031458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016475', 'logps_train/rejected': '-96.516', 'logps_train/chosen': '-126.66', 'loss/train': '0.68995', 'examples_per_second': '30.448', 'grad_norm': '185', 'counters/examples': 52192, 'counters/updates': 1631}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52256 examples: {'rewards_train/chosen': '0.063355', 'rewards_train/rejected': '-0.073425', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13678', 'logps_train/rejected': '-125.34', 'logps_train/chosen': '-175.22', 'loss/train': '0.63896', 'examples_per_second': '33.318', 'grad_norm': '59.5', 'counters/examples': 52256, 'counters/updates': 1633}
train stats after 52288 examples: {'rewards_train/chosen': '0.028541', 'rewards_train/rejected': '-0.018775', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047316', 'logps_train/rejected': '-179.72', 'logps_train/chosen': '-189.99', 'loss/train': '0.68761', 'examples_per_second': '31.554', 'grad_norm': '98', 'counters/examples': 52288, 'counters/updates': 1634}
train stats after 52320 examples: {'rewards_train/chosen': '0.01332', 'rewards_train/rejected': '-0.086779', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1001', 'logps_train/rejected': '-152.73', 'logps_train/chosen': '-139.03', 'loss/train': '0.67804', 'examples_per_second': '30.864', 'grad_norm': '230', 'counters/examples': 52320, 'counters/updates': 1635}
train stats after 52352 examples: {'rewards_train/chosen': '0.011985', 'rewards_train/rejected': '0.062641', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050655', 'logps_train/rejected': '-124.36', 'logps_train/chosen': '-119.63', 'loss/train': '0.72674', 'examples_per_second': '31.552', 'grad_norm': '89', 'counters/examples': 52352, 'counters/updates': 1636}
train stats after 52384 examples: {'rewards_train/chosen': '-0.0078198', 'rewards_train/rejected': '0.052111', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.059931', 'logps_train/rejected': '-105.28', 'logps_train/chosen': '-128.65', 'loss/train': '0.73927', 'examples_per_second': '31.551', 'grad_norm': '127', 'counters/examples': 52384, 'counters/updates': 1637}
skipping logging after 52416 examples to avoid logging too frequently
train stats after 52448 examples: {'rewards_train/chosen': '0.035027', 'rewards_train/rejected': '0.047139', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012111', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-152.51', 'loss/train': '0.71694', 'examples_per_second': '30.984', 'grad_norm': '103', 'counters/examples': 52448, 'counters/updates': 1639}
train stats after 52480 examples: {'rewards_train/chosen': '-0.060841', 'rewards_train/rejected': '0.032857', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.093698', 'logps_train/rejected': '-151.67', 'logps_train/chosen': '-214.14', 'loss/train': '0.75733', 'examples_per_second': '31.654', 'grad_norm': '140', 'counters/examples': 52480, 'counters/updates': 1640}
skipping logging after 52512 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '0.033597', 'rewards_train/rejected': '0.0065271', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02707', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-93.43', 'loss/train': '0.6896', 'examples_per_second': '30.255', 'grad_norm': '160', 'counters/examples': 52544, 'counters/updates': 1642}
train stats after 52576 examples: {'rewards_train/chosen': '-0.059818', 'rewards_train/rejected': '0.023474', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.083291', 'logps_train/rejected': '-91.755', 'logps_train/chosen': '-121.43', 'loss/train': '0.74173', 'examples_per_second': '32.27', 'grad_norm': '99', 'counters/examples': 52576, 'counters/updates': 1643}
train stats after 52608 examples: {'rewards_train/chosen': '0.097071', 'rewards_train/rejected': '-0.10105', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19812', 'logps_train/rejected': '-169.86', 'logps_train/chosen': '-151.24', 'loss/train': '0.61462', 'examples_per_second': '31.56', 'grad_norm': '96', 'counters/examples': 52608, 'counters/updates': 1644}
skipping logging after 52640 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '0.036077', 'rewards_train/rejected': '-0.043576', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079653', 'logps_train/rejected': '-134.46', 'logps_train/chosen': '-154.27', 'loss/train': '0.66473', 'examples_per_second': '30.541', 'grad_norm': '79', 'counters/examples': 52672, 'counters/updates': 1646}
train stats after 52704 examples: {'rewards_train/chosen': '0.078498', 'rewards_train/rejected': '0.11404', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.035539', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-116.18', 'loss/train': '0.75452', 'examples_per_second': '31.37', 'grad_norm': '116', 'counters/examples': 52704, 'counters/updates': 1647}
train stats after 52736 examples: {'rewards_train/chosen': '-0.0018637', 'rewards_train/rejected': '0.10811', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.10997', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-131.61', 'loss/train': '0.76646', 'examples_per_second': '30.14', 'grad_norm': '133', 'counters/examples': 52736, 'counters/updates': 1648}
train stats after 52768 examples: {'rewards_train/chosen': '0.11723', 'rewards_train/rejected': '0.058341', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058889', 'logps_train/rejected': '-133.89', 'logps_train/chosen': '-165.09', 'loss/train': '0.6841', 'examples_per_second': '30.133', 'grad_norm': '180', 'counters/examples': 52768, 'counters/updates': 1649}
train stats after 52800 examples: {'rewards_train/chosen': '0.01964', 'rewards_train/rejected': '0.0098062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0098336', 'logps_train/rejected': '-158.6', 'logps_train/chosen': '-172.06', 'loss/train': '0.6964', 'examples_per_second': '31.487', 'grad_norm': '82.5', 'counters/examples': 52800, 'counters/updates': 1650}
train stats after 52832 examples: {'rewards_train/chosen': '0.088399', 'rewards_train/rejected': '0.046137', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042262', 'logps_train/rejected': '-149.69', 'logps_train/chosen': '-175.97', 'loss/train': '0.67945', 'examples_per_second': '29.996', 'grad_norm': '80', 'counters/examples': 52832, 'counters/updates': 1651}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52896 examples: {'rewards_train/chosen': '0.018436', 'rewards_train/rejected': '0.028029', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0095924', 'logps_train/rejected': '-171.27', 'logps_train/chosen': '-182.27', 'loss/train': '0.71127', 'examples_per_second': '31.279', 'grad_norm': '132', 'counters/examples': 52896, 'counters/updates': 1653}
train stats after 52928 examples: {'rewards_train/chosen': '0.18808', 'rewards_train/rejected': '-0.0012851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18937', 'logps_train/rejected': '-151.61', 'logps_train/chosen': '-163.84', 'loss/train': '0.63696', 'examples_per_second': '30.175', 'grad_norm': '51', 'counters/examples': 52928, 'counters/updates': 1654}
train stats after 52960 examples: {'rewards_train/chosen': '0.004493', 'rewards_train/rejected': '0.055901', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051408', 'logps_train/rejected': '-160.41', 'logps_train/chosen': '-129.58', 'loss/train': '0.74523', 'examples_per_second': '31.581', 'grad_norm': '136', 'counters/examples': 52960, 'counters/updates': 1655}
train stats after 52992 examples: {'rewards_train/chosen': '0.047375', 'rewards_train/rejected': '0.035347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012028', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-236.79', 'loss/train': '0.69646', 'examples_per_second': '31.417', 'grad_norm': '93.5', 'counters/examples': 52992, 'counters/updates': 1656}
train stats after 53024 examples: {'rewards_train/chosen': '0.022204', 'rewards_train/rejected': '0.071066', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.048861', 'logps_train/rejected': '-151.99', 'logps_train/chosen': '-149.21', 'loss/train': '0.73283', 'examples_per_second': '30.514', 'grad_norm': '120', 'counters/examples': 53024, 'counters/updates': 1657}
train stats after 53056 examples: {'rewards_train/chosen': '0.02818', 'rewards_train/rejected': '-0.029615', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057795', 'logps_train/rejected': '-100.85', 'logps_train/chosen': '-135.23', 'loss/train': '0.66989', 'examples_per_second': '24.874', 'grad_norm': '82.5', 'counters/examples': 53056, 'counters/updates': 1658}
train stats after 53088 examples: {'rewards_train/chosen': '-0.025928', 'rewards_train/rejected': '-0.037471', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011543', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-121.85', 'loss/train': '0.69703', 'examples_per_second': '32.715', 'grad_norm': '71', 'counters/examples': 53088, 'counters/updates': 1659}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53152 examples: {'rewards_train/chosen': '-0.0018187', 'rewards_train/rejected': '0.11925', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12107', 'logps_train/rejected': '-98.723', 'logps_train/chosen': '-120.6', 'loss/train': '0.76648', 'examples_per_second': '30.159', 'grad_norm': '141', 'counters/examples': 53152, 'counters/updates': 1661}
train stats after 53184 examples: {'rewards_train/chosen': '0.029972', 'rewards_train/rejected': '0.014335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015637', 'logps_train/rejected': '-124.64', 'logps_train/chosen': '-122.25', 'loss/train': '0.6951', 'examples_per_second': '31.576', 'grad_norm': '46', 'counters/examples': 53184, 'counters/updates': 1662}
skipping logging after 53216 examples to avoid logging too frequently
train stats after 53248 examples: {'rewards_train/chosen': '-0.016066', 'rewards_train/rejected': '0.033355', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.049421', 'logps_train/rejected': '-142.4', 'logps_train/chosen': '-150.99', 'loss/train': '0.73668', 'examples_per_second': '31.197', 'grad_norm': '116', 'counters/examples': 53248, 'counters/updates': 1664}
train stats after 53280 examples: {'rewards_train/chosen': '-0.022405', 'rewards_train/rejected': '0.014589', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036993', 'logps_train/rejected': '-171.38', 'logps_train/chosen': '-147.3', 'loss/train': '0.73306', 'examples_per_second': '31.251', 'grad_norm': '87', 'counters/examples': 53280, 'counters/updates': 1665}
skipping logging after 53312 examples to avoid logging too frequently
train stats after 53344 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '-0.02941', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15985', 'logps_train/rejected': '-177.66', 'logps_train/chosen': '-150.3', 'loss/train': '0.62612', 'examples_per_second': '31.683', 'grad_norm': '51.75', 'counters/examples': 53344, 'counters/updates': 1667}
train stats after 53376 examples: {'rewards_train/chosen': '0.0088129', 'rewards_train/rejected': '-0.009392', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018205', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-131.76', 'loss/train': '0.69357', 'examples_per_second': '31.609', 'grad_norm': '163', 'counters/examples': 53376, 'counters/updates': 1668}
train stats after 53408 examples: {'rewards_train/chosen': '0.028725', 'rewards_train/rejected': '-0.02977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058495', 'logps_train/rejected': '-99.344', 'logps_train/chosen': '-139.3', 'loss/train': '0.68081', 'examples_per_second': '32.705', 'grad_norm': '115', 'counters/examples': 53408, 'counters/updates': 1669}
train stats after 53440 examples: {'rewards_train/chosen': '-0.065676', 'rewards_train/rejected': '-0.00090688', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.064769', 'logps_train/rejected': '-145.69', 'logps_train/chosen': '-139.68', 'loss/train': '0.73758', 'examples_per_second': '31.553', 'grad_norm': '70.5', 'counters/examples': 53440, 'counters/updates': 1670}
train stats after 53472 examples: {'rewards_train/chosen': '0.087555', 'rewards_train/rejected': '0.037813', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049742', 'logps_train/rejected': '-140.97', 'logps_train/chosen': '-172.06', 'loss/train': '0.67336', 'examples_per_second': '29.666', 'grad_norm': '88', 'counters/examples': 53472, 'counters/updates': 1671}
train stats after 53504 examples: {'rewards_train/chosen': '-0.035106', 'rewards_train/rejected': '0.040275', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.075381', 'logps_train/rejected': '-134.26', 'logps_train/chosen': '-130.01', 'loss/train': '0.74963', 'examples_per_second': '30.924', 'grad_norm': '115.5', 'counters/examples': 53504, 'counters/updates': 1672}
skipping logging after 53536 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '0.0097071', 'rewards_train/rejected': '0.023895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014188', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-139.16', 'loss/train': '0.71384', 'examples_per_second': '33.779', 'grad_norm': '103', 'counters/examples': 53568, 'counters/updates': 1674}
train stats after 53600 examples: {'rewards_train/chosen': '-0.019853', 'rewards_train/rejected': '0.040162', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.060015', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-146.77', 'loss/train': '0.73144', 'examples_per_second': '32.44', 'grad_norm': '55.75', 'counters/examples': 53600, 'counters/updates': 1675}
train stats after 53632 examples: {'rewards_train/chosen': '0.022725', 'rewards_train/rejected': '0.030451', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0077255', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-122.83', 'loss/train': '0.70348', 'examples_per_second': '32.387', 'grad_norm': '71.5', 'counters/examples': 53632, 'counters/updates': 1676}
train stats after 53664 examples: {'rewards_train/chosen': '0.1066', 'rewards_train/rejected': '-0.0079605', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11456', 'logps_train/rejected': '-128.54', 'logps_train/chosen': '-179.77', 'loss/train': '0.65643', 'examples_per_second': '31.31', 'grad_norm': '76', 'counters/examples': 53664, 'counters/updates': 1677}
train stats after 53696 examples: {'rewards_train/chosen': '0.015956', 'rewards_train/rejected': '-0.019988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035944', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-142.68', 'loss/train': '0.68057', 'examples_per_second': '30.87', 'grad_norm': '81', 'counters/examples': 53696, 'counters/updates': 1678}
train stats after 53728 examples: {'rewards_train/chosen': '0.023776', 'rewards_train/rejected': '-0.037783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061559', 'logps_train/rejected': '-155.25', 'logps_train/chosen': '-145.43', 'loss/train': '0.70481', 'examples_per_second': '30.704', 'grad_norm': '302', 'counters/examples': 53728, 'counters/updates': 1679}
train stats after 53760 examples: {'rewards_train/chosen': '0.047208', 'rewards_train/rejected': '0.078134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030926', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-182.05', 'loss/train': '0.72257', 'examples_per_second': '31.398', 'grad_norm': '249', 'counters/examples': 53760, 'counters/updates': 1680}
train stats after 53792 examples: {'rewards_train/chosen': '0.05922', 'rewards_train/rejected': '0.068214', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0089939', 'logps_train/rejected': '-183.95', 'logps_train/chosen': '-194.39', 'loss/train': '0.72006', 'examples_per_second': '32.131', 'grad_norm': '70.5', 'counters/examples': 53792, 'counters/updates': 1681}
train stats after 53824 examples: {'rewards_train/chosen': '0.081288', 'rewards_train/rejected': '0.06408', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017208', 'logps_train/rejected': '-161.45', 'logps_train/chosen': '-202.64', 'loss/train': '0.70725', 'examples_per_second': '31.328', 'grad_norm': '82.5', 'counters/examples': 53824, 'counters/updates': 1682}
train stats after 53856 examples: {'rewards_train/chosen': '0.086603', 'rewards_train/rejected': '-0.044673', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13128', 'logps_train/rejected': '-85.57', 'logps_train/chosen': '-163.91', 'loss/train': '0.65003', 'examples_per_second': '31.539', 'grad_norm': '107.5', 'counters/examples': 53856, 'counters/updates': 1683}
train stats after 53888 examples: {'rewards_train/chosen': '0.032977', 'rewards_train/rejected': '0.015485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017493', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-156.85', 'loss/train': '0.69575', 'examples_per_second': '31.786', 'grad_norm': '181', 'counters/examples': 53888, 'counters/updates': 1684}
skipping logging after 53920 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '-0.01629', 'rewards_train/rejected': '-0.014712', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0015777', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-188.69', 'loss/train': '0.71082', 'examples_per_second': '31.542', 'grad_norm': '116', 'counters/examples': 53952, 'counters/updates': 1686}
train stats after 53984 examples: {'rewards_train/chosen': '-0.049769', 'rewards_train/rejected': '-0.0086657', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041104', 'logps_train/rejected': '-155.73', 'logps_train/chosen': '-161.64', 'loss/train': '0.72769', 'examples_per_second': '30.903', 'grad_norm': '96.5', 'counters/examples': 53984, 'counters/updates': 1687}
train stats after 54016 examples: {'rewards_train/chosen': '-0.0011115', 'rewards_train/rejected': '-0.016214', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015102', 'logps_train/rejected': '-119.83', 'logps_train/chosen': '-139.91', 'loss/train': '0.69228', 'examples_per_second': '32.898', 'grad_norm': '101.5', 'counters/examples': 54016, 'counters/updates': 1688}
train stats after 54048 examples: {'rewards_train/chosen': '0.018015', 'rewards_train/rejected': '0.040355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02234', 'logps_train/rejected': '-145.71', 'logps_train/chosen': '-146.6', 'loss/train': '0.71381', 'examples_per_second': '30.706', 'grad_norm': '219', 'counters/examples': 54048, 'counters/updates': 1689}
train stats after 54080 examples: {'rewards_train/chosen': '0.083111', 'rewards_train/rejected': '0.010912', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072198', 'logps_train/rejected': '-157.06', 'logps_train/chosen': '-170.34', 'loss/train': '0.66703', 'examples_per_second': '30.556', 'grad_norm': '155', 'counters/examples': 54080, 'counters/updates': 1690}
skipping logging after 54112 examples to avoid logging too frequently
train stats after 54144 examples: {'rewards_train/chosen': '0.034269', 'rewards_train/rejected': '0.026723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0075454', 'logps_train/rejected': '-104.92', 'logps_train/chosen': '-172.79', 'loss/train': '0.71505', 'examples_per_second': '30.697', 'grad_norm': '160', 'counters/examples': 54144, 'counters/updates': 1692}
train stats after 54176 examples: {'rewards_train/chosen': '-0.046237', 'rewards_train/rejected': '-0.088988', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042752', 'logps_train/rejected': '-159.81', 'logps_train/chosen': '-133.6', 'loss/train': '0.68811', 'examples_per_second': '31.555', 'grad_norm': '68', 'counters/examples': 54176, 'counters/updates': 1693}
train stats after 54208 examples: {'rewards_train/chosen': '0.072202', 'rewards_train/rejected': '0.0070991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065103', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-132.83', 'loss/train': '0.67168', 'examples_per_second': '32.48', 'grad_norm': '223', 'counters/examples': 54208, 'counters/updates': 1694}
skipping logging after 54240 examples to avoid logging too frequently
train stats after 54272 examples: {'rewards_train/chosen': '0.040176', 'rewards_train/rejected': '-0.0054754', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045651', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-179.65', 'loss/train': '0.67892', 'examples_per_second': '34.209', 'grad_norm': '98', 'counters/examples': 54272, 'counters/updates': 1696}
train stats after 54304 examples: {'rewards_train/chosen': '-0.0049105', 'rewards_train/rejected': '0.027347', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032257', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-155.95', 'loss/train': '0.73387', 'examples_per_second': '31.558', 'grad_norm': '91', 'counters/examples': 54304, 'counters/updates': 1697}
train stats after 54336 examples: {'rewards_train/chosen': '0.056381', 'rewards_train/rejected': '0.016097', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040284', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-161.07', 'loss/train': '0.67991', 'examples_per_second': '31.536', 'grad_norm': '85.5', 'counters/examples': 54336, 'counters/updates': 1698}
train stats after 54368 examples: {'rewards_train/chosen': '0.061625', 'rewards_train/rejected': '-0.00014623', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061771', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-192.9', 'loss/train': '0.6751', 'examples_per_second': '30.938', 'grad_norm': '68.5', 'counters/examples': 54368, 'counters/updates': 1699}
train stats after 54400 examples: {'rewards_train/chosen': '0.022529', 'rewards_train/rejected': '0.0337', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011171', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-150.4', 'loss/train': '0.7073', 'examples_per_second': '31.526', 'grad_norm': '82', 'counters/examples': 54400, 'counters/updates': 1700}
train stats after 54432 examples: {'rewards_train/chosen': '0.033753', 'rewards_train/rejected': '0.040089', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0063362', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-198.97', 'loss/train': '0.71501', 'examples_per_second': '31.371', 'grad_norm': '93.5', 'counters/examples': 54432, 'counters/updates': 1701}
train stats after 54464 examples: {'rewards_train/chosen': '0.031847', 'rewards_train/rejected': '0.020368', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011479', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-183.9', 'loss/train': '0.69389', 'examples_per_second': '31.52', 'grad_norm': '125', 'counters/examples': 54464, 'counters/updates': 1702}
train stats after 54496 examples: {'rewards_train/chosen': '0.08868', 'rewards_train/rejected': '-0.033077', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12176', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-166.32', 'loss/train': '0.64678', 'examples_per_second': '30.084', 'grad_norm': '82', 'counters/examples': 54496, 'counters/updates': 1703}
train stats after 54528 examples: {'rewards_train/chosen': '-0.031616', 'rewards_train/rejected': '-0.047983', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016367', 'logps_train/rejected': '-178.54', 'logps_train/chosen': '-139.3', 'loss/train': '0.69808', 'examples_per_second': '31.446', 'grad_norm': '115.5', 'counters/examples': 54528, 'counters/updates': 1704}
train stats after 54560 examples: {'rewards_train/chosen': '0.10615', 'rewards_train/rejected': '-0.0024239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10858', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-210.01', 'loss/train': '0.65686', 'examples_per_second': '31.566', 'grad_norm': '80.5', 'counters/examples': 54560, 'counters/updates': 1705}
train stats after 54592 examples: {'rewards_train/chosen': '0.02549', 'rewards_train/rejected': '0.0072289', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018262', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-172.73', 'loss/train': '0.6914', 'examples_per_second': '30.643', 'grad_norm': '76', 'counters/examples': 54592, 'counters/updates': 1706}
train stats after 54624 examples: {'rewards_train/chosen': '0.047407', 'rewards_train/rejected': '0.070903', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023496', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-152.94', 'loss/train': '0.71231', 'examples_per_second': '31.526', 'grad_norm': '126', 'counters/examples': 54624, 'counters/updates': 1707}
train stats after 54656 examples: {'rewards_train/chosen': '-0.034993', 'rewards_train/rejected': '-0.011343', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02365', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-145.64', 'loss/train': '0.71331', 'examples_per_second': '31.489', 'grad_norm': '123.5', 'counters/examples': 54656, 'counters/updates': 1708}
train stats after 54688 examples: {'rewards_train/chosen': '-0.035748', 'rewards_train/rejected': '0.0064763', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.042224', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-179.75', 'loss/train': '0.73124', 'examples_per_second': '32.322', 'grad_norm': '111', 'counters/examples': 54688, 'counters/updates': 1709}
train stats after 54720 examples: {'rewards_train/chosen': '0.026777', 'rewards_train/rejected': '0.022966', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038108', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-127.17', 'loss/train': '0.70701', 'examples_per_second': '30.123', 'grad_norm': '75', 'counters/examples': 54720, 'counters/updates': 1710}
skipping logging after 54752 examples to avoid logging too frequently
train stats after 54784 examples: {'rewards_train/chosen': '0.13665', 'rewards_train/rejected': '0.026147', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1105', 'logps_train/rejected': '-140.16', 'logps_train/chosen': '-159.19', 'loss/train': '0.65464', 'examples_per_second': '32.299', 'grad_norm': '167', 'counters/examples': 54784, 'counters/updates': 1712}
train stats after 54816 examples: {'rewards_train/chosen': '0.05454', 'rewards_train/rejected': '0.063218', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0086781', 'logps_train/rejected': '-189.26', 'logps_train/chosen': '-127.52', 'loss/train': '0.72349', 'examples_per_second': '32.267', 'grad_norm': '388', 'counters/examples': 54816, 'counters/updates': 1713}
train stats after 54848 examples: {'rewards_train/chosen': '0.097746', 'rewards_train/rejected': '0.050448', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047298', 'logps_train/rejected': '-134.77', 'logps_train/chosen': '-173.89', 'loss/train': '0.68012', 'examples_per_second': '30.972', 'grad_norm': '71', 'counters/examples': 54848, 'counters/updates': 1714}
skipping logging after 54880 examples to avoid logging too frequently
train stats after 54912 examples: {'rewards_train/chosen': '0.062855', 'rewards_train/rejected': '-0.061726', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12458', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-167.96', 'loss/train': '0.64982', 'examples_per_second': '33.227', 'grad_norm': '72', 'counters/examples': 54912, 'counters/updates': 1716}
train stats after 54944 examples: {'rewards_train/chosen': '0.084867', 'rewards_train/rejected': '0.042361', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042505', 'logps_train/rejected': '-139.33', 'logps_train/chosen': '-138.85', 'loss/train': '0.68878', 'examples_per_second': '32.481', 'grad_norm': '86', 'counters/examples': 54944, 'counters/updates': 1717}
train stats after 54976 examples: {'rewards_train/chosen': '0.10961', 'rewards_train/rejected': '0.09013', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019483', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-158.53', 'loss/train': '0.70552', 'examples_per_second': '31.59', 'grad_norm': '103.5', 'counters/examples': 54976, 'counters/updates': 1718}
skipping logging after 55008 examples to avoid logging too frequently
train stats after 55040 examples: {'rewards_train/chosen': '0.053737', 'rewards_train/rejected': '-0.090495', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14423', 'logps_train/rejected': '-148.31', 'logps_train/chosen': '-176.05', 'loss/train': '0.63606', 'examples_per_second': '32.859', 'grad_norm': '312', 'counters/examples': 55040, 'counters/updates': 1720}
train stats after 55072 examples: {'rewards_train/chosen': '0.093938', 'rewards_train/rejected': '0.032937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061001', 'logps_train/rejected': '-94.325', 'logps_train/chosen': '-112.47', 'loss/train': '0.6728', 'examples_per_second': '31.627', 'grad_norm': '91', 'counters/examples': 55072, 'counters/updates': 1721}
skipping logging after 55104 examples to avoid logging too frequently
train stats after 55136 examples: {'rewards_train/chosen': '0.023369', 'rewards_train/rejected': '0.002589', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02078', 'logps_train/rejected': '-130.03', 'logps_train/chosen': '-153.29', 'loss/train': '0.68933', 'examples_per_second': '31.516', 'grad_norm': '129', 'counters/examples': 55136, 'counters/updates': 1723}
train stats after 55168 examples: {'rewards_train/chosen': '-0.019151', 'rewards_train/rejected': '-0.030274', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011123', 'logps_train/rejected': '-143.37', 'logps_train/chosen': '-169.01', 'loss/train': '0.70458', 'examples_per_second': '30.293', 'grad_norm': '74', 'counters/examples': 55168, 'counters/updates': 1724}
train stats after 55200 examples: {'rewards_train/chosen': '0.060379', 'rewards_train/rejected': '0.0086135', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051766', 'logps_train/rejected': '-102.18', 'logps_train/chosen': '-178.59', 'loss/train': '0.68128', 'examples_per_second': '30.875', 'grad_norm': '59.75', 'counters/examples': 55200, 'counters/updates': 1725}
train stats after 55232 examples: {'rewards_train/chosen': '0.035185', 'rewards_train/rejected': '0.055328', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020144', 'logps_train/rejected': '-178.34', 'logps_train/chosen': '-179.14', 'loss/train': '0.71434', 'examples_per_second': '32.485', 'grad_norm': '76.5', 'counters/examples': 55232, 'counters/updates': 1726}
train stats after 55264 examples: {'rewards_train/chosen': '-0.033076', 'rewards_train/rejected': '0.0022561', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035333', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-154.57', 'loss/train': '0.72426', 'examples_per_second': '31.891', 'grad_norm': '109.5', 'counters/examples': 55264, 'counters/updates': 1727}
train stats after 55296 examples: {'rewards_train/chosen': '0.10606', 'rewards_train/rejected': '0.04156', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.064502', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-146.98', 'loss/train': '0.67039', 'examples_per_second': '31.853', 'grad_norm': '173', 'counters/examples': 55296, 'counters/updates': 1728}
skipping logging after 55328 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '-0.0093449', 'rewards_train/rejected': '0.090512', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.099857', 'logps_train/rejected': '-157.54', 'logps_train/chosen': '-161.01', 'loss/train': '0.7955', 'examples_per_second': '30.113', 'grad_norm': '153', 'counters/examples': 55360, 'counters/updates': 1730}
train stats after 55392 examples: {'rewards_train/chosen': '0.0022975', 'rewards_train/rejected': '0.03056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.028263', 'logps_train/rejected': '-111.86', 'logps_train/chosen': '-127.61', 'loss/train': '0.71614', 'examples_per_second': '29.304', 'grad_norm': '146', 'counters/examples': 55392, 'counters/updates': 1731}
train stats after 55424 examples: {'rewards_train/chosen': '0.039797', 'rewards_train/rejected': '0.04011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00031288', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-143.88', 'loss/train': '0.71228', 'examples_per_second': '32.746', 'grad_norm': '99', 'counters/examples': 55424, 'counters/updates': 1732}
train stats after 55456 examples: {'rewards_train/chosen': '0.11713', 'rewards_train/rejected': '-0.050147', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16728', 'logps_train/rejected': '-190.6', 'logps_train/chosen': '-221.54', 'loss/train': '0.65274', 'examples_per_second': '30.551', 'grad_norm': '97', 'counters/examples': 55456, 'counters/updates': 1733}
train stats after 55488 examples: {'rewards_train/chosen': '0.042734', 'rewards_train/rejected': '0.053111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010376', 'logps_train/rejected': '-143.94', 'logps_train/chosen': '-165.25', 'loss/train': '0.71479', 'examples_per_second': '30.694', 'grad_norm': '130', 'counters/examples': 55488, 'counters/updates': 1734}
skipping logging after 55520 examples to avoid logging too frequently
train stats after 55552 examples: {'rewards_train/chosen': '0.064106', 'rewards_train/rejected': '-0.052602', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11671', 'logps_train/rejected': '-168.19', 'logps_train/chosen': '-186.72', 'loss/train': '0.6538', 'examples_per_second': '31.524', 'grad_norm': '93', 'counters/examples': 55552, 'counters/updates': 1736}
skipping logging after 55584 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '0.0042596', 'rewards_train/rejected': '0.0071118', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0028522', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-160.16', 'loss/train': '0.71006', 'examples_per_second': '29.969', 'grad_norm': '84.5', 'counters/examples': 55616, 'counters/updates': 1738}
skipping logging after 55648 examples to avoid logging too frequently
train stats after 55680 examples: {'rewards_train/chosen': '-0.0061816', 'rewards_train/rejected': '-0.12937', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-167.01', 'logps_train/chosen': '-139.91', 'loss/train': '0.65818', 'examples_per_second': '31.582', 'grad_norm': '115', 'counters/examples': 55680, 'counters/updates': 1740}
train stats after 55712 examples: {'rewards_train/chosen': '-0.032163', 'rewards_train/rejected': '0.0071894', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039353', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-176.04', 'loss/train': '0.72554', 'examples_per_second': '30.573', 'grad_norm': '92.5', 'counters/examples': 55712, 'counters/updates': 1741}
train stats after 55744 examples: {'rewards_train/chosen': '0.023179', 'rewards_train/rejected': '0.014606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0085729', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-137.32', 'loss/train': '0.70092', 'examples_per_second': '31.648', 'grad_norm': '87.5', 'counters/examples': 55744, 'counters/updates': 1742}
train stats after 55776 examples: {'rewards_train/chosen': '0.038369', 'rewards_train/rejected': '0.010532', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027837', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-165.63', 'loss/train': '0.69015', 'examples_per_second': '31.576', 'grad_norm': '185', 'counters/examples': 55776, 'counters/updates': 1743}
train stats after 55808 examples: {'rewards_train/chosen': '0.022332', 'rewards_train/rejected': '0.010246', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012086', 'logps_train/rejected': '-144.54', 'logps_train/chosen': '-157.19', 'loss/train': '0.69848', 'examples_per_second': '31.635', 'grad_norm': '162', 'counters/examples': 55808, 'counters/updates': 1744}
skipping logging after 55840 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '-0.0050097', 'rewards_train/rejected': '0.039386', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044396', 'logps_train/rejected': '-160.95', 'logps_train/chosen': '-152.84', 'loss/train': '0.73758', 'examples_per_second': '30.105', 'grad_norm': '133', 'counters/examples': 55872, 'counters/updates': 1746}
train stats after 55904 examples: {'rewards_train/chosen': '0.056252', 'rewards_train/rejected': '0.00022426', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056027', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-181.68', 'loss/train': '0.67666', 'examples_per_second': '31.934', 'grad_norm': '170', 'counters/examples': 55904, 'counters/updates': 1747}
train stats after 55936 examples: {'rewards_train/chosen': '0.058201', 'rewards_train/rejected': '0.10917', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.050967', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-134.53', 'loss/train': '0.72507', 'examples_per_second': '31.072', 'grad_norm': '67', 'counters/examples': 55936, 'counters/updates': 1748}
train stats after 55968 examples: {'rewards_train/chosen': '0.035405', 'rewards_train/rejected': '0.048333', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012928', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-155.6', 'loss/train': '0.711', 'examples_per_second': '31.089', 'grad_norm': '109', 'counters/examples': 55968, 'counters/updates': 1749}
train stats after 56000 examples: {'rewards_train/chosen': '0.046043', 'rewards_train/rejected': '-0.020444', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066487', 'logps_train/rejected': '-119.31', 'logps_train/chosen': '-168.73', 'loss/train': '0.66787', 'examples_per_second': '31.524', 'grad_norm': '120.5', 'counters/examples': 56000, 'counters/updates': 1750}
train stats after 56032 examples: {'rewards_train/chosen': '0.022439', 'rewards_train/rejected': '0.028294', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0058554', 'logps_train/rejected': '-92.71', 'logps_train/chosen': '-120.62', 'loss/train': '0.70102', 'examples_per_second': '32.596', 'grad_norm': '68', 'counters/examples': 56032, 'counters/updates': 1751}
train stats after 56064 examples: {'rewards_train/chosen': '-0.034833', 'rewards_train/rejected': '0.081613', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.11645', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-172.22', 'loss/train': '0.78265', 'examples_per_second': '30.168', 'grad_norm': '76.5', 'counters/examples': 56064, 'counters/updates': 1752}
train stats after 56096 examples: {'rewards_train/chosen': '-0.010371', 'rewards_train/rejected': '0.04594', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.056311', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-175.83', 'loss/train': '0.73222', 'examples_per_second': '30.552', 'grad_norm': '92', 'counters/examples': 56096, 'counters/updates': 1753}
train stats after 56128 examples: {'rewards_train/chosen': '0.11592', 'rewards_train/rejected': '0.031854', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084062', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-157.56', 'loss/train': '0.66703', 'examples_per_second': '31.046', 'grad_norm': '108', 'counters/examples': 56128, 'counters/updates': 1754}
train stats after 56160 examples: {'rewards_train/chosen': '0.035349', 'rewards_train/rejected': '0.077128', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041779', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-150.91', 'loss/train': '0.72156', 'examples_per_second': '31.807', 'grad_norm': '112.5', 'counters/examples': 56160, 'counters/updates': 1755}
train stats after 56192 examples: {'rewards_train/chosen': '0.053635', 'rewards_train/rejected': '0.044412', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0092233', 'logps_train/rejected': '-146.93', 'logps_train/chosen': '-177.6', 'loss/train': '0.70056', 'examples_per_second': '31.374', 'grad_norm': '264', 'counters/examples': 56192, 'counters/updates': 1756}
skipping logging after 56224 examples to avoid logging too frequently
train stats after 56256 examples: {'rewards_train/chosen': '0.049049', 'rewards_train/rejected': '0.014912', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034137', 'logps_train/rejected': '-183.43', 'logps_train/chosen': '-149.75', 'loss/train': '0.69126', 'examples_per_second': '30.871', 'grad_norm': '68.5', 'counters/examples': 56256, 'counters/updates': 1758}
train stats after 56288 examples: {'rewards_train/chosen': '0.0026097', 'rewards_train/rejected': '-0.021255', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.023864', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-140.11', 'loss/train': '0.69856', 'examples_per_second': '32.848', 'grad_norm': '59.25', 'counters/examples': 56288, 'counters/updates': 1759}
train stats after 56320 examples: {'rewards_train/chosen': '0.064719', 'rewards_train/rejected': '0.037018', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027701', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-146.97', 'loss/train': '0.68605', 'examples_per_second': '31.123', 'grad_norm': '79', 'counters/examples': 56320, 'counters/updates': 1760}
train stats after 56352 examples: {'rewards_train/chosen': '0.021568', 'rewards_train/rejected': '-0.0141', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035668', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-155.33', 'loss/train': '0.68833', 'examples_per_second': '31.468', 'grad_norm': '72.5', 'counters/examples': 56352, 'counters/updates': 1761}
train stats after 56384 examples: {'rewards_train/chosen': '0.049508', 'rewards_train/rejected': '0.064046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014537', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-178.53', 'loss/train': '0.72435', 'examples_per_second': '31.589', 'grad_norm': '105.5', 'counters/examples': 56384, 'counters/updates': 1762}
skipping logging after 56416 examples to avoid logging too frequently
train stats after 56448 examples: {'rewards_train/chosen': '0.05531', 'rewards_train/rejected': '0.061267', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0059562', 'logps_train/rejected': '-106.08', 'logps_train/chosen': '-147.93', 'loss/train': '0.70113', 'examples_per_second': '33.582', 'grad_norm': '63.25', 'counters/examples': 56448, 'counters/updates': 1764}
train stats after 56480 examples: {'rewards_train/chosen': '0.065673', 'rewards_train/rejected': '0.0078322', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057841', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-149.53', 'loss/train': '0.67553', 'examples_per_second': '30.466', 'grad_norm': '62.25', 'counters/examples': 56480, 'counters/updates': 1765}
skipping logging after 56512 examples to avoid logging too frequently
train stats after 56544 examples: {'rewards_train/chosen': '0.015407', 'rewards_train/rejected': '0.026317', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.01091', 'logps_train/rejected': '-146.16', 'logps_train/chosen': '-158.58', 'loss/train': '0.71432', 'examples_per_second': '31.982', 'grad_norm': '73.5', 'counters/examples': 56544, 'counters/updates': 1767}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56608 examples: {'rewards_train/chosen': '-0.056167', 'rewards_train/rejected': '-0.041502', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014665', 'logps_train/rejected': '-112.8', 'logps_train/chosen': '-137.42', 'loss/train': '0.70991', 'examples_per_second': '32.99', 'grad_norm': '70.5', 'counters/examples': 56608, 'counters/updates': 1769}
skipping logging after 56640 examples to avoid logging too frequently
train stats after 56672 examples: {'rewards_train/chosen': '0.050447', 'rewards_train/rejected': '0.014354', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.036093', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-136.71', 'loss/train': '0.69433', 'examples_per_second': '30.179', 'grad_norm': '69.5', 'counters/examples': 56672, 'counters/updates': 1771}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56736 examples: {'rewards_train/chosen': '0.060988', 'rewards_train/rejected': '-0.00022957', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061217', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-138.89', 'loss/train': '0.67457', 'examples_per_second': '31.801', 'grad_norm': '64.5', 'counters/examples': 56736, 'counters/updates': 1773}
train stats after 56768 examples: {'rewards_train/chosen': '-0.0035397', 'rewards_train/rejected': '-0.082019', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078479', 'logps_train/rejected': '-145.32', 'logps_train/chosen': '-170.54', 'loss/train': '0.67874', 'examples_per_second': '31.56', 'grad_norm': '125', 'counters/examples': 56768, 'counters/updates': 1774}
skipping logging after 56800 examples to avoid logging too frequently
train stats after 56832 examples: {'rewards_train/chosen': '0.071396', 'rewards_train/rejected': '0.018412', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052984', 'logps_train/rejected': '-90.058', 'logps_train/chosen': '-117.62', 'loss/train': '0.67962', 'examples_per_second': '32.075', 'grad_norm': '62.75', 'counters/examples': 56832, 'counters/updates': 1776}
skipping logging after 56864 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '-0.0037923', 'rewards_train/rejected': '0.014124', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017917', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-145.07', 'loss/train': '0.71926', 'examples_per_second': '34.332', 'grad_norm': '74.5', 'counters/examples': 56896, 'counters/updates': 1778}
train stats after 56928 examples: {'rewards_train/chosen': '0.067805', 'rewards_train/rejected': '0.025269', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042536', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-149.61', 'loss/train': '0.6787', 'examples_per_second': '31.642', 'grad_norm': '79.5', 'counters/examples': 56928, 'counters/updates': 1779}
train stats after 56960 examples: {'rewards_train/chosen': '-0.027384', 'rewards_train/rejected': '-0.0095352', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017849', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-148.19', 'loss/train': '0.71187', 'examples_per_second': '31.546', 'grad_norm': '79.5', 'counters/examples': 56960, 'counters/updates': 1780}
train stats after 56992 examples: {'rewards_train/chosen': '0.047972', 'rewards_train/rejected': '0.082316', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.034344', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-166.85', 'loss/train': '0.72045', 'examples_per_second': '30.564', 'grad_norm': '163', 'counters/examples': 56992, 'counters/updates': 1781}
train stats after 57024 examples: {'rewards_train/chosen': '0.019209', 'rewards_train/rejected': '0.015103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0041064', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-135.39', 'loss/train': '0.70376', 'examples_per_second': '32.392', 'grad_norm': '98.5', 'counters/examples': 57024, 'counters/updates': 1782}
skipping logging after 57056 examples to avoid logging too frequently
train stats after 57088 examples: {'rewards_train/chosen': '-0.024854', 'rewards_train/rejected': '0.031254', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056108', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-186.17', 'loss/train': '0.73654', 'examples_per_second': '31.562', 'grad_norm': '87.5', 'counters/examples': 57088, 'counters/updates': 1784}
train stats after 57120 examples: {'rewards_train/chosen': '0.012333', 'rewards_train/rejected': '-0.027607', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03994', 'logps_train/rejected': '-179.59', 'logps_train/chosen': '-158.22', 'loss/train': '0.68556', 'examples_per_second': '31.61', 'grad_norm': '247', 'counters/examples': 57120, 'counters/updates': 1785}
train stats after 57152 examples: {'rewards_train/chosen': '-0.016417', 'rewards_train/rejected': '-0.022241', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.005825', 'logps_train/rejected': '-122.63', 'logps_train/chosen': '-113.23', 'loss/train': '0.70111', 'examples_per_second': '30.536', 'grad_norm': '68.5', 'counters/examples': 57152, 'counters/updates': 1786}
train stats after 57184 examples: {'rewards_train/chosen': '0.024386', 'rewards_train/rejected': '-0.0018972', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026283', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-133.94', 'loss/train': '0.6892', 'examples_per_second': '30.109', 'grad_norm': '100.5', 'counters/examples': 57184, 'counters/updates': 1787}
train stats after 57216 examples: {'rewards_train/chosen': '0.092517', 'rewards_train/rejected': '0.087424', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0050933', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-187.05', 'loss/train': '0.71226', 'examples_per_second': '31.545', 'grad_norm': '86', 'counters/examples': 57216, 'counters/updates': 1788}
skipping logging after 57248 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '0.025713', 'rewards_train/rejected': '0.065233', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03952', 'logps_train/rejected': '-113.29', 'logps_train/chosen': '-139.98', 'loss/train': '0.72732', 'examples_per_second': '31.53', 'grad_norm': '88', 'counters/examples': 57280, 'counters/updates': 1790}
train stats after 57312 examples: {'rewards_train/chosen': '-0.0060961', 'rewards_train/rejected': '0.039643', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.045739', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-127.96', 'loss/train': '0.74102', 'examples_per_second': '31.651', 'grad_norm': '91.5', 'counters/examples': 57312, 'counters/updates': 1791}
train stats after 57344 examples: {'rewards_train/chosen': '0.0029355', 'rewards_train/rejected': '-0.0068262', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0097617', 'logps_train/rejected': '-100.72', 'logps_train/chosen': '-136.98', 'loss/train': '0.69722', 'examples_per_second': '30.372', 'grad_norm': '624', 'counters/examples': 57344, 'counters/updates': 1792}
train stats after 57376 examples: {'rewards_train/chosen': '0.062725', 'rewards_train/rejected': '0.056397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0063281', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-165.32', 'loss/train': '0.70698', 'examples_per_second': '24.446', 'grad_norm': '90.5', 'counters/examples': 57376, 'counters/updates': 1793}
train stats after 57408 examples: {'rewards_train/chosen': '0.084039', 'rewards_train/rejected': '0.052133', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031906', 'logps_train/rejected': '-197.45', 'logps_train/chosen': '-181.72', 'loss/train': '0.69712', 'examples_per_second': '31.517', 'grad_norm': '91.5', 'counters/examples': 57408, 'counters/updates': 1794}
skipping logging after 57440 examples to avoid logging too frequently
train stats after 57472 examples: {'rewards_train/chosen': '0.063288', 'rewards_train/rejected': '0.030508', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03278', 'logps_train/rejected': '-162.9', 'logps_train/chosen': '-124.25', 'loss/train': '0.69711', 'examples_per_second': '24.37', 'grad_norm': '89', 'counters/examples': 57472, 'counters/updates': 1796}
train stats after 57504 examples: {'rewards_train/chosen': '0.00063472', 'rewards_train/rejected': '0.12864', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.128', 'logps_train/rejected': '-159.07', 'logps_train/chosen': '-166.07', 'loss/train': '0.7715', 'examples_per_second': '31.714', 'grad_norm': '90', 'counters/examples': 57504, 'counters/updates': 1797}
train stats after 57536 examples: {'rewards_train/chosen': '0.05939', 'rewards_train/rejected': '-0.0053225', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064712', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-133.03', 'loss/train': '0.67726', 'examples_per_second': '32.334', 'grad_norm': '113.5', 'counters/examples': 57536, 'counters/updates': 1798}
train stats after 57568 examples: {'rewards_train/chosen': '0.078296', 'rewards_train/rejected': '0.079886', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0015904', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-174.41', 'loss/train': '0.70612', 'examples_per_second': '31.251', 'grad_norm': '225', 'counters/examples': 57568, 'counters/updates': 1799}
train stats after 57600 examples: {'rewards_train/chosen': '-0.029548', 'rewards_train/rejected': '0.0033722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.03292', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-163.28', 'loss/train': '0.7267', 'examples_per_second': '32.251', 'grad_norm': '82.5', 'counters/examples': 57600, 'counters/updates': 1800}
train stats after 57632 examples: {'rewards_train/chosen': '-0.013577', 'rewards_train/rejected': '-0.027388', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013812', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-155.01', 'loss/train': '0.69711', 'examples_per_second': '31.979', 'grad_norm': '85', 'counters/examples': 57632, 'counters/updates': 1801}
train stats after 57664 examples: {'rewards_train/chosen': '0.047492', 'rewards_train/rejected': '0.0038918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043601', 'logps_train/rejected': '-147.91', 'logps_train/chosen': '-135.94', 'loss/train': '0.68307', 'examples_per_second': '30.585', 'grad_norm': '75.5', 'counters/examples': 57664, 'counters/updates': 1802}
train stats after 57696 examples: {'rewards_train/chosen': '0.076582', 'rewards_train/rejected': '0.063564', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013018', 'logps_train/rejected': '-114.87', 'logps_train/chosen': '-179.69', 'loss/train': '0.70276', 'examples_per_second': '30.507', 'grad_norm': '102', 'counters/examples': 57696, 'counters/updates': 1803}
train stats after 57728 examples: {'rewards_train/chosen': '0.07924', 'rewards_train/rejected': '0.01411', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06513', 'logps_train/rejected': '-145.33', 'logps_train/chosen': '-147.75', 'loss/train': '0.6763', 'examples_per_second': '31.386', 'grad_norm': '164', 'counters/examples': 57728, 'counters/updates': 1804}
train stats after 57760 examples: {'rewards_train/chosen': '-0.015249', 'rewards_train/rejected': '0.038037', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.053286', 'logps_train/rejected': '-160.98', 'logps_train/chosen': '-165.28', 'loss/train': '0.74165', 'examples_per_second': '29.69', 'grad_norm': '140', 'counters/examples': 57760, 'counters/updates': 1805}
train stats after 57792 examples: {'rewards_train/chosen': '0.049481', 'rewards_train/rejected': '0.012593', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036887', 'logps_train/rejected': '-166.68', 'logps_train/chosen': '-190.18', 'loss/train': '0.69969', 'examples_per_second': '31.546', 'grad_norm': '83', 'counters/examples': 57792, 'counters/updates': 1806}
train stats after 57824 examples: {'rewards_train/chosen': '-0.0014438', 'rewards_train/rejected': '-0.015563', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014119', 'logps_train/rejected': '-142.07', 'logps_train/chosen': '-196.71', 'loss/train': '0.70734', 'examples_per_second': '30.285', 'grad_norm': '67', 'counters/examples': 57824, 'counters/updates': 1807}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57888 examples: {'rewards_train/chosen': '0.10669', 'rewards_train/rejected': '0.010437', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096249', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-147.35', 'loss/train': '0.65255', 'examples_per_second': '31.055', 'grad_norm': '89', 'counters/examples': 57888, 'counters/updates': 1809}
skipping logging after 57920 examples to avoid logging too frequently
train stats after 57952 examples: {'rewards_train/chosen': '0.062953', 'rewards_train/rejected': '0.013715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049238', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-134.02', 'loss/train': '0.68258', 'examples_per_second': '36.126', 'grad_norm': '111.5', 'counters/examples': 57952, 'counters/updates': 1811}
train stats after 57984 examples: {'rewards_train/chosen': '0.0075397', 'rewards_train/rejected': '0.042495', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.034956', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-140.99', 'loss/train': '0.72135', 'examples_per_second': '31.165', 'grad_norm': '144', 'counters/examples': 57984, 'counters/updates': 1812}
skipping logging after 58016 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '0.011419', 'rewards_train/rejected': '-0.0075717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018991', 'logps_train/rejected': '-107.76', 'logps_train/chosen': '-131.11', 'loss/train': '0.69294', 'examples_per_second': '36.871', 'grad_norm': '72.5', 'counters/examples': 58048, 'counters/updates': 1814}
train stats after 58080 examples: {'rewards_train/chosen': '0.057411', 'rewards_train/rejected': '0.0073575', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050053', 'logps_train/rejected': '-147.64', 'logps_train/chosen': '-174.35', 'loss/train': '0.6826', 'examples_per_second': '29.992', 'grad_norm': '189', 'counters/examples': 58080, 'counters/updates': 1815}
train stats after 58112 examples: {'rewards_train/chosen': '0.12201', 'rewards_train/rejected': '0.05377', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.068235', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-123.63', 'loss/train': '0.67063', 'examples_per_second': '31.577', 'grad_norm': '47.5', 'counters/examples': 58112, 'counters/updates': 1816}
train stats after 58144 examples: {'rewards_train/chosen': '0.077144', 'rewards_train/rejected': '-0.043637', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12078', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-155.39', 'loss/train': '0.63953', 'examples_per_second': '30.28', 'grad_norm': '60', 'counters/examples': 58144, 'counters/updates': 1817}
skipping logging after 58176 examples to avoid logging too frequently
train stats after 58208 examples: {'rewards_train/chosen': '0.033545', 'rewards_train/rejected': '0.042588', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0090426', 'logps_train/rejected': '-104.01', 'logps_train/chosen': '-164.09', 'loss/train': '0.71463', 'examples_per_second': '34.951', 'grad_norm': '116', 'counters/examples': 58208, 'counters/updates': 1819}
train stats after 58240 examples: {'rewards_train/chosen': '0.023577', 'rewards_train/rejected': '0.014019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0095581', 'logps_train/rejected': '-106.39', 'logps_train/chosen': '-144.71', 'loss/train': '0.69627', 'examples_per_second': '31.545', 'grad_norm': '57.5', 'counters/examples': 58240, 'counters/updates': 1820}
train stats after 58272 examples: {'rewards_train/chosen': '0.094686', 'rewards_train/rejected': '0.085827', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0088591', 'logps_train/rejected': '-109.83', 'logps_train/chosen': '-162.04', 'loss/train': '0.70494', 'examples_per_second': '31.879', 'grad_norm': '96.5', 'counters/examples': 58272, 'counters/updates': 1821}
train stats after 58304 examples: {'rewards_train/chosen': '0.054371', 'rewards_train/rejected': '0.02625', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028121', 'logps_train/rejected': '-165.25', 'logps_train/chosen': '-147.12', 'loss/train': '0.69595', 'examples_per_second': '32.622', 'grad_norm': '374', 'counters/examples': 58304, 'counters/updates': 1822}
train stats after 58336 examples: {'rewards_train/chosen': '-0.046058', 'rewards_train/rejected': '-0.0092762', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036782', 'logps_train/rejected': '-137.67', 'logps_train/chosen': '-184.36', 'loss/train': '0.72569', 'examples_per_second': '31.529', 'grad_norm': '169', 'counters/examples': 58336, 'counters/updates': 1823}
train stats after 58368 examples: {'rewards_train/chosen': '0.04993', 'rewards_train/rejected': '0.00018444', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049746', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-147.74', 'loss/train': '0.67271', 'examples_per_second': '31.199', 'grad_norm': '197', 'counters/examples': 58368, 'counters/updates': 1824}
skipping logging after 58400 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '0.09108', 'rewards_train/rejected': '-0.017663', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10874', 'logps_train/rejected': '-110.48', 'logps_train/chosen': '-131.39', 'loss/train': '0.65107', 'examples_per_second': '32.917', 'grad_norm': '67.5', 'counters/examples': 58432, 'counters/updates': 1826}
skipping logging after 58464 examples to avoid logging too frequently
train stats after 58496 examples: {'rewards_train/chosen': '0.07652', 'rewards_train/rejected': '0.0033423', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073178', 'logps_train/rejected': '-118.41', 'logps_train/chosen': '-152.76', 'loss/train': '0.67438', 'examples_per_second': '30.653', 'grad_norm': '388', 'counters/examples': 58496, 'counters/updates': 1828}
train stats after 58528 examples: {'rewards_train/chosen': '0.050353', 'rewards_train/rejected': '-0.030512', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080865', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-154.43', 'loss/train': '0.66156', 'examples_per_second': '30.941', 'grad_norm': '73.5', 'counters/examples': 58528, 'counters/updates': 1829}
train stats after 58560 examples: {'rewards_train/chosen': '0.0078067', 'rewards_train/rejected': '0.08971', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.081903', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-175.54', 'loss/train': '0.75654', 'examples_per_second': '31.636', 'grad_norm': '234', 'counters/examples': 58560, 'counters/updates': 1830}
train stats after 58592 examples: {'rewards_train/chosen': '0.099233', 'rewards_train/rejected': '-0.064466', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1637', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-147.75', 'loss/train': '0.63271', 'examples_per_second': '31.57', 'grad_norm': '130', 'counters/examples': 58592, 'counters/updates': 1831}
train stats after 58624 examples: {'rewards_train/chosen': '0.017115', 'rewards_train/rejected': '-0.026655', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043771', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-154.35', 'loss/train': '0.67991', 'examples_per_second': '25.928', 'grad_norm': '81', 'counters/examples': 58624, 'counters/updates': 1832}
train stats after 58656 examples: {'rewards_train/chosen': '-0.092132', 'rewards_train/rejected': '-0.0017014', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.090431', 'logps_train/rejected': '-143.37', 'logps_train/chosen': '-144.3', 'loss/train': '0.75033', 'examples_per_second': '30.063', 'grad_norm': '118.5', 'counters/examples': 58656, 'counters/updates': 1833}
train stats after 58688 examples: {'rewards_train/chosen': '0.044704', 'rewards_train/rejected': '-0.028904', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073608', 'logps_train/rejected': '-153.74', 'logps_train/chosen': '-134.33', 'loss/train': '0.66777', 'examples_per_second': '30.122', 'grad_norm': '308', 'counters/examples': 58688, 'counters/updates': 1834}
train stats after 58720 examples: {'rewards_train/chosen': '0.017975', 'rewards_train/rejected': '0.0216', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0036251', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-144.45', 'loss/train': '0.70556', 'examples_per_second': '32.132', 'grad_norm': '128', 'counters/examples': 58720, 'counters/updates': 1835}
train stats after 58752 examples: {'rewards_train/chosen': '0.045395', 'rewards_train/rejected': '0.050304', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0049093', 'logps_train/rejected': '-123.03', 'logps_train/chosen': '-135.38', 'loss/train': '0.70827', 'examples_per_second': '30.476', 'grad_norm': '107', 'counters/examples': 58752, 'counters/updates': 1836}
train stats after 58784 examples: {'rewards_train/chosen': '0.011293', 'rewards_train/rejected': '0.031435', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020142', 'logps_train/rejected': '-130.88', 'logps_train/chosen': '-139.39', 'loss/train': '0.71736', 'examples_per_second': '30.648', 'grad_norm': '106.5', 'counters/examples': 58784, 'counters/updates': 1837}
train stats after 58816 examples: {'rewards_train/chosen': '0.059937', 'rewards_train/rejected': '-0.04024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10018', 'logps_train/rejected': '-184.54', 'logps_train/chosen': '-181.85', 'loss/train': '0.66376', 'examples_per_second': '31.568', 'grad_norm': '68.5', 'counters/examples': 58816, 'counters/updates': 1838}
train stats after 58848 examples: {'rewards_train/chosen': '0.0066398', 'rewards_train/rejected': '-0.041617', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048257', 'logps_train/rejected': '-142.79', 'logps_train/chosen': '-122.36', 'loss/train': '0.67617', 'examples_per_second': '30.19', 'grad_norm': '121', 'counters/examples': 58848, 'counters/updates': 1839}
train stats after 58880 examples: {'rewards_train/chosen': '0.017143', 'rewards_train/rejected': '0.018839', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0016963', 'logps_train/rejected': '-126.69', 'logps_train/chosen': '-155.3', 'loss/train': '0.70206', 'examples_per_second': '30.153', 'grad_norm': '79.5', 'counters/examples': 58880, 'counters/updates': 1840}
skipping logging after 58912 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '-0.010713', 'rewards_train/rejected': '-0.0063362', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0043767', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-153.19', 'loss/train': '0.705', 'examples_per_second': '31.699', 'grad_norm': '108.5', 'counters/examples': 58944, 'counters/updates': 1842}
train stats after 58976 examples: {'rewards_train/chosen': '0.061845', 'rewards_train/rejected': '0.1291', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.067251', 'logps_train/rejected': '-178.67', 'logps_train/chosen': '-192.42', 'loss/train': '0.73477', 'examples_per_second': '31.346', 'grad_norm': '72.5', 'counters/examples': 58976, 'counters/updates': 1843}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59040 examples: {'rewards_train/chosen': '0.011775', 'rewards_train/rejected': '-0.029349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041123', 'logps_train/rejected': '-183.12', 'logps_train/chosen': '-138.37', 'loss/train': '0.68127', 'examples_per_second': '30.983', 'grad_norm': '91', 'counters/examples': 59040, 'counters/updates': 1845}
train stats after 59072 examples: {'rewards_train/chosen': '0.025468', 'rewards_train/rejected': '0.039051', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.013583', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-139.51', 'loss/train': '0.70782', 'examples_per_second': '30.936', 'grad_norm': '151', 'counters/examples': 59072, 'counters/updates': 1846}
skipping logging after 59104 examples to avoid logging too frequently
train stats after 59136 examples: {'rewards_train/chosen': '0.11789', 'rewards_train/rejected': '0.012982', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10491', 'logps_train/rejected': '-145.49', 'logps_train/chosen': '-179.86', 'loss/train': '0.65723', 'examples_per_second': '31.881', 'grad_norm': '120.5', 'counters/examples': 59136, 'counters/updates': 1848}
train stats after 59168 examples: {'rewards_train/chosen': '0.099641', 'rewards_train/rejected': '0.038917', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.060724', 'logps_train/rejected': '-158.23', 'logps_train/chosen': '-155.87', 'loss/train': '0.67627', 'examples_per_second': '31.444', 'grad_norm': '95', 'counters/examples': 59168, 'counters/updates': 1849}
train stats after 59200 examples: {'rewards_train/chosen': '0.019791', 'rewards_train/rejected': '0.059078', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039287', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-137.47', 'loss/train': '0.7216', 'examples_per_second': '31.038', 'grad_norm': '106.5', 'counters/examples': 59200, 'counters/updates': 1850}
train stats after 59232 examples: {'rewards_train/chosen': '0.029546', 'rewards_train/rejected': '0.070912', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041366', 'logps_train/rejected': '-137.17', 'logps_train/chosen': '-131.1', 'loss/train': '0.73615', 'examples_per_second': '30.528', 'grad_norm': '149', 'counters/examples': 59232, 'counters/updates': 1851}
train stats after 59264 examples: {'rewards_train/chosen': '0.098415', 'rewards_train/rejected': '-0.017503', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11592', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-173.15', 'loss/train': '0.6444', 'examples_per_second': '30.471', 'grad_norm': '99.5', 'counters/examples': 59264, 'counters/updates': 1852}
skipping logging after 59296 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '0.010613', 'rewards_train/rejected': '-0.017732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028345', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-139.82', 'loss/train': '0.69003', 'examples_per_second': '30.515', 'grad_norm': '81', 'counters/examples': 59328, 'counters/updates': 1854}
train stats after 59360 examples: {'rewards_train/chosen': '0.072342', 'rewards_train/rejected': '0.011249', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061093', 'logps_train/rejected': '-131.33', 'logps_train/chosen': '-157.52', 'loss/train': '0.67149', 'examples_per_second': '31.651', 'grad_norm': '57.75', 'counters/examples': 59360, 'counters/updates': 1855}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59424 examples: {'rewards_train/chosen': '0.076125', 'rewards_train/rejected': '0.03226', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043865', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-166.73', 'loss/train': '0.6945', 'examples_per_second': '31.787', 'grad_norm': '102.5', 'counters/examples': 59424, 'counters/updates': 1857}
train stats after 59456 examples: {'rewards_train/chosen': '0.019033', 'rewards_train/rejected': '0.040633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0216', 'logps_train/rejected': '-175.12', 'logps_train/chosen': '-167.24', 'loss/train': '0.71996', 'examples_per_second': '30.937', 'grad_norm': '112', 'counters/examples': 59456, 'counters/updates': 1858}
train stats after 59488 examples: {'rewards_train/chosen': '0.04868', 'rewards_train/rejected': '0.040483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0081963', 'logps_train/rejected': '-220.81', 'logps_train/chosen': '-178.98', 'loss/train': '0.70355', 'examples_per_second': '31.581', 'grad_norm': '143', 'counters/examples': 59488, 'counters/updates': 1859}
train stats after 59520 examples: {'rewards_train/chosen': '-0.044044', 'rewards_train/rejected': '-0.018429', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025616', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-145.43', 'loss/train': '0.72752', 'examples_per_second': '31.29', 'grad_norm': '147', 'counters/examples': 59520, 'counters/updates': 1860}
train stats after 59552 examples: {'rewards_train/chosen': '0.011195', 'rewards_train/rejected': '-0.01707', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028265', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-119.87', 'loss/train': '0.68667', 'examples_per_second': '31.93', 'grad_norm': '73', 'counters/examples': 59552, 'counters/updates': 1861}
train stats after 59584 examples: {'rewards_train/chosen': '0.048805', 'rewards_train/rejected': '0.0072515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041553', 'logps_train/rejected': '-171.49', 'logps_train/chosen': '-169.13', 'loss/train': '0.68791', 'examples_per_second': '31.56', 'grad_norm': '103', 'counters/examples': 59584, 'counters/updates': 1862}
train stats after 59616 examples: {'rewards_train/chosen': '-0.0087556', 'rewards_train/rejected': '0.0040018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012757', 'logps_train/rejected': '-87.945', 'logps_train/chosen': '-154.35', 'loss/train': '0.72664', 'examples_per_second': '31.74', 'grad_norm': '72.5', 'counters/examples': 59616, 'counters/updates': 1863}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59680 examples: {'rewards_train/chosen': '0.050811', 'rewards_train/rejected': '0.052106', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0012949', 'logps_train/rejected': '-130.35', 'logps_train/chosen': '-164.09', 'loss/train': '0.70051', 'examples_per_second': '31.583', 'grad_norm': '107', 'counters/examples': 59680, 'counters/updates': 1865}
skipping logging after 59712 examples to avoid logging too frequently
train stats after 59744 examples: {'rewards_train/chosen': '0.014896', 'rewards_train/rejected': '-0.040728', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055624', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-132.75', 'loss/train': '0.67392', 'examples_per_second': '30.643', 'grad_norm': '127', 'counters/examples': 59744, 'counters/updates': 1867}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59808 examples: {'rewards_train/chosen': '0.087308', 'rewards_train/rejected': '-0.021818', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10913', 'logps_train/rejected': '-141.04', 'logps_train/chosen': '-167.19', 'loss/train': '0.66614', 'examples_per_second': '33.878', 'grad_norm': '94.5', 'counters/examples': 59808, 'counters/updates': 1869}
train stats after 59840 examples: {'rewards_train/chosen': '-0.017946', 'rewards_train/rejected': '0.041957', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.059903', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-124.84', 'loss/train': '0.73224', 'examples_per_second': '32.465', 'grad_norm': '132', 'counters/examples': 59840, 'counters/updates': 1870}
train stats after 59872 examples: {'rewards_train/chosen': '0.060076', 'rewards_train/rejected': '-0.014875', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07495', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-133.93', 'loss/train': '0.66666', 'examples_per_second': '30.175', 'grad_norm': '128', 'counters/examples': 59872, 'counters/updates': 1871}
train stats after 59904 examples: {'rewards_train/chosen': '0.055584', 'rewards_train/rejected': '0.0093037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046281', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-148.45', 'loss/train': '0.67792', 'examples_per_second': '30.052', 'grad_norm': '86.5', 'counters/examples': 59904, 'counters/updates': 1872}
train stats after 59936 examples: {'rewards_train/chosen': '0.091888', 'rewards_train/rejected': '0.0056129', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086275', 'logps_train/rejected': '-120.6', 'logps_train/chosen': '-149.25', 'loss/train': '0.65887', 'examples_per_second': '31.516', 'grad_norm': '110.5', 'counters/examples': 59936, 'counters/updates': 1873}
train stats after 59968 examples: {'rewards_train/chosen': '0.067573', 'rewards_train/rejected': '-0.033985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10156', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-138.45', 'loss/train': '0.65306', 'examples_per_second': '30.259', 'grad_norm': '105.5', 'counters/examples': 59968, 'counters/updates': 1874}
train stats after 60000 examples: {'rewards_train/chosen': '0.028409', 'rewards_train/rejected': '-0.054018', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082427', 'logps_train/rejected': '-155.41', 'logps_train/chosen': '-166.99', 'loss/train': '0.66606', 'examples_per_second': '30.281', 'grad_norm': '61.25', 'counters/examples': 60000, 'counters/updates': 1875}
Running evaluation after 60000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 60000: {'rewards_eval/chosen': '0.063259', 'rewards_eval/rejected': '0.030425', 'rewards_eval/accuracies': '0.53125', 'rewards_eval/margins': '0.032835', 'logps_eval/rejected': '-127.84', 'logps_eval/chosen': '-150.37', 'loss/eval': '0.69146'}
skipping save for non epoch
train stats after 60032 examples: {'rewards_train/chosen': '0.035467', 'rewards_train/rejected': '-0.010949', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.046416', 'logps_train/rejected': '-154.39', 'logps_train/chosen': '-176.52', 'loss/train': '0.6836', 'examples_per_second': '31.1', 'grad_norm': '111', 'counters/examples': 60032, 'counters/updates': 1876}
train stats after 60064 examples: {'rewards_train/chosen': '0.036192', 'rewards_train/rejected': '0.078014', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041822', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-183.86', 'loss/train': '0.72092', 'examples_per_second': '32.626', 'grad_norm': '99', 'counters/examples': 60064, 'counters/updates': 1877}
train stats after 60096 examples: {'rewards_train/chosen': '0.057486', 'rewards_train/rejected': '0.073796', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01631', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-153.39', 'loss/train': '0.71101', 'examples_per_second': '31.569', 'grad_norm': '88', 'counters/examples': 60096, 'counters/updates': 1878}
train stats after 60128 examples: {'rewards_train/chosen': '0.034859', 'rewards_train/rejected': '0.0033354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031523', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-163.76', 'loss/train': '0.69413', 'examples_per_second': '31.299', 'grad_norm': '167', 'counters/examples': 60128, 'counters/updates': 1879}
train stats after 60160 examples: {'rewards_train/chosen': '-0.034843', 'rewards_train/rejected': '-0.00024419', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034598', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-168.86', 'loss/train': '0.7259', 'examples_per_second': '32.756', 'grad_norm': '72.5', 'counters/examples': 60160, 'counters/updates': 1880}
train stats after 60192 examples: {'rewards_train/chosen': '0.0055008', 'rewards_train/rejected': '0.026597', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.021096', 'logps_train/rejected': '-135.26', 'logps_train/chosen': '-125.95', 'loss/train': '0.71231', 'examples_per_second': '32.561', 'grad_norm': '384', 'counters/examples': 60192, 'counters/updates': 1881}
train stats after 60224 examples: {'rewards_train/chosen': '0.057832', 'rewards_train/rejected': '0.042415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015416', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-198.78', 'loss/train': '0.70168', 'examples_per_second': '31.844', 'grad_norm': '74.5', 'counters/examples': 60224, 'counters/updates': 1882}
train stats after 60256 examples: {'rewards_train/chosen': '-0.018088', 'rewards_train/rejected': '0.040988', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.059076', 'logps_train/rejected': '-107.69', 'logps_train/chosen': '-123.69', 'loss/train': '0.73302', 'examples_per_second': '30.769', 'grad_norm': '86.5', 'counters/examples': 60256, 'counters/updates': 1883}
train stats after 60288 examples: {'rewards_train/chosen': '-0.0076689', 'rewards_train/rejected': '0.0273', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034969', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-148.82', 'loss/train': '0.72271', 'examples_per_second': '29.987', 'grad_norm': '82.5', 'counters/examples': 60288, 'counters/updates': 1884}
train stats after 60320 examples: {'rewards_train/chosen': '0.015215', 'rewards_train/rejected': '0.028076', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012861', 'logps_train/rejected': '-152.82', 'logps_train/chosen': '-136.92', 'loss/train': '0.70664', 'examples_per_second': '30.267', 'grad_norm': '81', 'counters/examples': 60320, 'counters/updates': 1885}
train stats after 60352 examples: {'rewards_train/chosen': '0.083202', 'rewards_train/rejected': '0.035409', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047793', 'logps_train/rejected': '-112.11', 'logps_train/chosen': '-139.78', 'loss/train': '0.68075', 'examples_per_second': '30.692', 'grad_norm': '63.75', 'counters/examples': 60352, 'counters/updates': 1886}
train stats after 60384 examples: {'rewards_train/chosen': '0.07754', 'rewards_train/rejected': '0.012471', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065069', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-154.26', 'loss/train': '0.68068', 'examples_per_second': '31.56', 'grad_norm': '89', 'counters/examples': 60384, 'counters/updates': 1887}
train stats after 60416 examples: {'rewards_train/chosen': '0.045806', 'rewards_train/rejected': '-0.01159', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057396', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-161.88', 'loss/train': '0.67547', 'examples_per_second': '31.568', 'grad_norm': '129', 'counters/examples': 60416, 'counters/updates': 1888}
train stats after 60448 examples: {'rewards_train/chosen': '0.09556', 'rewards_train/rejected': '0.011604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083956', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-151.41', 'loss/train': '0.67048', 'examples_per_second': '31.282', 'grad_norm': '78', 'counters/examples': 60448, 'counters/updates': 1889}
train stats after 60480 examples: {'rewards_train/chosen': '0.014795', 'rewards_train/rejected': '0.042996', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028201', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-115.05', 'loss/train': '0.71819', 'examples_per_second': '31.984', 'grad_norm': '85', 'counters/examples': 60480, 'counters/updates': 1890}
train stats after 60512 examples: {'rewards_train/chosen': '0.0015186', 'rewards_train/rejected': '-0.052903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054422', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-146.53', 'loss/train': '0.68841', 'examples_per_second': '31.87', 'grad_norm': '68', 'counters/examples': 60512, 'counters/updates': 1891}
train stats after 60544 examples: {'rewards_train/chosen': '0.017806', 'rewards_train/rejected': '0.024451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0066452', 'logps_train/rejected': '-113.07', 'logps_train/chosen': '-122.91', 'loss/train': '0.70892', 'examples_per_second': '32.651', 'grad_norm': '57.75', 'counters/examples': 60544, 'counters/updates': 1892}
train stats after 60576 examples: {'rewards_train/chosen': '0.052207', 'rewards_train/rejected': '0.036026', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016181', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-146.98', 'loss/train': '0.69258', 'examples_per_second': '30.826', 'grad_norm': '82', 'counters/examples': 60576, 'counters/updates': 1893}
train stats after 60608 examples: {'rewards_train/chosen': '-0.0080838', 'rewards_train/rejected': '0.010659', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.018743', 'logps_train/rejected': '-149.34', 'logps_train/chosen': '-150.84', 'loss/train': '0.71075', 'examples_per_second': '30.77', 'grad_norm': '92.5', 'counters/examples': 60608, 'counters/updates': 1894}
train stats after 60640 examples: {'rewards_train/chosen': '0.054549', 'rewards_train/rejected': '0.050297', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0042521', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-142.83', 'loss/train': '0.6983', 'examples_per_second': '30.418', 'grad_norm': '124.5', 'counters/examples': 60640, 'counters/updates': 1895}
train stats after 60672 examples: {'rewards_train/chosen': '0.091985', 'rewards_train/rejected': '-0.017316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1093', 'logps_train/rejected': '-85.03', 'logps_train/chosen': '-159.89', 'loss/train': '0.65361', 'examples_per_second': '32.305', 'grad_norm': '60', 'counters/examples': 60672, 'counters/updates': 1896}
train stats after 60704 examples: {'rewards_train/chosen': '-0.05352', 'rewards_train/rejected': '-0.036366', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017154', 'logps_train/rejected': '-163.17', 'logps_train/chosen': '-190.15', 'loss/train': '0.70849', 'examples_per_second': '30.154', 'grad_norm': '91', 'counters/examples': 60704, 'counters/updates': 1897}
train stats after 60736 examples: {'rewards_train/chosen': '0.030252', 'rewards_train/rejected': '-0.052149', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082401', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-130.07', 'loss/train': '0.67619', 'examples_per_second': '30.545', 'grad_norm': '81', 'counters/examples': 60736, 'counters/updates': 1898}
train stats after 60768 examples: {'rewards_train/chosen': '-0.012871', 'rewards_train/rejected': '-0.028308', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015437', 'logps_train/rejected': '-123.41', 'logps_train/chosen': '-143.22', 'loss/train': '0.69209', 'examples_per_second': '30.039', 'grad_norm': '66', 'counters/examples': 60768, 'counters/updates': 1899}
train stats after 60800 examples: {'rewards_train/chosen': '0.056409', 'rewards_train/rejected': '0.017293', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039116', 'logps_train/rejected': '-119.18', 'logps_train/chosen': '-126.3', 'loss/train': '0.67897', 'examples_per_second': '32.213', 'grad_norm': '62', 'counters/examples': 60800, 'counters/updates': 1900}
train stats after 60832 examples: {'rewards_train/chosen': '-0.015296', 'rewards_train/rejected': '0.030804', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0461', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-116.17', 'loss/train': '0.72421', 'examples_per_second': '31.575', 'grad_norm': '70.5', 'counters/examples': 60832, 'counters/updates': 1901}
train stats after 60864 examples: {'rewards_train/chosen': '0.094267', 'rewards_train/rejected': '-0.002621', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096888', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-158.36', 'loss/train': '0.6659', 'examples_per_second': '30.682', 'grad_norm': '95', 'counters/examples': 60864, 'counters/updates': 1902}
train stats after 60896 examples: {'rewards_train/chosen': '-0.052053', 'rewards_train/rejected': '-0.045156', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0068965', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-150.69', 'loss/train': '0.70482', 'examples_per_second': '31.567', 'grad_norm': '60.5', 'counters/examples': 60896, 'counters/updates': 1903}
train stats after 60928 examples: {'rewards_train/chosen': '0.0048826', 'rewards_train/rejected': '-0.028443', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033326', 'logps_train/rejected': '-144.47', 'logps_train/chosen': '-163.18', 'loss/train': '0.69424', 'examples_per_second': '30.192', 'grad_norm': '169', 'counters/examples': 60928, 'counters/updates': 1904}
train stats after 60960 examples: {'rewards_train/chosen': '0.044322', 'rewards_train/rejected': '-0.029885', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074206', 'logps_train/rejected': '-120.06', 'logps_train/chosen': '-153.04', 'loss/train': '0.66496', 'examples_per_second': '30.54', 'grad_norm': '89.5', 'counters/examples': 60960, 'counters/updates': 1905}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61024 examples: {'rewards_train/chosen': '-0.032273', 'rewards_train/rejected': '-0.017843', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01443', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-170.73', 'loss/train': '0.71198', 'examples_per_second': '37.381', 'grad_norm': '75.5', 'counters/examples': 61024, 'counters/updates': 1907}
skipping logging after 61056 examples to avoid logging too frequently
train stats after 61088 examples: {'rewards_train/chosen': '0.069153', 'rewards_train/rejected': '0.00049307', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06866', 'logps_train/rejected': '-163.1', 'logps_train/chosen': '-172.38', 'loss/train': '0.6798', 'examples_per_second': '30.522', 'grad_norm': '103.5', 'counters/examples': 61088, 'counters/updates': 1909}
train stats after 61120 examples: {'rewards_train/chosen': '0.081657', 'rewards_train/rejected': '0.013715', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067942', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-149.93', 'loss/train': '0.67057', 'examples_per_second': '32.417', 'grad_norm': '78', 'counters/examples': 61120, 'counters/updates': 1910}
train stats after 61152 examples: {'rewards_train/chosen': '0.069161', 'rewards_train/rejected': '-0.031511', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10067', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-179.5', 'loss/train': '0.65668', 'examples_per_second': '29.982', 'grad_norm': '159', 'counters/examples': 61152, 'counters/updates': 1911}
train stats after 61184 examples: {'rewards_train/chosen': '-0.055427', 'rewards_train/rejected': '-0.037289', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.018138', 'logps_train/rejected': '-158.91', 'logps_train/chosen': '-148.92', 'loss/train': '0.70988', 'examples_per_second': '30.061', 'grad_norm': '54.75', 'counters/examples': 61184, 'counters/updates': 1912}
train stats after 61216 examples: {'rewards_train/chosen': '0.020629', 'rewards_train/rejected': '0.061771', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041143', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-162.99', 'loss/train': '0.72309', 'examples_per_second': '30.73', 'grad_norm': '91', 'counters/examples': 61216, 'counters/updates': 1913}
train stats after 61248 examples: {'rewards_train/chosen': '-0.033576', 'rewards_train/rejected': '0.01979', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.053366', 'logps_train/rejected': '-126.07', 'logps_train/chosen': '-145.93', 'loss/train': '0.72834', 'examples_per_second': '32.339', 'grad_norm': '78', 'counters/examples': 61248, 'counters/updates': 1914}
skipping logging after 61280 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '0.077158', 'rewards_train/rejected': '0.076842', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00031553', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-163.72', 'loss/train': '0.70095', 'examples_per_second': '30.578', 'grad_norm': '180', 'counters/examples': 61312, 'counters/updates': 1916}
train stats after 61344 examples: {'rewards_train/chosen': '0.094744', 'rewards_train/rejected': '-0.0075778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10232', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-144.53', 'loss/train': '0.67215', 'examples_per_second': '30.935', 'grad_norm': '85.5', 'counters/examples': 61344, 'counters/updates': 1917}
train stats after 61376 examples: {'rewards_train/chosen': '0.0064218', 'rewards_train/rejected': '0.024723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018301', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-151.68', 'loss/train': '0.7107', 'examples_per_second': '30.031', 'grad_norm': '158', 'counters/examples': 61376, 'counters/updates': 1918}
skipping logging after 61408 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '-0.022406', 'rewards_train/rejected': '0.01619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.038597', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-144.24', 'loss/train': '0.72396', 'examples_per_second': '31.558', 'grad_norm': '119', 'counters/examples': 61440, 'counters/updates': 1920}
train stats after 61472 examples: {'rewards_train/chosen': '0.020719', 'rewards_train/rejected': '0.0047256', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015994', 'logps_train/rejected': '-134.03', 'logps_train/chosen': '-194.21', 'loss/train': '0.69684', 'examples_per_second': '30.089', 'grad_norm': '187', 'counters/examples': 61472, 'counters/updates': 1921}
train stats after 61504 examples: {'rewards_train/chosen': '-0.057118', 'rewards_train/rejected': '0.077936', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.13505', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-118.73', 'loss/train': '0.76973', 'examples_per_second': '30.552', 'grad_norm': '123.5', 'counters/examples': 61504, 'counters/updates': 1922}
train stats after 61536 examples: {'rewards_train/chosen': '0.00047477', 'rewards_train/rejected': '0.070355', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.069881', 'logps_train/rejected': '-162.04', 'logps_train/chosen': '-198.98', 'loss/train': '0.7454', 'examples_per_second': '30.085', 'grad_norm': '80', 'counters/examples': 61536, 'counters/updates': 1923}
train stats after 61568 examples: {'rewards_train/chosen': '0.045879', 'rewards_train/rejected': '-0.030372', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076251', 'logps_train/rejected': '-153.24', 'logps_train/chosen': '-169.3', 'loss/train': '0.66945', 'examples_per_second': '31.575', 'grad_norm': '79.5', 'counters/examples': 61568, 'counters/updates': 1924}
train stats after 61600 examples: {'rewards_train/chosen': '-3.7727e-05', 'rewards_train/rejected': '0.0036289', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0036666', 'logps_train/rejected': '-151.61', 'logps_train/chosen': '-148.61', 'loss/train': '0.70713', 'examples_per_second': '31.564', 'grad_norm': '70.5', 'counters/examples': 61600, 'counters/updates': 1925}
train stats after 61632 examples: {'rewards_train/chosen': '-0.037376', 'rewards_train/rejected': '0.060486', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.097863', 'logps_train/rejected': '-128.78', 'logps_train/chosen': '-140.66', 'loss/train': '0.76762', 'examples_per_second': '32.295', 'grad_norm': '103.5', 'counters/examples': 61632, 'counters/updates': 1926}
train stats after 61664 examples: {'rewards_train/chosen': '0.030201', 'rewards_train/rejected': '0.058882', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028681', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-118.38', 'loss/train': '0.72218', 'examples_per_second': '31.579', 'grad_norm': '105', 'counters/examples': 61664, 'counters/updates': 1927}
train stats after 61696 examples: {'rewards_train/chosen': '0.053564', 'rewards_train/rejected': '0.062979', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0094152', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-112.33', 'loss/train': '0.70448', 'examples_per_second': '32.134', 'grad_norm': '86', 'counters/examples': 61696, 'counters/updates': 1928}
train stats after 61728 examples: {'rewards_train/chosen': '0.10915', 'rewards_train/rejected': '0.046894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062258', 'logps_train/rejected': '-160.91', 'logps_train/chosen': '-185.44', 'loss/train': '0.67842', 'examples_per_second': '31.877', 'grad_norm': '116', 'counters/examples': 61728, 'counters/updates': 1929}
train stats after 61760 examples: {'rewards_train/chosen': '-0.042071', 'rewards_train/rejected': '-0.045479', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0034079', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-128.05', 'loss/train': '0.70284', 'examples_per_second': '33.015', 'grad_norm': '77.5', 'counters/examples': 61760, 'counters/updates': 1930}
train stats after 61792 examples: {'rewards_train/chosen': '0.013104', 'rewards_train/rejected': '0.018158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0050543', 'logps_train/rejected': '-99.071', 'logps_train/chosen': '-154.07', 'loss/train': '0.7085', 'examples_per_second': '32.069', 'grad_norm': '120.5', 'counters/examples': 61792, 'counters/updates': 1931}
train stats after 61824 examples: {'rewards_train/chosen': '0.050214', 'rewards_train/rejected': '0.02389', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026324', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-170.2', 'loss/train': '0.69169', 'examples_per_second': '31.539', 'grad_norm': '110.5', 'counters/examples': 61824, 'counters/updates': 1932}
train stats after 61856 examples: {'rewards_train/chosen': '0.09681', 'rewards_train/rejected': '-0.0078292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10464', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-161.14', 'loss/train': '0.66471', 'examples_per_second': '31.308', 'grad_norm': '181', 'counters/examples': 61856, 'counters/updates': 1933}
train stats after 61888 examples: {'rewards_train/chosen': '0.040667', 'rewards_train/rejected': '0.07654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035873', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-121.28', 'loss/train': '0.71905', 'examples_per_second': '31.489', 'grad_norm': '80', 'counters/examples': 61888, 'counters/updates': 1934}
train stats after 61920 examples: {'rewards_train/chosen': '0.021447', 'rewards_train/rejected': '0.071911', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.050464', 'logps_train/rejected': '-107.58', 'logps_train/chosen': '-151.75', 'loss/train': '0.72981', 'examples_per_second': '31.678', 'grad_norm': '106', 'counters/examples': 61920, 'counters/updates': 1935}
train stats after 61952 examples: {'rewards_train/chosen': '-0.028839', 'rewards_train/rejected': '0.047622', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.076461', 'logps_train/rejected': '-150.99', 'logps_train/chosen': '-181.96', 'loss/train': '0.73925', 'examples_per_second': '30.874', 'grad_norm': '152', 'counters/examples': 61952, 'counters/updates': 1936}
train stats after 61984 examples: {'rewards_train/chosen': '0.014906', 'rewards_train/rejected': '0.0041918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010714', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-188.76', 'loss/train': '0.6996', 'examples_per_second': '32.056', 'grad_norm': '115', 'counters/examples': 61984, 'counters/updates': 1937}
train stats after 62016 examples: {'rewards_train/chosen': '0.0048337', 'rewards_train/rejected': '-0.078063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082897', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-129.85', 'loss/train': '0.66668', 'examples_per_second': '31.528', 'grad_norm': '107', 'counters/examples': 62016, 'counters/updates': 1938}
train stats after 62048 examples: {'rewards_train/chosen': '0.076002', 'rewards_train/rejected': '0.053023', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022979', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-201.07', 'loss/train': '0.69606', 'examples_per_second': '30.856', 'grad_norm': '105', 'counters/examples': 62048, 'counters/updates': 1939}
train stats after 62080 examples: {'rewards_train/chosen': '-0.0090073', 'rewards_train/rejected': '-0.085391', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076384', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-158.21', 'loss/train': '0.67616', 'examples_per_second': '29.877', 'grad_norm': '143', 'counters/examples': 62080, 'counters/updates': 1940}
skipping logging after 62112 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '0.045365', 'rewards_train/rejected': '0.054705', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0093403', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-158.58', 'loss/train': '0.7086', 'examples_per_second': '31.548', 'grad_norm': '92.5', 'counters/examples': 62144, 'counters/updates': 1942}
train stats after 62176 examples: {'rewards_train/chosen': '0.013296', 'rewards_train/rejected': '0.12935', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11605', 'logps_train/rejected': '-160.28', 'logps_train/chosen': '-140.68', 'loss/train': '0.76426', 'examples_per_second': '29.966', 'grad_norm': '76.5', 'counters/examples': 62176, 'counters/updates': 1943}
train stats after 62208 examples: {'rewards_train/chosen': '0.1194', 'rewards_train/rejected': '0.083461', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.035941', 'logps_train/rejected': '-137.73', 'logps_train/chosen': '-164.47', 'loss/train': '0.69812', 'examples_per_second': '31.235', 'grad_norm': '195', 'counters/examples': 62208, 'counters/updates': 1944}
train stats after 62240 examples: {'rewards_train/chosen': '0.016794', 'rewards_train/rejected': '-0.014047', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030841', 'logps_train/rejected': '-158.41', 'logps_train/chosen': '-160.14', 'loss/train': '0.68828', 'examples_per_second': '31.539', 'grad_norm': '238', 'counters/examples': 62240, 'counters/updates': 1945}
train stats after 62272 examples: {'rewards_train/chosen': '0.047496', 'rewards_train/rejected': '0.042282', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0052135', 'logps_train/rejected': '-127.14', 'logps_train/chosen': '-144.1', 'loss/train': '0.69819', 'examples_per_second': '31.562', 'grad_norm': '59.25', 'counters/examples': 62272, 'counters/updates': 1946}
train stats after 62304 examples: {'rewards_train/chosen': '0.052431', 'rewards_train/rejected': '-0.045279', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097711', 'logps_train/rejected': '-145.38', 'logps_train/chosen': '-166.28', 'loss/train': '0.65828', 'examples_per_second': '31.57', 'grad_norm': '114.5', 'counters/examples': 62304, 'counters/updates': 1947}
train stats after 62336 examples: {'rewards_train/chosen': '0.020542', 'rewards_train/rejected': '0.10509', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.084548', 'logps_train/rejected': '-159.74', 'logps_train/chosen': '-126.86', 'loss/train': '0.75781', 'examples_per_second': '30.927', 'grad_norm': '190', 'counters/examples': 62336, 'counters/updates': 1948}
skipping logging after 62368 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '0.039008', 'rewards_train/rejected': '0.015572', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023436', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-161.5', 'loss/train': '0.69541', 'examples_per_second': '32.222', 'grad_norm': '114', 'counters/examples': 62400, 'counters/updates': 1950}
train stats after 62432 examples: {'rewards_train/chosen': '0.13045', 'rewards_train/rejected': '0.11543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01502', 'logps_train/rejected': '-160.48', 'logps_train/chosen': '-167.68', 'loss/train': '0.70446', 'examples_per_second': '31.496', 'grad_norm': '133', 'counters/examples': 62432, 'counters/updates': 1951}
train stats after 62464 examples: {'rewards_train/chosen': '0.025617', 'rewards_train/rejected': '0.099048', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.073432', 'logps_train/rejected': '-138.87', 'logps_train/chosen': '-175.57', 'loss/train': '0.73512', 'examples_per_second': '30.238', 'grad_norm': '110.5', 'counters/examples': 62464, 'counters/updates': 1952}
train stats after 62496 examples: {'rewards_train/chosen': '0.020601', 'rewards_train/rejected': '-0.0019217', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022523', 'logps_train/rejected': '-121.71', 'logps_train/chosen': '-145.89', 'loss/train': '0.69485', 'examples_per_second': '32.815', 'grad_norm': '177', 'counters/examples': 62496, 'counters/updates': 1953}
train stats after 62528 examples: {'rewards_train/chosen': '0.075587', 'rewards_train/rejected': '0.028565', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047022', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-164.95', 'loss/train': '0.6798', 'examples_per_second': '32.038', 'grad_norm': '144', 'counters/examples': 62528, 'counters/updates': 1954}
skipping logging after 62560 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '0.044404', 'rewards_train/rejected': '-0.02707', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071474', 'logps_train/rejected': '-146.48', 'logps_train/chosen': '-171', 'loss/train': '0.66934', 'examples_per_second': '31.551', 'grad_norm': '89', 'counters/examples': 62592, 'counters/updates': 1956}
train stats after 62624 examples: {'rewards_train/chosen': '0.067659', 'rewards_train/rejected': '-0.006777', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074436', 'logps_train/rejected': '-161.53', 'logps_train/chosen': '-168.18', 'loss/train': '0.67231', 'examples_per_second': '32.738', 'grad_norm': '87', 'counters/examples': 62624, 'counters/updates': 1957}
train stats after 62656 examples: {'rewards_train/chosen': '0.017743', 'rewards_train/rejected': '0.0085002', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0092426', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-166.75', 'loss/train': '0.70194', 'examples_per_second': '33.242', 'grad_norm': '137', 'counters/examples': 62656, 'counters/updates': 1958}
train stats after 62688 examples: {'rewards_train/chosen': '-0.051543', 'rewards_train/rejected': '-0.0018149', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049728', 'logps_train/rejected': '-139.56', 'logps_train/chosen': '-129.09', 'loss/train': '0.72551', 'examples_per_second': '31.111', 'grad_norm': '115', 'counters/examples': 62688, 'counters/updates': 1959}
train stats after 62720 examples: {'rewards_train/chosen': '0.041755', 'rewards_train/rejected': '0.036736', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0050196', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-150.74', 'loss/train': '0.69858', 'examples_per_second': '32.762', 'grad_norm': '149', 'counters/examples': 62720, 'counters/updates': 1960}
train stats after 62752 examples: {'rewards_train/chosen': '0.016217', 'rewards_train/rejected': '0.0075608', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0086565', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-138.18', 'loss/train': '0.69909', 'examples_per_second': '31.748', 'grad_norm': '115.5', 'counters/examples': 62752, 'counters/updates': 1961}
train stats after 62784 examples: {'rewards_train/chosen': '0.092658', 'rewards_train/rejected': '0.025468', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06719', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-143.93', 'loss/train': '0.6746', 'examples_per_second': '32.284', 'grad_norm': '141', 'counters/examples': 62784, 'counters/updates': 1962}
train stats after 62816 examples: {'rewards_train/chosen': '0.04164', 'rewards_train/rejected': '0.050939', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0092995', 'logps_train/rejected': '-177.67', 'logps_train/chosen': '-141.8', 'loss/train': '0.70811', 'examples_per_second': '31.176', 'grad_norm': '101.5', 'counters/examples': 62816, 'counters/updates': 1963}
train stats after 62848 examples: {'rewards_train/chosen': '-0.028347', 'rewards_train/rejected': '-0.027087', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0012598', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-104.89', 'loss/train': '0.70583', 'examples_per_second': '21.902', 'grad_norm': '70.5', 'counters/examples': 62848, 'counters/updates': 1964}
skipping logging after 62880 examples to avoid logging too frequently
train stats after 62912 examples: {'rewards_train/chosen': '0.033333', 'rewards_train/rejected': '-0.027976', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061309', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-140.17', 'loss/train': '0.67224', 'examples_per_second': '32.325', 'grad_norm': '55.5', 'counters/examples': 62912, 'counters/updates': 1966}
train stats after 62944 examples: {'rewards_train/chosen': '0.05664', 'rewards_train/rejected': '0.020264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036376', 'logps_train/rejected': '-150.55', 'logps_train/chosen': '-153.66', 'loss/train': '0.69066', 'examples_per_second': '24.334', 'grad_norm': '120.5', 'counters/examples': 62944, 'counters/updates': 1967}
train stats after 62976 examples: {'rewards_train/chosen': '-0.012533', 'rewards_train/rejected': '0.020237', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032771', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-174.27', 'loss/train': '0.73701', 'examples_per_second': '31.655', 'grad_norm': '81.5', 'counters/examples': 62976, 'counters/updates': 1968}
skipping logging after 63008 examples to avoid logging too frequently
train stats after 63040 examples: {'rewards_train/chosen': '0.10259', 'rewards_train/rejected': '0.017137', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085448', 'logps_train/rejected': '-162.17', 'logps_train/chosen': '-161.36', 'loss/train': '0.6673', 'examples_per_second': '32.58', 'grad_norm': '80', 'counters/examples': 63040, 'counters/updates': 1970}
train stats after 63072 examples: {'rewards_train/chosen': '-0.023254', 'rewards_train/rejected': '-0.0030641', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.02019', 'logps_train/rejected': '-127.49', 'logps_train/chosen': '-149.64', 'loss/train': '0.72007', 'examples_per_second': '31.131', 'grad_norm': '99', 'counters/examples': 63072, 'counters/updates': 1971}
train stats after 63104 examples: {'rewards_train/chosen': '0.028933', 'rewards_train/rejected': '-0.056565', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085499', 'logps_train/rejected': '-155.49', 'logps_train/chosen': '-146.25', 'loss/train': '0.66765', 'examples_per_second': '30.975', 'grad_norm': '184', 'counters/examples': 63104, 'counters/updates': 1972}
train stats after 63136 examples: {'rewards_train/chosen': '-0.02151', 'rewards_train/rejected': '0.0048463', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026357', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-145.63', 'loss/train': '0.71257', 'examples_per_second': '29.807', 'grad_norm': '72', 'counters/examples': 63136, 'counters/updates': 1973}
train stats after 63168 examples: {'rewards_train/chosen': '-0.0097082', 'rewards_train/rejected': '0.048119', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.057827', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-124.47', 'loss/train': '0.73151', 'examples_per_second': '31.331', 'grad_norm': '87.5', 'counters/examples': 63168, 'counters/updates': 1974}
train stats after 63200 examples: {'rewards_train/chosen': '0.03898', 'rewards_train/rejected': '-0.016439', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.055419', 'logps_train/rejected': '-134.85', 'logps_train/chosen': '-146.29', 'loss/train': '0.68763', 'examples_per_second': '31.003', 'grad_norm': '115.5', 'counters/examples': 63200, 'counters/updates': 1975}
skipping logging after 63232 examples to avoid logging too frequently
train stats after 63264 examples: {'rewards_train/chosen': '-0.0049834', 'rewards_train/rejected': '0.052667', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057651', 'logps_train/rejected': '-153.05', 'logps_train/chosen': '-153.26', 'loss/train': '0.73683', 'examples_per_second': '31.665', 'grad_norm': '85.5', 'counters/examples': 63264, 'counters/updates': 1977}
train stats after 63296 examples: {'rewards_train/chosen': '0.066254', 'rewards_train/rejected': '0.021439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044815', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-165.56', 'loss/train': '0.67868', 'examples_per_second': '32.686', 'grad_norm': '84.5', 'counters/examples': 63296, 'counters/updates': 1978}
train stats after 63328 examples: {'rewards_train/chosen': '-0.011883', 'rewards_train/rejected': '0.088957', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.10084', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-139.44', 'loss/train': '0.76886', 'examples_per_second': '31.695', 'grad_norm': '95', 'counters/examples': 63328, 'counters/updates': 1979}
skipping logging after 63360 examples to avoid logging too frequently
train stats after 63392 examples: {'rewards_train/chosen': '0.08249', 'rewards_train/rejected': '-0.08303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16552', 'logps_train/rejected': '-134.17', 'logps_train/chosen': '-176.94', 'loss/train': '0.62576', 'examples_per_second': '34.363', 'grad_norm': '85.5', 'counters/examples': 63392, 'counters/updates': 1981}
train stats after 63424 examples: {'rewards_train/chosen': '0.059779', 'rewards_train/rejected': '0.08197', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022191', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-142.68', 'loss/train': '0.71038', 'examples_per_second': '32.321', 'grad_norm': '121', 'counters/examples': 63424, 'counters/updates': 1982}
train stats after 63456 examples: {'rewards_train/chosen': '-0.043696', 'rewards_train/rejected': '-0.053462', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0097661', 'logps_train/rejected': '-133.4', 'logps_train/chosen': '-134.69', 'loss/train': '0.69822', 'examples_per_second': '31.397', 'grad_norm': '107.5', 'counters/examples': 63456, 'counters/updates': 1983}
train stats after 63488 examples: {'rewards_train/chosen': '-0.037936', 'rewards_train/rejected': '0.029214', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06715', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-148.68', 'loss/train': '0.73364', 'examples_per_second': '30.287', 'grad_norm': '79', 'counters/examples': 63488, 'counters/updates': 1984}
train stats after 63520 examples: {'rewards_train/chosen': '0.058958', 'rewards_train/rejected': '-0.0052595', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064218', 'logps_train/rejected': '-145.38', 'logps_train/chosen': '-142.2', 'loss/train': '0.67511', 'examples_per_second': '32.982', 'grad_norm': '101.5', 'counters/examples': 63520, 'counters/updates': 1985}
train stats after 63552 examples: {'rewards_train/chosen': '0.068341', 'rewards_train/rejected': '-0.0046897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073031', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-138.37', 'loss/train': '0.66515', 'examples_per_second': '31.256', 'grad_norm': '92', 'counters/examples': 63552, 'counters/updates': 1986}
train stats after 63584 examples: {'rewards_train/chosen': '0.018994', 'rewards_train/rejected': '0.0011786', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017816', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-138.41', 'loss/train': '0.69567', 'examples_per_second': '32.1', 'grad_norm': '66.5', 'counters/examples': 63584, 'counters/updates': 1987}
train stats after 63616 examples: {'rewards_train/chosen': '0.067146', 'rewards_train/rejected': '0.0703', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0031539', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-197.27', 'loss/train': '0.71486', 'examples_per_second': '31.636', 'grad_norm': '148', 'counters/examples': 63616, 'counters/updates': 1988}
skipping logging after 63648 examples to avoid logging too frequently
train stats after 63680 examples: {'rewards_train/chosen': '-0.008253', 'rewards_train/rejected': '0.021416', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029669', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-143.09', 'loss/train': '0.73316', 'examples_per_second': '30.8', 'grad_norm': '118.5', 'counters/examples': 63680, 'counters/updates': 1990}
train stats after 63712 examples: {'rewards_train/chosen': '0.01109', 'rewards_train/rejected': '0.021759', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010669', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-163.03', 'loss/train': '0.70573', 'examples_per_second': '32.193', 'grad_norm': '90', 'counters/examples': 63712, 'counters/updates': 1991}
train stats after 63744 examples: {'rewards_train/chosen': '-0.0021779', 'rewards_train/rejected': '0.050453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.052631', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-215.06', 'loss/train': '0.73206', 'examples_per_second': '33.161', 'grad_norm': '77', 'counters/examples': 63744, 'counters/updates': 1992}
train stats after 63776 examples: {'rewards_train/chosen': '-0.012709', 'rewards_train/rejected': '-0.062569', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04986', 'logps_train/rejected': '-169.4', 'logps_train/chosen': '-168.41', 'loss/train': '0.67761', 'examples_per_second': '31.823', 'grad_norm': '228', 'counters/examples': 63776, 'counters/updates': 1993}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63840 examples: {'rewards_train/chosen': '-0.035944', 'rewards_train/rejected': '0.020615', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.056558', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-153.49', 'loss/train': '0.72816', 'examples_per_second': '31.38', 'grad_norm': '68', 'counters/examples': 63840, 'counters/updates': 1995}
train stats after 63872 examples: {'rewards_train/chosen': '0.076946', 'rewards_train/rejected': '-0.096495', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17344', 'logps_train/rejected': '-155.93', 'logps_train/chosen': '-146.31', 'loss/train': '0.62989', 'examples_per_second': '32.81', 'grad_norm': '102.5', 'counters/examples': 63872, 'counters/updates': 1996}
train stats after 63904 examples: {'rewards_train/chosen': '-0.028999', 'rewards_train/rejected': '-0.028374', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00062532', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-122.9', 'loss/train': '0.70223', 'examples_per_second': '31.076', 'grad_norm': '85', 'counters/examples': 63904, 'counters/updates': 1997}
train stats after 63936 examples: {'rewards_train/chosen': '0.052068', 'rewards_train/rejected': '0.011329', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040738', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-161.82', 'loss/train': '0.68403', 'examples_per_second': '31.01', 'grad_norm': '181', 'counters/examples': 63936, 'counters/updates': 1998}
train stats after 63968 examples: {'rewards_train/chosen': '0.008144', 'rewards_train/rejected': '-0.039769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047913', 'logps_train/rejected': '-123.57', 'logps_train/chosen': '-168.73', 'loss/train': '0.67528', 'examples_per_second': '31.628', 'grad_norm': '100.5', 'counters/examples': 63968, 'counters/updates': 1999}
train stats after 64000 examples: {'rewards_train/chosen': '0.077241', 'rewards_train/rejected': '-0.033059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1103', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-173.66', 'loss/train': '0.64522', 'examples_per_second': '32.136', 'grad_norm': '71', 'counters/examples': 64000, 'counters/updates': 2000}
train stats after 64032 examples: {'rewards_train/chosen': '0.059935', 'rewards_train/rejected': '0.037469', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022467', 'logps_train/rejected': '-107.21', 'logps_train/chosen': '-155.09', 'loss/train': '0.69283', 'examples_per_second': '31.982', 'grad_norm': '89.5', 'counters/examples': 64032, 'counters/updates': 2001}
train stats after 64064 examples: {'rewards_train/chosen': '0.032067', 'rewards_train/rejected': '-0.029928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061995', 'logps_train/rejected': '-157.36', 'logps_train/chosen': '-172.26', 'loss/train': '0.67368', 'examples_per_second': '31.648', 'grad_norm': '69.5', 'counters/examples': 64064, 'counters/updates': 2002}
skipping logging after 64096 examples to avoid logging too frequently
train stats after 64128 examples: {'rewards_train/chosen': '0.035748', 'rewards_train/rejected': '0.028114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0076332', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-133.92', 'loss/train': '0.70556', 'examples_per_second': '31.606', 'grad_norm': '79.5', 'counters/examples': 64128, 'counters/updates': 2004}
train stats after 64160 examples: {'rewards_train/chosen': '0.074309', 'rewards_train/rejected': '-0.0023419', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076651', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-143.22', 'loss/train': '0.67278', 'examples_per_second': '32.227', 'grad_norm': '77.5', 'counters/examples': 64160, 'counters/updates': 2005}
train stats after 64192 examples: {'rewards_train/chosen': '0.030194', 'rewards_train/rejected': '0.11192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.081722', 'logps_train/rejected': '-142.31', 'logps_train/chosen': '-159.15', 'loss/train': '0.7627', 'examples_per_second': '25.135', 'grad_norm': '186', 'counters/examples': 64192, 'counters/updates': 2006}
train stats after 64224 examples: {'rewards_train/chosen': '0.052829', 'rewards_train/rejected': '0.042718', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010111', 'logps_train/rejected': '-138.49', 'logps_train/chosen': '-144.9', 'loss/train': '0.70403', 'examples_per_second': '31.708', 'grad_norm': '67.5', 'counters/examples': 64224, 'counters/updates': 2007}
skipping logging after 64256 examples to avoid logging too frequently
train stats after 64288 examples: {'rewards_train/chosen': '0.056046', 'rewards_train/rejected': '0.0044976', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051548', 'logps_train/rejected': '-112.44', 'logps_train/chosen': '-164.17', 'loss/train': '0.6821', 'examples_per_second': '31.588', 'grad_norm': '97', 'counters/examples': 64288, 'counters/updates': 2009}
train stats after 64320 examples: {'rewards_train/chosen': '-0.013942', 'rewards_train/rejected': '-0.0022447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011698', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-132.1', 'loss/train': '0.71239', 'examples_per_second': '32.732', 'grad_norm': '110', 'counters/examples': 64320, 'counters/updates': 2010}
train stats after 64352 examples: {'rewards_train/chosen': '0.00016693', 'rewards_train/rejected': '0.021618', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021451', 'logps_train/rejected': '-154.13', 'logps_train/chosen': '-157.39', 'loss/train': '0.71213', 'examples_per_second': '30.301', 'grad_norm': '169', 'counters/examples': 64352, 'counters/updates': 2011}
train stats after 64384 examples: {'rewards_train/chosen': '0.084683', 'rewards_train/rejected': '0.066765', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017919', 'logps_train/rejected': '-137.41', 'logps_train/chosen': '-155.75', 'loss/train': '0.68854', 'examples_per_second': '31.418', 'grad_norm': '76', 'counters/examples': 64384, 'counters/updates': 2012}
skipping logging after 64416 examples to avoid logging too frequently
train stats after 64448 examples: {'rewards_train/chosen': '-0.046187', 'rewards_train/rejected': '0.097061', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.14325', 'logps_train/rejected': '-189.87', 'logps_train/chosen': '-208.47', 'loss/train': '0.77811', 'examples_per_second': '32.196', 'grad_norm': '155', 'counters/examples': 64448, 'counters/updates': 2014}
train stats after 64480 examples: {'rewards_train/chosen': '0.041392', 'rewards_train/rejected': '0.0054773', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035915', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-121.2', 'loss/train': '0.68583', 'examples_per_second': '31.408', 'grad_norm': '68', 'counters/examples': 64480, 'counters/updates': 2015}
skipping logging after 64512 examples to avoid logging too frequently
train stats after 64544 examples: {'rewards_train/chosen': '0.08694', 'rewards_train/rejected': '0.012341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0746', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-145.08', 'loss/train': '0.66807', 'examples_per_second': '32.668', 'grad_norm': '53.25', 'counters/examples': 64544, 'counters/updates': 2017}
train stats after 64576 examples: {'rewards_train/chosen': '0.15213', 'rewards_train/rejected': '0.12733', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.024802', 'logps_train/rejected': '-149.09', 'logps_train/chosen': '-177.71', 'loss/train': '0.68694', 'examples_per_second': '31.555', 'grad_norm': '95', 'counters/examples': 64576, 'counters/updates': 2018}
skipping logging after 64608 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '0.050727', 'rewards_train/rejected': '0.016709', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034018', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-137.87', 'loss/train': '0.68484', 'examples_per_second': '31.002', 'grad_norm': '59.75', 'counters/examples': 64640, 'counters/updates': 2020}
skipping logging after 64672 examples to avoid logging too frequently
train stats after 64704 examples: {'rewards_train/chosen': '-0.017519', 'rewards_train/rejected': '-0.018875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0013564', 'logps_train/rejected': '-101.8', 'logps_train/chosen': '-108.92', 'loss/train': '0.69878', 'examples_per_second': '36.029', 'grad_norm': '71', 'counters/examples': 64704, 'counters/updates': 2022}
train stats after 64736 examples: {'rewards_train/chosen': '0.05274', 'rewards_train/rejected': '-0.012895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065634', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-143.89', 'loss/train': '0.67454', 'examples_per_second': '30.654', 'grad_norm': '97', 'counters/examples': 64736, 'counters/updates': 2023}
skipping logging after 64768 examples to avoid logging too frequently
train stats after 64800 examples: {'rewards_train/chosen': '0.0062968', 'rewards_train/rejected': '0.003695', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0026018', 'logps_train/rejected': '-148.28', 'logps_train/chosen': '-126.35', 'loss/train': '0.70753', 'examples_per_second': '32.25', 'grad_norm': '69', 'counters/examples': 64800, 'counters/updates': 2025}
train stats after 64832 examples: {'rewards_train/chosen': '0.064282', 'rewards_train/rejected': '0.034015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030267', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-161.87', 'loss/train': '0.68777', 'examples_per_second': '30.61', 'grad_norm': '86', 'counters/examples': 64832, 'counters/updates': 2026}
train stats after 64864 examples: {'rewards_train/chosen': '0.022637', 'rewards_train/rejected': '0.030392', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0077548', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-170.44', 'loss/train': '0.7171', 'examples_per_second': '32', 'grad_norm': '1328', 'counters/examples': 64864, 'counters/updates': 2027}
train stats after 64896 examples: {'rewards_train/chosen': '0.012602', 'rewards_train/rejected': '-0.014634', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027236', 'logps_train/rejected': '-190.01', 'logps_train/chosen': '-195.83', 'loss/train': '0.68636', 'examples_per_second': '30.996', 'grad_norm': '103.5', 'counters/examples': 64896, 'counters/updates': 2028}
skipping logging after 64928 examples to avoid logging too frequently
train stats after 64960 examples: {'rewards_train/chosen': '0.044799', 'rewards_train/rejected': '0.0327', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.012099', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-131.76', 'loss/train': '0.70328', 'examples_per_second': '31.581', 'grad_norm': '85', 'counters/examples': 64960, 'counters/updates': 2030}
skipping logging after 64992 examples to avoid logging too frequently
train stats after 65024 examples: {'rewards_train/chosen': '-0.054793', 'rewards_train/rejected': '0.061367', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.11616', 'logps_train/rejected': '-115.25', 'logps_train/chosen': '-131.58', 'loss/train': '0.77039', 'examples_per_second': '30.447', 'grad_norm': '135', 'counters/examples': 65024, 'counters/updates': 2032}
train stats after 65056 examples: {'rewards_train/chosen': '0.047678', 'rewards_train/rejected': '0.039687', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0079915', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-163.07', 'loss/train': '0.70019', 'examples_per_second': '31.045', 'grad_norm': '65', 'counters/examples': 65056, 'counters/updates': 2033}
train stats after 65088 examples: {'rewards_train/chosen': '-0.030283', 'rewards_train/rejected': '0.046599', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.076882', 'logps_train/rejected': '-145.58', 'logps_train/chosen': '-149.58', 'loss/train': '0.7405', 'examples_per_second': '30.944', 'grad_norm': '500', 'counters/examples': 65088, 'counters/updates': 2034}
train stats after 65120 examples: {'rewards_train/chosen': '0.023424', 'rewards_train/rejected': '-0.001809', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025233', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-174.93', 'loss/train': '0.68639', 'examples_per_second': '31.632', 'grad_norm': '221', 'counters/examples': 65120, 'counters/updates': 2035}
train stats after 65152 examples: {'rewards_train/chosen': '-0.0090762', 'rewards_train/rejected': '-0.043196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034119', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-141.8', 'loss/train': '0.68149', 'examples_per_second': '31.246', 'grad_norm': '58', 'counters/examples': 65152, 'counters/updates': 2036}
train stats after 65184 examples: {'rewards_train/chosen': '0.046822', 'rewards_train/rejected': '0.047823', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0010005', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-166.86', 'loss/train': '0.71069', 'examples_per_second': '31.286', 'grad_norm': '131', 'counters/examples': 65184, 'counters/updates': 2037}
train stats after 65216 examples: {'rewards_train/chosen': '0.1552', 'rewards_train/rejected': '0.0073984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1478', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-172.64', 'loss/train': '0.6426', 'examples_per_second': '30.928', 'grad_norm': '113.5', 'counters/examples': 65216, 'counters/updates': 2038}
train stats after 65248 examples: {'rewards_train/chosen': '0.020872', 'rewards_train/rejected': '0.0050896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015782', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-155.77', 'loss/train': '0.69394', 'examples_per_second': '32.189', 'grad_norm': '116.5', 'counters/examples': 65248, 'counters/updates': 2039}
train stats after 65280 examples: {'rewards_train/chosen': '0.086888', 'rewards_train/rejected': '0.034779', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052109', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-145.25', 'loss/train': '0.68788', 'examples_per_second': '31.594', 'grad_norm': '98.5', 'counters/examples': 65280, 'counters/updates': 2040}
skipping logging after 65312 examples to avoid logging too frequently
train stats after 65344 examples: {'rewards_train/chosen': '0.062087', 'rewards_train/rejected': '0.0031993', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058888', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-178.13', 'loss/train': '0.67864', 'examples_per_second': '30.036', 'grad_norm': '107', 'counters/examples': 65344, 'counters/updates': 2042}
train stats after 65376 examples: {'rewards_train/chosen': '0.066397', 'rewards_train/rejected': '0.048531', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017866', 'logps_train/rejected': '-104.95', 'logps_train/chosen': '-165.55', 'loss/train': '0.68984', 'examples_per_second': '30.162', 'grad_norm': '109.5', 'counters/examples': 65376, 'counters/updates': 2043}
train stats after 65408 examples: {'rewards_train/chosen': '0.080624', 'rewards_train/rejected': '-0.014797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09542', 'logps_train/rejected': '-154.51', 'logps_train/chosen': '-197.81', 'loss/train': '0.67083', 'examples_per_second': '31.587', 'grad_norm': '83.5', 'counters/examples': 65408, 'counters/updates': 2044}
train stats after 65440 examples: {'rewards_train/chosen': '-0.017358', 'rewards_train/rejected': '0.033786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.051144', 'logps_train/rejected': '-149.19', 'logps_train/chosen': '-161.76', 'loss/train': '0.73025', 'examples_per_second': '31.65', 'grad_norm': '98.5', 'counters/examples': 65440, 'counters/updates': 2045}
train stats after 65472 examples: {'rewards_train/chosen': '0.038527', 'rewards_train/rejected': '0.076655', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038127', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-182.41', 'loss/train': '0.72375', 'examples_per_second': '30.325', 'grad_norm': '96.5', 'counters/examples': 65472, 'counters/updates': 2046}
skipping logging after 65504 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '0.043475', 'rewards_train/rejected': '0.058722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015247', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-130.63', 'loss/train': '0.71016', 'examples_per_second': '32.397', 'grad_norm': '91', 'counters/examples': 65536, 'counters/updates': 2048}
train stats after 65568 examples: {'rewards_train/chosen': '-0.0074947', 'rewards_train/rejected': '-0.11219', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1047', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-158.01', 'loss/train': '0.65564', 'examples_per_second': '30.805', 'grad_norm': '72.5', 'counters/examples': 65568, 'counters/updates': 2049}
train stats after 65600 examples: {'rewards_train/chosen': '0.076158', 'rewards_train/rejected': '0.059129', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.017029', 'logps_train/rejected': '-114.3', 'logps_train/chosen': '-163.18', 'loss/train': '0.69602', 'examples_per_second': '31.666', 'grad_norm': '89.5', 'counters/examples': 65600, 'counters/updates': 2050}
train stats after 65632 examples: {'rewards_train/chosen': '0.026679', 'rewards_train/rejected': '0.025409', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0012695', 'logps_train/rejected': '-99.572', 'logps_train/chosen': '-138.65', 'loss/train': '0.7042', 'examples_per_second': '32.942', 'grad_norm': '147', 'counters/examples': 65632, 'counters/updates': 2051}
train stats after 65664 examples: {'rewards_train/chosen': '0.054967', 'rewards_train/rejected': '-0.034684', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089651', 'logps_train/rejected': '-139.28', 'logps_train/chosen': '-193.35', 'loss/train': '0.66131', 'examples_per_second': '32.168', 'grad_norm': '109', 'counters/examples': 65664, 'counters/updates': 2052}
train stats after 65696 examples: {'rewards_train/chosen': '0.018812', 'rewards_train/rejected': '0.017631', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.001181', 'logps_train/rejected': '-97.251', 'logps_train/chosen': '-120.83', 'loss/train': '0.69859', 'examples_per_second': '30.804', 'grad_norm': '62.25', 'counters/examples': 65696, 'counters/updates': 2053}
skipping logging after 65728 examples to avoid logging too frequently
train stats after 65760 examples: {'rewards_train/chosen': '0.046511', 'rewards_train/rejected': '-0.034813', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081324', 'logps_train/rejected': '-148.19', 'logps_train/chosen': '-116.28', 'loss/train': '0.66282', 'examples_per_second': '31.166', 'grad_norm': '80.5', 'counters/examples': 65760, 'counters/updates': 2055}
skipping logging after 65792 examples to avoid logging too frequently
train stats after 65824 examples: {'rewards_train/chosen': '0.098567', 'rewards_train/rejected': '0.063352', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.035215', 'logps_train/rejected': '-152.8', 'logps_train/chosen': '-186.29', 'loss/train': '0.68805', 'examples_per_second': '31.419', 'grad_norm': '72.5', 'counters/examples': 65824, 'counters/updates': 2057}
train stats after 65856 examples: {'rewards_train/chosen': '0.031027', 'rewards_train/rejected': '-0.028974', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.06', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-157.32', 'loss/train': '0.67428', 'examples_per_second': '30.64', 'grad_norm': '97', 'counters/examples': 65856, 'counters/updates': 2058}
train stats after 65888 examples: {'rewards_train/chosen': '0.060555', 'rewards_train/rejected': '0.0010128', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059542', 'logps_train/rejected': '-89.139', 'logps_train/chosen': '-130.59', 'loss/train': '0.67055', 'examples_per_second': '31.526', 'grad_norm': '53.75', 'counters/examples': 65888, 'counters/updates': 2059}
train stats after 65920 examples: {'rewards_train/chosen': '0.12139', 'rewards_train/rejected': '0.2147', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.093314', 'logps_train/rejected': '-173.15', 'logps_train/chosen': '-177.47', 'loss/train': '0.8042', 'examples_per_second': '31.574', 'grad_norm': '326', 'counters/examples': 65920, 'counters/updates': 2060}
train stats after 65952 examples: {'rewards_train/chosen': '0.0090051', 'rewards_train/rejected': '0.0034647', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0055404', 'logps_train/rejected': '-90.096', 'logps_train/chosen': '-145.34', 'loss/train': '0.70514', 'examples_per_second': '31.673', 'grad_norm': '108.5', 'counters/examples': 65952, 'counters/updates': 2061}
train stats after 65984 examples: {'rewards_train/chosen': '0.049798', 'rewards_train/rejected': '-0.020887', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070685', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-152.62', 'loss/train': '0.66529', 'examples_per_second': '30.157', 'grad_norm': '84.5', 'counters/examples': 65984, 'counters/updates': 2062}
train stats after 66016 examples: {'rewards_train/chosen': '0.073302', 'rewards_train/rejected': '0.040583', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03272', 'logps_train/rejected': '-114.14', 'logps_train/chosen': '-158.31', 'loss/train': '0.68702', 'examples_per_second': '31.2', 'grad_norm': '147', 'counters/examples': 66016, 'counters/updates': 2063}
train stats after 66048 examples: {'rewards_train/chosen': '-0.001716', 'rewards_train/rejected': '0.033256', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034972', 'logps_train/rejected': '-148.79', 'logps_train/chosen': '-170.96', 'loss/train': '0.72095', 'examples_per_second': '31.618', 'grad_norm': '93.5', 'counters/examples': 66048, 'counters/updates': 2064}
train stats after 66080 examples: {'rewards_train/chosen': '0.047964', 'rewards_train/rejected': '0.055018', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0070543', 'logps_train/rejected': '-137.51', 'logps_train/chosen': '-137.38', 'loss/train': '0.71268', 'examples_per_second': '30.773', 'grad_norm': '102.5', 'counters/examples': 66080, 'counters/updates': 2065}
train stats after 66112 examples: {'rewards_train/chosen': '0.012979', 'rewards_train/rejected': '0.0097244', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.003255', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-130.39', 'loss/train': '0.69784', 'examples_per_second': '31.283', 'grad_norm': '71', 'counters/examples': 66112, 'counters/updates': 2066}
train stats after 66144 examples: {'rewards_train/chosen': '0.049655', 'rewards_train/rejected': '0.099117', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.049462', 'logps_train/rejected': '-137.11', 'logps_train/chosen': '-141.06', 'loss/train': '0.72771', 'examples_per_second': '31.168', 'grad_norm': '92.5', 'counters/examples': 66144, 'counters/updates': 2067}
skipping logging after 66176 examples to avoid logging too frequently
train stats after 66208 examples: {'rewards_train/chosen': '0.075063', 'rewards_train/rejected': '0.066175', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.008888', 'logps_train/rejected': '-115.54', 'logps_train/chosen': '-133.18', 'loss/train': '0.69618', 'examples_per_second': '32.913', 'grad_norm': '56.5', 'counters/examples': 66208, 'counters/updates': 2069}
train stats after 66240 examples: {'rewards_train/chosen': '-0.013103', 'rewards_train/rejected': '0.0011985', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014301', 'logps_train/rejected': '-148.1', 'logps_train/chosen': '-156.28', 'loss/train': '0.7054', 'examples_per_second': '31.929', 'grad_norm': '85', 'counters/examples': 66240, 'counters/updates': 2070}
train stats after 66272 examples: {'rewards_train/chosen': '0.074158', 'rewards_train/rejected': '-0.0051869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079345', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-171.42', 'loss/train': '0.66705', 'examples_per_second': '30.291', 'grad_norm': '61.75', 'counters/examples': 66272, 'counters/updates': 2071}
train stats after 66304 examples: {'rewards_train/chosen': '0.090734', 'rewards_train/rejected': '0.11553', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024792', 'logps_train/rejected': '-123.79', 'logps_train/chosen': '-137.6', 'loss/train': '0.71978', 'examples_per_second': '30.072', 'grad_norm': '116', 'counters/examples': 66304, 'counters/updates': 2072}
skipping logging after 66336 examples to avoid logging too frequently
train stats after 66368 examples: {'rewards_train/chosen': '-0.010917', 'rewards_train/rejected': '-0.074571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063654', 'logps_train/rejected': '-157.69', 'logps_train/chosen': '-200.58', 'loss/train': '0.68187', 'examples_per_second': '32.82', 'grad_norm': '165', 'counters/examples': 66368, 'counters/updates': 2074}
train stats after 66400 examples: {'rewards_train/chosen': '0.055167', 'rewards_train/rejected': '0.10328', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.048117', 'logps_train/rejected': '-164.03', 'logps_train/chosen': '-152.45', 'loss/train': '0.73195', 'examples_per_second': '30.641', 'grad_norm': '59.5', 'counters/examples': 66400, 'counters/updates': 2075}
train stats after 66432 examples: {'rewards_train/chosen': '0.084999', 'rewards_train/rejected': '0.056565', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028434', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-144.24', 'loss/train': '0.68648', 'examples_per_second': '31.811', 'grad_norm': '110.5', 'counters/examples': 66432, 'counters/updates': 2076}
train stats after 66464 examples: {'rewards_train/chosen': '0.049143', 'rewards_train/rejected': '0.05142', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0022767', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-180.93', 'loss/train': '0.70491', 'examples_per_second': '31.491', 'grad_norm': '114', 'counters/examples': 66464, 'counters/updates': 2077}
train stats after 66496 examples: {'rewards_train/chosen': '-0.0060868', 'rewards_train/rejected': '-0.060719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054632', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-147.67', 'loss/train': '0.67576', 'examples_per_second': '31.587', 'grad_norm': '87', 'counters/examples': 66496, 'counters/updates': 2078}
train stats after 66528 examples: {'rewards_train/chosen': '0.031717', 'rewards_train/rejected': '0.0085546', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023162', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-176.55', 'loss/train': '0.69383', 'examples_per_second': '31.823', 'grad_norm': '88.5', 'counters/examples': 66528, 'counters/updates': 2079}
train stats after 66560 examples: {'rewards_train/chosen': '0.055746', 'rewards_train/rejected': '0.060389', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0046427', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-139.06', 'loss/train': '0.70033', 'examples_per_second': '30.587', 'grad_norm': '77.5', 'counters/examples': 66560, 'counters/updates': 2080}
train stats after 66592 examples: {'rewards_train/chosen': '0.016475', 'rewards_train/rejected': '-0.048466', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064941', 'logps_train/rejected': '-128.9', 'logps_train/chosen': '-126.21', 'loss/train': '0.66791', 'examples_per_second': '31.793', 'grad_norm': '73.5', 'counters/examples': 66592, 'counters/updates': 2081}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66656 examples: {'rewards_train/chosen': '-0.00749', 'rewards_train/rejected': '-0.0080155', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.00052546', 'logps_train/rejected': '-90.317', 'logps_train/chosen': '-105.95', 'loss/train': '0.70363', 'examples_per_second': '32.828', 'grad_norm': '93', 'counters/examples': 66656, 'counters/updates': 2083}
skipping logging after 66688 examples to avoid logging too frequently
train stats after 66720 examples: {'rewards_train/chosen': '0.014615', 'rewards_train/rejected': '0.024988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010373', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-160.69', 'loss/train': '0.71246', 'examples_per_second': '31.372', 'grad_norm': '79', 'counters/examples': 66720, 'counters/updates': 2085}
train stats after 66752 examples: {'rewards_train/chosen': '0.09057', 'rewards_train/rejected': '0.056052', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034518', 'logps_train/rejected': '-175.46', 'logps_train/chosen': '-182.9', 'loss/train': '0.68693', 'examples_per_second': '31.648', 'grad_norm': '97.5', 'counters/examples': 66752, 'counters/updates': 2086}
skipping logging after 66784 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '0.068038', 'rewards_train/rejected': '0.055128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01291', 'logps_train/rejected': '-167.71', 'logps_train/chosen': '-165.32', 'loss/train': '0.7028', 'examples_per_second': '31.179', 'grad_norm': '112', 'counters/examples': 66816, 'counters/updates': 2088}
train stats after 66848 examples: {'rewards_train/chosen': '-0.057502', 'rewards_train/rejected': '-0.040084', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017417', 'logps_train/rejected': '-115.03', 'logps_train/chosen': '-141.32', 'loss/train': '0.70959', 'examples_per_second': '33.09', 'grad_norm': '59.75', 'counters/examples': 66848, 'counters/updates': 2089}
train stats after 66880 examples: {'rewards_train/chosen': '-0.015439', 'rewards_train/rejected': '-0.013894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0015455', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-142.38', 'loss/train': '0.70836', 'examples_per_second': '31.633', 'grad_norm': '87.5', 'counters/examples': 66880, 'counters/updates': 2090}
skipping logging after 66912 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '-0.020346', 'rewards_train/rejected': '0.057018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.077364', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-146.9', 'loss/train': '0.74859', 'examples_per_second': '32.502', 'grad_norm': '88.5', 'counters/examples': 66944, 'counters/updates': 2092}
train stats after 66976 examples: {'rewards_train/chosen': '0.080397', 'rewards_train/rejected': '0.051285', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029112', 'logps_train/rejected': '-113.78', 'logps_train/chosen': '-147.5', 'loss/train': '0.70101', 'examples_per_second': '32.705', 'grad_norm': '110', 'counters/examples': 66976, 'counters/updates': 2093}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67040 examples: {'rewards_train/chosen': '0.057286', 'rewards_train/rejected': '0.033696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023591', 'logps_train/rejected': '-148.33', 'logps_train/chosen': '-149.29', 'loss/train': '0.69123', 'examples_per_second': '31.285', 'grad_norm': '100', 'counters/examples': 67040, 'counters/updates': 2095}
train stats after 67072 examples: {'rewards_train/chosen': '-0.019308', 'rewards_train/rejected': '0.044665', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063974', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-120.65', 'loss/train': '0.73572', 'examples_per_second': '31.346', 'grad_norm': '106.5', 'counters/examples': 67072, 'counters/updates': 2096}
train stats after 67104 examples: {'rewards_train/chosen': '-0.042294', 'rewards_train/rejected': '0.01225', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.054544', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-146.48', 'loss/train': '0.73072', 'examples_per_second': '31.826', 'grad_norm': '100.5', 'counters/examples': 67104, 'counters/updates': 2097}
train stats after 67136 examples: {'rewards_train/chosen': '0.054171', 'rewards_train/rejected': '0.043052', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011119', 'logps_train/rejected': '-141.64', 'logps_train/chosen': '-159.13', 'loss/train': '0.69713', 'examples_per_second': '30.398', 'grad_norm': '83', 'counters/examples': 67136, 'counters/updates': 2098}
train stats after 67168 examples: {'rewards_train/chosen': '-0.047975', 'rewards_train/rejected': '-0.017053', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030921', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-129.87', 'loss/train': '0.71546', 'examples_per_second': '31.636', 'grad_norm': '88', 'counters/examples': 67168, 'counters/updates': 2099}
train stats after 67200 examples: {'rewards_train/chosen': '0.0050294', 'rewards_train/rejected': '0.048529', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0435', 'logps_train/rejected': '-114.26', 'logps_train/chosen': '-116.06', 'loss/train': '0.72114', 'examples_per_second': '30.605', 'grad_norm': '117.5', 'counters/examples': 67200, 'counters/updates': 2100}
skipping logging after 67232 examples to avoid logging too frequently
train stats after 67264 examples: {'rewards_train/chosen': '-0.01396', 'rewards_train/rejected': '-0.027345', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013384', 'logps_train/rejected': '-123.62', 'logps_train/chosen': '-146.94', 'loss/train': '0.69597', 'examples_per_second': '31.622', 'grad_norm': '123.5', 'counters/examples': 67264, 'counters/updates': 2102}
skipping logging after 67296 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '0.0024622', 'rewards_train/rejected': '0.037263', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034801', 'logps_train/rejected': '-153.39', 'logps_train/chosen': '-130.09', 'loss/train': '0.72462', 'examples_per_second': '32.145', 'grad_norm': '112.5', 'counters/examples': 67328, 'counters/updates': 2104}
skipping logging after 67360 examples to avoid logging too frequently
train stats after 67392 examples: {'rewards_train/chosen': '0.058746', 'rewards_train/rejected': '0.035389', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023357', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-127.8', 'loss/train': '0.68758', 'examples_per_second': '36.079', 'grad_norm': '43.75', 'counters/examples': 67392, 'counters/updates': 2106}
train stats after 67424 examples: {'rewards_train/chosen': '0.058654', 'rewards_train/rejected': '-0.069908', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12856', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-152.27', 'loss/train': '0.65248', 'examples_per_second': '31.514', 'grad_norm': '106', 'counters/examples': 67424, 'counters/updates': 2107}
skipping logging after 67456 examples to avoid logging too frequently
train stats after 67488 examples: {'rewards_train/chosen': '-0.017058', 'rewards_train/rejected': '0.0026567', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019715', 'logps_train/rejected': '-141.03', 'logps_train/chosen': '-162.93', 'loss/train': '0.71178', 'examples_per_second': '30.635', 'grad_norm': '158', 'counters/examples': 67488, 'counters/updates': 2109}
train stats after 67520 examples: {'rewards_train/chosen': '-0.017456', 'rewards_train/rejected': '0.063221', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.080677', 'logps_train/rejected': '-151.79', 'logps_train/chosen': '-138.67', 'loss/train': '0.74957', 'examples_per_second': '31.394', 'grad_norm': '138', 'counters/examples': 67520, 'counters/updates': 2110}
train stats after 67552 examples: {'rewards_train/chosen': '0.033628', 'rewards_train/rejected': '0.0031543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030474', 'logps_train/rejected': '-101.33', 'logps_train/chosen': '-159.45', 'loss/train': '0.68296', 'examples_per_second': '31.209', 'grad_norm': '72.5', 'counters/examples': 67552, 'counters/updates': 2111}
train stats after 67584 examples: {'rewards_train/chosen': '0.004368', 'rewards_train/rejected': '0.0029343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0014337', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-130.74', 'loss/train': '0.69958', 'examples_per_second': '32.728', 'grad_norm': '90.5', 'counters/examples': 67584, 'counters/updates': 2112}
train stats after 67616 examples: {'rewards_train/chosen': '0.07316', 'rewards_train/rejected': '0.099915', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.026755', 'logps_train/rejected': '-176.84', 'logps_train/chosen': '-155.54', 'loss/train': '0.71339', 'examples_per_second': '31.633', 'grad_norm': '126', 'counters/examples': 67616, 'counters/updates': 2113}
train stats after 67648 examples: {'rewards_train/chosen': '0.087055', 'rewards_train/rejected': '0.030027', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057027', 'logps_train/rejected': '-92.999', 'logps_train/chosen': '-164.64', 'loss/train': '0.67521', 'examples_per_second': '32.149', 'grad_norm': '61.25', 'counters/examples': 67648, 'counters/updates': 2114}
train stats after 67680 examples: {'rewards_train/chosen': '0.13038', 'rewards_train/rejected': '0.008559', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12182', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-145.75', 'loss/train': '0.65837', 'examples_per_second': '30.537', 'grad_norm': '96.5', 'counters/examples': 67680, 'counters/updates': 2115}
train stats after 67712 examples: {'rewards_train/chosen': '0.051542', 'rewards_train/rejected': '0.11534', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063802', 'logps_train/rejected': '-174.48', 'logps_train/chosen': '-178.94', 'loss/train': '0.73772', 'examples_per_second': '31.324', 'grad_norm': '116.5', 'counters/examples': 67712, 'counters/updates': 2116}
train stats after 67744 examples: {'rewards_train/chosen': '0.015729', 'rewards_train/rejected': '0.033095', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017366', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-141.94', 'loss/train': '0.71017', 'examples_per_second': '31.605', 'grad_norm': '70.5', 'counters/examples': 67744, 'counters/updates': 2117}
train stats after 67776 examples: {'rewards_train/chosen': '-0.0012515', 'rewards_train/rejected': '0.10034', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10159', 'logps_train/rejected': '-140.01', 'logps_train/chosen': '-138.9', 'loss/train': '0.75755', 'examples_per_second': '31.619', 'grad_norm': '217', 'counters/examples': 67776, 'counters/updates': 2118}
train stats after 67808 examples: {'rewards_train/chosen': '0.0093567', 'rewards_train/rejected': '-0.030369', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039725', 'logps_train/rejected': '-171.27', 'logps_train/chosen': '-172.77', 'loss/train': '0.68386', 'examples_per_second': '31.639', 'grad_norm': '99', 'counters/examples': 67808, 'counters/updates': 2119}
train stats after 67840 examples: {'rewards_train/chosen': '0.042281', 'rewards_train/rejected': '0.0052495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037032', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-115.65', 'loss/train': '0.68188', 'examples_per_second': '31.775', 'grad_norm': '61.25', 'counters/examples': 67840, 'counters/updates': 2120}
skipping logging after 67872 examples to avoid logging too frequently
train stats after 67904 examples: {'rewards_train/chosen': '0.041724', 'rewards_train/rejected': '0.01233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029394', 'logps_train/rejected': '-95.578', 'logps_train/chosen': '-125', 'loss/train': '0.68551', 'examples_per_second': '34.572', 'grad_norm': '83.5', 'counters/examples': 67904, 'counters/updates': 2122}
train stats after 67936 examples: {'rewards_train/chosen': '0.083257', 'rewards_train/rejected': '-0.016271', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099528', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-142.93', 'loss/train': '0.65223', 'examples_per_second': '32.702', 'grad_norm': '244', 'counters/examples': 67936, 'counters/updates': 2123}
train stats after 67968 examples: {'rewards_train/chosen': '0.073827', 'rewards_train/rejected': '0.044949', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028879', 'logps_train/rejected': '-113.23', 'logps_train/chosen': '-230.69', 'loss/train': '0.69885', 'examples_per_second': '31.262', 'grad_norm': '91', 'counters/examples': 67968, 'counters/updates': 2124}
skipping logging after 68000 examples to avoid logging too frequently
train stats after 68032 examples: {'rewards_train/chosen': '0.024915', 'rewards_train/rejected': '0.034251', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0093365', 'logps_train/rejected': '-135.04', 'logps_train/chosen': '-157.89', 'loss/train': '0.71381', 'examples_per_second': '31.679', 'grad_norm': '68', 'counters/examples': 68032, 'counters/updates': 2126}
train stats after 68064 examples: {'rewards_train/chosen': '0.074648', 'rewards_train/rejected': '0.026419', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048228', 'logps_train/rejected': '-148.93', 'logps_train/chosen': '-184.6', 'loss/train': '0.69014', 'examples_per_second': '30.616', 'grad_norm': '476', 'counters/examples': 68064, 'counters/updates': 2127}
train stats after 68096 examples: {'rewards_train/chosen': '0.093039', 'rewards_train/rejected': '-0.0069885', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10003', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-155.27', 'loss/train': '0.65194', 'examples_per_second': '30.133', 'grad_norm': '97.5', 'counters/examples': 68096, 'counters/updates': 2128}
skipping logging after 68128 examples to avoid logging too frequently
train stats after 68160 examples: {'rewards_train/chosen': '-0.014868', 'rewards_train/rejected': '0.0037966', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018664', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-174.2', 'loss/train': '0.7112', 'examples_per_second': '32.624', 'grad_norm': '105', 'counters/examples': 68160, 'counters/updates': 2130}
train stats after 68192 examples: {'rewards_train/chosen': '0.068025', 'rewards_train/rejected': '-0.0072386', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075263', 'logps_train/rejected': '-157.18', 'logps_train/chosen': '-155.54', 'loss/train': '0.67467', 'examples_per_second': '30.154', 'grad_norm': '86', 'counters/examples': 68192, 'counters/updates': 2131}
train stats after 68224 examples: {'rewards_train/chosen': '0.027447', 'rewards_train/rejected': '0.026419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0010275', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-143.3', 'loss/train': '0.71042', 'examples_per_second': '30.549', 'grad_norm': '115', 'counters/examples': 68224, 'counters/updates': 2132}
train stats after 68256 examples: {'rewards_train/chosen': '0.084429', 'rewards_train/rejected': '0.02526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059169', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-143.71', 'loss/train': '0.67476', 'examples_per_second': '30.524', 'grad_norm': '76.5', 'counters/examples': 68256, 'counters/updates': 2133}
train stats after 68288 examples: {'rewards_train/chosen': '0.085004', 'rewards_train/rejected': '0.051531', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033473', 'logps_train/rejected': '-123.2', 'logps_train/chosen': '-149.85', 'loss/train': '0.69749', 'examples_per_second': '31.969', 'grad_norm': '100', 'counters/examples': 68288, 'counters/updates': 2134}
train stats after 68320 examples: {'rewards_train/chosen': '0.037568', 'rewards_train/rejected': '-0.044376', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081944', 'logps_train/rejected': '-179.55', 'logps_train/chosen': '-170.65', 'loss/train': '0.66972', 'examples_per_second': '24.449', 'grad_norm': '111', 'counters/examples': 68320, 'counters/updates': 2135}
train stats after 68352 examples: {'rewards_train/chosen': '0.036148', 'rewards_train/rejected': '0.00081769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03533', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-163.96', 'loss/train': '0.69254', 'examples_per_second': '31.642', 'grad_norm': '125', 'counters/examples': 68352, 'counters/updates': 2136}
train stats after 68384 examples: {'rewards_train/chosen': '-0.040905', 'rewards_train/rejected': '0.021954', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.062858', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-176.05', 'loss/train': '0.73555', 'examples_per_second': '33.006', 'grad_norm': '157', 'counters/examples': 68384, 'counters/updates': 2137}
train stats after 68416 examples: {'rewards_train/chosen': '-0.039568', 'rewards_train/rejected': '-0.0073938', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032174', 'logps_train/rejected': '-146.92', 'logps_train/chosen': '-193.31', 'loss/train': '0.71919', 'examples_per_second': '24.396', 'grad_norm': '145', 'counters/examples': 68416, 'counters/updates': 2138}
train stats after 68448 examples: {'rewards_train/chosen': '0.0064099', 'rewards_train/rejected': '0.01435', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0079399', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-110.07', 'loss/train': '0.70669', 'examples_per_second': '32.209', 'grad_norm': '46.25', 'counters/examples': 68448, 'counters/updates': 2139}
train stats after 68480 examples: {'rewards_train/chosen': '-0.014824', 'rewards_train/rejected': '-0.026084', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01126', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-126.24', 'loss/train': '0.69999', 'examples_per_second': '33.11', 'grad_norm': '81.5', 'counters/examples': 68480, 'counters/updates': 2140}
train stats after 68512 examples: {'rewards_train/chosen': '-0.010282', 'rewards_train/rejected': '0.040373', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050655', 'logps_train/rejected': '-159', 'logps_train/chosen': '-160.78', 'loss/train': '0.7429', 'examples_per_second': '30.135', 'grad_norm': '93.5', 'counters/examples': 68512, 'counters/updates': 2141}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68576 examples: {'rewards_train/chosen': '0.014897', 'rewards_train/rejected': '-0.054211', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069108', 'logps_train/rejected': '-162.23', 'logps_train/chosen': '-150.9', 'loss/train': '0.67395', 'examples_per_second': '30.621', 'grad_norm': '95.5', 'counters/examples': 68576, 'counters/updates': 2143}
train stats after 68608 examples: {'rewards_train/chosen': '0.066649', 'rewards_train/rejected': '0.0088229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057827', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-172.59', 'loss/train': '0.67642', 'examples_per_second': '31.565', 'grad_norm': '119.5', 'counters/examples': 68608, 'counters/updates': 2144}
train stats after 68640 examples: {'rewards_train/chosen': '-0.0064391', 'rewards_train/rejected': '0.068279', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.074718', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-139.5', 'loss/train': '0.74714', 'examples_per_second': '33.708', 'grad_norm': '171', 'counters/examples': 68640, 'counters/updates': 2145}
train stats after 68672 examples: {'rewards_train/chosen': '0.013793', 'rewards_train/rejected': '-0.014785', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028578', 'logps_train/rejected': '-117.6', 'logps_train/chosen': '-130.44', 'loss/train': '0.68835', 'examples_per_second': '30.41', 'grad_norm': '116.5', 'counters/examples': 68672, 'counters/updates': 2146}
skipping logging after 68704 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '0.10427', 'rewards_train/rejected': '0.084836', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019431', 'logps_train/rejected': '-145.12', 'logps_train/chosen': '-188.33', 'loss/train': '0.70183', 'examples_per_second': '31.613', 'grad_norm': '134', 'counters/examples': 68736, 'counters/updates': 2148}
skipping logging after 68768 examples to avoid logging too frequently
train stats after 68800 examples: {'rewards_train/chosen': '0.0015516', 'rewards_train/rejected': '-0.013387', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.014939', 'logps_train/rejected': '-156.7', 'logps_train/chosen': '-143.86', 'loss/train': '0.69818', 'examples_per_second': '32.602', 'grad_norm': '84.5', 'counters/examples': 68800, 'counters/updates': 2150}
skipping logging after 68832 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '0.001119', 'rewards_train/rejected': '-0.011096', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012215', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-157.39', 'loss/train': '0.69432', 'examples_per_second': '30.415', 'grad_norm': '118.5', 'counters/examples': 68864, 'counters/updates': 2152}
train stats after 68896 examples: {'rewards_train/chosen': '0.044237', 'rewards_train/rejected': '0.062338', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.018101', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-158.09', 'loss/train': '0.71354', 'examples_per_second': '30.944', 'grad_norm': '65.5', 'counters/examples': 68896, 'counters/updates': 2153}
train stats after 68928 examples: {'rewards_train/chosen': '0.04495', 'rewards_train/rejected': '0.033126', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011824', 'logps_train/rejected': '-136.21', 'logps_train/chosen': '-181.36', 'loss/train': '0.70076', 'examples_per_second': '31.588', 'grad_norm': '90.5', 'counters/examples': 68928, 'counters/updates': 2154}
skipping logging after 68960 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '-0.023297', 'rewards_train/rejected': '0.045774', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069072', 'logps_train/rejected': '-164.21', 'logps_train/chosen': '-166.81', 'loss/train': '0.73643', 'examples_per_second': '31.432', 'grad_norm': '96.5', 'counters/examples': 68992, 'counters/updates': 2156}
train stats after 69024 examples: {'rewards_train/chosen': '0.10136', 'rewards_train/rejected': '0.01295', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088414', 'logps_train/rejected': '-186.49', 'logps_train/chosen': '-183.26', 'loss/train': '0.66088', 'examples_per_second': '30.101', 'grad_norm': '166', 'counters/examples': 69024, 'counters/updates': 2157}
train stats after 69056 examples: {'rewards_train/chosen': '0.055881', 'rewards_train/rejected': '-0.015181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071062', 'logps_train/rejected': '-117.38', 'logps_train/chosen': '-134.02', 'loss/train': '0.66452', 'examples_per_second': '32.72', 'grad_norm': '290', 'counters/examples': 69056, 'counters/updates': 2158}
train stats after 69088 examples: {'rewards_train/chosen': '0.0056486', 'rewards_train/rejected': '-0.026499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032148', 'logps_train/rejected': '-180.87', 'logps_train/chosen': '-155.24', 'loss/train': '0.69643', 'examples_per_second': '30.088', 'grad_norm': '88', 'counters/examples': 69088, 'counters/updates': 2159}
train stats after 69120 examples: {'rewards_train/chosen': '-0.028983', 'rewards_train/rejected': '-0.013471', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015511', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-133.68', 'loss/train': '0.7095', 'examples_per_second': '30.479', 'grad_norm': '113.5', 'counters/examples': 69120, 'counters/updates': 2160}
skipping logging after 69152 examples to avoid logging too frequently
train stats after 69184 examples: {'rewards_train/chosen': '0.038751', 'rewards_train/rejected': '0.13227', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.093522', 'logps_train/rejected': '-189.13', 'logps_train/chosen': '-211.34', 'loss/train': '0.75771', 'examples_per_second': '31.601', 'grad_norm': '145', 'counters/examples': 69184, 'counters/updates': 2162}
skipping logging after 69216 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '-0.028685', 'rewards_train/rejected': '0.014715', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0434', 'logps_train/rejected': '-141.74', 'logps_train/chosen': '-111.66', 'loss/train': '0.72509', 'examples_per_second': '30.751', 'grad_norm': '88', 'counters/examples': 69248, 'counters/updates': 2164}
train stats after 69280 examples: {'rewards_train/chosen': '0.0038934', 'rewards_train/rejected': '-0.023418', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027311', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-137.46', 'loss/train': '0.69136', 'examples_per_second': '31.684', 'grad_norm': '80.5', 'counters/examples': 69280, 'counters/updates': 2165}
train stats after 69312 examples: {'rewards_train/chosen': '-0.019934', 'rewards_train/rejected': '0.020735', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040669', 'logps_train/rejected': '-188.87', 'logps_train/chosen': '-196.84', 'loss/train': '0.73285', 'examples_per_second': '31.572', 'grad_norm': '84.5', 'counters/examples': 69312, 'counters/updates': 2166}
train stats after 69344 examples: {'rewards_train/chosen': '0.002705', 'rewards_train/rejected': '-0.025459', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028164', 'logps_train/rejected': '-135.28', 'logps_train/chosen': '-160.18', 'loss/train': '0.68643', 'examples_per_second': '30.109', 'grad_norm': '82.5', 'counters/examples': 69344, 'counters/updates': 2167}
train stats after 69376 examples: {'rewards_train/chosen': '-0.049042', 'rewards_train/rejected': '-0.072673', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023631', 'logps_train/rejected': '-122.73', 'logps_train/chosen': '-118.44', 'loss/train': '0.68914', 'examples_per_second': '32.737', 'grad_norm': '78.5', 'counters/examples': 69376, 'counters/updates': 2168}
train stats after 69408 examples: {'rewards_train/chosen': '0.087034', 'rewards_train/rejected': '0.11644', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029411', 'logps_train/rejected': '-125.66', 'logps_train/chosen': '-158.54', 'loss/train': '0.72577', 'examples_per_second': '30.745', 'grad_norm': '936', 'counters/examples': 69408, 'counters/updates': 2169}
train stats after 69440 examples: {'rewards_train/chosen': '0.050862', 'rewards_train/rejected': '0.06994', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019077', 'logps_train/rejected': '-148.15', 'logps_train/chosen': '-145.63', 'loss/train': '0.72473', 'examples_per_second': '30.704', 'grad_norm': '125', 'counters/examples': 69440, 'counters/updates': 2170}
train stats after 69472 examples: {'rewards_train/chosen': '0.096875', 'rewards_train/rejected': '0.016905', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07997', 'logps_train/rejected': '-161.9', 'logps_train/chosen': '-192.49', 'loss/train': '0.66323', 'examples_per_second': '31.006', 'grad_norm': '98.5', 'counters/examples': 69472, 'counters/updates': 2171}
skipping logging after 69504 examples to avoid logging too frequently
train stats after 69536 examples: {'rewards_train/chosen': '-0.045688', 'rewards_train/rejected': '-0.016668', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02902', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-187.85', 'loss/train': '0.73077', 'examples_per_second': '30.589', 'grad_norm': '104.5', 'counters/examples': 69536, 'counters/updates': 2173}
train stats after 69568 examples: {'rewards_train/chosen': '0.038487', 'rewards_train/rejected': '0.053223', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014736', 'logps_train/rejected': '-158.47', 'logps_train/chosen': '-158.79', 'loss/train': '0.70853', 'examples_per_second': '31.311', 'grad_norm': '114', 'counters/examples': 69568, 'counters/updates': 2174}
train stats after 69600 examples: {'rewards_train/chosen': '0.02093', 'rewards_train/rejected': '-0.0078046', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028734', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-154.97', 'loss/train': '0.69253', 'examples_per_second': '32.446', 'grad_norm': '74.5', 'counters/examples': 69600, 'counters/updates': 2175}
skipping logging after 69632 examples to avoid logging too frequently
train stats after 69664 examples: {'rewards_train/chosen': '0.10062', 'rewards_train/rejected': '0.020641', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079981', 'logps_train/rejected': '-81.128', 'logps_train/chosen': '-142.31', 'loss/train': '0.66553', 'examples_per_second': '31.552', 'grad_norm': '114.5', 'counters/examples': 69664, 'counters/updates': 2177}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69728 examples: {'rewards_train/chosen': '0.085333', 'rewards_train/rejected': '0.044985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040349', 'logps_train/rejected': '-170.23', 'logps_train/chosen': '-171.06', 'loss/train': '0.68168', 'examples_per_second': '31.793', 'grad_norm': '124', 'counters/examples': 69728, 'counters/updates': 2179}
train stats after 69760 examples: {'rewards_train/chosen': '0.13987', 'rewards_train/rejected': '-0.037409', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17728', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-114.25', 'loss/train': '0.63285', 'examples_per_second': '25.938', 'grad_norm': '61.5', 'counters/examples': 69760, 'counters/updates': 2180}
train stats after 69792 examples: {'rewards_train/chosen': '0.077526', 'rewards_train/rejected': '-0.019987', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.097514', 'logps_train/rejected': '-150.43', 'logps_train/chosen': '-133.85', 'loss/train': '0.66727', 'examples_per_second': '31.903', 'grad_norm': '93.5', 'counters/examples': 69792, 'counters/updates': 2181}
train stats after 69824 examples: {'rewards_train/chosen': '0.064524', 'rewards_train/rejected': '-0.0034013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067926', 'logps_train/rejected': '-141.53', 'logps_train/chosen': '-157.21', 'loss/train': '0.66831', 'examples_per_second': '31.537', 'grad_norm': '53.25', 'counters/examples': 69824, 'counters/updates': 2182}
train stats after 69856 examples: {'rewards_train/chosen': '0.063641', 'rewards_train/rejected': '0.027641', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-152.6', 'loss/train': '0.68321', 'examples_per_second': '31.246', 'grad_norm': '62', 'counters/examples': 69856, 'counters/updates': 2183}
skipping logging after 69888 examples to avoid logging too frequently
train stats after 69920 examples: {'rewards_train/chosen': '0.054437', 'rewards_train/rejected': '-0.0087232', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06316', 'logps_train/rejected': '-99.542', 'logps_train/chosen': '-161.91', 'loss/train': '0.66965', 'examples_per_second': '31.932', 'grad_norm': '382', 'counters/examples': 69920, 'counters/updates': 2185}
train stats after 69952 examples: {'rewards_train/chosen': '-0.04265', 'rewards_train/rejected': '0.01434', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.05699', 'logps_train/rejected': '-131.59', 'logps_train/chosen': '-159.47', 'loss/train': '0.74429', 'examples_per_second': '30.101', 'grad_norm': '97', 'counters/examples': 69952, 'counters/updates': 2186}
skipping logging after 69984 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '0.058782', 'rewards_train/rejected': '0.017399', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041382', 'logps_train/rejected': '-96.003', 'logps_train/chosen': '-119.48', 'loss/train': '0.67813', 'examples_per_second': '32.616', 'grad_norm': '83', 'counters/examples': 70016, 'counters/updates': 2188}
train stats after 70048 examples: {'rewards_train/chosen': '0.016838', 'rewards_train/rejected': '0.025816', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0089782', 'logps_train/rejected': '-156.6', 'logps_train/chosen': '-154.85', 'loss/train': '0.71009', 'examples_per_second': '30.076', 'grad_norm': '114.5', 'counters/examples': 70048, 'counters/updates': 2189}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70112 examples: {'rewards_train/chosen': '-0.047749', 'rewards_train/rejected': '0.02846', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.076209', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-112.42', 'loss/train': '0.75109', 'examples_per_second': '38.309', 'grad_norm': '53.25', 'counters/examples': 70112, 'counters/updates': 2191}
train stats after 70144 examples: {'rewards_train/chosen': '0.095303', 'rewards_train/rejected': '-0.012378', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10768', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-202.84', 'loss/train': '0.65046', 'examples_per_second': '30.036', 'grad_norm': '85', 'counters/examples': 70144, 'counters/updates': 2192}
train stats after 70176 examples: {'rewards_train/chosen': '0.022796', 'rewards_train/rejected': '0.056502', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033706', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-165.9', 'loss/train': '0.72646', 'examples_per_second': '32.555', 'grad_norm': '93.5', 'counters/examples': 70176, 'counters/updates': 2193}
train stats after 70208 examples: {'rewards_train/chosen': '-0.020692', 'rewards_train/rejected': '0.10687', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.12756', 'logps_train/rejected': '-156.64', 'logps_train/chosen': '-132.16', 'loss/train': '0.76884', 'examples_per_second': '31.056', 'grad_norm': '80', 'counters/examples': 70208, 'counters/updates': 2194}
train stats after 70240 examples: {'rewards_train/chosen': '-0.018731', 'rewards_train/rejected': '-0.047328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028598', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-147.78', 'loss/train': '0.69045', 'examples_per_second': '29.948', 'grad_norm': '508', 'counters/examples': 70240, 'counters/updates': 2195}
train stats after 70272 examples: {'rewards_train/chosen': '0.061232', 'rewards_train/rejected': '0.033649', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.027583', 'logps_train/rejected': '-135.4', 'logps_train/chosen': '-138.62', 'loss/train': '0.69263', 'examples_per_second': '32.342', 'grad_norm': '58.25', 'counters/examples': 70272, 'counters/updates': 2196}
skipping logging after 70304 examples to avoid logging too frequently
train stats after 70336 examples: {'rewards_train/chosen': '0.17643', 'rewards_train/rejected': '-0.033087', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20952', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-164.13', 'loss/train': '0.61117', 'examples_per_second': '35.947', 'grad_norm': '73.5', 'counters/examples': 70336, 'counters/updates': 2198}
skipping logging after 70368 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '0.032615', 'rewards_train/rejected': '0.03704', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0044248', 'logps_train/rejected': '-120.75', 'logps_train/chosen': '-160.68', 'loss/train': '0.70702', 'examples_per_second': '30.307', 'grad_norm': '88', 'counters/examples': 70400, 'counters/updates': 2200}
skipping logging after 70432 examples to avoid logging too frequently
train stats after 70464 examples: {'rewards_train/chosen': '-0.034673', 'rewards_train/rejected': '0.047313', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.081986', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-157.86', 'loss/train': '0.74825', 'examples_per_second': '30.099', 'grad_norm': '87', 'counters/examples': 70464, 'counters/updates': 2202}
train stats after 70496 examples: {'rewards_train/chosen': '0.0095516', 'rewards_train/rejected': '-0.0046602', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014212', 'logps_train/rejected': '-139.65', 'logps_train/chosen': '-180.15', 'loss/train': '0.70118', 'examples_per_second': '30.003', 'grad_norm': '61.25', 'counters/examples': 70496, 'counters/updates': 2203}
train stats after 70528 examples: {'rewards_train/chosen': '0.10495', 'rewards_train/rejected': '0.006001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098952', 'logps_train/rejected': '-139.59', 'logps_train/chosen': '-159.03', 'loss/train': '0.65279', 'examples_per_second': '30.441', 'grad_norm': '62.75', 'counters/examples': 70528, 'counters/updates': 2204}
train stats after 70560 examples: {'rewards_train/chosen': '-0.051346', 'rewards_train/rejected': '0.0075645', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.058911', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-168.99', 'loss/train': '0.73707', 'examples_per_second': '30.057', 'grad_norm': '94.5', 'counters/examples': 70560, 'counters/updates': 2205}
train stats after 70592 examples: {'rewards_train/chosen': '0.075194', 'rewards_train/rejected': '0.025922', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049272', 'logps_train/rejected': '-139.12', 'logps_train/chosen': '-151.87', 'loss/train': '0.68427', 'examples_per_second': '32.377', 'grad_norm': '193', 'counters/examples': 70592, 'counters/updates': 2206}
train stats after 70624 examples: {'rewards_train/chosen': '0.03548', 'rewards_train/rejected': '0.045118', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0096381', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-172.81', 'loss/train': '0.72485', 'examples_per_second': '31.498', 'grad_norm': '176', 'counters/examples': 70624, 'counters/updates': 2207}
train stats after 70656 examples: {'rewards_train/chosen': '0.01024', 'rewards_train/rejected': '0.05245', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.04221', 'logps_train/rejected': '-134.87', 'logps_train/chosen': '-176.63', 'loss/train': '0.73256', 'examples_per_second': '31.189', 'grad_norm': '199', 'counters/examples': 70656, 'counters/updates': 2208}
skipping logging after 70688 examples to avoid logging too frequently
train stats after 70720 examples: {'rewards_train/chosen': '0.16802', 'rewards_train/rejected': '0.015253', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15276', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-161.03', 'loss/train': '0.63633', 'examples_per_second': '32.074', 'grad_norm': '221', 'counters/examples': 70720, 'counters/updates': 2210}
train stats after 70752 examples: {'rewards_train/chosen': '-0.083613', 'rewards_train/rejected': '-0.0040922', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.079521', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-148.12', 'loss/train': '0.76297', 'examples_per_second': '31.534', 'grad_norm': '171', 'counters/examples': 70752, 'counters/updates': 2211}
train stats after 70784 examples: {'rewards_train/chosen': '0.045169', 'rewards_train/rejected': '0.035174', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0099947', 'logps_train/rejected': '-116.61', 'logps_train/chosen': '-171.5', 'loss/train': '0.70029', 'examples_per_second': '30.991', 'grad_norm': '75.5', 'counters/examples': 70784, 'counters/updates': 2212}
skipping logging after 70816 examples to avoid logging too frequently
train stats after 70848 examples: {'rewards_train/chosen': '0.096531', 'rewards_train/rejected': '0.032402', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064129', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-157.39', 'loss/train': '0.68137', 'examples_per_second': '30.729', 'grad_norm': '66', 'counters/examples': 70848, 'counters/updates': 2214}
train stats after 70880 examples: {'rewards_train/chosen': '0.0355', 'rewards_train/rejected': '-0.015665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051165', 'logps_train/rejected': '-134.1', 'logps_train/chosen': '-180.8', 'loss/train': '0.68237', 'examples_per_second': '31.596', 'grad_norm': '416', 'counters/examples': 70880, 'counters/updates': 2215}
train stats after 70912 examples: {'rewards_train/chosen': '0.058369', 'rewards_train/rejected': '0.02718', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03119', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-158.02', 'loss/train': '0.68904', 'examples_per_second': '32.446', 'grad_norm': '68', 'counters/examples': 70912, 'counters/updates': 2216}
skipping logging after 70944 examples to avoid logging too frequently
train stats after 70976 examples: {'rewards_train/chosen': '0.047679', 'rewards_train/rejected': '0.0021626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045516', 'logps_train/rejected': '-164.51', 'logps_train/chosen': '-150', 'loss/train': '0.67892', 'examples_per_second': '31.559', 'grad_norm': '56.75', 'counters/examples': 70976, 'counters/updates': 2218}
skipping logging after 71008 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '0.0060987', 'rewards_train/rejected': '0.090136', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.084037', 'logps_train/rejected': '-130.37', 'logps_train/chosen': '-152.34', 'loss/train': '0.75813', 'examples_per_second': '31.528', 'grad_norm': '84.5', 'counters/examples': 71040, 'counters/updates': 2220}
skipping logging after 71072 examples to avoid logging too frequently
train stats after 71104 examples: {'rewards_train/chosen': '0.072642', 'rewards_train/rejected': '0.083654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011012', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-128.14', 'loss/train': '0.71006', 'examples_per_second': '32.041', 'grad_norm': '76', 'counters/examples': 71104, 'counters/updates': 2222}
skipping logging after 71136 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '0.053046', 'rewards_train/rejected': '0.0083371', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044709', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-143', 'loss/train': '0.68787', 'examples_per_second': '34.256', 'grad_norm': '83', 'counters/examples': 71168, 'counters/updates': 2224}
train stats after 71200 examples: {'rewards_train/chosen': '0.0076578', 'rewards_train/rejected': '0.0023237', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0053341', 'logps_train/rejected': '-156.18', 'logps_train/chosen': '-172.85', 'loss/train': '0.71047', 'examples_per_second': '30.771', 'grad_norm': '100', 'counters/examples': 71200, 'counters/updates': 2225}
train stats after 71232 examples: {'rewards_train/chosen': '0.088625', 'rewards_train/rejected': '-0.0056921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094317', 'logps_train/rejected': '-94.765', 'logps_train/chosen': '-153.67', 'loss/train': '0.65759', 'examples_per_second': '32.579', 'grad_norm': '93', 'counters/examples': 71232, 'counters/updates': 2226}
train stats after 71264 examples: {'rewards_train/chosen': '0.089414', 'rewards_train/rejected': '0.043973', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045441', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-185.66', 'loss/train': '0.68305', 'examples_per_second': '31.334', 'grad_norm': '70.5', 'counters/examples': 71264, 'counters/updates': 2227}
train stats after 71296 examples: {'rewards_train/chosen': '0.064183', 'rewards_train/rejected': '0.038162', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026021', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-158.28', 'loss/train': '0.69304', 'examples_per_second': '30.768', 'grad_norm': '69', 'counters/examples': 71296, 'counters/updates': 2228}
train stats after 71328 examples: {'rewards_train/chosen': '0.065371', 'rewards_train/rejected': '0.016775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048596', 'logps_train/rejected': '-159.67', 'logps_train/chosen': '-168.79', 'loss/train': '0.67926', 'examples_per_second': '32.736', 'grad_norm': '85.5', 'counters/examples': 71328, 'counters/updates': 2229}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71392 examples: {'rewards_train/chosen': '0.053427', 'rewards_train/rejected': '-0.029577', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083004', 'logps_train/rejected': '-179.14', 'logps_train/chosen': '-143.03', 'loss/train': '0.66585', 'examples_per_second': '30.145', 'grad_norm': '61.75', 'counters/examples': 71392, 'counters/updates': 2231}
train stats after 71424 examples: {'rewards_train/chosen': '0.05428', 'rewards_train/rejected': '0.024166', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030114', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-114.1', 'loss/train': '0.68652', 'examples_per_second': '32.42', 'grad_norm': '101', 'counters/examples': 71424, 'counters/updates': 2232}
train stats after 71456 examples: {'rewards_train/chosen': '-0.018208', 'rewards_train/rejected': '0.02522', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043428', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-153.99', 'loss/train': '0.72612', 'examples_per_second': '31.836', 'grad_norm': '211', 'counters/examples': 71456, 'counters/updates': 2233}
train stats after 71488 examples: {'rewards_train/chosen': '0.03948', 'rewards_train/rejected': '0.057604', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018124', 'logps_train/rejected': '-158.25', 'logps_train/chosen': '-192.69', 'loss/train': '0.72629', 'examples_per_second': '31.07', 'grad_norm': '217', 'counters/examples': 71488, 'counters/updates': 2234}
train stats after 71520 examples: {'rewards_train/chosen': '-0.055501', 'rewards_train/rejected': '0.020061', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.075562', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-134.68', 'loss/train': '0.74108', 'examples_per_second': '30.696', 'grad_norm': '78', 'counters/examples': 71520, 'counters/updates': 2235}
train stats after 71552 examples: {'rewards_train/chosen': '0.01737', 'rewards_train/rejected': '0.053634', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036264', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-131.24', 'loss/train': '0.73391', 'examples_per_second': '32.295', 'grad_norm': '107.5', 'counters/examples': 71552, 'counters/updates': 2236}
train stats after 71584 examples: {'rewards_train/chosen': '0.19415', 'rewards_train/rejected': '0.032016', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16213', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-172.5', 'loss/train': '0.63385', 'examples_per_second': '30.365', 'grad_norm': '112', 'counters/examples': 71584, 'counters/updates': 2237}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71648 examples: {'rewards_train/chosen': '0.12777', 'rewards_train/rejected': '-0.0031732', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13094', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-158.66', 'loss/train': '0.64476', 'examples_per_second': '31.61', 'grad_norm': '109', 'counters/examples': 71648, 'counters/updates': 2239}
train stats after 71680 examples: {'rewards_train/chosen': '0.068702', 'rewards_train/rejected': '0.012448', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056254', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-155.8', 'loss/train': '0.67472', 'examples_per_second': '31.542', 'grad_norm': '115', 'counters/examples': 71680, 'counters/updates': 2240}
train stats after 71712 examples: {'rewards_train/chosen': '-0.01851', 'rewards_train/rejected': '-0.0095067', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0090029', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-136.48', 'loss/train': '0.70861', 'examples_per_second': '32.54', 'grad_norm': '125.5', 'counters/examples': 71712, 'counters/updates': 2241}
train stats after 71744 examples: {'rewards_train/chosen': '0.16922', 'rewards_train/rejected': '0.021742', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14748', 'logps_train/rejected': '-145.76', 'logps_train/chosen': '-195.26', 'loss/train': '0.63816', 'examples_per_second': '32.955', 'grad_norm': '108.5', 'counters/examples': 71744, 'counters/updates': 2242}
train stats after 71776 examples: {'rewards_train/chosen': '0.058895', 'rewards_train/rejected': '0.054757', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0041375', 'logps_train/rejected': '-103.35', 'logps_train/chosen': '-147.69', 'loss/train': '0.70317', 'examples_per_second': '30.629', 'grad_norm': '114', 'counters/examples': 71776, 'counters/updates': 2243}
skipping logging after 71808 examples to avoid logging too frequently
train stats after 71840 examples: {'rewards_train/chosen': '0.2515', 'rewards_train/rejected': '-0.036441', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28794', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-217.4', 'loss/train': '0.59227', 'examples_per_second': '31.9', 'grad_norm': '76.5', 'counters/examples': 71840, 'counters/updates': 2245}
train stats after 71872 examples: {'rewards_train/chosen': '0.051584', 'rewards_train/rejected': '-0.02002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071605', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-162.39', 'loss/train': '0.66867', 'examples_per_second': '31.271', 'grad_norm': '61.75', 'counters/examples': 71872, 'counters/updates': 2246}
train stats after 71904 examples: {'rewards_train/chosen': '0.082953', 'rewards_train/rejected': '0.0053892', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077564', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-138.86', 'loss/train': '0.66989', 'examples_per_second': '32.495', 'grad_norm': '109', 'counters/examples': 71904, 'counters/updates': 2247}
train stats after 71936 examples: {'rewards_train/chosen': '0.017994', 'rewards_train/rejected': '0.0083812', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0096127', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-175.71', 'loss/train': '0.70577', 'examples_per_second': '31.559', 'grad_norm': '131', 'counters/examples': 71936, 'counters/updates': 2248}
skipping logging after 71968 examples to avoid logging too frequently
train stats after 72000 examples: {'rewards_train/chosen': '0.010119', 'rewards_train/rejected': '-0.06894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079059', 'logps_train/rejected': '-157.06', 'logps_train/chosen': '-152.53', 'loss/train': '0.67615', 'examples_per_second': '32.528', 'grad_norm': '134', 'counters/examples': 72000, 'counters/updates': 2250}
Running evaluation after 72000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 72000: {'rewards_eval/chosen': '0.05556', 'rewards_eval/rejected': '0.025356', 'rewards_eval/accuracies': '0.50391', 'rewards_eval/margins': '0.030204', 'logps_eval/rejected': '-127.89', 'logps_eval/chosen': '-150.45', 'loss/eval': '0.68824'}
skipping save for non epoch
train stats after 72032 examples: {'rewards_train/chosen': '-0.0016289', 'rewards_train/rejected': '0.0080353', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0096642', 'logps_train/rejected': '-175.2', 'logps_train/chosen': '-165.63', 'loss/train': '0.70865', 'examples_per_second': '30.618', 'grad_norm': '78', 'counters/examples': 72032, 'counters/updates': 2251}
train stats after 72064 examples: {'rewards_train/chosen': '0.068045', 'rewards_train/rejected': '0.04176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026285', 'logps_train/rejected': '-97.891', 'logps_train/chosen': '-144.03', 'loss/train': '0.68859', 'examples_per_second': '29.809', 'grad_norm': '83', 'counters/examples': 72064, 'counters/updates': 2252}
train stats after 72096 examples: {'rewards_train/chosen': '0.0504', 'rewards_train/rejected': '0.11573', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.065333', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-199.38', 'loss/train': '0.73524', 'examples_per_second': '32.253', 'grad_norm': '124', 'counters/examples': 72096, 'counters/updates': 2253}
skipping logging after 72128 examples to avoid logging too frequently
train stats after 72160 examples: {'rewards_train/chosen': '0.023033', 'rewards_train/rejected': '0.034581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011548', 'logps_train/rejected': '-114.4', 'logps_train/chosen': '-124.4', 'loss/train': '0.70779', 'examples_per_second': '30.926', 'grad_norm': '82.5', 'counters/examples': 72160, 'counters/updates': 2255}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72224 examples: {'rewards_train/chosen': '0.0021225', 'rewards_train/rejected': '0.071386', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069263', 'logps_train/rejected': '-106.48', 'logps_train/chosen': '-174.71', 'loss/train': '0.73649', 'examples_per_second': '31.564', 'grad_norm': '76', 'counters/examples': 72224, 'counters/updates': 2257}
skipping logging after 72256 examples to avoid logging too frequently
train stats after 72288 examples: {'rewards_train/chosen': '0.059478', 'rewards_train/rejected': '-0.0046958', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064173', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-170.69', 'loss/train': '0.68317', 'examples_per_second': '31.586', 'grad_norm': '109', 'counters/examples': 72288, 'counters/updates': 2259}
train stats after 72320 examples: {'rewards_train/chosen': '0.088556', 'rewards_train/rejected': '0.02073', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067826', 'logps_train/rejected': '-132.19', 'logps_train/chosen': '-179.86', 'loss/train': '0.67973', 'examples_per_second': '32.319', 'grad_norm': '140', 'counters/examples': 72320, 'counters/updates': 2260}
skipping logging after 72352 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '0.054802', 'rewards_train/rejected': '0.033424', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021379', 'logps_train/rejected': '-170.22', 'logps_train/chosen': '-159.63', 'loss/train': '0.69949', 'examples_per_second': '32.626', 'grad_norm': '194', 'counters/examples': 72384, 'counters/updates': 2262}
train stats after 72416 examples: {'rewards_train/chosen': '0.0033104', 'rewards_train/rejected': '-0.0069039', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010214', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-139.28', 'loss/train': '0.69628', 'examples_per_second': '31.546', 'grad_norm': '74', 'counters/examples': 72416, 'counters/updates': 2263}
train stats after 72448 examples: {'rewards_train/chosen': '0.11905', 'rewards_train/rejected': '0.05028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068774', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-150.51', 'loss/train': '0.66922', 'examples_per_second': '32.355', 'grad_norm': '120', 'counters/examples': 72448, 'counters/updates': 2264}
train stats after 72480 examples: {'rewards_train/chosen': '-0.0301', 'rewards_train/rejected': '-0.019759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01034', 'logps_train/rejected': '-120.33', 'logps_train/chosen': '-150.98', 'loss/train': '0.7074', 'examples_per_second': '30.291', 'grad_norm': '72', 'counters/examples': 72480, 'counters/updates': 2265}
train stats after 72512 examples: {'rewards_train/chosen': '0.027355', 'rewards_train/rejected': '0.032645', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0052899', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-141.26', 'loss/train': '0.70878', 'examples_per_second': '31.504', 'grad_norm': '177', 'counters/examples': 72512, 'counters/updates': 2266}
skipping logging after 72544 examples to avoid logging too frequently
train stats after 72576 examples: {'rewards_train/chosen': '-0.046538', 'rewards_train/rejected': '0.0074574', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.053996', 'logps_train/rejected': '-160.22', 'logps_train/chosen': '-183.1', 'loss/train': '0.73429', 'examples_per_second': '31.416', 'grad_norm': '196', 'counters/examples': 72576, 'counters/updates': 2268}
train stats after 72608 examples: {'rewards_train/chosen': '-0.0022189', 'rewards_train/rejected': '0.025182', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027401', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-139.2', 'loss/train': '0.71673', 'examples_per_second': '32.671', 'grad_norm': '73.5', 'counters/examples': 72608, 'counters/updates': 2269}
train stats after 72640 examples: {'rewards_train/chosen': '0.10682', 'rewards_train/rejected': '0.079943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026877', 'logps_train/rejected': '-144.1', 'logps_train/chosen': '-138.06', 'loss/train': '0.68892', 'examples_per_second': '32.129', 'grad_norm': '69', 'counters/examples': 72640, 'counters/updates': 2270}
train stats after 72672 examples: {'rewards_train/chosen': '0.019707', 'rewards_train/rejected': '0.062903', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043197', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-139.08', 'loss/train': '0.72556', 'examples_per_second': '31.986', 'grad_norm': '89.5', 'counters/examples': 72672, 'counters/updates': 2271}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72736 examples: {'rewards_train/chosen': '0.055874', 'rewards_train/rejected': '-0.0044123', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060286', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-136.98', 'loss/train': '0.67477', 'examples_per_second': '32.477', 'grad_norm': '74.5', 'counters/examples': 72736, 'counters/updates': 2273}
train stats after 72768 examples: {'rewards_train/chosen': '0.050392', 'rewards_train/rejected': '0.0024742', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047918', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-143.8', 'loss/train': '0.68394', 'examples_per_second': '31.49', 'grad_norm': '113', 'counters/examples': 72768, 'counters/updates': 2274}
skipping logging after 72800 examples to avoid logging too frequently
train stats after 72832 examples: {'rewards_train/chosen': '0.076471', 'rewards_train/rejected': '0.11257', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.036097', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-150.5', 'loss/train': '0.72246', 'examples_per_second': '30.765', 'grad_norm': '107.5', 'counters/examples': 72832, 'counters/updates': 2276}
skipping logging after 72864 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '0.097709', 'rewards_train/rejected': '-0.037071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13478', 'logps_train/rejected': '-160.25', 'logps_train/chosen': '-154.82', 'loss/train': '0.65662', 'examples_per_second': '31.847', 'grad_norm': '121', 'counters/examples': 72896, 'counters/updates': 2278}
train stats after 72928 examples: {'rewards_train/chosen': '0.20199', 'rewards_train/rejected': '0.044973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15702', 'logps_train/rejected': '-99.138', 'logps_train/chosen': '-142', 'loss/train': '0.64887', 'examples_per_second': '33.068', 'grad_norm': '47.25', 'counters/examples': 72928, 'counters/updates': 2279}
train stats after 72960 examples: {'rewards_train/chosen': '0.0075266', 'rewards_train/rejected': '0.0064476', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.001079', 'logps_train/rejected': '-164.77', 'logps_train/chosen': '-115.49', 'loss/train': '0.69844', 'examples_per_second': '30.762', 'grad_norm': '63.75', 'counters/examples': 72960, 'counters/updates': 2280}
train stats after 72992 examples: {'rewards_train/chosen': '0.001102', 'rewards_train/rejected': '0.10332', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10222', 'logps_train/rejected': '-194.8', 'logps_train/chosen': '-160.21', 'loss/train': '0.76342', 'examples_per_second': '32.716', 'grad_norm': '201', 'counters/examples': 72992, 'counters/updates': 2281}
train stats after 73024 examples: {'rewards_train/chosen': '0.05923', 'rewards_train/rejected': '0.05326', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.00597', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-149.3', 'loss/train': '0.70233', 'examples_per_second': '31.54', 'grad_norm': '70.5', 'counters/examples': 73024, 'counters/updates': 2282}
skipping logging after 73056 examples to avoid logging too frequently
train stats after 73088 examples: {'rewards_train/chosen': '0.02824', 'rewards_train/rejected': '0.039333', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011094', 'logps_train/rejected': '-160.31', 'logps_train/chosen': '-156.29', 'loss/train': '0.70907', 'examples_per_second': '30.392', 'grad_norm': '127', 'counters/examples': 73088, 'counters/updates': 2284}
train stats after 73120 examples: {'rewards_train/chosen': '-0.015865', 'rewards_train/rejected': '-0.023064', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0071993', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-135.29', 'loss/train': '0.70684', 'examples_per_second': '30.901', 'grad_norm': '116.5', 'counters/examples': 73120, 'counters/updates': 2285}
train stats after 73152 examples: {'rewards_train/chosen': '0.082991', 'rewards_train/rejected': '0.020123', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062867', 'logps_train/rejected': '-168.68', 'logps_train/chosen': '-193.5', 'loss/train': '0.67608', 'examples_per_second': '33.309', 'grad_norm': '90.5', 'counters/examples': 73152, 'counters/updates': 2286}
train stats after 73184 examples: {'rewards_train/chosen': '0.09453', 'rewards_train/rejected': '0.043108', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051422', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-124.56', 'loss/train': '0.68463', 'examples_per_second': '32.708', 'grad_norm': '96.5', 'counters/examples': 73184, 'counters/updates': 2287}
train stats after 73216 examples: {'rewards_train/chosen': '0.054925', 'rewards_train/rejected': '0.026026', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028899', 'logps_train/rejected': '-122.57', 'logps_train/chosen': '-140.69', 'loss/train': '0.69304', 'examples_per_second': '31.576', 'grad_norm': '215', 'counters/examples': 73216, 'counters/updates': 2288}
train stats after 73248 examples: {'rewards_train/chosen': '0.014514', 'rewards_train/rejected': '0.00888', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0056337', 'logps_train/rejected': '-150.48', 'logps_train/chosen': '-148.11', 'loss/train': '0.73534', 'examples_per_second': '31.547', 'grad_norm': '119.5', 'counters/examples': 73248, 'counters/updates': 2289}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73312 examples: {'rewards_train/chosen': '0.014785', 'rewards_train/rejected': '-0.036655', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05144', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-161.82', 'loss/train': '0.68261', 'examples_per_second': '37.392', 'grad_norm': '576', 'counters/examples': 73312, 'counters/updates': 2291}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73376 examples: {'rewards_train/chosen': '0.021775', 'rewards_train/rejected': '0.029676', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0079012', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-145.87', 'loss/train': '0.70811', 'examples_per_second': '33.465', 'grad_norm': '99.5', 'counters/examples': 73376, 'counters/updates': 2293}
train stats after 73408 examples: {'rewards_train/chosen': '-0.070098', 'rewards_train/rejected': '0.017919', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.088016', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-152.97', 'loss/train': '0.75648', 'examples_per_second': '30.808', 'grad_norm': '88', 'counters/examples': 73408, 'counters/updates': 2294}
skipping logging after 73440 examples to avoid logging too frequently
train stats after 73472 examples: {'rewards_train/chosen': '-0.017557', 'rewards_train/rejected': '0.0013819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018939', 'logps_train/rejected': '-110.77', 'logps_train/chosen': '-127.95', 'loss/train': '0.70875', 'examples_per_second': '31.723', 'grad_norm': '58.5', 'counters/examples': 73472, 'counters/updates': 2296}
train stats after 73504 examples: {'rewards_train/chosen': '0.073986', 'rewards_train/rejected': '0.087471', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013485', 'logps_train/rejected': '-159.21', 'logps_train/chosen': '-152.41', 'loss/train': '0.70506', 'examples_per_second': '30.53', 'grad_norm': '114', 'counters/examples': 73504, 'counters/updates': 2297}
train stats after 73536 examples: {'rewards_train/chosen': '0.027757', 'rewards_train/rejected': '0.057789', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030033', 'logps_train/rejected': '-98.458', 'logps_train/chosen': '-148.34', 'loss/train': '0.72757', 'examples_per_second': '32.555', 'grad_norm': '90.5', 'counters/examples': 73536, 'counters/updates': 2298}
train stats after 73568 examples: {'rewards_train/chosen': '0.067433', 'rewards_train/rejected': '0.028421', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039012', 'logps_train/rejected': '-126.62', 'logps_train/chosen': '-169.35', 'loss/train': '0.68388', 'examples_per_second': '31.523', 'grad_norm': '70', 'counters/examples': 73568, 'counters/updates': 2299}
train stats after 73600 examples: {'rewards_train/chosen': '-0.034152', 'rewards_train/rejected': '-0.004664', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029488', 'logps_train/rejected': '-160.5', 'logps_train/chosen': '-175.26', 'loss/train': '0.72538', 'examples_per_second': '32.146', 'grad_norm': '94', 'counters/examples': 73600, 'counters/updates': 2300}
train stats after 73632 examples: {'rewards_train/chosen': '0.074862', 'rewards_train/rejected': '-0.0041666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079028', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-148.54', 'loss/train': '0.66401', 'examples_per_second': '32.21', 'grad_norm': '67.5', 'counters/examples': 73632, 'counters/updates': 2301}
train stats after 73664 examples: {'rewards_train/chosen': '0.0051039', 'rewards_train/rejected': '0.091357', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.086253', 'logps_train/rejected': '-185.29', 'logps_train/chosen': '-166.97', 'loss/train': '0.75156', 'examples_per_second': '31.653', 'grad_norm': '173', 'counters/examples': 73664, 'counters/updates': 2302}
train stats after 73696 examples: {'rewards_train/chosen': '0.036306', 'rewards_train/rejected': '0.017482', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018824', 'logps_train/rejected': '-154.26', 'logps_train/chosen': '-140.93', 'loss/train': '0.6966', 'examples_per_second': '30.537', 'grad_norm': '110', 'counters/examples': 73696, 'counters/updates': 2303}
train stats after 73728 examples: {'rewards_train/chosen': '0.0048035', 'rewards_train/rejected': '0.038558', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033755', 'logps_train/rejected': '-152.31', 'logps_train/chosen': '-143.99', 'loss/train': '0.72105', 'examples_per_second': '30.952', 'grad_norm': '91', 'counters/examples': 73728, 'counters/updates': 2304}
train stats after 73760 examples: {'rewards_train/chosen': '0.073249', 'rewards_train/rejected': '-0.021659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094908', 'logps_train/rejected': '-153.14', 'logps_train/chosen': '-153.73', 'loss/train': '0.66433', 'examples_per_second': '31.413', 'grad_norm': '109', 'counters/examples': 73760, 'counters/updates': 2305}
train stats after 73792 examples: {'rewards_train/chosen': '0.035429', 'rewards_train/rejected': '-0.035077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070506', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-132.91', 'loss/train': '0.66386', 'examples_per_second': '24.283', 'grad_norm': '61', 'counters/examples': 73792, 'counters/updates': 2306}
train stats after 73824 examples: {'rewards_train/chosen': '0.058219', 'rewards_train/rejected': '-0.013256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071475', 'logps_train/rejected': '-157.75', 'logps_train/chosen': '-167.55', 'loss/train': '0.66952', 'examples_per_second': '31.56', 'grad_norm': '122', 'counters/examples': 73824, 'counters/updates': 2307}
train stats after 73856 examples: {'rewards_train/chosen': '0.025759', 'rewards_train/rejected': '-0.059435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085194', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-143.48', 'loss/train': '0.66155', 'examples_per_second': '32.197', 'grad_norm': '133', 'counters/examples': 73856, 'counters/updates': 2308}
train stats after 73888 examples: {'rewards_train/chosen': '0.016211', 'rewards_train/rejected': '-0.033976', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050187', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-166.38', 'loss/train': '0.67949', 'examples_per_second': '23.645', 'grad_norm': '75.5', 'counters/examples': 73888, 'counters/updates': 2309}
train stats after 73920 examples: {'rewards_train/chosen': '0.059847', 'rewards_train/rejected': '0.044333', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015513', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-147.36', 'loss/train': '0.693', 'examples_per_second': '31.927', 'grad_norm': '177', 'counters/examples': 73920, 'counters/updates': 2310}
train stats after 73952 examples: {'rewards_train/chosen': '0.042173', 'rewards_train/rejected': '0.068516', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026343', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-171.39', 'loss/train': '0.71528', 'examples_per_second': '30.18', 'grad_norm': '96', 'counters/examples': 73952, 'counters/updates': 2311}
train stats after 73984 examples: {'rewards_train/chosen': '-0.044724', 'rewards_train/rejected': '-0.04225', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0024734', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-131.8', 'loss/train': '0.70223', 'examples_per_second': '30.403', 'grad_norm': '93', 'counters/examples': 73984, 'counters/updates': 2312}
skipping logging after 74016 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '0.084618', 'rewards_train/rejected': '-0.011792', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.09641', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-146.75', 'loss/train': '0.67267', 'examples_per_second': '34.026', 'grad_norm': '78.5', 'counters/examples': 74048, 'counters/updates': 2314}
train stats after 74080 examples: {'rewards_train/chosen': '-0.032034', 'rewards_train/rejected': '0.0031501', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035184', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-124.89', 'loss/train': '0.71642', 'examples_per_second': '31.491', 'grad_norm': '54.5', 'counters/examples': 74080, 'counters/updates': 2315}
train stats after 74112 examples: {'rewards_train/chosen': '0.1751', 'rewards_train/rejected': '0.066491', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10861', 'logps_train/rejected': '-152.15', 'logps_train/chosen': '-160.22', 'loss/train': '0.6554', 'examples_per_second': '29.901', 'grad_norm': '168', 'counters/examples': 74112, 'counters/updates': 2316}
skipping logging after 74144 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '0.0093395', 'rewards_train/rejected': '0.030336', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.020996', 'logps_train/rejected': '-95.293', 'logps_train/chosen': '-135.49', 'loss/train': '0.7232', 'examples_per_second': '31.009', 'grad_norm': '92.5', 'counters/examples': 74176, 'counters/updates': 2318}
train stats after 74208 examples: {'rewards_train/chosen': '0.061801', 'rewards_train/rejected': '0.024339', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037462', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-137.73', 'loss/train': '0.68407', 'examples_per_second': '31.844', 'grad_norm': '69.5', 'counters/examples': 74208, 'counters/updates': 2319}
train stats after 74240 examples: {'rewards_train/chosen': '0.005819', 'rewards_train/rejected': '-0.015407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021226', 'logps_train/rejected': '-109.02', 'logps_train/chosen': '-165.92', 'loss/train': '0.69131', 'examples_per_second': '31.652', 'grad_norm': '123.5', 'counters/examples': 74240, 'counters/updates': 2320}
train stats after 74272 examples: {'rewards_train/chosen': '0.045008', 'rewards_train/rejected': '0.027918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017091', 'logps_train/rejected': '-118.14', 'logps_train/chosen': '-167.11', 'loss/train': '0.69447', 'examples_per_second': '33.016', 'grad_norm': '280', 'counters/examples': 74272, 'counters/updates': 2321}
skipping logging after 74304 examples to avoid logging too frequently
train stats after 74336 examples: {'rewards_train/chosen': '0.15415', 'rewards_train/rejected': '0.020224', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13393', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-163.71', 'loss/train': '0.64836', 'examples_per_second': '33.005', 'grad_norm': '77', 'counters/examples': 74336, 'counters/updates': 2323}
train stats after 74368 examples: {'rewards_train/chosen': '0.11985', 'rewards_train/rejected': '0.044905', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074944', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-181.54', 'loss/train': '0.68076', 'examples_per_second': '31.809', 'grad_norm': '75.5', 'counters/examples': 74368, 'counters/updates': 2324}
train stats after 74400 examples: {'rewards_train/chosen': '-0.0284', 'rewards_train/rejected': '0.0016906', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030091', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-165.82', 'loss/train': '0.72618', 'examples_per_second': '30.692', 'grad_norm': '62.75', 'counters/examples': 74400, 'counters/updates': 2325}
train stats after 74432 examples: {'rewards_train/chosen': '0.089549', 'rewards_train/rejected': '0.031441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058108', 'logps_train/rejected': '-149.66', 'logps_train/chosen': '-127.33', 'loss/train': '0.67975', 'examples_per_second': '31.288', 'grad_norm': '185', 'counters/examples': 74432, 'counters/updates': 2326}
train stats after 74464 examples: {'rewards_train/chosen': '0.058', 'rewards_train/rejected': '0.050229', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0077707', 'logps_train/rejected': '-148.36', 'logps_train/chosen': '-174.75', 'loss/train': '0.69738', 'examples_per_second': '31.937', 'grad_norm': '302', 'counters/examples': 74464, 'counters/updates': 2327}
train stats after 74496 examples: {'rewards_train/chosen': '0.056444', 'rewards_train/rejected': '0.089188', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032744', 'logps_train/rejected': '-151.73', 'logps_train/chosen': '-144.92', 'loss/train': '0.73734', 'examples_per_second': '31.683', 'grad_norm': '163', 'counters/examples': 74496, 'counters/updates': 2328}
train stats after 74528 examples: {'rewards_train/chosen': '0.040515', 'rewards_train/rejected': '0.0038987', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036617', 'logps_train/rejected': '-115.87', 'logps_train/chosen': '-104.47', 'loss/train': '0.68489', 'examples_per_second': '30.413', 'grad_norm': '132', 'counters/examples': 74528, 'counters/updates': 2329}
train stats after 74560 examples: {'rewards_train/chosen': '0.014566', 'rewards_train/rejected': '0.02972', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015154', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-146.02', 'loss/train': '0.71379', 'examples_per_second': '31.795', 'grad_norm': '146', 'counters/examples': 74560, 'counters/updates': 2330}
skipping logging after 74592 examples to avoid logging too frequently
train stats after 74624 examples: {'rewards_train/chosen': '-0.034622', 'rewards_train/rejected': '0.046934', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.081556', 'logps_train/rejected': '-118.73', 'logps_train/chosen': '-136.27', 'loss/train': '0.7432', 'examples_per_second': '31.011', 'grad_norm': '75.5', 'counters/examples': 74624, 'counters/updates': 2332}
train stats after 74656 examples: {'rewards_train/chosen': '0.042908', 'rewards_train/rejected': '0.0060486', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03686', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-151.16', 'loss/train': '0.68821', 'examples_per_second': '31.544', 'grad_norm': '193', 'counters/examples': 74656, 'counters/updates': 2333}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74720 examples: {'rewards_train/chosen': '0.037981', 'rewards_train/rejected': '0.0096652', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028316', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-167.79', 'loss/train': '0.69236', 'examples_per_second': '31.54', 'grad_norm': '88.5', 'counters/examples': 74720, 'counters/updates': 2335}
train stats after 74752 examples: {'rewards_train/chosen': '0.012133', 'rewards_train/rejected': '-0.01969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031823', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-161.21', 'loss/train': '0.68539', 'examples_per_second': '31.317', 'grad_norm': '59.75', 'counters/examples': 74752, 'counters/updates': 2336}
train stats after 74784 examples: {'rewards_train/chosen': '0.049897', 'rewards_train/rejected': '0.040815', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0090815', 'logps_train/rejected': '-161.75', 'logps_train/chosen': '-143.78', 'loss/train': '0.70946', 'examples_per_second': '31.405', 'grad_norm': '139', 'counters/examples': 74784, 'counters/updates': 2337}
train stats after 74816 examples: {'rewards_train/chosen': '0.017556', 'rewards_train/rejected': '-0.025039', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042595', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-133.21', 'loss/train': '0.68058', 'examples_per_second': '31.643', 'grad_norm': '69', 'counters/examples': 74816, 'counters/updates': 2338}
train stats after 74848 examples: {'rewards_train/chosen': '0.0075349', 'rewards_train/rejected': '-0.034344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041879', 'logps_train/rejected': '-140.51', 'logps_train/chosen': '-156.49', 'loss/train': '0.68257', 'examples_per_second': '30.751', 'grad_norm': '83.5', 'counters/examples': 74848, 'counters/updates': 2339}
train stats after 74880 examples: {'rewards_train/chosen': '-0.015129', 'rewards_train/rejected': '0.012731', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02786', 'logps_train/rejected': '-84.208', 'logps_train/chosen': '-133.41', 'loss/train': '0.7137', 'examples_per_second': '31.728', 'grad_norm': '101.5', 'counters/examples': 74880, 'counters/updates': 2340}
train stats after 74912 examples: {'rewards_train/chosen': '0.021954', 'rewards_train/rejected': '-0.0073728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029326', 'logps_train/rejected': '-147.56', 'logps_train/chosen': '-133.08', 'loss/train': '0.69006', 'examples_per_second': '31.645', 'grad_norm': '78.5', 'counters/examples': 74912, 'counters/updates': 2341}
train stats after 74944 examples: {'rewards_train/chosen': '0.044168', 'rewards_train/rejected': '-0.0018214', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045989', 'logps_train/rejected': '-167.26', 'logps_train/chosen': '-146.22', 'loss/train': '0.68619', 'examples_per_second': '33.268', 'grad_norm': '142', 'counters/examples': 74944, 'counters/updates': 2342}
train stats after 74976 examples: {'rewards_train/chosen': '0.037095', 'rewards_train/rejected': '0.02238', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014715', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-155.33', 'loss/train': '0.69739', 'examples_per_second': '32.602', 'grad_norm': '72', 'counters/examples': 74976, 'counters/updates': 2343}
train stats after 75008 examples: {'rewards_train/chosen': '-0.016819', 'rewards_train/rejected': '0.0026327', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019452', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-145.3', 'loss/train': '0.71623', 'examples_per_second': '31.583', 'grad_norm': '258', 'counters/examples': 75008, 'counters/updates': 2344}
skipping logging after 75040 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '-0.071251', 'rewards_train/rejected': '0.027084', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.098335', 'logps_train/rejected': '-179.82', 'logps_train/chosen': '-172.44', 'loss/train': '0.75169', 'examples_per_second': '31.304', 'grad_norm': '77', 'counters/examples': 75072, 'counters/updates': 2346}
train stats after 75104 examples: {'rewards_train/chosen': '0.071701', 'rewards_train/rejected': '0.013009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058692', 'logps_train/rejected': '-102.26', 'logps_train/chosen': '-118.43', 'loss/train': '0.67775', 'examples_per_second': '31.359', 'grad_norm': '70', 'counters/examples': 75104, 'counters/updates': 2347}
train stats after 75136 examples: {'rewards_train/chosen': '0.063886', 'rewards_train/rejected': '-0.032893', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.096779', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-138.75', 'loss/train': '0.67015', 'examples_per_second': '30.267', 'grad_norm': '78.5', 'counters/examples': 75136, 'counters/updates': 2348}
train stats after 75168 examples: {'rewards_train/chosen': '0.0037513', 'rewards_train/rejected': '-0.0011026', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0048539', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-153.06', 'loss/train': '0.7111', 'examples_per_second': '30.665', 'grad_norm': '135', 'counters/examples': 75168, 'counters/updates': 2349}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75232 examples: {'rewards_train/chosen': '0.1949', 'rewards_train/rejected': '0.032015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16289', 'logps_train/rejected': '-117.58', 'logps_train/chosen': '-176.03', 'loss/train': '0.64459', 'examples_per_second': '31.401', 'grad_norm': '95', 'counters/examples': 75232, 'counters/updates': 2351}
train stats after 75264 examples: {'rewards_train/chosen': '0.0011308', 'rewards_train/rejected': '-0.01063', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01176', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-149.1', 'loss/train': '0.69746', 'examples_per_second': '30.669', 'grad_norm': '132', 'counters/examples': 75264, 'counters/updates': 2352}
skipping logging after 75296 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '0.15854', 'rewards_train/rejected': '0.026064', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13247', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-171.12', 'loss/train': '0.63718', 'examples_per_second': '24.244', 'grad_norm': '91.5', 'counters/examples': 75328, 'counters/updates': 2354}
train stats after 75360 examples: {'rewards_train/chosen': '0.07925', 'rewards_train/rejected': '0.066601', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012649', 'logps_train/rejected': '-103.45', 'logps_train/chosen': '-134.44', 'loss/train': '0.69834', 'examples_per_second': '32.951', 'grad_norm': '154', 'counters/examples': 75360, 'counters/updates': 2355}
train stats after 75392 examples: {'rewards_train/chosen': '-0.017916', 'rewards_train/rejected': '0.028269', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046185', 'logps_train/rejected': '-121.93', 'logps_train/chosen': '-128.49', 'loss/train': '0.72549', 'examples_per_second': '30.188', 'grad_norm': '66.5', 'counters/examples': 75392, 'counters/updates': 2356}
skipping logging after 75424 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '0.035908', 'rewards_train/rejected': '0.046139', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010231', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-170.44', 'loss/train': '0.70874', 'examples_per_second': '36.31', 'grad_norm': '109.5', 'counters/examples': 75456, 'counters/updates': 2358}
train stats after 75488 examples: {'rewards_train/chosen': '0.072267', 'rewards_train/rejected': '-0.017203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08947', 'logps_train/rejected': '-158.91', 'logps_train/chosen': '-160.38', 'loss/train': '0.6677', 'examples_per_second': '30.694', 'grad_norm': '66', 'counters/examples': 75488, 'counters/updates': 2359}
train stats after 75520 examples: {'rewards_train/chosen': '0.052758', 'rewards_train/rejected': '-0.010978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063735', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-190.82', 'loss/train': '0.67234', 'examples_per_second': '31.642', 'grad_norm': '81.5', 'counters/examples': 75520, 'counters/updates': 2360}
train stats after 75552 examples: {'rewards_train/chosen': '0.034507', 'rewards_train/rejected': '0.035981', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0014734', 'logps_train/rejected': '-151.97', 'logps_train/chosen': '-128.75', 'loss/train': '0.70033', 'examples_per_second': '30.687', 'grad_norm': '194', 'counters/examples': 75552, 'counters/updates': 2361}
train stats after 75584 examples: {'rewards_train/chosen': '0.068765', 'rewards_train/rejected': '0.026092', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042673', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-126.87', 'loss/train': '0.68818', 'examples_per_second': '30.227', 'grad_norm': '135', 'counters/examples': 75584, 'counters/updates': 2362}
train stats after 75616 examples: {'rewards_train/chosen': '0.039725', 'rewards_train/rejected': '0.012166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027559', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-133.1', 'loss/train': '0.68661', 'examples_per_second': '33.038', 'grad_norm': '61.25', 'counters/examples': 75616, 'counters/updates': 2363}
train stats after 75648 examples: {'rewards_train/chosen': '0.040439', 'rewards_train/rejected': '0.0083799', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032059', 'logps_train/rejected': '-160.61', 'logps_train/chosen': '-176.66', 'loss/train': '0.68939', 'examples_per_second': '31.957', 'grad_norm': '105', 'counters/examples': 75648, 'counters/updates': 2364}
train stats after 75680 examples: {'rewards_train/chosen': '0.022975', 'rewards_train/rejected': '0.068915', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.04594', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-132.43', 'loss/train': '0.73207', 'examples_per_second': '31.66', 'grad_norm': '94.5', 'counters/examples': 75680, 'counters/updates': 2365}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75744 examples: {'rewards_train/chosen': '0.12798', 'rewards_train/rejected': '0.074149', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053828', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-148.25', 'loss/train': '0.6783', 'examples_per_second': '34.696', 'grad_norm': '107', 'counters/examples': 75744, 'counters/updates': 2367}
train stats after 75776 examples: {'rewards_train/chosen': '0.051169', 'rewards_train/rejected': '0.065981', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014812', 'logps_train/rejected': '-112.31', 'logps_train/chosen': '-160.08', 'loss/train': '0.70581', 'examples_per_second': '30.227', 'grad_norm': '78', 'counters/examples': 75776, 'counters/updates': 2368}
train stats after 75808 examples: {'rewards_train/chosen': '0.019993', 'rewards_train/rejected': '-0.021492', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041485', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-192.2', 'loss/train': '0.68242', 'examples_per_second': '31.666', 'grad_norm': '72', 'counters/examples': 75808, 'counters/updates': 2369}
train stats after 75840 examples: {'rewards_train/chosen': '0.021616', 'rewards_train/rejected': '0.053829', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032213', 'logps_train/rejected': '-170.81', 'logps_train/chosen': '-146.69', 'loss/train': '0.71687', 'examples_per_second': '31.198', 'grad_norm': '304', 'counters/examples': 75840, 'counters/updates': 2370}
train stats after 75872 examples: {'rewards_train/chosen': '0.021451', 'rewards_train/rejected': '0.028785', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0073344', 'logps_train/rejected': '-100.3', 'logps_train/chosen': '-135.59', 'loss/train': '0.7104', 'examples_per_second': '31.656', 'grad_norm': '79.5', 'counters/examples': 75872, 'counters/updates': 2371}
train stats after 75904 examples: {'rewards_train/chosen': '0.076162', 'rewards_train/rejected': '0.034597', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041565', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-160.09', 'loss/train': '0.68306', 'examples_per_second': '31.373', 'grad_norm': '90.5', 'counters/examples': 75904, 'counters/updates': 2372}
train stats after 75936 examples: {'rewards_train/chosen': '0.030972', 'rewards_train/rejected': '-0.0040863', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035058', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-153.93', 'loss/train': '0.6878', 'examples_per_second': '31.65', 'grad_norm': '60.75', 'counters/examples': 75936, 'counters/updates': 2373}
train stats after 75968 examples: {'rewards_train/chosen': '0.035051', 'rewards_train/rejected': '0.015617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019434', 'logps_train/rejected': '-131.46', 'logps_train/chosen': '-143.4', 'loss/train': '0.69384', 'examples_per_second': '31.427', 'grad_norm': '92', 'counters/examples': 75968, 'counters/updates': 2374}
train stats after 76000 examples: {'rewards_train/chosen': '0.13146', 'rewards_train/rejected': '0.035018', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096441', 'logps_train/rejected': '-139.66', 'logps_train/chosen': '-126.03', 'loss/train': '0.66837', 'examples_per_second': '31.591', 'grad_norm': '72', 'counters/examples': 76000, 'counters/updates': 2375}
train stats after 76032 examples: {'rewards_train/chosen': '0.060018', 'rewards_train/rejected': '0.063846', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0038286', 'logps_train/rejected': '-168.18', 'logps_train/chosen': '-196.23', 'loss/train': '0.71427', 'examples_per_second': '33.072', 'grad_norm': '77', 'counters/examples': 76032, 'counters/updates': 2376}
train stats after 76064 examples: {'rewards_train/chosen': '0.016538', 'rewards_train/rejected': '-0.043544', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060082', 'logps_train/rejected': '-165.7', 'logps_train/chosen': '-166.77', 'loss/train': '0.67963', 'examples_per_second': '29.916', 'grad_norm': '156', 'counters/examples': 76064, 'counters/updates': 2377}
train stats after 76096 examples: {'rewards_train/chosen': '0.032039', 'rewards_train/rejected': '0.0042624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027777', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-127.4', 'loss/train': '0.69054', 'examples_per_second': '31.642', 'grad_norm': '65.5', 'counters/examples': 76096, 'counters/updates': 2378}
skipping logging after 76128 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '0.027228', 'rewards_train/rejected': '0.030593', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0033644', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-125.73', 'loss/train': '0.70712', 'examples_per_second': '34.961', 'grad_norm': '79.5', 'counters/examples': 76160, 'counters/updates': 2380}
skipping logging after 76192 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '0.081251', 'rewards_train/rejected': '0.019293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061958', 'logps_train/rejected': '-181.51', 'logps_train/chosen': '-176.34', 'loss/train': '0.68493', 'examples_per_second': '31.591', 'grad_norm': '243', 'counters/examples': 76224, 'counters/updates': 2382}
skipping logging after 76256 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '0.026567', 'rewards_train/rejected': '0.0297', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0031324', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-129.18', 'loss/train': '0.70603', 'examples_per_second': '31.902', 'grad_norm': '85', 'counters/examples': 76288, 'counters/updates': 2384}
train stats after 76320 examples: {'rewards_train/chosen': '-0.028767', 'rewards_train/rejected': '-0.024429', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0043382', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-155.8', 'loss/train': '0.71602', 'examples_per_second': '31.336', 'grad_norm': '121', 'counters/examples': 76320, 'counters/updates': 2385}
train stats after 76352 examples: {'rewards_train/chosen': '0.023187', 'rewards_train/rejected': '-0.059202', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082389', 'logps_train/rejected': '-153.76', 'logps_train/chosen': '-197.53', 'loss/train': '0.6695', 'examples_per_second': '30.464', 'grad_norm': '90.5', 'counters/examples': 76352, 'counters/updates': 2386}
train stats after 76384 examples: {'rewards_train/chosen': '0.09355', 'rewards_train/rejected': '0.059727', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.033823', 'logps_train/rejected': '-129.76', 'logps_train/chosen': '-130.53', 'loss/train': '0.68988', 'examples_per_second': '32.113', 'grad_norm': '94.5', 'counters/examples': 76384, 'counters/updates': 2387}
train stats after 76416 examples: {'rewards_train/chosen': '0.00035993', 'rewards_train/rejected': '-0.055152', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.055512', 'logps_train/rejected': '-154.66', 'logps_train/chosen': '-171.07', 'loss/train': '0.67924', 'examples_per_second': '31.776', 'grad_norm': '74.5', 'counters/examples': 76416, 'counters/updates': 2388}
train stats after 76448 examples: {'rewards_train/chosen': '0.012154', 'rewards_train/rejected': '0.097597', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.085442', 'logps_train/rejected': '-144.4', 'logps_train/chosen': '-155.04', 'loss/train': '0.75011', 'examples_per_second': '31.382', 'grad_norm': '147', 'counters/examples': 76448, 'counters/updates': 2389}
train stats after 76480 examples: {'rewards_train/chosen': '0.092492', 'rewards_train/rejected': '0.042385', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050107', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-154.69', 'loss/train': '0.69155', 'examples_per_second': '30.543', 'grad_norm': '102.5', 'counters/examples': 76480, 'counters/updates': 2390}
train stats after 76512 examples: {'rewards_train/chosen': '0.029145', 'rewards_train/rejected': '-0.008624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037769', 'logps_train/rejected': '-116.24', 'logps_train/chosen': '-150.96', 'loss/train': '0.68194', 'examples_per_second': '30.239', 'grad_norm': '100.5', 'counters/examples': 76512, 'counters/updates': 2391}
train stats after 76544 examples: {'rewards_train/chosen': '0.02456', 'rewards_train/rejected': '-0.046339', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0709', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-145.96', 'loss/train': '0.67012', 'examples_per_second': '31.662', 'grad_norm': '67.5', 'counters/examples': 76544, 'counters/updates': 2392}
skipping logging after 76576 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '-0.025612', 'rewards_train/rejected': '0.036888', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.062499', 'logps_train/rejected': '-154.07', 'logps_train/chosen': '-160.01', 'loss/train': '0.73482', 'examples_per_second': '35.925', 'grad_norm': '98.5', 'counters/examples': 76608, 'counters/updates': 2394}
train stats after 76640 examples: {'rewards_train/chosen': '0.032422', 'rewards_train/rejected': '-0.029854', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062277', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-118.18', 'loss/train': '0.67244', 'examples_per_second': '30.054', 'grad_norm': '90.5', 'counters/examples': 76640, 'counters/updates': 2395}
train stats after 76672 examples: {'rewards_train/chosen': '0.044936', 'rewards_train/rejected': '0.07392', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028984', 'logps_train/rejected': '-157.04', 'logps_train/chosen': '-177.69', 'loss/train': '0.7194', 'examples_per_second': '31.644', 'grad_norm': '95', 'counters/examples': 76672, 'counters/updates': 2396}
train stats after 76704 examples: {'rewards_train/chosen': '0.045755', 'rewards_train/rejected': '-0.083731', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12949', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-169.12', 'loss/train': '0.64232', 'examples_per_second': '31.625', 'grad_norm': '73.5', 'counters/examples': 76704, 'counters/updates': 2397}
train stats after 76736 examples: {'rewards_train/chosen': '-0.0069937', 'rewards_train/rejected': '-0.052062', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045069', 'logps_train/rejected': '-111.48', 'logps_train/chosen': '-120.83', 'loss/train': '0.68672', 'examples_per_second': '32.271', 'grad_norm': '75.5', 'counters/examples': 76736, 'counters/updates': 2398}
train stats after 76768 examples: {'rewards_train/chosen': '-0.013464', 'rewards_train/rejected': '-0.041853', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028389', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-152.03', 'loss/train': '0.68924', 'examples_per_second': '31.581', 'grad_norm': '109', 'counters/examples': 76768, 'counters/updates': 2399}
train stats after 76800 examples: {'rewards_train/chosen': '0.088997', 'rewards_train/rejected': '0.071046', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017952', 'logps_train/rejected': '-143.36', 'logps_train/chosen': '-149.23', 'loss/train': '0.69845', 'examples_per_second': '31.683', 'grad_norm': '78.5', 'counters/examples': 76800, 'counters/updates': 2400}
skipping logging after 76832 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '-0.0092691', 'rewards_train/rejected': '0.032472', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.041741', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-135.16', 'loss/train': '0.72959', 'examples_per_second': '31.352', 'grad_norm': '196', 'counters/examples': 76864, 'counters/updates': 2402}
train stats after 76896 examples: {'rewards_train/chosen': '0.017166', 'rewards_train/rejected': '-0.054981', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072146', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-170.16', 'loss/train': '0.67989', 'examples_per_second': '31.667', 'grad_norm': '190', 'counters/examples': 76896, 'counters/updates': 2403}
skipping logging after 76928 examples to avoid logging too frequently
train stats after 76960 examples: {'rewards_train/chosen': '0.0094239', 'rewards_train/rejected': '0.043303', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033879', 'logps_train/rejected': '-156.39', 'logps_train/chosen': '-171.63', 'loss/train': '0.72883', 'examples_per_second': '30.219', 'grad_norm': '241', 'counters/examples': 76960, 'counters/updates': 2405}
train stats after 76992 examples: {'rewards_train/chosen': '0.030184', 'rewards_train/rejected': '-0.045031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075215', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-144.08', 'loss/train': '0.67105', 'examples_per_second': '32.772', 'grad_norm': '61.25', 'counters/examples': 76992, 'counters/updates': 2406}
train stats after 77024 examples: {'rewards_train/chosen': '0.050544', 'rewards_train/rejected': '0.068925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01838', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-157.17', 'loss/train': '0.71779', 'examples_per_second': '33.014', 'grad_norm': '81.5', 'counters/examples': 77024, 'counters/updates': 2407}
train stats after 77056 examples: {'rewards_train/chosen': '0.057597', 'rewards_train/rejected': '-0.043298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1009', 'logps_train/rejected': '-122.06', 'logps_train/chosen': '-136.47', 'loss/train': '0.65267', 'examples_per_second': '31.232', 'grad_norm': '108', 'counters/examples': 77056, 'counters/updates': 2408}
train stats after 77088 examples: {'rewards_train/chosen': '-0.040948', 'rewards_train/rejected': '-0.00020473', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.040743', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-159.01', 'loss/train': '0.72948', 'examples_per_second': '32.686', 'grad_norm': '107', 'counters/examples': 77088, 'counters/updates': 2409}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77152 examples: {'rewards_train/chosen': '0.0023759', 'rewards_train/rejected': '0.0032988', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00092297', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-162.85', 'loss/train': '0.70468', 'examples_per_second': '31.207', 'grad_norm': '110', 'counters/examples': 77152, 'counters/updates': 2411}
train stats after 77184 examples: {'rewards_train/chosen': '0.17902', 'rewards_train/rejected': '0.043304', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13572', 'logps_train/rejected': '-114.4', 'logps_train/chosen': '-175.56', 'loss/train': '0.64938', 'examples_per_second': '30.932', 'grad_norm': '64', 'counters/examples': 77184, 'counters/updates': 2412}
train stats after 77216 examples: {'rewards_train/chosen': '0.11621', 'rewards_train/rejected': '-0.034969', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15118', 'logps_train/rejected': '-148.58', 'logps_train/chosen': '-156.41', 'loss/train': '0.63574', 'examples_per_second': '31.686', 'grad_norm': '63', 'counters/examples': 77216, 'counters/updates': 2413}
train stats after 77248 examples: {'rewards_train/chosen': '0.026242', 'rewards_train/rejected': '0.090577', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.064335', 'logps_train/rejected': '-138.11', 'logps_train/chosen': '-171.49', 'loss/train': '0.7339', 'examples_per_second': '30.144', 'grad_norm': '82', 'counters/examples': 77248, 'counters/updates': 2414}
train stats after 77280 examples: {'rewards_train/chosen': '0.038978', 'rewards_train/rejected': '0.012581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026397', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-127.1', 'loss/train': '0.68864', 'examples_per_second': '31.534', 'grad_norm': '116.5', 'counters/examples': 77280, 'counters/updates': 2415}
train stats after 77312 examples: {'rewards_train/chosen': '0.083803', 'rewards_train/rejected': '-0.022832', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10664', 'logps_train/rejected': '-169.15', 'logps_train/chosen': '-166.02', 'loss/train': '0.65616', 'examples_per_second': '30.096', 'grad_norm': '207', 'counters/examples': 77312, 'counters/updates': 2416}
train stats after 77344 examples: {'rewards_train/chosen': '0.038232', 'rewards_train/rejected': '0.053375', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015143', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-139.59', 'loss/train': '0.71907', 'examples_per_second': '31.788', 'grad_norm': '73', 'counters/examples': 77344, 'counters/updates': 2417}
train stats after 77376 examples: {'rewards_train/chosen': '0.036243', 'rewards_train/rejected': '0.015674', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020569', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-133.31', 'loss/train': '0.69248', 'examples_per_second': '31.7', 'grad_norm': '102.5', 'counters/examples': 77376, 'counters/updates': 2418}
train stats after 77408 examples: {'rewards_train/chosen': '0.11274', 'rewards_train/rejected': '-0.01201', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12475', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-135.29', 'loss/train': '0.6383', 'examples_per_second': '31.62', 'grad_norm': '79.5', 'counters/examples': 77408, 'counters/updates': 2419}
skipping logging after 77440 examples to avoid logging too frequently
train stats after 77472 examples: {'rewards_train/chosen': '0.0078049', 'rewards_train/rejected': '0.0049885', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0028163', 'logps_train/rejected': '-138.65', 'logps_train/chosen': '-117.51', 'loss/train': '0.70632', 'examples_per_second': '32.721', 'grad_norm': '81', 'counters/examples': 77472, 'counters/updates': 2421}
train stats after 77504 examples: {'rewards_train/chosen': '0.047356', 'rewards_train/rejected': '0.0075145', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039841', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-128.4', 'loss/train': '0.68523', 'examples_per_second': '31.327', 'grad_norm': '94.5', 'counters/examples': 77504, 'counters/updates': 2422}
skipping logging after 77536 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '0.058593', 'rewards_train/rejected': '0.030808', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027785', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-138', 'loss/train': '0.68952', 'examples_per_second': '30.272', 'grad_norm': '89.5', 'counters/examples': 77568, 'counters/updates': 2424}
train stats after 77600 examples: {'rewards_train/chosen': '0.066557', 'rewards_train/rejected': '0.059185', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0073721', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-155.31', 'loss/train': '0.69857', 'examples_per_second': '31.234', 'grad_norm': '93.5', 'counters/examples': 77600, 'counters/updates': 2425}
train stats after 77632 examples: {'rewards_train/chosen': '0.03448', 'rewards_train/rejected': '0.026319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0081604', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-142.47', 'loss/train': '0.70178', 'examples_per_second': '30.564', 'grad_norm': '68.5', 'counters/examples': 77632, 'counters/updates': 2426}
train stats after 77664 examples: {'rewards_train/chosen': '0.034046', 'rewards_train/rejected': '0.01524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018806', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-146.06', 'loss/train': '0.68836', 'examples_per_second': '32.184', 'grad_norm': '97', 'counters/examples': 77664, 'counters/updates': 2427}
train stats after 77696 examples: {'rewards_train/chosen': '0.036473', 'rewards_train/rejected': '0.037875', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014021', 'logps_train/rejected': '-153.63', 'logps_train/chosen': '-163.23', 'loss/train': '0.70268', 'examples_per_second': '31.644', 'grad_norm': '151', 'counters/examples': 77696, 'counters/updates': 2428}
train stats after 77728 examples: {'rewards_train/chosen': '0.058111', 'rewards_train/rejected': '0.030186', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027925', 'logps_train/rejected': '-155.57', 'logps_train/chosen': '-141.55', 'loss/train': '0.6888', 'examples_per_second': '32.751', 'grad_norm': '108', 'counters/examples': 77728, 'counters/updates': 2429}
train stats after 77760 examples: {'rewards_train/chosen': '0.025362', 'rewards_train/rejected': '-0.035688', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06105', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-176.43', 'loss/train': '0.67414', 'examples_per_second': '31.644', 'grad_norm': '141', 'counters/examples': 77760, 'counters/updates': 2430}
skipping logging after 77792 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '0.064393', 'rewards_train/rejected': '-0.038756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10315', 'logps_train/rejected': '-151.9', 'logps_train/chosen': '-180.25', 'loss/train': '0.66249', 'examples_per_second': '30.143', 'grad_norm': '97', 'counters/examples': 77824, 'counters/updates': 2432}
train stats after 77856 examples: {'rewards_train/chosen': '-0.0069551', 'rewards_train/rejected': '0.089869', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.096824', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-144.25', 'loss/train': '0.75435', 'examples_per_second': '30.668', 'grad_norm': '66.5', 'counters/examples': 77856, 'counters/updates': 2433}
train stats after 77888 examples: {'rewards_train/chosen': '0.064591', 'rewards_train/rejected': '0.036727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027865', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-137.01', 'loss/train': '0.69396', 'examples_per_second': '30.533', 'grad_norm': '68', 'counters/examples': 77888, 'counters/updates': 2434}
train stats after 77920 examples: {'rewards_train/chosen': '-0.038844', 'rewards_train/rejected': '0.050526', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.08937', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-166.78', 'loss/train': '0.74982', 'examples_per_second': '32.371', 'grad_norm': '87.5', 'counters/examples': 77920, 'counters/updates': 2435}
train stats after 77952 examples: {'rewards_train/chosen': '-0.021536', 'rewards_train/rejected': '-0.020402', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011339', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-149.05', 'loss/train': '0.69911', 'examples_per_second': '32.467', 'grad_norm': '84.5', 'counters/examples': 77952, 'counters/updates': 2436}
train stats after 77984 examples: {'rewards_train/chosen': '-0.015885', 'rewards_train/rejected': '0.033339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.049225', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-119.84', 'loss/train': '0.72858', 'examples_per_second': '30.269', 'grad_norm': '94.5', 'counters/examples': 77984, 'counters/updates': 2437}
train stats after 78016 examples: {'rewards_train/chosen': '0.10463', 'rewards_train/rejected': '-0.023861', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12849', 'logps_train/rejected': '-147.12', 'logps_train/chosen': '-171.48', 'loss/train': '0.64137', 'examples_per_second': '31.202', 'grad_norm': '82', 'counters/examples': 78016, 'counters/updates': 2438}
train stats after 78048 examples: {'rewards_train/chosen': '0.042442', 'rewards_train/rejected': '-0.0081834', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050625', 'logps_train/rejected': '-111.91', 'logps_train/chosen': '-134.62', 'loss/train': '0.67615', 'examples_per_second': '31.152', 'grad_norm': '74', 'counters/examples': 78048, 'counters/updates': 2439}
train stats after 78080 examples: {'rewards_train/chosen': '0.10834', 'rewards_train/rejected': '0.025221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083121', 'logps_train/rejected': '-173.57', 'logps_train/chosen': '-155.07', 'loss/train': '0.66289', 'examples_per_second': '31.581', 'grad_norm': '180', 'counters/examples': 78080, 'counters/updates': 2440}
skipping logging after 78112 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '-0.020442', 'rewards_train/rejected': '0.099172', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.11961', 'logps_train/rejected': '-134.09', 'logps_train/chosen': '-177.42', 'loss/train': '0.77556', 'examples_per_second': '36.608', 'grad_norm': '77.5', 'counters/examples': 78144, 'counters/updates': 2442}
train stats after 78176 examples: {'rewards_train/chosen': '-0.039379', 'rewards_train/rejected': '0.10642', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.1458', 'logps_train/rejected': '-152.02', 'logps_train/chosen': '-157.84', 'loss/train': '0.78681', 'examples_per_second': '30.154', 'grad_norm': '84.5', 'counters/examples': 78176, 'counters/updates': 2443}
train stats after 78208 examples: {'rewards_train/chosen': '-0.048322', 'rewards_train/rejected': '-0.036199', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012123', 'logps_train/rejected': '-154.16', 'logps_train/chosen': '-155.78', 'loss/train': '0.70758', 'examples_per_second': '31.334', 'grad_norm': '68.5', 'counters/examples': 78208, 'counters/updates': 2444}
train stats after 78240 examples: {'rewards_train/chosen': '0.046677', 'rewards_train/rejected': '0.013321', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033355', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-144.64', 'loss/train': '0.69036', 'examples_per_second': '29.936', 'grad_norm': '167', 'counters/examples': 78240, 'counters/updates': 2445}
train stats after 78272 examples: {'rewards_train/chosen': '0.084795', 'rewards_train/rejected': '0.066475', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.01832', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-167.02', 'loss/train': '0.70008', 'examples_per_second': '32.73', 'grad_norm': '145', 'counters/examples': 78272, 'counters/updates': 2446}
skipping logging after 78304 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '-0.016377', 'rewards_train/rejected': '0.028512', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044888', 'logps_train/rejected': '-144.9', 'logps_train/chosen': '-160.16', 'loss/train': '0.73284', 'examples_per_second': '31.52', 'grad_norm': '70', 'counters/examples': 78336, 'counters/updates': 2448}
train stats after 78368 examples: {'rewards_train/chosen': '0.026971', 'rewards_train/rejected': '0.025806', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.001165', 'logps_train/rejected': '-175.23', 'logps_train/chosen': '-136.31', 'loss/train': '0.70665', 'examples_per_second': '30.79', 'grad_norm': '69', 'counters/examples': 78368, 'counters/updates': 2449}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78432 examples: {'rewards_train/chosen': '-0.012668', 'rewards_train/rejected': '-0.010044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0026245', 'logps_train/rejected': '-119.8', 'logps_train/chosen': '-142.98', 'loss/train': '0.70893', 'examples_per_second': '34.058', 'grad_norm': '71.5', 'counters/examples': 78432, 'counters/updates': 2451}
train stats after 78464 examples: {'rewards_train/chosen': '0.028622', 'rewards_train/rejected': '-0.036554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065175', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-123.68', 'loss/train': '0.67153', 'examples_per_second': '31.648', 'grad_norm': '90', 'counters/examples': 78464, 'counters/updates': 2452}
train stats after 78496 examples: {'rewards_train/chosen': '0.21595', 'rewards_train/rejected': '-0.014985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23094', 'logps_train/rejected': '-103', 'logps_train/chosen': '-163.1', 'loss/train': '0.61339', 'examples_per_second': '31.785', 'grad_norm': '56.5', 'counters/examples': 78496, 'counters/updates': 2453}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78560 examples: {'rewards_train/chosen': '0.027196', 'rewards_train/rejected': '-0.026728', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053924', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-114.22', 'loss/train': '0.68319', 'examples_per_second': '31.597', 'grad_norm': '79.5', 'counters/examples': 78560, 'counters/updates': 2455}
skipping logging after 78592 examples to avoid logging too frequently
train stats after 78624 examples: {'rewards_train/chosen': '0.03653', 'rewards_train/rejected': '0.039856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0033253', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-137.71', 'loss/train': '0.70143', 'examples_per_second': '31.33', 'grad_norm': '87.5', 'counters/examples': 78624, 'counters/updates': 2457}
train stats after 78656 examples: {'rewards_train/chosen': '0.0027281', 'rewards_train/rejected': '-0.042891', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045619', 'logps_train/rejected': '-129.15', 'logps_train/chosen': '-198.34', 'loss/train': '0.68018', 'examples_per_second': '31.626', 'grad_norm': '70', 'counters/examples': 78656, 'counters/updates': 2458}
train stats after 78688 examples: {'rewards_train/chosen': '0.061909', 'rewards_train/rejected': '-0.019458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081367', 'logps_train/rejected': '-105.76', 'logps_train/chosen': '-137.24', 'loss/train': '0.67227', 'examples_per_second': '31.628', 'grad_norm': '78.5', 'counters/examples': 78688, 'counters/updates': 2459}
train stats after 78720 examples: {'rewards_train/chosen': '0.041936', 'rewards_train/rejected': '0.10413', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.062193', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-184.77', 'loss/train': '0.74013', 'examples_per_second': '30.613', 'grad_norm': '82.5', 'counters/examples': 78720, 'counters/updates': 2460}
train stats after 78752 examples: {'rewards_train/chosen': '0.13549', 'rewards_train/rejected': '0.044913', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090577', 'logps_train/rejected': '-97.199', 'logps_train/chosen': '-178.32', 'loss/train': '0.66149', 'examples_per_second': '31.102', 'grad_norm': '330', 'counters/examples': 78752, 'counters/updates': 2461}
train stats after 78784 examples: {'rewards_train/chosen': '0.081199', 'rewards_train/rejected': '0.034775', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046424', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-139.4', 'loss/train': '0.68677', 'examples_per_second': '31.667', 'grad_norm': '95.5', 'counters/examples': 78784, 'counters/updates': 2462}
train stats after 78816 examples: {'rewards_train/chosen': '0.065339', 'rewards_train/rejected': '0.044997', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020341', 'logps_train/rejected': '-134.14', 'logps_train/chosen': '-179.64', 'loss/train': '0.6948', 'examples_per_second': '32.81', 'grad_norm': '88.5', 'counters/examples': 78816, 'counters/updates': 2463}
train stats after 78848 examples: {'rewards_train/chosen': '0.015164', 'rewards_train/rejected': '0.033146', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017982', 'logps_train/rejected': '-175.09', 'logps_train/chosen': '-152.11', 'loss/train': '0.71554', 'examples_per_second': '33.135', 'grad_norm': '84', 'counters/examples': 78848, 'counters/updates': 2464}
train stats after 78880 examples: {'rewards_train/chosen': '0.057404', 'rewards_train/rejected': '-0.020192', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077596', 'logps_train/rejected': '-104.07', 'logps_train/chosen': '-129.38', 'loss/train': '0.66203', 'examples_per_second': '32.442', 'grad_norm': '59.75', 'counters/examples': 78880, 'counters/updates': 2465}
train stats after 78912 examples: {'rewards_train/chosen': '-0.019918', 'rewards_train/rejected': '0.041187', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.061105', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-165.09', 'loss/train': '0.74129', 'examples_per_second': '31.509', 'grad_norm': '86.5', 'counters/examples': 78912, 'counters/updates': 2466}
train stats after 78944 examples: {'rewards_train/chosen': '0.095645', 'rewards_train/rejected': '-0.012368', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10801', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-153.48', 'loss/train': '0.65604', 'examples_per_second': '30.962', 'grad_norm': '224', 'counters/examples': 78944, 'counters/updates': 2467}
train stats after 78976 examples: {'rewards_train/chosen': '0.0014418', 'rewards_train/rejected': '0.059695', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.058253', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-145.89', 'loss/train': '0.73503', 'examples_per_second': '30.056', 'grad_norm': '68', 'counters/examples': 78976, 'counters/updates': 2468}
train stats after 79008 examples: {'rewards_train/chosen': '0.051863', 'rewards_train/rejected': '0.01217', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039693', 'logps_train/rejected': '-103.78', 'logps_train/chosen': '-120.13', 'loss/train': '0.68329', 'examples_per_second': '31.321', 'grad_norm': '115', 'counters/examples': 79008, 'counters/updates': 2469}
train stats after 79040 examples: {'rewards_train/chosen': '0.053018', 'rewards_train/rejected': '0.049692', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0033263', 'logps_train/rejected': '-125.01', 'logps_train/chosen': '-161.26', 'loss/train': '0.70792', 'examples_per_second': '31.197', 'grad_norm': '105.5', 'counters/examples': 79040, 'counters/updates': 2470}
skipping logging after 79072 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '0.087733', 'rewards_train/rejected': '0.069375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018358', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-153.92', 'loss/train': '0.69513', 'examples_per_second': '34.953', 'grad_norm': '137', 'counters/examples': 79104, 'counters/updates': 2472}
skipping logging after 79136 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '0.058398', 'rewards_train/rejected': '0.046139', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012259', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-175.18', 'loss/train': '0.69417', 'examples_per_second': '31.787', 'grad_norm': '79.5', 'counters/examples': 79168, 'counters/updates': 2474}
train stats after 79200 examples: {'rewards_train/chosen': '-0.032684', 'rewards_train/rejected': '-0.091931', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059247', 'logps_train/rejected': '-164.61', 'logps_train/chosen': '-210.13', 'loss/train': '0.68048', 'examples_per_second': '30.615', 'grad_norm': '108', 'counters/examples': 79200, 'counters/updates': 2475}
skipping logging after 79232 examples to avoid logging too frequently
train stats after 79264 examples: {'rewards_train/chosen': '0.08235', 'rewards_train/rejected': '0.098478', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016128', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-150', 'loss/train': '0.71514', 'examples_per_second': '24.415', 'grad_norm': '119.5', 'counters/examples': 79264, 'counters/updates': 2477}
train stats after 79296 examples: {'rewards_train/chosen': '0.062774', 'rewards_train/rejected': '0.027644', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03513', 'logps_train/rejected': '-160.78', 'logps_train/chosen': '-155.89', 'loss/train': '0.70283', 'examples_per_second': '30.3', 'grad_norm': '93.5', 'counters/examples': 79296, 'counters/updates': 2478}
train stats after 79328 examples: {'rewards_train/chosen': '-0.0011385', 'rewards_train/rejected': '-0.035428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034289', 'logps_train/rejected': '-133.41', 'logps_train/chosen': '-147.76', 'loss/train': '0.68354', 'examples_per_second': '31.099', 'grad_norm': '57.75', 'counters/examples': 79328, 'counters/updates': 2479}
train stats after 79360 examples: {'rewards_train/chosen': '0.050422', 'rewards_train/rejected': '0.06474', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.014319', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-135.3', 'loss/train': '0.72087', 'examples_per_second': '27.831', 'grad_norm': '61.75', 'counters/examples': 79360, 'counters/updates': 2480}
train stats after 79392 examples: {'rewards_train/chosen': '0.042223', 'rewards_train/rejected': '0.059893', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017671', 'logps_train/rejected': '-139.38', 'logps_train/chosen': '-136.93', 'loss/train': '0.70779', 'examples_per_second': '31.331', 'grad_norm': '77.5', 'counters/examples': 79392, 'counters/updates': 2481}
train stats after 79424 examples: {'rewards_train/chosen': '0.05411', 'rewards_train/rejected': '-0.03393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08804', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-147.48', 'loss/train': '0.66138', 'examples_per_second': '32.178', 'grad_norm': '60', 'counters/examples': 79424, 'counters/updates': 2482}
train stats after 79456 examples: {'rewards_train/chosen': '0.05245', 'rewards_train/rejected': '0.063873', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011423', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-147.86', 'loss/train': '0.7133', 'examples_per_second': '32.153', 'grad_norm': '136', 'counters/examples': 79456, 'counters/updates': 2483}
train stats after 79488 examples: {'rewards_train/chosen': '0.098177', 'rewards_train/rejected': '-0.063376', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16155', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-159.79', 'loss/train': '0.64209', 'examples_per_second': '30.796', 'grad_norm': '79', 'counters/examples': 79488, 'counters/updates': 2484}
train stats after 79520 examples: {'rewards_train/chosen': '0.029791', 'rewards_train/rejected': '-0.020162', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049953', 'logps_train/rejected': '-157.67', 'logps_train/chosen': '-144.82', 'loss/train': '0.68377', 'examples_per_second': '31.626', 'grad_norm': '102', 'counters/examples': 79520, 'counters/updates': 2485}
skipping logging after 79552 examples to avoid logging too frequently
train stats after 79584 examples: {'rewards_train/chosen': '0.015775', 'rewards_train/rejected': '-0.026534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04231', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-197.17', 'loss/train': '0.68502', 'examples_per_second': '31.549', 'grad_norm': '85', 'counters/examples': 79584, 'counters/updates': 2487}
train stats after 79616 examples: {'rewards_train/chosen': '-0.03465', 'rewards_train/rejected': '0.03557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.070221', 'logps_train/rejected': '-181.06', 'logps_train/chosen': '-180.14', 'loss/train': '0.75249', 'examples_per_second': '31.456', 'grad_norm': '322', 'counters/examples': 79616, 'counters/updates': 2488}
train stats after 79648 examples: {'rewards_train/chosen': '0.13519', 'rewards_train/rejected': '0.076965', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058228', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-165.38', 'loss/train': '0.68963', 'examples_per_second': '32.07', 'grad_norm': '90.5', 'counters/examples': 79648, 'counters/updates': 2489}
train stats after 79680 examples: {'rewards_train/chosen': '0.082136', 'rewards_train/rejected': '-0.042424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12456', 'logps_train/rejected': '-124.81', 'logps_train/chosen': '-153.14', 'loss/train': '0.65593', 'examples_per_second': '31.61', 'grad_norm': '67', 'counters/examples': 79680, 'counters/updates': 2490}
train stats after 79712 examples: {'rewards_train/chosen': '0.16817', 'rewards_train/rejected': '0.0048197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16335', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-168.37', 'loss/train': '0.63647', 'examples_per_second': '31.144', 'grad_norm': '101', 'counters/examples': 79712, 'counters/updates': 2491}
train stats after 79744 examples: {'rewards_train/chosen': '-0.029695', 'rewards_train/rejected': '0.038937', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.068632', 'logps_train/rejected': '-136.85', 'logps_train/chosen': '-195.42', 'loss/train': '0.73636', 'examples_per_second': '30.617', 'grad_norm': '91.5', 'counters/examples': 79744, 'counters/updates': 2492}
train stats after 79776 examples: {'rewards_train/chosen': '-0.053611', 'rewards_train/rejected': '-0.039966', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013646', 'logps_train/rejected': '-168.41', 'logps_train/chosen': '-153.49', 'loss/train': '0.71319', 'examples_per_second': '31.611', 'grad_norm': '70.5', 'counters/examples': 79776, 'counters/updates': 2493}
train stats after 79808 examples: {'rewards_train/chosen': '0.028424', 'rewards_train/rejected': '-0.033791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062215', 'logps_train/rejected': '-153.8', 'logps_train/chosen': '-173.6', 'loss/train': '0.67443', 'examples_per_second': '32.475', 'grad_norm': '91', 'counters/examples': 79808, 'counters/updates': 2494}
train stats after 79840 examples: {'rewards_train/chosen': '0.0035292', 'rewards_train/rejected': '0.16157', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.15804', 'logps_train/rejected': '-145.11', 'logps_train/chosen': '-147.39', 'loss/train': '0.78933', 'examples_per_second': '30.946', 'grad_norm': '114', 'counters/examples': 79840, 'counters/updates': 2495}
train stats after 79872 examples: {'rewards_train/chosen': '-0.017902', 'rewards_train/rejected': '0.050681', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.068584', 'logps_train/rejected': '-165.23', 'logps_train/chosen': '-124.82', 'loss/train': '0.74706', 'examples_per_second': '31.614', 'grad_norm': '348', 'counters/examples': 79872, 'counters/updates': 2496}
train stats after 79904 examples: {'rewards_train/chosen': '0.010964', 'rewards_train/rejected': '0.044022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033057', 'logps_train/rejected': '-155.35', 'logps_train/chosen': '-152.01', 'loss/train': '0.72038', 'examples_per_second': '30.097', 'grad_norm': '125', 'counters/examples': 79904, 'counters/updates': 2497}
train stats after 79936 examples: {'rewards_train/chosen': '-0.032061', 'rewards_train/rejected': '0.024358', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.056419', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-167.97', 'loss/train': '0.73199', 'examples_per_second': '31.567', 'grad_norm': '117.5', 'counters/examples': 79936, 'counters/updates': 2498}
train stats after 79968 examples: {'rewards_train/chosen': '-0.0048992', 'rewards_train/rejected': '0.0081227', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013022', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-131.5', 'loss/train': '0.71285', 'examples_per_second': '31.54', 'grad_norm': '88.5', 'counters/examples': 79968, 'counters/updates': 2499}
train stats after 80000 examples: {'rewards_train/chosen': '-0.032098', 'rewards_train/rejected': '-0.063635', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031537', 'logps_train/rejected': '-181.84', 'logps_train/chosen': '-158.71', 'loss/train': '0.69021', 'examples_per_second': '31.565', 'grad_norm': '79', 'counters/examples': 80000, 'counters/updates': 2500}
train stats after 80032 examples: {'rewards_train/chosen': '0.01012', 'rewards_train/rejected': '0.0031918', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0069285', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-145.18', 'loss/train': '0.70321', 'examples_per_second': '30.501', 'grad_norm': '111', 'counters/examples': 80032, 'counters/updates': 2501}
skipping logging after 80064 examples to avoid logging too frequently
train stats after 80096 examples: {'rewards_train/chosen': '0.01276', 'rewards_train/rejected': '0.0062969', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0064632', 'logps_train/rejected': '-136.15', 'logps_train/chosen': '-157.57', 'loss/train': '0.69743', 'examples_per_second': '36.096', 'grad_norm': '79', 'counters/examples': 80096, 'counters/updates': 2503}
skipping logging after 80128 examples to avoid logging too frequently
train stats after 80160 examples: {'rewards_train/chosen': '0.071415', 'rewards_train/rejected': '0.054186', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017229', 'logps_train/rejected': '-122.69', 'logps_train/chosen': '-125.67', 'loss/train': '0.70143', 'examples_per_second': '34.524', 'grad_norm': '95.5', 'counters/examples': 80160, 'counters/updates': 2505}
train stats after 80192 examples: {'rewards_train/chosen': '-0.022277', 'rewards_train/rejected': '0.053289', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.075566', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-129.26', 'loss/train': '0.74445', 'examples_per_second': '31.85', 'grad_norm': '117.5', 'counters/examples': 80192, 'counters/updates': 2506}
train stats after 80224 examples: {'rewards_train/chosen': '0.048919', 'rewards_train/rejected': '-0.015845', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064763', 'logps_train/rejected': '-118.76', 'logps_train/chosen': '-146.81', 'loss/train': '0.67404', 'examples_per_second': '32.834', 'grad_norm': '70', 'counters/examples': 80224, 'counters/updates': 2507}
train stats after 80256 examples: {'rewards_train/chosen': '0.11783', 'rewards_train/rejected': '0.084796', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033033', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-154.7', 'loss/train': '0.69062', 'examples_per_second': '31.593', 'grad_norm': '102.5', 'counters/examples': 80256, 'counters/updates': 2508}
train stats after 80288 examples: {'rewards_train/chosen': '0.069921', 'rewards_train/rejected': '-0.023017', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092938', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-126.17', 'loss/train': '0.65981', 'examples_per_second': '32.456', 'grad_norm': '73', 'counters/examples': 80288, 'counters/updates': 2509}
train stats after 80320 examples: {'rewards_train/chosen': '-0.025585', 'rewards_train/rejected': '-0.09281', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067225', 'logps_train/rejected': '-180.09', 'logps_train/chosen': '-147.4', 'loss/train': '0.67975', 'examples_per_second': '30.456', 'grad_norm': '102', 'counters/examples': 80320, 'counters/updates': 2510}
train stats after 80352 examples: {'rewards_train/chosen': '0.050094', 'rewards_train/rejected': '0.045804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0042902', 'logps_train/rejected': '-123.41', 'logps_train/chosen': '-175.42', 'loss/train': '0.69909', 'examples_per_second': '30.163', 'grad_norm': '93', 'counters/examples': 80352, 'counters/updates': 2511}
skipping logging after 80384 examples to avoid logging too frequently
train stats after 80416 examples: {'rewards_train/chosen': '-0.011252', 'rewards_train/rejected': '-0.027837', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016585', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-129.2', 'loss/train': '0.69501', 'examples_per_second': '33.35', 'grad_norm': '94.5', 'counters/examples': 80416, 'counters/updates': 2513}
train stats after 80448 examples: {'rewards_train/chosen': '0.036674', 'rewards_train/rejected': '0.03801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0013357', 'logps_train/rejected': '-137.78', 'logps_train/chosen': '-130.2', 'loss/train': '0.69938', 'examples_per_second': '31.572', 'grad_norm': '59', 'counters/examples': 80448, 'counters/updates': 2514}
train stats after 80480 examples: {'rewards_train/chosen': '0.022382', 'rewards_train/rejected': '-0.010177', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032559', 'logps_train/rejected': '-100.03', 'logps_train/chosen': '-131.35', 'loss/train': '0.68545', 'examples_per_second': '31.777', 'grad_norm': '99', 'counters/examples': 80480, 'counters/updates': 2515}
skipping logging after 80512 examples to avoid logging too frequently
train stats after 80544 examples: {'rewards_train/chosen': '0.12061', 'rewards_train/rejected': '-0.0014113', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12202', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-118.43', 'loss/train': '0.64198', 'examples_per_second': '30.004', 'grad_norm': '55.5', 'counters/examples': 80544, 'counters/updates': 2517}
train stats after 80576 examples: {'rewards_train/chosen': '0.060188', 'rewards_train/rejected': '0.045783', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014406', 'logps_train/rejected': '-100.06', 'logps_train/chosen': '-120.42', 'loss/train': '0.69952', 'examples_per_second': '30.573', 'grad_norm': '99', 'counters/examples': 80576, 'counters/updates': 2518}
train stats after 80608 examples: {'rewards_train/chosen': '0.043727', 'rewards_train/rejected': '0.034328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0093982', 'logps_train/rejected': '-151.5', 'logps_train/chosen': '-178.28', 'loss/train': '0.6961', 'examples_per_second': '31.415', 'grad_norm': '288', 'counters/examples': 80608, 'counters/updates': 2519}
train stats after 80640 examples: {'rewards_train/chosen': '0.069179', 'rewards_train/rejected': '0.053107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016072', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-136', 'loss/train': '0.69004', 'examples_per_second': '30.471', 'grad_norm': '77', 'counters/examples': 80640, 'counters/updates': 2520}
train stats after 80672 examples: {'rewards_train/chosen': '0.048987', 'rewards_train/rejected': '0.068363', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019376', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-139.33', 'loss/train': '0.72415', 'examples_per_second': '30.901', 'grad_norm': '67.5', 'counters/examples': 80672, 'counters/updates': 2521}
train stats after 80704 examples: {'rewards_train/chosen': '0.090269', 'rewards_train/rejected': '0.024605', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065664', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-179.04', 'loss/train': '0.68241', 'examples_per_second': '31.622', 'grad_norm': '115.5', 'counters/examples': 80704, 'counters/updates': 2522}
train stats after 80736 examples: {'rewards_train/chosen': '0.071448', 'rewards_train/rejected': '-0.00096663', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072414', 'logps_train/rejected': '-108.94', 'logps_train/chosen': '-140.32', 'loss/train': '0.66997', 'examples_per_second': '32.918', 'grad_norm': '142', 'counters/examples': 80736, 'counters/updates': 2523}
train stats after 80768 examples: {'rewards_train/chosen': '-0.0017653', 'rewards_train/rejected': '0.02176', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023525', 'logps_train/rejected': '-133.35', 'logps_train/chosen': '-140.72', 'loss/train': '0.71672', 'examples_per_second': '30.573', 'grad_norm': '91', 'counters/examples': 80768, 'counters/updates': 2524}
train stats after 80800 examples: {'rewards_train/chosen': '-0.042137', 'rewards_train/rejected': '0.029981', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.072118', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-155.66', 'loss/train': '0.74821', 'examples_per_second': '32.986', 'grad_norm': '137', 'counters/examples': 80800, 'counters/updates': 2525}
train stats after 80832 examples: {'rewards_train/chosen': '0.093035', 'rewards_train/rejected': '0.053748', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039287', 'logps_train/rejected': '-133.19', 'logps_train/chosen': '-156.11', 'loss/train': '0.70391', 'examples_per_second': '31.593', 'grad_norm': '117.5', 'counters/examples': 80832, 'counters/updates': 2526}
train stats after 80864 examples: {'rewards_train/chosen': '-0.044847', 'rewards_train/rejected': '-0.01059', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034257', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-153.47', 'loss/train': '0.72561', 'examples_per_second': '31.53', 'grad_norm': '135', 'counters/examples': 80864, 'counters/updates': 2527}
train stats after 80896 examples: {'rewards_train/chosen': '-0.011263', 'rewards_train/rejected': '0.052763', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.064026', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-145.13', 'loss/train': '0.7332', 'examples_per_second': '24.817', 'grad_norm': '79', 'counters/examples': 80896, 'counters/updates': 2528}
train stats after 80928 examples: {'rewards_train/chosen': '0.032936', 'rewards_train/rejected': '0.10261', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069677', 'logps_train/rejected': '-169.47', 'logps_train/chosen': '-174.15', 'loss/train': '0.74546', 'examples_per_second': '31.104', 'grad_norm': '143', 'counters/examples': 80928, 'counters/updates': 2529}
train stats after 80960 examples: {'rewards_train/chosen': '0.054601', 'rewards_train/rejected': '0.069474', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014873', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-169.12', 'loss/train': '0.71931', 'examples_per_second': '31.506', 'grad_norm': '234', 'counters/examples': 80960, 'counters/updates': 2530}
skipping logging after 80992 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '0.012637', 'rewards_train/rejected': '-0.01062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023258', 'logps_train/rejected': '-109.36', 'logps_train/chosen': '-162.33', 'loss/train': '0.6993', 'examples_per_second': '34.333', 'grad_norm': '135', 'counters/examples': 81024, 'counters/updates': 2532}
train stats after 81056 examples: {'rewards_train/chosen': '0.076471', 'rewards_train/rejected': '0.037673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038799', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-145.57', 'loss/train': '0.68213', 'examples_per_second': '31.108', 'grad_norm': '76.5', 'counters/examples': 81056, 'counters/updates': 2533}
train stats after 81088 examples: {'rewards_train/chosen': '0.042678', 'rewards_train/rejected': '-0.014492', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05717', 'logps_train/rejected': '-152.4', 'logps_train/chosen': '-166.13', 'loss/train': '0.67459', 'examples_per_second': '31.407', 'grad_norm': '73', 'counters/examples': 81088, 'counters/updates': 2534}
train stats after 81120 examples: {'rewards_train/chosen': '0.12253', 'rewards_train/rejected': '0.0144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10813', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-154.65', 'loss/train': '0.66359', 'examples_per_second': '31.631', 'grad_norm': '88', 'counters/examples': 81120, 'counters/updates': 2535}
train stats after 81152 examples: {'rewards_train/chosen': '-0.038709', 'rewards_train/rejected': '-0.039717', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0010085', 'logps_train/rejected': '-121', 'logps_train/chosen': '-158.1', 'loss/train': '0.70622', 'examples_per_second': '30.779', 'grad_norm': '86', 'counters/examples': 81152, 'counters/updates': 2536}
skipping logging after 81184 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '0.017294', 'rewards_train/rejected': '-0.0090359', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02633', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-154.13', 'loss/train': '0.68315', 'examples_per_second': '31.764', 'grad_norm': '87', 'counters/examples': 81216, 'counters/updates': 2538}
train stats after 81248 examples: {'rewards_train/chosen': '0.050094', 'rewards_train/rejected': '0.079069', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028975', 'logps_train/rejected': '-157.69', 'logps_train/chosen': '-185.3', 'loss/train': '0.72263', 'examples_per_second': '31.777', 'grad_norm': '144', 'counters/examples': 81248, 'counters/updates': 2539}
train stats after 81280 examples: {'rewards_train/chosen': '0.045551', 'rewards_train/rejected': '0.10805', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.062501', 'logps_train/rejected': '-130.51', 'logps_train/chosen': '-125.03', 'loss/train': '0.74172', 'examples_per_second': '31.568', 'grad_norm': '155', 'counters/examples': 81280, 'counters/updates': 2540}
train stats after 81312 examples: {'rewards_train/chosen': '0.057339', 'rewards_train/rejected': '0.015063', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042275', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-144.81', 'loss/train': '0.68227', 'examples_per_second': '30.566', 'grad_norm': '85', 'counters/examples': 81312, 'counters/updates': 2541}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81376 examples: {'rewards_train/chosen': '-0.0056946', 'rewards_train/rejected': '-0.0016931', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0040015', 'logps_train/rejected': '-110.34', 'logps_train/chosen': '-131.2', 'loss/train': '0.7061', 'examples_per_second': '30.795', 'grad_norm': '71.5', 'counters/examples': 81376, 'counters/updates': 2543}
train stats after 81408 examples: {'rewards_train/chosen': '0.09885', 'rewards_train/rejected': '0.032027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066823', 'logps_train/rejected': '-156.73', 'logps_train/chosen': '-157.9', 'loss/train': '0.6695', 'examples_per_second': '31.311', 'grad_norm': '111.5', 'counters/examples': 81408, 'counters/updates': 2544}
skipping logging after 81440 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '0.091684', 'rewards_train/rejected': '0.034881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056803', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-188.98', 'loss/train': '0.6868', 'examples_per_second': '30.471', 'grad_norm': '89.5', 'counters/examples': 81472, 'counters/updates': 2546}
train stats after 81504 examples: {'rewards_train/chosen': '0.066418', 'rewards_train/rejected': '0.041967', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024452', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-137.43', 'loss/train': '0.6888', 'examples_per_second': '31.458', 'grad_norm': '133', 'counters/examples': 81504, 'counters/updates': 2547}
skipping logging after 81536 examples to avoid logging too frequently
train stats after 81568 examples: {'rewards_train/chosen': '-0.012988', 'rewards_train/rejected': '0.033801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046789', 'logps_train/rejected': '-137.49', 'logps_train/chosen': '-105.27', 'loss/train': '0.72701', 'examples_per_second': '38.575', 'grad_norm': '226', 'counters/examples': 81568, 'counters/updates': 2549}
train stats after 81600 examples: {'rewards_train/chosen': '0.10325', 'rewards_train/rejected': '0.055457', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047798', 'logps_train/rejected': '-140.15', 'logps_train/chosen': '-128.63', 'loss/train': '0.68449', 'examples_per_second': '30.114', 'grad_norm': '75.5', 'counters/examples': 81600, 'counters/updates': 2550}
train stats after 81632 examples: {'rewards_train/chosen': '0.05196', 'rewards_train/rejected': '-0.021166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073126', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-160.94', 'loss/train': '0.67162', 'examples_per_second': '31.58', 'grad_norm': '211', 'counters/examples': 81632, 'counters/updates': 2551}
train stats after 81664 examples: {'rewards_train/chosen': '0.01358', 'rewards_train/rejected': '-0.01112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0247', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-136.25', 'loss/train': '0.6907', 'examples_per_second': '30.579', 'grad_norm': '92.5', 'counters/examples': 81664, 'counters/updates': 2552}
train stats after 81696 examples: {'rewards_train/chosen': '0.078592', 'rewards_train/rejected': '0.067458', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011135', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-108.41', 'loss/train': '0.70106', 'examples_per_second': '32.989', 'grad_norm': '128', 'counters/examples': 81696, 'counters/updates': 2553}
train stats after 81728 examples: {'rewards_train/chosen': '0.034561', 'rewards_train/rejected': '0.009724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024837', 'logps_train/rejected': '-134.64', 'logps_train/chosen': '-178.8', 'loss/train': '0.68919', 'examples_per_second': '31.609', 'grad_norm': '80.5', 'counters/examples': 81728, 'counters/updates': 2554}
train stats after 81760 examples: {'rewards_train/chosen': '0.090667', 'rewards_train/rejected': '0.073609', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017058', 'logps_train/rejected': '-153.83', 'logps_train/chosen': '-183.83', 'loss/train': '0.69908', 'examples_per_second': '30.914', 'grad_norm': '74', 'counters/examples': 81760, 'counters/updates': 2555}
train stats after 81792 examples: {'rewards_train/chosen': '0.0389', 'rewards_train/rejected': '0.0098147', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029085', 'logps_train/rejected': '-151.53', 'logps_train/chosen': '-147.47', 'loss/train': '0.68943', 'examples_per_second': '31.664', 'grad_norm': '52.25', 'counters/examples': 81792, 'counters/updates': 2556}
train stats after 81824 examples: {'rewards_train/chosen': '0.028449', 'rewards_train/rejected': '-0.024456', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052905', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-163.44', 'loss/train': '0.67376', 'examples_per_second': '31.554', 'grad_norm': '65.5', 'counters/examples': 81824, 'counters/updates': 2557}
train stats after 81856 examples: {'rewards_train/chosen': '0.044989', 'rewards_train/rejected': '0.0059246', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039064', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-104.67', 'loss/train': '0.67972', 'examples_per_second': '31.585', 'grad_norm': '73', 'counters/examples': 81856, 'counters/updates': 2558}
train stats after 81888 examples: {'rewards_train/chosen': '0.060596', 'rewards_train/rejected': '0.0024148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058181', 'logps_train/rejected': '-160.66', 'logps_train/chosen': '-167.52', 'loss/train': '0.67835', 'examples_per_second': '31.542', 'grad_norm': '162', 'counters/examples': 81888, 'counters/updates': 2559}
train stats after 81920 examples: {'rewards_train/chosen': '0.037306', 'rewards_train/rejected': '0.12837', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.091065', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-142.08', 'loss/train': '0.7537', 'examples_per_second': '30.921', 'grad_norm': '157', 'counters/examples': 81920, 'counters/updates': 2560}
train stats after 81952 examples: {'rewards_train/chosen': '-0.019138', 'rewards_train/rejected': '0.043259', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.062397', 'logps_train/rejected': '-105.36', 'logps_train/chosen': '-138.85', 'loss/train': '0.73658', 'examples_per_second': '32.537', 'grad_norm': '119.5', 'counters/examples': 81952, 'counters/updates': 2561}
train stats after 81984 examples: {'rewards_train/chosen': '-0.00066201', 'rewards_train/rejected': '0.0082133', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0088753', 'logps_train/rejected': '-158.09', 'logps_train/chosen': '-194.16', 'loss/train': '0.71653', 'examples_per_second': '31.574', 'grad_norm': '117', 'counters/examples': 81984, 'counters/updates': 2562}
train stats after 82016 examples: {'rewards_train/chosen': '0.12738', 'rewards_train/rejected': '0.0015969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12578', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-169.27', 'loss/train': '0.65453', 'examples_per_second': '31.48', 'grad_norm': '64.5', 'counters/examples': 82016, 'counters/updates': 2563}
skipping logging after 82048 examples to avoid logging too frequently
train stats after 82080 examples: {'rewards_train/chosen': '0.030127', 'rewards_train/rejected': '-0.048966', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079094', 'logps_train/rejected': '-117.04', 'logps_train/chosen': '-121.93', 'loss/train': '0.66019', 'examples_per_second': '32.13', 'grad_norm': '75', 'counters/examples': 82080, 'counters/updates': 2565}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82144 examples: {'rewards_train/chosen': '-0.0026099', 'rewards_train/rejected': '0.033981', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036591', 'logps_train/rejected': '-140.05', 'logps_train/chosen': '-135.2', 'loss/train': '0.72922', 'examples_per_second': '31.561', 'grad_norm': '106', 'counters/examples': 82144, 'counters/updates': 2567}
train stats after 82176 examples: {'rewards_train/chosen': '0.095057', 'rewards_train/rejected': '-0.02374', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1188', 'logps_train/rejected': '-147.26', 'logps_train/chosen': '-156.67', 'loss/train': '0.64948', 'examples_per_second': '31.581', 'grad_norm': '112', 'counters/examples': 82176, 'counters/updates': 2568}
train stats after 82208 examples: {'rewards_train/chosen': '-0.0061683', 'rewards_train/rejected': '0.0080239', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014192', 'logps_train/rejected': '-133.74', 'logps_train/chosen': '-173.31', 'loss/train': '0.71096', 'examples_per_second': '32.708', 'grad_norm': '134', 'counters/examples': 82208, 'counters/updates': 2569}
train stats after 82240 examples: {'rewards_train/chosen': '-0.023823', 'rewards_train/rejected': '0.00089961', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024722', 'logps_train/rejected': '-95.267', 'logps_train/chosen': '-195.32', 'loss/train': '0.71841', 'examples_per_second': '32.52', 'grad_norm': '102', 'counters/examples': 82240, 'counters/updates': 2570}
train stats after 82272 examples: {'rewards_train/chosen': '0.087577', 'rewards_train/rejected': '0.049906', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037671', 'logps_train/rejected': '-129.84', 'logps_train/chosen': '-145.37', 'loss/train': '0.68465', 'examples_per_second': '32.1', 'grad_norm': '93', 'counters/examples': 82272, 'counters/updates': 2571}
train stats after 82304 examples: {'rewards_train/chosen': '0.019117', 'rewards_train/rejected': '0.063779', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044662', 'logps_train/rejected': '-148.02', 'logps_train/chosen': '-161.9', 'loss/train': '0.72537', 'examples_per_second': '31.803', 'grad_norm': '73', 'counters/examples': 82304, 'counters/updates': 2572}
skipping logging after 82336 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '0.017785', 'rewards_train/rejected': '-0.0026526', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.020438', 'logps_train/rejected': '-116.93', 'logps_train/chosen': '-156.69', 'loss/train': '0.69807', 'examples_per_second': '33.422', 'grad_norm': '74.5', 'counters/examples': 82368, 'counters/updates': 2574}
train stats after 82400 examples: {'rewards_train/chosen': '0.010429', 'rewards_train/rejected': '-0.043464', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053893', 'logps_train/rejected': '-183.34', 'logps_train/chosen': '-178.44', 'loss/train': '0.68265', 'examples_per_second': '31.566', 'grad_norm': '194', 'counters/examples': 82400, 'counters/updates': 2575}
train stats after 82432 examples: {'rewards_train/chosen': '-0.013308', 'rewards_train/rejected': '0.038439', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.051747', 'logps_train/rejected': '-160.53', 'logps_train/chosen': '-158.29', 'loss/train': '0.72989', 'examples_per_second': '32.226', 'grad_norm': '102', 'counters/examples': 82432, 'counters/updates': 2576}
train stats after 82464 examples: {'rewards_train/chosen': '0.00012271', 'rewards_train/rejected': '-0.063616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063738', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-136.35', 'loss/train': '0.67098', 'examples_per_second': '30.654', 'grad_norm': '66', 'counters/examples': 82464, 'counters/updates': 2577}
train stats after 82496 examples: {'rewards_train/chosen': '-0.0073772', 'rewards_train/rejected': '0.038533', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.04591', 'logps_train/rejected': '-174.79', 'logps_train/chosen': '-178.95', 'loss/train': '0.72855', 'examples_per_second': '31.562', 'grad_norm': '80.5', 'counters/examples': 82496, 'counters/updates': 2578}
train stats after 82528 examples: {'rewards_train/chosen': '0.048845', 'rewards_train/rejected': '0.055797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0069519', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-164.09', 'loss/train': '0.70376', 'examples_per_second': '30.463', 'grad_norm': '77.5', 'counters/examples': 82528, 'counters/updates': 2579}
train stats after 82560 examples: {'rewards_train/chosen': '0.10445', 'rewards_train/rejected': '0.021194', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083257', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-167.95', 'loss/train': '0.67104', 'examples_per_second': '30.787', 'grad_norm': '118.5', 'counters/examples': 82560, 'counters/updates': 2580}
train stats after 82592 examples: {'rewards_train/chosen': '0.0088956', 'rewards_train/rejected': '0.045204', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036309', 'logps_train/rejected': '-133', 'logps_train/chosen': '-169.34', 'loss/train': '0.72984', 'examples_per_second': '31.524', 'grad_norm': '153', 'counters/examples': 82592, 'counters/updates': 2581}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82656 examples: {'rewards_train/chosen': '0.08973', 'rewards_train/rejected': '0.064831', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024898', 'logps_train/rejected': '-128.03', 'logps_train/chosen': '-124.46', 'loss/train': '0.69528', 'examples_per_second': '31.764', 'grad_norm': '118.5', 'counters/examples': 82656, 'counters/updates': 2583}
train stats after 82688 examples: {'rewards_train/chosen': '0.053907', 'rewards_train/rejected': '0.13188', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077973', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-157.75', 'loss/train': '0.74088', 'examples_per_second': '29.928', 'grad_norm': '98.5', 'counters/examples': 82688, 'counters/updates': 2584}
train stats after 82720 examples: {'rewards_train/chosen': '0.10978', 'rewards_train/rejected': '-0.033875', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14366', 'logps_train/rejected': '-127.99', 'logps_train/chosen': '-132.53', 'loss/train': '0.63201', 'examples_per_second': '31.75', 'grad_norm': '111.5', 'counters/examples': 82720, 'counters/updates': 2585}
train stats after 82752 examples: {'rewards_train/chosen': '-0.0048106', 'rewards_train/rejected': '0.014521', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019332', 'logps_train/rejected': '-140.13', 'logps_train/chosen': '-158.41', 'loss/train': '0.71834', 'examples_per_second': '30.624', 'grad_norm': '125', 'counters/examples': 82752, 'counters/updates': 2586}
train stats after 82784 examples: {'rewards_train/chosen': '0.033195', 'rewards_train/rejected': '0.027504', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0056911', 'logps_train/rejected': '-126.95', 'logps_train/chosen': '-143.68', 'loss/train': '0.69622', 'examples_per_second': '31.594', 'grad_norm': '61', 'counters/examples': 82784, 'counters/updates': 2587}
train stats after 82816 examples: {'rewards_train/chosen': '0.081688', 'rewards_train/rejected': '0.035304', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046383', 'logps_train/rejected': '-111.26', 'logps_train/chosen': '-138.57', 'loss/train': '0.68191', 'examples_per_second': '32.442', 'grad_norm': '99.5', 'counters/examples': 82816, 'counters/updates': 2588}
train stats after 82848 examples: {'rewards_train/chosen': '0.088014', 'rewards_train/rejected': '0.021808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066206', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-170.79', 'loss/train': '0.68651', 'examples_per_second': '31.429', 'grad_norm': '70', 'counters/examples': 82848, 'counters/updates': 2589}
train stats after 82880 examples: {'rewards_train/chosen': '0.040106', 'rewards_train/rejected': '-0.059893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099999', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-136.92', 'loss/train': '0.6558', 'examples_per_second': '31.439', 'grad_norm': '89.5', 'counters/examples': 82880, 'counters/updates': 2590}
train stats after 82912 examples: {'rewards_train/chosen': '0.049144', 'rewards_train/rejected': '-0.01212', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061264', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-160.59', 'loss/train': '0.67106', 'examples_per_second': '31.487', 'grad_norm': '148', 'counters/examples': 82912, 'counters/updates': 2591}
train stats after 82944 examples: {'rewards_train/chosen': '0.012034', 'rewards_train/rejected': '0.046923', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03489', 'logps_train/rejected': '-176.44', 'logps_train/chosen': '-182.32', 'loss/train': '0.73127', 'examples_per_second': '31.399', 'grad_norm': '221', 'counters/examples': 82944, 'counters/updates': 2592}
train stats after 82976 examples: {'rewards_train/chosen': '0.024806', 'rewards_train/rejected': '0.088186', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.06338', 'logps_train/rejected': '-161.9', 'logps_train/chosen': '-185.24', 'loss/train': '0.73874', 'examples_per_second': '31.272', 'grad_norm': '135', 'counters/examples': 82976, 'counters/updates': 2593}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83040 examples: {'rewards_train/chosen': '0.11743', 'rewards_train/rejected': '0.048317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069112', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-160.3', 'loss/train': '0.67266', 'examples_per_second': '31.739', 'grad_norm': '95.5', 'counters/examples': 83040, 'counters/updates': 2595}
train stats after 83072 examples: {'rewards_train/chosen': '0.066225', 'rewards_train/rejected': '0.025417', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.040808', 'logps_train/rejected': '-147.94', 'logps_train/chosen': '-152.94', 'loss/train': '0.69277', 'examples_per_second': '30.136', 'grad_norm': '75.5', 'counters/examples': 83072, 'counters/updates': 2596}
train stats after 83104 examples: {'rewards_train/chosen': '0.057557', 'rewards_train/rejected': '-0.0087517', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066309', 'logps_train/rejected': '-128.76', 'logps_train/chosen': '-135.82', 'loss/train': '0.66631', 'examples_per_second': '30.185', 'grad_norm': '133', 'counters/examples': 83104, 'counters/updates': 2597}
train stats after 83136 examples: {'rewards_train/chosen': '0.079818', 'rewards_train/rejected': '0.043463', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036354', 'logps_train/rejected': '-148.21', 'logps_train/chosen': '-131.96', 'loss/train': '0.68453', 'examples_per_second': '30.857', 'grad_norm': '137', 'counters/examples': 83136, 'counters/updates': 2598}
train stats after 83168 examples: {'rewards_train/chosen': '0.013654', 'rewards_train/rejected': '0.060218', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.046564', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-142.13', 'loss/train': '0.73199', 'examples_per_second': '30.125', 'grad_norm': '88.5', 'counters/examples': 83168, 'counters/updates': 2599}
skipping logging after 83200 examples to avoid logging too frequently
train stats after 83232 examples: {'rewards_train/chosen': '0.02665', 'rewards_train/rejected': '0.057528', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030878', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-114.9', 'loss/train': '0.71499', 'examples_per_second': '33.952', 'grad_norm': '86', 'counters/examples': 83232, 'counters/updates': 2601}
train stats after 83264 examples: {'rewards_train/chosen': '0.1353', 'rewards_train/rejected': '0.046662', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088639', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-149.18', 'loss/train': '0.66013', 'examples_per_second': '31.442', 'grad_norm': '65', 'counters/examples': 83264, 'counters/updates': 2602}
train stats after 83296 examples: {'rewards_train/chosen': '-0.051939', 'rewards_train/rejected': '0.021574', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.073513', 'logps_train/rejected': '-149.23', 'logps_train/chosen': '-162.5', 'loss/train': '0.75051', 'examples_per_second': '32.602', 'grad_norm': '127', 'counters/examples': 83296, 'counters/updates': 2603}
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83360 examples: {'rewards_train/chosen': '0.05', 'rewards_train/rejected': '0.015341', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034659', 'logps_train/rejected': '-91.261', 'logps_train/chosen': '-167.21', 'loss/train': '0.69043', 'examples_per_second': '31.218', 'grad_norm': '89', 'counters/examples': 83360, 'counters/updates': 2605}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83424 examples: {'rewards_train/chosen': '-0.050063', 'rewards_train/rejected': '0.091382', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.14145', 'logps_train/rejected': '-150.62', 'logps_train/chosen': '-158.66', 'loss/train': '0.7827', 'examples_per_second': '30.987', 'grad_norm': '85.5', 'counters/examples': 83424, 'counters/updates': 2607}
train stats after 83456 examples: {'rewards_train/chosen': '-0.015952', 'rewards_train/rejected': '0.0272', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043152', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-161.99', 'loss/train': '0.72662', 'examples_per_second': '31.409', 'grad_norm': '117', 'counters/examples': 83456, 'counters/updates': 2608}
skipping logging after 83488 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '0.085396', 'rewards_train/rejected': '0.10294', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017547', 'logps_train/rejected': '-191.45', 'logps_train/chosen': '-165.94', 'loss/train': '0.71584', 'examples_per_second': '32.434', 'grad_norm': '89', 'counters/examples': 83520, 'counters/updates': 2610}
train stats after 83552 examples: {'rewards_train/chosen': '0.12916', 'rewards_train/rejected': '0.050897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078259', 'logps_train/rejected': '-154.76', 'logps_train/chosen': '-140.92', 'loss/train': '0.67051', 'examples_per_second': '32.739', 'grad_norm': '117.5', 'counters/examples': 83552, 'counters/updates': 2611}
skipping logging after 83584 examples to avoid logging too frequently
train stats after 83616 examples: {'rewards_train/chosen': '0.097927', 'rewards_train/rejected': '-0.017998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11592', 'logps_train/rejected': '-154.84', 'logps_train/chosen': '-138.68', 'loss/train': '0.64689', 'examples_per_second': '33.479', 'grad_norm': '147', 'counters/examples': 83616, 'counters/updates': 2613}
skipping logging after 83648 examples to avoid logging too frequently
train stats after 83680 examples: {'rewards_train/chosen': '0.050201', 'rewards_train/rejected': '-0.07415', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12435', 'logps_train/rejected': '-142.87', 'logps_train/chosen': '-131.18', 'loss/train': '0.6449', 'examples_per_second': '32.333', 'grad_norm': '136', 'counters/examples': 83680, 'counters/updates': 2615}
train stats after 83712 examples: {'rewards_train/chosen': '0.058548', 'rewards_train/rejected': '0.00092094', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057627', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-151.21', 'loss/train': '0.67175', 'examples_per_second': '31.057', 'grad_norm': '83.5', 'counters/examples': 83712, 'counters/updates': 2616}
train stats after 83744 examples: {'rewards_train/chosen': '0.075129', 'rewards_train/rejected': '0.029892', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045237', 'logps_train/rejected': '-130.78', 'logps_train/chosen': '-173.33', 'loss/train': '0.68405', 'examples_per_second': '30.366', 'grad_norm': '106', 'counters/examples': 83744, 'counters/updates': 2617}
skipping logging after 83776 examples to avoid logging too frequently
train stats after 83808 examples: {'rewards_train/chosen': '0.074222', 'rewards_train/rejected': '-0.0047847', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079006', 'logps_train/rejected': '-96.724', 'logps_train/chosen': '-95.206', 'loss/train': '0.66268', 'examples_per_second': '33.464', 'grad_norm': '76.5', 'counters/examples': 83808, 'counters/updates': 2619}
skipping logging after 83840 examples to avoid logging too frequently
train stats after 83872 examples: {'rewards_train/chosen': '0.0029307', 'rewards_train/rejected': '-0.0021274', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0050581', 'logps_train/rejected': '-126', 'logps_train/chosen': '-151.32', 'loss/train': '0.70109', 'examples_per_second': '30.418', 'grad_norm': '154', 'counters/examples': 83872, 'counters/updates': 2621}
train stats after 83904 examples: {'rewards_train/chosen': '0.062266', 'rewards_train/rejected': '-0.023681', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085947', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-151.32', 'loss/train': '0.65806', 'examples_per_second': '32.054', 'grad_norm': '44.5', 'counters/examples': 83904, 'counters/updates': 2622}
train stats after 83936 examples: {'rewards_train/chosen': '0.11619', 'rewards_train/rejected': '0.063756', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.052436', 'logps_train/rejected': '-147.01', 'logps_train/chosen': '-156.71', 'loss/train': '0.68338', 'examples_per_second': '32.284', 'grad_norm': '89.5', 'counters/examples': 83936, 'counters/updates': 2623}
train stats after 83968 examples: {'rewards_train/chosen': '0.012801', 'rewards_train/rejected': '-0.0061166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018918', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-160.33', 'loss/train': '0.69194', 'examples_per_second': '31.098', 'grad_norm': '75.5', 'counters/examples': 83968, 'counters/updates': 2624}
skipping logging after 84000 examples to avoid logging too frequently
Running evaluation after 84000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 84000: {'rewards_eval/chosen': '0.060953', 'rewards_eval/rejected': '0.019038', 'rewards_eval/accuracies': '0.53906', 'rewards_eval/margins': '0.041915', 'logps_eval/rejected': '-127.95', 'logps_eval/chosen': '-150.4', 'loss/eval': '0.68519'}
skipping save for non epoch
train stats after 84032 examples: {'rewards_train/chosen': '0.11736', 'rewards_train/rejected': '-0.030358', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14771', 'logps_train/rejected': '-142.36', 'logps_train/chosen': '-171.99', 'loss/train': '0.63593', 'examples_per_second': '31.286', 'grad_norm': '57', 'counters/examples': 84032, 'counters/updates': 2626}
train stats after 84064 examples: {'rewards_train/chosen': '0.020446', 'rewards_train/rejected': '-0.041046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061492', 'logps_train/rejected': '-149.66', 'logps_train/chosen': '-173.5', 'loss/train': '0.67312', 'examples_per_second': '30.466', 'grad_norm': '115.5', 'counters/examples': 84064, 'counters/updates': 2627}
train stats after 84096 examples: {'rewards_train/chosen': '0.023196', 'rewards_train/rejected': '-0.0096533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032849', 'logps_train/rejected': '-167.81', 'logps_train/chosen': '-166.93', 'loss/train': '0.70002', 'examples_per_second': '31.902', 'grad_norm': '342', 'counters/examples': 84096, 'counters/updates': 2628}
train stats after 84128 examples: {'rewards_train/chosen': '-0.047859', 'rewards_train/rejected': '0.012281', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.06014', 'logps_train/rejected': '-88.554', 'logps_train/chosen': '-149.13', 'loss/train': '0.73808', 'examples_per_second': '31.7', 'grad_norm': '68', 'counters/examples': 84128, 'counters/updates': 2629}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84192 examples: {'rewards_train/chosen': '0.084299', 'rewards_train/rejected': '-0.033669', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11797', 'logps_train/rejected': '-150.56', 'logps_train/chosen': '-111.9', 'loss/train': '0.65001', 'examples_per_second': '31.494', 'grad_norm': '91', 'counters/examples': 84192, 'counters/updates': 2631}
train stats after 84224 examples: {'rewards_train/chosen': '0.10794', 'rewards_train/rejected': '0.036322', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071621', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-178.32', 'loss/train': '0.66914', 'examples_per_second': '31.544', 'grad_norm': '250', 'counters/examples': 84224, 'counters/updates': 2632}
train stats after 84256 examples: {'rewards_train/chosen': '0.090578', 'rewards_train/rejected': '-0.048889', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13947', 'logps_train/rejected': '-145.61', 'logps_train/chosen': '-109.23', 'loss/train': '0.64141', 'examples_per_second': '30.852', 'grad_norm': '91.5', 'counters/examples': 84256, 'counters/updates': 2633}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84320 examples: {'rewards_train/chosen': '0.034214', 'rewards_train/rejected': '0.012555', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021659', 'logps_train/rejected': '-126.38', 'logps_train/chosen': '-139.77', 'loss/train': '0.69105', 'examples_per_second': '31.586', 'grad_norm': '79.5', 'counters/examples': 84320, 'counters/updates': 2635}
train stats after 84352 examples: {'rewards_train/chosen': '0.067811', 'rewards_train/rejected': '0.11401', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.046196', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-164.03', 'loss/train': '0.72609', 'examples_per_second': '30.08', 'grad_norm': '102', 'counters/examples': 84352, 'counters/updates': 2636}
train stats after 84384 examples: {'rewards_train/chosen': '0.12236', 'rewards_train/rejected': '0.0034679', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1189', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-182.58', 'loss/train': '0.65308', 'examples_per_second': '32.447', 'grad_norm': '66', 'counters/examples': 84384, 'counters/updates': 2637}
train stats after 84416 examples: {'rewards_train/chosen': '-0.0082106', 'rewards_train/rejected': '0.068506', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.076716', 'logps_train/rejected': '-145.81', 'logps_train/chosen': '-157.35', 'loss/train': '0.74652', 'examples_per_second': '30.328', 'grad_norm': '107.5', 'counters/examples': 84416, 'counters/updates': 2638}
train stats after 84448 examples: {'rewards_train/chosen': '0.048426', 'rewards_train/rejected': '0.053607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.005181', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-167.72', 'loss/train': '0.7083', 'examples_per_second': '30.733', 'grad_norm': '144', 'counters/examples': 84448, 'counters/updates': 2639}
train stats after 84480 examples: {'rewards_train/chosen': '0.03474', 'rewards_train/rejected': '0.060929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.026189', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-143.98', 'loss/train': '0.73376', 'examples_per_second': '30.11', 'grad_norm': '111.5', 'counters/examples': 84480, 'counters/updates': 2640}
skipping logging after 84512 examples to avoid logging too frequently
train stats after 84544 examples: {'rewards_train/chosen': '0.027744', 'rewards_train/rejected': '0.023931', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038136', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-177.59', 'loss/train': '0.70164', 'examples_per_second': '31.62', 'grad_norm': '127', 'counters/examples': 84544, 'counters/updates': 2642}
skipping logging after 84576 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '-0.038977', 'rewards_train/rejected': '0.039091', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.078068', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-117.55', 'loss/train': '0.74563', 'examples_per_second': '31.615', 'grad_norm': '87', 'counters/examples': 84608, 'counters/updates': 2644}
train stats after 84640 examples: {'rewards_train/chosen': '0.091376', 'rewards_train/rejected': '0.024896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06648', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-135.02', 'loss/train': '0.67564', 'examples_per_second': '31.929', 'grad_norm': '57.5', 'counters/examples': 84640, 'counters/updates': 2645}
train stats after 84672 examples: {'rewards_train/chosen': '0.0071463', 'rewards_train/rejected': '0.016842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0096954', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-135.39', 'loss/train': '0.71148', 'examples_per_second': '30.445', 'grad_norm': '342', 'counters/examples': 84672, 'counters/updates': 2646}
skipping logging after 84704 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '0.0064049', 'rewards_train/rejected': '0.060602', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.054197', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-151.2', 'loss/train': '0.73237', 'examples_per_second': '22.92', 'grad_norm': '146', 'counters/examples': 84736, 'counters/updates': 2648}
train stats after 84768 examples: {'rewards_train/chosen': '0.12074', 'rewards_train/rejected': '0.0043577', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11638', 'logps_train/rejected': '-97.109', 'logps_train/chosen': '-142.6', 'loss/train': '0.64714', 'examples_per_second': '31.005', 'grad_norm': '90', 'counters/examples': 84768, 'counters/updates': 2649}
train stats after 84800 examples: {'rewards_train/chosen': '-0.02338', 'rewards_train/rejected': '-0.033539', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010159', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-141.22', 'loss/train': '0.70421', 'examples_per_second': '31.364', 'grad_norm': '134', 'counters/examples': 84800, 'counters/updates': 2650}
train stats after 84832 examples: {'rewards_train/chosen': '0.052969', 'rewards_train/rejected': '0.056767', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.003798', 'logps_train/rejected': '-101.29', 'logps_train/chosen': '-122.77', 'loss/train': '0.7045', 'examples_per_second': '23.943', 'grad_norm': '94.5', 'counters/examples': 84832, 'counters/updates': 2651}
train stats after 84864 examples: {'rewards_train/chosen': '0.065902', 'rewards_train/rejected': '-0.023517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089419', 'logps_train/rejected': '-150.58', 'logps_train/chosen': '-148.38', 'loss/train': '0.65758', 'examples_per_second': '30.094', 'grad_norm': '74.5', 'counters/examples': 84864, 'counters/updates': 2652}
train stats after 84896 examples: {'rewards_train/chosen': '0.016575', 'rewards_train/rejected': '0.032044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015468', 'logps_train/rejected': '-160.17', 'logps_train/chosen': '-154.49', 'loss/train': '0.71587', 'examples_per_second': '31.559', 'grad_norm': '93', 'counters/examples': 84896, 'counters/updates': 2653}
train stats after 84928 examples: {'rewards_train/chosen': '0.038303', 'rewards_train/rejected': '-0.0088002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047103', 'logps_train/rejected': '-104.26', 'logps_train/chosen': '-146.83', 'loss/train': '0.67952', 'examples_per_second': '31.854', 'grad_norm': '79', 'counters/examples': 84928, 'counters/updates': 2654}
train stats after 84960 examples: {'rewards_train/chosen': '0.035522', 'rewards_train/rejected': '0.010497', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025025', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-140.51', 'loss/train': '0.69556', 'examples_per_second': '31.369', 'grad_norm': '72.5', 'counters/examples': 84960, 'counters/updates': 2655}
skipping logging after 84992 examples to avoid logging too frequently
train stats after 85024 examples: {'rewards_train/chosen': '-0.012177', 'rewards_train/rejected': '0.051563', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06374', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-142.16', 'loss/train': '0.74088', 'examples_per_second': '37.047', 'grad_norm': '63', 'counters/examples': 85024, 'counters/updates': 2657}
train stats after 85056 examples: {'rewards_train/chosen': '0.11098', 'rewards_train/rejected': '-0.036592', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14757', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-164.28', 'loss/train': '0.63557', 'examples_per_second': '31.517', 'grad_norm': '65', 'counters/examples': 85056, 'counters/updates': 2658}
train stats after 85088 examples: {'rewards_train/chosen': '0.065323', 'rewards_train/rejected': '0.046002', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019321', 'logps_train/rejected': '-127.14', 'logps_train/chosen': '-120.65', 'loss/train': '0.69497', 'examples_per_second': '30.396', 'grad_norm': '132', 'counters/examples': 85088, 'counters/updates': 2659}
train stats after 85120 examples: {'rewards_train/chosen': '0.051741', 'rewards_train/rejected': '0.070781', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019039', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-185.74', 'loss/train': '0.72044', 'examples_per_second': '31.255', 'grad_norm': '141', 'counters/examples': 85120, 'counters/updates': 2660}
train stats after 85152 examples: {'rewards_train/chosen': '-0.003811', 'rewards_train/rejected': '0.046533', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.050344', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-129.11', 'loss/train': '0.725', 'examples_per_second': '31.609', 'grad_norm': '91', 'counters/examples': 85152, 'counters/updates': 2661}
train stats after 85184 examples: {'rewards_train/chosen': '0.0091479', 'rewards_train/rejected': '-0.0028459', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011994', 'logps_train/rejected': '-151.43', 'logps_train/chosen': '-128.93', 'loss/train': '0.69602', 'examples_per_second': '31.762', 'grad_norm': '142', 'counters/examples': 85184, 'counters/updates': 2662}
train stats after 85216 examples: {'rewards_train/chosen': '0.080666', 'rewards_train/rejected': '-0.042621', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12329', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-145.16', 'loss/train': '0.64746', 'examples_per_second': '31.62', 'grad_norm': '247', 'counters/examples': 85216, 'counters/updates': 2663}
train stats after 85248 examples: {'rewards_train/chosen': '0.042331', 'rewards_train/rejected': '-0.020727', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063058', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-156.91', 'loss/train': '0.6791', 'examples_per_second': '32.716', 'grad_norm': '156', 'counters/examples': 85248, 'counters/updates': 2664}
train stats after 85280 examples: {'rewards_train/chosen': '0.012521', 'rewards_train/rejected': '0.030478', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017957', 'logps_train/rejected': '-138.35', 'logps_train/chosen': '-156.84', 'loss/train': '0.71477', 'examples_per_second': '30.021', 'grad_norm': '105', 'counters/examples': 85280, 'counters/updates': 2665}
train stats after 85312 examples: {'rewards_train/chosen': '0.072624', 'rewards_train/rejected': '-0.010541', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.083165', 'logps_train/rejected': '-153.96', 'logps_train/chosen': '-168.09', 'loss/train': '0.66977', 'examples_per_second': '31.876', 'grad_norm': '65', 'counters/examples': 85312, 'counters/updates': 2666}
skipping logging after 85344 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '0.0081624', 'rewards_train/rejected': '-0.049221', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057384', 'logps_train/rejected': '-175.75', 'logps_train/chosen': '-152.57', 'loss/train': '0.67751', 'examples_per_second': '32.536', 'grad_norm': '95', 'counters/examples': 85376, 'counters/updates': 2668}
train stats after 85408 examples: {'rewards_train/chosen': '0.10207', 'rewards_train/rejected': '0.060708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041363', 'logps_train/rejected': '-107.8', 'logps_train/chosen': '-178.73', 'loss/train': '0.68035', 'examples_per_second': '31.582', 'grad_norm': '126.5', 'counters/examples': 85408, 'counters/updates': 2669}
train stats after 85440 examples: {'rewards_train/chosen': '0.010866', 'rewards_train/rejected': '0.081368', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.070502', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-150.9', 'loss/train': '0.73746', 'examples_per_second': '32.811', 'grad_norm': '82.5', 'counters/examples': 85440, 'counters/updates': 2670}
train stats after 85472 examples: {'rewards_train/chosen': '-0.066167', 'rewards_train/rejected': '0.039635', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.1058', 'logps_train/rejected': '-114.77', 'logps_train/chosen': '-169.9', 'loss/train': '0.75448', 'examples_per_second': '31.917', 'grad_norm': '106.5', 'counters/examples': 85472, 'counters/updates': 2671}
skipping logging after 85504 examples to avoid logging too frequently
train stats after 85536 examples: {'rewards_train/chosen': '0.088587', 'rewards_train/rejected': '-0.020227', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10881', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-168.18', 'loss/train': '0.65166', 'examples_per_second': '31.913', 'grad_norm': '60.25', 'counters/examples': 85536, 'counters/updates': 2673}
train stats after 85568 examples: {'rewards_train/chosen': '-0.051238', 'rewards_train/rejected': '-0.0068835', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.044355', 'logps_train/rejected': '-100.41', 'logps_train/chosen': '-113.74', 'loss/train': '0.72474', 'examples_per_second': '31.502', 'grad_norm': '215', 'counters/examples': 85568, 'counters/updates': 2674}
train stats after 85600 examples: {'rewards_train/chosen': '0.0036023', 'rewards_train/rejected': '-0.017772', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021375', 'logps_train/rejected': '-125.32', 'logps_train/chosen': '-143.82', 'loss/train': '0.69069', 'examples_per_second': '31.472', 'grad_norm': '72.5', 'counters/examples': 85600, 'counters/updates': 2675}
train stats after 85632 examples: {'rewards_train/chosen': '0.031897', 'rewards_train/rejected': '-0.023314', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055211', 'logps_train/rejected': '-155.34', 'logps_train/chosen': '-163.31', 'loss/train': '0.67364', 'examples_per_second': '31.329', 'grad_norm': '67', 'counters/examples': 85632, 'counters/updates': 2676}
train stats after 85664 examples: {'rewards_train/chosen': '0.066006', 'rewards_train/rejected': '0.071724', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0057184', 'logps_train/rejected': '-158.38', 'logps_train/chosen': '-213.8', 'loss/train': '0.70623', 'examples_per_second': '30.95', 'grad_norm': '106.5', 'counters/examples': 85664, 'counters/updates': 2677}
train stats after 85696 examples: {'rewards_train/chosen': '0.011277', 'rewards_train/rejected': '0.057132', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045855', 'logps_train/rejected': '-123.18', 'logps_train/chosen': '-163.32', 'loss/train': '0.73317', 'examples_per_second': '32.791', 'grad_norm': '168', 'counters/examples': 85696, 'counters/updates': 2678}
skipping logging after 85728 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '0.080325', 'rewards_train/rejected': '0.091733', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011408', 'logps_train/rejected': '-145.32', 'logps_train/chosen': '-153.12', 'loss/train': '0.71508', 'examples_per_second': '31.609', 'grad_norm': '456', 'counters/examples': 85760, 'counters/updates': 2680}
skipping logging after 85792 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '0.049256', 'rewards_train/rejected': '0.042399', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0068562', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-164.16', 'loss/train': '0.69897', 'examples_per_second': '30.066', 'grad_norm': '93.5', 'counters/examples': 85824, 'counters/updates': 2682}
train stats after 85856 examples: {'rewards_train/chosen': '0.085536', 'rewards_train/rejected': '0.0024154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083121', 'logps_train/rejected': '-153.78', 'logps_train/chosen': '-173.58', 'loss/train': '0.66589', 'examples_per_second': '30.164', 'grad_norm': '56.25', 'counters/examples': 85856, 'counters/updates': 2683}
train stats after 85888 examples: {'rewards_train/chosen': '0.068272', 'rewards_train/rejected': '0.04976', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018512', 'logps_train/rejected': '-115.2', 'logps_train/chosen': '-165.74', 'loss/train': '0.70646', 'examples_per_second': '32.535', 'grad_norm': '77.5', 'counters/examples': 85888, 'counters/updates': 2684}
train stats after 85920 examples: {'rewards_train/chosen': '0.1169', 'rewards_train/rejected': '-0.052074', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16898', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-170.62', 'loss/train': '0.6273', 'examples_per_second': '30.763', 'grad_norm': '91.5', 'counters/examples': 85920, 'counters/updates': 2685}
train stats after 85952 examples: {'rewards_train/chosen': '-0.018009', 'rewards_train/rejected': '-0.0389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020891', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-160.44', 'loss/train': '0.69645', 'examples_per_second': '31.255', 'grad_norm': '105', 'counters/examples': 85952, 'counters/updates': 2686}
train stats after 85984 examples: {'rewards_train/chosen': '-0.037124', 'rewards_train/rejected': '0.057505', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.094629', 'logps_train/rejected': '-159.48', 'logps_train/chosen': '-137.45', 'loss/train': '0.76374', 'examples_per_second': '31.533', 'grad_norm': '105.5', 'counters/examples': 85984, 'counters/updates': 2687}
train stats after 86016 examples: {'rewards_train/chosen': '0.090176', 'rewards_train/rejected': '0.053646', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.036529', 'logps_train/rejected': '-135.37', 'logps_train/chosen': '-159.7', 'loss/train': '0.69531', 'examples_per_second': '31.403', 'grad_norm': '70.5', 'counters/examples': 86016, 'counters/updates': 2688}
train stats after 86048 examples: {'rewards_train/chosen': '0.10813', 'rewards_train/rejected': '-0.02863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13677', 'logps_train/rejected': '-160.05', 'logps_train/chosen': '-149.65', 'loss/train': '0.6357', 'examples_per_second': '31.608', 'grad_norm': '109', 'counters/examples': 86048, 'counters/updates': 2689}
train stats after 86080 examples: {'rewards_train/chosen': '0.059396', 'rewards_train/rejected': '0.043166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01623', 'logps_train/rejected': '-192.7', 'logps_train/chosen': '-202.29', 'loss/train': '0.70337', 'examples_per_second': '30.191', 'grad_norm': '122', 'counters/examples': 86080, 'counters/updates': 2690}
train stats after 86112 examples: {'rewards_train/chosen': '0.10219', 'rewards_train/rejected': '0.029232', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072956', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-130.19', 'loss/train': '0.66582', 'examples_per_second': '31.286', 'grad_norm': '55', 'counters/examples': 86112, 'counters/updates': 2691}
skipping logging after 86144 examples to avoid logging too frequently
train stats after 86176 examples: {'rewards_train/chosen': '0.097114', 'rewards_train/rejected': '0.010615', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086499', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-150.73', 'loss/train': '0.66815', 'examples_per_second': '32.292', 'grad_norm': '136', 'counters/examples': 86176, 'counters/updates': 2693}
train stats after 86208 examples: {'rewards_train/chosen': '0.047302', 'rewards_train/rejected': '0.090444', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043141', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-135.43', 'loss/train': '0.72224', 'examples_per_second': '32.676', 'grad_norm': '75', 'counters/examples': 86208, 'counters/updates': 2694}
train stats after 86240 examples: {'rewards_train/chosen': '0.019207', 'rewards_train/rejected': '-0.031836', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051044', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-125.27', 'loss/train': '0.67944', 'examples_per_second': '31.595', 'grad_norm': '120.5', 'counters/examples': 86240, 'counters/updates': 2695}
skipping logging after 86272 examples to avoid logging too frequently
train stats after 86304 examples: {'rewards_train/chosen': '-0.028983', 'rewards_train/rejected': '0.036847', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.06583', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-120.98', 'loss/train': '0.73555', 'examples_per_second': '31.028', 'grad_norm': '68.5', 'counters/examples': 86304, 'counters/updates': 2697}
train stats after 86336 examples: {'rewards_train/chosen': '0.039377', 'rewards_train/rejected': '0.085187', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.04581', 'logps_train/rejected': '-162.28', 'logps_train/chosen': '-117.99', 'loss/train': '0.72462', 'examples_per_second': '32.741', 'grad_norm': '75.5', 'counters/examples': 86336, 'counters/updates': 2698}
train stats after 86368 examples: {'rewards_train/chosen': '0.21846', 'rewards_train/rejected': '0.024121', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19434', 'logps_train/rejected': '-166.19', 'logps_train/chosen': '-182.84', 'loss/train': '0.63791', 'examples_per_second': '31.465', 'grad_norm': '80.5', 'counters/examples': 86368, 'counters/updates': 2699}
train stats after 86400 examples: {'rewards_train/chosen': '-0.01412', 'rewards_train/rejected': '0.030747', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044868', 'logps_train/rejected': '-123.08', 'logps_train/chosen': '-131.74', 'loss/train': '0.72904', 'examples_per_second': '30.634', 'grad_norm': '87.5', 'counters/examples': 86400, 'counters/updates': 2700}
train stats after 86432 examples: {'rewards_train/chosen': '0.059447', 'rewards_train/rejected': '0.11693', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057483', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-142.42', 'loss/train': '0.73144', 'examples_per_second': '31.563', 'grad_norm': '80', 'counters/examples': 86432, 'counters/updates': 2701}
train stats after 86464 examples: {'rewards_train/chosen': '0.027821', 'rewards_train/rejected': '0.091092', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.06327', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-172.04', 'loss/train': '0.7387', 'examples_per_second': '24.61', 'grad_norm': '130', 'counters/examples': 86464, 'counters/updates': 2702}
train stats after 86496 examples: {'rewards_train/chosen': '0.012695', 'rewards_train/rejected': '0.0027119', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0099827', 'logps_train/rejected': '-133.2', 'logps_train/chosen': '-167.22', 'loss/train': '0.70276', 'examples_per_second': '31.509', 'grad_norm': '696', 'counters/examples': 86496, 'counters/updates': 2703}
train stats after 86528 examples: {'rewards_train/chosen': '0.036578', 'rewards_train/rejected': '0.077363', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040785', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-133.27', 'loss/train': '0.73103', 'examples_per_second': '31.538', 'grad_norm': '108', 'counters/examples': 86528, 'counters/updates': 2704}
train stats after 86560 examples: {'rewards_train/chosen': '0.016567', 'rewards_train/rejected': '-0.0076459', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024213', 'logps_train/rejected': '-148.07', 'logps_train/chosen': '-155.64', 'loss/train': '0.68961', 'examples_per_second': '31.557', 'grad_norm': '171', 'counters/examples': 86560, 'counters/updates': 2705}
skipping logging after 86592 examples to avoid logging too frequently
train stats after 86624 examples: {'rewards_train/chosen': '0.019891', 'rewards_train/rejected': '0.044373', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.024482', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-182.8', 'loss/train': '0.72799', 'examples_per_second': '31.555', 'grad_norm': '86.5', 'counters/examples': 86624, 'counters/updates': 2707}
train stats after 86656 examples: {'rewards_train/chosen': '0.18818', 'rewards_train/rejected': '0.014769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17341', 'logps_train/rejected': '-183.01', 'logps_train/chosen': '-189.71', 'loss/train': '0.63301', 'examples_per_second': '31.567', 'grad_norm': '110.5', 'counters/examples': 86656, 'counters/updates': 2708}
train stats after 86688 examples: {'rewards_train/chosen': '0.012737', 'rewards_train/rejected': '0.10932', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.096582', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-123.01', 'loss/train': '0.75405', 'examples_per_second': '31.055', 'grad_norm': '93', 'counters/examples': 86688, 'counters/updates': 2709}
train stats after 86720 examples: {'rewards_train/chosen': '-0.047969', 'rewards_train/rejected': '0.058153', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10612', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-126.76', 'loss/train': '0.76251', 'examples_per_second': '30.05', 'grad_norm': '90', 'counters/examples': 86720, 'counters/updates': 2710}
train stats after 86752 examples: {'rewards_train/chosen': '0.044086', 'rewards_train/rejected': '0.029798', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014288', 'logps_train/rejected': '-127.25', 'logps_train/chosen': '-134', 'loss/train': '0.69654', 'examples_per_second': '30.241', 'grad_norm': '71.5', 'counters/examples': 86752, 'counters/updates': 2711}
train stats after 86784 examples: {'rewards_train/chosen': '0.11002', 'rewards_train/rejected': '0.028964', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081053', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-124.49', 'loss/train': '0.66074', 'examples_per_second': '30.044', 'grad_norm': '111.5', 'counters/examples': 86784, 'counters/updates': 2712}
train stats after 86816 examples: {'rewards_train/chosen': '0.030228', 'rewards_train/rejected': '0.018496', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011733', 'logps_train/rejected': '-94.644', 'logps_train/chosen': '-162.94', 'loss/train': '0.69772', 'examples_per_second': '31.524', 'grad_norm': '139', 'counters/examples': 86816, 'counters/updates': 2713}
train stats after 86848 examples: {'rewards_train/chosen': '0.066565', 'rewards_train/rejected': '0.060706', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.005859', 'logps_train/rejected': '-134.98', 'logps_train/chosen': '-143.51', 'loss/train': '0.70893', 'examples_per_second': '32.478', 'grad_norm': '74.5', 'counters/examples': 86848, 'counters/updates': 2714}
train stats after 86880 examples: {'rewards_train/chosen': '0.20602', 'rewards_train/rejected': '-0.006438', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21245', 'logps_train/rejected': '-172.57', 'logps_train/chosen': '-217.69', 'loss/train': '0.61586', 'examples_per_second': '31.519', 'grad_norm': '56.75', 'counters/examples': 86880, 'counters/updates': 2715}
train stats after 86912 examples: {'rewards_train/chosen': '0.10063', 'rewards_train/rejected': '0.069819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030815', 'logps_train/rejected': '-147.09', 'logps_train/chosen': '-122.67', 'loss/train': '0.69988', 'examples_per_second': '32.767', 'grad_norm': '75.5', 'counters/examples': 86912, 'counters/updates': 2716}
train stats after 86944 examples: {'rewards_train/chosen': '0.026616', 'rewards_train/rejected': '0.084639', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.058023', 'logps_train/rejected': '-160.26', 'logps_train/chosen': '-159.14', 'loss/train': '0.73288', 'examples_per_second': '31.177', 'grad_norm': '68', 'counters/examples': 86944, 'counters/updates': 2717}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87008 examples: {'rewards_train/chosen': '0.02247', 'rewards_train/rejected': '0.028665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.006195', 'logps_train/rejected': '-187.03', 'logps_train/chosen': '-157.66', 'loss/train': '0.71372', 'examples_per_second': '31.465', 'grad_norm': '106', 'counters/examples': 87008, 'counters/updates': 2719}
skipping logging after 87040 examples to avoid logging too frequently
train stats after 87072 examples: {'rewards_train/chosen': '0.01703', 'rewards_train/rejected': '0.040635', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023605', 'logps_train/rejected': '-151.99', 'logps_train/chosen': '-109.63', 'loss/train': '0.71582', 'examples_per_second': '31.559', 'grad_norm': '85.5', 'counters/examples': 87072, 'counters/updates': 2721}
train stats after 87104 examples: {'rewards_train/chosen': '0.11773', 'rewards_train/rejected': '0.078511', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039219', 'logps_train/rejected': '-129.93', 'logps_train/chosen': '-166.02', 'loss/train': '0.68568', 'examples_per_second': '31.495', 'grad_norm': '588', 'counters/examples': 87104, 'counters/updates': 2722}
train stats after 87136 examples: {'rewards_train/chosen': '0.059384', 'rewards_train/rejected': '0.16312', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.10374', 'logps_train/rejected': '-159.06', 'logps_train/chosen': '-135.56', 'loss/train': '0.75642', 'examples_per_second': '29.838', 'grad_norm': '102', 'counters/examples': 87136, 'counters/updates': 2723}
train stats after 87168 examples: {'rewards_train/chosen': '0.051592', 'rewards_train/rejected': '0.046207', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0053848', 'logps_train/rejected': '-164.87', 'logps_train/chosen': '-136.48', 'loss/train': '0.71098', 'examples_per_second': '30.983', 'grad_norm': '77.5', 'counters/examples': 87168, 'counters/updates': 2724}
train stats after 87200 examples: {'rewards_train/chosen': '0.054445', 'rewards_train/rejected': '0.046905', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0075395', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-150.53', 'loss/train': '0.69607', 'examples_per_second': '32.138', 'grad_norm': '93', 'counters/examples': 87200, 'counters/updates': 2725}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87264 examples: {'rewards_train/chosen': '0.090224', 'rewards_train/rejected': '-0.01705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10727', 'logps_train/rejected': '-159.94', 'logps_train/chosen': '-177.93', 'loss/train': '0.65875', 'examples_per_second': '31.212', 'grad_norm': '70', 'counters/examples': 87264, 'counters/updates': 2727}
skipping logging after 87296 examples to avoid logging too frequently
train stats after 87328 examples: {'rewards_train/chosen': '0.069935', 'rewards_train/rejected': '-0.044208', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11414', 'logps_train/rejected': '-136.82', 'logps_train/chosen': '-130.01', 'loss/train': '0.65232', 'examples_per_second': '32.258', 'grad_norm': '89.5', 'counters/examples': 87328, 'counters/updates': 2729}
train stats after 87360 examples: {'rewards_train/chosen': '0.036922', 'rewards_train/rejected': '0.052121', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015199', 'logps_train/rejected': '-93.988', 'logps_train/chosen': '-143.87', 'loss/train': '0.70917', 'examples_per_second': '30.045', 'grad_norm': '77', 'counters/examples': 87360, 'counters/updates': 2730}
train stats after 87392 examples: {'rewards_train/chosen': '-0.01287', 'rewards_train/rejected': '0.021284', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.034154', 'logps_train/rejected': '-114.01', 'logps_train/chosen': '-151.31', 'loss/train': '0.73324', 'examples_per_second': '31.929', 'grad_norm': '176', 'counters/examples': 87392, 'counters/updates': 2731}
train stats after 87424 examples: {'rewards_train/chosen': '0.090688', 'rewards_train/rejected': '0.1019', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011214', 'logps_train/rejected': '-156.87', 'logps_train/chosen': '-126.02', 'loss/train': '0.71436', 'examples_per_second': '31.506', 'grad_norm': '179', 'counters/examples': 87424, 'counters/updates': 2732}
train stats after 87456 examples: {'rewards_train/chosen': '0.20438', 'rewards_train/rejected': '0.11094', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093433', 'logps_train/rejected': '-139.69', 'logps_train/chosen': '-174.12', 'loss/train': '0.69258', 'examples_per_second': '32.655', 'grad_norm': '106.5', 'counters/examples': 87456, 'counters/updates': 2733}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87520 examples: {'rewards_train/chosen': '-0.02724', 'rewards_train/rejected': '0.0045013', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.031741', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-152.03', 'loss/train': '0.72104', 'examples_per_second': '31.506', 'grad_norm': '102.5', 'counters/examples': 87520, 'counters/updates': 2735}
train stats after 87552 examples: {'rewards_train/chosen': '0.011091', 'rewards_train/rejected': '0.017494', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0064029', 'logps_train/rejected': '-140.29', 'logps_train/chosen': '-153.52', 'loss/train': '0.70522', 'examples_per_second': '32.163', 'grad_norm': '52.75', 'counters/examples': 87552, 'counters/updates': 2736}
train stats after 87584 examples: {'rewards_train/chosen': '0.033193', 'rewards_train/rejected': '0.037337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0041438', 'logps_train/rejected': '-150.97', 'logps_train/chosen': '-154.75', 'loss/train': '0.71338', 'examples_per_second': '31.237', 'grad_norm': '256', 'counters/examples': 87584, 'counters/updates': 2737}
train stats after 87616 examples: {'rewards_train/chosen': '0.046624', 'rewards_train/rejected': '0.0068174', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039807', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-133.64', 'loss/train': '0.68183', 'examples_per_second': '30.544', 'grad_norm': '85.5', 'counters/examples': 87616, 'counters/updates': 2738}
train stats after 87648 examples: {'rewards_train/chosen': '-0.049255', 'rewards_train/rejected': '-0.026727', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022529', 'logps_train/rejected': '-101.19', 'logps_train/chosen': '-131.08', 'loss/train': '0.71296', 'examples_per_second': '31.508', 'grad_norm': '65.5', 'counters/examples': 87648, 'counters/updates': 2739}
train stats after 87680 examples: {'rewards_train/chosen': '-0.0071869', 'rewards_train/rejected': '0.019537', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026724', 'logps_train/rejected': '-141.86', 'logps_train/chosen': '-146.85', 'loss/train': '0.72067', 'examples_per_second': '31.494', 'grad_norm': '99', 'counters/examples': 87680, 'counters/updates': 2740}
skipping logging after 87712 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '0.18873', 'rewards_train/rejected': '-0.01185', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20058', 'logps_train/rejected': '-110.49', 'logps_train/chosen': '-182.08', 'loss/train': '0.62624', 'examples_per_second': '32.644', 'grad_norm': '268', 'counters/examples': 87744, 'counters/updates': 2742}
train stats after 87776 examples: {'rewards_train/chosen': '0.020813', 'rewards_train/rejected': '-0.011109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031921', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-129.26', 'loss/train': '0.68174', 'examples_per_second': '31.754', 'grad_norm': '107', 'counters/examples': 87776, 'counters/updates': 2743}
skipping logging after 87808 examples to avoid logging too frequently
train stats after 87840 examples: {'rewards_train/chosen': '0.070323', 'rewards_train/rejected': '0.020186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050137', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-139.57', 'loss/train': '0.67887', 'examples_per_second': '39.049', 'grad_norm': '79.5', 'counters/examples': 87840, 'counters/updates': 2745}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87904 examples: {'rewards_train/chosen': '0.056554', 'rewards_train/rejected': '-0.00021231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056766', 'logps_train/rejected': '-113.07', 'logps_train/chosen': '-122.55', 'loss/train': '0.6708', 'examples_per_second': '31.493', 'grad_norm': '64', 'counters/examples': 87904, 'counters/updates': 2747}
train stats after 87936 examples: {'rewards_train/chosen': '0.10814', 'rewards_train/rejected': '0.028829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079306', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-137.55', 'loss/train': '0.66797', 'examples_per_second': '30.036', 'grad_norm': '81', 'counters/examples': 87936, 'counters/updates': 2748}
train stats after 87968 examples: {'rewards_train/chosen': '0.061844', 'rewards_train/rejected': '0.12447', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.062627', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-146.83', 'loss/train': '0.74021', 'examples_per_second': '30.279', 'grad_norm': '160', 'counters/examples': 87968, 'counters/updates': 2749}
train stats after 88000 examples: {'rewards_train/chosen': '0.079975', 'rewards_train/rejected': '0.010522', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069453', 'logps_train/rejected': '-100.22', 'logps_train/chosen': '-96.29', 'loss/train': '0.66527', 'examples_per_second': '33.109', 'grad_norm': '82.5', 'counters/examples': 88000, 'counters/updates': 2750}
train stats after 88032 examples: {'rewards_train/chosen': '0.032879', 'rewards_train/rejected': '-0.0030494', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035928', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-156.27', 'loss/train': '0.68454', 'examples_per_second': '31.526', 'grad_norm': '87', 'counters/examples': 88032, 'counters/updates': 2751}
train stats after 88064 examples: {'rewards_train/chosen': '0.11041', 'rewards_train/rejected': '-0.004004', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11441', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-170.06', 'loss/train': '0.6466', 'examples_per_second': '32.354', 'grad_norm': '62.5', 'counters/examples': 88064, 'counters/updates': 2752}
train stats after 88096 examples: {'rewards_train/chosen': '0.079109', 'rewards_train/rejected': '0.086107', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0069984', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-129.7', 'loss/train': '0.70598', 'examples_per_second': '32.636', 'grad_norm': '101', 'counters/examples': 88096, 'counters/updates': 2753}
train stats after 88128 examples: {'rewards_train/chosen': '0.17873', 'rewards_train/rejected': '0.057876', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12085', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-141.42', 'loss/train': '0.65129', 'examples_per_second': '31.612', 'grad_norm': '74', 'counters/examples': 88128, 'counters/updates': 2754}
train stats after 88160 examples: {'rewards_train/chosen': '0.10685', 'rewards_train/rejected': '0.021576', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.085276', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-193.38', 'loss/train': '0.67089', 'examples_per_second': '31.448', 'grad_norm': '122.5', 'counters/examples': 88160, 'counters/updates': 2755}
train stats after 88192 examples: {'rewards_train/chosen': '0.077306', 'rewards_train/rejected': '-0.02271', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10002', 'logps_train/rejected': '-132.9', 'logps_train/chosen': '-119.79', 'loss/train': '0.66634', 'examples_per_second': '30.035', 'grad_norm': '70.5', 'counters/examples': 88192, 'counters/updates': 2756}
train stats after 88224 examples: {'rewards_train/chosen': '0.013416', 'rewards_train/rejected': '-0.03853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051946', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-133.02', 'loss/train': '0.68215', 'examples_per_second': '31.442', 'grad_norm': '63.5', 'counters/examples': 88224, 'counters/updates': 2757}
train stats after 88256 examples: {'rewards_train/chosen': '0.075332', 'rewards_train/rejected': '-0.005319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080651', 'logps_train/rejected': '-110.38', 'logps_train/chosen': '-149.47', 'loss/train': '0.66493', 'examples_per_second': '32.075', 'grad_norm': '65', 'counters/examples': 88256, 'counters/updates': 2758}
train stats after 88288 examples: {'rewards_train/chosen': '0.047461', 'rewards_train/rejected': '0.050896', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0034346', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-166.32', 'loss/train': '0.71661', 'examples_per_second': '31.29', 'grad_norm': '119.5', 'counters/examples': 88288, 'counters/updates': 2759}
train stats after 88320 examples: {'rewards_train/chosen': '0.020865', 'rewards_train/rejected': '0.026626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0057609', 'logps_train/rejected': '-153.37', 'logps_train/chosen': '-135.82', 'loss/train': '0.71242', 'examples_per_second': '31.038', 'grad_norm': '85.5', 'counters/examples': 88320, 'counters/updates': 2760}
train stats after 88352 examples: {'rewards_train/chosen': '0.024197', 'rewards_train/rejected': '-0.02447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048667', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-104.38', 'loss/train': '0.67538', 'examples_per_second': '31.229', 'grad_norm': '103', 'counters/examples': 88352, 'counters/updates': 2761}
train stats after 88384 examples: {'rewards_train/chosen': '-0.01999', 'rewards_train/rejected': '-0.022919', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0029292', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-141.23', 'loss/train': '0.69944', 'examples_per_second': '30.035', 'grad_norm': '70', 'counters/examples': 88384, 'counters/updates': 2762}
train stats after 88416 examples: {'rewards_train/chosen': '0.074392', 'rewards_train/rejected': '0.03175', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.042642', 'logps_train/rejected': '-131.98', 'logps_train/chosen': '-182.09', 'loss/train': '0.68479', 'examples_per_second': '31.536', 'grad_norm': '116', 'counters/examples': 88416, 'counters/updates': 2763}
train stats after 88448 examples: {'rewards_train/chosen': '-0.0039644', 'rewards_train/rejected': '0.020326', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024291', 'logps_train/rejected': '-122.63', 'logps_train/chosen': '-151.92', 'loss/train': '0.7298', 'examples_per_second': '32.846', 'grad_norm': '80', 'counters/examples': 88448, 'counters/updates': 2764}
train stats after 88480 examples: {'rewards_train/chosen': '0.17634', 'rewards_train/rejected': '0.07152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10482', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-175.21', 'loss/train': '0.66327', 'examples_per_second': '31.113', 'grad_norm': '103.5', 'counters/examples': 88480, 'counters/updates': 2765}
train stats after 88512 examples: {'rewards_train/chosen': '0.049925', 'rewards_train/rejected': '-0.025725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07565', 'logps_train/rejected': '-158.6', 'logps_train/chosen': '-180.86', 'loss/train': '0.67573', 'examples_per_second': '30.041', 'grad_norm': '81', 'counters/examples': 88512, 'counters/updates': 2766}
train stats after 88544 examples: {'rewards_train/chosen': '0.078348', 'rewards_train/rejected': '-0.082581', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16093', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-144.73', 'loss/train': '0.62187', 'examples_per_second': '31.411', 'grad_norm': '70.5', 'counters/examples': 88544, 'counters/updates': 2767}
train stats after 88576 examples: {'rewards_train/chosen': '0.050492', 'rewards_train/rejected': '-0.026271', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076763', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-170.81', 'loss/train': '0.66503', 'examples_per_second': '31.62', 'grad_norm': '148', 'counters/examples': 88576, 'counters/updates': 2768}
train stats after 88608 examples: {'rewards_train/chosen': '0.047905', 'rewards_train/rejected': '0.03141', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016495', 'logps_train/rejected': '-133.02', 'logps_train/chosen': '-152.82', 'loss/train': '0.72453', 'examples_per_second': '33.055', 'grad_norm': '76', 'counters/examples': 88608, 'counters/updates': 2769}
train stats after 88640 examples: {'rewards_train/chosen': '0.10995', 'rewards_train/rejected': '0.0037677', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10619', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-152.45', 'loss/train': '0.65426', 'examples_per_second': '30.455', 'grad_norm': '77.5', 'counters/examples': 88640, 'counters/updates': 2770}
skipping logging after 88672 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '0.054242', 'rewards_train/rejected': '0.019314', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034928', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-137.77', 'loss/train': '0.68555', 'examples_per_second': '34.162', 'grad_norm': '88.5', 'counters/examples': 88704, 'counters/updates': 2772}
skipping logging after 88736 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '0.048394', 'rewards_train/rejected': '0.035166', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013228', 'logps_train/rejected': '-142.98', 'logps_train/chosen': '-137.36', 'loss/train': '0.69695', 'examples_per_second': '31.55', 'grad_norm': '93', 'counters/examples': 88768, 'counters/updates': 2774}
train stats after 88800 examples: {'rewards_train/chosen': '0.083739', 'rewards_train/rejected': '-0.045179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12892', 'logps_train/rejected': '-146.21', 'logps_train/chosen': '-142.67', 'loss/train': '0.64608', 'examples_per_second': '30.127', 'grad_norm': '59.5', 'counters/examples': 88800, 'counters/updates': 2775}
train stats after 88832 examples: {'rewards_train/chosen': '0.022724', 'rewards_train/rejected': '0.0073201', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015404', 'logps_train/rejected': '-102.1', 'logps_train/chosen': '-155.62', 'loss/train': '0.69568', 'examples_per_second': '32.746', 'grad_norm': '86.5', 'counters/examples': 88832, 'counters/updates': 2776}
train stats after 88864 examples: {'rewards_train/chosen': '0.08932', 'rewards_train/rejected': '0.064603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024717', 'logps_train/rejected': '-153.67', 'logps_train/chosen': '-160.65', 'loss/train': '0.69171', 'examples_per_second': '31.588', 'grad_norm': '110', 'counters/examples': 88864, 'counters/updates': 2777}
train stats after 88896 examples: {'rewards_train/chosen': '0.08778', 'rewards_train/rejected': '-0.0070392', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094819', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-143.53', 'loss/train': '0.65627', 'examples_per_second': '31.236', 'grad_norm': '136', 'counters/examples': 88896, 'counters/updates': 2778}
train stats after 88928 examples: {'rewards_train/chosen': '0.057181', 'rewards_train/rejected': '0.087646', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030465', 'logps_train/rejected': '-158.01', 'logps_train/chosen': '-161.26', 'loss/train': '0.72361', 'examples_per_second': '30.562', 'grad_norm': '129', 'counters/examples': 88928, 'counters/updates': 2779}
skipping logging after 88960 examples to avoid logging too frequently
train stats after 88992 examples: {'rewards_train/chosen': '0.096568', 'rewards_train/rejected': '-0.014416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11098', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-144.13', 'loss/train': '0.65214', 'examples_per_second': '31.588', 'grad_norm': '120.5', 'counters/examples': 88992, 'counters/updates': 2781}
train stats after 89024 examples: {'rewards_train/chosen': '0.021136', 'rewards_train/rejected': '0.02513', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0039941', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-129.83', 'loss/train': '0.70376', 'examples_per_second': '32.933', 'grad_norm': '76', 'counters/examples': 89024, 'counters/updates': 2782}
train stats after 89056 examples: {'rewards_train/chosen': '-0.018756', 'rewards_train/rejected': '0.0014773', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020233', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-143.55', 'loss/train': '0.71358', 'examples_per_second': '31.56', 'grad_norm': '108.5', 'counters/examples': 89056, 'counters/updates': 2783}
train stats after 89088 examples: {'rewards_train/chosen': '0.029132', 'rewards_train/rejected': '0.019149', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0099835', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-177.3', 'loss/train': '0.69874', 'examples_per_second': '32.015', 'grad_norm': '66', 'counters/examples': 89088, 'counters/updates': 2784}
train stats after 89120 examples: {'rewards_train/chosen': '0.039213', 'rewards_train/rejected': '-0.033891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073104', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-124.75', 'loss/train': '0.6658', 'examples_per_second': '32.123', 'grad_norm': '79.5', 'counters/examples': 89120, 'counters/updates': 2785}
train stats after 89152 examples: {'rewards_train/chosen': '0.053727', 'rewards_train/rejected': '0.04951', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0042173', 'logps_train/rejected': '-145.72', 'logps_train/chosen': '-178.17', 'loss/train': '0.70346', 'examples_per_second': '30.449', 'grad_norm': '82.5', 'counters/examples': 89152, 'counters/updates': 2786}
train stats after 89184 examples: {'rewards_train/chosen': '0.087867', 'rewards_train/rejected': '-0.0066424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094509', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-204.95', 'loss/train': '0.65705', 'examples_per_second': '30.067', 'grad_norm': '103', 'counters/examples': 89184, 'counters/updates': 2787}
train stats after 89216 examples: {'rewards_train/chosen': '0.081613', 'rewards_train/rejected': '-0.0042317', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085845', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-164.5', 'loss/train': '0.66861', 'examples_per_second': '31.309', 'grad_norm': '65', 'counters/examples': 89216, 'counters/updates': 2788}
skipping logging after 89248 examples to avoid logging too frequently
train stats after 89280 examples: {'rewards_train/chosen': '-0.11187', 'rewards_train/rejected': '0.003385', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11526', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-157.25', 'loss/train': '0.77187', 'examples_per_second': '31.47', 'grad_norm': '193', 'counters/examples': 89280, 'counters/updates': 2790}
train stats after 89312 examples: {'rewards_train/chosen': '0.008773', 'rewards_train/rejected': '0.025124', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016351', 'logps_train/rejected': '-145.28', 'logps_train/chosen': '-148.42', 'loss/train': '0.71533', 'examples_per_second': '31.04', 'grad_norm': '91.5', 'counters/examples': 89312, 'counters/updates': 2791}
train stats after 89344 examples: {'rewards_train/chosen': '0.054042', 'rewards_train/rejected': '-0.0025367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056579', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-198.77', 'loss/train': '0.68307', 'examples_per_second': '31.562', 'grad_norm': '147', 'counters/examples': 89344, 'counters/updates': 2792}
train stats after 89376 examples: {'rewards_train/chosen': '0.042571', 'rewards_train/rejected': '0.040751', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00182', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-196.47', 'loss/train': '0.70297', 'examples_per_second': '31.56', 'grad_norm': '138', 'counters/examples': 89376, 'counters/updates': 2793}
train stats after 89408 examples: {'rewards_train/chosen': '0.062954', 'rewards_train/rejected': '-0.020783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083737', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-135.37', 'loss/train': '0.66222', 'examples_per_second': '32.124', 'grad_norm': '116.5', 'counters/examples': 89408, 'counters/updates': 2794}
train stats after 89440 examples: {'rewards_train/chosen': '0.12526', 'rewards_train/rejected': '0.05829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066974', 'logps_train/rejected': '-145.28', 'logps_train/chosen': '-161.57', 'loss/train': '0.66893', 'examples_per_second': '32.499', 'grad_norm': '65.5', 'counters/examples': 89440, 'counters/updates': 2795}
train stats after 89472 examples: {'rewards_train/chosen': '0.17113', 'rewards_train/rejected': '0.013502', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15763', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-148.39', 'loss/train': '0.63751', 'examples_per_second': '32.572', 'grad_norm': '75', 'counters/examples': 89472, 'counters/updates': 2796}
train stats after 89504 examples: {'rewards_train/chosen': '0.042435', 'rewards_train/rejected': '0.015789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026646', 'logps_train/rejected': '-156.52', 'logps_train/chosen': '-168.36', 'loss/train': '0.68866', 'examples_per_second': '31.581', 'grad_norm': '108.5', 'counters/examples': 89504, 'counters/updates': 2797}
train stats after 89536 examples: {'rewards_train/chosen': '0.036994', 'rewards_train/rejected': '0.073098', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036104', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-164.26', 'loss/train': '0.7227', 'examples_per_second': '31.576', 'grad_norm': '138', 'counters/examples': 89536, 'counters/updates': 2798}
train stats after 89568 examples: {'rewards_train/chosen': '0.041814', 'rewards_train/rejected': '0.014615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027198', 'logps_train/rejected': '-94.473', 'logps_train/chosen': '-148.2', 'loss/train': '0.68868', 'examples_per_second': '30.723', 'grad_norm': '60.25', 'counters/examples': 89568, 'counters/updates': 2799}
train stats after 89600 examples: {'rewards_train/chosen': '0.0063355', 'rewards_train/rejected': '0.023408', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017072', 'logps_train/rejected': '-163.58', 'logps_train/chosen': '-137.16', 'loss/train': '0.70955', 'examples_per_second': '30.627', 'grad_norm': '112.5', 'counters/examples': 89600, 'counters/updates': 2800}
skipping logging after 89632 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '0.14729', 'rewards_train/rejected': '0.044929', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10236', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-154.98', 'loss/train': '0.6619', 'examples_per_second': '32.127', 'grad_norm': '77', 'counters/examples': 89664, 'counters/updates': 2802}
skipping logging after 89696 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '0.050973', 'rewards_train/rejected': '0.11912', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.068142', 'logps_train/rejected': '-151.02', 'logps_train/chosen': '-174.65', 'loss/train': '0.73917', 'examples_per_second': '31.207', 'grad_norm': '106', 'counters/examples': 89728, 'counters/updates': 2804}
skipping logging after 89760 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '-0.0068452', 'rewards_train/rejected': '-0.0097529', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0029077', 'logps_train/rejected': '-183.3', 'logps_train/chosen': '-133.47', 'loss/train': '0.70892', 'examples_per_second': '31.587', 'grad_norm': '188', 'counters/examples': 89792, 'counters/updates': 2806}
train stats after 89824 examples: {'rewards_train/chosen': '0.027751', 'rewards_train/rejected': '0.035027', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0072758', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-135.37', 'loss/train': '0.70554', 'examples_per_second': '30.091', 'grad_norm': '82.5', 'counters/examples': 89824, 'counters/updates': 2807}
train stats after 89856 examples: {'rewards_train/chosen': '0.028226', 'rewards_train/rejected': '4.1878e-05', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028184', 'logps_train/rejected': '-116.29', 'logps_train/chosen': '-165.58', 'loss/train': '0.6915', 'examples_per_second': '31.642', 'grad_norm': '109.5', 'counters/examples': 89856, 'counters/updates': 2808}
train stats after 89888 examples: {'rewards_train/chosen': '-0.016735', 'rewards_train/rejected': '-0.0084774', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0082573', 'logps_train/rejected': '-165.98', 'logps_train/chosen': '-144.48', 'loss/train': '0.70713', 'examples_per_second': '30.635', 'grad_norm': '142', 'counters/examples': 89888, 'counters/updates': 2809}
train stats after 89920 examples: {'rewards_train/chosen': '0.0018007', 'rewards_train/rejected': '-0.024855', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026656', 'logps_train/rejected': '-142.31', 'logps_train/chosen': '-159.39', 'loss/train': '0.6992', 'examples_per_second': '30.401', 'grad_norm': '81.5', 'counters/examples': 89920, 'counters/updates': 2810}
train stats after 89952 examples: {'rewards_train/chosen': '-0.084368', 'rewards_train/rejected': '0.010013', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.094381', 'logps_train/rejected': '-142.33', 'logps_train/chosen': '-182.38', 'loss/train': '0.755', 'examples_per_second': '31.587', 'grad_norm': '165', 'counters/examples': 89952, 'counters/updates': 2811}
train stats after 89984 examples: {'rewards_train/chosen': '0.028714', 'rewards_train/rejected': '0.025505', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0032092', 'logps_train/rejected': '-137.44', 'logps_train/chosen': '-193.07', 'loss/train': '0.6983', 'examples_per_second': '32.097', 'grad_norm': '82', 'counters/examples': 89984, 'counters/updates': 2812}
train stats after 90016 examples: {'rewards_train/chosen': '0.036314', 'rewards_train/rejected': '-0.007964', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044278', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-158.09', 'loss/train': '0.6788', 'examples_per_second': '31.42', 'grad_norm': '67', 'counters/examples': 90016, 'counters/updates': 2813}
train stats after 90048 examples: {'rewards_train/chosen': '0.089102', 'rewards_train/rejected': '0.13166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.042561', 'logps_train/rejected': '-181.56', 'logps_train/chosen': '-143.22', 'loss/train': '0.73404', 'examples_per_second': '30.561', 'grad_norm': '172', 'counters/examples': 90048, 'counters/updates': 2814}
train stats after 90080 examples: {'rewards_train/chosen': '0.15873', 'rewards_train/rejected': '0.084755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073978', 'logps_train/rejected': '-164.13', 'logps_train/chosen': '-167.99', 'loss/train': '0.68242', 'examples_per_second': '33.091', 'grad_norm': '103', 'counters/examples': 90080, 'counters/updates': 2815}
train stats after 90112 examples: {'rewards_train/chosen': '0.2183', 'rewards_train/rejected': '0.030719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18758', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-163.34', 'loss/train': '0.62178', 'examples_per_second': '31.678', 'grad_norm': '49.25', 'counters/examples': 90112, 'counters/updates': 2816}
train stats after 90144 examples: {'rewards_train/chosen': '0.12053', 'rewards_train/rejected': '0.024613', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095915', 'logps_train/rejected': '-154.1', 'logps_train/chosen': '-157.61', 'loss/train': '0.66122', 'examples_per_second': '33.134', 'grad_norm': '88', 'counters/examples': 90144, 'counters/updates': 2817}
train stats after 90176 examples: {'rewards_train/chosen': '0.07326', 'rewards_train/rejected': '0.049932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023329', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-158.78', 'loss/train': '0.69622', 'examples_per_second': '30.588', 'grad_norm': '92', 'counters/examples': 90176, 'counters/updates': 2818}
train stats after 90208 examples: {'rewards_train/chosen': '-0.018969', 'rewards_train/rejected': '0.032367', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051336', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-162.97', 'loss/train': '0.7302', 'examples_per_second': '25.289', 'grad_norm': '128', 'counters/examples': 90208, 'counters/updates': 2819}
train stats after 90240 examples: {'rewards_train/chosen': '-0.030827', 'rewards_train/rejected': '-0.045822', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014995', 'logps_train/rejected': '-111.42', 'logps_train/chosen': '-194.22', 'loss/train': '0.69937', 'examples_per_second': '30.296', 'grad_norm': '76.5', 'counters/examples': 90240, 'counters/updates': 2820}
train stats after 90272 examples: {'rewards_train/chosen': '0.012173', 'rewards_train/rejected': '-0.032932', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045106', 'logps_train/rejected': '-161.41', 'logps_train/chosen': '-134.75', 'loss/train': '0.67707', 'examples_per_second': '30.74', 'grad_norm': '90.5', 'counters/examples': 90272, 'counters/updates': 2821}
train stats after 90304 examples: {'rewards_train/chosen': '0.032056', 'rewards_train/rejected': '0.078935', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.046879', 'logps_train/rejected': '-138.87', 'logps_train/chosen': '-157.28', 'loss/train': '0.73259', 'examples_per_second': '23.873', 'grad_norm': '129', 'counters/examples': 90304, 'counters/updates': 2822}
train stats after 90336 examples: {'rewards_train/chosen': '0.1255', 'rewards_train/rejected': '0.073053', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052449', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-145.62', 'loss/train': '0.68406', 'examples_per_second': '31.175', 'grad_norm': '60.25', 'counters/examples': 90336, 'counters/updates': 2823}
train stats after 90368 examples: {'rewards_train/chosen': '0.045373', 'rewards_train/rejected': '0.046026', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00065296', 'logps_train/rejected': '-109.86', 'logps_train/chosen': '-123.79', 'loss/train': '0.70729', 'examples_per_second': '32.015', 'grad_norm': '86', 'counters/examples': 90368, 'counters/updates': 2824}
train stats after 90400 examples: {'rewards_train/chosen': '0.01766', 'rewards_train/rejected': '0.050607', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032947', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-133.97', 'loss/train': '0.71607', 'examples_per_second': '32.773', 'grad_norm': '116', 'counters/examples': 90400, 'counters/updates': 2825}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90464 examples: {'rewards_train/chosen': '0.055184', 'rewards_train/rejected': '0.015395', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039788', 'logps_train/rejected': '-113.32', 'logps_train/chosen': '-143', 'loss/train': '0.68248', 'examples_per_second': '30.265', 'grad_norm': '118.5', 'counters/examples': 90464, 'counters/updates': 2827}
train stats after 90496 examples: {'rewards_train/chosen': '0.028651', 'rewards_train/rejected': '0.086487', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057835', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-125.37', 'loss/train': '0.73165', 'examples_per_second': '30.103', 'grad_norm': '128', 'counters/examples': 90496, 'counters/updates': 2828}
train stats after 90528 examples: {'rewards_train/chosen': '0.082366', 'rewards_train/rejected': '0.064108', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018258', 'logps_train/rejected': '-154.13', 'logps_train/chosen': '-164.87', 'loss/train': '0.72513', 'examples_per_second': '30.085', 'grad_norm': '154', 'counters/examples': 90528, 'counters/updates': 2829}
train stats after 90560 examples: {'rewards_train/chosen': '0.0027379', 'rewards_train/rejected': '-0.029191', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031929', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-166.96', 'loss/train': '0.68857', 'examples_per_second': '30.087', 'grad_norm': '101.5', 'counters/examples': 90560, 'counters/updates': 2830}
train stats after 90592 examples: {'rewards_train/chosen': '0.10443', 'rewards_train/rejected': '0.079469', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024959', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-126.96', 'loss/train': '0.69079', 'examples_per_second': '32.071', 'grad_norm': '72.5', 'counters/examples': 90592, 'counters/updates': 2831}
train stats after 90624 examples: {'rewards_train/chosen': '0.030718', 'rewards_train/rejected': '0.10133', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.070615', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-117.2', 'loss/train': '0.7396', 'examples_per_second': '32.696', 'grad_norm': '53', 'counters/examples': 90624, 'counters/updates': 2832}
skipping logging after 90656 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '0.072047', 'rewards_train/rejected': '0.011174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060872', 'logps_train/rejected': '-121.9', 'logps_train/chosen': '-129.5', 'loss/train': '0.66832', 'examples_per_second': '34.247', 'grad_norm': '54.25', 'counters/examples': 90688, 'counters/updates': 2834}
train stats after 90720 examples: {'rewards_train/chosen': '0.027877', 'rewards_train/rejected': '0.0075208', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020357', 'logps_train/rejected': '-162.08', 'logps_train/chosen': '-173.58', 'loss/train': '0.69025', 'examples_per_second': '31.588', 'grad_norm': '110.5', 'counters/examples': 90720, 'counters/updates': 2835}
train stats after 90752 examples: {'rewards_train/chosen': '0.068431', 'rewards_train/rejected': '-0.028173', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096604', 'logps_train/rejected': '-82.957', 'logps_train/chosen': '-117.41', 'loss/train': '0.65113', 'examples_per_second': '32.347', 'grad_norm': '66.5', 'counters/examples': 90752, 'counters/updates': 2836}
skipping logging after 90784 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '0.08837', 'rewards_train/rejected': '0.057645', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030725', 'logps_train/rejected': '-180.64', 'logps_train/chosen': '-154', 'loss/train': '0.69363', 'examples_per_second': '31.579', 'grad_norm': '252', 'counters/examples': 90816, 'counters/updates': 2838}
train stats after 90848 examples: {'rewards_train/chosen': '-0.015749', 'rewards_train/rejected': '-0.048058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032309', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-165.71', 'loss/train': '0.69495', 'examples_per_second': '30.896', 'grad_norm': '77.5', 'counters/examples': 90848, 'counters/updates': 2839}
train stats after 90880 examples: {'rewards_train/chosen': '0.090063', 'rewards_train/rejected': '-0.017203', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10727', 'logps_train/rejected': '-198.91', 'logps_train/chosen': '-184.76', 'loss/train': '0.66013', 'examples_per_second': '31.507', 'grad_norm': '108.5', 'counters/examples': 90880, 'counters/updates': 2840}
train stats after 90912 examples: {'rewards_train/chosen': '0.042288', 'rewards_train/rejected': '-0.002643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044931', 'logps_train/rejected': '-144.15', 'logps_train/chosen': '-177.59', 'loss/train': '0.68686', 'examples_per_second': '31.068', 'grad_norm': '117', 'counters/examples': 90912, 'counters/updates': 2841}
train stats after 90944 examples: {'rewards_train/chosen': '0.065732', 'rewards_train/rejected': '0.029758', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035974', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-160.1', 'loss/train': '0.68521', 'examples_per_second': '33.016', 'grad_norm': '63.25', 'counters/examples': 90944, 'counters/updates': 2842}
train stats after 90976 examples: {'rewards_train/chosen': '0.012793', 'rewards_train/rejected': '0.051697', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.038904', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-136.31', 'loss/train': '0.71926', 'examples_per_second': '31.505', 'grad_norm': '110.5', 'counters/examples': 90976, 'counters/updates': 2843}
train stats after 91008 examples: {'rewards_train/chosen': '0.046219', 'rewards_train/rejected': '0.09918', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.052961', 'logps_train/rejected': '-148.11', 'logps_train/chosen': '-140.21', 'loss/train': '0.73006', 'examples_per_second': '31.203', 'grad_norm': '209', 'counters/examples': 91008, 'counters/updates': 2844}
train stats after 91040 examples: {'rewards_train/chosen': '0.052286', 'rewards_train/rejected': '0.022033', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030253', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-128.49', 'loss/train': '0.6967', 'examples_per_second': '32.263', 'grad_norm': '69.5', 'counters/examples': 91040, 'counters/updates': 2845}
train stats after 91072 examples: {'rewards_train/chosen': '0.078243', 'rewards_train/rejected': '0.099832', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.021589', 'logps_train/rejected': '-140.54', 'logps_train/chosen': '-156.3', 'loss/train': '0.71596', 'examples_per_second': '32.156', 'grad_norm': '65', 'counters/examples': 91072, 'counters/updates': 2846}
train stats after 91104 examples: {'rewards_train/chosen': '0.12787', 'rewards_train/rejected': '-0.030007', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15788', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-150.93', 'loss/train': '0.63466', 'examples_per_second': '32.549', 'grad_norm': '56.25', 'counters/examples': 91104, 'counters/updates': 2847}
train stats after 91136 examples: {'rewards_train/chosen': '0.068036', 'rewards_train/rejected': '0.0025678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065468', 'logps_train/rejected': '-105.61', 'logps_train/chosen': '-153.1', 'loss/train': '0.67242', 'examples_per_second': '31.613', 'grad_norm': '149', 'counters/examples': 91136, 'counters/updates': 2848}
train stats after 91168 examples: {'rewards_train/chosen': '0.13158', 'rewards_train/rejected': '0.0047674', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12681', 'logps_train/rejected': '-124.43', 'logps_train/chosen': '-143.01', 'loss/train': '0.64387', 'examples_per_second': '32.041', 'grad_norm': '67.5', 'counters/examples': 91168, 'counters/updates': 2849}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91232 examples: {'rewards_train/chosen': '0.049208', 'rewards_train/rejected': '0.058861', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0096529', 'logps_train/rejected': '-153.86', 'logps_train/chosen': '-160.3', 'loss/train': '0.71013', 'examples_per_second': '32.535', 'grad_norm': '98.5', 'counters/examples': 91232, 'counters/updates': 2851}
train stats after 91264 examples: {'rewards_train/chosen': '0.0086826', 'rewards_train/rejected': '0.050533', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04185', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-140.9', 'loss/train': '0.72243', 'examples_per_second': '31.082', 'grad_norm': '79', 'counters/examples': 91264, 'counters/updates': 2852}
train stats after 91296 examples: {'rewards_train/chosen': '0.083715', 'rewards_train/rejected': '-0.029066', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11278', 'logps_train/rejected': '-146.36', 'logps_train/chosen': '-139.36', 'loss/train': '0.64664', 'examples_per_second': '30.048', 'grad_norm': '145', 'counters/examples': 91296, 'counters/updates': 2853}
train stats after 91328 examples: {'rewards_train/chosen': '0.029968', 'rewards_train/rejected': '0.047903', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017935', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-145.44', 'loss/train': '0.71362', 'examples_per_second': '31.771', 'grad_norm': '99.5', 'counters/examples': 91328, 'counters/updates': 2854}
train stats after 91360 examples: {'rewards_train/chosen': '-0.0015622', 'rewards_train/rejected': '0.027119', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.028681', 'logps_train/rejected': '-146.23', 'logps_train/chosen': '-142.27', 'loss/train': '0.72194', 'examples_per_second': '32.2', 'grad_norm': '90.5', 'counters/examples': 91360, 'counters/updates': 2855}
train stats after 91392 examples: {'rewards_train/chosen': '0.11136', 'rewards_train/rejected': '0.059522', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051842', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-149.78', 'loss/train': '0.68145', 'examples_per_second': '30.6', 'grad_norm': '100', 'counters/examples': 91392, 'counters/updates': 2856}
skipping logging after 91424 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '-0.012187', 'rewards_train/rejected': '-0.0061633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0060236', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-170.05', 'loss/train': '0.70239', 'examples_per_second': '31.608', 'grad_norm': '61', 'counters/examples': 91456, 'counters/updates': 2858}
train stats after 91488 examples: {'rewards_train/chosen': '0.060245', 'rewards_train/rejected': '-0.023176', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083422', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-125.49', 'loss/train': '0.66592', 'examples_per_second': '32.47', 'grad_norm': '104.5', 'counters/examples': 91488, 'counters/updates': 2859}
skipping logging after 91520 examples to avoid logging too frequently
train stats after 91552 examples: {'rewards_train/chosen': '-0.01144', 'rewards_train/rejected': '0.051309', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.062749', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-149.52', 'loss/train': '0.73606', 'examples_per_second': '31.087', 'grad_norm': '65.5', 'counters/examples': 91552, 'counters/updates': 2861}
train stats after 91584 examples: {'rewards_train/chosen': '0.048249', 'rewards_train/rejected': '0.0037704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044479', 'logps_train/rejected': '-171.45', 'logps_train/chosen': '-159.63', 'loss/train': '0.68493', 'examples_per_second': '31.549', 'grad_norm': '68.5', 'counters/examples': 91584, 'counters/updates': 2862}
train stats after 91616 examples: {'rewards_train/chosen': '0.0039071', 'rewards_train/rejected': '0.037836', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033928', 'logps_train/rejected': '-119.9', 'logps_train/chosen': '-124.53', 'loss/train': '0.72201', 'examples_per_second': '32.513', 'grad_norm': '130', 'counters/examples': 91616, 'counters/updates': 2863}
train stats after 91648 examples: {'rewards_train/chosen': '0.023185', 'rewards_train/rejected': '-0.0032696', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.026454', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-142.4', 'loss/train': '0.69264', 'examples_per_second': '31.021', 'grad_norm': '99.5', 'counters/examples': 91648, 'counters/updates': 2864}
skipping logging after 91680 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '0.052165', 'rewards_train/rejected': '-0.04327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095435', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-149.4', 'loss/train': '0.65538', 'examples_per_second': '31.644', 'grad_norm': '100', 'counters/examples': 91712, 'counters/updates': 2866}
train stats after 91744 examples: {'rewards_train/chosen': '0.067249', 'rewards_train/rejected': '0.048288', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018961', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-137.84', 'loss/train': '0.69211', 'examples_per_second': '30.862', 'grad_norm': '112.5', 'counters/examples': 91744, 'counters/updates': 2867}
train stats after 91776 examples: {'rewards_train/chosen': '0.037546', 'rewards_train/rejected': '0.00092853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036617', 'logps_train/rejected': '-96.657', 'logps_train/chosen': '-146.05', 'loss/train': '0.68303', 'examples_per_second': '31.198', 'grad_norm': '82.5', 'counters/examples': 91776, 'counters/updates': 2868}
train stats after 91808 examples: {'rewards_train/chosen': '-0.0082843', 'rewards_train/rejected': '0.015967', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024251', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-159.81', 'loss/train': '0.71611', 'examples_per_second': '32.262', 'grad_norm': '82', 'counters/examples': 91808, 'counters/updates': 2869}
skipping logging after 91840 examples to avoid logging too frequently
train stats after 91872 examples: {'rewards_train/chosen': '0.076336', 'rewards_train/rejected': '0.050469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025867', 'logps_train/rejected': '-161.95', 'logps_train/chosen': '-129.73', 'loss/train': '0.68642', 'examples_per_second': '30.792', 'grad_norm': '70.5', 'counters/examples': 91872, 'counters/updates': 2871}
train stats after 91904 examples: {'rewards_train/chosen': '0.063619', 'rewards_train/rejected': '0.043489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02013', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-165.01', 'loss/train': '0.69383', 'examples_per_second': '32.451', 'grad_norm': '63.75', 'counters/examples': 91904, 'counters/updates': 2872}
train stats after 91936 examples: {'rewards_train/chosen': '0.0078859', 'rewards_train/rejected': '0.017893', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010007', 'logps_train/rejected': '-93.699', 'logps_train/chosen': '-125.57', 'loss/train': '0.70423', 'examples_per_second': '31.061', 'grad_norm': '70.5', 'counters/examples': 91936, 'counters/updates': 2873}
train stats after 91968 examples: {'rewards_train/chosen': '0.030546', 'rewards_train/rejected': '0.078236', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.047691', 'logps_train/rejected': '-150.09', 'logps_train/chosen': '-174.36', 'loss/train': '0.72617', 'examples_per_second': '31.218', 'grad_norm': '73', 'counters/examples': 91968, 'counters/updates': 2874}
train stats after 92000 examples: {'rewards_train/chosen': '0.085697', 'rewards_train/rejected': '0.002144', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083553', 'logps_train/rejected': '-168.04', 'logps_train/chosen': '-157.45', 'loss/train': '0.66503', 'examples_per_second': '30.167', 'grad_norm': '264', 'counters/examples': 92000, 'counters/updates': 2875}
train stats after 92032 examples: {'rewards_train/chosen': '0.0022447', 'rewards_train/rejected': '0.028389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.026144', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-149.75', 'loss/train': '0.7157', 'examples_per_second': '26.778', 'grad_norm': '68.5', 'counters/examples': 92032, 'counters/updates': 2876}
train stats after 92064 examples: {'rewards_train/chosen': '0.092479', 'rewards_train/rejected': '0.058927', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033551', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-157.24', 'loss/train': '0.68688', 'examples_per_second': '32.109', 'grad_norm': '113', 'counters/examples': 92064, 'counters/updates': 2877}
train stats after 92096 examples: {'rewards_train/chosen': '0.066005', 'rewards_train/rejected': '0.062649', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0033554', 'logps_train/rejected': '-154.81', 'logps_train/chosen': '-148.48', 'loss/train': '0.70692', 'examples_per_second': '32.088', 'grad_norm': '91', 'counters/examples': 92096, 'counters/updates': 2878}
train stats after 92128 examples: {'rewards_train/chosen': '0.062652', 'rewards_train/rejected': '-0.054839', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11749', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-161.55', 'loss/train': '0.64324', 'examples_per_second': '31.58', 'grad_norm': '64.5', 'counters/examples': 92128, 'counters/updates': 2879}
skipping logging after 92160 examples to avoid logging too frequently
train stats after 92192 examples: {'rewards_train/chosen': '0.040166', 'rewards_train/rejected': '0.099013', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.058847', 'logps_train/rejected': '-145.64', 'logps_train/chosen': '-176.22', 'loss/train': '0.73767', 'examples_per_second': '30.739', 'grad_norm': '139', 'counters/examples': 92192, 'counters/updates': 2881}
train stats after 92224 examples: {'rewards_train/chosen': '0.015555', 'rewards_train/rejected': '0.017', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0014457', 'logps_train/rejected': '-157.3', 'logps_train/chosen': '-157.88', 'loss/train': '0.7021', 'examples_per_second': '31.612', 'grad_norm': '111', 'counters/examples': 92224, 'counters/updates': 2882}
skipping logging after 92256 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '0.038674', 'rewards_train/rejected': '0.010434', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02824', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-155.57', 'loss/train': '0.68982', 'examples_per_second': '31.625', 'grad_norm': '181', 'counters/examples': 92288, 'counters/updates': 2884}
train stats after 92320 examples: {'rewards_train/chosen': '0.097861', 'rewards_train/rejected': '0.064712', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033149', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-192.79', 'loss/train': '0.71073', 'examples_per_second': '32.278', 'grad_norm': '74.5', 'counters/examples': 92320, 'counters/updates': 2885}
train stats after 92352 examples: {'rewards_train/chosen': '0.0020214', 'rewards_train/rejected': '-0.032667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034688', 'logps_train/rejected': '-130.98', 'logps_train/chosen': '-191.06', 'loss/train': '0.68816', 'examples_per_second': '30.627', 'grad_norm': '77', 'counters/examples': 92352, 'counters/updates': 2886}
train stats after 92384 examples: {'rewards_train/chosen': '0.026485', 'rewards_train/rejected': '0.011817', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014668', 'logps_train/rejected': '-160.17', 'logps_train/chosen': '-135.59', 'loss/train': '0.70501', 'examples_per_second': '31.661', 'grad_norm': '131', 'counters/examples': 92384, 'counters/updates': 2887}
skipping logging after 92416 examples to avoid logging too frequently
train stats after 92448 examples: {'rewards_train/chosen': '0.060842', 'rewards_train/rejected': '0.031227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029614', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-151.66', 'loss/train': '0.69119', 'examples_per_second': '31.569', 'grad_norm': '67', 'counters/examples': 92448, 'counters/updates': 2889}
train stats after 92480 examples: {'rewards_train/chosen': '0.0219', 'rewards_train/rejected': '-0.061615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083515', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-167.1', 'loss/train': '0.66173', 'examples_per_second': '31.609', 'grad_norm': '94.5', 'counters/examples': 92480, 'counters/updates': 2890}
skipping logging after 92512 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '0.05583', 'rewards_train/rejected': '0.026312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029518', 'logps_train/rejected': '-144.72', 'logps_train/chosen': '-154.92', 'loss/train': '0.70325', 'examples_per_second': '31.924', 'grad_norm': '70', 'counters/examples': 92544, 'counters/updates': 2892}
train stats after 92576 examples: {'rewards_train/chosen': '0.026475', 'rewards_train/rejected': '0.015755', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010719', 'logps_train/rejected': '-91.764', 'logps_train/chosen': '-140.9', 'loss/train': '0.70105', 'examples_per_second': '30.954', 'grad_norm': '55.5', 'counters/examples': 92576, 'counters/updates': 2893}
train stats after 92608 examples: {'rewards_train/chosen': '0.10894', 'rewards_train/rejected': '0.13233', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023384', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-180.39', 'loss/train': '0.71907', 'examples_per_second': '30.216', 'grad_norm': '77', 'counters/examples': 92608, 'counters/updates': 2894}
train stats after 92640 examples: {'rewards_train/chosen': '0.0078941', 'rewards_train/rejected': '-0.019543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027437', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-158.76', 'loss/train': '0.70164', 'examples_per_second': '30.155', 'grad_norm': '91', 'counters/examples': 92640, 'counters/updates': 2895}
skipping logging after 92672 examples to avoid logging too frequently
train stats after 92704 examples: {'rewards_train/chosen': '0.017245', 'rewards_train/rejected': '-0.0085631', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025808', 'logps_train/rejected': '-143.5', 'logps_train/chosen': '-157.54', 'loss/train': '0.69188', 'examples_per_second': '32.042', 'grad_norm': '53.25', 'counters/examples': 92704, 'counters/updates': 2897}
train stats after 92736 examples: {'rewards_train/chosen': '0.067426', 'rewards_train/rejected': '0.032718', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034708', 'logps_train/rejected': '-108.58', 'logps_train/chosen': '-129.98', 'loss/train': '0.69322', 'examples_per_second': '33.085', 'grad_norm': '58', 'counters/examples': 92736, 'counters/updates': 2898}
train stats after 92768 examples: {'rewards_train/chosen': '0.0539', 'rewards_train/rejected': '0.036086', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017814', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-119.16', 'loss/train': '0.69124', 'examples_per_second': '31.302', 'grad_norm': '105.5', 'counters/examples': 92768, 'counters/updates': 2899}
skipping logging after 92800 examples to avoid logging too frequently
train stats after 92832 examples: {'rewards_train/chosen': '0.10597', 'rewards_train/rejected': '0.055467', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050501', 'logps_train/rejected': '-162.61', 'logps_train/chosen': '-183.62', 'loss/train': '0.69407', 'examples_per_second': '34.059', 'grad_norm': '90', 'counters/examples': 92832, 'counters/updates': 2901}
train stats after 92864 examples: {'rewards_train/chosen': '0.002651', 'rewards_train/rejected': '-0.0041052', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0067561', 'logps_train/rejected': '-106.56', 'logps_train/chosen': '-165.09', 'loss/train': '0.70029', 'examples_per_second': '30.344', 'grad_norm': '250', 'counters/examples': 92864, 'counters/updates': 2902}
train stats after 92896 examples: {'rewards_train/chosen': '0.066665', 'rewards_train/rejected': '0.045321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.021344', 'logps_train/rejected': '-143.96', 'logps_train/chosen': '-109.66', 'loss/train': '0.69501', 'examples_per_second': '31.618', 'grad_norm': '112', 'counters/examples': 92896, 'counters/updates': 2903}
train stats after 92928 examples: {'rewards_train/chosen': '0.013375', 'rewards_train/rejected': '-0.021589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034964', 'logps_train/rejected': '-107.87', 'logps_train/chosen': '-148.11', 'loss/train': '0.68438', 'examples_per_second': '31.036', 'grad_norm': '78', 'counters/examples': 92928, 'counters/updates': 2904}
train stats after 92960 examples: {'rewards_train/chosen': '0.061271', 'rewards_train/rejected': '0.020791', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04048', 'logps_train/rejected': '-87.83', 'logps_train/chosen': '-160.36', 'loss/train': '0.67934', 'examples_per_second': '31.143', 'grad_norm': '70.5', 'counters/examples': 92960, 'counters/updates': 2905}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93024 examples: {'rewards_train/chosen': '0.028041', 'rewards_train/rejected': '0.0068138', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021228', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-180.69', 'loss/train': '0.69802', 'examples_per_second': '30.998', 'grad_norm': '106.5', 'counters/examples': 93024, 'counters/updates': 2907}
skipping logging after 93056 examples to avoid logging too frequently
train stats after 93088 examples: {'rewards_train/chosen': '-0.010655', 'rewards_train/rejected': '0.041019', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.051674', 'logps_train/rejected': '-178.87', 'logps_train/chosen': '-161.94', 'loss/train': '0.73203', 'examples_per_second': '33.644', 'grad_norm': '249', 'counters/examples': 93088, 'counters/updates': 2909}
train stats after 93120 examples: {'rewards_train/chosen': '0.0015995', 'rewards_train/rejected': '0.025217', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023618', 'logps_train/rejected': '-156.06', 'logps_train/chosen': '-163.98', 'loss/train': '0.71619', 'examples_per_second': '30.52', 'grad_norm': '74', 'counters/examples': 93120, 'counters/updates': 2910}
train stats after 93152 examples: {'rewards_train/chosen': '0.046467', 'rewards_train/rejected': '0.041232', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0052354', 'logps_train/rejected': '-83.432', 'logps_train/chosen': '-131.45', 'loss/train': '0.69544', 'examples_per_second': '31.514', 'grad_norm': '58.25', 'counters/examples': 93152, 'counters/updates': 2911}
skipping logging after 93184 examples to avoid logging too frequently
train stats after 93216 examples: {'rewards_train/chosen': '0.06139', 'rewards_train/rejected': '0.021643', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.039747', 'logps_train/rejected': '-138.76', 'logps_train/chosen': '-184.6', 'loss/train': '0.68352', 'examples_per_second': '33.495', 'grad_norm': '65.5', 'counters/examples': 93216, 'counters/updates': 2913}
train stats after 93248 examples: {'rewards_train/chosen': '0.045191', 'rewards_train/rejected': '0.046239', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0010482', 'logps_train/rejected': '-149.92', 'logps_train/chosen': '-131.97', 'loss/train': '0.70467', 'examples_per_second': '31.347', 'grad_norm': '70.5', 'counters/examples': 93248, 'counters/updates': 2914}
train stats after 93280 examples: {'rewards_train/chosen': '0.068878', 'rewards_train/rejected': '0.014594', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054284', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-173.59', 'loss/train': '0.67366', 'examples_per_second': '31.718', 'grad_norm': '80', 'counters/examples': 93280, 'counters/updates': 2915}
train stats after 93312 examples: {'rewards_train/chosen': '0.059069', 'rewards_train/rejected': '-0.017602', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076671', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-109.9', 'loss/train': '0.6615', 'examples_per_second': '31.415', 'grad_norm': '65.5', 'counters/examples': 93312, 'counters/updates': 2916}
train stats after 93344 examples: {'rewards_train/chosen': '0.066082', 'rewards_train/rejected': '0.065762', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00031968', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-168', 'loss/train': '0.70608', 'examples_per_second': '31.686', 'grad_norm': '88.5', 'counters/examples': 93344, 'counters/updates': 2917}
train stats after 93376 examples: {'rewards_train/chosen': '-0.015229', 'rewards_train/rejected': '-0.021997', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.006768', 'logps_train/rejected': '-178.75', 'logps_train/chosen': '-161.36', 'loss/train': '0.70368', 'examples_per_second': '31.523', 'grad_norm': '80.5', 'counters/examples': 93376, 'counters/updates': 2918}
train stats after 93408 examples: {'rewards_train/chosen': '0.027029', 'rewards_train/rejected': '-0.035746', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062775', 'logps_train/rejected': '-161.17', 'logps_train/chosen': '-180.64', 'loss/train': '0.6771', 'examples_per_second': '31.619', 'grad_norm': '123', 'counters/examples': 93408, 'counters/updates': 2919}
train stats after 93440 examples: {'rewards_train/chosen': '0.094644', 'rewards_train/rejected': '0.042288', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052356', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-167.35', 'loss/train': '0.67506', 'examples_per_second': '31.646', 'grad_norm': '116.5', 'counters/examples': 93440, 'counters/updates': 2920}
train stats after 93472 examples: {'rewards_train/chosen': '0.089389', 'rewards_train/rejected': '0.048399', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04099', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-142.18', 'loss/train': '0.68291', 'examples_per_second': '31.019', 'grad_norm': '67', 'counters/examples': 93472, 'counters/updates': 2921}
train stats after 93504 examples: {'rewards_train/chosen': '0.05442', 'rewards_train/rejected': '0.020626', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033794', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-164.07', 'loss/train': '0.6897', 'examples_per_second': '31.65', 'grad_norm': '85.5', 'counters/examples': 93504, 'counters/updates': 2922}
train stats after 93536 examples: {'rewards_train/chosen': '-0.091734', 'rewards_train/rejected': '-0.038539', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.053195', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-191.33', 'loss/train': '0.74819', 'examples_per_second': '31.419', 'grad_norm': '72', 'counters/examples': 93536, 'counters/updates': 2923}
train stats after 93568 examples: {'rewards_train/chosen': '0.089501', 'rewards_train/rejected': '0.0045655', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084935', 'logps_train/rejected': '-189.55', 'logps_train/chosen': '-190.4', 'loss/train': '0.66514', 'examples_per_second': '31.606', 'grad_norm': '127.5', 'counters/examples': 93568, 'counters/updates': 2924}
train stats after 93600 examples: {'rewards_train/chosen': '0.051503', 'rewards_train/rejected': '0.080883', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029379', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-161.07', 'loss/train': '0.71882', 'examples_per_second': '30.145', 'grad_norm': '150', 'counters/examples': 93600, 'counters/updates': 2925}
train stats after 93632 examples: {'rewards_train/chosen': '0.074129', 'rewards_train/rejected': '0.0070105', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067118', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-165.94', 'loss/train': '0.67721', 'examples_per_second': '31.31', 'grad_norm': '63.25', 'counters/examples': 93632, 'counters/updates': 2926}
train stats after 93664 examples: {'rewards_train/chosen': '0.06093', 'rewards_train/rejected': '0.029355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031574', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-166.54', 'loss/train': '0.68535', 'examples_per_second': '30.17', 'grad_norm': '54', 'counters/examples': 93664, 'counters/updates': 2927}
train stats after 93696 examples: {'rewards_train/chosen': '0.05337', 'rewards_train/rejected': '0.039541', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013829', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-182.53', 'loss/train': '0.7024', 'examples_per_second': '31.645', 'grad_norm': '101', 'counters/examples': 93696, 'counters/updates': 2928}
train stats after 93728 examples: {'rewards_train/chosen': '0.15131', 'rewards_train/rejected': '0.13895', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012368', 'logps_train/rejected': '-169.33', 'logps_train/chosen': '-118.41', 'loss/train': '0.69866', 'examples_per_second': '31.599', 'grad_norm': '92.5', 'counters/examples': 93728, 'counters/updates': 2929}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93792 examples: {'rewards_train/chosen': '0.14668', 'rewards_train/rejected': '0.12078', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025909', 'logps_train/rejected': '-162.4', 'logps_train/chosen': '-149.19', 'loss/train': '0.71834', 'examples_per_second': '30.078', 'grad_norm': '150', 'counters/examples': 93792, 'counters/updates': 2931}
train stats after 93824 examples: {'rewards_train/chosen': '0.0076283', 'rewards_train/rejected': '0.0019437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0056847', 'logps_train/rejected': '-96.518', 'logps_train/chosen': '-135.24', 'loss/train': '0.70163', 'examples_per_second': '31.584', 'grad_norm': '155', 'counters/examples': 93824, 'counters/updates': 2932}
train stats after 93856 examples: {'rewards_train/chosen': '0.034521', 'rewards_train/rejected': '-0.0021051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036626', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-166.9', 'loss/train': '0.68146', 'examples_per_second': '31.895', 'grad_norm': '53.5', 'counters/examples': 93856, 'counters/updates': 2933}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93920 examples: {'rewards_train/chosen': '0.065712', 'rewards_train/rejected': '0.023427', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042285', 'logps_train/rejected': '-147', 'logps_train/chosen': '-167.08', 'loss/train': '0.68719', 'examples_per_second': '30.139', 'grad_norm': '101', 'counters/examples': 93920, 'counters/updates': 2935}
train stats after 93952 examples: {'rewards_train/chosen': '0.080934', 'rewards_train/rejected': '0.069681', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011252', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-145.31', 'loss/train': '0.69841', 'examples_per_second': '32.992', 'grad_norm': '122', 'counters/examples': 93952, 'counters/updates': 2936}
train stats after 93984 examples: {'rewards_train/chosen': '0.11075', 'rewards_train/rejected': '-0.046439', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15719', 'logps_train/rejected': '-158.76', 'logps_train/chosen': '-169.22', 'loss/train': '0.63123', 'examples_per_second': '32.104', 'grad_norm': '69.5', 'counters/examples': 93984, 'counters/updates': 2937}
train stats after 94016 examples: {'rewards_train/chosen': '0.0086331', 'rewards_train/rejected': '0.090341', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.081708', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-149.31', 'loss/train': '0.74168', 'examples_per_second': '29.987', 'grad_norm': '84', 'counters/examples': 94016, 'counters/updates': 2938}
skipping logging after 94048 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '0.097125', 'rewards_train/rejected': '0.064231', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032894', 'logps_train/rejected': '-163.36', 'logps_train/chosen': '-175.56', 'loss/train': '0.70368', 'examples_per_second': '31.52', 'grad_norm': '135', 'counters/examples': 94080, 'counters/updates': 2940}
skipping logging after 94112 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '0.066468', 'rewards_train/rejected': '0.012257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054211', 'logps_train/rejected': '-140.37', 'logps_train/chosen': '-192.8', 'loss/train': '0.67987', 'examples_per_second': '31.493', 'grad_norm': '102', 'counters/examples': 94144, 'counters/updates': 2942}
train stats after 94176 examples: {'rewards_train/chosen': '0.1082', 'rewards_train/rejected': '-0.0022405', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11044', 'logps_train/rejected': '-152.1', 'logps_train/chosen': '-173.67', 'loss/train': '0.65498', 'examples_per_second': '31.292', 'grad_norm': '101', 'counters/examples': 94176, 'counters/updates': 2943}
train stats after 94208 examples: {'rewards_train/chosen': '0.079699', 'rewards_train/rejected': '0.012644', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067055', 'logps_train/rejected': '-145.11', 'logps_train/chosen': '-185.64', 'loss/train': '0.66712', 'examples_per_second': '31.535', 'grad_norm': '85', 'counters/examples': 94208, 'counters/updates': 2944}
train stats after 94240 examples: {'rewards_train/chosen': '0.044108', 'rewards_train/rejected': '-0.002377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046485', 'logps_train/rejected': '-92.522', 'logps_train/chosen': '-116.88', 'loss/train': '0.68011', 'examples_per_second': '30.065', 'grad_norm': '68.5', 'counters/examples': 94240, 'counters/updates': 2945}
train stats after 94272 examples: {'rewards_train/chosen': '0.19081', 'rewards_train/rejected': '-0.0089921', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1998', 'logps_train/rejected': '-148.13', 'logps_train/chosen': '-176.12', 'loss/train': '0.62795', 'examples_per_second': '33.051', 'grad_norm': '146', 'counters/examples': 94272, 'counters/updates': 2946}
train stats after 94304 examples: {'rewards_train/chosen': '0.1347', 'rewards_train/rejected': '0.028212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10649', 'logps_train/rejected': '-107.69', 'logps_train/chosen': '-151.22', 'loss/train': '0.65356', 'examples_per_second': '32.124', 'grad_norm': '71', 'counters/examples': 94304, 'counters/updates': 2947}
train stats after 94336 examples: {'rewards_train/chosen': '0.0022606', 'rewards_train/rejected': '0.020829', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018568', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-128.02', 'loss/train': '0.71439', 'examples_per_second': '30.239', 'grad_norm': '64.5', 'counters/examples': 94336, 'counters/updates': 2948}
train stats after 94368 examples: {'rewards_train/chosen': '0.061958', 'rewards_train/rejected': '0.031222', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030736', 'logps_train/rejected': '-113.3', 'logps_train/chosen': '-116.58', 'loss/train': '0.68881', 'examples_per_second': '32.297', 'grad_norm': '113.5', 'counters/examples': 94368, 'counters/updates': 2949}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94432 examples: {'rewards_train/chosen': '-0.063366', 'rewards_train/rejected': '0.022286', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.085652', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-165.85', 'loss/train': '0.7493', 'examples_per_second': '31.888', 'grad_norm': '104', 'counters/examples': 94432, 'counters/updates': 2951}
train stats after 94464 examples: {'rewards_train/chosen': '0.051295', 'rewards_train/rejected': '0.028872', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022423', 'logps_train/rejected': '-179.52', 'logps_train/chosen': '-163.81', 'loss/train': '0.69921', 'examples_per_second': '31.533', 'grad_norm': '77', 'counters/examples': 94464, 'counters/updates': 2952}
train stats after 94496 examples: {'rewards_train/chosen': '0.021014', 'rewards_train/rejected': '0.044111', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023097', 'logps_train/rejected': '-164.93', 'logps_train/chosen': '-173.9', 'loss/train': '0.71038', 'examples_per_second': '31.278', 'grad_norm': '83.5', 'counters/examples': 94496, 'counters/updates': 2953}
train stats after 94528 examples: {'rewards_train/chosen': '0.043929', 'rewards_train/rejected': '0.013223', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030706', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-126.83', 'loss/train': '0.68752', 'examples_per_second': '30.235', 'grad_norm': '56.75', 'counters/examples': 94528, 'counters/updates': 2954}
train stats after 94560 examples: {'rewards_train/chosen': '0.032544', 'rewards_train/rejected': '0.048348', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015804', 'logps_train/rejected': '-110.98', 'logps_train/chosen': '-155.28', 'loss/train': '0.71376', 'examples_per_second': '31.234', 'grad_norm': '82.5', 'counters/examples': 94560, 'counters/updates': 2955}
train stats after 94592 examples: {'rewards_train/chosen': '0.065504', 'rewards_train/rejected': '-0.0038417', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069346', 'logps_train/rejected': '-158', 'logps_train/chosen': '-153.4', 'loss/train': '0.67506', 'examples_per_second': '31.275', 'grad_norm': '91', 'counters/examples': 94592, 'counters/updates': 2956}
train stats after 94624 examples: {'rewards_train/chosen': '0.055364', 'rewards_train/rejected': '0.0040587', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.051306', 'logps_train/rejected': '-145.85', 'logps_train/chosen': '-192.54', 'loss/train': '0.67998', 'examples_per_second': '30.229', 'grad_norm': '82', 'counters/examples': 94624, 'counters/updates': 2957}
skipping logging after 94656 examples to avoid logging too frequently
train stats after 94688 examples: {'rewards_train/chosen': '0.079113', 'rewards_train/rejected': '7.891e-06', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079105', 'logps_train/rejected': '-111.94', 'logps_train/chosen': '-111.85', 'loss/train': '0.67271', 'examples_per_second': '31.391', 'grad_norm': '79', 'counters/examples': 94688, 'counters/updates': 2959}
skipping logging after 94720 examples to avoid logging too frequently
train stats after 94752 examples: {'rewards_train/chosen': '0.059468', 'rewards_train/rejected': '0.043326', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016142', 'logps_train/rejected': '-159.87', 'logps_train/chosen': '-151.02', 'loss/train': '0.69713', 'examples_per_second': '30.919', 'grad_norm': '94', 'counters/examples': 94752, 'counters/updates': 2961}
train stats after 94784 examples: {'rewards_train/chosen': '-0.0040383', 'rewards_train/rejected': '-0.017088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01305', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-133.62', 'loss/train': '0.69801', 'examples_per_second': '31.49', 'grad_norm': '222', 'counters/examples': 94784, 'counters/updates': 2962}
skipping logging after 94816 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '0.13171', 'rewards_train/rejected': '-0.026731', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15844', 'logps_train/rejected': '-139.96', 'logps_train/chosen': '-146.6', 'loss/train': '0.6373', 'examples_per_second': '32.08', 'grad_norm': '165', 'counters/examples': 94848, 'counters/updates': 2964}
train stats after 94880 examples: {'rewards_train/chosen': '0.075975', 'rewards_train/rejected': '0.045715', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03026', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-145.83', 'loss/train': '0.69223', 'examples_per_second': '31.306', 'grad_norm': '65', 'counters/examples': 94880, 'counters/updates': 2965}
train stats after 94912 examples: {'rewards_train/chosen': '0.034786', 'rewards_train/rejected': '0.025598', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0091884', 'logps_train/rejected': '-146.66', 'logps_train/chosen': '-137.57', 'loss/train': '0.69468', 'examples_per_second': '32.105', 'grad_norm': '67.5', 'counters/examples': 94912, 'counters/updates': 2966}
skipping logging after 94944 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '0.011182', 'rewards_train/rejected': '0.022271', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011089', 'logps_train/rejected': '-169.73', 'logps_train/chosen': '-143.49', 'loss/train': '0.70968', 'examples_per_second': '32.459', 'grad_norm': '114.5', 'counters/examples': 94976, 'counters/updates': 2968}
train stats after 95008 examples: {'rewards_train/chosen': '0.072346', 'rewards_train/rejected': '0.041792', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030554', 'logps_train/rejected': '-149.96', 'logps_train/chosen': '-150.26', 'loss/train': '0.68934', 'examples_per_second': '32.327', 'grad_norm': '108', 'counters/examples': 95008, 'counters/updates': 2969}
train stats after 95040 examples: {'rewards_train/chosen': '0.069502', 'rewards_train/rejected': '0.058501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011', 'logps_train/rejected': '-154.08', 'logps_train/chosen': '-131.33', 'loss/train': '0.7106', 'examples_per_second': '30.769', 'grad_norm': '119.5', 'counters/examples': 95040, 'counters/updates': 2970}
train stats after 95072 examples: {'rewards_train/chosen': '0.086004', 'rewards_train/rejected': '0.061921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024083', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-170.61', 'loss/train': '0.68861', 'examples_per_second': '31.513', 'grad_norm': '107', 'counters/examples': 95072, 'counters/updates': 2971}
train stats after 95104 examples: {'rewards_train/chosen': '0.044859', 'rewards_train/rejected': '0.0775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032642', 'logps_train/rejected': '-111.92', 'logps_train/chosen': '-169.15', 'loss/train': '0.73054', 'examples_per_second': '31.659', 'grad_norm': '105.5', 'counters/examples': 95104, 'counters/updates': 2972}
train stats after 95136 examples: {'rewards_train/chosen': '0.063306', 'rewards_train/rejected': '0.064182', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00087661', 'logps_train/rejected': '-116.86', 'logps_train/chosen': '-126.41', 'loss/train': '0.7046', 'examples_per_second': '31.951', 'grad_norm': '72', 'counters/examples': 95136, 'counters/updates': 2973}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95200 examples: {'rewards_train/chosen': '0.06385', 'rewards_train/rejected': '0.0054354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058415', 'logps_train/rejected': '-102.49', 'logps_train/chosen': '-142.82', 'loss/train': '0.67417', 'examples_per_second': '32.736', 'grad_norm': '55.5', 'counters/examples': 95200, 'counters/updates': 2975}
skipping logging after 95232 examples to avoid logging too frequently
train stats after 95264 examples: {'rewards_train/chosen': '0.066277', 'rewards_train/rejected': '0.082753', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016476', 'logps_train/rejected': '-120.83', 'logps_train/chosen': '-136.2', 'loss/train': '0.71369', 'examples_per_second': '30.992', 'grad_norm': '72.5', 'counters/examples': 95264, 'counters/updates': 2977}
train stats after 95296 examples: {'rewards_train/chosen': '0.12488', 'rewards_train/rejected': '0.039586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085293', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-141', 'loss/train': '0.66303', 'examples_per_second': '31.629', 'grad_norm': '90.5', 'counters/examples': 95296, 'counters/updates': 2978}
train stats after 95328 examples: {'rewards_train/chosen': '0.049783', 'rewards_train/rejected': '0.038202', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011581', 'logps_train/rejected': '-161.25', 'logps_train/chosen': '-169.03', 'loss/train': '0.69674', 'examples_per_second': '32.392', 'grad_norm': '66.5', 'counters/examples': 95328, 'counters/updates': 2979}
train stats after 95360 examples: {'rewards_train/chosen': '0.018408', 'rewards_train/rejected': '0.00026732', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018141', 'logps_train/rejected': '-155.51', 'logps_train/chosen': '-126.12', 'loss/train': '0.69505', 'examples_per_second': '31.6', 'grad_norm': '103.5', 'counters/examples': 95360, 'counters/updates': 2980}
train stats after 95392 examples: {'rewards_train/chosen': '-0.0038786', 'rewards_train/rejected': '0.01067', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014548', 'logps_train/rejected': '-137.44', 'logps_train/chosen': '-150.5', 'loss/train': '0.71343', 'examples_per_second': '31.627', 'grad_norm': '109.5', 'counters/examples': 95392, 'counters/updates': 2981}
train stats after 95424 examples: {'rewards_train/chosen': '0.042565', 'rewards_train/rejected': '0.0032056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03936', 'logps_train/rejected': '-159.06', 'logps_train/chosen': '-166.35', 'loss/train': '0.68364', 'examples_per_second': '30.561', 'grad_norm': '90', 'counters/examples': 95424, 'counters/updates': 2982}
train stats after 95456 examples: {'rewards_train/chosen': '0.052148', 'rewards_train/rejected': '0.051686', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00046209', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-142.39', 'loss/train': '0.70243', 'examples_per_second': '30.087', 'grad_norm': '164', 'counters/examples': 95456, 'counters/updates': 2983}
train stats after 95488 examples: {'rewards_train/chosen': '0.010059', 'rewards_train/rejected': '-0.0020162', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012076', 'logps_train/rejected': '-177.37', 'logps_train/chosen': '-145.81', 'loss/train': '0.69921', 'examples_per_second': '31.646', 'grad_norm': '96', 'counters/examples': 95488, 'counters/updates': 2984}
train stats after 95520 examples: {'rewards_train/chosen': '-0.087037', 'rewards_train/rejected': '-0.060017', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027021', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-148.55', 'loss/train': '0.7317', 'examples_per_second': '33.333', 'grad_norm': '154', 'counters/examples': 95520, 'counters/updates': 2985}
train stats after 95552 examples: {'rewards_train/chosen': '0.032758', 'rewards_train/rejected': '0.042817', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010059', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-162.73', 'loss/train': '0.71834', 'examples_per_second': '31.654', 'grad_norm': '82.5', 'counters/examples': 95552, 'counters/updates': 2986}
train stats after 95584 examples: {'rewards_train/chosen': '0.096464', 'rewards_train/rejected': '0.073314', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023151', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-144.9', 'loss/train': '0.69956', 'examples_per_second': '31.102', 'grad_norm': '108.5', 'counters/examples': 95584, 'counters/updates': 2987}
train stats after 95616 examples: {'rewards_train/chosen': '0.04864', 'rewards_train/rejected': '-0.018294', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066934', 'logps_train/rejected': '-139.06', 'logps_train/chosen': '-197.23', 'loss/train': '0.67436', 'examples_per_second': '31.537', 'grad_norm': '243', 'counters/examples': 95616, 'counters/updates': 2988}
train stats after 95648 examples: {'rewards_train/chosen': '0.024066', 'rewards_train/rejected': '0.049701', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025636', 'logps_train/rejected': '-134.57', 'logps_train/chosen': '-171.62', 'loss/train': '0.71269', 'examples_per_second': '33.34', 'grad_norm': '72', 'counters/examples': 95648, 'counters/updates': 2989}
train stats after 95680 examples: {'rewards_train/chosen': '0.038516', 'rewards_train/rejected': '0.014196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02432', 'logps_train/rejected': '-130.8', 'logps_train/chosen': '-181.79', 'loss/train': '0.69739', 'examples_per_second': '24.437', 'grad_norm': '111', 'counters/examples': 95680, 'counters/updates': 2990}
train stats after 95712 examples: {'rewards_train/chosen': '0.060387', 'rewards_train/rejected': '0.012627', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.047761', 'logps_train/rejected': '-125.76', 'logps_train/chosen': '-132.47', 'loss/train': '0.68028', 'examples_per_second': '32.452', 'grad_norm': '63', 'counters/examples': 95712, 'counters/updates': 2991}
train stats after 95744 examples: {'rewards_train/chosen': '0.090432', 'rewards_train/rejected': '0.12425', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033822', 'logps_train/rejected': '-160.07', 'logps_train/chosen': '-152.14', 'loss/train': '0.72355', 'examples_per_second': '30.186', 'grad_norm': '111.5', 'counters/examples': 95744, 'counters/updates': 2992}
train stats after 95776 examples: {'rewards_train/chosen': '0.0019513', 'rewards_train/rejected': '0.048506', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046555', 'logps_train/rejected': '-129.56', 'logps_train/chosen': '-148.85', 'loss/train': '0.73743', 'examples_per_second': '24.304', 'grad_norm': '148', 'counters/examples': 95776, 'counters/updates': 2993}
train stats after 95808 examples: {'rewards_train/chosen': '0.013309', 'rewards_train/rejected': '0.039027', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025718', 'logps_train/rejected': '-158.11', 'logps_train/chosen': '-153.41', 'loss/train': '0.72117', 'examples_per_second': '30.327', 'grad_norm': '103', 'counters/examples': 95808, 'counters/updates': 2994}
skipping logging after 95840 examples to avoid logging too frequently
train stats after 95872 examples: {'rewards_train/chosen': '0.18604', 'rewards_train/rejected': '-0.0019935', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18803', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-147.9', 'loss/train': '0.61658', 'examples_per_second': '31.192', 'grad_norm': '159', 'counters/examples': 95872, 'counters/updates': 2996}
train stats after 95904 examples: {'rewards_train/chosen': '-0.05266', 'rewards_train/rejected': '0.041935', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.094596', 'logps_train/rejected': '-153.64', 'logps_train/chosen': '-172.64', 'loss/train': '0.75349', 'examples_per_second': '31.621', 'grad_norm': '141', 'counters/examples': 95904, 'counters/updates': 2997}
train stats after 95936 examples: {'rewards_train/chosen': '0.0043999', 'rewards_train/rejected': '-0.0034668', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0078667', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-112.3', 'loss/train': '0.69591', 'examples_per_second': '30.562', 'grad_norm': '54.5', 'counters/examples': 95936, 'counters/updates': 2998}
skipping logging after 95968 examples to avoid logging too frequently
train stats after 96000 examples: {'rewards_train/chosen': '0.027816', 'rewards_train/rejected': '0.053854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026038', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-172.87', 'loss/train': '0.71977', 'examples_per_second': '31.598', 'grad_norm': '85', 'counters/examples': 96000, 'counters/updates': 3000}
Running evaluation after 96000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.27it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.95it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.86it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.01it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.02it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]
eval after 96000: {'rewards_eval/chosen': '0.066455', 'rewards_eval/rejected': '0.038141', 'rewards_eval/accuracies': '0.50391', 'rewards_eval/margins': '0.028315', 'logps_eval/rejected': '-127.76', 'logps_eval/chosen': '-150.34', 'loss/eval': '0.69228'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716/step-96000...
writing checkpoint to .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716/step-96000/policy.pt...
train stats after 96032 examples: {'rewards_train/chosen': '0.019087', 'rewards_train/rejected': '-0.020606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039693', 'logps_train/rejected': '-126.44', 'logps_train/chosen': '-191.58', 'loss/train': '0.68847', 'examples_per_second': '22.629', 'grad_norm': '229', 'counters/examples': 96032, 'counters/updates': 3001}
train stats after 96064 examples: {'rewards_train/chosen': '-0.012683', 'rewards_train/rejected': '0.053515', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066197', 'logps_train/rejected': '-104.49', 'logps_train/chosen': '-161.37', 'loss/train': '0.73342', 'examples_per_second': '30.439', 'grad_norm': '48.75', 'counters/examples': 96064, 'counters/updates': 3002}
train stats after 96096 examples: {'rewards_train/chosen': '0.031736', 'rewards_train/rejected': '0.030883', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00085355', 'logps_train/rejected': '-111.55', 'logps_train/chosen': '-172.46', 'loss/train': '0.70343', 'examples_per_second': '31.825', 'grad_norm': '55.5', 'counters/examples': 96096, 'counters/updates': 3003}
train stats after 96128 examples: {'rewards_train/chosen': '0.11776', 'rewards_train/rejected': '-0.031538', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14929', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-165.95', 'loss/train': '0.64094', 'examples_per_second': '31.585', 'grad_norm': '63.25', 'counters/examples': 96128, 'counters/updates': 3004}
train stats after 96160 examples: {'rewards_train/chosen': '0.067074', 'rewards_train/rejected': '-0.036668', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10374', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-135.29', 'loss/train': '0.65782', 'examples_per_second': '29.862', 'grad_norm': '118.5', 'counters/examples': 96160, 'counters/updates': 3005}
train stats after 96192 examples: {'rewards_train/chosen': '0.018567', 'rewards_train/rejected': '0.013339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0052285', 'logps_train/rejected': '-114.29', 'logps_train/chosen': '-134.1', 'loss/train': '0.69842', 'examples_per_second': '31.81', 'grad_norm': '155', 'counters/examples': 96192, 'counters/updates': 3006}
train stats after 96224 examples: {'rewards_train/chosen': '0.052643', 'rewards_train/rejected': '-0.016792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069435', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-180.74', 'loss/train': '0.67009', 'examples_per_second': '30.612', 'grad_norm': '91', 'counters/examples': 96224, 'counters/updates': 3007}
train stats after 96256 examples: {'rewards_train/chosen': '0.057153', 'rewards_train/rejected': '0.053455', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0036983', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-174.67', 'loss/train': '0.70215', 'examples_per_second': '32.558', 'grad_norm': '63.5', 'counters/examples': 96256, 'counters/updates': 3008}
train stats after 96288 examples: {'rewards_train/chosen': '0.13655', 'rewards_train/rejected': '0.051988', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084559', 'logps_train/rejected': '-110.93', 'logps_train/chosen': '-134.47', 'loss/train': '0.66297', 'examples_per_second': '31.678', 'grad_norm': '80.5', 'counters/examples': 96288, 'counters/updates': 3009}
train stats after 96320 examples: {'rewards_train/chosen': '-0.055383', 'rewards_train/rejected': '0.012309', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.067692', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-161.92', 'loss/train': '0.74265', 'examples_per_second': '31.215', 'grad_norm': '111.5', 'counters/examples': 96320, 'counters/updates': 3010}
train stats after 96352 examples: {'rewards_train/chosen': '0.09208', 'rewards_train/rejected': '0.06117', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03091', 'logps_train/rejected': '-129.88', 'logps_train/chosen': '-152.03', 'loss/train': '0.68701', 'examples_per_second': '30.833', 'grad_norm': '92.5', 'counters/examples': 96352, 'counters/updates': 3011}
train stats after 96384 examples: {'rewards_train/chosen': '0.050154', 'rewards_train/rejected': '-0.0038395', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053994', 'logps_train/rejected': '-147.69', 'logps_train/chosen': '-194.11', 'loss/train': '0.6816', 'examples_per_second': '31.387', 'grad_norm': '73', 'counters/examples': 96384, 'counters/updates': 3012}
train stats after 96416 examples: {'rewards_train/chosen': '0.0088433', 'rewards_train/rejected': '-0.0057227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014566', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-131.14', 'loss/train': '0.69442', 'examples_per_second': '30.224', 'grad_norm': '55.75', 'counters/examples': 96416, 'counters/updates': 3013}
train stats after 96448 examples: {'rewards_train/chosen': '0.015266', 'rewards_train/rejected': '0.073011', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057745', 'logps_train/rejected': '-103', 'logps_train/chosen': '-142.56', 'loss/train': '0.73398', 'examples_per_second': '30.852', 'grad_norm': '76.5', 'counters/examples': 96448, 'counters/updates': 3014}
train stats after 96480 examples: {'rewards_train/chosen': '0.11642', 'rewards_train/rejected': '0.013343', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10308', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-198.97', 'loss/train': '0.66665', 'examples_per_second': '31.651', 'grad_norm': '78.5', 'counters/examples': 96480, 'counters/updates': 3015}
train stats after 96512 examples: {'rewards_train/chosen': '0.064651', 'rewards_train/rejected': '0.050201', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01445', 'logps_train/rejected': '-139.31', 'logps_train/chosen': '-153.43', 'loss/train': '0.69779', 'examples_per_second': '31.46', 'grad_norm': '214', 'counters/examples': 96512, 'counters/updates': 3016}
train stats after 96544 examples: {'rewards_train/chosen': '0.038911', 'rewards_train/rejected': '-0.039723', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078634', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-136.96', 'loss/train': '0.67146', 'examples_per_second': '31.744', 'grad_norm': '71', 'counters/examples': 96544, 'counters/updates': 3017}
skipping logging after 96576 examples to avoid logging too frequently
train stats after 96608 examples: {'rewards_train/chosen': '0.066728', 'rewards_train/rejected': '-0.0002116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06694', 'logps_train/rejected': '-177.43', 'logps_train/chosen': '-162.61', 'loss/train': '0.6695', 'examples_per_second': '31.51', 'grad_norm': '91.5', 'counters/examples': 96608, 'counters/updates': 3019}
train stats after 96640 examples: {'rewards_train/chosen': '0.062744', 'rewards_train/rejected': '0.016199', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046544', 'logps_train/rejected': '-175.53', 'logps_train/chosen': '-155.72', 'loss/train': '0.68387', 'examples_per_second': '31.675', 'grad_norm': '79.5', 'counters/examples': 96640, 'counters/updates': 3020}
train stats after 96672 examples: {'rewards_train/chosen': '0.064678', 'rewards_train/rejected': '-0.011933', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076612', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-165.23', 'loss/train': '0.66626', 'examples_per_second': '31.618', 'grad_norm': '56', 'counters/examples': 96672, 'counters/updates': 3021}
train stats after 96704 examples: {'rewards_train/chosen': '0.0462', 'rewards_train/rejected': '-0.012229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058429', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-133.08', 'loss/train': '0.66836', 'examples_per_second': '31.357', 'grad_norm': '85', 'counters/examples': 96704, 'counters/updates': 3022}
train stats after 96736 examples: {'rewards_train/chosen': '-0.02036', 'rewards_train/rejected': '0.046642', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.067003', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-162.32', 'loss/train': '0.74086', 'examples_per_second': '32.431', 'grad_norm': '96.5', 'counters/examples': 96736, 'counters/updates': 3023}
train stats after 96768 examples: {'rewards_train/chosen': '0.11452', 'rewards_train/rejected': '-0.021184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13571', 'logps_train/rejected': '-163.89', 'logps_train/chosen': '-166.74', 'loss/train': '0.64728', 'examples_per_second': '31.326', 'grad_norm': '69.5', 'counters/examples': 96768, 'counters/updates': 3024}
train stats after 96800 examples: {'rewards_train/chosen': '0.076006', 'rewards_train/rejected': '0.041603', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034403', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-136.87', 'loss/train': '0.68402', 'examples_per_second': '30.855', 'grad_norm': '63', 'counters/examples': 96800, 'counters/updates': 3025}
skipping logging after 96832 examples to avoid logging too frequently
train stats after 96864 examples: {'rewards_train/chosen': '0.07811', 'rewards_train/rejected': '0.010495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067615', 'logps_train/rejected': '-157.01', 'logps_train/chosen': '-160.46', 'loss/train': '0.67355', 'examples_per_second': '31.771', 'grad_norm': '89.5', 'counters/examples': 96864, 'counters/updates': 3027}
train stats after 96896 examples: {'rewards_train/chosen': '-0.014891', 'rewards_train/rejected': '0.005502', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.020393', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-160.38', 'loss/train': '0.70956', 'examples_per_second': '31.352', 'grad_norm': '114', 'counters/examples': 96896, 'counters/updates': 3028}
train stats after 96928 examples: {'rewards_train/chosen': '0.040421', 'rewards_train/rejected': '0.063921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0235', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-172.34', 'loss/train': '0.71695', 'examples_per_second': '32.51', 'grad_norm': '107', 'counters/examples': 96928, 'counters/updates': 3029}
train stats after 96960 examples: {'rewards_train/chosen': '0.035701', 'rewards_train/rejected': '0.070962', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035261', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-137.73', 'loss/train': '0.72549', 'examples_per_second': '32.552', 'grad_norm': '178', 'counters/examples': 96960, 'counters/updates': 3030}
train stats after 96992 examples: {'rewards_train/chosen': '0.046314', 'rewards_train/rejected': '-0.0088658', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05518', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-160.6', 'loss/train': '0.68012', 'examples_per_second': '31.289', 'grad_norm': '236', 'counters/examples': 96992, 'counters/updates': 3031}
train stats after 97024 examples: {'rewards_train/chosen': '0.020035', 'rewards_train/rejected': '0.017015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0030196', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-125.09', 'loss/train': '0.69674', 'examples_per_second': '31.561', 'grad_norm': '109.5', 'counters/examples': 97024, 'counters/updates': 3032}
train stats after 97056 examples: {'rewards_train/chosen': '0.046405', 'rewards_train/rejected': '0.034802', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011603', 'logps_train/rejected': '-109.23', 'logps_train/chosen': '-141.17', 'loss/train': '0.69455', 'examples_per_second': '31.598', 'grad_norm': '53.5', 'counters/examples': 97056, 'counters/updates': 3033}
train stats after 97088 examples: {'rewards_train/chosen': '0.078553', 'rewards_train/rejected': '0.033557', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044996', 'logps_train/rejected': '-117.7', 'logps_train/chosen': '-147.72', 'loss/train': '0.67982', 'examples_per_second': '31.364', 'grad_norm': '69', 'counters/examples': 97088, 'counters/updates': 3034}
train stats after 97120 examples: {'rewards_train/chosen': '0.085069', 'rewards_train/rejected': '-0.0069617', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092031', 'logps_train/rejected': '-160.05', 'logps_train/chosen': '-144.42', 'loss/train': '0.66021', 'examples_per_second': '33.145', 'grad_norm': '71', 'counters/examples': 97120, 'counters/updates': 3035}
train stats after 97152 examples: {'rewards_train/chosen': '0.068425', 'rewards_train/rejected': '0.0070867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061338', 'logps_train/rejected': '-190.85', 'logps_train/chosen': '-127.94', 'loss/train': '0.67749', 'examples_per_second': '31.277', 'grad_norm': '168', 'counters/examples': 97152, 'counters/updates': 3036}
train stats after 97184 examples: {'rewards_train/chosen': '0.090909', 'rewards_train/rejected': '0.014547', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076361', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-140.91', 'loss/train': '0.6663', 'examples_per_second': '30.858', 'grad_norm': '53', 'counters/examples': 97184, 'counters/updates': 3037}
train stats after 97216 examples: {'rewards_train/chosen': '0.017775', 'rewards_train/rejected': '0.0045173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013258', 'logps_train/rejected': '-136.02', 'logps_train/chosen': '-152.02', 'loss/train': '0.69521', 'examples_per_second': '33.154', 'grad_norm': '59', 'counters/examples': 97216, 'counters/updates': 3038}
train stats after 97248 examples: {'rewards_train/chosen': '0.075551', 'rewards_train/rejected': '0.084991', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0094397', 'logps_train/rejected': '-151.77', 'logps_train/chosen': '-120.73', 'loss/train': '0.70984', 'examples_per_second': '31.379', 'grad_norm': '108.5', 'counters/examples': 97248, 'counters/updates': 3039}
train stats after 97280 examples: {'rewards_train/chosen': '-0.012305', 'rewards_train/rejected': '0.048299', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.060604', 'logps_train/rejected': '-148.97', 'logps_train/chosen': '-135.14', 'loss/train': '0.73761', 'examples_per_second': '30.142', 'grad_norm': '113', 'counters/examples': 97280, 'counters/updates': 3040}
train stats after 97312 examples: {'rewards_train/chosen': '0.017237', 'rewards_train/rejected': '0.06863', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.051393', 'logps_train/rejected': '-160.42', 'logps_train/chosen': '-175.17', 'loss/train': '0.73648', 'examples_per_second': '31.602', 'grad_norm': '134', 'counters/examples': 97312, 'counters/updates': 3041}
train stats after 97344 examples: {'rewards_train/chosen': '-0.014909', 'rewards_train/rejected': '0.0060028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020912', 'logps_train/rejected': '-106.61', 'logps_train/chosen': '-154.24', 'loss/train': '0.71548', 'examples_per_second': '31.03', 'grad_norm': '74.5', 'counters/examples': 97344, 'counters/updates': 3042}
train stats after 97376 examples: {'rewards_train/chosen': '0.047584', 'rewards_train/rejected': '-0.021373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068957', 'logps_train/rejected': '-98.472', 'logps_train/chosen': '-149.22', 'loss/train': '0.67201', 'examples_per_second': '31.416', 'grad_norm': '74.5', 'counters/examples': 97376, 'counters/updates': 3043}
train stats after 97408 examples: {'rewards_train/chosen': '0.0060789', 'rewards_train/rejected': '0.011935', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0058561', 'logps_train/rejected': '-156.66', 'logps_train/chosen': '-138.24', 'loss/train': '0.70802', 'examples_per_second': '30.626', 'grad_norm': '195', 'counters/examples': 97408, 'counters/updates': 3044}
train stats after 97440 examples: {'rewards_train/chosen': '0.067206', 'rewards_train/rejected': '0.079143', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011937', 'logps_train/rejected': '-201.9', 'logps_train/chosen': '-145.98', 'loss/train': '0.722', 'examples_per_second': '30.624', 'grad_norm': '158', 'counters/examples': 97440, 'counters/updates': 3045}
train stats after 97472 examples: {'rewards_train/chosen': '0.051915', 'rewards_train/rejected': '0.064763', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012848', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-145.28', 'loss/train': '0.71217', 'examples_per_second': '31.771', 'grad_norm': '147', 'counters/examples': 97472, 'counters/updates': 3046}
train stats after 97504 examples: {'rewards_train/chosen': '0.061247', 'rewards_train/rejected': '0.024458', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036789', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-167.96', 'loss/train': '0.68415', 'examples_per_second': '30.927', 'grad_norm': '113', 'counters/examples': 97504, 'counters/updates': 3047}
train stats after 97536 examples: {'rewards_train/chosen': '0.026092', 'rewards_train/rejected': '-0.00044991', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026542', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-154.8', 'loss/train': '0.69031', 'examples_per_second': '31.572', 'grad_norm': '121', 'counters/examples': 97536, 'counters/updates': 3048}
train stats after 97568 examples: {'rewards_train/chosen': '0.094194', 'rewards_train/rejected': '0.008186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086008', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-133.94', 'loss/train': '0.65792', 'examples_per_second': '31.494', 'grad_norm': '53.75', 'counters/examples': 97568, 'counters/updates': 3049}
train stats after 97600 examples: {'rewards_train/chosen': '0.10343', 'rewards_train/rejected': '0.042574', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060857', 'logps_train/rejected': '-156.81', 'logps_train/chosen': '-169.93', 'loss/train': '0.67392', 'examples_per_second': '31.46', 'grad_norm': '86.5', 'counters/examples': 97600, 'counters/updates': 3050}
train stats after 97632 examples: {'rewards_train/chosen': '0.098023', 'rewards_train/rejected': '0.056811', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041212', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-142.48', 'loss/train': '0.68594', 'examples_per_second': '24.469', 'grad_norm': '63.25', 'counters/examples': 97632, 'counters/updates': 3051}
train stats after 97664 examples: {'rewards_train/chosen': '0.062284', 'rewards_train/rejected': '-0.0075612', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069845', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-141.6', 'loss/train': '0.66765', 'examples_per_second': '31.378', 'grad_norm': '53.75', 'counters/examples': 97664, 'counters/updates': 3052}
train stats after 97696 examples: {'rewards_train/chosen': '0.033927', 'rewards_train/rejected': '0.029882', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0040449', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-167.75', 'loss/train': '0.70162', 'examples_per_second': '31.605', 'grad_norm': '107', 'counters/examples': 97696, 'counters/updates': 3053}
train stats after 97728 examples: {'rewards_train/chosen': '0.12956', 'rewards_train/rejected': '-0.046032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17559', 'logps_train/rejected': '-114.89', 'logps_train/chosen': '-173.88', 'loss/train': '0.62399', 'examples_per_second': '31.603', 'grad_norm': '73', 'counters/examples': 97728, 'counters/updates': 3054}
skipping logging after 97760 examples to avoid logging too frequently
train stats after 97792 examples: {'rewards_train/chosen': '0.025491', 'rewards_train/rejected': '0.015587', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0099037', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-180.95', 'loss/train': '0.70041', 'examples_per_second': '30.981', 'grad_norm': '161', 'counters/examples': 97792, 'counters/updates': 3056}
skipping logging after 97824 examples to avoid logging too frequently
train stats after 97856 examples: {'rewards_train/chosen': '0.034357', 'rewards_train/rejected': '-0.087393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12175', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-180.9', 'loss/train': '0.64785', 'examples_per_second': '31.416', 'grad_norm': '155', 'counters/examples': 97856, 'counters/updates': 3058}
skipping logging after 97888 examples to avoid logging too frequently
train stats after 97920 examples: {'rewards_train/chosen': '0.014493', 'rewards_train/rejected': '0.01324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0012524', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-139.31', 'loss/train': '0.7026', 'examples_per_second': '32.452', 'grad_norm': '233', 'counters/examples': 97920, 'counters/updates': 3060}
train stats after 97952 examples: {'rewards_train/chosen': '0.10562', 'rewards_train/rejected': '0.029173', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076442', 'logps_train/rejected': '-111.18', 'logps_train/chosen': '-110.71', 'loss/train': '0.67651', 'examples_per_second': '32.298', 'grad_norm': '282', 'counters/examples': 97952, 'counters/updates': 3061}
train stats after 97984 examples: {'rewards_train/chosen': '0.022638', 'rewards_train/rejected': '0.025751', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0031125', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-115.79', 'loss/train': '0.69954', 'examples_per_second': '31.66', 'grad_norm': '65.5', 'counters/examples': 97984, 'counters/updates': 3062}
train stats after 98016 examples: {'rewards_train/chosen': '0.076495', 'rewards_train/rejected': '0.077513', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0010175', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-167.85', 'loss/train': '0.71335', 'examples_per_second': '30.616', 'grad_norm': '119', 'counters/examples': 98016, 'counters/updates': 3063}
train stats after 98048 examples: {'rewards_train/chosen': '0.053077', 'rewards_train/rejected': '-0.0048451', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.057922', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-130.39', 'loss/train': '0.68216', 'examples_per_second': '32.016', 'grad_norm': '67.5', 'counters/examples': 98048, 'counters/updates': 3064}
train stats after 98080 examples: {'rewards_train/chosen': '0.042852', 'rewards_train/rejected': '0.076038', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.033186', 'logps_train/rejected': '-162.51', 'logps_train/chosen': '-145.14', 'loss/train': '0.71782', 'examples_per_second': '30.257', 'grad_norm': '92', 'counters/examples': 98080, 'counters/updates': 3065}
train stats after 98112 examples: {'rewards_train/chosen': '0.035189', 'rewards_train/rejected': '-0.040586', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.075775', 'logps_train/rejected': '-120.71', 'logps_train/chosen': '-177', 'loss/train': '0.66897', 'examples_per_second': '31.321', 'grad_norm': '114.5', 'counters/examples': 98112, 'counters/updates': 3066}
train stats after 98144 examples: {'rewards_train/chosen': '0.036158', 'rewards_train/rejected': '0.055883', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019725', 'logps_train/rejected': '-148.21', 'logps_train/chosen': '-185.93', 'loss/train': '0.72155', 'examples_per_second': '30.119', 'grad_norm': '100', 'counters/examples': 98144, 'counters/updates': 3067}
train stats after 98176 examples: {'rewards_train/chosen': '0.092496', 'rewards_train/rejected': '0.018472', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074024', 'logps_train/rejected': '-102.27', 'logps_train/chosen': '-146.12', 'loss/train': '0.66625', 'examples_per_second': '31.643', 'grad_norm': '88.5', 'counters/examples': 98176, 'counters/updates': 3068}
train stats after 98208 examples: {'rewards_train/chosen': '-0.017476', 'rewards_train/rejected': '0.050183', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.067658', 'logps_train/rejected': '-147.41', 'logps_train/chosen': '-129.58', 'loss/train': '0.73455', 'examples_per_second': '31.625', 'grad_norm': '68.5', 'counters/examples': 98208, 'counters/updates': 3069}
train stats after 98240 examples: {'rewards_train/chosen': '0.063426', 'rewards_train/rejected': '-0.033387', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096813', 'logps_train/rejected': '-115.3', 'logps_train/chosen': '-138.6', 'loss/train': '0.6579', 'examples_per_second': '30.758', 'grad_norm': '60.25', 'counters/examples': 98240, 'counters/updates': 3070}
train stats after 98272 examples: {'rewards_train/chosen': '0.054251', 'rewards_train/rejected': '0.084759', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030508', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-110.57', 'loss/train': '0.72056', 'examples_per_second': '32.35', 'grad_norm': '102', 'counters/examples': 98272, 'counters/updates': 3071}
train stats after 98304 examples: {'rewards_train/chosen': '0.030418', 'rewards_train/rejected': '0.028292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0021261', 'logps_train/rejected': '-172.43', 'logps_train/chosen': '-131.46', 'loss/train': '0.70281', 'examples_per_second': '31.563', 'grad_norm': '89', 'counters/examples': 98304, 'counters/updates': 3072}
train stats after 98336 examples: {'rewards_train/chosen': '-0.040772', 'rewards_train/rejected': '-0.019835', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020936', 'logps_train/rejected': '-110.73', 'logps_train/chosen': '-182.31', 'loss/train': '0.71295', 'examples_per_second': '31.526', 'grad_norm': '87.5', 'counters/examples': 98336, 'counters/updates': 3073}
train stats after 98368 examples: {'rewards_train/chosen': '0.033924', 'rewards_train/rejected': '0.10961', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.07569', 'logps_train/rejected': '-152.52', 'logps_train/chosen': '-187.47', 'loss/train': '0.7487', 'examples_per_second': '31.932', 'grad_norm': '217', 'counters/examples': 98368, 'counters/updates': 3074}
train stats after 98400 examples: {'rewards_train/chosen': '0.019825', 'rewards_train/rejected': '-0.014596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034422', 'logps_train/rejected': '-171.4', 'logps_train/chosen': '-156.62', 'loss/train': '0.69088', 'examples_per_second': '32.884', 'grad_norm': '174', 'counters/examples': 98400, 'counters/updates': 3075}
train stats after 98432 examples: {'rewards_train/chosen': '0.10344', 'rewards_train/rejected': '-0.013126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11656', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-179.34', 'loss/train': '0.65623', 'examples_per_second': '31.6', 'grad_norm': '126', 'counters/examples': 98432, 'counters/updates': 3076}
skipping logging after 98464 examples to avoid logging too frequently
train stats after 98496 examples: {'rewards_train/chosen': '0.083541', 'rewards_train/rejected': '0.052625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030916', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-159.19', 'loss/train': '0.68883', 'examples_per_second': '30.12', 'grad_norm': '85.5', 'counters/examples': 98496, 'counters/updates': 3078}
train stats after 98528 examples: {'rewards_train/chosen': '0.014487', 'rewards_train/rejected': '0.013876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00061092', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-164.7', 'loss/train': '0.69959', 'examples_per_second': '30.103', 'grad_norm': '76', 'counters/examples': 98528, 'counters/updates': 3079}
train stats after 98560 examples: {'rewards_train/chosen': '0.099319', 'rewards_train/rejected': '0.043553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055767', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-161.14', 'loss/train': '0.68649', 'examples_per_second': '31.598', 'grad_norm': '126.5', 'counters/examples': 98560, 'counters/updates': 3080}
skipping logging after 98592 examples to avoid logging too frequently
train stats after 98624 examples: {'rewards_train/chosen': '0.10516', 'rewards_train/rejected': '-0.038223', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14339', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-153.76', 'loss/train': '0.63688', 'examples_per_second': '31.065', 'grad_norm': '68', 'counters/examples': 98624, 'counters/updates': 3082}
train stats after 98656 examples: {'rewards_train/chosen': '0.037971', 'rewards_train/rejected': '-0.013008', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05098', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-155.57', 'loss/train': '0.67986', 'examples_per_second': '30.823', 'grad_norm': '61', 'counters/examples': 98656, 'counters/updates': 3083}
train stats after 98688 examples: {'rewards_train/chosen': '0.063791', 'rewards_train/rejected': '0.065251', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00146', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-148.44', 'loss/train': '0.70074', 'examples_per_second': '31.325', 'grad_norm': '156', 'counters/examples': 98688, 'counters/updates': 3084}
train stats after 98720 examples: {'rewards_train/chosen': '0.0008317', 'rewards_train/rejected': '-0.034794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035626', 'logps_train/rejected': '-130.93', 'logps_train/chosen': '-159.91', 'loss/train': '0.68617', 'examples_per_second': '30.657', 'grad_norm': '192', 'counters/examples': 98720, 'counters/updates': 3085}
train stats after 98752 examples: {'rewards_train/chosen': '0.010911', 'rewards_train/rejected': '0.048302', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.037392', 'logps_train/rejected': '-165.35', 'logps_train/chosen': '-141.35', 'loss/train': '0.72584', 'examples_per_second': '31.656', 'grad_norm': '118.5', 'counters/examples': 98752, 'counters/updates': 3086}
skipping logging after 98784 examples to avoid logging too frequently
train stats after 98816 examples: {'rewards_train/chosen': '0.048359', 'rewards_train/rejected': '0.093282', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.044923', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-134.44', 'loss/train': '0.73918', 'examples_per_second': '31.696', 'grad_norm': '207', 'counters/examples': 98816, 'counters/updates': 3088}
train stats after 98848 examples: {'rewards_train/chosen': '0.19113', 'rewards_train/rejected': '0.093359', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097774', 'logps_train/rejected': '-183.58', 'logps_train/chosen': '-178.75', 'loss/train': '0.66761', 'examples_per_second': '31.623', 'grad_norm': '158', 'counters/examples': 98848, 'counters/updates': 3089}
train stats after 98880 examples: {'rewards_train/chosen': '0.18264', 'rewards_train/rejected': '0.045937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1367', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-141.07', 'loss/train': '0.63527', 'examples_per_second': '31.63', 'grad_norm': '71', 'counters/examples': 98880, 'counters/updates': 3090}
train stats after 98912 examples: {'rewards_train/chosen': '-0.015063', 'rewards_train/rejected': '0.15115', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.16621', 'logps_train/rejected': '-150.36', 'logps_train/chosen': '-168.84', 'loss/train': '0.79383', 'examples_per_second': '31.609', 'grad_norm': '125.5', 'counters/examples': 98912, 'counters/updates': 3091}
skipping logging after 98944 examples to avoid logging too frequently
train stats after 98976 examples: {'rewards_train/chosen': '0.078416', 'rewards_train/rejected': '0.11192', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033508', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-151.82', 'loss/train': '0.72668', 'examples_per_second': '34.032', 'grad_norm': '97', 'counters/examples': 98976, 'counters/updates': 3093}
train stats after 99008 examples: {'rewards_train/chosen': '-0.035323', 'rewards_train/rejected': '-0.033249', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0020736', 'logps_train/rejected': '-141.22', 'logps_train/chosen': '-143.81', 'loss/train': '0.70175', 'examples_per_second': '30.477', 'grad_norm': '163', 'counters/examples': 99008, 'counters/updates': 3094}
train stats after 99040 examples: {'rewards_train/chosen': '0.053116', 'rewards_train/rejected': '0.049739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0033767', 'logps_train/rejected': '-149.97', 'logps_train/chosen': '-130.19', 'loss/train': '0.70125', 'examples_per_second': '31.649', 'grad_norm': '69', 'counters/examples': 99040, 'counters/updates': 3095}
train stats after 99072 examples: {'rewards_train/chosen': '0.0991', 'rewards_train/rejected': '-0.018045', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11714', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-144.89', 'loss/train': '0.65142', 'examples_per_second': '32.569', 'grad_norm': '65', 'counters/examples': 99072, 'counters/updates': 3096}
train stats after 99104 examples: {'rewards_train/chosen': '0.031787', 'rewards_train/rejected': '0.014498', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.017288', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-151.35', 'loss/train': '0.69276', 'examples_per_second': '30.482', 'grad_norm': '71', 'counters/examples': 99104, 'counters/updates': 3097}
skipping logging after 99136 examples to avoid logging too frequently
train stats after 99168 examples: {'rewards_train/chosen': '0.087076', 'rewards_train/rejected': '0.075671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011406', 'logps_train/rejected': '-112.59', 'logps_train/chosen': '-129.95', 'loss/train': '0.70011', 'examples_per_second': '32.625', 'grad_norm': '52', 'counters/examples': 99168, 'counters/updates': 3099}
train stats after 99200 examples: {'rewards_train/chosen': '0.0029493', 'rewards_train/rejected': '-0.0066479', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0095972', 'logps_train/rejected': '-153.59', 'logps_train/chosen': '-144.07', 'loss/train': '0.69705', 'examples_per_second': '30.681', 'grad_norm': '68', 'counters/examples': 99200, 'counters/updates': 3100}
train stats after 99232 examples: {'rewards_train/chosen': '0.14312', 'rewards_train/rejected': '0.021281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12184', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-149.62', 'loss/train': '0.64283', 'examples_per_second': '32.537', 'grad_norm': '151', 'counters/examples': 99232, 'counters/updates': 3101}
train stats after 99264 examples: {'rewards_train/chosen': '0.027249', 'rewards_train/rejected': '-0.017454', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044703', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-157.5', 'loss/train': '0.68754', 'examples_per_second': '30.935', 'grad_norm': '121', 'counters/examples': 99264, 'counters/updates': 3102}
train stats after 99296 examples: {'rewards_train/chosen': '-0.016466', 'rewards_train/rejected': '0.056313', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.072779', 'logps_train/rejected': '-150.02', 'logps_train/chosen': '-132.92', 'loss/train': '0.7493', 'examples_per_second': '31.604', 'grad_norm': '105', 'counters/examples': 99296, 'counters/updates': 3103}
train stats after 99328 examples: {'rewards_train/chosen': '0.10347', 'rewards_train/rejected': '0.021434', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08204', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-165.33', 'loss/train': '0.66461', 'examples_per_second': '30.479', 'grad_norm': '75.5', 'counters/examples': 99328, 'counters/updates': 3104}
train stats after 99360 examples: {'rewards_train/chosen': '0.012468', 'rewards_train/rejected': '-0.052108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064576', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-127.83', 'loss/train': '0.66772', 'examples_per_second': '32.726', 'grad_norm': '76', 'counters/examples': 99360, 'counters/updates': 3105}
train stats after 99392 examples: {'rewards_train/chosen': '0.013972', 'rewards_train/rejected': '0.015097', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011246', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-156.2', 'loss/train': '0.70685', 'examples_per_second': '30.012', 'grad_norm': '230', 'counters/examples': 99392, 'counters/updates': 3106}
skipping logging after 99424 examples to avoid logging too frequently
train stats after 99456 examples: {'rewards_train/chosen': '0.064908', 'rewards_train/rejected': '0.041522', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023386', 'logps_train/rejected': '-141.07', 'logps_train/chosen': '-147.87', 'loss/train': '0.70853', 'examples_per_second': '32.689', 'grad_norm': '66.5', 'counters/examples': 99456, 'counters/updates': 3108}
skipping logging after 99488 examples to avoid logging too frequently
train stats after 99520 examples: {'rewards_train/chosen': '0.033943', 'rewards_train/rejected': '0.031133', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0028096', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-141.98', 'loss/train': '0.71166', 'examples_per_second': '32.434', 'grad_norm': '214', 'counters/examples': 99520, 'counters/updates': 3110}
train stats after 99552 examples: {'rewards_train/chosen': '0.02352', 'rewards_train/rejected': '0.047557', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024037', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-136.35', 'loss/train': '0.71334', 'examples_per_second': '32.801', 'grad_norm': '57.75', 'counters/examples': 99552, 'counters/updates': 3111}
train stats after 99584 examples: {'rewards_train/chosen': '0.099156', 'rewards_train/rejected': '0.055211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043945', 'logps_train/rejected': '-150.99', 'logps_train/chosen': '-173.9', 'loss/train': '0.69165', 'examples_per_second': '31.786', 'grad_norm': '77.5', 'counters/examples': 99584, 'counters/updates': 3112}
skipping logging after 99616 examples to avoid logging too frequently
train stats after 99648 examples: {'rewards_train/chosen': '0.030687', 'rewards_train/rejected': '0.064781', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034094', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-136.53', 'loss/train': '0.71666', 'examples_per_second': '32.625', 'grad_norm': '82', 'counters/examples': 99648, 'counters/updates': 3114}
train stats after 99680 examples: {'rewards_train/chosen': '0.054727', 'rewards_train/rejected': '0.037693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017033', 'logps_train/rejected': '-109.98', 'logps_train/chosen': '-133.76', 'loss/train': '0.69441', 'examples_per_second': '31.657', 'grad_norm': '96.5', 'counters/examples': 99680, 'counters/updates': 3115}
skipping logging after 99712 examples to avoid logging too frequently
train stats after 99744 examples: {'rewards_train/chosen': '0.0322', 'rewards_train/rejected': '0.044441', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01224', 'logps_train/rejected': '-98.149', 'logps_train/chosen': '-121.74', 'loss/train': '0.70879', 'examples_per_second': '31.555', 'grad_norm': '85.5', 'counters/examples': 99744, 'counters/updates': 3117}
train stats after 99776 examples: {'rewards_train/chosen': '0.038738', 'rewards_train/rejected': '-0.021464', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060201', 'logps_train/rejected': '-159.77', 'logps_train/chosen': '-124.73', 'loss/train': '0.67215', 'examples_per_second': '30.736', 'grad_norm': '204', 'counters/examples': 99776, 'counters/updates': 3118}
skipping logging after 99808 examples to avoid logging too frequently
train stats after 99840 examples: {'rewards_train/chosen': '0.050683', 'rewards_train/rejected': '0.028802', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021881', 'logps_train/rejected': '-176.76', 'logps_train/chosen': '-194.15', 'loss/train': '0.6999', 'examples_per_second': '30.182', 'grad_norm': '90', 'counters/examples': 99840, 'counters/updates': 3120}
train stats after 99872 examples: {'rewards_train/chosen': '0.04277', 'rewards_train/rejected': '-0.008322', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051092', 'logps_train/rejected': '-152.76', 'logps_train/chosen': '-146.08', 'loss/train': '0.67538', 'examples_per_second': '30.653', 'grad_norm': '92', 'counters/examples': 99872, 'counters/updates': 3121}
train stats after 99904 examples: {'rewards_train/chosen': '0.082636', 'rewards_train/rejected': '0.11258', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.029948', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-190.31', 'loss/train': '0.72216', 'examples_per_second': '31.649', 'grad_norm': '88', 'counters/examples': 99904, 'counters/updates': 3122}
train stats after 99936 examples: {'rewards_train/chosen': '0.18579', 'rewards_train/rejected': '0.054201', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13158', 'logps_train/rejected': '-181.82', 'logps_train/chosen': '-200.9', 'loss/train': '0.65513', 'examples_per_second': '32.338', 'grad_norm': '238', 'counters/examples': 99936, 'counters/updates': 3123}
skipping logging after 99968 examples to avoid logging too frequently
train stats after 100000 examples: {'rewards_train/chosen': '0.015625', 'rewards_train/rejected': '0.045754', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.03013', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-160.43', 'loss/train': '0.73349', 'examples_per_second': '31.674', 'grad_norm': '164', 'counters/examples': 100000, 'counters/updates': 3125}
train stats after 100032 examples: {'rewards_train/chosen': '0.099958', 'rewards_train/rejected': '0.020675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079283', 'logps_train/rejected': '-116.29', 'logps_train/chosen': '-134.7', 'loss/train': '0.66502', 'examples_per_second': '32.894', 'grad_norm': '67.5', 'counters/examples': 100032, 'counters/updates': 3126}
skipping logging after 100064 examples to avoid logging too frequently
train stats after 100096 examples: {'rewards_train/chosen': '0.11726', 'rewards_train/rejected': '0.012903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10436', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-138.38', 'loss/train': '0.66242', 'examples_per_second': '33.829', 'grad_norm': '69.5', 'counters/examples': 100096, 'counters/updates': 3128}
skipping logging after 100128 examples to avoid logging too frequently
train stats after 100160 examples: {'rewards_train/chosen': '0.063469', 'rewards_train/rejected': '-0.0022789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065748', 'logps_train/rejected': '-139.06', 'logps_train/chosen': '-150.14', 'loss/train': '0.6675', 'examples_per_second': '31.192', 'grad_norm': '129', 'counters/examples': 100160, 'counters/updates': 3130}
train stats after 100192 examples: {'rewards_train/chosen': '-0.025977', 'rewards_train/rejected': '0.062911', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.088888', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-121.35', 'loss/train': '0.75041', 'examples_per_second': '31.984', 'grad_norm': '74.5', 'counters/examples': 100192, 'counters/updates': 3131}
skipping logging after 100224 examples to avoid logging too frequently
train stats after 100256 examples: {'rewards_train/chosen': '0.080559', 'rewards_train/rejected': '0.052367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028192', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-143.29', 'loss/train': '0.68756', 'examples_per_second': '31.616', 'grad_norm': '103.5', 'counters/examples': 100256, 'counters/updates': 3133}
skipping logging after 100288 examples to avoid logging too frequently
train stats after 100320 examples: {'rewards_train/chosen': '0.055455', 'rewards_train/rejected': '0.058854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0033992', 'logps_train/rejected': '-138.46', 'logps_train/chosen': '-129.8', 'loss/train': '0.70726', 'examples_per_second': '31.726', 'grad_norm': '76', 'counters/examples': 100320, 'counters/updates': 3135}
train stats after 100352 examples: {'rewards_train/chosen': '0.056718', 'rewards_train/rejected': '-0.0029836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059702', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-141.78', 'loss/train': '0.6721', 'examples_per_second': '31.26', 'grad_norm': '256', 'counters/examples': 100352, 'counters/updates': 3136}
train stats after 100384 examples: {'rewards_train/chosen': '0.055192', 'rewards_train/rejected': '0.028874', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026318', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-158.3', 'loss/train': '0.68955', 'examples_per_second': '30.363', 'grad_norm': '99', 'counters/examples': 100384, 'counters/updates': 3137}
train stats after 100416 examples: {'rewards_train/chosen': '0.12624', 'rewards_train/rejected': '0.053349', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072891', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-124.94', 'loss/train': '0.66814', 'examples_per_second': '32.951', 'grad_norm': '72', 'counters/examples': 100416, 'counters/updates': 3138}
train stats after 100448 examples: {'rewards_train/chosen': '0.028708', 'rewards_train/rejected': '0.053675', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024966', 'logps_train/rejected': '-163.01', 'logps_train/chosen': '-154.42', 'loss/train': '0.7135', 'examples_per_second': '31.345', 'grad_norm': '197', 'counters/examples': 100448, 'counters/updates': 3139}
train stats after 100480 examples: {'rewards_train/chosen': '0.02361', 'rewards_train/rejected': '0.014469', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0091405', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-122.1', 'loss/train': '0.70415', 'examples_per_second': '30.205', 'grad_norm': '104', 'counters/examples': 100480, 'counters/updates': 3140}
train stats after 100512 examples: {'rewards_train/chosen': '0.065107', 'rewards_train/rejected': '0.032319', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032787', 'logps_train/rejected': '-122.09', 'logps_train/chosen': '-141.46', 'loss/train': '0.68808', 'examples_per_second': '32.115', 'grad_norm': '83.5', 'counters/examples': 100512, 'counters/updates': 3141}
train stats after 100544 examples: {'rewards_train/chosen': '0.03836', 'rewards_train/rejected': '-0.017029', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055389', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-152.46', 'loss/train': '0.67665', 'examples_per_second': '30.956', 'grad_norm': '88.5', 'counters/examples': 100544, 'counters/updates': 3142}
train stats after 100576 examples: {'rewards_train/chosen': '0.070834', 'rewards_train/rejected': '0.045936', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024899', 'logps_train/rejected': '-152.16', 'logps_train/chosen': '-171.04', 'loss/train': '0.69338', 'examples_per_second': '33.104', 'grad_norm': '115.5', 'counters/examples': 100576, 'counters/updates': 3143}
train stats after 100608 examples: {'rewards_train/chosen': '0.056424', 'rewards_train/rejected': '0.010151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046273', 'logps_train/rejected': '-152.54', 'logps_train/chosen': '-183.95', 'loss/train': '0.67678', 'examples_per_second': '30.598', 'grad_norm': '80', 'counters/examples': 100608, 'counters/updates': 3144}
train stats after 100640 examples: {'rewards_train/chosen': '0.069528', 'rewards_train/rejected': '0.014073', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055455', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-185.9', 'loss/train': '0.68886', 'examples_per_second': '31.635', 'grad_norm': '69.5', 'counters/examples': 100640, 'counters/updates': 3145}
skipping logging after 100672 examples to avoid logging too frequently
train stats after 100704 examples: {'rewards_train/chosen': '-0.0078112', 'rewards_train/rejected': '0.032215', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040026', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-165.19', 'loss/train': '0.73258', 'examples_per_second': '30.251', 'grad_norm': '165', 'counters/examples': 100704, 'counters/updates': 3147}
train stats after 100736 examples: {'rewards_train/chosen': '-0.0041175', 'rewards_train/rejected': '-0.027581', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023464', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-214.86', 'loss/train': '0.69805', 'examples_per_second': '30.627', 'grad_norm': '76.5', 'counters/examples': 100736, 'counters/updates': 3148}
train stats after 100768 examples: {'rewards_train/chosen': '0.010509', 'rewards_train/rejected': '0.0019765', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0085327', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-129.36', 'loss/train': '0.70352', 'examples_per_second': '32.282', 'grad_norm': '67', 'counters/examples': 100768, 'counters/updates': 3149}
train stats after 100800 examples: {'rewards_train/chosen': '0.10963', 'rewards_train/rejected': '-0.013614', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12325', 'logps_train/rejected': '-141.29', 'logps_train/chosen': '-153.82', 'loss/train': '0.64564', 'examples_per_second': '31.563', 'grad_norm': '78', 'counters/examples': 100800, 'counters/updates': 3150}
train stats after 100832 examples: {'rewards_train/chosen': '0.0031749', 'rewards_train/rejected': '-0.001635', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0048099', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-210.11', 'loss/train': '0.70253', 'examples_per_second': '30.607', 'grad_norm': '201', 'counters/examples': 100832, 'counters/updates': 3151}
train stats after 100864 examples: {'rewards_train/chosen': '0.018975', 'rewards_train/rejected': '0.022662', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0036876', 'logps_train/rejected': '-96.963', 'logps_train/chosen': '-140', 'loss/train': '0.70955', 'examples_per_second': '31.155', 'grad_norm': '80.5', 'counters/examples': 100864, 'counters/updates': 3152}
train stats after 100896 examples: {'rewards_train/chosen': '0.10796', 'rewards_train/rejected': '0.033778', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074181', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-152.71', 'loss/train': '0.66672', 'examples_per_second': '30.454', 'grad_norm': '90', 'counters/examples': 100896, 'counters/updates': 3153}
train stats after 100928 examples: {'rewards_train/chosen': '0.064103', 'rewards_train/rejected': '0.07792', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013818', 'logps_train/rejected': '-164.38', 'logps_train/chosen': '-121.62', 'loss/train': '0.70961', 'examples_per_second': '31.085', 'grad_norm': '78', 'counters/examples': 100928, 'counters/updates': 3154}
train stats after 100960 examples: {'rewards_train/chosen': '0.022821', 'rewards_train/rejected': '0.042513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019692', 'logps_train/rejected': '-134.32', 'logps_train/chosen': '-167.83', 'loss/train': '0.71608', 'examples_per_second': '30.42', 'grad_norm': '80', 'counters/examples': 100960, 'counters/updates': 3155}
train stats after 100992 examples: {'rewards_train/chosen': '0.094149', 'rewards_train/rejected': '0.054707', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039442', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-121.66', 'loss/train': '0.68173', 'examples_per_second': '31.202', 'grad_norm': '106', 'counters/examples': 100992, 'counters/updates': 3156}
train stats after 101024 examples: {'rewards_train/chosen': '0.099123', 'rewards_train/rejected': '0.049856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049266', 'logps_train/rejected': '-152.66', 'logps_train/chosen': '-148.06', 'loss/train': '0.67767', 'examples_per_second': '29.95', 'grad_norm': '61.75', 'counters/examples': 101024, 'counters/updates': 3157}
skipping logging after 101056 examples to avoid logging too frequently
train stats after 101088 examples: {'rewards_train/chosen': '0.08974', 'rewards_train/rejected': '0.064172', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '0.025568', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-142.28', 'loss/train': '0.71902', 'examples_per_second': '30.155', 'grad_norm': '170', 'counters/examples': 101088, 'counters/updates': 3159}
train stats after 101120 examples: {'rewards_train/chosen': '0.17005', 'rewards_train/rejected': '-0.036569', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20662', 'logps_train/rejected': '-160.76', 'logps_train/chosen': '-183.61', 'loss/train': '0.61024', 'examples_per_second': '31.947', 'grad_norm': '209', 'counters/examples': 101120, 'counters/updates': 3160}
train stats after 101152 examples: {'rewards_train/chosen': '0.083974', 'rewards_train/rejected': '0.023838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060136', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-147.1', 'loss/train': '0.67124', 'examples_per_second': '31.183', 'grad_norm': '126.5', 'counters/examples': 101152, 'counters/updates': 3161}
train stats after 101184 examples: {'rewards_train/chosen': '0.061552', 'rewards_train/rejected': '-0.012234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073786', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-161.01', 'loss/train': '0.67623', 'examples_per_second': '31.797', 'grad_norm': '254', 'counters/examples': 101184, 'counters/updates': 3162}
train stats after 101216 examples: {'rewards_train/chosen': '0.12568', 'rewards_train/rejected': '0.036394', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089284', 'logps_train/rejected': '-138.48', 'logps_train/chosen': '-170.67', 'loss/train': '0.67167', 'examples_per_second': '31.502', 'grad_norm': '127', 'counters/examples': 101216, 'counters/updates': 3163}
train stats after 101248 examples: {'rewards_train/chosen': '0.1124', 'rewards_train/rejected': '4.7146e-06', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11239', 'logps_train/rejected': '-118.11', 'logps_train/chosen': '-131.95', 'loss/train': '0.64776', 'examples_per_second': '33.178', 'grad_norm': '77', 'counters/examples': 101248, 'counters/updates': 3164}
train stats after 101280 examples: {'rewards_train/chosen': '0.088884', 'rewards_train/rejected': '0.010289', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078594', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-141.99', 'loss/train': '0.67836', 'examples_per_second': '31.105', 'grad_norm': '84.5', 'counters/examples': 101280, 'counters/updates': 3165}
skipping logging after 101312 examples to avoid logging too frequently
train stats after 101344 examples: {'rewards_train/chosen': '0.068959', 'rewards_train/rejected': '0.0046976', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064261', 'logps_train/rejected': '-88.318', 'logps_train/chosen': '-118.79', 'loss/train': '0.67296', 'examples_per_second': '30.258', 'grad_norm': '50.25', 'counters/examples': 101344, 'counters/updates': 3167}
skipping logging after 101376 examples to avoid logging too frequently
train stats after 101408 examples: {'rewards_train/chosen': '0.087777', 'rewards_train/rejected': '0.011768', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076008', 'logps_train/rejected': '-191.93', 'logps_train/chosen': '-168.53', 'loss/train': '0.66469', 'examples_per_second': '30.572', 'grad_norm': '102', 'counters/examples': 101408, 'counters/updates': 3169}
train stats after 101440 examples: {'rewards_train/chosen': '0.04853', 'rewards_train/rejected': '0.018054', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030476', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-125.28', 'loss/train': '0.69009', 'examples_per_second': '30.607', 'grad_norm': '89.5', 'counters/examples': 101440, 'counters/updates': 3170}
train stats after 101472 examples: {'rewards_train/chosen': '0.08342', 'rewards_train/rejected': '0.075527', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0078931', 'logps_train/rejected': '-151.11', 'logps_train/chosen': '-156.83', 'loss/train': '0.71246', 'examples_per_second': '31.604', 'grad_norm': '92.5', 'counters/examples': 101472, 'counters/updates': 3171}
train stats after 101504 examples: {'rewards_train/chosen': '0.11179', 'rewards_train/rejected': '0.1392', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027416', 'logps_train/rejected': '-154.02', 'logps_train/chosen': '-172.7', 'loss/train': '0.71975', 'examples_per_second': '31.619', 'grad_norm': '92.5', 'counters/examples': 101504, 'counters/updates': 3172}
train stats after 101536 examples: {'rewards_train/chosen': '0.037017', 'rewards_train/rejected': '-0.016298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053315', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-128.59', 'loss/train': '0.67147', 'examples_per_second': '30.343', 'grad_norm': '95.5', 'counters/examples': 101536, 'counters/updates': 3173}
train stats after 101568 examples: {'rewards_train/chosen': '0.033055', 'rewards_train/rejected': '-0.0046736', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037728', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-163.12', 'loss/train': '0.68663', 'examples_per_second': '30.122', 'grad_norm': '86.5', 'counters/examples': 101568, 'counters/updates': 3174}
skipping logging after 101600 examples to avoid logging too frequently
train stats after 101632 examples: {'rewards_train/chosen': '0.096462', 'rewards_train/rejected': '0.0341', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062362', 'logps_train/rejected': '-144.6', 'logps_train/chosen': '-159.23', 'loss/train': '0.67873', 'examples_per_second': '30.277', 'grad_norm': '95', 'counters/examples': 101632, 'counters/updates': 3176}
train stats after 101664 examples: {'rewards_train/chosen': '-0.0028024', 'rewards_train/rejected': '0.0175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020303', 'logps_train/rejected': '-103.77', 'logps_train/chosen': '-131.84', 'loss/train': '0.71153', 'examples_per_second': '31.603', 'grad_norm': '79.5', 'counters/examples': 101664, 'counters/updates': 3177}
train stats after 101696 examples: {'rewards_train/chosen': '0.021961', 'rewards_train/rejected': '-0.013628', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.035588', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-115.13', 'loss/train': '0.69093', 'examples_per_second': '30.636', 'grad_norm': '99.5', 'counters/examples': 101696, 'counters/updates': 3178}
train stats after 101728 examples: {'rewards_train/chosen': '0.15353', 'rewards_train/rejected': '0.064764', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088766', 'logps_train/rejected': '-138.22', 'logps_train/chosen': '-146.36', 'loss/train': '0.65653', 'examples_per_second': '31.173', 'grad_norm': '80', 'counters/examples': 101728, 'counters/updates': 3179}
train stats after 101760 examples: {'rewards_train/chosen': '0.056022', 'rewards_train/rejected': '0.011393', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044629', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-117.74', 'loss/train': '0.67468', 'examples_per_second': '31.557', 'grad_norm': '65.5', 'counters/examples': 101760, 'counters/updates': 3180}
train stats after 101792 examples: {'rewards_train/chosen': '0.027459', 'rewards_train/rejected': '-0.035701', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06316', 'logps_train/rejected': '-131.59', 'logps_train/chosen': '-137.26', 'loss/train': '0.66868', 'examples_per_second': '31.624', 'grad_norm': '110', 'counters/examples': 101792, 'counters/updates': 3181}
skipping logging after 101824 examples to avoid logging too frequently
train stats after 101856 examples: {'rewards_train/chosen': '0.023837', 'rewards_train/rejected': '-0.0015443', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025381', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-135.57', 'loss/train': '0.68474', 'examples_per_second': '34.677', 'grad_norm': '92', 'counters/examples': 101856, 'counters/updates': 3183}
train stats after 101888 examples: {'rewards_train/chosen': '0.089794', 'rewards_train/rejected': '0.025481', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064313', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-165.71', 'loss/train': '0.67406', 'examples_per_second': '32.669', 'grad_norm': '74', 'counters/examples': 101888, 'counters/updates': 3184}
train stats after 101920 examples: {'rewards_train/chosen': '0.10283', 'rewards_train/rejected': '-0.018632', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12146', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-183.98', 'loss/train': '0.64786', 'examples_per_second': '31.603', 'grad_norm': '74', 'counters/examples': 101920, 'counters/updates': 3185}
train stats after 101952 examples: {'rewards_train/chosen': '0.031125', 'rewards_train/rejected': '0.029403', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0017222', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-158.17', 'loss/train': '0.70459', 'examples_per_second': '30.16', 'grad_norm': '72', 'counters/examples': 101952, 'counters/updates': 3186}
train stats after 101984 examples: {'rewards_train/chosen': '0.092343', 'rewards_train/rejected': '0.075065', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.017278', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-181.33', 'loss/train': '0.71362', 'examples_per_second': '31.58', 'grad_norm': '100.5', 'counters/examples': 101984, 'counters/updates': 3187}
train stats after 102016 examples: {'rewards_train/chosen': '0.0047274', 'rewards_train/rejected': '0.0010711', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0036563', 'logps_train/rejected': '-159.31', 'logps_train/chosen': '-138.54', 'loss/train': '0.70277', 'examples_per_second': '31.544', 'grad_norm': '129', 'counters/examples': 102016, 'counters/updates': 3188}
skipping logging after 102048 examples to avoid logging too frequently
train stats after 102080 examples: {'rewards_train/chosen': '0.11538', 'rewards_train/rejected': '0.056086', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059293', 'logps_train/rejected': '-151.78', 'logps_train/chosen': '-167.17', 'loss/train': '0.67984', 'examples_per_second': '31.597', 'grad_norm': '167', 'counters/examples': 102080, 'counters/updates': 3190}
train stats after 102112 examples: {'rewards_train/chosen': '0.0075844', 'rewards_train/rejected': '0.012913', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0053281', 'logps_train/rejected': '-150.67', 'logps_train/chosen': '-170.39', 'loss/train': '0.70663', 'examples_per_second': '30.601', 'grad_norm': '98', 'counters/examples': 102112, 'counters/updates': 3191}
train stats after 102144 examples: {'rewards_train/chosen': '0.15287', 'rewards_train/rejected': '0.084102', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.068767', 'logps_train/rejected': '-101.55', 'logps_train/chosen': '-116.42', 'loss/train': '0.67232', 'examples_per_second': '32.65', 'grad_norm': '95.5', 'counters/examples': 102144, 'counters/updates': 3192}
train stats after 102176 examples: {'rewards_train/chosen': '0.086334', 'rewards_train/rejected': '-0.037645', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12398', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-155.2', 'loss/train': '0.64177', 'examples_per_second': '31.403', 'grad_norm': '63', 'counters/examples': 102176, 'counters/updates': 3193}
train stats after 102208 examples: {'rewards_train/chosen': '0.061758', 'rewards_train/rejected': '0.0040788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057679', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-165.36', 'loss/train': '0.68097', 'examples_per_second': '31.581', 'grad_norm': '73.5', 'counters/examples': 102208, 'counters/updates': 3194}
train stats after 102240 examples: {'rewards_train/chosen': '0.00028162', 'rewards_train/rejected': '0.090854', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.090572', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-135.97', 'loss/train': '0.74833', 'examples_per_second': '32.535', 'grad_norm': '210', 'counters/examples': 102240, 'counters/updates': 3195}
train stats after 102272 examples: {'rewards_train/chosen': '0.10673', 'rewards_train/rejected': '-0.008098', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11483', 'logps_train/rejected': '-91.015', 'logps_train/chosen': '-137.54', 'loss/train': '0.64511', 'examples_per_second': '30.864', 'grad_norm': '63.75', 'counters/examples': 102272, 'counters/updates': 3196}
train stats after 102304 examples: {'rewards_train/chosen': '0.039247', 'rewards_train/rejected': '0.080765', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041517', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-196.56', 'loss/train': '0.72523', 'examples_per_second': '33.081', 'grad_norm': '77', 'counters/examples': 102304, 'counters/updates': 3197}
skipping logging after 102336 examples to avoid logging too frequently
train stats after 102368 examples: {'rewards_train/chosen': '0.10545', 'rewards_train/rejected': '0.062171', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043281', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-130.29', 'loss/train': '0.68068', 'examples_per_second': '31.935', 'grad_norm': '81', 'counters/examples': 102368, 'counters/updates': 3199}
train stats after 102400 examples: {'rewards_train/chosen': '-0.00487', 'rewards_train/rejected': '-0.0018444', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0030256', 'logps_train/rejected': '-151', 'logps_train/chosen': '-115.64', 'loss/train': '0.70543', 'examples_per_second': '31.331', 'grad_norm': '100', 'counters/examples': 102400, 'counters/updates': 3200}
train stats after 102432 examples: {'rewards_train/chosen': '0.088065', 'rewards_train/rejected': '0.0099115', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078153', 'logps_train/rejected': '-151.05', 'logps_train/chosen': '-146.74', 'loss/train': '0.66886', 'examples_per_second': '30.202', 'grad_norm': '434', 'counters/examples': 102432, 'counters/updates': 3201}
train stats after 102464 examples: {'rewards_train/chosen': '0.013085', 'rewards_train/rejected': '0.06293', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049845', 'logps_train/rejected': '-148.33', 'logps_train/chosen': '-158.24', 'loss/train': '0.7266', 'examples_per_second': '29.835', 'grad_norm': '100', 'counters/examples': 102464, 'counters/updates': 3202}
skipping logging after 102496 examples to avoid logging too frequently
train stats after 102528 examples: {'rewards_train/chosen': '0.0095409', 'rewards_train/rejected': '-0.064644', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074185', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-119.47', 'loss/train': '0.66901', 'examples_per_second': '32.718', 'grad_norm': '69', 'counters/examples': 102528, 'counters/updates': 3204}
train stats after 102560 examples: {'rewards_train/chosen': '0.090493', 'rewards_train/rejected': '0.026986', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063507', 'logps_train/rejected': '-154.87', 'logps_train/chosen': '-149.08', 'loss/train': '0.68369', 'examples_per_second': '23.744', 'grad_norm': '476', 'counters/examples': 102560, 'counters/updates': 3205}
train stats after 102592 examples: {'rewards_train/chosen': '0.029133', 'rewards_train/rejected': '0.10694', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.077805', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-168.01', 'loss/train': '0.74681', 'examples_per_second': '30.033', 'grad_norm': '109', 'counters/examples': 102592, 'counters/updates': 3206}
train stats after 102624 examples: {'rewards_train/chosen': '0.080856', 'rewards_train/rejected': '-0.028001', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10886', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-174.54', 'loss/train': '0.64902', 'examples_per_second': '30.439', 'grad_norm': '119.5', 'counters/examples': 102624, 'counters/updates': 3207}
train stats after 102656 examples: {'rewards_train/chosen': '0.0093137', 'rewards_train/rejected': '-0.013303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022617', 'logps_train/rejected': '-156.24', 'logps_train/chosen': '-170.72', 'loss/train': '0.70246', 'examples_per_second': '22.842', 'grad_norm': '86', 'counters/examples': 102656, 'counters/updates': 3208}
train stats after 102688 examples: {'rewards_train/chosen': '-0.0037997', 'rewards_train/rejected': '0.01008', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01388', 'logps_train/rejected': '-145', 'logps_train/chosen': '-147.48', 'loss/train': '0.706', 'examples_per_second': '32.557', 'grad_norm': '68', 'counters/examples': 102688, 'counters/updates': 3209}
train stats after 102720 examples: {'rewards_train/chosen': '-0.0076544', 'rewards_train/rejected': '-0.048188', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.040534', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-155.41', 'loss/train': '0.69476', 'examples_per_second': '30.974', 'grad_norm': '123', 'counters/examples': 102720, 'counters/updates': 3210}
train stats after 102752 examples: {'rewards_train/chosen': '0.097874', 'rewards_train/rejected': '-0.042071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13994', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-141.97', 'loss/train': '0.63335', 'examples_per_second': '31.625', 'grad_norm': '95.5', 'counters/examples': 102752, 'counters/updates': 3211}
train stats after 102784 examples: {'rewards_train/chosen': '0.031033', 'rewards_train/rejected': '-0.011504', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042537', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-154.24', 'loss/train': '0.67898', 'examples_per_second': '30.927', 'grad_norm': '66.5', 'counters/examples': 102784, 'counters/updates': 3212}
train stats after 102816 examples: {'rewards_train/chosen': '0.087133', 'rewards_train/rejected': '0.1368', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.049663', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-169.01', 'loss/train': '0.73208', 'examples_per_second': '31.74', 'grad_norm': '100', 'counters/examples': 102816, 'counters/updates': 3213}
train stats after 102848 examples: {'rewards_train/chosen': '0.0055637', 'rewards_train/rejected': '0.10011', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.094544', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-150.11', 'loss/train': '0.75159', 'examples_per_second': '31.344', 'grad_norm': '76.5', 'counters/examples': 102848, 'counters/updates': 3214}
skipping logging after 102880 examples to avoid logging too frequently
train stats after 102912 examples: {'rewards_train/chosen': '0.043136', 'rewards_train/rejected': '-0.0064218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049558', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-169.62', 'loss/train': '0.67982', 'examples_per_second': '31.568', 'grad_norm': '111', 'counters/examples': 102912, 'counters/updates': 3216}
train stats after 102944 examples: {'rewards_train/chosen': '0.015567', 'rewards_train/rejected': '-0.011458', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027025', 'logps_train/rejected': '-95.699', 'logps_train/chosen': '-131.09', 'loss/train': '0.68818', 'examples_per_second': '30.896', 'grad_norm': '73', 'counters/examples': 102944, 'counters/updates': 3217}
train stats after 102976 examples: {'rewards_train/chosen': '-0.031199', 'rewards_train/rejected': '0.035067', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.066266', 'logps_train/rejected': '-180.52', 'logps_train/chosen': '-153.18', 'loss/train': '0.74811', 'examples_per_second': '30.872', 'grad_norm': '94', 'counters/examples': 102976, 'counters/updates': 3218}
train stats after 103008 examples: {'rewards_train/chosen': '0.087958', 'rewards_train/rejected': '0.082313', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056453', 'logps_train/rejected': '-123.53', 'logps_train/chosen': '-126.72', 'loss/train': '0.70033', 'examples_per_second': '30.172', 'grad_norm': '58.75', 'counters/examples': 103008, 'counters/updates': 3219}
train stats after 103040 examples: {'rewards_train/chosen': '-0.043886', 'rewards_train/rejected': '0.064083', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10797', 'logps_train/rejected': '-203', 'logps_train/chosen': '-196.18', 'loss/train': '0.7717', 'examples_per_second': '31.627', 'grad_norm': '217', 'counters/examples': 103040, 'counters/updates': 3220}
skipping logging after 103072 examples to avoid logging too frequently
train stats after 103104 examples: {'rewards_train/chosen': '0.045317', 'rewards_train/rejected': '0.063077', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01776', 'logps_train/rejected': '-139.28', 'logps_train/chosen': '-145.78', 'loss/train': '0.71641', 'examples_per_second': '30.141', 'grad_norm': '75', 'counters/examples': 103104, 'counters/updates': 3222}
skipping logging after 103136 examples to avoid logging too frequently
train stats after 103168 examples: {'rewards_train/chosen': '0.073788', 'rewards_train/rejected': '0.022639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051149', 'logps_train/rejected': '-158', 'logps_train/chosen': '-179.39', 'loss/train': '0.67857', 'examples_per_second': '30.077', 'grad_norm': '148', 'counters/examples': 103168, 'counters/updates': 3224}
train stats after 103200 examples: {'rewards_train/chosen': '0.029623', 'rewards_train/rejected': '0.053678', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024055', 'logps_train/rejected': '-107.08', 'logps_train/chosen': '-148.09', 'loss/train': '0.72276', 'examples_per_second': '33.081', 'grad_norm': '109', 'counters/examples': 103200, 'counters/updates': 3225}
train stats after 103232 examples: {'rewards_train/chosen': '0.045705', 'rewards_train/rejected': '0.081945', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03624', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-126.64', 'loss/train': '0.72181', 'examples_per_second': '32.459', 'grad_norm': '50.75', 'counters/examples': 103232, 'counters/updates': 3226}
train stats after 103264 examples: {'rewards_train/chosen': '0.055536', 'rewards_train/rejected': '0.059784', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0042477', 'logps_train/rejected': '-158.76', 'logps_train/chosen': '-163.02', 'loss/train': '0.70931', 'examples_per_second': '30.344', 'grad_norm': '200', 'counters/examples': 103264, 'counters/updates': 3227}
train stats after 103296 examples: {'rewards_train/chosen': '0.050133', 'rewards_train/rejected': '0.038578', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011555', 'logps_train/rejected': '-144.14', 'logps_train/chosen': '-190.1', 'loss/train': '0.70188', 'examples_per_second': '30.215', 'grad_norm': '60.75', 'counters/examples': 103296, 'counters/updates': 3228}
train stats after 103328 examples: {'rewards_train/chosen': '0.053514', 'rewards_train/rejected': '0.042429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011085', 'logps_train/rejected': '-156.97', 'logps_train/chosen': '-146.87', 'loss/train': '0.7006', 'examples_per_second': '31.634', 'grad_norm': '82.5', 'counters/examples': 103328, 'counters/updates': 3229}
train stats after 103360 examples: {'rewards_train/chosen': '0.070996', 'rewards_train/rejected': '0.024116', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04688', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-127.92', 'loss/train': '0.67919', 'examples_per_second': '31.002', 'grad_norm': '73.5', 'counters/examples': 103360, 'counters/updates': 3230}
train stats after 103392 examples: {'rewards_train/chosen': '0.10776', 'rewards_train/rejected': '0.014366', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093399', 'logps_train/rejected': '-103.94', 'logps_train/chosen': '-164.31', 'loss/train': '0.66523', 'examples_per_second': '32.751', 'grad_norm': '73.5', 'counters/examples': 103392, 'counters/updates': 3231}
skipping logging after 103424 examples to avoid logging too frequently
train stats after 103456 examples: {'rewards_train/chosen': '0.12872', 'rewards_train/rejected': '0.030652', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098072', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-173.6', 'loss/train': '0.65546', 'examples_per_second': '33.832', 'grad_norm': '73.5', 'counters/examples': 103456, 'counters/updates': 3233}
train stats after 103488 examples: {'rewards_train/chosen': '-0.00011371', 'rewards_train/rejected': '0.011065', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011179', 'logps_train/rejected': '-184.73', 'logps_train/chosen': '-137.63', 'loss/train': '0.70812', 'examples_per_second': '31.141', 'grad_norm': '140', 'counters/examples': 103488, 'counters/updates': 3234}
skipping logging after 103520 examples to avoid logging too frequently
train stats after 103552 examples: {'rewards_train/chosen': '0.04245', 'rewards_train/rejected': '0.032179', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010272', 'logps_train/rejected': '-156.95', 'logps_train/chosen': '-166.45', 'loss/train': '0.69756', 'examples_per_second': '33.143', 'grad_norm': '97.5', 'counters/examples': 103552, 'counters/updates': 3236}
train stats after 103584 examples: {'rewards_train/chosen': '0.12977', 'rewards_train/rejected': '0.076564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05321', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-168.42', 'loss/train': '0.68864', 'examples_per_second': '32.268', 'grad_norm': '235', 'counters/examples': 103584, 'counters/updates': 3237}
skipping logging after 103616 examples to avoid logging too frequently
train stats after 103648 examples: {'rewards_train/chosen': '0.18659', 'rewards_train/rejected': '0.021775', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16481', 'logps_train/rejected': '-96.06', 'logps_train/chosen': '-152.25', 'loss/train': '0.62606', 'examples_per_second': '31.875', 'grad_norm': '239', 'counters/examples': 103648, 'counters/updates': 3239}
train stats after 103680 examples: {'rewards_train/chosen': '0.074849', 'rewards_train/rejected': '-0.02171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096559', 'logps_train/rejected': '-157.44', 'logps_train/chosen': '-198.65', 'loss/train': '0.6679', 'examples_per_second': '30.068', 'grad_norm': '83', 'counters/examples': 103680, 'counters/updates': 3240}
train stats after 103712 examples: {'rewards_train/chosen': '0.07564', 'rewards_train/rejected': '0.018606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057034', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-112.72', 'loss/train': '0.67258', 'examples_per_second': '30.938', 'grad_norm': '125', 'counters/examples': 103712, 'counters/updates': 3241}
skipping logging after 103744 examples to avoid logging too frequently
train stats after 103776 examples: {'rewards_train/chosen': '0.086735', 'rewards_train/rejected': '0.075473', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011262', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-140.48', 'loss/train': '0.70364', 'examples_per_second': '31.606', 'grad_norm': '131', 'counters/examples': 103776, 'counters/updates': 3243}
train stats after 103808 examples: {'rewards_train/chosen': '0.11005', 'rewards_train/rejected': '0.010763', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099291', 'logps_train/rejected': '-152.4', 'logps_train/chosen': '-161.94', 'loss/train': '0.66065', 'examples_per_second': '33.249', 'grad_norm': '99.5', 'counters/examples': 103808, 'counters/updates': 3244}
train stats after 103840 examples: {'rewards_train/chosen': '0.053491', 'rewards_train/rejected': '-0.01598', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069471', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-126.64', 'loss/train': '0.66861', 'examples_per_second': '30.851', 'grad_norm': '83.5', 'counters/examples': 103840, 'counters/updates': 3245}
train stats after 103872 examples: {'rewards_train/chosen': '-0.0040048', 'rewards_train/rejected': '-0.032473', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028468', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-101.6', 'loss/train': '0.68368', 'examples_per_second': '31.607', 'grad_norm': '83', 'counters/examples': 103872, 'counters/updates': 3246}
train stats after 103904 examples: {'rewards_train/chosen': '0.040211', 'rewards_train/rejected': '0.07307', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032859', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-153.57', 'loss/train': '0.72', 'examples_per_second': '30.991', 'grad_norm': '80.5', 'counters/examples': 103904, 'counters/updates': 3247}
train stats after 103936 examples: {'rewards_train/chosen': '0.11985', 'rewards_train/rejected': '0.11154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0083071', 'logps_train/rejected': '-213.8', 'logps_train/chosen': '-203.22', 'loss/train': '0.71369', 'examples_per_second': '30.379', 'grad_norm': '100.5', 'counters/examples': 103936, 'counters/updates': 3248}
train stats after 103968 examples: {'rewards_train/chosen': '0.045435', 'rewards_train/rejected': '0.0087484', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036686', 'logps_train/rejected': '-115.46', 'logps_train/chosen': '-139.06', 'loss/train': '0.6819', 'examples_per_second': '30.625', 'grad_norm': '75.5', 'counters/examples': 103968, 'counters/updates': 3249}
train stats after 104000 examples: {'rewards_train/chosen': '0.07809', 'rewards_train/rejected': '0.026705', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051384', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-148.13', 'loss/train': '0.67387', 'examples_per_second': '30.538', 'grad_norm': '73.5', 'counters/examples': 104000, 'counters/updates': 3250}
train stats after 104032 examples: {'rewards_train/chosen': '0.11591', 'rewards_train/rejected': '0.10161', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014293', 'logps_train/rejected': '-159.77', 'logps_train/chosen': '-196.59', 'loss/train': '0.70383', 'examples_per_second': '31.24', 'grad_norm': '69.5', 'counters/examples': 104032, 'counters/updates': 3251}
train stats after 104064 examples: {'rewards_train/chosen': '0.069391', 'rewards_train/rejected': '0.045497', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023895', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-162.14', 'loss/train': '0.70008', 'examples_per_second': '30.965', 'grad_norm': '94', 'counters/examples': 104064, 'counters/updates': 3252}
skipping logging after 104096 examples to avoid logging too frequently
train stats after 104128 examples: {'rewards_train/chosen': '0.13513', 'rewards_train/rejected': '0.013602', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12153', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-183.04', 'loss/train': '0.65395', 'examples_per_second': '30.132', 'grad_norm': '133', 'counters/examples': 104128, 'counters/updates': 3254}
train stats after 104160 examples: {'rewards_train/chosen': '0.052503', 'rewards_train/rejected': '0.02706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025443', 'logps_train/rejected': '-132.54', 'logps_train/chosen': '-144.93', 'loss/train': '0.68933', 'examples_per_second': '32.984', 'grad_norm': '59.5', 'counters/examples': 104160, 'counters/updates': 3255}
train stats after 104192 examples: {'rewards_train/chosen': '0.12507', 'rewards_train/rejected': '0.041994', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083078', 'logps_train/rejected': '-152.61', 'logps_train/chosen': '-157.72', 'loss/train': '0.67091', 'examples_per_second': '32.461', 'grad_norm': '115.5', 'counters/examples': 104192, 'counters/updates': 3256}
train stats after 104224 examples: {'rewards_train/chosen': '0.017178', 'rewards_train/rejected': '0.0035735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013604', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-127.97', 'loss/train': '0.701', 'examples_per_second': '31.512', 'grad_norm': '122', 'counters/examples': 104224, 'counters/updates': 3257}
train stats after 104256 examples: {'rewards_train/chosen': '-0.04597', 'rewards_train/rejected': '0.040824', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.086794', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-144.56', 'loss/train': '0.77709', 'examples_per_second': '30.135', 'grad_norm': '382', 'counters/examples': 104256, 'counters/updates': 3258}
skipping logging after 104288 examples to avoid logging too frequently
train stats after 104320 examples: {'rewards_train/chosen': '-0.066405', 'rewards_train/rejected': '0.00048613', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.066891', 'logps_train/rejected': '-137.46', 'logps_train/chosen': '-206.63', 'loss/train': '0.74563', 'examples_per_second': '31.601', 'grad_norm': '190', 'counters/examples': 104320, 'counters/updates': 3260}
train stats after 104352 examples: {'rewards_train/chosen': '0.015849', 'rewards_train/rejected': '0.016288', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00043953', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-112.07', 'loss/train': '0.70729', 'examples_per_second': '31.304', 'grad_norm': '70.5', 'counters/examples': 104352, 'counters/updates': 3261}
train stats after 104384 examples: {'rewards_train/chosen': '0.074028', 'rewards_train/rejected': '-0.0035528', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07758', 'logps_train/rejected': '-117', 'logps_train/chosen': '-123.23', 'loss/train': '0.66375', 'examples_per_second': '31.732', 'grad_norm': '85', 'counters/examples': 104384, 'counters/updates': 3262}
train stats after 104416 examples: {'rewards_train/chosen': '0.0031545', 'rewards_train/rejected': '-0.056221', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059376', 'logps_train/rejected': '-149.8', 'logps_train/chosen': '-138.26', 'loss/train': '0.67755', 'examples_per_second': '30.651', 'grad_norm': '78.5', 'counters/examples': 104416, 'counters/updates': 3263}
train stats after 104448 examples: {'rewards_train/chosen': '0.067486', 'rewards_train/rejected': '0.068222', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00073528', 'logps_train/rejected': '-172.32', 'logps_train/chosen': '-138.83', 'loss/train': '0.70265', 'examples_per_second': '30.926', 'grad_norm': '86', 'counters/examples': 104448, 'counters/updates': 3264}
skipping logging after 104480 examples to avoid logging too frequently
train stats after 104512 examples: {'rewards_train/chosen': '0.037', 'rewards_train/rejected': '-0.0048624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041862', 'logps_train/rejected': '-153.25', 'logps_train/chosen': '-146.58', 'loss/train': '0.68154', 'examples_per_second': '31.332', 'grad_norm': '125.5', 'counters/examples': 104512, 'counters/updates': 3266}
train stats after 104544 examples: {'rewards_train/chosen': '0.061195', 'rewards_train/rejected': '0.0014674', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059727', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-150.92', 'loss/train': '0.67626', 'examples_per_second': '31.474', 'grad_norm': '202', 'counters/examples': 104544, 'counters/updates': 3267}
train stats after 104576 examples: {'rewards_train/chosen': '0.043068', 'rewards_train/rejected': '0.03089', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012178', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-146.03', 'loss/train': '0.70224', 'examples_per_second': '31.649', 'grad_norm': '110', 'counters/examples': 104576, 'counters/updates': 3268}
train stats after 104608 examples: {'rewards_train/chosen': '0.07811', 'rewards_train/rejected': '0.052375', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.025734', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-165.9', 'loss/train': '0.69383', 'examples_per_second': '31.603', 'grad_norm': '66', 'counters/examples': 104608, 'counters/updates': 3269}
train stats after 104640 examples: {'rewards_train/chosen': '0.096677', 'rewards_train/rejected': '0.045121', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051556', 'logps_train/rejected': '-107.47', 'logps_train/chosen': '-128.38', 'loss/train': '0.67636', 'examples_per_second': '24.103', 'grad_norm': '93', 'counters/examples': 104640, 'counters/updates': 3270}
train stats after 104672 examples: {'rewards_train/chosen': '0.053311', 'rewards_train/rejected': '-0.0075064', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060817', 'logps_train/rejected': '-135.39', 'logps_train/chosen': '-169.7', 'loss/train': '0.67323', 'examples_per_second': '29.868', 'grad_norm': '67.5', 'counters/examples': 104672, 'counters/updates': 3271}
train stats after 104704 examples: {'rewards_train/chosen': '0.090534', 'rewards_train/rejected': '0.03585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054684', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-159.77', 'loss/train': '0.67543', 'examples_per_second': '31.4', 'grad_norm': '82', 'counters/examples': 104704, 'counters/updates': 3272}
train stats after 104736 examples: {'rewards_train/chosen': '0.056307', 'rewards_train/rejected': '0.049388', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0069189', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-134.46', 'loss/train': '0.70072', 'examples_per_second': '31.484', 'grad_norm': '92', 'counters/examples': 104736, 'counters/updates': 3273}
train stats after 104768 examples: {'rewards_train/chosen': '0.062013', 'rewards_train/rejected': '0.11759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.055575', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-162.65', 'loss/train': '0.74331', 'examples_per_second': '32.292', 'grad_norm': '97', 'counters/examples': 104768, 'counters/updates': 3274}
train stats after 104800 examples: {'rewards_train/chosen': '0.049272', 'rewards_train/rejected': '-0.051551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10082', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-133.33', 'loss/train': '0.65493', 'examples_per_second': '31.678', 'grad_norm': '75.5', 'counters/examples': 104800, 'counters/updates': 3275}
train stats after 104832 examples: {'rewards_train/chosen': '0.081829', 'rewards_train/rejected': '-0.029678', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11151', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-162.52', 'loss/train': '0.64969', 'examples_per_second': '31.36', 'grad_norm': '112.5', 'counters/examples': 104832, 'counters/updates': 3276}
skipping logging after 104864 examples to avoid logging too frequently
train stats after 104896 examples: {'rewards_train/chosen': '0.070032', 'rewards_train/rejected': '0.006667', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063365', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-147.78', 'loss/train': '0.66905', 'examples_per_second': '33.672', 'grad_norm': '97.5', 'counters/examples': 104896, 'counters/updates': 3278}
train stats after 104928 examples: {'rewards_train/chosen': '0.071447', 'rewards_train/rejected': '0.055384', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016063', 'logps_train/rejected': '-117', 'logps_train/chosen': '-153.29', 'loss/train': '0.70004', 'examples_per_second': '31.389', 'grad_norm': '97', 'counters/examples': 104928, 'counters/updates': 3279}
train stats after 104960 examples: {'rewards_train/chosen': '0.056375', 'rewards_train/rejected': '0.046927', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.009448', 'logps_train/rejected': '-118.75', 'logps_train/chosen': '-168.04', 'loss/train': '0.69392', 'examples_per_second': '31.436', 'grad_norm': '80.5', 'counters/examples': 104960, 'counters/updates': 3280}
train stats after 104992 examples: {'rewards_train/chosen': '0.062036', 'rewards_train/rejected': '0.057582', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0044536', 'logps_train/rejected': '-164.01', 'logps_train/chosen': '-147.18', 'loss/train': '0.71038', 'examples_per_second': '32.794', 'grad_norm': '148', 'counters/examples': 104992, 'counters/updates': 3281}
train stats after 105024 examples: {'rewards_train/chosen': '0.029611', 'rewards_train/rejected': '-0.022345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051956', 'logps_train/rejected': '-121.11', 'logps_train/chosen': '-149.23', 'loss/train': '0.67417', 'examples_per_second': '30.265', 'grad_norm': '86.5', 'counters/examples': 105024, 'counters/updates': 3282}
train stats after 105056 examples: {'rewards_train/chosen': '0.069676', 'rewards_train/rejected': '0.12465', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054974', 'logps_train/rejected': '-128.37', 'logps_train/chosen': '-160.78', 'loss/train': '0.73619', 'examples_per_second': '30.15', 'grad_norm': '90.5', 'counters/examples': 105056, 'counters/updates': 3283}
train stats after 105088 examples: {'rewards_train/chosen': '0.054188', 'rewards_train/rejected': '0.041123', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013065', 'logps_train/rejected': '-166.78', 'logps_train/chosen': '-156.8', 'loss/train': '0.7024', 'examples_per_second': '31.59', 'grad_norm': '126.5', 'counters/examples': 105088, 'counters/updates': 3284}
train stats after 105120 examples: {'rewards_train/chosen': '0.063099', 'rewards_train/rejected': '0.007003', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056096', 'logps_train/rejected': '-150.37', 'logps_train/chosen': '-146.07', 'loss/train': '0.67851', 'examples_per_second': '31.775', 'grad_norm': '92', 'counters/examples': 105120, 'counters/updates': 3285}
train stats after 105152 examples: {'rewards_train/chosen': '0.068041', 'rewards_train/rejected': '-0.010993', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079033', 'logps_train/rejected': '-143.71', 'logps_train/chosen': '-129.62', 'loss/train': '0.66265', 'examples_per_second': '30.66', 'grad_norm': '81.5', 'counters/examples': 105152, 'counters/updates': 3286}
train stats after 105184 examples: {'rewards_train/chosen': '0.090164', 'rewards_train/rejected': '0.16533', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.07517', 'logps_train/rejected': '-153.62', 'logps_train/chosen': '-163.43', 'loss/train': '0.75458', 'examples_per_second': '31.599', 'grad_norm': '129', 'counters/examples': 105184, 'counters/updates': 3287}
train stats after 105216 examples: {'rewards_train/chosen': '0.090573', 'rewards_train/rejected': '0.066063', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024509', 'logps_train/rejected': '-149.74', 'logps_train/chosen': '-122.47', 'loss/train': '0.68609', 'examples_per_second': '33.075', 'grad_norm': '70', 'counters/examples': 105216, 'counters/updates': 3288}
train stats after 105248 examples: {'rewards_train/chosen': '0.02894', 'rewards_train/rejected': '-0.037171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066111', 'logps_train/rejected': '-127.76', 'logps_train/chosen': '-111.12', 'loss/train': '0.6741', 'examples_per_second': '31.129', 'grad_norm': '61', 'counters/examples': 105248, 'counters/updates': 3289}
train stats after 105280 examples: {'rewards_train/chosen': '0.027735', 'rewards_train/rejected': '0.070129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.042394', 'logps_train/rejected': '-146.38', 'logps_train/chosen': '-136.6', 'loss/train': '0.72973', 'examples_per_second': '31.572', 'grad_norm': '69.5', 'counters/examples': 105280, 'counters/updates': 3290}
train stats after 105312 examples: {'rewards_train/chosen': '0.088806', 'rewards_train/rejected': '0.0054735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083332', 'logps_train/rejected': '-168.85', 'logps_train/chosen': '-153.84', 'loss/train': '0.67613', 'examples_per_second': '31.596', 'grad_norm': '152', 'counters/examples': 105312, 'counters/updates': 3291}
train stats after 105344 examples: {'rewards_train/chosen': '0.072028', 'rewards_train/rejected': '0.033155', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038873', 'logps_train/rejected': '-116.16', 'logps_train/chosen': '-155.62', 'loss/train': '0.68555', 'examples_per_second': '30.735', 'grad_norm': '173', 'counters/examples': 105344, 'counters/updates': 3292}
train stats after 105376 examples: {'rewards_train/chosen': '0.04029', 'rewards_train/rejected': '0.13907', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.098781', 'logps_train/rejected': '-167.88', 'logps_train/chosen': '-144.39', 'loss/train': '0.7556', 'examples_per_second': '32.302', 'grad_norm': '111', 'counters/examples': 105376, 'counters/updates': 3293}
skipping logging after 105408 examples to avoid logging too frequently
train stats after 105440 examples: {'rewards_train/chosen': '-0.006039', 'rewards_train/rejected': '0.037995', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.044034', 'logps_train/rejected': '-112.71', 'logps_train/chosen': '-135.55', 'loss/train': '0.72534', 'examples_per_second': '33.368', 'grad_norm': '102', 'counters/examples': 105440, 'counters/updates': 3295}
train stats after 105472 examples: {'rewards_train/chosen': '0.099218', 'rewards_train/rejected': '0.0094259', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089792', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-174.37', 'loss/train': '0.66619', 'examples_per_second': '32.725', 'grad_norm': '85.5', 'counters/examples': 105472, 'counters/updates': 3296}
train stats after 105504 examples: {'rewards_train/chosen': '-0.037294', 'rewards_train/rejected': '0.042405', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.079699', 'logps_train/rejected': '-96.333', 'logps_train/chosen': '-136.8', 'loss/train': '0.74703', 'examples_per_second': '31.91', 'grad_norm': '99.5', 'counters/examples': 105504, 'counters/updates': 3297}
skipping logging after 105536 examples to avoid logging too frequently
train stats after 105568 examples: {'rewards_train/chosen': '0.042879', 'rewards_train/rejected': '0.07995', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037071', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-142.2', 'loss/train': '0.71748', 'examples_per_second': '31.772', 'grad_norm': '64', 'counters/examples': 105568, 'counters/updates': 3299}
train stats after 105600 examples: {'rewards_train/chosen': '0.024605', 'rewards_train/rejected': '0.082066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.057461', 'logps_train/rejected': '-109.17', 'logps_train/chosen': '-149.23', 'loss/train': '0.73258', 'examples_per_second': '31.622', 'grad_norm': '85.5', 'counters/examples': 105600, 'counters/updates': 3300}
train stats after 105632 examples: {'rewards_train/chosen': '0.077155', 'rewards_train/rejected': '0.035827', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041328', 'logps_train/rejected': '-102.98', 'logps_train/chosen': '-156.1', 'loss/train': '0.6873', 'examples_per_second': '32.259', 'grad_norm': '110', 'counters/examples': 105632, 'counters/updates': 3301}
train stats after 105664 examples: {'rewards_train/chosen': '0.064227', 'rewards_train/rejected': '-0.00048708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064714', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-112.2', 'loss/train': '0.67353', 'examples_per_second': '31.639', 'grad_norm': '109', 'counters/examples': 105664, 'counters/updates': 3302}
train stats after 105696 examples: {'rewards_train/chosen': '0.046322', 'rewards_train/rejected': '0.021078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025244', 'logps_train/rejected': '-107.79', 'logps_train/chosen': '-139.7', 'loss/train': '0.68868', 'examples_per_second': '31.109', 'grad_norm': '68', 'counters/examples': 105696, 'counters/updates': 3303}
train stats after 105728 examples: {'rewards_train/chosen': '0.019479', 'rewards_train/rejected': '0.025067', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0055879', 'logps_train/rejected': '-105.64', 'logps_train/chosen': '-159.27', 'loss/train': '0.7146', 'examples_per_second': '31.438', 'grad_norm': '75', 'counters/examples': 105728, 'counters/updates': 3304}
train stats after 105760 examples: {'rewards_train/chosen': '0.078272', 'rewards_train/rejected': '0.040778', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037494', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-179.15', 'loss/train': '0.68796', 'examples_per_second': '30.462', 'grad_norm': '101', 'counters/examples': 105760, 'counters/updates': 3305}
train stats after 105792 examples: {'rewards_train/chosen': '0.061974', 'rewards_train/rejected': '0.056474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0054994', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-211.57', 'loss/train': '0.70199', 'examples_per_second': '30.577', 'grad_norm': '80', 'counters/examples': 105792, 'counters/updates': 3306}
train stats after 105824 examples: {'rewards_train/chosen': '0.069749', 'rewards_train/rejected': '0.016179', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05357', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-149.84', 'loss/train': '0.68075', 'examples_per_second': '30.33', 'grad_norm': '94.5', 'counters/examples': 105824, 'counters/updates': 3307}
train stats after 105856 examples: {'rewards_train/chosen': '-0.014066', 'rewards_train/rejected': '0.020063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034129', 'logps_train/rejected': '-123.15', 'logps_train/chosen': '-174.85', 'loss/train': '0.73073', 'examples_per_second': '30.116', 'grad_norm': '123', 'counters/examples': 105856, 'counters/updates': 3308}
train stats after 105888 examples: {'rewards_train/chosen': '0.12698', 'rewards_train/rejected': '0.12652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0004672', 'logps_train/rejected': '-152.98', 'logps_train/chosen': '-146.91', 'loss/train': '0.70578', 'examples_per_second': '31.631', 'grad_norm': '66', 'counters/examples': 105888, 'counters/updates': 3309}
train stats after 105920 examples: {'rewards_train/chosen': '-0.0071855', 'rewards_train/rejected': '-0.00060504', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0065805', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-178.15', 'loss/train': '0.70404', 'examples_per_second': '30.659', 'grad_norm': '79', 'counters/examples': 105920, 'counters/updates': 3310}
train stats after 105952 examples: {'rewards_train/chosen': '0.12283', 'rewards_train/rejected': '0.063355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059474', 'logps_train/rejected': '-152.2', 'logps_train/chosen': '-164.55', 'loss/train': '0.67272', 'examples_per_second': '31.633', 'grad_norm': '108', 'counters/examples': 105952, 'counters/updates': 3311}
skipping logging after 105984 examples to avoid logging too frequently
train stats after 106016 examples: {'rewards_train/chosen': '-0.064463', 'rewards_train/rejected': '-0.0026017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.061861', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-204.1', 'loss/train': '0.74425', 'examples_per_second': '31.634', 'grad_norm': '229', 'counters/examples': 106016, 'counters/updates': 3313}
train stats after 106048 examples: {'rewards_train/chosen': '0.051314', 'rewards_train/rejected': '0.029884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02143', 'logps_train/rejected': '-110.05', 'logps_train/chosen': '-158.35', 'loss/train': '0.69802', 'examples_per_second': '30.297', 'grad_norm': '112', 'counters/examples': 106048, 'counters/updates': 3314}
train stats after 106080 examples: {'rewards_train/chosen': '-0.014389', 'rewards_train/rejected': '0.0033682', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017757', 'logps_train/rejected': '-118.73', 'logps_train/chosen': '-150.68', 'loss/train': '0.72162', 'examples_per_second': '31.641', 'grad_norm': '82', 'counters/examples': 106080, 'counters/updates': 3315}
skipping logging after 106112 examples to avoid logging too frequently
train stats after 106144 examples: {'rewards_train/chosen': '0.076415', 'rewards_train/rejected': '-0.0077213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084137', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-170.93', 'loss/train': '0.66233', 'examples_per_second': '31.94', 'grad_norm': '88', 'counters/examples': 106144, 'counters/updates': 3317}
skipping logging after 106176 examples to avoid logging too frequently
train stats after 106208 examples: {'rewards_train/chosen': '0.055544', 'rewards_train/rejected': '0.037218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018326', 'logps_train/rejected': '-100.57', 'logps_train/chosen': '-130.85', 'loss/train': '0.69636', 'examples_per_second': '30.671', 'grad_norm': '81', 'counters/examples': 106208, 'counters/updates': 3319}
train stats after 106240 examples: {'rewards_train/chosen': '0.1048', 'rewards_train/rejected': '0.10501', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00021093', 'logps_train/rejected': '-105.76', 'logps_train/chosen': '-141.59', 'loss/train': '0.7043', 'examples_per_second': '31.21', 'grad_norm': '171', 'counters/examples': 106240, 'counters/updates': 3320}
train stats after 106272 examples: {'rewards_train/chosen': '0.01166', 'rewards_train/rejected': '0.050467', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.038807', 'logps_train/rejected': '-121.22', 'logps_train/chosen': '-142.05', 'loss/train': '0.72161', 'examples_per_second': '31.818', 'grad_norm': '98', 'counters/examples': 106272, 'counters/updates': 3321}
skipping logging after 106304 examples to avoid logging too frequently
train stats after 106336 examples: {'rewards_train/chosen': '0.013113', 'rewards_train/rejected': '0.015199', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0020862', 'logps_train/rejected': '-116.69', 'logps_train/chosen': '-127.18', 'loss/train': '0.70403', 'examples_per_second': '33.048', 'grad_norm': '104.5', 'counters/examples': 106336, 'counters/updates': 3323}
train stats after 106368 examples: {'rewards_train/chosen': '0.081095', 'rewards_train/rejected': '0.040187', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040909', 'logps_train/rejected': '-115.81', 'logps_train/chosen': '-154.16', 'loss/train': '0.6858', 'examples_per_second': '31.547', 'grad_norm': '52', 'counters/examples': 106368, 'counters/updates': 3324}
train stats after 106400 examples: {'rewards_train/chosen': '0.066355', 'rewards_train/rejected': '-0.011724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078079', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-148.87', 'loss/train': '0.67755', 'examples_per_second': '30.075', 'grad_norm': '72.5', 'counters/examples': 106400, 'counters/updates': 3325}
skipping logging after 106432 examples to avoid logging too frequently
train stats after 106464 examples: {'rewards_train/chosen': '0.018605', 'rewards_train/rejected': '0.045849', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027243', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-179.27', 'loss/train': '0.72298', 'examples_per_second': '30.18', 'grad_norm': '131', 'counters/examples': 106464, 'counters/updates': 3327}
train stats after 106496 examples: {'rewards_train/chosen': '-0.0052291', 'rewards_train/rejected': '0.015956', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.021185', 'logps_train/rejected': '-151.34', 'logps_train/chosen': '-143.06', 'loss/train': '0.73318', 'examples_per_second': '31.688', 'grad_norm': '125', 'counters/examples': 106496, 'counters/updates': 3328}
train stats after 106528 examples: {'rewards_train/chosen': '-0.020891', 'rewards_train/rejected': '0.0036656', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024557', 'logps_train/rejected': '-172.02', 'logps_train/chosen': '-192.9', 'loss/train': '0.71969', 'examples_per_second': '31.662', 'grad_norm': '121.5', 'counters/examples': 106528, 'counters/updates': 3329}
train stats after 106560 examples: {'rewards_train/chosen': '0.06946', 'rewards_train/rejected': '0.02009', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04937', 'logps_train/rejected': '-158.89', 'logps_train/chosen': '-175.71', 'loss/train': '0.68087', 'examples_per_second': '31.112', 'grad_norm': '89.5', 'counters/examples': 106560, 'counters/updates': 3330}
skipping logging after 106592 examples to avoid logging too frequently
train stats after 106624 examples: {'rewards_train/chosen': '0.038587', 'rewards_train/rejected': '0.040167', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0015803', 'logps_train/rejected': '-132.44', 'logps_train/chosen': '-125.6', 'loss/train': '0.70102', 'examples_per_second': '31.159', 'grad_norm': '82', 'counters/examples': 106624, 'counters/updates': 3332}
train stats after 106656 examples: {'rewards_train/chosen': '0.08083', 'rewards_train/rejected': '0.12936', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.048526', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-137.78', 'loss/train': '0.72594', 'examples_per_second': '32.172', 'grad_norm': '138', 'counters/examples': 106656, 'counters/updates': 3333}
train stats after 106688 examples: {'rewards_train/chosen': '0.03782', 'rewards_train/rejected': '0.034652', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0031675', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-150.47', 'loss/train': '0.69791', 'examples_per_second': '31.807', 'grad_norm': '93.5', 'counters/examples': 106688, 'counters/updates': 3334}
train stats after 106720 examples: {'rewards_train/chosen': '0.080581', 'rewards_train/rejected': '-0.079088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15967', 'logps_train/rejected': '-126', 'logps_train/chosen': '-145.16', 'loss/train': '0.64146', 'examples_per_second': '30.125', 'grad_norm': '47.5', 'counters/examples': 106720, 'counters/updates': 3335}
train stats after 106752 examples: {'rewards_train/chosen': '0.035599', 'rewards_train/rejected': '0.058912', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023313', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-147.92', 'loss/train': '0.71754', 'examples_per_second': '31.659', 'grad_norm': '268', 'counters/examples': 106752, 'counters/updates': 3336}
skipping logging after 106784 examples to avoid logging too frequently
train stats after 106816 examples: {'rewards_train/chosen': '0.056538', 'rewards_train/rejected': '0.047795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0087428', 'logps_train/rejected': '-137.48', 'logps_train/chosen': '-112.48', 'loss/train': '0.69732', 'examples_per_second': '31.63', 'grad_norm': '76', 'counters/examples': 106816, 'counters/updates': 3338}
train stats after 106848 examples: {'rewards_train/chosen': '0.040477', 'rewards_train/rejected': '0.10591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.065437', 'logps_train/rejected': '-181.26', 'logps_train/chosen': '-148.2', 'loss/train': '0.74144', 'examples_per_second': '31.545', 'grad_norm': '144', 'counters/examples': 106848, 'counters/updates': 3339}
train stats after 106880 examples: {'rewards_train/chosen': '0.076925', 'rewards_train/rejected': '0.022355', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05457', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-175.42', 'loss/train': '0.67965', 'examples_per_second': '31.612', 'grad_norm': '125', 'counters/examples': 106880, 'counters/updates': 3340}
train stats after 106912 examples: {'rewards_train/chosen': '0.10192', 'rewards_train/rejected': '0.031406', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070513', 'logps_train/rejected': '-115.71', 'logps_train/chosen': '-160.79', 'loss/train': '0.67095', 'examples_per_second': '30.125', 'grad_norm': '73.5', 'counters/examples': 106912, 'counters/updates': 3341}
train stats after 106944 examples: {'rewards_train/chosen': '-0.02631', 'rewards_train/rejected': '0.019605', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045916', 'logps_train/rejected': '-148.18', 'logps_train/chosen': '-143.86', 'loss/train': '0.73768', 'examples_per_second': '31.616', 'grad_norm': '165', 'counters/examples': 106944, 'counters/updates': 3342}
train stats after 106976 examples: {'rewards_train/chosen': '0.037497', 'rewards_train/rejected': '-0.0027414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040238', 'logps_train/rejected': '-96.564', 'logps_train/chosen': '-136.26', 'loss/train': '0.68722', 'examples_per_second': '31.952', 'grad_norm': '56.25', 'counters/examples': 106976, 'counters/updates': 3343}
train stats after 107008 examples: {'rewards_train/chosen': '0.14025', 'rewards_train/rejected': '0.12312', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017122', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-178.11', 'loss/train': '0.69794', 'examples_per_second': '32.142', 'grad_norm': '148', 'counters/examples': 107008, 'counters/updates': 3344}
train stats after 107040 examples: {'rewards_train/chosen': '-0.014411', 'rewards_train/rejected': '0.083664', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.098075', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-139.97', 'loss/train': '0.74981', 'examples_per_second': '29.954', 'grad_norm': '85', 'counters/examples': 107040, 'counters/updates': 3345}
skipping logging after 107072 examples to avoid logging too frequently
train stats after 107104 examples: {'rewards_train/chosen': '0.070493', 'rewards_train/rejected': '0.076078', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0055859', 'logps_train/rejected': '-143.24', 'logps_train/chosen': '-131.71', 'loss/train': '0.71266', 'examples_per_second': '32.555', 'grad_norm': '70.5', 'counters/examples': 107104, 'counters/updates': 3347}
train stats after 107136 examples: {'rewards_train/chosen': '0.026863', 'rewards_train/rejected': '-0.022361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049224', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-142.63', 'loss/train': '0.67785', 'examples_per_second': '31.632', 'grad_norm': '125.5', 'counters/examples': 107136, 'counters/updates': 3348}
train stats after 107168 examples: {'rewards_train/chosen': '0.059248', 'rewards_train/rejected': '0.022295', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036952', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-173.74', 'loss/train': '0.69255', 'examples_per_second': '32.253', 'grad_norm': '83.5', 'counters/examples': 107168, 'counters/updates': 3349}
train stats after 107200 examples: {'rewards_train/chosen': '-0.02013', 'rewards_train/rejected': '0.022682', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042812', 'logps_train/rejected': '-123.71', 'logps_train/chosen': '-113.71', 'loss/train': '0.72143', 'examples_per_second': '31.744', 'grad_norm': '87.5', 'counters/examples': 107200, 'counters/updates': 3350}
skipping logging after 107232 examples to avoid logging too frequently
train stats after 107264 examples: {'rewards_train/chosen': '-0.015535', 'rewards_train/rejected': '-0.025844', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010308', 'logps_train/rejected': '-122.86', 'logps_train/chosen': '-132.91', 'loss/train': '0.69747', 'examples_per_second': '31.557', 'grad_norm': '81', 'counters/examples': 107264, 'counters/updates': 3352}
train stats after 107296 examples: {'rewards_train/chosen': '0.092726', 'rewards_train/rejected': '-0.045357', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13808', 'logps_train/rejected': '-157.95', 'logps_train/chosen': '-105.69', 'loss/train': '0.65422', 'examples_per_second': '30.082', 'grad_norm': '97.5', 'counters/examples': 107296, 'counters/updates': 3353}
train stats after 107328 examples: {'rewards_train/chosen': '0.016078', 'rewards_train/rejected': '0.083902', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.067825', 'logps_train/rejected': '-107.75', 'logps_train/chosen': '-136.43', 'loss/train': '0.73616', 'examples_per_second': '30.931', 'grad_norm': '106', 'counters/examples': 107328, 'counters/updates': 3354}
skipping logging after 107360 examples to avoid logging too frequently
train stats after 107392 examples: {'rewards_train/chosen': '0.05007', 'rewards_train/rejected': '0.067925', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017854', 'logps_train/rejected': '-156.28', 'logps_train/chosen': '-160.19', 'loss/train': '0.72078', 'examples_per_second': '31.638', 'grad_norm': '92.5', 'counters/examples': 107392, 'counters/updates': 3356}
train stats after 107424 examples: {'rewards_train/chosen': '0.057492', 'rewards_train/rejected': '0.085099', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027607', 'logps_train/rejected': '-166.45', 'logps_train/chosen': '-191.83', 'loss/train': '0.73847', 'examples_per_second': '31.63', 'grad_norm': '220', 'counters/examples': 107424, 'counters/updates': 3357}
train stats after 107456 examples: {'rewards_train/chosen': '0.13674', 'rewards_train/rejected': '0.027205', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10954', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-151.29', 'loss/train': '0.65509', 'examples_per_second': '31.408', 'grad_norm': '162', 'counters/examples': 107456, 'counters/updates': 3358}
train stats after 107488 examples: {'rewards_train/chosen': '0.047715', 'rewards_train/rejected': '0.090937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043222', 'logps_train/rejected': '-161.29', 'logps_train/chosen': '-132.62', 'loss/train': '0.73721', 'examples_per_second': '31.916', 'grad_norm': '284', 'counters/examples': 107488, 'counters/updates': 3359}
train stats after 107520 examples: {'rewards_train/chosen': '0.080937', 'rewards_train/rejected': '0.0024898', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078447', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-172.63', 'loss/train': '0.66418', 'examples_per_second': '31.615', 'grad_norm': '72', 'counters/examples': 107520, 'counters/updates': 3360}
train stats after 107552 examples: {'rewards_train/chosen': '0.086337', 'rewards_train/rejected': '0.039033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047303', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-178.53', 'loss/train': '0.68201', 'examples_per_second': '31.595', 'grad_norm': '138', 'counters/examples': 107552, 'counters/updates': 3361}
skipping logging after 107584 examples to avoid logging too frequently
train stats after 107616 examples: {'rewards_train/chosen': '0.10531', 'rewards_train/rejected': '0.044803', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060508', 'logps_train/rejected': '-154.99', 'logps_train/chosen': '-158.65', 'loss/train': '0.67403', 'examples_per_second': '31.625', 'grad_norm': '131', 'counters/examples': 107616, 'counters/updates': 3363}
skipping logging after 107648 examples to avoid logging too frequently
train stats after 107680 examples: {'rewards_train/chosen': '-0.00044686', 'rewards_train/rejected': '0.062424', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.062871', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-126.64', 'loss/train': '0.73766', 'examples_per_second': '31.873', 'grad_norm': '131', 'counters/examples': 107680, 'counters/updates': 3365}
train stats after 107712 examples: {'rewards_train/chosen': '0.036576', 'rewards_train/rejected': '0.069472', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032896', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-141.22', 'loss/train': '0.73189', 'examples_per_second': '31.378', 'grad_norm': '71.5', 'counters/examples': 107712, 'counters/updates': 3366}
train stats after 107744 examples: {'rewards_train/chosen': '0.035084', 'rewards_train/rejected': '0.1019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.06682', 'logps_train/rejected': '-170.88', 'logps_train/chosen': '-117.78', 'loss/train': '0.7443', 'examples_per_second': '31.589', 'grad_norm': '103.5', 'counters/examples': 107744, 'counters/updates': 3367}
train stats after 107776 examples: {'rewards_train/chosen': '0.12766', 'rewards_train/rejected': '-0.0035708', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13123', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-148.55', 'loss/train': '0.64477', 'examples_per_second': '32.597', 'grad_norm': '92', 'counters/examples': 107776, 'counters/updates': 3368}
skipping logging after 107808 examples to avoid logging too frequently
train stats after 107840 examples: {'rewards_train/chosen': '0.07098', 'rewards_train/rejected': '0.058464', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012516', 'logps_train/rejected': '-152.19', 'logps_train/chosen': '-121.69', 'loss/train': '0.70339', 'examples_per_second': '31.321', 'grad_norm': '104', 'counters/examples': 107840, 'counters/updates': 3370}
skipping logging after 107872 examples to avoid logging too frequently
train stats after 107904 examples: {'rewards_train/chosen': '0.03496', 'rewards_train/rejected': '0.022296', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012664', 'logps_train/rejected': '-144.69', 'logps_train/chosen': '-150.17', 'loss/train': '0.69828', 'examples_per_second': '31.551', 'grad_norm': '117', 'counters/examples': 107904, 'counters/updates': 3372}
train stats after 107936 examples: {'rewards_train/chosen': '0.0034971', 'rewards_train/rejected': '0.02183', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018333', 'logps_train/rejected': '-105.48', 'logps_train/chosen': '-135.08', 'loss/train': '0.71159', 'examples_per_second': '32.315', 'grad_norm': '121', 'counters/examples': 107936, 'counters/updates': 3373}
skipping logging after 107968 examples to avoid logging too frequently
train stats after 108000 examples: {'rewards_train/chosen': '0.088998', 'rewards_train/rejected': '0.056874', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.032124', 'logps_train/rejected': '-154.69', 'logps_train/chosen': '-157.39', 'loss/train': '0.69295', 'examples_per_second': '31.974', 'grad_norm': '101.5', 'counters/examples': 108000, 'counters/updates': 3375}
Running evaluation after 108000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.95it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]
eval after 108000: {'rewards_eval/chosen': '0.084501', 'rewards_eval/rejected': '0.032788', 'rewards_eval/accuracies': '0.52734', 'rewards_eval/margins': '0.051713', 'logps_eval/rejected': '-127.82', 'logps_eval/chosen': '-150.16', 'loss/eval': '0.67895'}
skipping save for non epoch
train stats after 108032 examples: {'rewards_train/chosen': '0.019628', 'rewards_train/rejected': '0.020959', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0013309', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-125.94', 'loss/train': '0.70085', 'examples_per_second': '34.084', 'grad_norm': '140', 'counters/examples': 108032, 'counters/updates': 3376}
skipping logging after 108064 examples to avoid logging too frequently
train stats after 108096 examples: {'rewards_train/chosen': '0.033092', 'rewards_train/rejected': '-0.0043566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037449', 'logps_train/rejected': '-130.87', 'logps_train/chosen': '-139.27', 'loss/train': '0.68695', 'examples_per_second': '30.665', 'grad_norm': '129', 'counters/examples': 108096, 'counters/updates': 3378}
skipping logging after 108128 examples to avoid logging too frequently
train stats after 108160 examples: {'rewards_train/chosen': '-0.020937', 'rewards_train/rejected': '-0.049069', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028132', 'logps_train/rejected': '-96.394', 'logps_train/chosen': '-157.02', 'loss/train': '0.68895', 'examples_per_second': '26.325', 'grad_norm': '110.5', 'counters/examples': 108160, 'counters/updates': 3380}
train stats after 108192 examples: {'rewards_train/chosen': '0.027749', 'rewards_train/rejected': '0.033316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0055673', 'logps_train/rejected': '-149.3', 'logps_train/chosen': '-164.86', 'loss/train': '0.70889', 'examples_per_second': '32.553', 'grad_norm': '170', 'counters/examples': 108192, 'counters/updates': 3381}
train stats after 108224 examples: {'rewards_train/chosen': '0.08968', 'rewards_train/rejected': '0.0033301', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08635', 'logps_train/rejected': '-192.89', 'logps_train/chosen': '-164.5', 'loss/train': '0.66878', 'examples_per_second': '31.648', 'grad_norm': '105.5', 'counters/examples': 108224, 'counters/updates': 3382}
train stats after 108256 examples: {'rewards_train/chosen': '0.014291', 'rewards_train/rejected': '0.049636', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035345', 'logps_train/rejected': '-146.7', 'logps_train/chosen': '-153.22', 'loss/train': '0.72595', 'examples_per_second': '24.203', 'grad_norm': '87.5', 'counters/examples': 108256, 'counters/updates': 3383}
skipping logging after 108288 examples to avoid logging too frequently
train stats after 108320 examples: {'rewards_train/chosen': '0.050558', 'rewards_train/rejected': '-0.014112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06467', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-132.05', 'loss/train': '0.69477', 'examples_per_second': '31.416', 'grad_norm': '182', 'counters/examples': 108320, 'counters/updates': 3385}
skipping logging after 108352 examples to avoid logging too frequently
train stats after 108384 examples: {'rewards_train/chosen': '0.080332', 'rewards_train/rejected': '0.023085', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057248', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-153.69', 'loss/train': '0.67593', 'examples_per_second': '30.207', 'grad_norm': '89.5', 'counters/examples': 108384, 'counters/updates': 3387}
skipping logging after 108416 examples to avoid logging too frequently
train stats after 108448 examples: {'rewards_train/chosen': '0.14569', 'rewards_train/rejected': '0.086103', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.059583', 'logps_train/rejected': '-158.54', 'logps_train/chosen': '-192.69', 'loss/train': '0.68491', 'examples_per_second': '30.058', 'grad_norm': '82.5', 'counters/examples': 108448, 'counters/updates': 3389}
train stats after 108480 examples: {'rewards_train/chosen': '0.072961', 'rewards_train/rejected': '0.03098', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041981', 'logps_train/rejected': '-140.25', 'logps_train/chosen': '-141.37', 'loss/train': '0.68244', 'examples_per_second': '30.92', 'grad_norm': '232', 'counters/examples': 108480, 'counters/updates': 3390}
train stats after 108512 examples: {'rewards_train/chosen': '0.1212', 'rewards_train/rejected': '0.048747', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072456', 'logps_train/rejected': '-167.89', 'logps_train/chosen': '-175.63', 'loss/train': '0.67759', 'examples_per_second': '31.307', 'grad_norm': '97', 'counters/examples': 108512, 'counters/updates': 3391}
skipping logging after 108544 examples to avoid logging too frequently
train stats after 108576 examples: {'rewards_train/chosen': '0.055891', 'rewards_train/rejected': '0.10717', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.051282', 'logps_train/rejected': '-163.95', 'logps_train/chosen': '-183.52', 'loss/train': '0.75493', 'examples_per_second': '31.663', 'grad_norm': '96.5', 'counters/examples': 108576, 'counters/updates': 3393}
train stats after 108608 examples: {'rewards_train/chosen': '0.053397', 'rewards_train/rejected': '0.087493', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.034097', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-145.25', 'loss/train': '0.71967', 'examples_per_second': '30.206', 'grad_norm': '888', 'counters/examples': 108608, 'counters/updates': 3394}
skipping logging after 108640 examples to avoid logging too frequently
train stats after 108672 examples: {'rewards_train/chosen': '0.12688', 'rewards_train/rejected': '0.13967', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012789', 'logps_train/rejected': '-133.08', 'logps_train/chosen': '-141.64', 'loss/train': '0.70854', 'examples_per_second': '32.296', 'grad_norm': '117.5', 'counters/examples': 108672, 'counters/updates': 3396}
train stats after 108704 examples: {'rewards_train/chosen': '0.075844', 'rewards_train/rejected': '0.069891', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0059531', 'logps_train/rejected': '-130.46', 'logps_train/chosen': '-146.01', 'loss/train': '0.70331', 'examples_per_second': '31.147', 'grad_norm': '118.5', 'counters/examples': 108704, 'counters/updates': 3397}
train stats after 108736 examples: {'rewards_train/chosen': '0.15904', 'rewards_train/rejected': '0.047087', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11196', 'logps_train/rejected': '-148.88', 'logps_train/chosen': '-171.61', 'loss/train': '0.64741', 'examples_per_second': '30.501', 'grad_norm': '94.5', 'counters/examples': 108736, 'counters/updates': 3398}
train stats after 108768 examples: {'rewards_train/chosen': '0.11295', 'rewards_train/rejected': '0.04382', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069129', 'logps_train/rejected': '-146.9', 'logps_train/chosen': '-165.21', 'loss/train': '0.67155', 'examples_per_second': '30.247', 'grad_norm': '87', 'counters/examples': 108768, 'counters/updates': 3399}
train stats after 108800 examples: {'rewards_train/chosen': '0.058168', 'rewards_train/rejected': '0.066846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0086775', 'logps_train/rejected': '-90.459', 'logps_train/chosen': '-113.98', 'loss/train': '0.70608', 'examples_per_second': '30.659', 'grad_norm': '72', 'counters/examples': 108800, 'counters/updates': 3400}
skipping logging after 108832 examples to avoid logging too frequently
train stats after 108864 examples: {'rewards_train/chosen': '0.015505', 'rewards_train/rejected': '0.02463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0091248', 'logps_train/rejected': '-89.576', 'logps_train/chosen': '-125.4', 'loss/train': '0.70242', 'examples_per_second': '31.652', 'grad_norm': '114.5', 'counters/examples': 108864, 'counters/updates': 3402}
train stats after 108896 examples: {'rewards_train/chosen': '0.12636', 'rewards_train/rejected': '0.0095984', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11676', 'logps_train/rejected': '-156.81', 'logps_train/chosen': '-180.4', 'loss/train': '0.64941', 'examples_per_second': '31.632', 'grad_norm': '81', 'counters/examples': 108896, 'counters/updates': 3403}
skipping logging after 108928 examples to avoid logging too frequently
train stats after 108960 examples: {'rewards_train/chosen': '0.011908', 'rewards_train/rejected': '0.024441', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012533', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-141.29', 'loss/train': '0.71392', 'examples_per_second': '36.059', 'grad_norm': '80.5', 'counters/examples': 108960, 'counters/updates': 3405}
train stats after 108992 examples: {'rewards_train/chosen': '0.029799', 'rewards_train/rejected': '0.013591', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016207', 'logps_train/rejected': '-152.43', 'logps_train/chosen': '-182.33', 'loss/train': '0.69954', 'examples_per_second': '30.694', 'grad_norm': '298', 'counters/examples': 108992, 'counters/updates': 3406}
train stats after 109024 examples: {'rewards_train/chosen': '0.071518', 'rewards_train/rejected': '0.046987', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024531', 'logps_train/rejected': '-125.01', 'logps_train/chosen': '-167.95', 'loss/train': '0.69073', 'examples_per_second': '33.14', 'grad_norm': '81.5', 'counters/examples': 109024, 'counters/updates': 3407}
train stats after 109056 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.0048353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11002', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-215.75', 'loss/train': '0.65271', 'examples_per_second': '31.641', 'grad_norm': '102', 'counters/examples': 109056, 'counters/updates': 3408}
train stats after 109088 examples: {'rewards_train/chosen': '0.055552', 'rewards_train/rejected': '-0.010936', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066488', 'logps_train/rejected': '-107.31', 'logps_train/chosen': '-159.77', 'loss/train': '0.67308', 'examples_per_second': '32.345', 'grad_norm': '131', 'counters/examples': 109088, 'counters/updates': 3409}
train stats after 109120 examples: {'rewards_train/chosen': '0.031476', 'rewards_train/rejected': '0.073684', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.042208', 'logps_train/rejected': '-123.66', 'logps_train/chosen': '-150.86', 'loss/train': '0.7222', 'examples_per_second': '32.003', 'grad_norm': '80.5', 'counters/examples': 109120, 'counters/updates': 3410}
train stats after 109152 examples: {'rewards_train/chosen': '-0.03224', 'rewards_train/rejected': '0.044114', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.076354', 'logps_train/rejected': '-96.576', 'logps_train/chosen': '-138.18', 'loss/train': '0.74467', 'examples_per_second': '32.987', 'grad_norm': '608', 'counters/examples': 109152, 'counters/updates': 3411}
skipping logging after 109184 examples to avoid logging too frequently
train stats after 109216 examples: {'rewards_train/chosen': '0.10527', 'rewards_train/rejected': '0.055289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049979', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-115.78', 'loss/train': '0.68173', 'examples_per_second': '33.156', 'grad_norm': '162', 'counters/examples': 109216, 'counters/updates': 3413}
skipping logging after 109248 examples to avoid logging too frequently
train stats after 109280 examples: {'rewards_train/chosen': '0.11704', 'rewards_train/rejected': '0.04773', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069308', 'logps_train/rejected': '-150.92', 'logps_train/chosen': '-169.26', 'loss/train': '0.68125', 'examples_per_second': '33.135', 'grad_norm': '95.5', 'counters/examples': 109280, 'counters/updates': 3415}
train stats after 109312 examples: {'rewards_train/chosen': '0.029006', 'rewards_train/rejected': '0.028539', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00046704', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-168.69', 'loss/train': '0.70668', 'examples_per_second': '31.708', 'grad_norm': '66.5', 'counters/examples': 109312, 'counters/updates': 3416}
train stats after 109344 examples: {'rewards_train/chosen': '0.060605', 'rewards_train/rejected': '0.0022344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058371', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-149.2', 'loss/train': '0.67222', 'examples_per_second': '30.506', 'grad_norm': '158', 'counters/examples': 109344, 'counters/updates': 3417}
train stats after 109376 examples: {'rewards_train/chosen': '0.03742', 'rewards_train/rejected': '0.068824', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031404', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-160.19', 'loss/train': '0.72184', 'examples_per_second': '31.661', 'grad_norm': '60.25', 'counters/examples': 109376, 'counters/updates': 3418}
train stats after 109408 examples: {'rewards_train/chosen': '-0.036912', 'rewards_train/rejected': '0.011045', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.047957', 'logps_train/rejected': '-148.47', 'logps_train/chosen': '-136.28', 'loss/train': '0.72785', 'examples_per_second': '30.934', 'grad_norm': '91.5', 'counters/examples': 109408, 'counters/updates': 3419}
train stats after 109440 examples: {'rewards_train/chosen': '0.20006', 'rewards_train/rejected': '0.049481', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15058', 'logps_train/rejected': '-147.48', 'logps_train/chosen': '-137.7', 'loss/train': '0.63829', 'examples_per_second': '31.426', 'grad_norm': '65.5', 'counters/examples': 109440, 'counters/updates': 3420}
train stats after 109472 examples: {'rewards_train/chosen': '0.055817', 'rewards_train/rejected': '-0.039711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095528', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-155.31', 'loss/train': '0.65337', 'examples_per_second': '31.799', 'grad_norm': '88.5', 'counters/examples': 109472, 'counters/updates': 3421}
train stats after 109504 examples: {'rewards_train/chosen': '-0.0054961', 'rewards_train/rejected': '0.00099278', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0064889', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-171.53', 'loss/train': '0.70719', 'examples_per_second': '32.746', 'grad_norm': '135', 'counters/examples': 109504, 'counters/updates': 3422}
train stats after 109536 examples: {'rewards_train/chosen': '0.13821', 'rewards_train/rejected': '0.1241', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01411', 'logps_train/rejected': '-156.6', 'logps_train/chosen': '-159.89', 'loss/train': '0.69745', 'examples_per_second': '31.331', 'grad_norm': '101', 'counters/examples': 109536, 'counters/updates': 3423}
skipping logging after 109568 examples to avoid logging too frequently
train stats after 109600 examples: {'rewards_train/chosen': '0.078621', 'rewards_train/rejected': '0.045454', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033167', 'logps_train/rejected': '-138.63', 'logps_train/chosen': '-158.44', 'loss/train': '0.69161', 'examples_per_second': '30.999', 'grad_norm': '117', 'counters/examples': 109600, 'counters/updates': 3425}
train stats after 109632 examples: {'rewards_train/chosen': '0.029131', 'rewards_train/rejected': '-0.042819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07195', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-145.48', 'loss/train': '0.66778', 'examples_per_second': '31.389', 'grad_norm': '86', 'counters/examples': 109632, 'counters/updates': 3426}
train stats after 109664 examples: {'rewards_train/chosen': '0.032468', 'rewards_train/rejected': '0.071502', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039034', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-176.99', 'loss/train': '0.72627', 'examples_per_second': '31.638', 'grad_norm': '152', 'counters/examples': 109664, 'counters/updates': 3427}
train stats after 109696 examples: {'rewards_train/chosen': '0.16175', 'rewards_train/rejected': '0.085386', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076361', 'logps_train/rejected': '-191.31', 'logps_train/chosen': '-188.66', 'loss/train': '0.67763', 'examples_per_second': '30.647', 'grad_norm': '98', 'counters/examples': 109696, 'counters/updates': 3428}
skipping logging after 109728 examples to avoid logging too frequently
train stats after 109760 examples: {'rewards_train/chosen': '0.022789', 'rewards_train/rejected': '0.079522', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.056733', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-126.23', 'loss/train': '0.74288', 'examples_per_second': '31.017', 'grad_norm': '131', 'counters/examples': 109760, 'counters/updates': 3430}
train stats after 109792 examples: {'rewards_train/chosen': '0.078273', 'rewards_train/rejected': '0.095678', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017406', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-130.61', 'loss/train': '0.70554', 'examples_per_second': '31.859', 'grad_norm': '106.5', 'counters/examples': 109792, 'counters/updates': 3431}
train stats after 109824 examples: {'rewards_train/chosen': '0.14008', 'rewards_train/rejected': '0.040206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099872', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-198.92', 'loss/train': '0.67084', 'examples_per_second': '31.624', 'grad_norm': '132', 'counters/examples': 109824, 'counters/updates': 3432}
train stats after 109856 examples: {'rewards_train/chosen': '0.082919', 'rewards_train/rejected': '0.014746', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068173', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-161.01', 'loss/train': '0.67593', 'examples_per_second': '30.772', 'grad_norm': '129', 'counters/examples': 109856, 'counters/updates': 3433}
train stats after 109888 examples: {'rewards_train/chosen': '0.16218', 'rewards_train/rejected': '-0.0054785', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16766', 'logps_train/rejected': '-149.81', 'logps_train/chosen': '-201', 'loss/train': '0.62486', 'examples_per_second': '32.411', 'grad_norm': '135', 'counters/examples': 109888, 'counters/updates': 3434}
train stats after 109920 examples: {'rewards_train/chosen': '0.048447', 'rewards_train/rejected': '0.054795', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0063478', 'logps_train/rejected': '-152.78', 'logps_train/chosen': '-176.51', 'loss/train': '0.70613', 'examples_per_second': '30.523', 'grad_norm': '107.5', 'counters/examples': 109920, 'counters/updates': 3435}
train stats after 109952 examples: {'rewards_train/chosen': '0.015035', 'rewards_train/rejected': '0.060908', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045873', 'logps_train/rejected': '-170.99', 'logps_train/chosen': '-142.28', 'loss/train': '0.72691', 'examples_per_second': '31.675', 'grad_norm': '83', 'counters/examples': 109952, 'counters/updates': 3436}
train stats after 109984 examples: {'rewards_train/chosen': '0.028118', 'rewards_train/rejected': '-0.015801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043919', 'logps_train/rejected': '-163.55', 'logps_train/chosen': '-126.57', 'loss/train': '0.68286', 'examples_per_second': '31.424', 'grad_norm': '300', 'counters/examples': 109984, 'counters/updates': 3437}
train stats after 110016 examples: {'rewards_train/chosen': '0.11224', 'rewards_train/rejected': '0.018984', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093254', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-132.78', 'loss/train': '0.66505', 'examples_per_second': '31.852', 'grad_norm': '74.5', 'counters/examples': 110016, 'counters/updates': 3438}
train stats after 110048 examples: {'rewards_train/chosen': '0.046563', 'rewards_train/rejected': '0.05447', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0079073', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-144.38', 'loss/train': '0.70418', 'examples_per_second': '31.373', 'grad_norm': '151', 'counters/examples': 110048, 'counters/updates': 3439}
train stats after 110080 examples: {'rewards_train/chosen': '-0.024188', 'rewards_train/rejected': '0.02808', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.052268', 'logps_train/rejected': '-126.64', 'logps_train/chosen': '-132.29', 'loss/train': '0.72873', 'examples_per_second': '30.292', 'grad_norm': '80.5', 'counters/examples': 110080, 'counters/updates': 3440}
train stats after 110112 examples: {'rewards_train/chosen': '0.033244', 'rewards_train/rejected': '-0.076274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10952', 'logps_train/rejected': '-146.22', 'logps_train/chosen': '-145.85', 'loss/train': '0.65548', 'examples_per_second': '31.68', 'grad_norm': '83.5', 'counters/examples': 110112, 'counters/updates': 3441}
train stats after 110144 examples: {'rewards_train/chosen': '0.018712', 'rewards_train/rejected': '-0.0090502', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027763', 'logps_train/rejected': '-107.81', 'logps_train/chosen': '-127.12', 'loss/train': '0.68666', 'examples_per_second': '31.609', 'grad_norm': '79.5', 'counters/examples': 110144, 'counters/updates': 3442}
train stats after 110176 examples: {'rewards_train/chosen': '0.07919', 'rewards_train/rejected': '0.03213', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04706', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-137.54', 'loss/train': '0.67773', 'examples_per_second': '30.122', 'grad_norm': '90', 'counters/examples': 110176, 'counters/updates': 3443}
train stats after 110208 examples: {'rewards_train/chosen': '0.13302', 'rewards_train/rejected': '0.07099', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062034', 'logps_train/rejected': '-113.4', 'logps_train/chosen': '-152.71', 'loss/train': '0.67971', 'examples_per_second': '31.112', 'grad_norm': '68.5', 'counters/examples': 110208, 'counters/updates': 3444}
train stats after 110240 examples: {'rewards_train/chosen': '0.14871', 'rewards_train/rejected': '0.035123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11359', 'logps_train/rejected': '-124.05', 'logps_train/chosen': '-155.94', 'loss/train': '0.66555', 'examples_per_second': '24.771', 'grad_norm': '100.5', 'counters/examples': 110240, 'counters/updates': 3445}
train stats after 110272 examples: {'rewards_train/chosen': '0.053587', 'rewards_train/rejected': '0.060791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0072042', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-152.47', 'loss/train': '0.70887', 'examples_per_second': '31.622', 'grad_norm': '80.5', 'counters/examples': 110272, 'counters/updates': 3446}
skipping logging after 110304 examples to avoid logging too frequently
train stats after 110336 examples: {'rewards_train/chosen': '-0.060046', 'rewards_train/rejected': '-0.051122', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0089242', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-128.11', 'loss/train': '0.70958', 'examples_per_second': '35.856', 'grad_norm': '73.5', 'counters/examples': 110336, 'counters/updates': 3448}
train stats after 110368 examples: {'rewards_train/chosen': '0.099548', 'rewards_train/rejected': '0.017678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081869', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-149.73', 'loss/train': '0.66387', 'examples_per_second': '30.3', 'grad_norm': '107', 'counters/examples': 110368, 'counters/updates': 3449}
train stats after 110400 examples: {'rewards_train/chosen': '0.041897', 'rewards_train/rejected': '0.061395', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019498', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-157.92', 'loss/train': '0.71781', 'examples_per_second': '31.29', 'grad_norm': '130', 'counters/examples': 110400, 'counters/updates': 3450}
train stats after 110432 examples: {'rewards_train/chosen': '0.024522', 'rewards_train/rejected': '-0.035475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059997', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-155.58', 'loss/train': '0.6749', 'examples_per_second': '31.581', 'grad_norm': '133', 'counters/examples': 110432, 'counters/updates': 3451}
train stats after 110464 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.034757', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093129', 'logps_train/rejected': '-116.73', 'logps_train/chosen': '-166.02', 'loss/train': '0.6663', 'examples_per_second': '32.626', 'grad_norm': '63.5', 'counters/examples': 110464, 'counters/updates': 3452}
skipping logging after 110496 examples to avoid logging too frequently
train stats after 110528 examples: {'rewards_train/chosen': '0.038905', 'rewards_train/rejected': '-0.01917', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058075', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-166.65', 'loss/train': '0.67506', 'examples_per_second': '31.661', 'grad_norm': '66', 'counters/examples': 110528, 'counters/updates': 3454}
train stats after 110560 examples: {'rewards_train/chosen': '0.031963', 'rewards_train/rejected': '0.094428', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.062465', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-146.57', 'loss/train': '0.73591', 'examples_per_second': '32.591', 'grad_norm': '440', 'counters/examples': 110560, 'counters/updates': 3455}
train stats after 110592 examples: {'rewards_train/chosen': '0.040258', 'rewards_train/rejected': '0.03778', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0024783', 'logps_train/rejected': '-164.3', 'logps_train/chosen': '-157.81', 'loss/train': '0.71087', 'examples_per_second': '31.783', 'grad_norm': '304', 'counters/examples': 110592, 'counters/updates': 3456}
skipping logging after 110624 examples to avoid logging too frequently
train stats after 110656 examples: {'rewards_train/chosen': '0.099087', 'rewards_train/rejected': '0.05058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048507', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-152.69', 'loss/train': '0.68117', 'examples_per_second': '31.605', 'grad_norm': '80.5', 'counters/examples': 110656, 'counters/updates': 3458}
train stats after 110688 examples: {'rewards_train/chosen': '-0.025026', 'rewards_train/rejected': '-0.010851', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014175', 'logps_train/rejected': '-158.5', 'logps_train/chosen': '-175.49', 'loss/train': '0.71239', 'examples_per_second': '31.49', 'grad_norm': '87.5', 'counters/examples': 110688, 'counters/updates': 3459}
skipping logging after 110720 examples to avoid logging too frequently
train stats after 110752 examples: {'rewards_train/chosen': '0.017842', 'rewards_train/rejected': '0.042005', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024163', 'logps_train/rejected': '-104.86', 'logps_train/chosen': '-105.49', 'loss/train': '0.71039', 'examples_per_second': '33.517', 'grad_norm': '116', 'counters/examples': 110752, 'counters/updates': 3461}
train stats after 110784 examples: {'rewards_train/chosen': '-0.019352', 'rewards_train/rejected': '-0.025157', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0058052', 'logps_train/rejected': '-119.8', 'logps_train/chosen': '-163.05', 'loss/train': '0.70174', 'examples_per_second': '32.897', 'grad_norm': '51', 'counters/examples': 110784, 'counters/updates': 3462}
train stats after 110816 examples: {'rewards_train/chosen': '0.0053648', 'rewards_train/rejected': '0.11647', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.1111', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-177.38', 'loss/train': '0.77223', 'examples_per_second': '31.495', 'grad_norm': '129', 'counters/examples': 110816, 'counters/updates': 3463}
skipping logging after 110848 examples to avoid logging too frequently
train stats after 110880 examples: {'rewards_train/chosen': '0.057666', 'rewards_train/rejected': '0.11018', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052511', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-153.3', 'loss/train': '0.72633', 'examples_per_second': '31.664', 'grad_norm': '60.75', 'counters/examples': 110880, 'counters/updates': 3465}
train stats after 110912 examples: {'rewards_train/chosen': '0.052163', 'rewards_train/rejected': '0.076168', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.024005', 'logps_train/rejected': '-133.78', 'logps_train/chosen': '-145.91', 'loss/train': '0.72006', 'examples_per_second': '31.672', 'grad_norm': '97', 'counters/examples': 110912, 'counters/updates': 3466}
train stats after 110944 examples: {'rewards_train/chosen': '0.064176', 'rewards_train/rejected': '-0.054141', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11832', 'logps_train/rejected': '-150.32', 'logps_train/chosen': '-130.92', 'loss/train': '0.6494', 'examples_per_second': '31.614', 'grad_norm': '65', 'counters/examples': 110944, 'counters/updates': 3467}
skipping logging after 110976 examples to avoid logging too frequently
train stats after 111008 examples: {'rewards_train/chosen': '0.11248', 'rewards_train/rejected': '0.014978', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097504', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-151.73', 'loss/train': '0.6552', 'examples_per_second': '32.581', 'grad_norm': '83.5', 'counters/examples': 111008, 'counters/updates': 3469}
train stats after 111040 examples: {'rewards_train/chosen': '0.018723', 'rewards_train/rejected': '0.11947', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10074', 'logps_train/rejected': '-138.3', 'logps_train/chosen': '-159.59', 'loss/train': '0.76487', 'examples_per_second': '31.008', 'grad_norm': '168', 'counters/examples': 111040, 'counters/updates': 3470}
train stats after 111072 examples: {'rewards_train/chosen': '0.07979', 'rewards_train/rejected': '0.0069125', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072877', 'logps_train/rejected': '-145.93', 'logps_train/chosen': '-147.28', 'loss/train': '0.67053', 'examples_per_second': '30.178', 'grad_norm': '117', 'counters/examples': 111072, 'counters/updates': 3471}
skipping logging after 111104 examples to avoid logging too frequently
train stats after 111136 examples: {'rewards_train/chosen': '0.070503', 'rewards_train/rejected': '0.074479', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0039758', 'logps_train/rejected': '-139.51', 'logps_train/chosen': '-141.14', 'loss/train': '0.70636', 'examples_per_second': '30.216', 'grad_norm': '117.5', 'counters/examples': 111136, 'counters/updates': 3473}
train stats after 111168 examples: {'rewards_train/chosen': '0.011848', 'rewards_train/rejected': '0.015773', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0039254', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-128.04', 'loss/train': '0.70999', 'examples_per_second': '31.649', 'grad_norm': '94.5', 'counters/examples': 111168, 'counters/updates': 3474}
train stats after 111200 examples: {'rewards_train/chosen': '0.050165', 'rewards_train/rejected': '0.07819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.028025', 'logps_train/rejected': '-158.67', 'logps_train/chosen': '-221.7', 'loss/train': '0.72214', 'examples_per_second': '31.677', 'grad_norm': '98', 'counters/examples': 111200, 'counters/updates': 3475}
train stats after 111232 examples: {'rewards_train/chosen': '0.12291', 'rewards_train/rejected': '0.000534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12238', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-151.05', 'loss/train': '0.64193', 'examples_per_second': '30.823', 'grad_norm': '75.5', 'counters/examples': 111232, 'counters/updates': 3476}
skipping logging after 111264 examples to avoid logging too frequently
train stats after 111296 examples: {'rewards_train/chosen': '0.078537', 'rewards_train/rejected': '0.057992', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.020546', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-131.17', 'loss/train': '0.68772', 'examples_per_second': '32.191', 'grad_norm': '69', 'counters/examples': 111296, 'counters/updates': 3478}
train stats after 111328 examples: {'rewards_train/chosen': '0.0649', 'rewards_train/rejected': '0.019501', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0454', 'logps_train/rejected': '-119.9', 'logps_train/chosen': '-178.46', 'loss/train': '0.68212', 'examples_per_second': '31.283', 'grad_norm': '73', 'counters/examples': 111328, 'counters/updates': 3479}
skipping logging after 111360 examples to avoid logging too frequently
train stats after 111392 examples: {'rewards_train/chosen': '0.11025', 'rewards_train/rejected': '0.014963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095285', 'logps_train/rejected': '-170.16', 'logps_train/chosen': '-149.98', 'loss/train': '0.66262', 'examples_per_second': '31.601', 'grad_norm': '86', 'counters/examples': 111392, 'counters/updates': 3481}
train stats after 111424 examples: {'rewards_train/chosen': '0.18786', 'rewards_train/rejected': '0.081497', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10636', 'logps_train/rejected': '-129.49', 'logps_train/chosen': '-177.88', 'loss/train': '0.65135', 'examples_per_second': '31.551', 'grad_norm': '80.5', 'counters/examples': 111424, 'counters/updates': 3482}
train stats after 111456 examples: {'rewards_train/chosen': '-0.017293', 'rewards_train/rejected': '0.02326', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040553', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-176.42', 'loss/train': '0.72243', 'examples_per_second': '30.688', 'grad_norm': '124', 'counters/examples': 111456, 'counters/updates': 3483}
skipping logging after 111488 examples to avoid logging too frequently
train stats after 111520 examples: {'rewards_train/chosen': '0.051673', 'rewards_train/rejected': '-0.0028913', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054564', 'logps_train/rejected': '-96.044', 'logps_train/chosen': '-154.3', 'loss/train': '0.67887', 'examples_per_second': '31.655', 'grad_norm': '71.5', 'counters/examples': 111520, 'counters/updates': 3485}
train stats after 111552 examples: {'rewards_train/chosen': '0.035716', 'rewards_train/rejected': '0.053535', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017819', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-127.98', 'loss/train': '0.71006', 'examples_per_second': '32.8', 'grad_norm': '75', 'counters/examples': 111552, 'counters/updates': 3486}
train stats after 111584 examples: {'rewards_train/chosen': '0.070817', 'rewards_train/rejected': '-0.026266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097083', 'logps_train/rejected': '-161.49', 'logps_train/chosen': '-136.7', 'loss/train': '0.66109', 'examples_per_second': '30.144', 'grad_norm': '81.5', 'counters/examples': 111584, 'counters/updates': 3487}
train stats after 111616 examples: {'rewards_train/chosen': '0.10653', 'rewards_train/rejected': '-0.037905', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14444', 'logps_train/rejected': '-94.731', 'logps_train/chosen': '-166.32', 'loss/train': '0.63963', 'examples_per_second': '30.271', 'grad_norm': '103', 'counters/examples': 111616, 'counters/updates': 3488}
train stats after 111648 examples: {'rewards_train/chosen': '0.028119', 'rewards_train/rejected': '0.026832', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0012866', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-114.64', 'loss/train': '0.70538', 'examples_per_second': '32.386', 'grad_norm': '57', 'counters/examples': 111648, 'counters/updates': 3489}
train stats after 111680 examples: {'rewards_train/chosen': '0.06271', 'rewards_train/rejected': '-0.0071236', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069833', 'logps_train/rejected': '-89.522', 'logps_train/chosen': '-158.39', 'loss/train': '0.66534', 'examples_per_second': '32.099', 'grad_norm': '75', 'counters/examples': 111680, 'counters/updates': 3490}
train stats after 111712 examples: {'rewards_train/chosen': '0.049688', 'rewards_train/rejected': '0.0014644', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048223', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-160.77', 'loss/train': '0.67971', 'examples_per_second': '30.114', 'grad_norm': '322', 'counters/examples': 111712, 'counters/updates': 3491}
train stats after 111744 examples: {'rewards_train/chosen': '0.04841', 'rewards_train/rejected': '0.013927', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034483', 'logps_train/rejected': '-107.48', 'logps_train/chosen': '-165.08', 'loss/train': '0.68569', 'examples_per_second': '30.103', 'grad_norm': '97', 'counters/examples': 111744, 'counters/updates': 3492}
train stats after 111776 examples: {'rewards_train/chosen': '0.10333', 'rewards_train/rejected': '0.0021429', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10119', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-127.2', 'loss/train': '0.65345', 'examples_per_second': '31.407', 'grad_norm': '236', 'counters/examples': 111776, 'counters/updates': 3493}
skipping logging after 111808 examples to avoid logging too frequently
train stats after 111840 examples: {'rewards_train/chosen': '0.081761', 'rewards_train/rejected': '0.048413', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033348', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-158.02', 'loss/train': '0.68915', 'examples_per_second': '30.551', 'grad_norm': '52', 'counters/examples': 111840, 'counters/updates': 3495}
train stats after 111872 examples: {'rewards_train/chosen': '0.087308', 'rewards_train/rejected': '0.034445', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.052863', 'logps_train/rejected': '-93.816', 'logps_train/chosen': '-133.09', 'loss/train': '0.68358', 'examples_per_second': '31.438', 'grad_norm': '69', 'counters/examples': 111872, 'counters/updates': 3496}
train stats after 111904 examples: {'rewards_train/chosen': '0.088255', 'rewards_train/rejected': '0.070768', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017487', 'logps_train/rejected': '-139.58', 'logps_train/chosen': '-130.81', 'loss/train': '0.69196', 'examples_per_second': '32.484', 'grad_norm': '79', 'counters/examples': 111904, 'counters/updates': 3497}
train stats after 111936 examples: {'rewards_train/chosen': '0.061925', 'rewards_train/rejected': '0.014028', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047896', 'logps_train/rejected': '-141.23', 'logps_train/chosen': '-124.39', 'loss/train': '0.68526', 'examples_per_second': '31.52', 'grad_norm': '68.5', 'counters/examples': 111936, 'counters/updates': 3498}
train stats after 111968 examples: {'rewards_train/chosen': '0.032697', 'rewards_train/rejected': '-0.063812', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09651', 'logps_train/rejected': '-151.28', 'logps_train/chosen': '-159.99', 'loss/train': '0.66185', 'examples_per_second': '32.105', 'grad_norm': '163', 'counters/examples': 111968, 'counters/updates': 3499}
train stats after 112000 examples: {'rewards_train/chosen': '0.040789', 'rewards_train/rejected': '0.14023', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.09944', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-125.25', 'loss/train': '0.75844', 'examples_per_second': '30.445', 'grad_norm': '90', 'counters/examples': 112000, 'counters/updates': 3500}
train stats after 112032 examples: {'rewards_train/chosen': '0.1046', 'rewards_train/rejected': '-0.0056271', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11022', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-172.57', 'loss/train': '0.66256', 'examples_per_second': '31.636', 'grad_norm': '71.5', 'counters/examples': 112032, 'counters/updates': 3501}
train stats after 112064 examples: {'rewards_train/chosen': '0.10892', 'rewards_train/rejected': '-0.034099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14302', 'logps_train/rejected': '-171.71', 'logps_train/chosen': '-201.7', 'loss/train': '0.64069', 'examples_per_second': '30.336', 'grad_norm': '124.5', 'counters/examples': 112064, 'counters/updates': 3502}
train stats after 112096 examples: {'rewards_train/chosen': '0.11132', 'rewards_train/rejected': '0.070628', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040695', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-158.98', 'loss/train': '0.68086', 'examples_per_second': '31.086', 'grad_norm': '78.5', 'counters/examples': 112096, 'counters/updates': 3503}
train stats after 112128 examples: {'rewards_train/chosen': '0.09236', 'rewards_train/rejected': '0.060724', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031635', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-166.01', 'loss/train': '0.68331', 'examples_per_second': '31.154', 'grad_norm': '86.5', 'counters/examples': 112128, 'counters/updates': 3504}
train stats after 112160 examples: {'rewards_train/chosen': '0.059558', 'rewards_train/rejected': '0.10898', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.049421', 'logps_train/rejected': '-154.57', 'logps_train/chosen': '-207.09', 'loss/train': '0.73723', 'examples_per_second': '31.666', 'grad_norm': '96', 'counters/examples': 112160, 'counters/updates': 3505}
train stats after 112192 examples: {'rewards_train/chosen': '0.18575', 'rewards_train/rejected': '0.071491', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11425', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-177.12', 'loss/train': '0.65188', 'examples_per_second': '32.011', 'grad_norm': '122.5', 'counters/examples': 112192, 'counters/updates': 3506}
train stats after 112224 examples: {'rewards_train/chosen': '0.094989', 'rewards_train/rejected': '0.030062', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064927', 'logps_train/rejected': '-133.22', 'logps_train/chosen': '-147.32', 'loss/train': '0.67102', 'examples_per_second': '31.576', 'grad_norm': '105', 'counters/examples': 112224, 'counters/updates': 3507}
train stats after 112256 examples: {'rewards_train/chosen': '0.015007', 'rewards_train/rejected': '0.0368', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021792', 'logps_train/rejected': '-141.45', 'logps_train/chosen': '-174.76', 'loss/train': '0.71682', 'examples_per_second': '33.32', 'grad_norm': '120', 'counters/examples': 112256, 'counters/updates': 3508}
train stats after 112288 examples: {'rewards_train/chosen': '0.024481', 'rewards_train/rejected': '0.055616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.031135', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-123.53', 'loss/train': '0.73196', 'examples_per_second': '32.485', 'grad_norm': '97', 'counters/examples': 112288, 'counters/updates': 3509}
train stats after 112320 examples: {'rewards_train/chosen': '0.10264', 'rewards_train/rejected': '0.096951', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0056934', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-139.54', 'loss/train': '0.71029', 'examples_per_second': '30.423', 'grad_norm': '82', 'counters/examples': 112320, 'counters/updates': 3510}
train stats after 112352 examples: {'rewards_train/chosen': '0.068453', 'rewards_train/rejected': '0.032207', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036246', 'logps_train/rejected': '-99.453', 'logps_train/chosen': '-118.47', 'loss/train': '0.68448', 'examples_per_second': '32.118', 'grad_norm': '61.5', 'counters/examples': 112352, 'counters/updates': 3511}
skipping logging after 112384 examples to avoid logging too frequently
train stats after 112416 examples: {'rewards_train/chosen': '0.14504', 'rewards_train/rejected': '-0.01251', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15755', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-173.23', 'loss/train': '0.63869', 'examples_per_second': '31.632', 'grad_norm': '99', 'counters/examples': 112416, 'counters/updates': 3513}
train stats after 112448 examples: {'rewards_train/chosen': '0.085715', 'rewards_train/rejected': '0.10972', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024001', 'logps_train/rejected': '-164.28', 'logps_train/chosen': '-175.92', 'loss/train': '0.7221', 'examples_per_second': '31.66', 'grad_norm': '190', 'counters/examples': 112448, 'counters/updates': 3514}
train stats after 112480 examples: {'rewards_train/chosen': '0.11128', 'rewards_train/rejected': '-0.027584', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13886', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-136.37', 'loss/train': '0.63565', 'examples_per_second': '32.876', 'grad_norm': '118', 'counters/examples': 112480, 'counters/updates': 3515}
train stats after 112512 examples: {'rewards_train/chosen': '0.056733', 'rewards_train/rejected': '0.013702', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043031', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-151.25', 'loss/train': '0.67747', 'examples_per_second': '30.014', 'grad_norm': '98', 'counters/examples': 112512, 'counters/updates': 3516}
train stats after 112544 examples: {'rewards_train/chosen': '-0.0046765', 'rewards_train/rejected': '0.012615', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017291', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-155.37', 'loss/train': '0.73639', 'examples_per_second': '31.847', 'grad_norm': '119', 'counters/examples': 112544, 'counters/updates': 3517}
train stats after 112576 examples: {'rewards_train/chosen': '0.011154', 'rewards_train/rejected': '0.059419', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.048265', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-148.14', 'loss/train': '0.72743', 'examples_per_second': '31.183', 'grad_norm': '150', 'counters/examples': 112576, 'counters/updates': 3518}
train stats after 112608 examples: {'rewards_train/chosen': '0.080408', 'rewards_train/rejected': '0.037191', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043218', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-153.91', 'loss/train': '0.69451', 'examples_per_second': '29.813', 'grad_norm': '90.5', 'counters/examples': 112608, 'counters/updates': 3519}
train stats after 112640 examples: {'rewards_train/chosen': '0.058765', 'rewards_train/rejected': '0.022367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036398', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-153.46', 'loss/train': '0.69024', 'examples_per_second': '31.652', 'grad_norm': '93', 'counters/examples': 112640, 'counters/updates': 3520}
train stats after 112672 examples: {'rewards_train/chosen': '0.096312', 'rewards_train/rejected': '0.053509', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042803', 'logps_train/rejected': '-147.13', 'logps_train/chosen': '-167.76', 'loss/train': '0.68294', 'examples_per_second': '30.575', 'grad_norm': '90', 'counters/examples': 112672, 'counters/updates': 3521}
skipping logging after 112704 examples to avoid logging too frequently
train stats after 112736 examples: {'rewards_train/chosen': '0.098606', 'rewards_train/rejected': '0.059617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038989', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-152.91', 'loss/train': '0.67958', 'examples_per_second': '33.901', 'grad_norm': '73.5', 'counters/examples': 112736, 'counters/updates': 3523}
train stats after 112768 examples: {'rewards_train/chosen': '-0.03752', 'rewards_train/rejected': '0.09913', 'rewards_train/accuracies': '0.25', 'rewards_train/margins': '-0.13665', 'logps_train/rejected': '-152', 'logps_train/chosen': '-125.09', 'loss/train': '0.77047', 'examples_per_second': '31.244', 'grad_norm': '159', 'counters/examples': 112768, 'counters/updates': 3524}
skipping logging after 112800 examples to avoid logging too frequently
train stats after 112832 examples: {'rewards_train/chosen': '0.038829', 'rewards_train/rejected': '0.070646', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031817', 'logps_train/rejected': '-169.04', 'logps_train/chosen': '-160.3', 'loss/train': '0.72748', 'examples_per_second': '31.167', 'grad_norm': '114.5', 'counters/examples': 112832, 'counters/updates': 3526}
skipping logging after 112864 examples to avoid logging too frequently
train stats after 112896 examples: {'rewards_train/chosen': '0.11277', 'rewards_train/rejected': '0.069909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042863', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-189.19', 'loss/train': '0.68504', 'examples_per_second': '31.944', 'grad_norm': '145', 'counters/examples': 112896, 'counters/updates': 3528}
skipping logging after 112928 examples to avoid logging too frequently
train stats after 112960 examples: {'rewards_train/chosen': '0.084375', 'rewards_train/rejected': '0.054023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030353', 'logps_train/rejected': '-178.15', 'logps_train/chosen': '-152.58', 'loss/train': '0.68874', 'examples_per_second': '31.17', 'grad_norm': '71.5', 'counters/examples': 112960, 'counters/updates': 3530}
train stats after 112992 examples: {'rewards_train/chosen': '0.060083', 'rewards_train/rejected': '0.0011743', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058908', 'logps_train/rejected': '-142.01', 'logps_train/chosen': '-156.41', 'loss/train': '0.67545', 'examples_per_second': '30.134', 'grad_norm': '58.25', 'counters/examples': 112992, 'counters/updates': 3531}
train stats after 113024 examples: {'rewards_train/chosen': '0.083923', 'rewards_train/rejected': '0.080939', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0029836', 'logps_train/rejected': '-160.53', 'logps_train/chosen': '-216.36', 'loss/train': '0.71989', 'examples_per_second': '31.608', 'grad_norm': '137', 'counters/examples': 113024, 'counters/updates': 3532}
train stats after 113056 examples: {'rewards_train/chosen': '-0.010349', 'rewards_train/rejected': '0.068803', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.079152', 'logps_train/rejected': '-109.27', 'logps_train/chosen': '-126.23', 'loss/train': '0.74825', 'examples_per_second': '30.058', 'grad_norm': '156', 'counters/examples': 113056, 'counters/updates': 3533}
train stats after 113088 examples: {'rewards_train/chosen': '0.075005', 'rewards_train/rejected': '-0.027168', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10217', 'logps_train/rejected': '-145.77', 'logps_train/chosen': '-170.63', 'loss/train': '0.66225', 'examples_per_second': '31.54', 'grad_norm': '74', 'counters/examples': 113088, 'counters/updates': 3534}
train stats after 113120 examples: {'rewards_train/chosen': '0.11868', 'rewards_train/rejected': '0.029551', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089127', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-147.35', 'loss/train': '0.66292', 'examples_per_second': '30.185', 'grad_norm': '63.5', 'counters/examples': 113120, 'counters/updates': 3535}
train stats after 113152 examples: {'rewards_train/chosen': '0.13581', 'rewards_train/rejected': '0.017355', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11845', 'logps_train/rejected': '-160.73', 'logps_train/chosen': '-186.94', 'loss/train': '0.65955', 'examples_per_second': '31.531', 'grad_norm': '126', 'counters/examples': 113152, 'counters/updates': 3536}
train stats after 113184 examples: {'rewards_train/chosen': '0.070183', 'rewards_train/rejected': '0.049046', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021137', 'logps_train/rejected': '-155.79', 'logps_train/chosen': '-158.26', 'loss/train': '0.69699', 'examples_per_second': '32.204', 'grad_norm': '81', 'counters/examples': 113184, 'counters/updates': 3537}
train stats after 113216 examples: {'rewards_train/chosen': '0.013407', 'rewards_train/rejected': '0.049155', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035748', 'logps_train/rejected': '-153.87', 'logps_train/chosen': '-182.74', 'loss/train': '0.72573', 'examples_per_second': '30.971', 'grad_norm': '133', 'counters/examples': 113216, 'counters/updates': 3538}
train stats after 113248 examples: {'rewards_train/chosen': '0.026973', 'rewards_train/rejected': '0.053804', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.026832', 'logps_train/rejected': '-140.79', 'logps_train/chosen': '-135.19', 'loss/train': '0.72448', 'examples_per_second': '31.653', 'grad_norm': '63.75', 'counters/examples': 113248, 'counters/updates': 3539}
skipping logging after 113280 examples to avoid logging too frequently
train stats after 113312 examples: {'rewards_train/chosen': '0.10167', 'rewards_train/rejected': '0.034103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067564', 'logps_train/rejected': '-122', 'logps_train/chosen': '-159.37', 'loss/train': '0.67506', 'examples_per_second': '31.675', 'grad_norm': '69.5', 'counters/examples': 113312, 'counters/updates': 3541}
skipping logging after 113344 examples to avoid logging too frequently
train stats after 113376 examples: {'rewards_train/chosen': '0.077274', 'rewards_train/rejected': '0.020272', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057002', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-147.72', 'loss/train': '0.67291', 'examples_per_second': '31.611', 'grad_norm': '113.5', 'counters/examples': 113376, 'counters/updates': 3543}
train stats after 113408 examples: {'rewards_train/chosen': '0.074496', 'rewards_train/rejected': '0.074631', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00013503', 'logps_train/rejected': '-115.92', 'logps_train/chosen': '-161.04', 'loss/train': '0.70595', 'examples_per_second': '30.236', 'grad_norm': '96', 'counters/examples': 113408, 'counters/updates': 3544}
train stats after 113440 examples: {'rewards_train/chosen': '0.14175', 'rewards_train/rejected': '0.037789', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10396', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-163.54', 'loss/train': '0.66564', 'examples_per_second': '30.63', 'grad_norm': '65', 'counters/examples': 113440, 'counters/updates': 3545}
train stats after 113472 examples: {'rewards_train/chosen': '0.019483', 'rewards_train/rejected': '0.061764', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042281', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-161.89', 'loss/train': '0.73165', 'examples_per_second': '30.706', 'grad_norm': '87.5', 'counters/examples': 113472, 'counters/updates': 3546}
train stats after 113504 examples: {'rewards_train/chosen': '0.11114', 'rewards_train/rejected': '0.048367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062777', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-146.21', 'loss/train': '0.67233', 'examples_per_second': '33.13', 'grad_norm': '60.25', 'counters/examples': 113504, 'counters/updates': 3547}
train stats after 113536 examples: {'rewards_train/chosen': '0.083107', 'rewards_train/rejected': '-0.016793', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0999', 'logps_train/rejected': '-155.87', 'logps_train/chosen': '-185.74', 'loss/train': '0.67252', 'examples_per_second': '31.4', 'grad_norm': '160', 'counters/examples': 113536, 'counters/updates': 3548}
train stats after 113568 examples: {'rewards_train/chosen': '-0.0081398', 'rewards_train/rejected': '0.054456', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.062596', 'logps_train/rejected': '-124.45', 'logps_train/chosen': '-124.45', 'loss/train': '0.7381', 'examples_per_second': '31.495', 'grad_norm': '71.5', 'counters/examples': 113568, 'counters/updates': 3549}
train stats after 113600 examples: {'rewards_train/chosen': '0.032435', 'rewards_train/rejected': '0.015715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01672', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-132.98', 'loss/train': '0.70717', 'examples_per_second': '31.079', 'grad_norm': '74', 'counters/examples': 113600, 'counters/updates': 3550}
train stats after 113632 examples: {'rewards_train/chosen': '0.16845', 'rewards_train/rejected': '-0.060294', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22874', 'logps_train/rejected': '-153.35', 'logps_train/chosen': '-186.88', 'loss/train': '0.60689', 'examples_per_second': '24.473', 'grad_norm': '120', 'counters/examples': 113632, 'counters/updates': 3551}
train stats after 113664 examples: {'rewards_train/chosen': '0.016823', 'rewards_train/rejected': '0.013237', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0035861', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-171.47', 'loss/train': '0.70643', 'examples_per_second': '31.996', 'grad_norm': '95', 'counters/examples': 113664, 'counters/updates': 3552}
train stats after 113696 examples: {'rewards_train/chosen': '0.05748', 'rewards_train/rejected': '0.027151', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030329', 'logps_train/rejected': '-84.587', 'logps_train/chosen': '-122.5', 'loss/train': '0.68887', 'examples_per_second': '32.583', 'grad_norm': '120.5', 'counters/examples': 113696, 'counters/updates': 3553}
train stats after 113728 examples: {'rewards_train/chosen': '0.049458', 'rewards_train/rejected': '0.086052', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036594', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-131.76', 'loss/train': '0.72136', 'examples_per_second': '25.089', 'grad_norm': '81', 'counters/examples': 113728, 'counters/updates': 3554}
train stats after 113760 examples: {'rewards_train/chosen': '0.097955', 'rewards_train/rejected': '0.076515', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021439', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-132.92', 'loss/train': '0.70351', 'examples_per_second': '32.449', 'grad_norm': '70', 'counters/examples': 113760, 'counters/updates': 3555}
train stats after 113792 examples: {'rewards_train/chosen': '0.066978', 'rewards_train/rejected': '0.01818', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048798', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-166.87', 'loss/train': '0.6788', 'examples_per_second': '31.239', 'grad_norm': '95.5', 'counters/examples': 113792, 'counters/updates': 3556}
skipping logging after 113824 examples to avoid logging too frequently
train stats after 113856 examples: {'rewards_train/chosen': '0.12001', 'rewards_train/rejected': '0.17248', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.05247', 'logps_train/rejected': '-166.55', 'logps_train/chosen': '-170.97', 'loss/train': '0.73495', 'examples_per_second': '31.741', 'grad_norm': '97.5', 'counters/examples': 113856, 'counters/updates': 3558}
train stats after 113888 examples: {'rewards_train/chosen': '0.047278', 'rewards_train/rejected': '0.14602', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.098738', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-172.07', 'loss/train': '0.75817', 'examples_per_second': '31.317', 'grad_norm': '103.5', 'counters/examples': 113888, 'counters/updates': 3559}
train stats after 113920 examples: {'rewards_train/chosen': '0.066064', 'rewards_train/rejected': '0.062534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0035296', 'logps_train/rejected': '-152.14', 'logps_train/chosen': '-152.5', 'loss/train': '0.70458', 'examples_per_second': '32.413', 'grad_norm': '81', 'counters/examples': 113920, 'counters/updates': 3560}
skipping logging after 113952 examples to avoid logging too frequently
train stats after 113984 examples: {'rewards_train/chosen': '0.071905', 'rewards_train/rejected': '-0.053715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12562', 'logps_train/rejected': '-144.7', 'logps_train/chosen': '-135.59', 'loss/train': '0.65181', 'examples_per_second': '30.816', 'grad_norm': '92', 'counters/examples': 113984, 'counters/updates': 3562}
skipping logging after 114016 examples to avoid logging too frequently
train stats after 114048 examples: {'rewards_train/chosen': '0.058491', 'rewards_train/rejected': '0.03046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02803', 'logps_train/rejected': '-104.98', 'logps_train/chosen': '-117', 'loss/train': '0.68659', 'examples_per_second': '34.329', 'grad_norm': '51', 'counters/examples': 114048, 'counters/updates': 3564}
skipping logging after 114080 examples to avoid logging too frequently
train stats after 114112 examples: {'rewards_train/chosen': '0.036547', 'rewards_train/rejected': '-0.026168', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062715', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-137.97', 'loss/train': '0.67017', 'examples_per_second': '32.784', 'grad_norm': '52.75', 'counters/examples': 114112, 'counters/updates': 3566}
skipping logging after 114144 examples to avoid logging too frequently
train stats after 114176 examples: {'rewards_train/chosen': '0.041776', 'rewards_train/rejected': '0.033351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0084242', 'logps_train/rejected': '-131.88', 'logps_train/chosen': '-156.81', 'loss/train': '0.6977', 'examples_per_second': '34.604', 'grad_norm': '75', 'counters/examples': 114176, 'counters/updates': 3568}
train stats after 114208 examples: {'rewards_train/chosen': '0.10242', 'rewards_train/rejected': '0.06204', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040375', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-109.96', 'loss/train': '0.67881', 'examples_per_second': '32.572', 'grad_norm': '63.75', 'counters/examples': 114208, 'counters/updates': 3569}
train stats after 114240 examples: {'rewards_train/chosen': '0.12576', 'rewards_train/rejected': '0.0063222', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11944', 'logps_train/rejected': '-166.91', 'logps_train/chosen': '-153.98', 'loss/train': '0.64614', 'examples_per_second': '30.162', 'grad_norm': '193', 'counters/examples': 114240, 'counters/updates': 3570}
train stats after 114272 examples: {'rewards_train/chosen': '0.017601', 'rewards_train/rejected': '0.036595', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018994', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-135.11', 'loss/train': '0.71076', 'examples_per_second': '30.847', 'grad_norm': '115', 'counters/examples': 114272, 'counters/updates': 3571}
train stats after 114304 examples: {'rewards_train/chosen': '0.087861', 'rewards_train/rejected': '0.0073031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080558', 'logps_train/rejected': '-104.55', 'logps_train/chosen': '-131.32', 'loss/train': '0.66501', 'examples_per_second': '30.084', 'grad_norm': '93', 'counters/examples': 114304, 'counters/updates': 3572}
train stats after 114336 examples: {'rewards_train/chosen': '0.12507', 'rewards_train/rejected': '-0.045174', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17024', 'logps_train/rejected': '-117.4', 'logps_train/chosen': '-159.59', 'loss/train': '0.63198', 'examples_per_second': '31.834', 'grad_norm': '61', 'counters/examples': 114336, 'counters/updates': 3573}
train stats after 114368 examples: {'rewards_train/chosen': '0.062138', 'rewards_train/rejected': '0.022499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039639', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-161.62', 'loss/train': '0.68924', 'examples_per_second': '31.637', 'grad_norm': '57.5', 'counters/examples': 114368, 'counters/updates': 3574}
train stats after 114400 examples: {'rewards_train/chosen': '0.08875', 'rewards_train/rejected': '0.020662', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068088', 'logps_train/rejected': '-117.4', 'logps_train/chosen': '-142.98', 'loss/train': '0.67389', 'examples_per_second': '32.711', 'grad_norm': '131', 'counters/examples': 114400, 'counters/updates': 3575}
train stats after 114432 examples: {'rewards_train/chosen': '0.058454', 'rewards_train/rejected': '-0.034993', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093447', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-161.69', 'loss/train': '0.66763', 'examples_per_second': '31.677', 'grad_norm': '162', 'counters/examples': 114432, 'counters/updates': 3576}
train stats after 114464 examples: {'rewards_train/chosen': '0.063853', 'rewards_train/rejected': '0.014871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048982', 'logps_train/rejected': '-163.68', 'logps_train/chosen': '-143.45', 'loss/train': '0.68073', 'examples_per_second': '30.151', 'grad_norm': '156', 'counters/examples': 114464, 'counters/updates': 3577}
train stats after 114496 examples: {'rewards_train/chosen': '0.12043', 'rewards_train/rejected': '0.07657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043864', 'logps_train/rejected': '-136.01', 'logps_train/chosen': '-172.18', 'loss/train': '0.68533', 'examples_per_second': '30.816', 'grad_norm': '86.5', 'counters/examples': 114496, 'counters/updates': 3578}
skipping logging after 114528 examples to avoid logging too frequently
train stats after 114560 examples: {'rewards_train/chosen': '0.072182', 'rewards_train/rejected': '0.023832', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.04835', 'logps_train/rejected': '-152.21', 'logps_train/chosen': '-217.15', 'loss/train': '0.68345', 'examples_per_second': '31.371', 'grad_norm': '98', 'counters/examples': 114560, 'counters/updates': 3580}
train stats after 114592 examples: {'rewards_train/chosen': '0.093469', 'rewards_train/rejected': '0.032453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061016', 'logps_train/rejected': '-110.79', 'logps_train/chosen': '-156.99', 'loss/train': '0.6739', 'examples_per_second': '31.758', 'grad_norm': '115.5', 'counters/examples': 114592, 'counters/updates': 3581}
skipping logging after 114624 examples to avoid logging too frequently
train stats after 114656 examples: {'rewards_train/chosen': '0.17649', 'rewards_train/rejected': '0.10925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067237', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-131.57', 'loss/train': '0.6768', 'examples_per_second': '32.45', 'grad_norm': '125.5', 'counters/examples': 114656, 'counters/updates': 3583}
train stats after 114688 examples: {'rewards_train/chosen': '0.01494', 'rewards_train/rejected': '0.0082986', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0066418', 'logps_train/rejected': '-152.19', 'logps_train/chosen': '-157.64', 'loss/train': '0.70209', 'examples_per_second': '31.392', 'grad_norm': '71.5', 'counters/examples': 114688, 'counters/updates': 3584}
train stats after 114720 examples: {'rewards_train/chosen': '0.0023023', 'rewards_train/rejected': '-0.011144', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013446', 'logps_train/rejected': '-150.67', 'logps_train/chosen': '-154.76', 'loss/train': '0.6976', 'examples_per_second': '31.599', 'grad_norm': '69.5', 'counters/examples': 114720, 'counters/updates': 3585}
train stats after 114752 examples: {'rewards_train/chosen': '0.15982', 'rewards_train/rejected': '-0.027727', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18755', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-164.12', 'loss/train': '0.61831', 'examples_per_second': '31.884', 'grad_norm': '94', 'counters/examples': 114752, 'counters/updates': 3586}
train stats after 114784 examples: {'rewards_train/chosen': '0.033675', 'rewards_train/rejected': '0.083894', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.050219', 'logps_train/rejected': '-122.63', 'logps_train/chosen': '-160.64', 'loss/train': '0.73155', 'examples_per_second': '31.638', 'grad_norm': '77', 'counters/examples': 114784, 'counters/updates': 3587}
train stats after 114816 examples: {'rewards_train/chosen': '0.14195', 'rewards_train/rejected': '0.079856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062095', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-197.69', 'loss/train': '0.68503', 'examples_per_second': '31.737', 'grad_norm': '145', 'counters/examples': 114816, 'counters/updates': 3588}
skipping logging after 114848 examples to avoid logging too frequently
train stats after 114880 examples: {'rewards_train/chosen': '0.16788', 'rewards_train/rejected': '0.023429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14445', 'logps_train/rejected': '-125.1', 'logps_train/chosen': '-142.1', 'loss/train': '0.64965', 'examples_per_second': '32.34', 'grad_norm': '92.5', 'counters/examples': 114880, 'counters/updates': 3590}
train stats after 114912 examples: {'rewards_train/chosen': '0.011866', 'rewards_train/rejected': '0.054087', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.042221', 'logps_train/rejected': '-146.52', 'logps_train/chosen': '-182.99', 'loss/train': '0.73589', 'examples_per_second': '30.062', 'grad_norm': '80', 'counters/examples': 114912, 'counters/updates': 3591}
train stats after 114944 examples: {'rewards_train/chosen': '0.060802', 'rewards_train/rejected': '0.031993', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02881', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-169.91', 'loss/train': '0.69966', 'examples_per_second': '32.486', 'grad_norm': '74', 'counters/examples': 114944, 'counters/updates': 3592}
train stats after 114976 examples: {'rewards_train/chosen': '-0.039989', 'rewards_train/rejected': '0.037691', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077681', 'logps_train/rejected': '-116.31', 'logps_train/chosen': '-130.47', 'loss/train': '0.74474', 'examples_per_second': '31.178', 'grad_norm': '174', 'counters/examples': 114976, 'counters/updates': 3593}
train stats after 115008 examples: {'rewards_train/chosen': '0.04685', 'rewards_train/rejected': '0.029124', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017727', 'logps_train/rejected': '-121.83', 'logps_train/chosen': '-131.72', 'loss/train': '0.69995', 'examples_per_second': '31.138', 'grad_norm': '101.5', 'counters/examples': 115008, 'counters/updates': 3594}
skipping logging after 115040 examples to avoid logging too frequently
train stats after 115072 examples: {'rewards_train/chosen': '0.027546', 'rewards_train/rejected': '0.039851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012305', 'logps_train/rejected': '-138.11', 'logps_train/chosen': '-163.91', 'loss/train': '0.73054', 'examples_per_second': '31.298', 'grad_norm': '145', 'counters/examples': 115072, 'counters/updates': 3596}
train stats after 115104 examples: {'rewards_train/chosen': '0.080392', 'rewards_train/rejected': '0.028724', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051668', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-119.16', 'loss/train': '0.67845', 'examples_per_second': '30.469', 'grad_norm': '72.5', 'counters/examples': 115104, 'counters/updates': 3597}
skipping logging after 115136 examples to avoid logging too frequently
train stats after 115168 examples: {'rewards_train/chosen': '0.08712', 'rewards_train/rejected': '0.0060733', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081047', 'logps_train/rejected': '-117.22', 'logps_train/chosen': '-148.55', 'loss/train': '0.66561', 'examples_per_second': '31.889', 'grad_norm': '71.5', 'counters/examples': 115168, 'counters/updates': 3599}
skipping logging after 115200 examples to avoid logging too frequently
train stats after 115232 examples: {'rewards_train/chosen': '0.075743', 'rewards_train/rejected': '0.036274', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.039468', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-122.86', 'loss/train': '0.68713', 'examples_per_second': '33.811', 'grad_norm': '108.5', 'counters/examples': 115232, 'counters/updates': 3601}
skipping logging after 115264 examples to avoid logging too frequently
train stats after 115296 examples: {'rewards_train/chosen': '0.083917', 'rewards_train/rejected': '0.039049', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044868', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-159.08', 'loss/train': '0.68602', 'examples_per_second': '31.925', 'grad_norm': '146', 'counters/examples': 115296, 'counters/updates': 3603}
train stats after 115328 examples: {'rewards_train/chosen': '0.0713', 'rewards_train/rejected': '0.020091', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051209', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-110.64', 'loss/train': '0.67407', 'examples_per_second': '31.553', 'grad_norm': '86.5', 'counters/examples': 115328, 'counters/updates': 3604}
train stats after 115360 examples: {'rewards_train/chosen': '0.063293', 'rewards_train/rejected': '0.023843', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03945', 'logps_train/rejected': '-120.5', 'logps_train/chosen': '-177.11', 'loss/train': '0.69213', 'examples_per_second': '32.527', 'grad_norm': '89', 'counters/examples': 115360, 'counters/updates': 3605}
skipping logging after 115392 examples to avoid logging too frequently
train stats after 115424 examples: {'rewards_train/chosen': '0.054753', 'rewards_train/rejected': '-0.047067', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10182', 'logps_train/rejected': '-134.86', 'logps_train/chosen': '-148.5', 'loss/train': '0.65567', 'examples_per_second': '30.098', 'grad_norm': '71.5', 'counters/examples': 115424, 'counters/updates': 3607}
train stats after 115456 examples: {'rewards_train/chosen': '0.18557', 'rewards_train/rejected': '0.074904', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11066', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-160.23', 'loss/train': '0.6497', 'examples_per_second': '30.342', 'grad_norm': '80', 'counters/examples': 115456, 'counters/updates': 3608}
skipping logging after 115488 examples to avoid logging too frequently
train stats after 115520 examples: {'rewards_train/chosen': '-0.017422', 'rewards_train/rejected': '0.14139', 'rewards_train/accuracies': '0.21875', 'rewards_train/margins': '-0.15881', 'logps_train/rejected': '-160.93', 'logps_train/chosen': '-184.46', 'loss/train': '0.78678', 'examples_per_second': '30.337', 'grad_norm': '116', 'counters/examples': 115520, 'counters/updates': 3610}
train stats after 115552 examples: {'rewards_train/chosen': '0.11368', 'rewards_train/rejected': '0.019147', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094533', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-172.69', 'loss/train': '0.6719', 'examples_per_second': '32.38', 'grad_norm': '142', 'counters/examples': 115552, 'counters/updates': 3611}
skipping logging after 115584 examples to avoid logging too frequently
train stats after 115616 examples: {'rewards_train/chosen': '0.099925', 'rewards_train/rejected': '0.0035562', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096368', 'logps_train/rejected': '-148.41', 'logps_train/chosen': '-145.22', 'loss/train': '0.66205', 'examples_per_second': '31.382', 'grad_norm': '116.5', 'counters/examples': 115616, 'counters/updates': 3613}
train stats after 115648 examples: {'rewards_train/chosen': '-0.018327', 'rewards_train/rejected': '0.0041779', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022505', 'logps_train/rejected': '-118.25', 'logps_train/chosen': '-129.92', 'loss/train': '0.71297', 'examples_per_second': '31.43', 'grad_norm': '72.5', 'counters/examples': 115648, 'counters/updates': 3614}
train stats after 115680 examples: {'rewards_train/chosen': '0.046502', 'rewards_train/rejected': '0.07878', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032278', 'logps_train/rejected': '-143.27', 'logps_train/chosen': '-171.83', 'loss/train': '0.72462', 'examples_per_second': '31.251', 'grad_norm': '102.5', 'counters/examples': 115680, 'counters/updates': 3615}
train stats after 115712 examples: {'rewards_train/chosen': '0.077809', 'rewards_train/rejected': '-0.027212', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10502', 'logps_train/rejected': '-132.92', 'logps_train/chosen': '-152.37', 'loss/train': '0.65904', 'examples_per_second': '30.236', 'grad_norm': '104.5', 'counters/examples': 115712, 'counters/updates': 3616}
train stats after 115744 examples: {'rewards_train/chosen': '0.043813', 'rewards_train/rejected': '0.069906', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026093', 'logps_train/rejected': '-118.15', 'logps_train/chosen': '-113.9', 'loss/train': '0.72181', 'examples_per_second': '31.236', 'grad_norm': '66', 'counters/examples': 115744, 'counters/updates': 3617}
train stats after 115776 examples: {'rewards_train/chosen': '0.055468', 'rewards_train/rejected': '-0.01951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074978', 'logps_train/rejected': '-154.01', 'logps_train/chosen': '-210.12', 'loss/train': '0.68485', 'examples_per_second': '31.625', 'grad_norm': '82.5', 'counters/examples': 115776, 'counters/updates': 3618}
train stats after 115808 examples: {'rewards_train/chosen': '0.048881', 'rewards_train/rejected': '-0.02428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073161', 'logps_train/rejected': '-176.74', 'logps_train/chosen': '-170.71', 'loss/train': '0.68207', 'examples_per_second': '24.661', 'grad_norm': '245', 'counters/examples': 115808, 'counters/updates': 3619}
train stats after 115840 examples: {'rewards_train/chosen': '0.066973', 'rewards_train/rejected': '0.066802', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00017131', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-145.51', 'loss/train': '0.70367', 'examples_per_second': '32.348', 'grad_norm': '123', 'counters/examples': 115840, 'counters/updates': 3620}
skipping logging after 115872 examples to avoid logging too frequently
train stats after 115904 examples: {'rewards_train/chosen': '0.025767', 'rewards_train/rejected': '0.041238', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015471', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-189.34', 'loss/train': '0.71142', 'examples_per_second': '31.607', 'grad_norm': '112.5', 'counters/examples': 115904, 'counters/updates': 3622}
skipping logging after 115936 examples to avoid logging too frequently
train stats after 115968 examples: {'rewards_train/chosen': '-0.0070537', 'rewards_train/rejected': '-0.026959', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019905', 'logps_train/rejected': '-117.94', 'logps_train/chosen': '-163.44', 'loss/train': '0.70083', 'examples_per_second': '30.159', 'grad_norm': '77.5', 'counters/examples': 115968, 'counters/updates': 3624}
train stats after 116000 examples: {'rewards_train/chosen': '-0.025084', 'rewards_train/rejected': '-0.0070753', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018009', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-153.68', 'loss/train': '0.71647', 'examples_per_second': '30.9', 'grad_norm': '87.5', 'counters/examples': 116000, 'counters/updates': 3625}
train stats after 116032 examples: {'rewards_train/chosen': '0.13469', 'rewards_train/rejected': '0.057119', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077572', 'logps_train/rejected': '-140.48', 'logps_train/chosen': '-129.93', 'loss/train': '0.66928', 'examples_per_second': '30.447', 'grad_norm': '137', 'counters/examples': 116032, 'counters/updates': 3626}
train stats after 116064 examples: {'rewards_train/chosen': '-0.03566', 'rewards_train/rejected': '0.043014', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.078674', 'logps_train/rejected': '-119.61', 'logps_train/chosen': '-134.51', 'loss/train': '0.74791', 'examples_per_second': '31.655', 'grad_norm': '85.5', 'counters/examples': 116064, 'counters/updates': 3627}
train stats after 116096 examples: {'rewards_train/chosen': '0.078039', 'rewards_train/rejected': '-0.014932', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092972', 'logps_train/rejected': '-161.81', 'logps_train/chosen': '-165.81', 'loss/train': '0.6607', 'examples_per_second': '31.402', 'grad_norm': '132', 'counters/examples': 116096, 'counters/updates': 3628}
skipping logging after 116128 examples to avoid logging too frequently
train stats after 116160 examples: {'rewards_train/chosen': '0.13067', 'rewards_train/rejected': '0.095825', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034848', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-136.34', 'loss/train': '0.6832', 'examples_per_second': '30.648', 'grad_norm': '90', 'counters/examples': 116160, 'counters/updates': 3630}
train stats after 116192 examples: {'rewards_train/chosen': '0.089976', 'rewards_train/rejected': '0.066676', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0233', 'logps_train/rejected': '-155.64', 'logps_train/chosen': '-143.34', 'loss/train': '0.69258', 'examples_per_second': '30.107', 'grad_norm': '73', 'counters/examples': 116192, 'counters/updates': 3631}
train stats after 116224 examples: {'rewards_train/chosen': '0.052655', 'rewards_train/rejected': '0.045313', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0073414', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-169.28', 'loss/train': '0.70115', 'examples_per_second': '30.747', 'grad_norm': '86', 'counters/examples': 116224, 'counters/updates': 3632}
skipping logging after 116256 examples to avoid logging too frequently
train stats after 116288 examples: {'rewards_train/chosen': '0.062042', 'rewards_train/rejected': '0.11454', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052498', 'logps_train/rejected': '-148.04', 'logps_train/chosen': '-151.89', 'loss/train': '0.73178', 'examples_per_second': '30.116', 'grad_norm': '103.5', 'counters/examples': 116288, 'counters/updates': 3634}
train stats after 116320 examples: {'rewards_train/chosen': '0.041291', 'rewards_train/rejected': '0.028909', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012382', 'logps_train/rejected': '-134.94', 'logps_train/chosen': '-155.26', 'loss/train': '0.70635', 'examples_per_second': '31.138', 'grad_norm': '103.5', 'counters/examples': 116320, 'counters/updates': 3635}
train stats after 116352 examples: {'rewards_train/chosen': '0.030881', 'rewards_train/rejected': '0.03686', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0059797', 'logps_train/rejected': '-104.77', 'logps_train/chosen': '-134.07', 'loss/train': '0.7086', 'examples_per_second': '32.061', 'grad_norm': '56.5', 'counters/examples': 116352, 'counters/updates': 3636}
train stats after 116384 examples: {'rewards_train/chosen': '0.06702', 'rewards_train/rejected': '-0.072276', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1393', 'logps_train/rejected': '-166.69', 'logps_train/chosen': '-139.07', 'loss/train': '0.64316', 'examples_per_second': '29.85', 'grad_norm': '58.75', 'counters/examples': 116384, 'counters/updates': 3637}
train stats after 116416 examples: {'rewards_train/chosen': '0.019693', 'rewards_train/rejected': '0.017079', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0026147', 'logps_train/rejected': '-146.26', 'logps_train/chosen': '-179.04', 'loss/train': '0.70392', 'examples_per_second': '31.609', 'grad_norm': '134', 'counters/examples': 116416, 'counters/updates': 3638}
train stats after 116448 examples: {'rewards_train/chosen': '0.051171', 'rewards_train/rejected': '0.062092', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010921', 'logps_train/rejected': '-134.07', 'logps_train/chosen': '-138.08', 'loss/train': '0.71116', 'examples_per_second': '31.971', 'grad_norm': '67', 'counters/examples': 116448, 'counters/updates': 3639}
train stats after 116480 examples: {'rewards_train/chosen': '0.14157', 'rewards_train/rejected': '0.059641', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08193', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-153.14', 'loss/train': '0.66654', 'examples_per_second': '30.538', 'grad_norm': '135', 'counters/examples': 116480, 'counters/updates': 3640}
skipping logging after 116512 examples to avoid logging too frequently
train stats after 116544 examples: {'rewards_train/chosen': '0.062215', 'rewards_train/rejected': '0.047199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015016', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-174.11', 'loss/train': '0.69693', 'examples_per_second': '30.131', 'grad_norm': '219', 'counters/examples': 116544, 'counters/updates': 3642}
train stats after 116576 examples: {'rewards_train/chosen': '0.071095', 'rewards_train/rejected': '-0.0032601', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074355', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-131.72', 'loss/train': '0.6628', 'examples_per_second': '31.286', 'grad_norm': '256', 'counters/examples': 116576, 'counters/updates': 3643}
train stats after 116608 examples: {'rewards_train/chosen': '0.078438', 'rewards_train/rejected': '-0.021669', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10011', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-124.95', 'loss/train': '0.66191', 'examples_per_second': '31.398', 'grad_norm': '55.5', 'counters/examples': 116608, 'counters/updates': 3644}
train stats after 116640 examples: {'rewards_train/chosen': '0.032626', 'rewards_train/rejected': '0.0318', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00082623', 'logps_train/rejected': '-119.91', 'logps_train/chosen': '-154.37', 'loss/train': '0.70266', 'examples_per_second': '31.614', 'grad_norm': '73.5', 'counters/examples': 116640, 'counters/updates': 3645}
train stats after 116672 examples: {'rewards_train/chosen': '0.030079', 'rewards_train/rejected': '-0.039613', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069692', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-124.31', 'loss/train': '0.66586', 'examples_per_second': '32.56', 'grad_norm': '95.5', 'counters/examples': 116672, 'counters/updates': 3646}
skipping logging after 116704 examples to avoid logging too frequently
train stats after 116736 examples: {'rewards_train/chosen': '0.065338', 'rewards_train/rejected': '-0.033267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098605', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-151.59', 'loss/train': '0.65282', 'examples_per_second': '30.696', 'grad_norm': '46.75', 'counters/examples': 116736, 'counters/updates': 3648}
train stats after 116768 examples: {'rewards_train/chosen': '0.042523', 'rewards_train/rejected': '0.042995', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00047256', 'logps_train/rejected': '-152.78', 'logps_train/chosen': '-123', 'loss/train': '0.70028', 'examples_per_second': '31.321', 'grad_norm': '90', 'counters/examples': 116768, 'counters/updates': 3649}
skipping logging after 116800 examples to avoid logging too frequently
train stats after 116832 examples: {'rewards_train/chosen': '0.06095', 'rewards_train/rejected': '-0.01364', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074591', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-172.27', 'loss/train': '0.67485', 'examples_per_second': '31.058', 'grad_norm': '133', 'counters/examples': 116832, 'counters/updates': 3651}
train stats after 116864 examples: {'rewards_train/chosen': '0.02977', 'rewards_train/rejected': '0.099217', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.069448', 'logps_train/rejected': '-141.23', 'logps_train/chosen': '-110.03', 'loss/train': '0.74218', 'examples_per_second': '31.639', 'grad_norm': '66.5', 'counters/examples': 116864, 'counters/updates': 3652}
train stats after 116896 examples: {'rewards_train/chosen': '0.053273', 'rewards_train/rejected': '0.088376', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035103', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-148.95', 'loss/train': '0.71934', 'examples_per_second': '30.148', 'grad_norm': '294', 'counters/examples': 116896, 'counters/updates': 3653}
train stats after 116928 examples: {'rewards_train/chosen': '0.023831', 'rewards_train/rejected': '-0.017797', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041628', 'logps_train/rejected': '-143.27', 'logps_train/chosen': '-149.25', 'loss/train': '0.69094', 'examples_per_second': '31.098', 'grad_norm': '72', 'counters/examples': 116928, 'counters/updates': 3654}
train stats after 116960 examples: {'rewards_train/chosen': '-0.0074165', 'rewards_train/rejected': '0.058493', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06591', 'logps_train/rejected': '-168.14', 'logps_train/chosen': '-144.48', 'loss/train': '0.73529', 'examples_per_second': '31.163', 'grad_norm': '72.5', 'counters/examples': 116960, 'counters/updates': 3655}
train stats after 116992 examples: {'rewards_train/chosen': '0.012', 'rewards_train/rejected': '0.036939', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.024939', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-179.57', 'loss/train': '0.71637', 'examples_per_second': '31.455', 'grad_norm': '91.5', 'counters/examples': 116992, 'counters/updates': 3656}
train stats after 117024 examples: {'rewards_train/chosen': '0.15145', 'rewards_train/rejected': '-0.011894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16334', 'logps_train/rejected': '-109.63', 'logps_train/chosen': '-156.04', 'loss/train': '0.62947', 'examples_per_second': '32.17', 'grad_norm': '87.5', 'counters/examples': 117024, 'counters/updates': 3657}
train stats after 117056 examples: {'rewards_train/chosen': '0.060592', 'rewards_train/rejected': '-0.0060663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066658', 'logps_train/rejected': '-110.1', 'logps_train/chosen': '-146.21', 'loss/train': '0.68213', 'examples_per_second': '31.593', 'grad_norm': '55.25', 'counters/examples': 117056, 'counters/updates': 3658}
train stats after 117088 examples: {'rewards_train/chosen': '0.091017', 'rewards_train/rejected': '-0.02923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12025', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-126.89', 'loss/train': '0.65588', 'examples_per_second': '31.076', 'grad_norm': '115.5', 'counters/examples': 117088, 'counters/updates': 3659}
train stats after 117120 examples: {'rewards_train/chosen': '0.045023', 'rewards_train/rejected': '0.086325', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041302', 'logps_train/rejected': '-153.58', 'logps_train/chosen': '-171.25', 'loss/train': '0.72207', 'examples_per_second': '32.846', 'grad_norm': '79', 'counters/examples': 117120, 'counters/updates': 3660}
skipping logging after 117152 examples to avoid logging too frequently
train stats after 117184 examples: {'rewards_train/chosen': '0.048683', 'rewards_train/rejected': '0.037668', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011016', 'logps_train/rejected': '-127.93', 'logps_train/chosen': '-124.63', 'loss/train': '0.69583', 'examples_per_second': '30.703', 'grad_norm': '137', 'counters/examples': 117184, 'counters/updates': 3662}
train stats after 117216 examples: {'rewards_train/chosen': '0.062948', 'rewards_train/rejected': '0.029214', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033735', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-148.36', 'loss/train': '0.68493', 'examples_per_second': '32.05', 'grad_norm': '169', 'counters/examples': 117216, 'counters/updates': 3663}
train stats after 117248 examples: {'rewards_train/chosen': '0.017949', 'rewards_train/rejected': '-0.0023812', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02033', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-124.34', 'loss/train': '0.69607', 'examples_per_second': '30.325', 'grad_norm': '121.5', 'counters/examples': 117248, 'counters/updates': 3664}
train stats after 117280 examples: {'rewards_train/chosen': '0.083972', 'rewards_train/rejected': '-0.029488', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11346', 'logps_train/rejected': '-154.23', 'logps_train/chosen': '-176.82', 'loss/train': '0.65473', 'examples_per_second': '30.05', 'grad_norm': '95', 'counters/examples': 117280, 'counters/updates': 3665}
train stats after 117312 examples: {'rewards_train/chosen': '0.0041807', 'rewards_train/rejected': '0.034228', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030048', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-145.69', 'loss/train': '0.71664', 'examples_per_second': '31.546', 'grad_norm': '65.5', 'counters/examples': 117312, 'counters/updates': 3666}
train stats after 117344 examples: {'rewards_train/chosen': '0.14836', 'rewards_train/rejected': '-0.012774', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16114', 'logps_train/rejected': '-107.81', 'logps_train/chosen': '-146.65', 'loss/train': '0.64354', 'examples_per_second': '30.318', 'grad_norm': '80', 'counters/examples': 117344, 'counters/updates': 3667}
train stats after 117376 examples: {'rewards_train/chosen': '0.083534', 'rewards_train/rejected': '0.021397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062137', 'logps_train/rejected': '-134.33', 'logps_train/chosen': '-187.16', 'loss/train': '0.67102', 'examples_per_second': '32.295', 'grad_norm': '82.5', 'counters/examples': 117376, 'counters/updates': 3668}
skipping logging after 117408 examples to avoid logging too frequently
train stats after 117440 examples: {'rewards_train/chosen': '0.026914', 'rewards_train/rejected': '-0.015971', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042885', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-124.73', 'loss/train': '0.68134', 'examples_per_second': '31.581', 'grad_norm': '63', 'counters/examples': 117440, 'counters/updates': 3670}
train stats after 117472 examples: {'rewards_train/chosen': '0.050311', 'rewards_train/rejected': '0.012782', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037528', 'logps_train/rejected': '-157.05', 'logps_train/chosen': '-164.78', 'loss/train': '0.68625', 'examples_per_second': '31.623', 'grad_norm': '98', 'counters/examples': 117472, 'counters/updates': 3671}
skipping logging after 117504 examples to avoid logging too frequently
train stats after 117536 examples: {'rewards_train/chosen': '0.034595', 'rewards_train/rejected': '0.047416', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012821', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-156.93', 'loss/train': '0.71155', 'examples_per_second': '32.715', 'grad_norm': '68', 'counters/examples': 117536, 'counters/updates': 3673}
train stats after 117568 examples: {'rewards_train/chosen': '-0.019781', 'rewards_train/rejected': '-0.02645', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0066692', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-120.41', 'loss/train': '0.69955', 'examples_per_second': '32.906', 'grad_norm': '96', 'counters/examples': 117568, 'counters/updates': 3674}
train stats after 117600 examples: {'rewards_train/chosen': '0.076635', 'rewards_train/rejected': '0.076619', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '1.6109e-05', 'logps_train/rejected': '-110', 'logps_train/chosen': '-174.5', 'loss/train': '0.7046', 'examples_per_second': '31.754', 'grad_norm': '82.5', 'counters/examples': 117600, 'counters/updates': 3675}
skipping logging after 117632 examples to avoid logging too frequently
train stats after 117664 examples: {'rewards_train/chosen': '0.033972', 'rewards_train/rejected': '0.026716', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0072555', 'logps_train/rejected': '-107.83', 'logps_train/chosen': '-120.62', 'loss/train': '0.70551', 'examples_per_second': '30.645', 'grad_norm': '75.5', 'counters/examples': 117664, 'counters/updates': 3677}
train stats after 117696 examples: {'rewards_train/chosen': '-0.023948', 'rewards_train/rejected': '0.070092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.094039', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-178.93', 'loss/train': '0.76091', 'examples_per_second': '30.551', 'grad_norm': '94', 'counters/examples': 117696, 'counters/updates': 3678}
skipping logging after 117728 examples to avoid logging too frequently
train stats after 117760 examples: {'rewards_train/chosen': '0.084611', 'rewards_train/rejected': '-0.037202', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12181', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-133.9', 'loss/train': '0.65008', 'examples_per_second': '31.554', 'grad_norm': '58.25', 'counters/examples': 117760, 'counters/updates': 3680}
skipping logging after 117792 examples to avoid logging too frequently
train stats after 117824 examples: {'rewards_train/chosen': '0.045396', 'rewards_train/rejected': '0.01861', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026786', 'logps_train/rejected': '-113.01', 'logps_train/chosen': '-148.77', 'loss/train': '0.68851', 'examples_per_second': '30.527', 'grad_norm': '54.75', 'counters/examples': 117824, 'counters/updates': 3682}
skipping logging after 117856 examples to avoid logging too frequently
train stats after 117888 examples: {'rewards_train/chosen': '0.085895', 'rewards_train/rejected': '0.070158', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015738', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-165.71', 'loss/train': '0.70569', 'examples_per_second': '30.53', 'grad_norm': '176', 'counters/examples': 117888, 'counters/updates': 3684}
train stats after 117920 examples: {'rewards_train/chosen': '0.044245', 'rewards_train/rejected': '0.05236', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0081154', 'logps_train/rejected': '-157.48', 'logps_train/chosen': '-129.48', 'loss/train': '0.71037', 'examples_per_second': '31.649', 'grad_norm': '89.5', 'counters/examples': 117920, 'counters/updates': 3685}
train stats after 117952 examples: {'rewards_train/chosen': '0.016473', 'rewards_train/rejected': '0.028485', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012012', 'logps_train/rejected': '-116.22', 'logps_train/chosen': '-129.91', 'loss/train': '0.71206', 'examples_per_second': '32.118', 'grad_norm': '68', 'counters/examples': 117952, 'counters/updates': 3686}
train stats after 117984 examples: {'rewards_train/chosen': '0.082144', 'rewards_train/rejected': '0.099503', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.017359', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-126.11', 'loss/train': '0.72333', 'examples_per_second': '32.146', 'grad_norm': '69.5', 'counters/examples': 117984, 'counters/updates': 3687}
train stats after 118016 examples: {'rewards_train/chosen': '0.12694', 'rewards_train/rejected': '-0.0042828', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13122', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-133.51', 'loss/train': '0.66002', 'examples_per_second': '31.739', 'grad_norm': '75', 'counters/examples': 118016, 'counters/updates': 3688}
train stats after 118048 examples: {'rewards_train/chosen': '0.043597', 'rewards_train/rejected': '0.00062998', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042967', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-116.23', 'loss/train': '0.69564', 'examples_per_second': '31.663', 'grad_norm': '72.5', 'counters/examples': 118048, 'counters/updates': 3689}
train stats after 118080 examples: {'rewards_train/chosen': '0.034257', 'rewards_train/rejected': '0.02965', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0046075', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-156.56', 'loss/train': '0.70228', 'examples_per_second': '30.106', 'grad_norm': '70', 'counters/examples': 118080, 'counters/updates': 3690}
train stats after 118112 examples: {'rewards_train/chosen': '-0.012081', 'rewards_train/rejected': '0.053769', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.06585', 'logps_train/rejected': '-150.72', 'logps_train/chosen': '-151.04', 'loss/train': '0.73865', 'examples_per_second': '30.669', 'grad_norm': '78.5', 'counters/examples': 118112, 'counters/updates': 3691}
skipping logging after 118144 examples to avoid logging too frequently
train stats after 118176 examples: {'rewards_train/chosen': '-0.0072035', 'rewards_train/rejected': '-0.029969', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.022765', 'logps_train/rejected': '-152.03', 'logps_train/chosen': '-111.58', 'loss/train': '0.70172', 'examples_per_second': '31.701', 'grad_norm': '102', 'counters/examples': 118176, 'counters/updates': 3693}
train stats after 118208 examples: {'rewards_train/chosen': '0.08816', 'rewards_train/rejected': '0.17683', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.088667', 'logps_train/rejected': '-168.81', 'logps_train/chosen': '-179.63', 'loss/train': '0.76719', 'examples_per_second': '31.645', 'grad_norm': '125.5', 'counters/examples': 118208, 'counters/updates': 3694}
skipping logging after 118240 examples to avoid logging too frequently
train stats after 118272 examples: {'rewards_train/chosen': '0.088004', 'rewards_train/rejected': '-0.018681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10668', 'logps_train/rejected': '-102.1', 'logps_train/chosen': '-163.37', 'loss/train': '0.65509', 'examples_per_second': '30.125', 'grad_norm': '141', 'counters/examples': 118272, 'counters/updates': 3696}
skipping logging after 118304 examples to avoid logging too frequently
train stats after 118336 examples: {'rewards_train/chosen': '0.055644', 'rewards_train/rejected': '0.012022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043621', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-138.32', 'loss/train': '0.68215', 'examples_per_second': '31.278', 'grad_norm': '64.5', 'counters/examples': 118336, 'counters/updates': 3698}
train stats after 118368 examples: {'rewards_train/chosen': '0.10845', 'rewards_train/rejected': '0.084444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024001', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-160.02', 'loss/train': '0.69842', 'examples_per_second': '32.071', 'grad_norm': '95', 'counters/examples': 118368, 'counters/updates': 3699}
train stats after 118400 examples: {'rewards_train/chosen': '0.17063', 'rewards_train/rejected': '0.030204', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14043', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-121.05', 'loss/train': '0.63551', 'examples_per_second': '31.859', 'grad_norm': '74', 'counters/examples': 118400, 'counters/updates': 3700}
skipping logging after 118432 examples to avoid logging too frequently
train stats after 118464 examples: {'rewards_train/chosen': '0.032647', 'rewards_train/rejected': '-0.070691', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10334', 'logps_train/rejected': '-170.42', 'logps_train/chosen': '-150.9', 'loss/train': '0.67383', 'examples_per_second': '31.674', 'grad_norm': '79.5', 'counters/examples': 118464, 'counters/updates': 3702}
train stats after 118496 examples: {'rewards_train/chosen': '0.27937', 'rewards_train/rejected': '0.063436', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21594', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-151.52', 'loss/train': '0.62716', 'examples_per_second': '31.5', 'grad_norm': '99', 'counters/examples': 118496, 'counters/updates': 3703}
skipping logging after 118528 examples to avoid logging too frequently
train stats after 118560 examples: {'rewards_train/chosen': '0.027162', 'rewards_train/rejected': '0.074955', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.047794', 'logps_train/rejected': '-121.18', 'logps_train/chosen': '-137.25', 'loss/train': '0.72426', 'examples_per_second': '33.283', 'grad_norm': '253', 'counters/examples': 118560, 'counters/updates': 3705}
train stats after 118592 examples: {'rewards_train/chosen': '0.036793', 'rewards_train/rejected': '0.040677', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0038838', 'logps_train/rejected': '-111.46', 'logps_train/chosen': '-110.67', 'loss/train': '0.70126', 'examples_per_second': '31.596', 'grad_norm': '123', 'counters/examples': 118592, 'counters/updates': 3706}
train stats after 118624 examples: {'rewards_train/chosen': '0.12114', 'rewards_train/rejected': '-0.0071754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12832', 'logps_train/rejected': '-108.4', 'logps_train/chosen': '-152.38', 'loss/train': '0.64843', 'examples_per_second': '32.05', 'grad_norm': '52.75', 'counters/examples': 118624, 'counters/updates': 3707}
skipping logging after 118656 examples to avoid logging too frequently
train stats after 118688 examples: {'rewards_train/chosen': '0.089384', 'rewards_train/rejected': '0.033521', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055863', 'logps_train/rejected': '-108.98', 'logps_train/chosen': '-132.25', 'loss/train': '0.68195', 'examples_per_second': '35.864', 'grad_norm': '90', 'counters/examples': 118688, 'counters/updates': 3709}
train stats after 118720 examples: {'rewards_train/chosen': '0.1075', 'rewards_train/rejected': '0.14505', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037541', 'logps_train/rejected': '-168.4', 'logps_train/chosen': '-159.85', 'loss/train': '0.7296', 'examples_per_second': '32.7', 'grad_norm': '66.5', 'counters/examples': 118720, 'counters/updates': 3710}
train stats after 118752 examples: {'rewards_train/chosen': '0.07261', 'rewards_train/rejected': '-0.010547', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083157', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-173.33', 'loss/train': '0.66562', 'examples_per_second': '31.66', 'grad_norm': '79.5', 'counters/examples': 118752, 'counters/updates': 3711}
train stats after 118784 examples: {'rewards_train/chosen': '0.063305', 'rewards_train/rejected': '0.096868', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.033563', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-149.6', 'loss/train': '0.7216', 'examples_per_second': '31.662', 'grad_norm': '140', 'counters/examples': 118784, 'counters/updates': 3712}
skipping logging after 118816 examples to avoid logging too frequently
train stats after 118848 examples: {'rewards_train/chosen': '0.043284', 'rewards_train/rejected': '0.024961', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018323', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-173.67', 'loss/train': '0.69222', 'examples_per_second': '31.115', 'grad_norm': '88.5', 'counters/examples': 118848, 'counters/updates': 3714}
train stats after 118880 examples: {'rewards_train/chosen': '0.16173', 'rewards_train/rejected': '0.046242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11549', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-144.52', 'loss/train': '0.65638', 'examples_per_second': '31.82', 'grad_norm': '61', 'counters/examples': 118880, 'counters/updates': 3715}
skipping logging after 118912 examples to avoid logging too frequently
train stats after 118944 examples: {'rewards_train/chosen': '0.16908', 'rewards_train/rejected': '0.095609', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07347', 'logps_train/rejected': '-98.354', 'logps_train/chosen': '-124.4', 'loss/train': '0.66487', 'examples_per_second': '33.341', 'grad_norm': '78', 'counters/examples': 118944, 'counters/updates': 3717}
train stats after 118976 examples: {'rewards_train/chosen': '0.014096', 'rewards_train/rejected': '0.04006', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025964', 'logps_train/rejected': '-119.83', 'logps_train/chosen': '-158.39', 'loss/train': '0.72172', 'examples_per_second': '30.297', 'grad_norm': '121.5', 'counters/examples': 118976, 'counters/updates': 3718}
train stats after 119008 examples: {'rewards_train/chosen': '0.038588', 'rewards_train/rejected': '0.01688', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021708', 'logps_train/rejected': '-95.535', 'logps_train/chosen': '-152.39', 'loss/train': '0.68842', 'examples_per_second': '31.334', 'grad_norm': '50.5', 'counters/examples': 119008, 'counters/updates': 3719}
skipping logging after 119040 examples to avoid logging too frequently
train stats after 119072 examples: {'rewards_train/chosen': '0.060404', 'rewards_train/rejected': '0.050284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01012', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-143.03', 'loss/train': '0.70151', 'examples_per_second': '35.981', 'grad_norm': '67.5', 'counters/examples': 119072, 'counters/updates': 3721}
train stats after 119104 examples: {'rewards_train/chosen': '0.025445', 'rewards_train/rejected': '-0.016445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04189', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-130.28', 'loss/train': '0.68635', 'examples_per_second': '26.645', 'grad_norm': '71', 'counters/examples': 119104, 'counters/updates': 3722}
train stats after 119136 examples: {'rewards_train/chosen': '0.12117', 'rewards_train/rejected': '0.092943', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028223', 'logps_train/rejected': '-123.78', 'logps_train/chosen': '-151.33', 'loss/train': '0.69373', 'examples_per_second': '30.726', 'grad_norm': '70.5', 'counters/examples': 119136, 'counters/updates': 3723}
train stats after 119168 examples: {'rewards_train/chosen': '0.017723', 'rewards_train/rejected': '-0.0072765', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-139.97', 'loss/train': '0.69583', 'examples_per_second': '31.499', 'grad_norm': '100', 'counters/examples': 119168, 'counters/updates': 3724}
train stats after 119200 examples: {'rewards_train/chosen': '0.014123', 'rewards_train/rejected': '-0.0319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046024', 'logps_train/rejected': '-111.13', 'logps_train/chosen': '-143.56', 'loss/train': '0.67965', 'examples_per_second': '24.776', 'grad_norm': '67.5', 'counters/examples': 119200, 'counters/updates': 3725}
train stats after 119232 examples: {'rewards_train/chosen': '0.15337', 'rewards_train/rejected': '0.1879', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034533', 'logps_train/rejected': '-151.96', 'logps_train/chosen': '-166.12', 'loss/train': '0.73217', 'examples_per_second': '30.4', 'grad_norm': '100.5', 'counters/examples': 119232, 'counters/updates': 3726}
train stats after 119264 examples: {'rewards_train/chosen': '0.025605', 'rewards_train/rejected': '-0.010021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035626', 'logps_train/rejected': '-139.02', 'logps_train/chosen': '-170.86', 'loss/train': '0.69911', 'examples_per_second': '31.326', 'grad_norm': '56.25', 'counters/examples': 119264, 'counters/updates': 3727}
train stats after 119296 examples: {'rewards_train/chosen': '0.077568', 'rewards_train/rejected': '0.028423', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049144', 'logps_train/rejected': '-157.05', 'logps_train/chosen': '-137.7', 'loss/train': '0.68408', 'examples_per_second': '30.133', 'grad_norm': '230', 'counters/examples': 119296, 'counters/updates': 3728}
train stats after 119328 examples: {'rewards_train/chosen': '0.087298', 'rewards_train/rejected': '0.0057043', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081594', 'logps_train/rejected': '-108.33', 'logps_train/chosen': '-137.73', 'loss/train': '0.666', 'examples_per_second': '32.81', 'grad_norm': '68', 'counters/examples': 119328, 'counters/updates': 3729}
skipping logging after 119360 examples to avoid logging too frequently
train stats after 119392 examples: {'rewards_train/chosen': '0.042972', 'rewards_train/rejected': '0.049351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0063784', 'logps_train/rejected': '-150.13', 'logps_train/chosen': '-140.45', 'loss/train': '0.71413', 'examples_per_second': '33.188', 'grad_norm': '118', 'counters/examples': 119392, 'counters/updates': 3731}
train stats after 119424 examples: {'rewards_train/chosen': '0.10197', 'rewards_train/rejected': '-0.070694', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17267', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-178.72', 'loss/train': '0.62115', 'examples_per_second': '31.569', 'grad_norm': '60.5', 'counters/examples': 119424, 'counters/updates': 3732}
skipping logging after 119456 examples to avoid logging too frequently
train stats after 119488 examples: {'rewards_train/chosen': '0.10617', 'rewards_train/rejected': '0.11811', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011943', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-159.02', 'loss/train': '0.71192', 'examples_per_second': '31.567', 'grad_norm': '174', 'counters/examples': 119488, 'counters/updates': 3734}
train stats after 119520 examples: {'rewards_train/chosen': '-0.0043023', 'rewards_train/rejected': '-0.0051241', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00082188', 'logps_train/rejected': '-139.18', 'logps_train/chosen': '-151.1', 'loss/train': '0.70492', 'examples_per_second': '30.82', 'grad_norm': '118.5', 'counters/examples': 119520, 'counters/updates': 3735}
train stats after 119552 examples: {'rewards_train/chosen': '0.11722', 'rewards_train/rejected': '0.021351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09587', 'logps_train/rejected': '-105.87', 'logps_train/chosen': '-154.44', 'loss/train': '0.65397', 'examples_per_second': '31.834', 'grad_norm': '71', 'counters/examples': 119552, 'counters/updates': 3736}
train stats after 119584 examples: {'rewards_train/chosen': '-0.0064379', 'rewards_train/rejected': '0.054391', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.060829', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-183.54', 'loss/train': '0.73601', 'examples_per_second': '31.391', 'grad_norm': '83.5', 'counters/examples': 119584, 'counters/updates': 3737}
train stats after 119616 examples: {'rewards_train/chosen': '0.1528', 'rewards_train/rejected': '0.13116', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021646', 'logps_train/rejected': '-151.9', 'logps_train/chosen': '-192.05', 'loss/train': '0.70354', 'examples_per_second': '31.047', 'grad_norm': '130', 'counters/examples': 119616, 'counters/updates': 3738}
train stats after 119648 examples: {'rewards_train/chosen': '0.063996', 'rewards_train/rejected': '0.056943', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0070528', 'logps_train/rejected': '-137.45', 'logps_train/chosen': '-147.17', 'loss/train': '0.69723', 'examples_per_second': '32.589', 'grad_norm': '78', 'counters/examples': 119648, 'counters/updates': 3739}
train stats after 119680 examples: {'rewards_train/chosen': '0.083486', 'rewards_train/rejected': '-0.022293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10578', 'logps_train/rejected': '-156.35', 'logps_train/chosen': '-155.37', 'loss/train': '0.65997', 'examples_per_second': '32.269', 'grad_norm': '147', 'counters/examples': 119680, 'counters/updates': 3740}
train stats after 119712 examples: {'rewards_train/chosen': '0.059194', 'rewards_train/rejected': '0.030966', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028228', 'logps_train/rejected': '-111.25', 'logps_train/chosen': '-178.7', 'loss/train': '0.69176', 'examples_per_second': '30.619', 'grad_norm': '70', 'counters/examples': 119712, 'counters/updates': 3741}
train stats after 119744 examples: {'rewards_train/chosen': '0.086989', 'rewards_train/rejected': '0.083145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0038447', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-164.82', 'loss/train': '0.71875', 'examples_per_second': '30.513', 'grad_norm': '74', 'counters/examples': 119744, 'counters/updates': 3742}
train stats after 119776 examples: {'rewards_train/chosen': '0.090816', 'rewards_train/rejected': '0.046711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044105', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-147.9', 'loss/train': '0.68739', 'examples_per_second': '32.265', 'grad_norm': '83', 'counters/examples': 119776, 'counters/updates': 3743}
train stats after 119808 examples: {'rewards_train/chosen': '0.051828', 'rewards_train/rejected': '0.05024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0015883', 'logps_train/rejected': '-145.45', 'logps_train/chosen': '-158.65', 'loss/train': '0.70576', 'examples_per_second': '31.351', 'grad_norm': '104.5', 'counters/examples': 119808, 'counters/updates': 3744}
train stats after 119840 examples: {'rewards_train/chosen': '0.10322', 'rewards_train/rejected': '0.078591', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024624', 'logps_train/rejected': '-156.59', 'logps_train/chosen': '-161.41', 'loss/train': '0.69163', 'examples_per_second': '30.331', 'grad_norm': '92.5', 'counters/examples': 119840, 'counters/updates': 3745}
train stats after 119872 examples: {'rewards_train/chosen': '0.16905', 'rewards_train/rejected': '0.020288', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14876', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-146.85', 'loss/train': '0.63296', 'examples_per_second': '31.756', 'grad_norm': '198', 'counters/examples': 119872, 'counters/updates': 3746}
train stats after 119904 examples: {'rewards_train/chosen': '0.11108', 'rewards_train/rejected': '0.012903', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.098181', 'logps_train/rejected': '-157.6', 'logps_train/chosen': '-163.25', 'loss/train': '0.6798', 'examples_per_second': '30.083', 'grad_norm': '72.5', 'counters/examples': 119904, 'counters/updates': 3747}
train stats after 119936 examples: {'rewards_train/chosen': '0.093482', 'rewards_train/rejected': '0.13611', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.042631', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-149.39', 'loss/train': '0.72858', 'examples_per_second': '32.101', 'grad_norm': '85', 'counters/examples': 119936, 'counters/updates': 3748}
train stats after 119968 examples: {'rewards_train/chosen': '0.073641', 'rewards_train/rejected': '0.073181', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00045983', 'logps_train/rejected': '-156.22', 'logps_train/chosen': '-139.42', 'loss/train': '0.70179', 'examples_per_second': '33.319', 'grad_norm': '97.5', 'counters/examples': 119968, 'counters/updates': 3749}
train stats after 120000 examples: {'rewards_train/chosen': '0.039501', 'rewards_train/rejected': '0.030003', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0094985', 'logps_train/rejected': '-140.19', 'logps_train/chosen': '-129.11', 'loss/train': '0.69994', 'examples_per_second': '31.073', 'grad_norm': '219', 'counters/examples': 120000, 'counters/updates': 3750}
Running evaluation after 120000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.86it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]
eval after 120000: {'rewards_eval/chosen': '0.077451', 'rewards_eval/rejected': '0.043492', 'rewards_eval/accuracies': '0.53906', 'rewards_eval/margins': '0.033959', 'logps_eval/rejected': '-127.71', 'logps_eval/chosen': '-150.23', 'loss/eval': '0.68709'}
skipping save for non epoch
train stats after 120032 examples: {'rewards_train/chosen': '0.035714', 'rewards_train/rejected': '0.012846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022867', 'logps_train/rejected': '-99.007', 'logps_train/chosen': '-155.8', 'loss/train': '0.68992', 'examples_per_second': '34.021', 'grad_norm': '51.75', 'counters/examples': 120032, 'counters/updates': 3751}
skipping logging after 120064 examples to avoid logging too frequently
train stats after 120096 examples: {'rewards_train/chosen': '0.1365', 'rewards_train/rejected': '0.061282', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075213', 'logps_train/rejected': '-101.42', 'logps_train/chosen': '-151.69', 'loss/train': '0.66923', 'examples_per_second': '32.826', 'grad_norm': '51', 'counters/examples': 120096, 'counters/updates': 3753}
train stats after 120128 examples: {'rewards_train/chosen': '0.10264', 'rewards_train/rejected': '0.03415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068494', 'logps_train/rejected': '-145.59', 'logps_train/chosen': '-161.38', 'loss/train': '0.67805', 'examples_per_second': '31.63', 'grad_norm': '137', 'counters/examples': 120128, 'counters/updates': 3754}
skipping logging after 120160 examples to avoid logging too frequently
train stats after 120192 examples: {'rewards_train/chosen': '0.10808', 'rewards_train/rejected': '0.065411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042664', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-139.82', 'loss/train': '0.67795', 'examples_per_second': '32.721', 'grad_norm': '99.5', 'counters/examples': 120192, 'counters/updates': 3756}
train stats after 120224 examples: {'rewards_train/chosen': '0.17036', 'rewards_train/rejected': '0.09858', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071784', 'logps_train/rejected': '-152.56', 'logps_train/chosen': '-195.25', 'loss/train': '0.68932', 'examples_per_second': '33.437', 'grad_norm': '89', 'counters/examples': 120224, 'counters/updates': 3757}
train stats after 120256 examples: {'rewards_train/chosen': '0.042853', 'rewards_train/rejected': '0.084187', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.041334', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-185.25', 'loss/train': '0.72818', 'examples_per_second': '30.127', 'grad_norm': '160', 'counters/examples': 120256, 'counters/updates': 3758}
train stats after 120288 examples: {'rewards_train/chosen': '0.034029', 'rewards_train/rejected': '0.018051', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015978', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-139.31', 'loss/train': '0.69313', 'examples_per_second': '30.693', 'grad_norm': '174', 'counters/examples': 120288, 'counters/updates': 3759}
train stats after 120320 examples: {'rewards_train/chosen': '0.036965', 'rewards_train/rejected': '0.053556', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016591', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-157.55', 'loss/train': '0.71631', 'examples_per_second': '31.129', 'grad_norm': '106.5', 'counters/examples': 120320, 'counters/updates': 3760}
train stats after 120352 examples: {'rewards_train/chosen': '0.049699', 'rewards_train/rejected': '0.028143', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021556', 'logps_train/rejected': '-142.38', 'logps_train/chosen': '-158.17', 'loss/train': '0.69209', 'examples_per_second': '30.527', 'grad_norm': '255', 'counters/examples': 120352, 'counters/updates': 3761}
train stats after 120384 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.0077112', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12572', 'logps_train/rejected': '-180.97', 'logps_train/chosen': '-204.55', 'loss/train': '0.64861', 'examples_per_second': '31.587', 'grad_norm': '201', 'counters/examples': 120384, 'counters/updates': 3762}
train stats after 120416 examples: {'rewards_train/chosen': '0.19047', 'rewards_train/rejected': '0.089281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10118', 'logps_train/rejected': '-139.78', 'logps_train/chosen': '-145.51', 'loss/train': '0.66829', 'examples_per_second': '32.393', 'grad_norm': '226', 'counters/examples': 120416, 'counters/updates': 3763}
train stats after 120448 examples: {'rewards_train/chosen': '0.077985', 'rewards_train/rejected': '0.014341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063644', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-144.29', 'loss/train': '0.67563', 'examples_per_second': '30.606', 'grad_norm': '65', 'counters/examples': 120448, 'counters/updates': 3764}
skipping logging after 120480 examples to avoid logging too frequently
train stats after 120512 examples: {'rewards_train/chosen': '0.09533', 'rewards_train/rejected': '-0.0046526', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099983', 'logps_train/rejected': '-116.6', 'logps_train/chosen': '-148.95', 'loss/train': '0.65429', 'examples_per_second': '30.61', 'grad_norm': '57.25', 'counters/examples': 120512, 'counters/updates': 3766}
train stats after 120544 examples: {'rewards_train/chosen': '0.054783', 'rewards_train/rejected': '-0.016599', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071382', 'logps_train/rejected': '-142.93', 'logps_train/chosen': '-175.33', 'loss/train': '0.66737', 'examples_per_second': '31.123', 'grad_norm': '91.5', 'counters/examples': 120544, 'counters/updates': 3767}
skipping logging after 120576 examples to avoid logging too frequently
train stats after 120608 examples: {'rewards_train/chosen': '0.028866', 'rewards_train/rejected': '-0.0059965', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034863', 'logps_train/rejected': '-197.97', 'logps_train/chosen': '-149.69', 'loss/train': '0.69595', 'examples_per_second': '31.768', 'grad_norm': '118', 'counters/examples': 120608, 'counters/updates': 3769}
train stats after 120640 examples: {'rewards_train/chosen': '0.055751', 'rewards_train/rejected': '0.023252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032499', 'logps_train/rejected': '-111.32', 'logps_train/chosen': '-150.07', 'loss/train': '0.68996', 'examples_per_second': '31.071', 'grad_norm': '65', 'counters/examples': 120640, 'counters/updates': 3770}
train stats after 120672 examples: {'rewards_train/chosen': '0.14557', 'rewards_train/rejected': '0.12386', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02172', 'logps_train/rejected': '-165.83', 'logps_train/chosen': '-163.32', 'loss/train': '0.69409', 'examples_per_second': '32.58', 'grad_norm': '91.5', 'counters/examples': 120672, 'counters/updates': 3771}
train stats after 120704 examples: {'rewards_train/chosen': '0.13777', 'rewards_train/rejected': '0.026028', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11174', 'logps_train/rejected': '-146.82', 'logps_train/chosen': '-163.04', 'loss/train': '0.64948', 'examples_per_second': '32.636', 'grad_norm': '88', 'counters/examples': 120704, 'counters/updates': 3772}
train stats after 120736 examples: {'rewards_train/chosen': '0.12393', 'rewards_train/rejected': '0.054254', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069679', 'logps_train/rejected': '-166.48', 'logps_train/chosen': '-197.65', 'loss/train': '0.70608', 'examples_per_second': '31.298', 'grad_norm': '93.5', 'counters/examples': 120736, 'counters/updates': 3773}
train stats after 120768 examples: {'rewards_train/chosen': '0.014939', 'rewards_train/rejected': '0.027498', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012559', 'logps_train/rejected': '-153.1', 'logps_train/chosen': '-161.29', 'loss/train': '0.70982', 'examples_per_second': '30.49', 'grad_norm': '127', 'counters/examples': 120768, 'counters/updates': 3774}
train stats after 120800 examples: {'rewards_train/chosen': '0.1045', 'rewards_train/rejected': '0.053417', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051087', 'logps_train/rejected': '-155.88', 'logps_train/chosen': '-191.65', 'loss/train': '0.69457', 'examples_per_second': '31.418', 'grad_norm': '89', 'counters/examples': 120800, 'counters/updates': 3775}
skipping logging after 120832 examples to avoid logging too frequently
train stats after 120864 examples: {'rewards_train/chosen': '0.14655', 'rewards_train/rejected': '0.070579', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075975', 'logps_train/rejected': '-125.42', 'logps_train/chosen': '-157.27', 'loss/train': '0.66261', 'examples_per_second': '32.159', 'grad_norm': '187', 'counters/examples': 120864, 'counters/updates': 3777}
train stats after 120896 examples: {'rewards_train/chosen': '0.11542', 'rewards_train/rejected': '0.06105', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054375', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-169.29', 'loss/train': '0.67941', 'examples_per_second': '32.316', 'grad_norm': '86.5', 'counters/examples': 120896, 'counters/updates': 3778}
skipping logging after 120928 examples to avoid logging too frequently
train stats after 120960 examples: {'rewards_train/chosen': '0.072496', 'rewards_train/rejected': '0.11501', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042517', 'logps_train/rejected': '-130.76', 'logps_train/chosen': '-165.67', 'loss/train': '0.72559', 'examples_per_second': '30.475', 'grad_norm': '83', 'counters/examples': 120960, 'counters/updates': 3780}
train stats after 120992 examples: {'rewards_train/chosen': '0.08143', 'rewards_train/rejected': '0.048846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032583', 'logps_train/rejected': '-148.94', 'logps_train/chosen': '-179.7', 'loss/train': '0.68773', 'examples_per_second': '31.618', 'grad_norm': '86.5', 'counters/examples': 120992, 'counters/updates': 3781}
skipping logging after 121024 examples to avoid logging too frequently
train stats after 121056 examples: {'rewards_train/chosen': '0.057943', 'rewards_train/rejected': '0.10273', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044792', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-157.51', 'loss/train': '0.73465', 'examples_per_second': '32.882', 'grad_norm': '94.5', 'counters/examples': 121056, 'counters/updates': 3783}
train stats after 121088 examples: {'rewards_train/chosen': '0.10379', 'rewards_train/rejected': '-0.0087369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11253', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-149.91', 'loss/train': '0.6517', 'examples_per_second': '32.031', 'grad_norm': '326', 'counters/examples': 121088, 'counters/updates': 3784}
train stats after 121120 examples: {'rewards_train/chosen': '0.096455', 'rewards_train/rejected': '0.0068708', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089585', 'logps_train/rejected': '-115.76', 'logps_train/chosen': '-163.35', 'loss/train': '0.66114', 'examples_per_second': '30.014', 'grad_norm': '72', 'counters/examples': 121120, 'counters/updates': 3785}
train stats after 121152 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '0.049618', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070121', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-155.16', 'loss/train': '0.66796', 'examples_per_second': '31.619', 'grad_norm': '79', 'counters/examples': 121152, 'counters/updates': 3786}
skipping logging after 121184 examples to avoid logging too frequently
train stats after 121216 examples: {'rewards_train/chosen': '0.090558', 'rewards_train/rejected': '0.025384', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.065174', 'logps_train/rejected': '-166.71', 'logps_train/chosen': '-143.65', 'loss/train': '0.67748', 'examples_per_second': '31.558', 'grad_norm': '75', 'counters/examples': 121216, 'counters/updates': 3788}
train stats after 121248 examples: {'rewards_train/chosen': '0.14429', 'rewards_train/rejected': '0.059154', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08514', 'logps_train/rejected': '-113.18', 'logps_train/chosen': '-177.96', 'loss/train': '0.66917', 'examples_per_second': '31.309', 'grad_norm': '163', 'counters/examples': 121248, 'counters/updates': 3789}
skipping logging after 121280 examples to avoid logging too frequently
train stats after 121312 examples: {'rewards_train/chosen': '0.07242', 'rewards_train/rejected': '0.11155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.039128', 'logps_train/rejected': '-155.03', 'logps_train/chosen': '-146.86', 'loss/train': '0.73003', 'examples_per_second': '31.563', 'grad_norm': '118.5', 'counters/examples': 121312, 'counters/updates': 3791}
train stats after 121344 examples: {'rewards_train/chosen': '0.10599', 'rewards_train/rejected': '0.018537', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087456', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-170.31', 'loss/train': '0.66167', 'examples_per_second': '31.417', 'grad_norm': '122.5', 'counters/examples': 121344, 'counters/updates': 3792}
train stats after 121376 examples: {'rewards_train/chosen': '0.057198', 'rewards_train/rejected': '0.14027', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.083077', 'logps_train/rejected': '-170.77', 'logps_train/chosen': '-162.11', 'loss/train': '0.74682', 'examples_per_second': '22.565', 'grad_norm': '121', 'counters/examples': 121376, 'counters/updates': 3793}
skipping logging after 121408 examples to avoid logging too frequently
train stats after 121440 examples: {'rewards_train/chosen': '0.10416', 'rewards_train/rejected': '0.044594', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059561', 'logps_train/rejected': '-111.89', 'logps_train/chosen': '-135.98', 'loss/train': '0.66994', 'examples_per_second': '30.649', 'grad_norm': '73', 'counters/examples': 121440, 'counters/updates': 3795}
skipping logging after 121472 examples to avoid logging too frequently
train stats after 121504 examples: {'rewards_train/chosen': '0.076671', 'rewards_train/rejected': '0.11499', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.038316', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-145.39', 'loss/train': '0.72203', 'examples_per_second': '34.454', 'grad_norm': '72.5', 'counters/examples': 121504, 'counters/updates': 3797}
train stats after 121536 examples: {'rewards_train/chosen': '0.059965', 'rewards_train/rejected': '0.063761', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0037962', 'logps_train/rejected': '-122.66', 'logps_train/chosen': '-169.34', 'loss/train': '0.70857', 'examples_per_second': '30.081', 'grad_norm': '82', 'counters/examples': 121536, 'counters/updates': 3798}
skipping logging after 121568 examples to avoid logging too frequently
train stats after 121600 examples: {'rewards_train/chosen': '0.081763', 'rewards_train/rejected': '-0.060605', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14237', 'logps_train/rejected': '-139.88', 'logps_train/chosen': '-133', 'loss/train': '0.63804', 'examples_per_second': '34.318', 'grad_norm': '75', 'counters/examples': 121600, 'counters/updates': 3800}
train stats after 121632 examples: {'rewards_train/chosen': '-0.0067247', 'rewards_train/rejected': '-0.027668', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.020943', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-129.72', 'loss/train': '0.69383', 'examples_per_second': '32.956', 'grad_norm': '133', 'counters/examples': 121632, 'counters/updates': 3801}
train stats after 121664 examples: {'rewards_train/chosen': '0.11506', 'rewards_train/rejected': '0.11003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0050337', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-138.87', 'loss/train': '0.70645', 'examples_per_second': '31.255', 'grad_norm': '107.5', 'counters/examples': 121664, 'counters/updates': 3802}
train stats after 121696 examples: {'rewards_train/chosen': '0.06692', 'rewards_train/rejected': '0.0072015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059719', 'logps_train/rejected': '-157.65', 'logps_train/chosen': '-143.81', 'loss/train': '0.67633', 'examples_per_second': '30.112', 'grad_norm': '103', 'counters/examples': 121696, 'counters/updates': 3803}
train stats after 121728 examples: {'rewards_train/chosen': '0.12508', 'rewards_train/rejected': '0.090957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034123', 'logps_train/rejected': '-157.55', 'logps_train/chosen': '-181.88', 'loss/train': '0.69425', 'examples_per_second': '31.619', 'grad_norm': '113', 'counters/examples': 121728, 'counters/updates': 3804}
train stats after 121760 examples: {'rewards_train/chosen': '0.026111', 'rewards_train/rejected': '0.018717', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0073937', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-157.37', 'loss/train': '0.69995', 'examples_per_second': '31.108', 'grad_norm': '97', 'counters/examples': 121760, 'counters/updates': 3805}
train stats after 121792 examples: {'rewards_train/chosen': '0.036039', 'rewards_train/rejected': '0.0033028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032736', 'logps_train/rejected': '-133.17', 'logps_train/chosen': '-172.18', 'loss/train': '0.68335', 'examples_per_second': '31.901', 'grad_norm': '99.5', 'counters/examples': 121792, 'counters/updates': 3806}
train stats after 121824 examples: {'rewards_train/chosen': '0.09073', 'rewards_train/rejected': '0.0046804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086049', 'logps_train/rejected': '-160.66', 'logps_train/chosen': '-184.22', 'loss/train': '0.67659', 'examples_per_second': '30.005', 'grad_norm': '89', 'counters/examples': 121824, 'counters/updates': 3807}
train stats after 121856 examples: {'rewards_train/chosen': '0.14265', 'rewards_train/rejected': '0.065789', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076861', 'logps_train/rejected': '-104.55', 'logps_train/chosen': '-137.38', 'loss/train': '0.66609', 'examples_per_second': '32.112', 'grad_norm': '93.5', 'counters/examples': 121856, 'counters/updates': 3808}
skipping logging after 121888 examples to avoid logging too frequently
train stats after 121920 examples: {'rewards_train/chosen': '0.049147', 'rewards_train/rejected': '0.03137', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017778', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-132.65', 'loss/train': '0.70295', 'examples_per_second': '36.3', 'grad_norm': '207', 'counters/examples': 121920, 'counters/updates': 3810}
train stats after 121952 examples: {'rewards_train/chosen': '0.095045', 'rewards_train/rejected': '-0.0053874', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10043', 'logps_train/rejected': '-98.395', 'logps_train/chosen': '-141.74', 'loss/train': '0.65409', 'examples_per_second': '32.578', 'grad_norm': '53.75', 'counters/examples': 121952, 'counters/updates': 3811}
train stats after 121984 examples: {'rewards_train/chosen': '0.11926', 'rewards_train/rejected': '0.034397', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084863', 'logps_train/rejected': '-143.9', 'logps_train/chosen': '-132.82', 'loss/train': '0.66034', 'examples_per_second': '30.133', 'grad_norm': '81.5', 'counters/examples': 121984, 'counters/updates': 3812}
train stats after 122016 examples: {'rewards_train/chosen': '0.066651', 'rewards_train/rejected': '0.015166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051485', 'logps_train/rejected': '-130.19', 'logps_train/chosen': '-137.55', 'loss/train': '0.6793', 'examples_per_second': '31.763', 'grad_norm': '63.25', 'counters/examples': 122016, 'counters/updates': 3813}
train stats after 122048 examples: {'rewards_train/chosen': '0.12527', 'rewards_train/rejected': '0.0065855', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11869', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-163.19', 'loss/train': '0.65392', 'examples_per_second': '31.42', 'grad_norm': '75', 'counters/examples': 122048, 'counters/updates': 3814}
skipping logging after 122080 examples to avoid logging too frequently
train stats after 122112 examples: {'rewards_train/chosen': '0.062224', 'rewards_train/rejected': '-0.052465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11469', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-132.11', 'loss/train': '0.65165', 'examples_per_second': '31.578', 'grad_norm': '77', 'counters/examples': 122112, 'counters/updates': 3816}
skipping logging after 122144 examples to avoid logging too frequently
train stats after 122176 examples: {'rewards_train/chosen': '0.10957', 'rewards_train/rejected': '0.086511', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023064', 'logps_train/rejected': '-129.07', 'logps_train/chosen': '-146.02', 'loss/train': '0.69799', 'examples_per_second': '36.308', 'grad_norm': '82', 'counters/examples': 122176, 'counters/updates': 3818}
train stats after 122208 examples: {'rewards_train/chosen': '0.06087', 'rewards_train/rejected': '0.060919', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-4.9513e-05', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-140.31', 'loss/train': '0.7064', 'examples_per_second': '30.719', 'grad_norm': '157', 'counters/examples': 122208, 'counters/updates': 3819}
skipping logging after 122240 examples to avoid logging too frequently
train stats after 122272 examples: {'rewards_train/chosen': '0.088869', 'rewards_train/rejected': '0.046614', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042255', 'logps_train/rejected': '-134.16', 'logps_train/chosen': '-168.86', 'loss/train': '0.69203', 'examples_per_second': '36.075', 'grad_norm': '74', 'counters/examples': 122272, 'counters/updates': 3821}
skipping logging after 122304 examples to avoid logging too frequently
train stats after 122336 examples: {'rewards_train/chosen': '0.078351', 'rewards_train/rejected': '0.058586', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019765', 'logps_train/rejected': '-142.03', 'logps_train/chosen': '-118.72', 'loss/train': '0.69548', 'examples_per_second': '31.618', 'grad_norm': '194', 'counters/examples': 122336, 'counters/updates': 3823}
train stats after 122368 examples: {'rewards_train/chosen': '0.070366', 'rewards_train/rejected': '0.061925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0084404', 'logps_train/rejected': '-149.06', 'logps_train/chosen': '-129.78', 'loss/train': '0.70642', 'examples_per_second': '32.184', 'grad_norm': '89', 'counters/examples': 122368, 'counters/updates': 3824}
skipping logging after 122400 examples to avoid logging too frequently
train stats after 122432 examples: {'rewards_train/chosen': '0.0076543', 'rewards_train/rejected': '0.032891', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025237', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-147.53', 'loss/train': '0.7167', 'examples_per_second': '31.544', 'grad_norm': '105.5', 'counters/examples': 122432, 'counters/updates': 3826}
train stats after 122464 examples: {'rewards_train/chosen': '0.060809', 'rewards_train/rejected': '0.13907', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.078259', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-119.76', 'loss/train': '0.75164', 'examples_per_second': '32.145', 'grad_norm': '64.5', 'counters/examples': 122464, 'counters/updates': 3827}
train stats after 122496 examples: {'rewards_train/chosen': '0.12895', 'rewards_train/rejected': '0.050158', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078792', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-157.58', 'loss/train': '0.67473', 'examples_per_second': '31.597', 'grad_norm': '74.5', 'counters/examples': 122496, 'counters/updates': 3828}
train stats after 122528 examples: {'rewards_train/chosen': '0.042254', 'rewards_train/rejected': '0.075991', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033736', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-132.32', 'loss/train': '0.7186', 'examples_per_second': '30.642', 'grad_norm': '145', 'counters/examples': 122528, 'counters/updates': 3829}
train stats after 122560 examples: {'rewards_train/chosen': '0.083619', 'rewards_train/rejected': '0.02156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062059', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-145.63', 'loss/train': '0.67319', 'examples_per_second': '31.198', 'grad_norm': '45.75', 'counters/examples': 122560, 'counters/updates': 3830}
train stats after 122592 examples: {'rewards_train/chosen': '0.082212', 'rewards_train/rejected': '0.079454', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0027578', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-158.95', 'loss/train': '0.70921', 'examples_per_second': '30.551', 'grad_norm': '50.75', 'counters/examples': 122592, 'counters/updates': 3831}
train stats after 122624 examples: {'rewards_train/chosen': '0.13218', 'rewards_train/rejected': '-0.0017775', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13395', 'logps_train/rejected': '-166.61', 'logps_train/chosen': '-169.5', 'loss/train': '0.65197', 'examples_per_second': '31.598', 'grad_norm': '202', 'counters/examples': 122624, 'counters/updates': 3832}
skipping logging after 122656 examples to avoid logging too frequently
train stats after 122688 examples: {'rewards_train/chosen': '0.061716', 'rewards_train/rejected': '-0.037541', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099257', 'logps_train/rejected': '-130.3', 'logps_train/chosen': '-164.9', 'loss/train': '0.65759', 'examples_per_second': '30.111', 'grad_norm': '64', 'counters/examples': 122688, 'counters/updates': 3834}
skipping logging after 122720 examples to avoid logging too frequently
train stats after 122752 examples: {'rewards_train/chosen': '0.050149', 'rewards_train/rejected': '-0.081578', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13173', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-117.78', 'loss/train': '0.65327', 'examples_per_second': '31.793', 'grad_norm': '55.5', 'counters/examples': 122752, 'counters/updates': 3836}
train stats after 122784 examples: {'rewards_train/chosen': '0.10343', 'rewards_train/rejected': '0.050891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052535', 'logps_train/rejected': '-94.169', 'logps_train/chosen': '-104.73', 'loss/train': '0.67368', 'examples_per_second': '31.102', 'grad_norm': '74.5', 'counters/examples': 122784, 'counters/updates': 3837}
skipping logging after 122816 examples to avoid logging too frequently
train stats after 122848 examples: {'rewards_train/chosen': '0.042968', 'rewards_train/rejected': '0.08063', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037662', 'logps_train/rejected': '-126.66', 'logps_train/chosen': '-134.89', 'loss/train': '0.72518', 'examples_per_second': '32.448', 'grad_norm': '67', 'counters/examples': 122848, 'counters/updates': 3839}
train stats after 122880 examples: {'rewards_train/chosen': '0.06338', 'rewards_train/rejected': '-0.0029794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066359', 'logps_train/rejected': '-211.74', 'logps_train/chosen': '-215.88', 'loss/train': '0.68953', 'examples_per_second': '31.282', 'grad_norm': '96', 'counters/examples': 122880, 'counters/updates': 3840}
train stats after 122912 examples: {'rewards_train/chosen': '0.046388', 'rewards_train/rejected': '0.10805', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.061666', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-138.28', 'loss/train': '0.74291', 'examples_per_second': '30.614', 'grad_norm': '67.5', 'counters/examples': 122912, 'counters/updates': 3841}
skipping logging after 122944 examples to avoid logging too frequently
train stats after 122976 examples: {'rewards_train/chosen': '0.10713', 'rewards_train/rejected': '0.022851', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084282', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-165.26', 'loss/train': '0.66146', 'examples_per_second': '34.391', 'grad_norm': '85', 'counters/examples': 122976, 'counters/updates': 3843}
skipping logging after 123008 examples to avoid logging too frequently
train stats after 123040 examples: {'rewards_train/chosen': '0.030078', 'rewards_train/rejected': '0.03567', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0055919', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-141.82', 'loss/train': '0.70801', 'examples_per_second': '30.944', 'grad_norm': '65.5', 'counters/examples': 123040, 'counters/updates': 3845}
train stats after 123072 examples: {'rewards_train/chosen': '0.099631', 'rewards_train/rejected': '0.096472', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0031584', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-167.15', 'loss/train': '0.71019', 'examples_per_second': '32.1', 'grad_norm': '108.5', 'counters/examples': 123072, 'counters/updates': 3846}
train stats after 123104 examples: {'rewards_train/chosen': '0.12078', 'rewards_train/rejected': '0.097895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022884', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-136.7', 'loss/train': '0.68837', 'examples_per_second': '30.383', 'grad_norm': '58.25', 'counters/examples': 123104, 'counters/updates': 3847}
skipping logging after 123136 examples to avoid logging too frequently
train stats after 123168 examples: {'rewards_train/chosen': '0.076349', 'rewards_train/rejected': '0.07868', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0023319', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-150.63', 'loss/train': '0.71148', 'examples_per_second': '31.157', 'grad_norm': '84', 'counters/examples': 123168, 'counters/updates': 3849}
train stats after 123200 examples: {'rewards_train/chosen': '0.14088', 'rewards_train/rejected': '0.00043992', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14044', 'logps_train/rejected': '-131.27', 'logps_train/chosen': '-164.45', 'loss/train': '0.63454', 'examples_per_second': '31.485', 'grad_norm': '93', 'counters/examples': 123200, 'counters/updates': 3850}
train stats after 123232 examples: {'rewards_train/chosen': '0.12667', 'rewards_train/rejected': '0.083059', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043614', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-151.91', 'loss/train': '0.68401', 'examples_per_second': '31.516', 'grad_norm': '94.5', 'counters/examples': 123232, 'counters/updates': 3851}
train stats after 123264 examples: {'rewards_train/chosen': '0.041975', 'rewards_train/rejected': '0.049256', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0072818', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-170.47', 'loss/train': '0.71258', 'examples_per_second': '30.392', 'grad_norm': '82.5', 'counters/examples': 123264, 'counters/updates': 3852}
train stats after 123296 examples: {'rewards_train/chosen': '0.046002', 'rewards_train/rejected': '0.061635', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015632', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-130.64', 'loss/train': '0.71586', 'examples_per_second': '32.494', 'grad_norm': '83.5', 'counters/examples': 123296, 'counters/updates': 3853}
train stats after 123328 examples: {'rewards_train/chosen': '0.01114', 'rewards_train/rejected': '0.13122', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.12008', 'logps_train/rejected': '-153.92', 'logps_train/chosen': '-151.44', 'loss/train': '0.78521', 'examples_per_second': '31.574', 'grad_norm': '167', 'counters/examples': 123328, 'counters/updates': 3854}
train stats after 123360 examples: {'rewards_train/chosen': '0.069265', 'rewards_train/rejected': '0.013488', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055777', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-195.44', 'loss/train': '0.68153', 'examples_per_second': '31.179', 'grad_norm': '179', 'counters/examples': 123360, 'counters/updates': 3855}
train stats after 123392 examples: {'rewards_train/chosen': '0.11498', 'rewards_train/rejected': '0.032638', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.082346', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-181.93', 'loss/train': '0.6783', 'examples_per_second': '31.355', 'grad_norm': '76', 'counters/examples': 123392, 'counters/updates': 3856}
skipping logging after 123424 examples to avoid logging too frequently
train stats after 123456 examples: {'rewards_train/chosen': '-0.0087899', 'rewards_train/rejected': '0.035107', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043897', 'logps_train/rejected': '-180.69', 'logps_train/chosen': '-191.85', 'loss/train': '0.7355', 'examples_per_second': '31.603', 'grad_norm': '124.5', 'counters/examples': 123456, 'counters/updates': 3858}
train stats after 123488 examples: {'rewards_train/chosen': '0.0067707', 'rewards_train/rejected': '0.0094732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027025', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-209.09', 'loss/train': '0.7115', 'examples_per_second': '30.523', 'grad_norm': '158', 'counters/examples': 123488, 'counters/updates': 3859}
train stats after 123520 examples: {'rewards_train/chosen': '0.035607', 'rewards_train/rejected': '0.032155', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0034512', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-149.3', 'loss/train': '0.70251', 'examples_per_second': '31.632', 'grad_norm': '66', 'counters/examples': 123520, 'counters/updates': 3860}
train stats after 123552 examples: {'rewards_train/chosen': '0.11934', 'rewards_train/rejected': '0.10221', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017127', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-138.69', 'loss/train': '0.70762', 'examples_per_second': '30.802', 'grad_norm': '71', 'counters/examples': 123552, 'counters/updates': 3861}
train stats after 123584 examples: {'rewards_train/chosen': '0.16267', 'rewards_train/rejected': '0.054957', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10771', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-181.39', 'loss/train': '0.65361', 'examples_per_second': '31.519', 'grad_norm': '92.5', 'counters/examples': 123584, 'counters/updates': 3862}
train stats after 123616 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.043749', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12612', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-174.44', 'loss/train': '0.6482', 'examples_per_second': '31.48', 'grad_norm': '156', 'counters/examples': 123616, 'counters/updates': 3863}
train stats after 123648 examples: {'rewards_train/chosen': '0.048928', 'rewards_train/rejected': '0.12398', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.075051', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-138.48', 'loss/train': '0.75074', 'examples_per_second': '31.638', 'grad_norm': '175', 'counters/examples': 123648, 'counters/updates': 3864}
train stats after 123680 examples: {'rewards_train/chosen': '0.10528', 'rewards_train/rejected': '0.040523', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064757', 'logps_train/rejected': '-141.24', 'logps_train/chosen': '-134.87', 'loss/train': '0.67237', 'examples_per_second': '30.355', 'grad_norm': '85.5', 'counters/examples': 123680, 'counters/updates': 3865}
train stats after 123712 examples: {'rewards_train/chosen': '0.020106', 'rewards_train/rejected': '-0.036259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056365', 'logps_train/rejected': '-136.4', 'logps_train/chosen': '-152.76', 'loss/train': '0.67551', 'examples_per_second': '31.298', 'grad_norm': '163', 'counters/examples': 123712, 'counters/updates': 3866}
train stats after 123744 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '0.056533', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.058391', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-145.57', 'loss/train': '0.68435', 'examples_per_second': '30.549', 'grad_norm': '61', 'counters/examples': 123744, 'counters/updates': 3867}
train stats after 123776 examples: {'rewards_train/chosen': '0.082809', 'rewards_train/rejected': '0.078503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0043052', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-146.32', 'loss/train': '0.70609', 'examples_per_second': '31.233', 'grad_norm': '396', 'counters/examples': 123776, 'counters/updates': 3868}
train stats after 123808 examples: {'rewards_train/chosen': '0.036329', 'rewards_train/rejected': '0.058942', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022613', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-172.07', 'loss/train': '0.71599', 'examples_per_second': '32.767', 'grad_norm': '68.5', 'counters/examples': 123808, 'counters/updates': 3869}
train stats after 123840 examples: {'rewards_train/chosen': '0.068743', 'rewards_train/rejected': '0.0045105', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.064233', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-127.08', 'loss/train': '0.66777', 'examples_per_second': '30.844', 'grad_norm': '74', 'counters/examples': 123840, 'counters/updates': 3870}
train stats after 123872 examples: {'rewards_train/chosen': '0.090874', 'rewards_train/rejected': '-0.03275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12362', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-143.11', 'loss/train': '0.64522', 'examples_per_second': '30.635', 'grad_norm': '59', 'counters/examples': 123872, 'counters/updates': 3871}
train stats after 123904 examples: {'rewards_train/chosen': '0.12041', 'rewards_train/rejected': '0.004337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11608', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-164.37', 'loss/train': '0.64946', 'examples_per_second': '31.914', 'grad_norm': '282', 'counters/examples': 123904, 'counters/updates': 3872}
skipping logging after 123936 examples to avoid logging too frequently
train stats after 123968 examples: {'rewards_train/chosen': '0.051602', 'rewards_train/rejected': '0.064613', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01301', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-136.97', 'loss/train': '0.7162', 'examples_per_second': '33.461', 'grad_norm': '100', 'counters/examples': 123968, 'counters/updates': 3874}
train stats after 124000 examples: {'rewards_train/chosen': '0.054917', 'rewards_train/rejected': '0.022293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032624', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-157.64', 'loss/train': '0.68986', 'examples_per_second': '30.164', 'grad_norm': '140', 'counters/examples': 124000, 'counters/updates': 3875}
skipping logging after 124032 examples to avoid logging too frequently
train stats after 124064 examples: {'rewards_train/chosen': '0.060899', 'rewards_train/rejected': '0.075965', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015066', 'logps_train/rejected': '-166.18', 'logps_train/chosen': '-183.21', 'loss/train': '0.71828', 'examples_per_second': '31.453', 'grad_norm': '68', 'counters/examples': 124064, 'counters/updates': 3877}
train stats after 124096 examples: {'rewards_train/chosen': '0.069875', 'rewards_train/rejected': '-0.033285', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10316', 'logps_train/rejected': '-141.49', 'logps_train/chosen': '-156.14', 'loss/train': '0.65375', 'examples_per_second': '32.744', 'grad_norm': '85.5', 'counters/examples': 124096, 'counters/updates': 3878}
train stats after 124128 examples: {'rewards_train/chosen': '0.16816', 'rewards_train/rejected': '0.061022', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10713', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-154.83', 'loss/train': '0.65543', 'examples_per_second': '31.607', 'grad_norm': '89', 'counters/examples': 124128, 'counters/updates': 3879}
train stats after 124160 examples: {'rewards_train/chosen': '-0.020212', 'rewards_train/rejected': '0.01469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034902', 'logps_train/rejected': '-118.68', 'logps_train/chosen': '-186.53', 'loss/train': '0.73694', 'examples_per_second': '30.664', 'grad_norm': '67.5', 'counters/examples': 124160, 'counters/updates': 3880}
train stats after 124192 examples: {'rewards_train/chosen': '0.060617', 'rewards_train/rejected': '0.0070953', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053522', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-170.69', 'loss/train': '0.67957', 'examples_per_second': '32.553', 'grad_norm': '131', 'counters/examples': 124192, 'counters/updates': 3881}
train stats after 124224 examples: {'rewards_train/chosen': '0.050819', 'rewards_train/rejected': '0.013429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03739', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-188.39', 'loss/train': '0.69489', 'examples_per_second': '33.321', 'grad_norm': '83', 'counters/examples': 124224, 'counters/updates': 3882}
train stats after 124256 examples: {'rewards_train/chosen': '0.12481', 'rewards_train/rejected': '0.068248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056565', 'logps_train/rejected': '-145.46', 'logps_train/chosen': '-197.13', 'loss/train': '0.68354', 'examples_per_second': '31.657', 'grad_norm': '85.5', 'counters/examples': 124256, 'counters/updates': 3883}
train stats after 124288 examples: {'rewards_train/chosen': '0.021443', 'rewards_train/rejected': '0.019477', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0019652', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-149.31', 'loss/train': '0.70726', 'examples_per_second': '31.343', 'grad_norm': '141', 'counters/examples': 124288, 'counters/updates': 3884}
train stats after 124320 examples: {'rewards_train/chosen': '0.016393', 'rewards_train/rejected': '0.0096667', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0067265', 'logps_train/rejected': '-124', 'logps_train/chosen': '-185.01', 'loss/train': '0.70359', 'examples_per_second': '31.648', 'grad_norm': '149', 'counters/examples': 124320, 'counters/updates': 3885}
train stats after 124352 examples: {'rewards_train/chosen': '0.15624', 'rewards_train/rejected': '0.022002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13424', 'logps_train/rejected': '-106.56', 'logps_train/chosen': '-145.68', 'loss/train': '0.64583', 'examples_per_second': '30.13', 'grad_norm': '172', 'counters/examples': 124352, 'counters/updates': 3886}
skipping logging after 124384 examples to avoid logging too frequently
train stats after 124416 examples: {'rewards_train/chosen': '0.11726', 'rewards_train/rejected': '0.017593', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099665', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-180.04', 'loss/train': '0.66386', 'examples_per_second': '34.043', 'grad_norm': '106.5', 'counters/examples': 124416, 'counters/updates': 3888}
train stats after 124448 examples: {'rewards_train/chosen': '0.097036', 'rewards_train/rejected': '0.025193', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071843', 'logps_train/rejected': '-166.03', 'logps_train/chosen': '-136.32', 'loss/train': '0.67263', 'examples_per_second': '31.553', 'grad_norm': '84.5', 'counters/examples': 124448, 'counters/updates': 3889}
train stats after 124480 examples: {'rewards_train/chosen': '0.018564', 'rewards_train/rejected': '0.087596', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069033', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-166.18', 'loss/train': '0.73739', 'examples_per_second': '32.602', 'grad_norm': '109.5', 'counters/examples': 124480, 'counters/updates': 3890}
train stats after 124512 examples: {'rewards_train/chosen': '0.027275', 'rewards_train/rejected': '0.037825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.01055', 'logps_train/rejected': '-161.2', 'logps_train/chosen': '-112.13', 'loss/train': '0.70967', 'examples_per_second': '32.277', 'grad_norm': '53', 'counters/examples': 124512, 'counters/updates': 3891}
skipping logging after 124544 examples to avoid logging too frequently
train stats after 124576 examples: {'rewards_train/chosen': '0.10501', 'rewards_train/rejected': '0.081695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023317', 'logps_train/rejected': '-117.55', 'logps_train/chosen': '-140.87', 'loss/train': '0.71313', 'examples_per_second': '27.136', 'grad_norm': '205', 'counters/examples': 124576, 'counters/updates': 3893}
train stats after 124608 examples: {'rewards_train/chosen': '0.084831', 'rewards_train/rejected': '0.067222', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017609', 'logps_train/rejected': '-139.7', 'logps_train/chosen': '-145.78', 'loss/train': '0.70031', 'examples_per_second': '31.172', 'grad_norm': '108.5', 'counters/examples': 124608, 'counters/updates': 3894}
train stats after 124640 examples: {'rewards_train/chosen': '0.10161', 'rewards_train/rejected': '0.10807', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0064579', 'logps_train/rejected': '-196.75', 'logps_train/chosen': '-151.23', 'loss/train': '0.72795', 'examples_per_second': '33.143', 'grad_norm': '71.5', 'counters/examples': 124640, 'counters/updates': 3895}
train stats after 124672 examples: {'rewards_train/chosen': '0.047382', 'rewards_train/rejected': '0.13895', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.091565', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-165.44', 'loss/train': '0.74793', 'examples_per_second': '23.94', 'grad_norm': '65', 'counters/examples': 124672, 'counters/updates': 3896}
train stats after 124704 examples: {'rewards_train/chosen': '0.014564', 'rewards_train/rejected': '-0.012992', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027556', 'logps_train/rejected': '-125.78', 'logps_train/chosen': '-156.8', 'loss/train': '0.70251', 'examples_per_second': '30.674', 'grad_norm': '205', 'counters/examples': 124704, 'counters/updates': 3897}
train stats after 124736 examples: {'rewards_train/chosen': '0.10118', 'rewards_train/rejected': '0.12529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02411', 'logps_train/rejected': '-157.37', 'logps_train/chosen': '-184.08', 'loss/train': '0.72255', 'examples_per_second': '30.839', 'grad_norm': '104.5', 'counters/examples': 124736, 'counters/updates': 3898}
train stats after 124768 examples: {'rewards_train/chosen': '0.096488', 'rewards_train/rejected': '0.022573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073915', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-139.32', 'loss/train': '0.66522', 'examples_per_second': '30.85', 'grad_norm': '55', 'counters/examples': 124768, 'counters/updates': 3899}
skipping logging after 124800 examples to avoid logging too frequently
train stats after 124832 examples: {'rewards_train/chosen': '0.10038', 'rewards_train/rejected': '-0.0057206', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1061', 'logps_train/rejected': '-137.25', 'logps_train/chosen': '-140.26', 'loss/train': '0.64999', 'examples_per_second': '32.597', 'grad_norm': '70', 'counters/examples': 124832, 'counters/updates': 3901}
train stats after 124864 examples: {'rewards_train/chosen': '0.11663', 'rewards_train/rejected': '-0.01197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1286', 'logps_train/rejected': '-122.06', 'logps_train/chosen': '-204.52', 'loss/train': '0.64871', 'examples_per_second': '31.677', 'grad_norm': '96.5', 'counters/examples': 124864, 'counters/updates': 3902}
train stats after 124896 examples: {'rewards_train/chosen': '-0.014296', 'rewards_train/rejected': '0.046092', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.060388', 'logps_train/rejected': '-142.46', 'logps_train/chosen': '-149.99', 'loss/train': '0.73957', 'examples_per_second': '31.66', 'grad_norm': '62', 'counters/examples': 124896, 'counters/updates': 3903}
train stats after 124928 examples: {'rewards_train/chosen': '0.061372', 'rewards_train/rejected': '0.037041', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024331', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-125.78', 'loss/train': '0.69648', 'examples_per_second': '31.674', 'grad_norm': '90', 'counters/examples': 124928, 'counters/updates': 3904}
train stats after 124960 examples: {'rewards_train/chosen': '0.055966', 'rewards_train/rejected': '0.10396', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.047998', 'logps_train/rejected': '-140.77', 'logps_train/chosen': '-150.11', 'loss/train': '0.73447', 'examples_per_second': '33.336', 'grad_norm': '123', 'counters/examples': 124960, 'counters/updates': 3905}
train stats after 124992 examples: {'rewards_train/chosen': '0.060879', 'rewards_train/rejected': '0.024982', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035897', 'logps_train/rejected': '-152.31', 'logps_train/chosen': '-180.06', 'loss/train': '0.68803', 'examples_per_second': '31.661', 'grad_norm': '229', 'counters/examples': 124992, 'counters/updates': 3906}
train stats after 125024 examples: {'rewards_train/chosen': '0.052873', 'rewards_train/rejected': '0.053438', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.00056491', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-130.81', 'loss/train': '0.70728', 'examples_per_second': '31.572', 'grad_norm': '174', 'counters/examples': 125024, 'counters/updates': 3907}
train stats after 125056 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '0.0064013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095858', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-179.92', 'loss/train': '0.664', 'examples_per_second': '31.66', 'grad_norm': '155', 'counters/examples': 125056, 'counters/updates': 3908}
train stats after 125088 examples: {'rewards_train/chosen': '-0.014293', 'rewards_train/rejected': '0.064261', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.078554', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-147.99', 'loss/train': '0.74001', 'examples_per_second': '31.266', 'grad_norm': '131', 'counters/examples': 125088, 'counters/updates': 3909}
train stats after 125120 examples: {'rewards_train/chosen': '0.070092', 'rewards_train/rejected': '-0.0070251', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077117', 'logps_train/rejected': '-127.86', 'logps_train/chosen': '-197.48', 'loss/train': '0.67113', 'examples_per_second': '30.143', 'grad_norm': '151', 'counters/examples': 125120, 'counters/updates': 3910}
train stats after 125152 examples: {'rewards_train/chosen': '0.14881', 'rewards_train/rejected': '0.051266', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097548', 'logps_train/rejected': '-147.39', 'logps_train/chosen': '-194.28', 'loss/train': '0.65817', 'examples_per_second': '30.277', 'grad_norm': '450', 'counters/examples': 125152, 'counters/updates': 3911}
skipping logging after 125184 examples to avoid logging too frequently
train stats after 125216 examples: {'rewards_train/chosen': '0.029965', 'rewards_train/rejected': '0.021268', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0086974', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-141.93', 'loss/train': '0.7003', 'examples_per_second': '35.024', 'grad_norm': '98.5', 'counters/examples': 125216, 'counters/updates': 3913}
train stats after 125248 examples: {'rewards_train/chosen': '0.090388', 'rewards_train/rejected': '0.12766', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.037274', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-160.64', 'loss/train': '0.72646', 'examples_per_second': '32.553', 'grad_norm': '99', 'counters/examples': 125248, 'counters/updates': 3914}
train stats after 125280 examples: {'rewards_train/chosen': '0.13781', 'rewards_train/rejected': '0.00049194', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13732', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-133.94', 'loss/train': '0.64717', 'examples_per_second': '30.504', 'grad_norm': '68', 'counters/examples': 125280, 'counters/updates': 3915}
skipping logging after 125312 examples to avoid logging too frequently
train stats after 125344 examples: {'rewards_train/chosen': '0.098039', 'rewards_train/rejected': '0.0096931', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088346', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-155.88', 'loss/train': '0.67153', 'examples_per_second': '34.074', 'grad_norm': '67.5', 'counters/examples': 125344, 'counters/updates': 3917}
skipping logging after 125376 examples to avoid logging too frequently
train stats after 125408 examples: {'rewards_train/chosen': '0.01127', 'rewards_train/rejected': '0.12383', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11256', 'logps_train/rejected': '-162.67', 'logps_train/chosen': '-126.74', 'loss/train': '0.76235', 'examples_per_second': '34.387', 'grad_norm': '129', 'counters/examples': 125408, 'counters/updates': 3919}
train stats after 125440 examples: {'rewards_train/chosen': '0.023268', 'rewards_train/rejected': '0.019377', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038917', 'logps_train/rejected': '-101.2', 'logps_train/chosen': '-163.19', 'loss/train': '0.70838', 'examples_per_second': '30.366', 'grad_norm': '67', 'counters/examples': 125440, 'counters/updates': 3920}
train stats after 125472 examples: {'rewards_train/chosen': '0.049746', 'rewards_train/rejected': '0.018452', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031294', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-132.14', 'loss/train': '0.68674', 'examples_per_second': '31.401', 'grad_norm': '151', 'counters/examples': 125472, 'counters/updates': 3921}
train stats after 125504 examples: {'rewards_train/chosen': '0.069667', 'rewards_train/rejected': '0.047121', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022546', 'logps_train/rejected': '-109.78', 'logps_train/chosen': '-111.38', 'loss/train': '0.69087', 'examples_per_second': '30.274', 'grad_norm': '88', 'counters/examples': 125504, 'counters/updates': 3922}
train stats after 125536 examples: {'rewards_train/chosen': '0.079035', 'rewards_train/rejected': '0.036359', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042676', 'logps_train/rejected': '-110.45', 'logps_train/chosen': '-130.9', 'loss/train': '0.67963', 'examples_per_second': '31.639', 'grad_norm': '69', 'counters/examples': 125536, 'counters/updates': 3923}
train stats after 125568 examples: {'rewards_train/chosen': '0.079554', 'rewards_train/rejected': '-0.014026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09358', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-165.44', 'loss/train': '0.65842', 'examples_per_second': '30.568', 'grad_norm': '109', 'counters/examples': 125568, 'counters/updates': 3924}
train stats after 125600 examples: {'rewards_train/chosen': '0.045751', 'rewards_train/rejected': '0.099642', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.05389', 'logps_train/rejected': '-116.86', 'logps_train/chosen': '-150.67', 'loss/train': '0.73772', 'examples_per_second': '31.71', 'grad_norm': '82.5', 'counters/examples': 125600, 'counters/updates': 3925}
train stats after 125632 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.039578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10404', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-176.7', 'loss/train': '0.65731', 'examples_per_second': '29.525', 'grad_norm': '47.25', 'counters/examples': 125632, 'counters/updates': 3926}
train stats after 125664 examples: {'rewards_train/chosen': '0.052884', 'rewards_train/rejected': '-0.0089141', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061798', 'logps_train/rejected': '-146.62', 'logps_train/chosen': '-152.14', 'loss/train': '0.66868', 'examples_per_second': '30.144', 'grad_norm': '138', 'counters/examples': 125664, 'counters/updates': 3927}
train stats after 125696 examples: {'rewards_train/chosen': '0.10286', 'rewards_train/rejected': '0.043249', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.059614', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-137.62', 'loss/train': '0.67121', 'examples_per_second': '31.759', 'grad_norm': '61.5', 'counters/examples': 125696, 'counters/updates': 3928}
train stats after 125728 examples: {'rewards_train/chosen': '0.010795', 'rewards_train/rejected': '-0.067085', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07788', 'logps_train/rejected': '-157.96', 'logps_train/chosen': '-179.82', 'loss/train': '0.66199', 'examples_per_second': '31.646', 'grad_norm': '132', 'counters/examples': 125728, 'counters/updates': 3929}
train stats after 125760 examples: {'rewards_train/chosen': '0.13506', 'rewards_train/rejected': '-0.014636', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1497', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-166.7', 'loss/train': '0.63165', 'examples_per_second': '31.596', 'grad_norm': '80.5', 'counters/examples': 125760, 'counters/updates': 3930}
train stats after 125792 examples: {'rewards_train/chosen': '0.1246', 'rewards_train/rejected': '0.0553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069303', 'logps_train/rejected': '-168.7', 'logps_train/chosen': '-183.91', 'loss/train': '0.67371', 'examples_per_second': '31.618', 'grad_norm': '66', 'counters/examples': 125792, 'counters/updates': 3931}
train stats after 125824 examples: {'rewards_train/chosen': '0.1233', 'rewards_train/rejected': '-0.063515', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18682', 'logps_train/rejected': '-149.74', 'logps_train/chosen': '-153.52', 'loss/train': '0.61851', 'examples_per_second': '32.278', 'grad_norm': '82', 'counters/examples': 125824, 'counters/updates': 3932}
train stats after 125856 examples: {'rewards_train/chosen': '0.077978', 'rewards_train/rejected': '0.074924', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.003054', 'logps_train/rejected': '-150.17', 'logps_train/chosen': '-144.54', 'loss/train': '0.70332', 'examples_per_second': '31.742', 'grad_norm': '135', 'counters/examples': 125856, 'counters/updates': 3933}
skipping logging after 125888 examples to avoid logging too frequently
train stats after 125920 examples: {'rewards_train/chosen': '0.16586', 'rewards_train/rejected': '0.027313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13855', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-196.49', 'loss/train': '0.64056', 'examples_per_second': '30.546', 'grad_norm': '92.5', 'counters/examples': 125920, 'counters/updates': 3935}
train stats after 125952 examples: {'rewards_train/chosen': '0.16195', 'rewards_train/rejected': '-0.014858', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17681', 'logps_train/rejected': '-130.22', 'logps_train/chosen': '-163.2', 'loss/train': '0.62147', 'examples_per_second': '31.354', 'grad_norm': '54.75', 'counters/examples': 125952, 'counters/updates': 3936}
train stats after 125984 examples: {'rewards_train/chosen': '0.056895', 'rewards_train/rejected': '0.0011278', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055767', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-127.03', 'loss/train': '0.6745', 'examples_per_second': '33.039', 'grad_norm': '61', 'counters/examples': 125984, 'counters/updates': 3937}
train stats after 126016 examples: {'rewards_train/chosen': '0.1239', 'rewards_train/rejected': '0.061427', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062476', 'logps_train/rejected': '-125.56', 'logps_train/chosen': '-146.15', 'loss/train': '0.67609', 'examples_per_second': '31.287', 'grad_norm': '77.5', 'counters/examples': 126016, 'counters/updates': 3938}
train stats after 126048 examples: {'rewards_train/chosen': '0.087257', 'rewards_train/rejected': '0.069044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018212', 'logps_train/rejected': '-127.53', 'logps_train/chosen': '-137.66', 'loss/train': '0.6936', 'examples_per_second': '31.089', 'grad_norm': '69.5', 'counters/examples': 126048, 'counters/updates': 3939}
train stats after 126080 examples: {'rewards_train/chosen': '0.038109', 'rewards_train/rejected': '0.16572', 'rewards_train/accuracies': '0.21875', 'rewards_train/margins': '-0.12761', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-194.63', 'loss/train': '0.79978', 'examples_per_second': '31.611', 'grad_norm': '88.5', 'counters/examples': 126080, 'counters/updates': 3940}
skipping logging after 126112 examples to avoid logging too frequently
train stats after 126144 examples: {'rewards_train/chosen': '0.15672', 'rewards_train/rejected': '0.055468', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10125', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-168.34', 'loss/train': '0.66207', 'examples_per_second': '30.122', 'grad_norm': '66', 'counters/examples': 126144, 'counters/updates': 3942}
train stats after 126176 examples: {'rewards_train/chosen': '0.085982', 'rewards_train/rejected': '0.046868', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039114', 'logps_train/rejected': '-117.74', 'logps_train/chosen': '-122.06', 'loss/train': '0.69168', 'examples_per_second': '32.126', 'grad_norm': '91', 'counters/examples': 126176, 'counters/updates': 3943}
skipping logging after 126208 examples to avoid logging too frequently
train stats after 126240 examples: {'rewards_train/chosen': '0.07356', 'rewards_train/rejected': '0.016377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057183', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-151.43', 'loss/train': '0.67665', 'examples_per_second': '31.643', 'grad_norm': '62.5', 'counters/examples': 126240, 'counters/updates': 3945}
train stats after 126272 examples: {'rewards_train/chosen': '0.032886', 'rewards_train/rejected': '0.069236', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03635', 'logps_train/rejected': '-146.14', 'logps_train/chosen': '-182.98', 'loss/train': '0.72552', 'examples_per_second': '30.324', 'grad_norm': '121', 'counters/examples': 126272, 'counters/updates': 3946}
train stats after 126304 examples: {'rewards_train/chosen': '0.10924', 'rewards_train/rejected': '0.070707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038528', 'logps_train/rejected': '-161.43', 'logps_train/chosen': '-203.51', 'loss/train': '0.68754', 'examples_per_second': '30.866', 'grad_norm': '113', 'counters/examples': 126304, 'counters/updates': 3947}
train stats after 126336 examples: {'rewards_train/chosen': '0.087819', 'rewards_train/rejected': '0.11859', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.030772', 'logps_train/rejected': '-168.75', 'logps_train/chosen': '-123.97', 'loss/train': '0.7286', 'examples_per_second': '30.663', 'grad_norm': '50.5', 'counters/examples': 126336, 'counters/updates': 3948}
train stats after 126368 examples: {'rewards_train/chosen': '0.14425', 'rewards_train/rejected': '0.043058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10119', 'logps_train/rejected': '-151.41', 'logps_train/chosen': '-200.46', 'loss/train': '0.66882', 'examples_per_second': '30.458', 'grad_norm': '95.5', 'counters/examples': 126368, 'counters/updates': 3949}
train stats after 126400 examples: {'rewards_train/chosen': '0.13167', 'rewards_train/rejected': '0.09845', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.033221', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-200.54', 'loss/train': '0.70134', 'examples_per_second': '30.786', 'grad_norm': '87', 'counters/examples': 126400, 'counters/updates': 3950}
train stats after 126432 examples: {'rewards_train/chosen': '0.05123', 'rewards_train/rejected': '0.037101', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.014129', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-166.77', 'loss/train': '0.70503', 'examples_per_second': '31.14', 'grad_norm': '99.5', 'counters/examples': 126432, 'counters/updates': 3951}
train stats after 126464 examples: {'rewards_train/chosen': '0.090657', 'rewards_train/rejected': '0.036393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054264', 'logps_train/rejected': '-174.23', 'logps_train/chosen': '-179', 'loss/train': '0.68067', 'examples_per_second': '31.6', 'grad_norm': '120.5', 'counters/examples': 126464, 'counters/updates': 3952}
train stats after 126496 examples: {'rewards_train/chosen': '0.094787', 'rewards_train/rejected': '0.023155', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071633', 'logps_train/rejected': '-131.91', 'logps_train/chosen': '-134.72', 'loss/train': '0.66507', 'examples_per_second': '31.613', 'grad_norm': '85.5', 'counters/examples': 126496, 'counters/updates': 3953}
train stats after 126528 examples: {'rewards_train/chosen': '0.078778', 'rewards_train/rejected': '0.039656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039121', 'logps_train/rejected': '-101.43', 'logps_train/chosen': '-140.8', 'loss/train': '0.67934', 'examples_per_second': '31.618', 'grad_norm': '112.5', 'counters/examples': 126528, 'counters/updates': 3954}
train stats after 126560 examples: {'rewards_train/chosen': '0.026328', 'rewards_train/rejected': '0.092426', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.066099', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-127.52', 'loss/train': '0.73747', 'examples_per_second': '31.015', 'grad_norm': '79', 'counters/examples': 126560, 'counters/updates': 3955}
train stats after 126592 examples: {'rewards_train/chosen': '0.12458', 'rewards_train/rejected': '-0.027558', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15214', 'logps_train/rejected': '-102.55', 'logps_train/chosen': '-128.48', 'loss/train': '0.63312', 'examples_per_second': '30.538', 'grad_norm': '95.5', 'counters/examples': 126592, 'counters/updates': 3956}
train stats after 126624 examples: {'rewards_train/chosen': '0.092364', 'rewards_train/rejected': '0.091829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0005346', 'logps_train/rejected': '-115.3', 'logps_train/chosen': '-139.66', 'loss/train': '0.70887', 'examples_per_second': '30.305', 'grad_norm': '151', 'counters/examples': 126624, 'counters/updates': 3957}
train stats after 126656 examples: {'rewards_train/chosen': '0.087723', 'rewards_train/rejected': '0.016109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071615', 'logps_train/rejected': '-174.07', 'logps_train/chosen': '-170.91', 'loss/train': '0.673', 'examples_per_second': '32.383', 'grad_norm': '73', 'counters/examples': 126656, 'counters/updates': 3958}
skipping logging after 126688 examples to avoid logging too frequently
train stats after 126720 examples: {'rewards_train/chosen': '0.066932', 'rewards_train/rejected': '0.084643', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017711', 'logps_train/rejected': '-154.84', 'logps_train/chosen': '-161.34', 'loss/train': '0.71407', 'examples_per_second': '35.73', 'grad_norm': '85', 'counters/examples': 126720, 'counters/updates': 3960}
skipping logging after 126752 examples to avoid logging too frequently
train stats after 126784 examples: {'rewards_train/chosen': '-0.0086168', 'rewards_train/rejected': '0.017932', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026548', 'logps_train/rejected': '-105.21', 'logps_train/chosen': '-122.88', 'loss/train': '0.71257', 'examples_per_second': '31.444', 'grad_norm': '97.5', 'counters/examples': 126784, 'counters/updates': 3962}
train stats after 126816 examples: {'rewards_train/chosen': '0.080636', 'rewards_train/rejected': '-0.044759', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1254', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-150.07', 'loss/train': '0.64915', 'examples_per_second': '30.623', 'grad_norm': '71', 'counters/examples': 126816, 'counters/updates': 3963}
train stats after 126848 examples: {'rewards_train/chosen': '0.03619', 'rewards_train/rejected': '0.088898', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.052708', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-161.79', 'loss/train': '0.73392', 'examples_per_second': '30.6', 'grad_norm': '95', 'counters/examples': 126848, 'counters/updates': 3964}
train stats after 126880 examples: {'rewards_train/chosen': '0.070489', 'rewards_train/rejected': '-0.0044419', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074931', 'logps_train/rejected': '-106.12', 'logps_train/chosen': '-158.7', 'loss/train': '0.66292', 'examples_per_second': '31.658', 'grad_norm': '106', 'counters/examples': 126880, 'counters/updates': 3965}
train stats after 126912 examples: {'rewards_train/chosen': '0.025391', 'rewards_train/rejected': '0.090496', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.065105', 'logps_train/rejected': '-145.49', 'logps_train/chosen': '-110.76', 'loss/train': '0.73329', 'examples_per_second': '31.427', 'grad_norm': '104', 'counters/examples': 126912, 'counters/updates': 3966}
train stats after 126944 examples: {'rewards_train/chosen': '0.08656', 'rewards_train/rejected': '0.084798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0017624', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-141.44', 'loss/train': '0.69651', 'examples_per_second': '24.568', 'grad_norm': '84', 'counters/examples': 126944, 'counters/updates': 3967}
train stats after 126976 examples: {'rewards_train/chosen': '0.011318', 'rewards_train/rejected': '0.030268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018951', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-152.77', 'loss/train': '0.71372', 'examples_per_second': '31.6', 'grad_norm': '96', 'counters/examples': 126976, 'counters/updates': 3968}
train stats after 127008 examples: {'rewards_train/chosen': '0.19153', 'rewards_train/rejected': '0.014157', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17737', 'logps_train/rejected': '-117.85', 'logps_train/chosen': '-152.29', 'loss/train': '0.63054', 'examples_per_second': '33.098', 'grad_norm': '59', 'counters/examples': 127008, 'counters/updates': 3969}
train stats after 127040 examples: {'rewards_train/chosen': '0.038873', 'rewards_train/rejected': '0.031198', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0076747', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-133.93', 'loss/train': '0.69513', 'examples_per_second': '31.347', 'grad_norm': '70', 'counters/examples': 127040, 'counters/updates': 3970}
train stats after 127072 examples: {'rewards_train/chosen': '0.11971', 'rewards_train/rejected': '0.11898', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00073259', 'logps_train/rejected': '-167.28', 'logps_train/chosen': '-198.86', 'loss/train': '0.70497', 'examples_per_second': '31.821', 'grad_norm': '83', 'counters/examples': 127072, 'counters/updates': 3971}
train stats after 127104 examples: {'rewards_train/chosen': '0.11138', 'rewards_train/rejected': '-0.028139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13952', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-137.39', 'loss/train': '0.64428', 'examples_per_second': '30.664', 'grad_norm': '80', 'counters/examples': 127104, 'counters/updates': 3972}
train stats after 127136 examples: {'rewards_train/chosen': '0.11067', 'rewards_train/rejected': '0.039146', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071528', 'logps_train/rejected': '-102.36', 'logps_train/chosen': '-177.53', 'loss/train': '0.67004', 'examples_per_second': '32.819', 'grad_norm': '85', 'counters/examples': 127136, 'counters/updates': 3973}
skipping logging after 127168 examples to avoid logging too frequently
train stats after 127200 examples: {'rewards_train/chosen': '0.057312', 'rewards_train/rejected': '0.073103', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015791', 'logps_train/rejected': '-129.43', 'logps_train/chosen': '-130.41', 'loss/train': '0.70742', 'examples_per_second': '30.155', 'grad_norm': '120.5', 'counters/examples': 127200, 'counters/updates': 3975}
train stats after 127232 examples: {'rewards_train/chosen': '0.15824', 'rewards_train/rejected': '0.065233', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.09301', 'logps_train/rejected': '-101.61', 'logps_train/chosen': '-181.66', 'loss/train': '0.67592', 'examples_per_second': '30.819', 'grad_norm': '121.5', 'counters/examples': 127232, 'counters/updates': 3976}
train stats after 127264 examples: {'rewards_train/chosen': '0.16123', 'rewards_train/rejected': '-0.0033652', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1646', 'logps_train/rejected': '-151.98', 'logps_train/chosen': '-199.56', 'loss/train': '0.62709', 'examples_per_second': '31.52', 'grad_norm': '66', 'counters/examples': 127264, 'counters/updates': 3977}
train stats after 127296 examples: {'rewards_train/chosen': '0.13245', 'rewards_train/rejected': '0.010189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12226', 'logps_train/rejected': '-156.13', 'logps_train/chosen': '-176.62', 'loss/train': '0.65162', 'examples_per_second': '31.658', 'grad_norm': '103.5', 'counters/examples': 127296, 'counters/updates': 3978}
train stats after 127328 examples: {'rewards_train/chosen': '0.092577', 'rewards_train/rejected': '0.02383', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.068747', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-150.96', 'loss/train': '0.66888', 'examples_per_second': '32.4', 'grad_norm': '152', 'counters/examples': 127328, 'counters/updates': 3979}
train stats after 127360 examples: {'rewards_train/chosen': '0.05933', 'rewards_train/rejected': '0.11917', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.059845', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-130.04', 'loss/train': '0.7336', 'examples_per_second': '31.163', 'grad_norm': '94.5', 'counters/examples': 127360, 'counters/updates': 3980}
train stats after 127392 examples: {'rewards_train/chosen': '0.11086', 'rewards_train/rejected': '0.092748', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018108', 'logps_train/rejected': '-170.65', 'logps_train/chosen': '-191.71', 'loss/train': '0.69676', 'examples_per_second': '31.751', 'grad_norm': '167', 'counters/examples': 127392, 'counters/updates': 3981}
train stats after 127424 examples: {'rewards_train/chosen': '0.066237', 'rewards_train/rejected': '0.040521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025716', 'logps_train/rejected': '-126.7', 'logps_train/chosen': '-179.61', 'loss/train': '0.69521', 'examples_per_second': '30.431', 'grad_norm': '66', 'counters/examples': 127424, 'counters/updates': 3982}
skipping logging after 127456 examples to avoid logging too frequently
train stats after 127488 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '0.0060063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098405', 'logps_train/rejected': '-122.56', 'logps_train/chosen': '-132.35', 'loss/train': '0.65406', 'examples_per_second': '30.554', 'grad_norm': '101', 'counters/examples': 127488, 'counters/updates': 3984}
train stats after 127520 examples: {'rewards_train/chosen': '0.073726', 'rewards_train/rejected': '0.037243', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.036483', 'logps_train/rejected': '-161.52', 'logps_train/chosen': '-149.61', 'loss/train': '0.7012', 'examples_per_second': '33.286', 'grad_norm': '174', 'counters/examples': 127520, 'counters/updates': 3985}
train stats after 127552 examples: {'rewards_train/chosen': '0.050184', 'rewards_train/rejected': '0.05982', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0096359', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-140.62', 'loss/train': '0.70672', 'examples_per_second': '31.478', 'grad_norm': '84', 'counters/examples': 127552, 'counters/updates': 3986}
skipping logging after 127584 examples to avoid logging too frequently
train stats after 127616 examples: {'rewards_train/chosen': '0.06122', 'rewards_train/rejected': '0.098594', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.037374', 'logps_train/rejected': '-130.21', 'logps_train/chosen': '-143.03', 'loss/train': '0.7207', 'examples_per_second': '30.103', 'grad_norm': '63', 'counters/examples': 127616, 'counters/updates': 3988}
skipping logging after 127648 examples to avoid logging too frequently
train stats after 127680 examples: {'rewards_train/chosen': '0.15133', 'rewards_train/rejected': '0.035855', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11548', 'logps_train/rejected': '-146.43', 'logps_train/chosen': '-142.07', 'loss/train': '0.66153', 'examples_per_second': '32.491', 'grad_norm': '69.5', 'counters/examples': 127680, 'counters/updates': 3990}
train stats after 127712 examples: {'rewards_train/chosen': '0.067634', 'rewards_train/rejected': '0.096689', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.029055', 'logps_train/rejected': '-154.87', 'logps_train/chosen': '-185.75', 'loss/train': '0.71905', 'examples_per_second': '31.451', 'grad_norm': '110.5', 'counters/examples': 127712, 'counters/updates': 3991}
train stats after 127744 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '-0.0062349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16602', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-148.6', 'loss/train': '0.65857', 'examples_per_second': '29.962', 'grad_norm': '78', 'counters/examples': 127744, 'counters/updates': 3992}
train stats after 127776 examples: {'rewards_train/chosen': '0.048895', 'rewards_train/rejected': '-0.044506', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0934', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-122.66', 'loss/train': '0.66202', 'examples_per_second': '31.679', 'grad_norm': '70', 'counters/examples': 127776, 'counters/updates': 3993}
train stats after 127808 examples: {'rewards_train/chosen': '0.071172', 'rewards_train/rejected': '0.0067355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064436', 'logps_train/rejected': '-154.12', 'logps_train/chosen': '-152.08', 'loss/train': '0.67342', 'examples_per_second': '30.121', 'grad_norm': '166', 'counters/examples': 127808, 'counters/updates': 3994}
train stats after 127840 examples: {'rewards_train/chosen': '0.053751', 'rewards_train/rejected': '-0.025871', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079622', 'logps_train/rejected': '-153.97', 'logps_train/chosen': '-182.9', 'loss/train': '0.70951', 'examples_per_second': '32.46', 'grad_norm': '74.5', 'counters/examples': 127840, 'counters/updates': 3995}
train stats after 127872 examples: {'rewards_train/chosen': '0.051441', 'rewards_train/rejected': '-0.033793', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085234', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-136.29', 'loss/train': '0.66013', 'examples_per_second': '31.654', 'grad_norm': '340', 'counters/examples': 127872, 'counters/updates': 3996}
train stats after 127904 examples: {'rewards_train/chosen': '0.009251', 'rewards_train/rejected': '0.047053', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037802', 'logps_train/rejected': '-92.667', 'logps_train/chosen': '-100.58', 'loss/train': '0.72015', 'examples_per_second': '32.636', 'grad_norm': '85.5', 'counters/examples': 127904, 'counters/updates': 3997}
train stats after 127936 examples: {'rewards_train/chosen': '0.13842', 'rewards_train/rejected': '0.033834', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10458', 'logps_train/rejected': '-166.8', 'logps_train/chosen': '-166.24', 'loss/train': '0.65122', 'examples_per_second': '33.226', 'grad_norm': '78', 'counters/examples': 127936, 'counters/updates': 3998}
train stats after 127968 examples: {'rewards_train/chosen': '0.17176', 'rewards_train/rejected': '0.054886', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11687', 'logps_train/rejected': '-120.56', 'logps_train/chosen': '-179.13', 'loss/train': '0.65995', 'examples_per_second': '31.618', 'grad_norm': '98.5', 'counters/examples': 127968, 'counters/updates': 3999}
train stats after 128000 examples: {'rewards_train/chosen': '0.037663', 'rewards_train/rejected': '-0.076539', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1142', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-127.93', 'loss/train': '0.73286', 'examples_per_second': '30.661', 'grad_norm': '62.25', 'counters/examples': 128000, 'counters/updates': 4000}
skipping logging after 128032 examples to avoid logging too frequently
train stats after 128064 examples: {'rewards_train/chosen': '0.075609', 'rewards_train/rejected': '0.18698', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.11137', 'logps_train/rejected': '-160.3', 'logps_train/chosen': '-131.8', 'loss/train': '0.76507', 'examples_per_second': '32.104', 'grad_norm': '68.5', 'counters/examples': 128064, 'counters/updates': 4002}
train stats after 128096 examples: {'rewards_train/chosen': '0.13559', 'rewards_train/rejected': '0.080799', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054793', 'logps_train/rejected': '-179.64', 'logps_train/chosen': '-140.34', 'loss/train': '0.68801', 'examples_per_second': '31.659', 'grad_norm': '76', 'counters/examples': 128096, 'counters/updates': 4003}
train stats after 128128 examples: {'rewards_train/chosen': '-0.021437', 'rewards_train/rejected': '0.079043', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.10048', 'logps_train/rejected': '-122.18', 'logps_train/chosen': '-120.29', 'loss/train': '0.749', 'examples_per_second': '31.637', 'grad_norm': '96', 'counters/examples': 128128, 'counters/updates': 4004}
train stats after 128160 examples: {'rewards_train/chosen': '0.0471', 'rewards_train/rejected': '0.046688', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.000412', 'logps_train/rejected': '-105.86', 'logps_train/chosen': '-126.76', 'loss/train': '0.70094', 'examples_per_second': '31.351', 'grad_norm': '73', 'counters/examples': 128160, 'counters/updates': 4005}
train stats after 128192 examples: {'rewards_train/chosen': '0.13095', 'rewards_train/rejected': '0.061949', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069004', 'logps_train/rejected': '-113.44', 'logps_train/chosen': '-138.95', 'loss/train': '0.67209', 'examples_per_second': '32.19', 'grad_norm': '61.5', 'counters/examples': 128192, 'counters/updates': 4006}
train stats after 128224 examples: {'rewards_train/chosen': '0.074625', 'rewards_train/rejected': '-0.026825', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10145', 'logps_train/rejected': '-129.64', 'logps_train/chosen': '-144.45', 'loss/train': '0.65542', 'examples_per_second': '31.632', 'grad_norm': '131', 'counters/examples': 128224, 'counters/updates': 4007}
train stats after 128256 examples: {'rewards_train/chosen': '0.11836', 'rewards_train/rejected': '0.062775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055582', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-176.73', 'loss/train': '0.67837', 'examples_per_second': '31.506', 'grad_norm': '120.5', 'counters/examples': 128256, 'counters/updates': 4008}
skipping logging after 128288 examples to avoid logging too frequently
train stats after 128320 examples: {'rewards_train/chosen': '0.082611', 'rewards_train/rejected': '0.093822', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011212', 'logps_train/rejected': '-131.26', 'logps_train/chosen': '-186.32', 'loss/train': '0.71346', 'examples_per_second': '30.066', 'grad_norm': '115', 'counters/examples': 128320, 'counters/updates': 4010}
train stats after 128352 examples: {'rewards_train/chosen': '0.10532', 'rewards_train/rejected': '0.062778', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042546', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-128.33', 'loss/train': '0.6991', 'examples_per_second': '30.966', 'grad_norm': '47.25', 'counters/examples': 128352, 'counters/updates': 4011}
train stats after 128384 examples: {'rewards_train/chosen': '-0.0090765', 'rewards_train/rejected': '0.029755', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038832', 'logps_train/rejected': '-157.93', 'logps_train/chosen': '-175.55', 'loss/train': '0.73084', 'examples_per_second': '31.391', 'grad_norm': '83.5', 'counters/examples': 128384, 'counters/updates': 4012}
train stats after 128416 examples: {'rewards_train/chosen': '0.14259', 'rewards_train/rejected': '0.0078304', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13476', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-140.96', 'loss/train': '0.64642', 'examples_per_second': '31.597', 'grad_norm': '142', 'counters/examples': 128416, 'counters/updates': 4013}
train stats after 128448 examples: {'rewards_train/chosen': '-0.013854', 'rewards_train/rejected': '0.001255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015109', 'logps_train/rejected': '-150.61', 'logps_train/chosen': '-125.65', 'loss/train': '0.71686', 'examples_per_second': '30.928', 'grad_norm': '102.5', 'counters/examples': 128448, 'counters/updates': 4014}
skipping logging after 128480 examples to avoid logging too frequently
train stats after 128512 examples: {'rewards_train/chosen': '0.058999', 'rewards_train/rejected': '0.088263', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029264', 'logps_train/rejected': '-153.55', 'logps_train/chosen': '-162.4', 'loss/train': '0.72078', 'examples_per_second': '31.03', 'grad_norm': '113', 'counters/examples': 128512, 'counters/updates': 4016}
train stats after 128544 examples: {'rewards_train/chosen': '0.033599', 'rewards_train/rejected': '0.091537', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.057938', 'logps_train/rejected': '-141.54', 'logps_train/chosen': '-141.48', 'loss/train': '0.73101', 'examples_per_second': '32.356', 'grad_norm': '95', 'counters/examples': 128544, 'counters/updates': 4017}
train stats after 128576 examples: {'rewards_train/chosen': '0.10159', 'rewards_train/rejected': '0.02544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076148', 'logps_train/rejected': '-137.6', 'logps_train/chosen': '-155.29', 'loss/train': '0.66785', 'examples_per_second': '30.945', 'grad_norm': '79', 'counters/examples': 128576, 'counters/updates': 4018}
train stats after 128608 examples: {'rewards_train/chosen': '0.13338', 'rewards_train/rejected': '0.037601', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095778', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-136.83', 'loss/train': '0.66036', 'examples_per_second': '31.635', 'grad_norm': '113.5', 'counters/examples': 128608, 'counters/updates': 4019}
train stats after 128640 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.10245', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020154', 'logps_train/rejected': '-153.1', 'logps_train/chosen': '-176.33', 'loss/train': '0.696', 'examples_per_second': '30.311', 'grad_norm': '132', 'counters/examples': 128640, 'counters/updates': 4020}
train stats after 128672 examples: {'rewards_train/chosen': '0.064359', 'rewards_train/rejected': '0.023251', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041108', 'logps_train/rejected': '-172.63', 'logps_train/chosen': '-168.98', 'loss/train': '0.69194', 'examples_per_second': '31.168', 'grad_norm': '79.5', 'counters/examples': 128672, 'counters/updates': 4021}
train stats after 128704 examples: {'rewards_train/chosen': '0.11567', 'rewards_train/rejected': '0.14406', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028396', 'logps_train/rejected': '-145.3', 'logps_train/chosen': '-127.17', 'loss/train': '0.72334', 'examples_per_second': '31.746', 'grad_norm': '104', 'counters/examples': 128704, 'counters/updates': 4022}
skipping logging after 128736 examples to avoid logging too frequently
train stats after 128768 examples: {'rewards_train/chosen': '0.081738', 'rewards_train/rejected': '0.084665', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0029266', 'logps_train/rejected': '-151.74', 'logps_train/chosen': '-190.58', 'loss/train': '0.71049', 'examples_per_second': '30.777', 'grad_norm': '110.5', 'counters/examples': 128768, 'counters/updates': 4024}
train stats after 128800 examples: {'rewards_train/chosen': '0.086734', 'rewards_train/rejected': '0.036603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050131', 'logps_train/rejected': '-105.15', 'logps_train/chosen': '-119.47', 'loss/train': '0.67759', 'examples_per_second': '30.939', 'grad_norm': '87.5', 'counters/examples': 128800, 'counters/updates': 4025}
train stats after 128832 examples: {'rewards_train/chosen': '0.064249', 'rewards_train/rejected': '0.075589', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01134', 'logps_train/rejected': '-159.09', 'logps_train/chosen': '-164.51', 'loss/train': '0.70945', 'examples_per_second': '31.496', 'grad_norm': '60.75', 'counters/examples': 128832, 'counters/updates': 4026}
skipping logging after 128864 examples to avoid logging too frequently
train stats after 128896 examples: {'rewards_train/chosen': '0.035587', 'rewards_train/rejected': '0.069028', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033441', 'logps_train/rejected': '-143.51', 'logps_train/chosen': '-151.36', 'loss/train': '0.71962', 'examples_per_second': '32.043', 'grad_norm': '536', 'counters/examples': 128896, 'counters/updates': 4028}
train stats after 128928 examples: {'rewards_train/chosen': '0.03111', 'rewards_train/rejected': '0.079124', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.048015', 'logps_train/rejected': '-168.08', 'logps_train/chosen': '-162.96', 'loss/train': '0.728', 'examples_per_second': '30.828', 'grad_norm': '181', 'counters/examples': 128928, 'counters/updates': 4029}
train stats after 128960 examples: {'rewards_train/chosen': '0.052022', 'rewards_train/rejected': '-0.0052039', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057226', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-151.3', 'loss/train': '0.69183', 'examples_per_second': '32.041', 'grad_norm': '62.5', 'counters/examples': 128960, 'counters/updates': 4030}
train stats after 128992 examples: {'rewards_train/chosen': '0.11477', 'rewards_train/rejected': '0.070096', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044675', 'logps_train/rejected': '-151.14', 'logps_train/chosen': '-188.83', 'loss/train': '0.6817', 'examples_per_second': '31.978', 'grad_norm': '132', 'counters/examples': 128992, 'counters/updates': 4031}
train stats after 129024 examples: {'rewards_train/chosen': '0.044536', 'rewards_train/rejected': '0.0732', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028664', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-110.96', 'loss/train': '0.71754', 'examples_per_second': '31.304', 'grad_norm': '77.5', 'counters/examples': 129024, 'counters/updates': 4032}
train stats after 129056 examples: {'rewards_train/chosen': '0.0068965', 'rewards_train/rejected': '-0.0069653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013862', 'logps_train/rejected': '-122.26', 'logps_train/chosen': '-133.82', 'loss/train': '0.69735', 'examples_per_second': '32.904', 'grad_norm': '60.5', 'counters/examples': 129056, 'counters/updates': 4033}
train stats after 129088 examples: {'rewards_train/chosen': '0.016777', 'rewards_train/rejected': '0.069015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.052238', 'logps_train/rejected': '-111.09', 'logps_train/chosen': '-151.07', 'loss/train': '0.74069', 'examples_per_second': '32.343', 'grad_norm': '174', 'counters/examples': 129088, 'counters/updates': 4034}
train stats after 129120 examples: {'rewards_train/chosen': '0.078036', 'rewards_train/rejected': '0.11571', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03767', 'logps_train/rejected': '-158.79', 'logps_train/chosen': '-138.78', 'loss/train': '0.72729', 'examples_per_second': '32.778', 'grad_norm': '70', 'counters/examples': 129120, 'counters/updates': 4035}
train stats after 129152 examples: {'rewards_train/chosen': '0.049496', 'rewards_train/rejected': '0.0079367', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041559', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-135.53', 'loss/train': '0.67942', 'examples_per_second': '31.627', 'grad_norm': '84', 'counters/examples': 129152, 'counters/updates': 4036}
train stats after 129184 examples: {'rewards_train/chosen': '0.13124', 'rewards_train/rejected': '0.013871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11737', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-152.8', 'loss/train': '0.64567', 'examples_per_second': '31.615', 'grad_norm': '55.75', 'counters/examples': 129184, 'counters/updates': 4037}
skipping logging after 129216 examples to avoid logging too frequently
train stats after 129248 examples: {'rewards_train/chosen': '0.066169', 'rewards_train/rejected': '-0.018726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084895', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-165.94', 'loss/train': '0.66223', 'examples_per_second': '34.677', 'grad_norm': '69', 'counters/examples': 129248, 'counters/updates': 4039}
train stats after 129280 examples: {'rewards_train/chosen': '0.034581', 'rewards_train/rejected': '0.032291', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0022907', 'logps_train/rejected': '-129.16', 'logps_train/chosen': '-132.67', 'loss/train': '0.70123', 'examples_per_second': '31.587', 'grad_norm': '70.5', 'counters/examples': 129280, 'counters/updates': 4040}
train stats after 129312 examples: {'rewards_train/chosen': '0.063703', 'rewards_train/rejected': '0.065921', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0022172', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-155.18', 'loss/train': '0.70569', 'examples_per_second': '32.672', 'grad_norm': '113.5', 'counters/examples': 129312, 'counters/updates': 4041}
train stats after 129344 examples: {'rewards_train/chosen': '0.051718', 'rewards_train/rejected': '-0.029895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081613', 'logps_train/rejected': '-127.99', 'logps_train/chosen': '-133.84', 'loss/train': '0.66735', 'examples_per_second': '30.084', 'grad_norm': '139', 'counters/examples': 129344, 'counters/updates': 4042}
train stats after 129376 examples: {'rewards_train/chosen': '0.093756', 'rewards_train/rejected': '0.071364', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022392', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-121.83', 'loss/train': '0.68894', 'examples_per_second': '31.09', 'grad_norm': '65', 'counters/examples': 129376, 'counters/updates': 4043}
train stats after 129408 examples: {'rewards_train/chosen': '0.099122', 'rewards_train/rejected': '0.093198', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0059233', 'logps_train/rejected': '-142.01', 'logps_train/chosen': '-160.59', 'loss/train': '0.70085', 'examples_per_second': '33.069', 'grad_norm': '142', 'counters/examples': 129408, 'counters/updates': 4044}
train stats after 129440 examples: {'rewards_train/chosen': '0.1379', 'rewards_train/rejected': '0.08789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050007', 'logps_train/rejected': '-184.94', 'logps_train/chosen': '-135.83', 'loss/train': '0.69533', 'examples_per_second': '31.512', 'grad_norm': '107', 'counters/examples': 129440, 'counters/updates': 4045}
train stats after 129472 examples: {'rewards_train/chosen': '0.075094', 'rewards_train/rejected': '-0.00064475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075739', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-120.69', 'loss/train': '0.67011', 'examples_per_second': '31.58', 'grad_norm': '119.5', 'counters/examples': 129472, 'counters/updates': 4046}
train stats after 129504 examples: {'rewards_train/chosen': '0.11155', 'rewards_train/rejected': '0.074607', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03694', 'logps_train/rejected': '-175.42', 'logps_train/chosen': '-174.21', 'loss/train': '0.69869', 'examples_per_second': '31.599', 'grad_norm': '94', 'counters/examples': 129504, 'counters/updates': 4047}
train stats after 129536 examples: {'rewards_train/chosen': '0.086378', 'rewards_train/rejected': '0.070431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015947', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-162.92', 'loss/train': '0.70047', 'examples_per_second': '31.608', 'grad_norm': '73.5', 'counters/examples': 129536, 'counters/updates': 4048}
train stats after 129568 examples: {'rewards_train/chosen': '0.040054', 'rewards_train/rejected': '0.11882', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.078763', 'logps_train/rejected': '-167.79', 'logps_train/chosen': '-131.76', 'loss/train': '0.74694', 'examples_per_second': '31.526', 'grad_norm': '282', 'counters/examples': 129568, 'counters/updates': 4049}
train stats after 129600 examples: {'rewards_train/chosen': '0.19635', 'rewards_train/rejected': '0.10199', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.094362', 'logps_train/rejected': '-164.31', 'logps_train/chosen': '-194.07', 'loss/train': '0.66303', 'examples_per_second': '31.543', 'grad_norm': '133', 'counters/examples': 129600, 'counters/updates': 4050}
train stats after 129632 examples: {'rewards_train/chosen': '0.10012', 'rewards_train/rejected': '0.1249', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024778', 'logps_train/rejected': '-172.01', 'logps_train/chosen': '-168.28', 'loss/train': '0.71967', 'examples_per_second': '33.091', 'grad_norm': '185', 'counters/examples': 129632, 'counters/updates': 4051}
train stats after 129664 examples: {'rewards_train/chosen': '0.18984', 'rewards_train/rejected': '0.015362', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17448', 'logps_train/rejected': '-134.97', 'logps_train/chosen': '-148.58', 'loss/train': '0.62278', 'examples_per_second': '30.78', 'grad_norm': '77', 'counters/examples': 129664, 'counters/updates': 4052}
train stats after 129696 examples: {'rewards_train/chosen': '0.071898', 'rewards_train/rejected': '0.039624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032273', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-142.57', 'loss/train': '0.69118', 'examples_per_second': '31.569', 'grad_norm': '179', 'counters/examples': 129696, 'counters/updates': 4053}
skipping logging after 129728 examples to avoid logging too frequently
train stats after 129760 examples: {'rewards_train/chosen': '0.083349', 'rewards_train/rejected': '0.066771', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016578', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-155.4', 'loss/train': '0.69805', 'examples_per_second': '31.586', 'grad_norm': '77.5', 'counters/examples': 129760, 'counters/updates': 4055}
train stats after 129792 examples: {'rewards_train/chosen': '0.13792', 'rewards_train/rejected': '-0.024793', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16271', 'logps_train/rejected': '-186.92', 'logps_train/chosen': '-166.74', 'loss/train': '0.64301', 'examples_per_second': '31.337', 'grad_norm': '77.5', 'counters/examples': 129792, 'counters/updates': 4056}
train stats after 129824 examples: {'rewards_train/chosen': '0.034131', 'rewards_train/rejected': '0.12544', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.091311', 'logps_train/rejected': '-161.27', 'logps_train/chosen': '-155.35', 'loss/train': '0.7532', 'examples_per_second': '31.586', 'grad_norm': '53.25', 'counters/examples': 129824, 'counters/updates': 4057}
skipping logging after 129856 examples to avoid logging too frequently
train stats after 129888 examples: {'rewards_train/chosen': '0.061434', 'rewards_train/rejected': '0.10018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03875', 'logps_train/rejected': '-149.94', 'logps_train/chosen': '-153.45', 'loss/train': '0.72611', 'examples_per_second': '34.194', 'grad_norm': '132', 'counters/examples': 129888, 'counters/updates': 4059}
skipping logging after 129920 examples to avoid logging too frequently
train stats after 129952 examples: {'rewards_train/chosen': '0.040971', 'rewards_train/rejected': '0.071641', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03067', 'logps_train/rejected': '-117.85', 'logps_train/chosen': '-131.39', 'loss/train': '0.71718', 'examples_per_second': '31.101', 'grad_norm': '85.5', 'counters/examples': 129952, 'counters/updates': 4061}
train stats after 129984 examples: {'rewards_train/chosen': '0.0075042', 'rewards_train/rejected': '0.018894', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01139', 'logps_train/rejected': '-95.247', 'logps_train/chosen': '-143.27', 'loss/train': '0.70457', 'examples_per_second': '32.136', 'grad_norm': '105.5', 'counters/examples': 129984, 'counters/updates': 4062}
train stats after 130016 examples: {'rewards_train/chosen': '0.059971', 'rewards_train/rejected': '0.023376', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036595', 'logps_train/rejected': '-168.52', 'logps_train/chosen': '-135.87', 'loss/train': '0.69064', 'examples_per_second': '31.35', 'grad_norm': '264', 'counters/examples': 130016, 'counters/updates': 4063}
train stats after 130048 examples: {'rewards_train/chosen': '0.1003', 'rewards_train/rejected': '0.040755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059545', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-118.83', 'loss/train': '0.67838', 'examples_per_second': '23.451', 'grad_norm': '74.5', 'counters/examples': 130048, 'counters/updates': 4064}
train stats after 130080 examples: {'rewards_train/chosen': '0.058269', 'rewards_train/rejected': '0.073913', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015644', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-126.38', 'loss/train': '0.71003', 'examples_per_second': '30.15', 'grad_norm': '77.5', 'counters/examples': 130080, 'counters/updates': 4065}
skipping logging after 130112 examples to avoid logging too frequently
train stats after 130144 examples: {'rewards_train/chosen': '0.077919', 'rewards_train/rejected': '0.0096796', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068239', 'logps_train/rejected': '-107.52', 'logps_train/chosen': '-110.24', 'loss/train': '0.67365', 'examples_per_second': '29.498', 'grad_norm': '55.75', 'counters/examples': 130144, 'counters/updates': 4067}
skipping logging after 130176 examples to avoid logging too frequently
train stats after 130208 examples: {'rewards_train/chosen': '-0.022791', 'rewards_train/rejected': '0.086153', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10894', 'logps_train/rejected': '-112.47', 'logps_train/chosen': '-166.96', 'loss/train': '0.76275', 'examples_per_second': '32.223', 'grad_norm': '258', 'counters/examples': 130208, 'counters/updates': 4069}
train stats after 130240 examples: {'rewards_train/chosen': '-0.0042212', 'rewards_train/rejected': '0.0089916', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013213', 'logps_train/rejected': '-99.28', 'logps_train/chosen': '-149.82', 'loss/train': '0.7098', 'examples_per_second': '32.404', 'grad_norm': '73.5', 'counters/examples': 130240, 'counters/updates': 4070}
skipping logging after 130272 examples to avoid logging too frequently
train stats after 130304 examples: {'rewards_train/chosen': '0.022124', 'rewards_train/rejected': '0.058769', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036645', 'logps_train/rejected': '-107.28', 'logps_train/chosen': '-139.61', 'loss/train': '0.73337', 'examples_per_second': '31.561', 'grad_norm': '84.5', 'counters/examples': 130304, 'counters/updates': 4072}
train stats after 130336 examples: {'rewards_train/chosen': '0.065709', 'rewards_train/rejected': '-0.0097058', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075415', 'logps_train/rejected': '-160.27', 'logps_train/chosen': '-198.86', 'loss/train': '0.67127', 'examples_per_second': '31.547', 'grad_norm': '168', 'counters/examples': 130336, 'counters/updates': 4073}
train stats after 130368 examples: {'rewards_train/chosen': '0.078509', 'rewards_train/rejected': '0.11923', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040726', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-100.48', 'loss/train': '0.72163', 'examples_per_second': '30.524', 'grad_norm': '94', 'counters/examples': 130368, 'counters/updates': 4074}
train stats after 130400 examples: {'rewards_train/chosen': '0.081402', 'rewards_train/rejected': '-0.025704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10711', 'logps_train/rejected': '-107.76', 'logps_train/chosen': '-111.97', 'loss/train': '0.65349', 'examples_per_second': '32.872', 'grad_norm': '66.5', 'counters/examples': 130400, 'counters/updates': 4075}
train stats after 130432 examples: {'rewards_train/chosen': '0.10044', 'rewards_train/rejected': '0.10906', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.008626', 'logps_train/rejected': '-155.47', 'logps_train/chosen': '-119.27', 'loss/train': '0.71622', 'examples_per_second': '32.73', 'grad_norm': '320', 'counters/examples': 130432, 'counters/updates': 4076}
skipping logging after 130464 examples to avoid logging too frequently
train stats after 130496 examples: {'rewards_train/chosen': '0.059402', 'rewards_train/rejected': '0.06523', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.005828', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-164.61', 'loss/train': '0.71592', 'examples_per_second': '30.826', 'grad_norm': '68.5', 'counters/examples': 130496, 'counters/updates': 4078}
train stats after 130528 examples: {'rewards_train/chosen': '0.021193', 'rewards_train/rejected': '0.029107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0079133', 'logps_train/rejected': '-119.55', 'logps_train/chosen': '-174.45', 'loss/train': '0.70811', 'examples_per_second': '30.76', 'grad_norm': '71', 'counters/examples': 130528, 'counters/updates': 4079}
train stats after 130560 examples: {'rewards_train/chosen': '0.19511', 'rewards_train/rejected': '0.059836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13527', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-186.67', 'loss/train': '0.64191', 'examples_per_second': '31.333', 'grad_norm': '101', 'counters/examples': 130560, 'counters/updates': 4080}
train stats after 130592 examples: {'rewards_train/chosen': '0.027071', 'rewards_train/rejected': '-0.0013425', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.028413', 'logps_train/rejected': '-102.76', 'logps_train/chosen': '-110.43', 'loss/train': '0.68575', 'examples_per_second': '30.573', 'grad_norm': '87', 'counters/examples': 130592, 'counters/updates': 4081}
train stats after 130624 examples: {'rewards_train/chosen': '0.088768', 'rewards_train/rejected': '0.055668', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033101', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-151.64', 'loss/train': '0.68899', 'examples_per_second': '32.981', 'grad_norm': '122', 'counters/examples': 130624, 'counters/updates': 4082}
skipping logging after 130656 examples to avoid logging too frequently
train stats after 130688 examples: {'rewards_train/chosen': '0.07797', 'rewards_train/rejected': '0.033796', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044174', 'logps_train/rejected': '-148.32', 'logps_train/chosen': '-156.28', 'loss/train': '0.69563', 'examples_per_second': '32.603', 'grad_norm': '133', 'counters/examples': 130688, 'counters/updates': 4084}
train stats after 130720 examples: {'rewards_train/chosen': '0.09887', 'rewards_train/rejected': '0.1056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0067273', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-190.65', 'loss/train': '0.7275', 'examples_per_second': '31.547', 'grad_norm': '108.5', 'counters/examples': 130720, 'counters/updates': 4085}
train stats after 130752 examples: {'rewards_train/chosen': '0.18678', 'rewards_train/rejected': '0.063032', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12374', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-144.47', 'loss/train': '0.64368', 'examples_per_second': '31.111', 'grad_norm': '58', 'counters/examples': 130752, 'counters/updates': 4086}
train stats after 130784 examples: {'rewards_train/chosen': '0.15745', 'rewards_train/rejected': '0.14701', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010435', 'logps_train/rejected': '-162.64', 'logps_train/chosen': '-143.33', 'loss/train': '0.70124', 'examples_per_second': '31.667', 'grad_norm': '92', 'counters/examples': 130784, 'counters/updates': 4087}
train stats after 130816 examples: {'rewards_train/chosen': '0.039457', 'rewards_train/rejected': '0.082297', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04284', 'logps_train/rejected': '-131.44', 'logps_train/chosen': '-137.01', 'loss/train': '0.72959', 'examples_per_second': '30.909', 'grad_norm': '73', 'counters/examples': 130816, 'counters/updates': 4088}
skipping logging after 130848 examples to avoid logging too frequently
train stats after 130880 examples: {'rewards_train/chosen': '0.13285', 'rewards_train/rejected': '0.072863', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059985', 'logps_train/rejected': '-144.72', 'logps_train/chosen': '-152.41', 'loss/train': '0.68007', 'examples_per_second': '30.016', 'grad_norm': '175', 'counters/examples': 130880, 'counters/updates': 4090}
train stats after 130912 examples: {'rewards_train/chosen': '0.035055', 'rewards_train/rejected': '0.056011', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020957', 'logps_train/rejected': '-128.2', 'logps_train/chosen': '-173.95', 'loss/train': '0.72686', 'examples_per_second': '31.514', 'grad_norm': '61', 'counters/examples': 130912, 'counters/updates': 4091}
skipping logging after 130944 examples to avoid logging too frequently
train stats after 130976 examples: {'rewards_train/chosen': '-0.0013486', 'rewards_train/rejected': '-0.011599', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.010251', 'logps_train/rejected': '-107.77', 'logps_train/chosen': '-141.34', 'loss/train': '0.7008', 'examples_per_second': '30.557', 'grad_norm': '77.5', 'counters/examples': 130976, 'counters/updates': 4093}
train stats after 131008 examples: {'rewards_train/chosen': '0.055581', 'rewards_train/rejected': '0.04394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.011641', 'logps_train/rejected': '-147.48', 'logps_train/chosen': '-150.78', 'loss/train': '0.70027', 'examples_per_second': '30.038', 'grad_norm': '60.25', 'counters/examples': 131008, 'counters/updates': 4094}
train stats after 131040 examples: {'rewards_train/chosen': '-0.0033163', 'rewards_train/rejected': '0.16623', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.16955', 'logps_train/rejected': '-192.04', 'logps_train/chosen': '-215.64', 'loss/train': '0.83595', 'examples_per_second': '31.447', 'grad_norm': '129', 'counters/examples': 131040, 'counters/updates': 4095}
train stats after 131072 examples: {'rewards_train/chosen': '0.11793', 'rewards_train/rejected': '-0.023184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14112', 'logps_train/rejected': '-157.42', 'logps_train/chosen': '-142.54', 'loss/train': '0.64679', 'examples_per_second': '30.847', 'grad_norm': '65.5', 'counters/examples': 131072, 'counters/updates': 4096}
skipping logging after 131104 examples to avoid logging too frequently
train stats after 131136 examples: {'rewards_train/chosen': '0.12783', 'rewards_train/rejected': '0.068524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059307', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-165.55', 'loss/train': '0.69051', 'examples_per_second': '30.572', 'grad_norm': '77.5', 'counters/examples': 131136, 'counters/updates': 4098}
train stats after 131168 examples: {'rewards_train/chosen': '0.068969', 'rewards_train/rejected': '0.054752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014218', 'logps_train/rejected': '-150.16', 'logps_train/chosen': '-168.17', 'loss/train': '0.70181', 'examples_per_second': '31.477', 'grad_norm': '82', 'counters/examples': 131168, 'counters/updates': 4099}
train stats after 131200 examples: {'rewards_train/chosen': '0.067079', 'rewards_train/rejected': '-0.0021026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069182', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-175.93', 'loss/train': '0.66611', 'examples_per_second': '31.254', 'grad_norm': '64', 'counters/examples': 131200, 'counters/updates': 4100}
train stats after 131232 examples: {'rewards_train/chosen': '-0.0009479', 'rewards_train/rejected': '0.039464', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040412', 'logps_train/rejected': '-149.36', 'logps_train/chosen': '-154.94', 'loss/train': '0.72545', 'examples_per_second': '31.055', 'grad_norm': '256', 'counters/examples': 131232, 'counters/updates': 4101}
train stats after 131264 examples: {'rewards_train/chosen': '0.11114', 'rewards_train/rejected': '0.07982', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031317', 'logps_train/rejected': '-153.43', 'logps_train/chosen': '-179.95', 'loss/train': '0.69389', 'examples_per_second': '30.018', 'grad_norm': '127', 'counters/examples': 131264, 'counters/updates': 4102}
skipping logging after 131296 examples to avoid logging too frequently
train stats after 131328 examples: {'rewards_train/chosen': '0.14838', 'rewards_train/rejected': '0.059658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088718', 'logps_train/rejected': '-185.39', 'logps_train/chosen': '-150.36', 'loss/train': '0.66948', 'examples_per_second': '31.492', 'grad_norm': '71', 'counters/examples': 131328, 'counters/updates': 4104}
train stats after 131360 examples: {'rewards_train/chosen': '0.086331', 'rewards_train/rejected': '0.033956', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052375', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-122.92', 'loss/train': '0.67429', 'examples_per_second': '29.936', 'grad_norm': '102.5', 'counters/examples': 131360, 'counters/updates': 4105}
train stats after 131392 examples: {'rewards_train/chosen': '0.081935', 'rewards_train/rejected': '-0.058848', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14078', 'logps_train/rejected': '-150.8', 'logps_train/chosen': '-183.5', 'loss/train': '0.63771', 'examples_per_second': '30.556', 'grad_norm': '88.5', 'counters/examples': 131392, 'counters/updates': 4106}
train stats after 131424 examples: {'rewards_train/chosen': '0.1108', 'rewards_train/rejected': '-0.0094659', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12027', 'logps_train/rejected': '-116.93', 'logps_train/chosen': '-147.3', 'loss/train': '0.6509', 'examples_per_second': '31.565', 'grad_norm': '104', 'counters/examples': 131424, 'counters/updates': 4107}
skipping logging after 131456 examples to avoid logging too frequently
train stats after 131488 examples: {'rewards_train/chosen': '0.1854', 'rewards_train/rejected': '0.056205', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12919', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-193.95', 'loss/train': '0.65211', 'examples_per_second': '31.522', 'grad_norm': '84.5', 'counters/examples': 131488, 'counters/updates': 4109}
skipping logging after 131520 examples to avoid logging too frequently
train stats after 131552 examples: {'rewards_train/chosen': '0.014761', 'rewards_train/rejected': '0.015264', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00050348', 'logps_train/rejected': '-175.47', 'logps_train/chosen': '-164.95', 'loss/train': '0.70845', 'examples_per_second': '32.988', 'grad_norm': '190', 'counters/examples': 131552, 'counters/updates': 4111}
train stats after 131584 examples: {'rewards_train/chosen': '0.1156', 'rewards_train/rejected': '0.093188', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022413', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-132.13', 'loss/train': '0.69398', 'examples_per_second': '30.698', 'grad_norm': '113', 'counters/examples': 131584, 'counters/updates': 4112}
skipping logging after 131616 examples to avoid logging too frequently
train stats after 131648 examples: {'rewards_train/chosen': '0.076616', 'rewards_train/rejected': '0.078274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0016582', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-120.62', 'loss/train': '0.72402', 'examples_per_second': '31.581', 'grad_norm': '95', 'counters/examples': 131648, 'counters/updates': 4114}
train stats after 131680 examples: {'rewards_train/chosen': '0.05055', 'rewards_train/rejected': '0.056614', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0060644', 'logps_train/rejected': '-144.95', 'logps_train/chosen': '-142.26', 'loss/train': '0.70406', 'examples_per_second': '31.004', 'grad_norm': '116.5', 'counters/examples': 131680, 'counters/updates': 4115}
train stats after 131712 examples: {'rewards_train/chosen': '0.12278', 'rewards_train/rejected': '0.01739', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10539', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-155.66', 'loss/train': '0.65458', 'examples_per_second': '32.785', 'grad_norm': '172', 'counters/examples': 131712, 'counters/updates': 4116}
train stats after 131744 examples: {'rewards_train/chosen': '0.074348', 'rewards_train/rejected': '-0.012739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087087', 'logps_train/rejected': '-92.391', 'logps_train/chosen': '-135.33', 'loss/train': '0.66139', 'examples_per_second': '30.382', 'grad_norm': '58.25', 'counters/examples': 131744, 'counters/updates': 4117}
skipping logging after 131776 examples to avoid logging too frequently
train stats after 131808 examples: {'rewards_train/chosen': '0.045102', 'rewards_train/rejected': '0.093784', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.048682', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-147.8', 'loss/train': '0.72808', 'examples_per_second': '30.063', 'grad_norm': '94', 'counters/examples': 131808, 'counters/updates': 4119}
skipping logging after 131840 examples to avoid logging too frequently
train stats after 131872 examples: {'rewards_train/chosen': '0.16624', 'rewards_train/rejected': '0.046043', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12019', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-137.94', 'loss/train': '0.65638', 'examples_per_second': '38.285', 'grad_norm': '98', 'counters/examples': 131872, 'counters/updates': 4121}
train stats after 131904 examples: {'rewards_train/chosen': '0.050855', 'rewards_train/rejected': '0.046959', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0038964', 'logps_train/rejected': '-146.58', 'logps_train/chosen': '-155.51', 'loss/train': '0.70377', 'examples_per_second': '32.16', 'grad_norm': '212', 'counters/examples': 131904, 'counters/updates': 4122}
skipping logging after 131936 examples to avoid logging too frequently
train stats after 131968 examples: {'rewards_train/chosen': '0.092859', 'rewards_train/rejected': '0.047575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045284', 'logps_train/rejected': '-137.2', 'logps_train/chosen': '-146.62', 'loss/train': '0.68859', 'examples_per_second': '32.765', 'grad_norm': '75', 'counters/examples': 131968, 'counters/updates': 4124}
skipping logging after 132000 examples to avoid logging too frequently
Running evaluation after 132000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.92it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.89it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.90it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.89it/s]
eval after 132000: {'rewards_eval/chosen': '0.11537', 'rewards_eval/rejected': '0.065941', 'rewards_eval/accuracies': '0.53516', 'rewards_eval/margins': '0.049434', 'logps_eval/rejected': '-127.48', 'logps_eval/chosen': '-149.85', 'loss/eval': '0.68555'}
skipping save for non epoch
train stats after 132032 examples: {'rewards_train/chosen': '0.12965', 'rewards_train/rejected': '0.015577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11408', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-173', 'loss/train': '0.65631', 'examples_per_second': '30.762', 'grad_norm': '75.5', 'counters/examples': 132032, 'counters/updates': 4126}
train stats after 132064 examples: {'rewards_train/chosen': '0.091347', 'rewards_train/rejected': '0.15076', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.059409', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-137.76', 'loss/train': '0.73906', 'examples_per_second': '30.4', 'grad_norm': '95', 'counters/examples': 132064, 'counters/updates': 4127}
skipping logging after 132096 examples to avoid logging too frequently
train stats after 132128 examples: {'rewards_train/chosen': '0.082375', 'rewards_train/rejected': '-0.02627', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10865', 'logps_train/rejected': '-144.35', 'logps_train/chosen': '-101.18', 'loss/train': '0.64803', 'examples_per_second': '30.6', 'grad_norm': '52.75', 'counters/examples': 132128, 'counters/updates': 4129}
train stats after 132160 examples: {'rewards_train/chosen': '0.038786', 'rewards_train/rejected': '0.065646', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02686', 'logps_train/rejected': '-135.37', 'logps_train/chosen': '-141.68', 'loss/train': '0.71143', 'examples_per_second': '31.325', 'grad_norm': '110.5', 'counters/examples': 132160, 'counters/updates': 4130}
train stats after 132192 examples: {'rewards_train/chosen': '0.15636', 'rewards_train/rejected': '0.021236', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13512', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-182.65', 'loss/train': '0.6452', 'examples_per_second': '30.762', 'grad_norm': '70.5', 'counters/examples': 132192, 'counters/updates': 4131}
train stats after 132224 examples: {'rewards_train/chosen': '0.072535', 'rewards_train/rejected': '-0.018387', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090922', 'logps_train/rejected': '-132.14', 'logps_train/chosen': '-146.9', 'loss/train': '0.6611', 'examples_per_second': '31.471', 'grad_norm': '182', 'counters/examples': 132224, 'counters/updates': 4132}
train stats after 132256 examples: {'rewards_train/chosen': '0.038301', 'rewards_train/rejected': '0.077744', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.039443', 'logps_train/rejected': '-148.14', 'logps_train/chosen': '-159.88', 'loss/train': '0.72641', 'examples_per_second': '32.391', 'grad_norm': '76', 'counters/examples': 132256, 'counters/updates': 4133}
train stats after 132288 examples: {'rewards_train/chosen': '0.1652', 'rewards_train/rejected': '-0.078279', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.24348', 'logps_train/rejected': '-166', 'logps_train/chosen': '-177.69', 'loss/train': '0.63965', 'examples_per_second': '29.861', 'grad_norm': '108', 'counters/examples': 132288, 'counters/updates': 4134}
train stats after 132320 examples: {'rewards_train/chosen': '0.10154', 'rewards_train/rejected': '0.059306', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042234', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-152.85', 'loss/train': '0.68754', 'examples_per_second': '32.689', 'grad_norm': '64.5', 'counters/examples': 132320, 'counters/updates': 4135}
train stats after 132352 examples: {'rewards_train/chosen': '0.17752', 'rewards_train/rejected': '0.018472', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15905', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-183.22', 'loss/train': '0.63142', 'examples_per_second': '32.141', 'grad_norm': '111', 'counters/examples': 132352, 'counters/updates': 4136}
train stats after 132384 examples: {'rewards_train/chosen': '0.11325', 'rewards_train/rejected': '-0.024388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13764', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-130.63', 'loss/train': '0.63886', 'examples_per_second': '31.483', 'grad_norm': '105', 'counters/examples': 132384, 'counters/updates': 4137}
train stats after 132416 examples: {'rewards_train/chosen': '0.091412', 'rewards_train/rejected': '0.074728', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016685', 'logps_train/rejected': '-142.75', 'logps_train/chosen': '-201.89', 'loss/train': '0.70327', 'examples_per_second': '31.533', 'grad_norm': '114', 'counters/examples': 132416, 'counters/updates': 4138}
train stats after 132448 examples: {'rewards_train/chosen': '0.032359', 'rewards_train/rejected': '0.039368', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0070084', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-177.44', 'loss/train': '0.70858', 'examples_per_second': '30.275', 'grad_norm': '137', 'counters/examples': 132448, 'counters/updates': 4139}
train stats after 132480 examples: {'rewards_train/chosen': '0.10379', 'rewards_train/rejected': '0.08207', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021715', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-159.04', 'loss/train': '0.70803', 'examples_per_second': '31.509', 'grad_norm': '74', 'counters/examples': 132480, 'counters/updates': 4140}
train stats after 132512 examples: {'rewards_train/chosen': '0.059611', 'rewards_train/rejected': '0.068069', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0084582', 'logps_train/rejected': '-152.67', 'logps_train/chosen': '-154.64', 'loss/train': '0.70252', 'examples_per_second': '23.798', 'grad_norm': '124.5', 'counters/examples': 132512, 'counters/updates': 4141}
train stats after 132544 examples: {'rewards_train/chosen': '0.048822', 'rewards_train/rejected': '0.038759', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010063', 'logps_train/rejected': '-171.26', 'logps_train/chosen': '-172.34', 'loss/train': '0.70938', 'examples_per_second': '31.229', 'grad_norm': '146', 'counters/examples': 132544, 'counters/updates': 4142}
train stats after 132576 examples: {'rewards_train/chosen': '0.069063', 'rewards_train/rejected': '0.054134', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014929', 'logps_train/rejected': '-120.5', 'logps_train/chosen': '-122.94', 'loss/train': '0.70277', 'examples_per_second': '30.451', 'grad_norm': '170', 'counters/examples': 132576, 'counters/updates': 4143}
train stats after 132608 examples: {'rewards_train/chosen': '0.13143', 'rewards_train/rejected': '0.045769', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085659', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-137.25', 'loss/train': '0.67014', 'examples_per_second': '30.741', 'grad_norm': '94.5', 'counters/examples': 132608, 'counters/updates': 4144}
skipping logging after 132640 examples to avoid logging too frequently
train stats after 132672 examples: {'rewards_train/chosen': '0.07573', 'rewards_train/rejected': '0.070432', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0052983', 'logps_train/rejected': '-121.67', 'logps_train/chosen': '-166.01', 'loss/train': '0.70569', 'examples_per_second': '31.523', 'grad_norm': '174', 'counters/examples': 132672, 'counters/updates': 4146}
train stats after 132704 examples: {'rewards_train/chosen': '0.12708', 'rewards_train/rejected': '0.05184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075238', 'logps_train/rejected': '-146.11', 'logps_train/chosen': '-137.38', 'loss/train': '0.66831', 'examples_per_second': '30.865', 'grad_norm': '58.5', 'counters/examples': 132704, 'counters/updates': 4147}
train stats after 132736 examples: {'rewards_train/chosen': '0.12871', 'rewards_train/rejected': '0.0082022', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1205', 'logps_train/rejected': '-91.866', 'logps_train/chosen': '-149.91', 'loss/train': '0.64769', 'examples_per_second': '29.93', 'grad_norm': '62.5', 'counters/examples': 132736, 'counters/updates': 4148}
train stats after 132768 examples: {'rewards_train/chosen': '0.071303', 'rewards_train/rejected': '0.01734', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053963', 'logps_train/rejected': '-159.96', 'logps_train/chosen': '-160.65', 'loss/train': '0.68175', 'examples_per_second': '29.943', 'grad_norm': '102.5', 'counters/examples': 132768, 'counters/updates': 4149}
train stats after 132800 examples: {'rewards_train/chosen': '0.14235', 'rewards_train/rejected': '-0.027784', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17013', 'logps_train/rejected': '-118.18', 'logps_train/chosen': '-148.87', 'loss/train': '0.62338', 'examples_per_second': '31.519', 'grad_norm': '66', 'counters/examples': 132800, 'counters/updates': 4150}
train stats after 132832 examples: {'rewards_train/chosen': '0.091615', 'rewards_train/rejected': '0.037384', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05423', 'logps_train/rejected': '-170.35', 'logps_train/chosen': '-205.6', 'loss/train': '0.68358', 'examples_per_second': '31.84', 'grad_norm': '78', 'counters/examples': 132832, 'counters/updates': 4151}
train stats after 132864 examples: {'rewards_train/chosen': '0.11813', 'rewards_train/rejected': '-0.0045265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12265', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-158.91', 'loss/train': '0.65543', 'examples_per_second': '31.811', 'grad_norm': '78', 'counters/examples': 132864, 'counters/updates': 4152}
train stats after 132896 examples: {'rewards_train/chosen': '0.041833', 'rewards_train/rejected': '0.10072', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.058892', 'logps_train/rejected': '-134.04', 'logps_train/chosen': '-160.48', 'loss/train': '0.73345', 'examples_per_second': '31.317', 'grad_norm': '62.25', 'counters/examples': 132896, 'counters/updates': 4153}
train stats after 132928 examples: {'rewards_train/chosen': '0.14471', 'rewards_train/rejected': '0.035979', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10874', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-146.41', 'loss/train': '0.66607', 'examples_per_second': '29.974', 'grad_norm': '74', 'counters/examples': 132928, 'counters/updates': 4154}
train stats after 132960 examples: {'rewards_train/chosen': '0.17501', 'rewards_train/rejected': '0.013224', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16179', 'logps_train/rejected': '-138.76', 'logps_train/chosen': '-157.69', 'loss/train': '0.63587', 'examples_per_second': '30.034', 'grad_norm': '77', 'counters/examples': 132960, 'counters/updates': 4155}
train stats after 132992 examples: {'rewards_train/chosen': '0.075664', 'rewards_train/rejected': '0.072399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.003265', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-178.32', 'loss/train': '0.70902', 'examples_per_second': '31.99', 'grad_norm': '62.75', 'counters/examples': 132992, 'counters/updates': 4156}
train stats after 133024 examples: {'rewards_train/chosen': '0.11558', 'rewards_train/rejected': '0.094518', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021059', 'logps_train/rejected': '-150.26', 'logps_train/chosen': '-163.28', 'loss/train': '0.68958', 'examples_per_second': '31.067', 'grad_norm': '75.5', 'counters/examples': 133024, 'counters/updates': 4157}
train stats after 133056 examples: {'rewards_train/chosen': '0.12415', 'rewards_train/rejected': '0.021093', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10306', 'logps_train/rejected': '-120.45', 'logps_train/chosen': '-166.57', 'loss/train': '0.65573', 'examples_per_second': '31.706', 'grad_norm': '68', 'counters/examples': 133056, 'counters/updates': 4158}
skipping logging after 133088 examples to avoid logging too frequently
train stats after 133120 examples: {'rewards_train/chosen': '0.092121', 'rewards_train/rejected': '0.058198', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033923', 'logps_train/rejected': '-169.14', 'logps_train/chosen': '-162.89', 'loss/train': '0.69233', 'examples_per_second': '31.69', 'grad_norm': '239', 'counters/examples': 133120, 'counters/updates': 4160}
train stats after 133152 examples: {'rewards_train/chosen': '0.04036', 'rewards_train/rejected': '0.0071695', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03319', 'logps_train/rejected': '-109.26', 'logps_train/chosen': '-112.05', 'loss/train': '0.68218', 'examples_per_second': '31.534', 'grad_norm': '64', 'counters/examples': 133152, 'counters/updates': 4161}
train stats after 133184 examples: {'rewards_train/chosen': '0.080773', 'rewards_train/rejected': '0.0037445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077029', 'logps_train/rejected': '-133.4', 'logps_train/chosen': '-127.78', 'loss/train': '0.67112', 'examples_per_second': '31.541', 'grad_norm': '98.5', 'counters/examples': 133184, 'counters/updates': 4162}
skipping logging after 133216 examples to avoid logging too frequently
train stats after 133248 examples: {'rewards_train/chosen': '0.07186', 'rewards_train/rejected': '0.069251', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0026087', 'logps_train/rejected': '-122.48', 'logps_train/chosen': '-167.53', 'loss/train': '0.70624', 'examples_per_second': '31.558', 'grad_norm': '100.5', 'counters/examples': 133248, 'counters/updates': 4164}
skipping logging after 133280 examples to avoid logging too frequently
train stats after 133312 examples: {'rewards_train/chosen': '0.075121', 'rewards_train/rejected': '0.096859', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021737', 'logps_train/rejected': '-138.87', 'logps_train/chosen': '-157.8', 'loss/train': '0.71641', 'examples_per_second': '31.314', 'grad_norm': '85.5', 'counters/examples': 133312, 'counters/updates': 4166}
train stats after 133344 examples: {'rewards_train/chosen': '0.11957', 'rewards_train/rejected': '0.05968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059886', 'logps_train/rejected': '-157.36', 'logps_train/chosen': '-203.4', 'loss/train': '0.67218', 'examples_per_second': '30.05', 'grad_norm': '74', 'counters/examples': 133344, 'counters/updates': 4167}
train stats after 133376 examples: {'rewards_train/chosen': '0.055375', 'rewards_train/rejected': '0.045669', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0097057', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-133.69', 'loss/train': '0.69812', 'examples_per_second': '31.615', 'grad_norm': '78.5', 'counters/examples': 133376, 'counters/updates': 4168}
train stats after 133408 examples: {'rewards_train/chosen': '0.038841', 'rewards_train/rejected': '0.069378', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030537', 'logps_train/rejected': '-127.95', 'logps_train/chosen': '-167.83', 'loss/train': '0.71994', 'examples_per_second': '31.426', 'grad_norm': '66.5', 'counters/examples': 133408, 'counters/updates': 4169}
skipping logging after 133440 examples to avoid logging too frequently
train stats after 133472 examples: {'rewards_train/chosen': '0.13798', 'rewards_train/rejected': '0.012472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12551', 'logps_train/rejected': '-94.157', 'logps_train/chosen': '-119.53', 'loss/train': '0.64489', 'examples_per_second': '31.479', 'grad_norm': '61.5', 'counters/examples': 133472, 'counters/updates': 4171}
train stats after 133504 examples: {'rewards_train/chosen': '0.067177', 'rewards_train/rejected': '-0.0020408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069217', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-140.38', 'loss/train': '0.66887', 'examples_per_second': '30.081', 'grad_norm': '78', 'counters/examples': 133504, 'counters/updates': 4172}
train stats after 133536 examples: {'rewards_train/chosen': '0.060943', 'rewards_train/rejected': '0.11615', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.055211', 'logps_train/rejected': '-152.57', 'logps_train/chosen': '-147.42', 'loss/train': '0.74712', 'examples_per_second': '31.393', 'grad_norm': '90.5', 'counters/examples': 133536, 'counters/updates': 4173}
train stats after 133568 examples: {'rewards_train/chosen': '0.084167', 'rewards_train/rejected': '0.052851', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031316', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-149.65', 'loss/train': '0.69352', 'examples_per_second': '31.98', 'grad_norm': '51.5', 'counters/examples': 133568, 'counters/updates': 4174}
train stats after 133600 examples: {'rewards_train/chosen': '0.18292', 'rewards_train/rejected': '0.042764', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14015', 'logps_train/rejected': '-155.39', 'logps_train/chosen': '-196.74', 'loss/train': '0.64573', 'examples_per_second': '30.691', 'grad_norm': '106', 'counters/examples': 133600, 'counters/updates': 4175}
train stats after 133632 examples: {'rewards_train/chosen': '0.081513', 'rewards_train/rejected': '0.022981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058532', 'logps_train/rejected': '-125', 'logps_train/chosen': '-149.76', 'loss/train': '0.67507', 'examples_per_second': '31.55', 'grad_norm': '82', 'counters/examples': 133632, 'counters/updates': 4176}
train stats after 133664 examples: {'rewards_train/chosen': '0.073126', 'rewards_train/rejected': '0.005293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067833', 'logps_train/rejected': '-98.614', 'logps_train/chosen': '-131.66', 'loss/train': '0.66661', 'examples_per_second': '30.542', 'grad_norm': '102.5', 'counters/examples': 133664, 'counters/updates': 4177}
train stats after 133696 examples: {'rewards_train/chosen': '0.14562', 'rewards_train/rejected': '0.016617', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.12901', 'logps_train/rejected': '-147.48', 'logps_train/chosen': '-191.01', 'loss/train': '0.65346', 'examples_per_second': '33.223', 'grad_norm': '130', 'counters/examples': 133696, 'counters/updates': 4178}
train stats after 133728 examples: {'rewards_train/chosen': '0.048454', 'rewards_train/rejected': '0.14195', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.093497', 'logps_train/rejected': '-125.56', 'logps_train/chosen': '-127.41', 'loss/train': '0.77414', 'examples_per_second': '31.536', 'grad_norm': '241', 'counters/examples': 133728, 'counters/updates': 4179}
train stats after 133760 examples: {'rewards_train/chosen': '-0.035656', 'rewards_train/rejected': '-0.038604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0029478', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-178.22', 'loss/train': '0.71204', 'examples_per_second': '32.439', 'grad_norm': '159', 'counters/examples': 133760, 'counters/updates': 4180}
train stats after 133792 examples: {'rewards_train/chosen': '0.04204', 'rewards_train/rejected': '-0.003764', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.045804', 'logps_train/rejected': '-118.78', 'logps_train/chosen': '-144.67', 'loss/train': '0.67666', 'examples_per_second': '32.239', 'grad_norm': '75', 'counters/examples': 133792, 'counters/updates': 4181}
train stats after 133824 examples: {'rewards_train/chosen': '0.071388', 'rewards_train/rejected': '0.0098048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061583', 'logps_train/rejected': '-134.67', 'logps_train/chosen': '-141.68', 'loss/train': '0.67627', 'examples_per_second': '32.084', 'grad_norm': '123', 'counters/examples': 133824, 'counters/updates': 4182}
train stats after 133856 examples: {'rewards_train/chosen': '0.209', 'rewards_train/rejected': '0.02377', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18522', 'logps_train/rejected': '-153.59', 'logps_train/chosen': '-143.3', 'loss/train': '0.62354', 'examples_per_second': '31.647', 'grad_norm': '73.5', 'counters/examples': 133856, 'counters/updates': 4183}
train stats after 133888 examples: {'rewards_train/chosen': '0.047363', 'rewards_train/rejected': '0.046192', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0011706', 'logps_train/rejected': '-111.32', 'logps_train/chosen': '-160.91', 'loss/train': '0.7009', 'examples_per_second': '31.662', 'grad_norm': '93.5', 'counters/examples': 133888, 'counters/updates': 4184}
train stats after 133920 examples: {'rewards_train/chosen': '0.0093878', 'rewards_train/rejected': '0.056394', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.047006', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-149.43', 'loss/train': '0.72534', 'examples_per_second': '32.643', 'grad_norm': '99.5', 'counters/examples': 133920, 'counters/updates': 4185}
train stats after 133952 examples: {'rewards_train/chosen': '0.1105', 'rewards_train/rejected': '0.058066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052434', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-153.07', 'loss/train': '0.68039', 'examples_per_second': '32.384', 'grad_norm': '102', 'counters/examples': 133952, 'counters/updates': 4186}
train stats after 133984 examples: {'rewards_train/chosen': '0.081653', 'rewards_train/rejected': '0.059544', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022109', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-113.14', 'loss/train': '0.68743', 'examples_per_second': '32.833', 'grad_norm': '50.25', 'counters/examples': 133984, 'counters/updates': 4187}
train stats after 134016 examples: {'rewards_train/chosen': '0.10971', 'rewards_train/rejected': '0.070862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.038853', 'logps_train/rejected': '-172.67', 'logps_train/chosen': '-160.49', 'loss/train': '0.69217', 'examples_per_second': '30.022', 'grad_norm': '80', 'counters/examples': 134016, 'counters/updates': 4188}
train stats after 134048 examples: {'rewards_train/chosen': '0.1391', 'rewards_train/rejected': '0.087882', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051214', 'logps_train/rejected': '-116.12', 'logps_train/chosen': '-148.96', 'loss/train': '0.69129', 'examples_per_second': '31.542', 'grad_norm': '70.5', 'counters/examples': 134048, 'counters/updates': 4189}
train stats after 134080 examples: {'rewards_train/chosen': '0.096285', 'rewards_train/rejected': '0.09047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0058155', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-158.74', 'loss/train': '0.70032', 'examples_per_second': '31.39', 'grad_norm': '91.5', 'counters/examples': 134080, 'counters/updates': 4190}
train stats after 134112 examples: {'rewards_train/chosen': '0.079988', 'rewards_train/rejected': '0.07039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0095978', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-157.95', 'loss/train': '0.69982', 'examples_per_second': '31.763', 'grad_norm': '54.75', 'counters/examples': 134112, 'counters/updates': 4191}
train stats after 134144 examples: {'rewards_train/chosen': '0.095795', 'rewards_train/rejected': '0.017269', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078526', 'logps_train/rejected': '-119.29', 'logps_train/chosen': '-161.84', 'loss/train': '0.67163', 'examples_per_second': '31.563', 'grad_norm': '58.5', 'counters/examples': 134144, 'counters/updates': 4192}
train stats after 134176 examples: {'rewards_train/chosen': '0.10851', 'rewards_train/rejected': '0.041326', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067182', 'logps_train/rejected': '-111.97', 'logps_train/chosen': '-115.51', 'loss/train': '0.67244', 'examples_per_second': '30.468', 'grad_norm': '117.5', 'counters/examples': 134176, 'counters/updates': 4193}
train stats after 134208 examples: {'rewards_train/chosen': '0.098963', 'rewards_train/rejected': '0.045687', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053277', 'logps_train/rejected': '-144.99', 'logps_train/chosen': '-191.77', 'loss/train': '0.6802', 'examples_per_second': '30.78', 'grad_norm': '88.5', 'counters/examples': 134208, 'counters/updates': 4194}
train stats after 134240 examples: {'rewards_train/chosen': '0.19827', 'rewards_train/rejected': '0.095843', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10243', 'logps_train/rejected': '-131.46', 'logps_train/chosen': '-158.06', 'loss/train': '0.66302', 'examples_per_second': '31.527', 'grad_norm': '71.5', 'counters/examples': 134240, 'counters/updates': 4195}
train stats after 134272 examples: {'rewards_train/chosen': '0.14154', 'rewards_train/rejected': '0.13115', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010393', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-166.68', 'loss/train': '0.70372', 'examples_per_second': '31.487', 'grad_norm': '59.75', 'counters/examples': 134272, 'counters/updates': 4196}
train stats after 134304 examples: {'rewards_train/chosen': '0.10249', 'rewards_train/rejected': '0.092466', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010022', 'logps_train/rejected': '-144.47', 'logps_train/chosen': '-135', 'loss/train': '0.69665', 'examples_per_second': '31.577', 'grad_norm': '101.5', 'counters/examples': 134304, 'counters/updates': 4197}
train stats after 134336 examples: {'rewards_train/chosen': '0.054879', 'rewards_train/rejected': '0.010418', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044461', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-152.64', 'loss/train': '0.68158', 'examples_per_second': '31.27', 'grad_norm': '70', 'counters/examples': 134336, 'counters/updates': 4198}
train stats after 134368 examples: {'rewards_train/chosen': '0.078639', 'rewards_train/rejected': '3.0081e-05', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078609', 'logps_train/rejected': '-167.51', 'logps_train/chosen': '-140.29', 'loss/train': '0.66704', 'examples_per_second': '30.715', 'grad_norm': '89', 'counters/examples': 134368, 'counters/updates': 4199}
train stats after 134400 examples: {'rewards_train/chosen': '0.073993', 'rewards_train/rejected': '-0.010343', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084336', 'logps_train/rejected': '-96.262', 'logps_train/chosen': '-161.62', 'loss/train': '0.66165', 'examples_per_second': '32.716', 'grad_norm': '57.25', 'counters/examples': 134400, 'counters/updates': 4200}
train stats after 134432 examples: {'rewards_train/chosen': '0.077256', 'rewards_train/rejected': '0.024146', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05311', 'logps_train/rejected': '-107.03', 'logps_train/chosen': '-129.21', 'loss/train': '0.68262', 'examples_per_second': '31.439', 'grad_norm': '72', 'counters/examples': 134432, 'counters/updates': 4201}
train stats after 134464 examples: {'rewards_train/chosen': '0.062224', 'rewards_train/rejected': '0.026881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035342', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-154.97', 'loss/train': '0.69', 'examples_per_second': '30.844', 'grad_norm': '77', 'counters/examples': 134464, 'counters/updates': 4202}
train stats after 134496 examples: {'rewards_train/chosen': '0.1069', 'rewards_train/rejected': '0.069658', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037244', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-160.71', 'loss/train': '0.69037', 'examples_per_second': '30.746', 'grad_norm': '115', 'counters/examples': 134496, 'counters/updates': 4203}
train stats after 134528 examples: {'rewards_train/chosen': '0.0048612', 'rewards_train/rejected': '0.1254', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12053', 'logps_train/rejected': '-148.14', 'logps_train/chosen': '-154.14', 'loss/train': '0.77116', 'examples_per_second': '30.85', 'grad_norm': '140', 'counters/examples': 134528, 'counters/updates': 4204}
train stats after 134560 examples: {'rewards_train/chosen': '0.041667', 'rewards_train/rejected': '-0.032552', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074219', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-122.1', 'loss/train': '0.66997', 'examples_per_second': '31.491', 'grad_norm': '58', 'counters/examples': 134560, 'counters/updates': 4205}
train stats after 134592 examples: {'rewards_train/chosen': '0.062003', 'rewards_train/rejected': '0.014976', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047027', 'logps_train/rejected': '-151.71', 'logps_train/chosen': '-151.81', 'loss/train': '0.6746', 'examples_per_second': '31.58', 'grad_norm': '74', 'counters/examples': 134592, 'counters/updates': 4206}
train stats after 134624 examples: {'rewards_train/chosen': '0.09898', 'rewards_train/rejected': '0.091856', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0071236', 'logps_train/rejected': '-152.01', 'logps_train/chosen': '-175.14', 'loss/train': '0.70517', 'examples_per_second': '31.521', 'grad_norm': '156', 'counters/examples': 134624, 'counters/updates': 4207}
train stats after 134656 examples: {'rewards_train/chosen': '0.13763', 'rewards_train/rejected': '0.15518', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01755', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-167.23', 'loss/train': '0.72378', 'examples_per_second': '32.122', 'grad_norm': '163', 'counters/examples': 134656, 'counters/updates': 4208}
train stats after 134688 examples: {'rewards_train/chosen': '0.036177', 'rewards_train/rejected': '-0.0032695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039446', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-162.68', 'loss/train': '0.68521', 'examples_per_second': '31.662', 'grad_norm': '80.5', 'counters/examples': 134688, 'counters/updates': 4209}
train stats after 134720 examples: {'rewards_train/chosen': '0.083523', 'rewards_train/rejected': '0.064227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019296', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-157.24', 'loss/train': '0.69169', 'examples_per_second': '30.053', 'grad_norm': '67.5', 'counters/examples': 134720, 'counters/updates': 4210}
train stats after 134752 examples: {'rewards_train/chosen': '0.12742', 'rewards_train/rejected': '0.068921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058499', 'logps_train/rejected': '-132.02', 'logps_train/chosen': '-170.37', 'loss/train': '0.67596', 'examples_per_second': '31.572', 'grad_norm': '79', 'counters/examples': 134752, 'counters/updates': 4211}
skipping logging after 134784 examples to avoid logging too frequently
train stats after 134816 examples: {'rewards_train/chosen': '0.071249', 'rewards_train/rejected': '0.06607', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0051792', 'logps_train/rejected': '-153.47', 'logps_train/chosen': '-149.68', 'loss/train': '0.70681', 'examples_per_second': '30.944', 'grad_norm': '89', 'counters/examples': 134816, 'counters/updates': 4213}
train stats after 134848 examples: {'rewards_train/chosen': '0.18546', 'rewards_train/rejected': '0.1194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066059', 'logps_train/rejected': '-151.56', 'logps_train/chosen': '-165.92', 'loss/train': '0.68337', 'examples_per_second': '30.92', 'grad_norm': '79.5', 'counters/examples': 134848, 'counters/updates': 4214}
train stats after 134880 examples: {'rewards_train/chosen': '0.067305', 'rewards_train/rejected': '0.096449', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029144', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-143.69', 'loss/train': '0.72592', 'examples_per_second': '30.422', 'grad_norm': '87', 'counters/examples': 134880, 'counters/updates': 4215}
skipping logging after 134912 examples to avoid logging too frequently
train stats after 134944 examples: {'rewards_train/chosen': '0.068746', 'rewards_train/rejected': '0.0020972', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066648', 'logps_train/rejected': '-157.13', 'logps_train/chosen': '-165.22', 'loss/train': '0.67724', 'examples_per_second': '34.483', 'grad_norm': '72', 'counters/examples': 134944, 'counters/updates': 4217}
train stats after 134976 examples: {'rewards_train/chosen': '0.10008', 'rewards_train/rejected': '0.077122', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022959', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-118.12', 'loss/train': '0.69289', 'examples_per_second': '31.3', 'grad_norm': '126', 'counters/examples': 134976, 'counters/updates': 4218}
train stats after 135008 examples: {'rewards_train/chosen': '0.065179', 'rewards_train/rejected': '-0.0051307', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.07031', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-140.99', 'loss/train': '0.67078', 'examples_per_second': '30.965', 'grad_norm': '65', 'counters/examples': 135008, 'counters/updates': 4219}
train stats after 135040 examples: {'rewards_train/chosen': '0.055131', 'rewards_train/rejected': '0.0093355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045795', 'logps_train/rejected': '-132.8', 'logps_train/chosen': '-143.95', 'loss/train': '0.67834', 'examples_per_second': '29.895', 'grad_norm': '83', 'counters/examples': 135040, 'counters/updates': 4220}
skipping logging after 135072 examples to avoid logging too frequently
train stats after 135104 examples: {'rewards_train/chosen': '0.056408', 'rewards_train/rejected': '0.098552', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.042144', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-175.27', 'loss/train': '0.73176', 'examples_per_second': '32.378', 'grad_norm': '103.5', 'counters/examples': 135104, 'counters/updates': 4222}
train stats after 135136 examples: {'rewards_train/chosen': '0.080665', 'rewards_train/rejected': '0.078666', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0019991', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-143.52', 'loss/train': '0.69753', 'examples_per_second': '30.113', 'grad_norm': '87.5', 'counters/examples': 135136, 'counters/updates': 4223}
skipping logging after 135168 examples to avoid logging too frequently
train stats after 135200 examples: {'rewards_train/chosen': '0.10358', 'rewards_train/rejected': '0.056745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046831', 'logps_train/rejected': '-171.76', 'logps_train/chosen': '-142.65', 'loss/train': '0.68281', 'examples_per_second': '31.465', 'grad_norm': '103', 'counters/examples': 135200, 'counters/updates': 4225}
skipping logging after 135232 examples to avoid logging too frequently
train stats after 135264 examples: {'rewards_train/chosen': '0.061655', 'rewards_train/rejected': '0.058998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0026565', 'logps_train/rejected': '-116.85', 'logps_train/chosen': '-170.77', 'loss/train': '0.70491', 'examples_per_second': '31.507', 'grad_norm': '127.5', 'counters/examples': 135264, 'counters/updates': 4227}
train stats after 135296 examples: {'rewards_train/chosen': '0.13339', 'rewards_train/rejected': '0.058056', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075336', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-148.81', 'loss/train': '0.68466', 'examples_per_second': '32.022', 'grad_norm': '95', 'counters/examples': 135296, 'counters/updates': 4228}
train stats after 135328 examples: {'rewards_train/chosen': '0.11231', 'rewards_train/rejected': '0.051993', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060313', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-155.45', 'loss/train': '0.67219', 'examples_per_second': '31.722', 'grad_norm': '112', 'counters/examples': 135328, 'counters/updates': 4229}
train stats after 135360 examples: {'rewards_train/chosen': '0.094391', 'rewards_train/rejected': '-0.030407', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1248', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-168.44', 'loss/train': '0.64205', 'examples_per_second': '32.598', 'grad_norm': '113.5', 'counters/examples': 135360, 'counters/updates': 4230}
skipping logging after 135392 examples to avoid logging too frequently
train stats after 135424 examples: {'rewards_train/chosen': '0.11738', 'rewards_train/rejected': '0.078099', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039279', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-163.18', 'loss/train': '0.7291', 'examples_per_second': '30.468', 'grad_norm': '109', 'counters/examples': 135424, 'counters/updates': 4232}
train stats after 135456 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '0.033381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068878', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-121.15', 'loss/train': '0.668', 'examples_per_second': '32.579', 'grad_norm': '72', 'counters/examples': 135456, 'counters/updates': 4233}
train stats after 135488 examples: {'rewards_train/chosen': '0.0090909', 'rewards_train/rejected': '-0.031846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040937', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-124.33', 'loss/train': '0.67803', 'examples_per_second': '31.026', 'grad_norm': '70.5', 'counters/examples': 135488, 'counters/updates': 4234}
train stats after 135520 examples: {'rewards_train/chosen': '0.068629', 'rewards_train/rejected': '0.075986', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.007357', 'logps_train/rejected': '-109.22', 'logps_train/chosen': '-156.88', 'loss/train': '0.70381', 'examples_per_second': '23.755', 'grad_norm': '91.5', 'counters/examples': 135520, 'counters/updates': 4235}
train stats after 135552 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.16924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.042281', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-148.73', 'loss/train': '0.76347', 'examples_per_second': '31.543', 'grad_norm': '230', 'counters/examples': 135552, 'counters/updates': 4236}
train stats after 135584 examples: {'rewards_train/chosen': '0.064976', 'rewards_train/rejected': '-0.013872', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078847', 'logps_train/rejected': '-106.4', 'logps_train/chosen': '-132.98', 'loss/train': '0.67079', 'examples_per_second': '31.339', 'grad_norm': '108.5', 'counters/examples': 135584, 'counters/updates': 4237}
train stats after 135616 examples: {'rewards_train/chosen': '0.077947', 'rewards_train/rejected': '0.014205', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063742', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-142.53', 'loss/train': '0.67717', 'examples_per_second': '24.662', 'grad_norm': '109.5', 'counters/examples': 135616, 'counters/updates': 4238}
train stats after 135648 examples: {'rewards_train/chosen': '0.10744', 'rewards_train/rejected': '0.060998', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046439', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-158.36', 'loss/train': '0.69199', 'examples_per_second': '32.721', 'grad_norm': '76', 'counters/examples': 135648, 'counters/updates': 4239}
train stats after 135680 examples: {'rewards_train/chosen': '0.11445', 'rewards_train/rejected': '0.14504', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.030585', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-168.32', 'loss/train': '0.72385', 'examples_per_second': '31.555', 'grad_norm': '72.5', 'counters/examples': 135680, 'counters/updates': 4240}
train stats after 135712 examples: {'rewards_train/chosen': '0.092974', 'rewards_train/rejected': '0.093882', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00090772', 'logps_train/rejected': '-165.1', 'logps_train/chosen': '-184.08', 'loss/train': '0.70661', 'examples_per_second': '30.738', 'grad_norm': '191', 'counters/examples': 135712, 'counters/updates': 4241}
train stats after 135744 examples: {'rewards_train/chosen': '0.016387', 'rewards_train/rejected': '0.033288', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0169', 'logps_train/rejected': '-138.9', 'logps_train/chosen': '-137.32', 'loss/train': '0.71395', 'examples_per_second': '32.074', 'grad_norm': '65', 'counters/examples': 135744, 'counters/updates': 4242}
train stats after 135776 examples: {'rewards_train/chosen': '0.15356', 'rewards_train/rejected': '0.02704', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12652', 'logps_train/rejected': '-138.03', 'logps_train/chosen': '-164.34', 'loss/train': '0.64621', 'examples_per_second': '31.288', 'grad_norm': '66', 'counters/examples': 135776, 'counters/updates': 4243}
train stats after 135808 examples: {'rewards_train/chosen': '0.079382', 'rewards_train/rejected': '0.20512', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12574', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-164.55', 'loss/train': '0.81364', 'examples_per_second': '31.565', 'grad_norm': '122', 'counters/examples': 135808, 'counters/updates': 4244}
train stats after 135840 examples: {'rewards_train/chosen': '0.12232', 'rewards_train/rejected': '0.013017', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1093', 'logps_train/rejected': '-153.5', 'logps_train/chosen': '-153.56', 'loss/train': '0.65863', 'examples_per_second': '30.552', 'grad_norm': '105.5', 'counters/examples': 135840, 'counters/updates': 4245}
train stats after 135872 examples: {'rewards_train/chosen': '0.11016', 'rewards_train/rejected': '0.15679', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046632', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-127.56', 'loss/train': '0.73622', 'examples_per_second': '32.63', 'grad_norm': '175', 'counters/examples': 135872, 'counters/updates': 4246}
train stats after 135904 examples: {'rewards_train/chosen': '0.17163', 'rewards_train/rejected': '-0.021232', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19286', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-139.72', 'loss/train': '0.62947', 'examples_per_second': '32.322', 'grad_norm': '87.5', 'counters/examples': 135904, 'counters/updates': 4247}
train stats after 135936 examples: {'rewards_train/chosen': '0.098848', 'rewards_train/rejected': '0.033834', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065014', 'logps_train/rejected': '-111.75', 'logps_train/chosen': '-189.79', 'loss/train': '0.67074', 'examples_per_second': '30.737', 'grad_norm': '62.5', 'counters/examples': 135936, 'counters/updates': 4248}
train stats after 135968 examples: {'rewards_train/chosen': '0.10405', 'rewards_train/rejected': '0.038694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065354', 'logps_train/rejected': '-140.62', 'logps_train/chosen': '-144.1', 'loss/train': '0.68077', 'examples_per_second': '32.987', 'grad_norm': '116.5', 'counters/examples': 135968, 'counters/updates': 4249}
train stats after 136000 examples: {'rewards_train/chosen': '0.025973', 'rewards_train/rejected': '0.089239', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.063267', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-133.42', 'loss/train': '0.73351', 'examples_per_second': '30.779', 'grad_norm': '79', 'counters/examples': 136000, 'counters/updates': 4250}
skipping logging after 136032 examples to avoid logging too frequently
train stats after 136064 examples: {'rewards_train/chosen': '0.021638', 'rewards_train/rejected': '-0.044171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065808', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-141.59', 'loss/train': '0.67584', 'examples_per_second': '30.44', 'grad_norm': '84.5', 'counters/examples': 136064, 'counters/updates': 4252}
train stats after 136096 examples: {'rewards_train/chosen': '0.19808', 'rewards_train/rejected': '0.047325', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15075', 'logps_train/rejected': '-158.63', 'logps_train/chosen': '-192.39', 'loss/train': '0.6458', 'examples_per_second': '31.859', 'grad_norm': '91.5', 'counters/examples': 136096, 'counters/updates': 4253}
skipping logging after 136128 examples to avoid logging too frequently
train stats after 136160 examples: {'rewards_train/chosen': '0.12694', 'rewards_train/rejected': '0.015958', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11098', 'logps_train/rejected': '-100.85', 'logps_train/chosen': '-163.34', 'loss/train': '0.65542', 'examples_per_second': '33.375', 'grad_norm': '119', 'counters/examples': 136160, 'counters/updates': 4255}
train stats after 136192 examples: {'rewards_train/chosen': '0.030447', 'rewards_train/rejected': '0.089019', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.058571', 'logps_train/rejected': '-149.06', 'logps_train/chosen': '-162.02', 'loss/train': '0.73922', 'examples_per_second': '31.516', 'grad_norm': '118.5', 'counters/examples': 136192, 'counters/updates': 4256}
train stats after 136224 examples: {'rewards_train/chosen': '0.066883', 'rewards_train/rejected': '0.045782', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021101', 'logps_train/rejected': '-149.58', 'logps_train/chosen': '-140.14', 'loss/train': '0.70111', 'examples_per_second': '30.945', 'grad_norm': '97.5', 'counters/examples': 136224, 'counters/updates': 4257}
train stats after 136256 examples: {'rewards_train/chosen': '0.11967', 'rewards_train/rejected': '0.035777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083895', 'logps_train/rejected': '-112.65', 'logps_train/chosen': '-135.5', 'loss/train': '0.67168', 'examples_per_second': '31.473', 'grad_norm': '125.5', 'counters/examples': 136256, 'counters/updates': 4258}
train stats after 136288 examples: {'rewards_train/chosen': '0.19856', 'rewards_train/rejected': '0.084353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11421', 'logps_train/rejected': '-156.94', 'logps_train/chosen': '-155.28', 'loss/train': '0.65743', 'examples_per_second': '31.159', 'grad_norm': '90', 'counters/examples': 136288, 'counters/updates': 4259}
skipping logging after 136320 examples to avoid logging too frequently
train stats after 136352 examples: {'rewards_train/chosen': '0.11219', 'rewards_train/rejected': '-0.0069762', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11916', 'logps_train/rejected': '-145.44', 'logps_train/chosen': '-181.37', 'loss/train': '0.6603', 'examples_per_second': '31.547', 'grad_norm': '80', 'counters/examples': 136352, 'counters/updates': 4261}
skipping logging after 136384 examples to avoid logging too frequently
train stats after 136416 examples: {'rewards_train/chosen': '0.025336', 'rewards_train/rejected': '0.082256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.05692', 'logps_train/rejected': '-153.32', 'logps_train/chosen': '-150.99', 'loss/train': '0.73385', 'examples_per_second': '31.465', 'grad_norm': '89.5', 'counters/examples': 136416, 'counters/updates': 4263}
train stats after 136448 examples: {'rewards_train/chosen': '0.1324', 'rewards_train/rejected': '0.01757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11483', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-142.29', 'loss/train': '0.65743', 'examples_per_second': '30.381', 'grad_norm': '45', 'counters/examples': 136448, 'counters/updates': 4264}
train stats after 136480 examples: {'rewards_train/chosen': '0.17003', 'rewards_train/rejected': '0.053933', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1161', 'logps_train/rejected': '-124.12', 'logps_train/chosen': '-173.97', 'loss/train': '0.64911', 'examples_per_second': '31.334', 'grad_norm': '74', 'counters/examples': 136480, 'counters/updates': 4265}
skipping logging after 136512 examples to avoid logging too frequently
train stats after 136544 examples: {'rewards_train/chosen': '0.14272', 'rewards_train/rejected': '0.086953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055763', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-169.14', 'loss/train': '0.67521', 'examples_per_second': '31.502', 'grad_norm': '81.5', 'counters/examples': 136544, 'counters/updates': 4267}
train stats after 136576 examples: {'rewards_train/chosen': '0.062069', 'rewards_train/rejected': '0.092056', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029987', 'logps_train/rejected': '-155.58', 'logps_train/chosen': '-149.75', 'loss/train': '0.72639', 'examples_per_second': '30.179', 'grad_norm': '132', 'counters/examples': 136576, 'counters/updates': 4268}
train stats after 136608 examples: {'rewards_train/chosen': '0.087069', 'rewards_train/rejected': '0.07474', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012329', 'logps_train/rejected': '-162.93', 'logps_train/chosen': '-132.68', 'loss/train': '0.69268', 'examples_per_second': '31.085', 'grad_norm': '76', 'counters/examples': 136608, 'counters/updates': 4269}
train stats after 136640 examples: {'rewards_train/chosen': '0.07209', 'rewards_train/rejected': '0.029896', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042194', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-134.06', 'loss/train': '0.68223', 'examples_per_second': '30.927', 'grad_norm': '65.5', 'counters/examples': 136640, 'counters/updates': 4270}
train stats after 136672 examples: {'rewards_train/chosen': '0.156', 'rewards_train/rejected': '0.038254', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11775', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-176.9', 'loss/train': '0.65593', 'examples_per_second': '30.624', 'grad_norm': '103', 'counters/examples': 136672, 'counters/updates': 4271}
train stats after 136704 examples: {'rewards_train/chosen': '0.1357', 'rewards_train/rejected': '0.0039218', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13178', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-174.99', 'loss/train': '0.64856', 'examples_per_second': '31.533', 'grad_norm': '80.5', 'counters/examples': 136704, 'counters/updates': 4272}
skipping logging after 136736 examples to avoid logging too frequently
train stats after 136768 examples: {'rewards_train/chosen': '0.041571', 'rewards_train/rejected': '0.024353', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017219', 'logps_train/rejected': '-123.98', 'logps_train/chosen': '-131.68', 'loss/train': '0.6994', 'examples_per_second': '33.39', 'grad_norm': '70', 'counters/examples': 136768, 'counters/updates': 4274}
train stats after 136800 examples: {'rewards_train/chosen': '0.077606', 'rewards_train/rejected': '0.018517', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.059089', 'logps_train/rejected': '-100.57', 'logps_train/chosen': '-120.64', 'loss/train': '0.67325', 'examples_per_second': '30.533', 'grad_norm': '76', 'counters/examples': 136800, 'counters/updates': 4275}
train stats after 136832 examples: {'rewards_train/chosen': '0.074441', 'rewards_train/rejected': '0.03783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036611', 'logps_train/rejected': '-134.72', 'logps_train/chosen': '-156.73', 'loss/train': '0.68737', 'examples_per_second': '32.679', 'grad_norm': '89.5', 'counters/examples': 136832, 'counters/updates': 4276}
train stats after 136864 examples: {'rewards_train/chosen': '0.11023', 'rewards_train/rejected': '0.048953', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061276', 'logps_train/rejected': '-145.37', 'logps_train/chosen': '-146.49', 'loss/train': '0.67312', 'examples_per_second': '30.845', 'grad_norm': '74', 'counters/examples': 136864, 'counters/updates': 4277}
train stats after 136896 examples: {'rewards_train/chosen': '0.096234', 'rewards_train/rejected': '0.060587', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035648', 'logps_train/rejected': '-151.18', 'logps_train/chosen': '-146.9', 'loss/train': '0.68843', 'examples_per_second': '31.519', 'grad_norm': '91.5', 'counters/examples': 136896, 'counters/updates': 4278}
train stats after 136928 examples: {'rewards_train/chosen': '0.014959', 'rewards_train/rejected': '0.068264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.053305', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-150.14', 'loss/train': '0.73568', 'examples_per_second': '31.518', 'grad_norm': '280', 'counters/examples': 136928, 'counters/updates': 4279}
train stats after 136960 examples: {'rewards_train/chosen': '0.14661', 'rewards_train/rejected': '0.10759', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039019', 'logps_train/rejected': '-151.2', 'logps_train/chosen': '-161.83', 'loss/train': '0.6896', 'examples_per_second': '32.967', 'grad_norm': '183', 'counters/examples': 136960, 'counters/updates': 4280}
train stats after 136992 examples: {'rewards_train/chosen': '0.068569', 'rewards_train/rejected': '0.019804', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048765', 'logps_train/rejected': '-93.69', 'logps_train/chosen': '-160.66', 'loss/train': '0.68554', 'examples_per_second': '30.121', 'grad_norm': '89', 'counters/examples': 136992, 'counters/updates': 4281}
train stats after 137024 examples: {'rewards_train/chosen': '0.060393', 'rewards_train/rejected': '0.1438', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.083404', 'logps_train/rejected': '-181.96', 'logps_train/chosen': '-190.96', 'loss/train': '0.74985', 'examples_per_second': '32.807', 'grad_norm': '112', 'counters/examples': 137024, 'counters/updates': 4282}
train stats after 137056 examples: {'rewards_train/chosen': '0.1439', 'rewards_train/rejected': '0.021583', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12232', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-165.54', 'loss/train': '0.64664', 'examples_per_second': '31.559', 'grad_norm': '98', 'counters/examples': 137056, 'counters/updates': 4283}
train stats after 137088 examples: {'rewards_train/chosen': '0.11483', 'rewards_train/rejected': '0.084423', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03041', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-136.98', 'loss/train': '0.6916', 'examples_per_second': '30.543', 'grad_norm': '80', 'counters/examples': 137088, 'counters/updates': 4284}
train stats after 137120 examples: {'rewards_train/chosen': '0.13063', 'rewards_train/rejected': '0.048921', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081706', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-199.13', 'loss/train': '0.68537', 'examples_per_second': '31.568', 'grad_norm': '83.5', 'counters/examples': 137120, 'counters/updates': 4285}
train stats after 137152 examples: {'rewards_train/chosen': '0.054264', 'rewards_train/rejected': '0.11642', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.062161', 'logps_train/rejected': '-143.58', 'logps_train/chosen': '-175.81', 'loss/train': '0.75054', 'examples_per_second': '31.508', 'grad_norm': '148', 'counters/examples': 137152, 'counters/updates': 4286}
train stats after 137184 examples: {'rewards_train/chosen': '0.1155', 'rewards_train/rejected': '-0.0022791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11778', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-179.74', 'loss/train': '0.65175', 'examples_per_second': '31.444', 'grad_norm': '87.5', 'counters/examples': 137184, 'counters/updates': 4287}
train stats after 137216 examples: {'rewards_train/chosen': '0.071531', 'rewards_train/rejected': '0.016699', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054833', 'logps_train/rejected': '-100.14', 'logps_train/chosen': '-144.46', 'loss/train': '0.67531', 'examples_per_second': '31.732', 'grad_norm': '52.75', 'counters/examples': 137216, 'counters/updates': 4288}
train stats after 137248 examples: {'rewards_train/chosen': '0.084085', 'rewards_train/rejected': '0.027902', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.056183', 'logps_train/rejected': '-153.29', 'logps_train/chosen': '-188.4', 'loss/train': '0.67903', 'examples_per_second': '30.086', 'grad_norm': '104', 'counters/examples': 137248, 'counters/updates': 4289}
skipping logging after 137280 examples to avoid logging too frequently
train stats after 137312 examples: {'rewards_train/chosen': '0.062099', 'rewards_train/rejected': '-0.027775', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089874', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-147.04', 'loss/train': '0.65927', 'examples_per_second': '35.728', 'grad_norm': '85', 'counters/examples': 137312, 'counters/updates': 4291}
train stats after 137344 examples: {'rewards_train/chosen': '0.039824', 'rewards_train/rejected': '0.024693', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015131', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-144.65', 'loss/train': '0.69343', 'examples_per_second': '30.002', 'grad_norm': '80', 'counters/examples': 137344, 'counters/updates': 4292}
train stats after 137376 examples: {'rewards_train/chosen': '0.033412', 'rewards_train/rejected': '0.079234', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045821', 'logps_train/rejected': '-144.94', 'logps_train/chosen': '-179.28', 'loss/train': '0.73277', 'examples_per_second': '32.071', 'grad_norm': '66', 'counters/examples': 137376, 'counters/updates': 4293}
train stats after 137408 examples: {'rewards_train/chosen': '0.11529', 'rewards_train/rejected': '0.056525', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058764', 'logps_train/rejected': '-176.77', 'logps_train/chosen': '-169.27', 'loss/train': '0.68064', 'examples_per_second': '32.364', 'grad_norm': '75.5', 'counters/examples': 137408, 'counters/updates': 4294}
skipping logging after 137440 examples to avoid logging too frequently
train stats after 137472 examples: {'rewards_train/chosen': '0.11138', 'rewards_train/rejected': '-0.030442', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14182', 'logps_train/rejected': '-157.83', 'logps_train/chosen': '-171.53', 'loss/train': '0.63845', 'examples_per_second': '31.457', 'grad_norm': '83.5', 'counters/examples': 137472, 'counters/updates': 4296}
train stats after 137504 examples: {'rewards_train/chosen': '-0.029953', 'rewards_train/rejected': '0.0036941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033647', 'logps_train/rejected': '-113.48', 'logps_train/chosen': '-141.3', 'loss/train': '0.71805', 'examples_per_second': '31.293', 'grad_norm': '64.5', 'counters/examples': 137504, 'counters/updates': 4297}
train stats after 137536 examples: {'rewards_train/chosen': '-0.0062785', 'rewards_train/rejected': '-0.019954', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013675', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-135.73', 'loss/train': '0.69614', 'examples_per_second': '31.47', 'grad_norm': '90.5', 'counters/examples': 137536, 'counters/updates': 4298}
train stats after 137568 examples: {'rewards_train/chosen': '0.10792', 'rewards_train/rejected': '0.066805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041119', 'logps_train/rejected': '-171.71', 'logps_train/chosen': '-191.2', 'loss/train': '0.68126', 'examples_per_second': '29.954', 'grad_norm': '142', 'counters/examples': 137568, 'counters/updates': 4299}
skipping logging after 137600 examples to avoid logging too frequently
train stats after 137632 examples: {'rewards_train/chosen': '0.098664', 'rewards_train/rejected': '0.066943', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03172', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-149.96', 'loss/train': '0.70711', 'examples_per_second': '31.704', 'grad_norm': '73', 'counters/examples': 137632, 'counters/updates': 4301}
train stats after 137664 examples: {'rewards_train/chosen': '0.032094', 'rewards_train/rejected': '-0.044082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076175', 'logps_train/rejected': '-87.786', 'logps_train/chosen': '-125.55', 'loss/train': '0.67382', 'examples_per_second': '32.087', 'grad_norm': '53.75', 'counters/examples': 137664, 'counters/updates': 4302}
train stats after 137696 examples: {'rewards_train/chosen': '0.093828', 'rewards_train/rejected': '0.031133', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062696', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-176.84', 'loss/train': '0.67431', 'examples_per_second': '31.568', 'grad_norm': '95', 'counters/examples': 137696, 'counters/updates': 4303}
train stats after 137728 examples: {'rewards_train/chosen': '-0.016416', 'rewards_train/rejected': '0.081116', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.097533', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-157.42', 'loss/train': '0.75931', 'examples_per_second': '32.715', 'grad_norm': '121', 'counters/examples': 137728, 'counters/updates': 4304}
train stats after 137760 examples: {'rewards_train/chosen': '0.062954', 'rewards_train/rejected': '0.083015', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020061', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-133.59', 'loss/train': '0.71436', 'examples_per_second': '30.613', 'grad_norm': '88.5', 'counters/examples': 137760, 'counters/updates': 4305}
train stats after 137792 examples: {'rewards_train/chosen': '0.088737', 'rewards_train/rejected': '0.061685', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027052', 'logps_train/rejected': '-104.48', 'logps_train/chosen': '-131.68', 'loss/train': '0.68756', 'examples_per_second': '32.238', 'grad_norm': '79', 'counters/examples': 137792, 'counters/updates': 4306}
train stats after 137824 examples: {'rewards_train/chosen': '0.083109', 'rewards_train/rejected': '0.014307', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068802', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-166.83', 'loss/train': '0.67333', 'examples_per_second': '31.593', 'grad_norm': '120', 'counters/examples': 137824, 'counters/updates': 4307}
train stats after 137856 examples: {'rewards_train/chosen': '0.10723', 'rewards_train/rejected': '-0.034688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14191', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-132.19', 'loss/train': '0.63574', 'examples_per_second': '30.575', 'grad_norm': '60.75', 'counters/examples': 137856, 'counters/updates': 4308}
train stats after 137888 examples: {'rewards_train/chosen': '0.16663', 'rewards_train/rejected': '0.084603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082026', 'logps_train/rejected': '-131.05', 'logps_train/chosen': '-127.56', 'loss/train': '0.67176', 'examples_per_second': '32.407', 'grad_norm': '103.5', 'counters/examples': 137888, 'counters/updates': 4309}
train stats after 137920 examples: {'rewards_train/chosen': '0.15701', 'rewards_train/rejected': '0.027223', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12979', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-144.48', 'loss/train': '0.64238', 'examples_per_second': '31.277', 'grad_norm': '86.5', 'counters/examples': 137920, 'counters/updates': 4310}
train stats after 137952 examples: {'rewards_train/chosen': '0.1319', 'rewards_train/rejected': '0.012777', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11913', 'logps_train/rejected': '-153.09', 'logps_train/chosen': '-164.5', 'loss/train': '0.66262', 'examples_per_second': '30.539', 'grad_norm': '90.5', 'counters/examples': 137952, 'counters/updates': 4311}
train stats after 137984 examples: {'rewards_train/chosen': '0.095379', 'rewards_train/rejected': '0.055467', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039912', 'logps_train/rejected': '-149.77', 'logps_train/chosen': '-184.3', 'loss/train': '0.69706', 'examples_per_second': '30.88', 'grad_norm': '111.5', 'counters/examples': 137984, 'counters/updates': 4312}
train stats after 138016 examples: {'rewards_train/chosen': '0.10588', 'rewards_train/rejected': '0.048522', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057361', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-150.28', 'loss/train': '0.6731', 'examples_per_second': '30.215', 'grad_norm': '92.5', 'counters/examples': 138016, 'counters/updates': 4313}
train stats after 138048 examples: {'rewards_train/chosen': '0.1151', 'rewards_train/rejected': '0.042925', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072174', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-138.81', 'loss/train': '0.67692', 'examples_per_second': '31.093', 'grad_norm': '57.25', 'counters/examples': 138048, 'counters/updates': 4314}
train stats after 138080 examples: {'rewards_train/chosen': '0.15658', 'rewards_train/rejected': '0.010107', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14647', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-153.78', 'loss/train': '0.63466', 'examples_per_second': '24.124', 'grad_norm': '88', 'counters/examples': 138080, 'counters/updates': 4315}
train stats after 138112 examples: {'rewards_train/chosen': '0.16549', 'rewards_train/rejected': '0.040398', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12509', 'logps_train/rejected': '-174.17', 'logps_train/chosen': '-200.54', 'loss/train': '0.63961', 'examples_per_second': '31.309', 'grad_norm': '121', 'counters/examples': 138112, 'counters/updates': 4316}
train stats after 138144 examples: {'rewards_train/chosen': '-0.032579', 'rewards_train/rejected': '0.094587', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.12717', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-184.55', 'loss/train': '0.77969', 'examples_per_second': '32.417', 'grad_norm': '136', 'counters/examples': 138144, 'counters/updates': 4317}
skipping logging after 138176 examples to avoid logging too frequently
train stats after 138208 examples: {'rewards_train/chosen': '0.21548', 'rewards_train/rejected': '-0.028886', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24437', 'logps_train/rejected': '-178.13', 'logps_train/chosen': '-167.45', 'loss/train': '0.59666', 'examples_per_second': '30.056', 'grad_norm': '173', 'counters/examples': 138208, 'counters/updates': 4319}
train stats after 138240 examples: {'rewards_train/chosen': '0.0067087', 'rewards_train/rejected': '0.02413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.017421', 'logps_train/rejected': '-113.36', 'logps_train/chosen': '-138.94', 'loss/train': '0.71293', 'examples_per_second': '32.086', 'grad_norm': '74.5', 'counters/examples': 138240, 'counters/updates': 4320}
train stats after 138272 examples: {'rewards_train/chosen': '-0.007053', 'rewards_train/rejected': '0.058581', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.065634', 'logps_train/rejected': '-130.05', 'logps_train/chosen': '-103.45', 'loss/train': '0.73664', 'examples_per_second': '31.192', 'grad_norm': '95', 'counters/examples': 138272, 'counters/updates': 4321}
train stats after 138304 examples: {'rewards_train/chosen': '0.12802', 'rewards_train/rejected': '0.087053', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040964', 'logps_train/rejected': '-131.68', 'logps_train/chosen': '-140.9', 'loss/train': '0.69216', 'examples_per_second': '30.252', 'grad_norm': '105.5', 'counters/examples': 138304, 'counters/updates': 4322}
train stats after 138336 examples: {'rewards_train/chosen': '0.0010198', 'rewards_train/rejected': '0.041646', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040626', 'logps_train/rejected': '-145.06', 'logps_train/chosen': '-147.95', 'loss/train': '0.72596', 'examples_per_second': '31.586', 'grad_norm': '117', 'counters/examples': 138336, 'counters/updates': 4323}
train stats after 138368 examples: {'rewards_train/chosen': '0.14839', 'rewards_train/rejected': '0.049769', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098619', 'logps_train/rejected': '-139.81', 'logps_train/chosen': '-169.8', 'loss/train': '0.66379', 'examples_per_second': '31.238', 'grad_norm': '97', 'counters/examples': 138368, 'counters/updates': 4324}
train stats after 138400 examples: {'rewards_train/chosen': '0.068846', 'rewards_train/rejected': '0.060396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0084494', 'logps_train/rejected': '-109.93', 'logps_train/chosen': '-134.12', 'loss/train': '0.70235', 'examples_per_second': '30.929', 'grad_norm': '141', 'counters/examples': 138400, 'counters/updates': 4325}
train stats after 138432 examples: {'rewards_train/chosen': '0.074495', 'rewards_train/rejected': '0.019947', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054548', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-112.19', 'loss/train': '0.67381', 'examples_per_second': '31.037', 'grad_norm': '62.75', 'counters/examples': 138432, 'counters/updates': 4326}
train stats after 138464 examples: {'rewards_train/chosen': '0.19748', 'rewards_train/rejected': '0.073643', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12383', 'logps_train/rejected': '-170.04', 'logps_train/chosen': '-188', 'loss/train': '0.65271', 'examples_per_second': '31.584', 'grad_norm': '54.5', 'counters/examples': 138464, 'counters/updates': 4327}
train stats after 138496 examples: {'rewards_train/chosen': '0.12152', 'rewards_train/rejected': '0.036851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084674', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-150.64', 'loss/train': '0.66608', 'examples_per_second': '31.544', 'grad_norm': '74.5', 'counters/examples': 138496, 'counters/updates': 4328}
train stats after 138528 examples: {'rewards_train/chosen': '0.14333', 'rewards_train/rejected': '0.09435', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048978', 'logps_train/rejected': '-174.6', 'logps_train/chosen': '-173.45', 'loss/train': '0.67982', 'examples_per_second': '31.752', 'grad_norm': '448', 'counters/examples': 138528, 'counters/updates': 4329}
train stats after 138560 examples: {'rewards_train/chosen': '0.057664', 'rewards_train/rejected': '0.02216', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035504', 'logps_train/rejected': '-153.81', 'logps_train/chosen': '-153.82', 'loss/train': '0.6901', 'examples_per_second': '31.522', 'grad_norm': '63.25', 'counters/examples': 138560, 'counters/updates': 4330}
train stats after 138592 examples: {'rewards_train/chosen': '0.10098', 'rewards_train/rejected': '0.12845', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.027472', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-158.18', 'loss/train': '0.73351', 'examples_per_second': '30.728', 'grad_norm': '67', 'counters/examples': 138592, 'counters/updates': 4331}
train stats after 138624 examples: {'rewards_train/chosen': '0.054589', 'rewards_train/rejected': '0.10091', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.046322', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-119.21', 'loss/train': '0.73127', 'examples_per_second': '33.244', 'grad_norm': '81.5', 'counters/examples': 138624, 'counters/updates': 4332}
train stats after 138656 examples: {'rewards_train/chosen': '0.097609', 'rewards_train/rejected': '0.11027', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012665', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-147.84', 'loss/train': '0.70673', 'examples_per_second': '32.827', 'grad_norm': '56.25', 'counters/examples': 138656, 'counters/updates': 4333}
train stats after 138688 examples: {'rewards_train/chosen': '0.12407', 'rewards_train/rejected': '0.0039994', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12007', 'logps_train/rejected': '-104.8', 'logps_train/chosen': '-126.67', 'loss/train': '0.64896', 'examples_per_second': '30.345', 'grad_norm': '82.5', 'counters/examples': 138688, 'counters/updates': 4334}
train stats after 138720 examples: {'rewards_train/chosen': '0.050497', 'rewards_train/rejected': '0.043253', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0072434', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-162.76', 'loss/train': '0.70248', 'examples_per_second': '31.19', 'grad_norm': '128', 'counters/examples': 138720, 'counters/updates': 4335}
train stats after 138752 examples: {'rewards_train/chosen': '0.094773', 'rewards_train/rejected': '-0.06598', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16075', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-157.25', 'loss/train': '0.62724', 'examples_per_second': '31.498', 'grad_norm': '150', 'counters/examples': 138752, 'counters/updates': 4336}
skipping logging after 138784 examples to avoid logging too frequently
train stats after 138816 examples: {'rewards_train/chosen': '0.10648', 'rewards_train/rejected': '0.11386', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0073826', 'logps_train/rejected': '-90.683', 'logps_train/chosen': '-114.67', 'loss/train': '0.70087', 'examples_per_second': '36.213', 'grad_norm': '41.75', 'counters/examples': 138816, 'counters/updates': 4338}
train stats after 138848 examples: {'rewards_train/chosen': '0.058352', 'rewards_train/rejected': '0.084809', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026457', 'logps_train/rejected': '-148.66', 'logps_train/chosen': '-162.24', 'loss/train': '0.7182', 'examples_per_second': '31.609', 'grad_norm': '69.5', 'counters/examples': 138848, 'counters/updates': 4339}
train stats after 138880 examples: {'rewards_train/chosen': '0.091031', 'rewards_train/rejected': '0.030965', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060066', 'logps_train/rejected': '-135.83', 'logps_train/chosen': '-142.99', 'loss/train': '0.67326', 'examples_per_second': '32.38', 'grad_norm': '66.5', 'counters/examples': 138880, 'counters/updates': 4340}
train stats after 138912 examples: {'rewards_train/chosen': '-0.02952', 'rewards_train/rejected': '0.052209', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.08173', 'logps_train/rejected': '-142.92', 'logps_train/chosen': '-168.81', 'loss/train': '0.74663', 'examples_per_second': '30.428', 'grad_norm': '106.5', 'counters/examples': 138912, 'counters/updates': 4341}
skipping logging after 138944 examples to avoid logging too frequently
train stats after 138976 examples: {'rewards_train/chosen': '0.077543', 'rewards_train/rejected': '0.068841', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0087024', 'logps_train/rejected': '-146.97', 'logps_train/chosen': '-149.61', 'loss/train': '0.70375', 'examples_per_second': '31.067', 'grad_norm': '69.5', 'counters/examples': 138976, 'counters/updates': 4343}
train stats after 139008 examples: {'rewards_train/chosen': '0.11389', 'rewards_train/rejected': '0.058708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055183', 'logps_train/rejected': '-128.78', 'logps_train/chosen': '-161.62', 'loss/train': '0.6786', 'examples_per_second': '32.403', 'grad_norm': '119.5', 'counters/examples': 139008, 'counters/updates': 4344}
train stats after 139040 examples: {'rewards_train/chosen': '0.053058', 'rewards_train/rejected': '0.0066293', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046429', 'logps_train/rejected': '-89.861', 'logps_train/chosen': '-160.24', 'loss/train': '0.68449', 'examples_per_second': '29.589', 'grad_norm': '75.5', 'counters/examples': 139040, 'counters/updates': 4345}
train stats after 139072 examples: {'rewards_train/chosen': '0.16624', 'rewards_train/rejected': '0.077083', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089152', 'logps_train/rejected': '-174.63', 'logps_train/chosen': '-188.3', 'loss/train': '0.67193', 'examples_per_second': '31.176', 'grad_norm': '78', 'counters/examples': 139072, 'counters/updates': 4346}
train stats after 139104 examples: {'rewards_train/chosen': '0.12195', 'rewards_train/rejected': '0.056122', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065823', 'logps_train/rejected': '-117.24', 'logps_train/chosen': '-128.59', 'loss/train': '0.67787', 'examples_per_second': '30.538', 'grad_norm': '91', 'counters/examples': 139104, 'counters/updates': 4347}
train stats after 139136 examples: {'rewards_train/chosen': '0.18642', 'rewards_train/rejected': '0.013231', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17319', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-150.52', 'loss/train': '0.61721', 'examples_per_second': '32.332', 'grad_norm': '64.5', 'counters/examples': 139136, 'counters/updates': 4348}
train stats after 139168 examples: {'rewards_train/chosen': '0.050913', 'rewards_train/rejected': '0.013176', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037738', 'logps_train/rejected': '-168.35', 'logps_train/chosen': '-159.79', 'loss/train': '0.69423', 'examples_per_second': '31.987', 'grad_norm': '64', 'counters/examples': 139168, 'counters/updates': 4349}
train stats after 139200 examples: {'rewards_train/chosen': '0.17517', 'rewards_train/rejected': '0.059484', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11568', 'logps_train/rejected': '-158.73', 'logps_train/chosen': '-184', 'loss/train': '0.65736', 'examples_per_second': '32.444', 'grad_norm': '116', 'counters/examples': 139200, 'counters/updates': 4350}
train stats after 139232 examples: {'rewards_train/chosen': '0.022192', 'rewards_train/rejected': '0.14184', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.11965', 'logps_train/rejected': '-175.85', 'logps_train/chosen': '-149.58', 'loss/train': '0.77109', 'examples_per_second': '31.551', 'grad_norm': '99.5', 'counters/examples': 139232, 'counters/updates': 4351}
train stats after 139264 examples: {'rewards_train/chosen': '0.053873', 'rewards_train/rejected': '0.035347', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018526', 'logps_train/rejected': '-167.24', 'logps_train/chosen': '-149.23', 'loss/train': '0.69348', 'examples_per_second': '31.496', 'grad_norm': '129', 'counters/examples': 139264, 'counters/updates': 4352}
train stats after 139296 examples: {'rewards_train/chosen': '0.10312', 'rewards_train/rejected': '0.079393', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023725', 'logps_train/rejected': '-155.93', 'logps_train/chosen': '-137.46', 'loss/train': '0.70512', 'examples_per_second': '31.542', 'grad_norm': '107', 'counters/examples': 139296, 'counters/updates': 4353}
skipping logging after 139328 examples to avoid logging too frequently
train stats after 139360 examples: {'rewards_train/chosen': '0.094135', 'rewards_train/rejected': '1.9439e-05', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094116', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-150.21', 'loss/train': '0.66132', 'examples_per_second': '32.068', 'grad_norm': '87.5', 'counters/examples': 139360, 'counters/updates': 4355}
train stats after 139392 examples: {'rewards_train/chosen': '0.1603', 'rewards_train/rejected': '0.0011581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15914', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-166.88', 'loss/train': '0.65149', 'examples_per_second': '32.456', 'grad_norm': '187', 'counters/examples': 139392, 'counters/updates': 4356}
skipping logging after 139424 examples to avoid logging too frequently
train stats after 139456 examples: {'rewards_train/chosen': '0.074584', 'rewards_train/rejected': '0.060137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014448', 'logps_train/rejected': '-164.5', 'logps_train/chosen': '-160.53', 'loss/train': '0.70246', 'examples_per_second': '31.541', 'grad_norm': '76', 'counters/examples': 139456, 'counters/updates': 4358}
train stats after 139488 examples: {'rewards_train/chosen': '0.072713', 'rewards_train/rejected': '0.027002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045711', 'logps_train/rejected': '-115.4', 'logps_train/chosen': '-136.74', 'loss/train': '0.67549', 'examples_per_second': '30.765', 'grad_norm': '63.75', 'counters/examples': 139488, 'counters/updates': 4359}
train stats after 139520 examples: {'rewards_train/chosen': '0.081733', 'rewards_train/rejected': '0.001323', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08041', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-155.69', 'loss/train': '0.6653', 'examples_per_second': '31.465', 'grad_norm': '81.5', 'counters/examples': 139520, 'counters/updates': 4360}
train stats after 139552 examples: {'rewards_train/chosen': '0.014635', 'rewards_train/rejected': '-0.051461', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066096', 'logps_train/rejected': '-103.72', 'logps_train/chosen': '-135.97', 'loss/train': '0.68204', 'examples_per_second': '31.733', 'grad_norm': '121', 'counters/examples': 139552, 'counters/updates': 4361}
train stats after 139584 examples: {'rewards_train/chosen': '0.058572', 'rewards_train/rejected': '0.068328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0097562', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-134.77', 'loss/train': '0.70963', 'examples_per_second': '31.3', 'grad_norm': '111', 'counters/examples': 139584, 'counters/updates': 4362}
train stats after 139616 examples: {'rewards_train/chosen': '0.12049', 'rewards_train/rejected': '-0.013668', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13416', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-186.83', 'loss/train': '0.65147', 'examples_per_second': '32.068', 'grad_norm': '76', 'counters/examples': 139616, 'counters/updates': 4363}
skipping logging after 139648 examples to avoid logging too frequently
train stats after 139680 examples: {'rewards_train/chosen': '0.081468', 'rewards_train/rejected': '0.022806', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.058663', 'logps_train/rejected': '-114.39', 'logps_train/chosen': '-146.7', 'loss/train': '0.67647', 'examples_per_second': '36.64', 'grad_norm': '80', 'counters/examples': 139680, 'counters/updates': 4365}
train stats after 139712 examples: {'rewards_train/chosen': '0.064532', 'rewards_train/rejected': '0.050312', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.01422', 'logps_train/rejected': '-149.71', 'logps_train/chosen': '-165.61', 'loss/train': '0.70198', 'examples_per_second': '30.772', 'grad_norm': '86.5', 'counters/examples': 139712, 'counters/updates': 4366}
train stats after 139744 examples: {'rewards_train/chosen': '0.02376', 'rewards_train/rejected': '-0.0027082', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026468', 'logps_train/rejected': '-123.64', 'logps_train/chosen': '-141.72', 'loss/train': '0.69162', 'examples_per_second': '31.586', 'grad_norm': '97', 'counters/examples': 139744, 'counters/updates': 4367}
train stats after 139776 examples: {'rewards_train/chosen': '0.068385', 'rewards_train/rejected': '0.11824', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.049851', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-133.52', 'loss/train': '0.73932', 'examples_per_second': '32.949', 'grad_norm': '123.5', 'counters/examples': 139776, 'counters/updates': 4368}
skipping logging after 139808 examples to avoid logging too frequently
train stats after 139840 examples: {'rewards_train/chosen': '0.03408', 'rewards_train/rejected': '0.084543', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.050463', 'logps_train/rejected': '-170.01', 'logps_train/chosen': '-151.74', 'loss/train': '0.73297', 'examples_per_second': '30.346', 'grad_norm': '101', 'counters/examples': 139840, 'counters/updates': 4370}
train stats after 139872 examples: {'rewards_train/chosen': '-0.028865', 'rewards_train/rejected': '0.025759', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054624', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-155.06', 'loss/train': '0.74129', 'examples_per_second': '31.47', 'grad_norm': '111.5', 'counters/examples': 139872, 'counters/updates': 4371}
train stats after 139904 examples: {'rewards_train/chosen': '0.081768', 'rewards_train/rejected': '0.00027084', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081498', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-149.06', 'loss/train': '0.66489', 'examples_per_second': '30.362', 'grad_norm': '55.75', 'counters/examples': 139904, 'counters/updates': 4372}
train stats after 139936 examples: {'rewards_train/chosen': '0.019265', 'rewards_train/rejected': '0.11207', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.092804', 'logps_train/rejected': '-174.18', 'logps_train/chosen': '-170.14', 'loss/train': '0.74982', 'examples_per_second': '31.531', 'grad_norm': '111.5', 'counters/examples': 139936, 'counters/updates': 4373}
train stats after 139968 examples: {'rewards_train/chosen': '0.14375', 'rewards_train/rejected': '0.089928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053819', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-144.98', 'loss/train': '0.67404', 'examples_per_second': '31.51', 'grad_norm': '93', 'counters/examples': 139968, 'counters/updates': 4374}
train stats after 140000 examples: {'rewards_train/chosen': '0.073011', 'rewards_train/rejected': '0.078659', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0056483', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-139.21', 'loss/train': '0.70463', 'examples_per_second': '31.505', 'grad_norm': '71', 'counters/examples': 140000, 'counters/updates': 4375}
train stats after 140032 examples: {'rewards_train/chosen': '0.099643', 'rewards_train/rejected': '0.15313', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.05349', 'logps_train/rejected': '-171.23', 'logps_train/chosen': '-132.29', 'loss/train': '0.73866', 'examples_per_second': '30.717', 'grad_norm': '87.5', 'counters/examples': 140032, 'counters/updates': 4376}
train stats after 140064 examples: {'rewards_train/chosen': '0.032408', 'rewards_train/rejected': '0.034401', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0019929', 'logps_train/rejected': '-103.38', 'logps_train/chosen': '-133.28', 'loss/train': '0.69992', 'examples_per_second': '31.594', 'grad_norm': '90', 'counters/examples': 140064, 'counters/updates': 4377}
train stats after 140096 examples: {'rewards_train/chosen': '0.11766', 'rewards_train/rejected': '-0.06487', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18253', 'logps_train/rejected': '-94.536', 'logps_train/chosen': '-160.79', 'loss/train': '0.6279', 'examples_per_second': '32.059', 'grad_norm': '103', 'counters/examples': 140096, 'counters/updates': 4378}
train stats after 140128 examples: {'rewards_train/chosen': '0.11073', 'rewards_train/rejected': '0.073259', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03747', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-150.32', 'loss/train': '0.68716', 'examples_per_second': '30.031', 'grad_norm': '71', 'counters/examples': 140128, 'counters/updates': 4379}
train stats after 140160 examples: {'rewards_train/chosen': '0.023778', 'rewards_train/rejected': '0.015176', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0086016', 'logps_train/rejected': '-111.46', 'logps_train/chosen': '-158.84', 'loss/train': '0.69577', 'examples_per_second': '30.556', 'grad_norm': '83.5', 'counters/examples': 140160, 'counters/updates': 4380}
train stats after 140192 examples: {'rewards_train/chosen': '0.09647', 'rewards_train/rejected': '0.061194', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035276', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-135.71', 'loss/train': '0.69393', 'examples_per_second': '31.323', 'grad_norm': '80', 'counters/examples': 140192, 'counters/updates': 4381}
train stats after 140224 examples: {'rewards_train/chosen': '0.087949', 'rewards_train/rejected': '0.031078', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056871', 'logps_train/rejected': '-117.41', 'logps_train/chosen': '-205.92', 'loss/train': '0.67632', 'examples_per_second': '31.523', 'grad_norm': '138', 'counters/examples': 140224, 'counters/updates': 4382}
train stats after 140256 examples: {'rewards_train/chosen': '0.14626', 'rewards_train/rejected': '0.039371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10689', 'logps_train/rejected': '-191.92', 'logps_train/chosen': '-183.22', 'loss/train': '0.65807', 'examples_per_second': '33.021', 'grad_norm': '110.5', 'counters/examples': 140256, 'counters/updates': 4383}
train stats after 140288 examples: {'rewards_train/chosen': '0.16328', 'rewards_train/rejected': '0.0029496', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16033', 'logps_train/rejected': '-157.1', 'logps_train/chosen': '-163.91', 'loss/train': '0.62612', 'examples_per_second': '31.465', 'grad_norm': '80', 'counters/examples': 140288, 'counters/updates': 4384}
train stats after 140320 examples: {'rewards_train/chosen': '0.12126', 'rewards_train/rejected': '0.010942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11032', 'logps_train/rejected': '-117.34', 'logps_train/chosen': '-148.71', 'loss/train': '0.65615', 'examples_per_second': '30.526', 'grad_norm': '46.25', 'counters/examples': 140320, 'counters/updates': 4385}
skipping logging after 140352 examples to avoid logging too frequently
train stats after 140384 examples: {'rewards_train/chosen': '0.050244', 'rewards_train/rejected': '0.034036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016208', 'logps_train/rejected': '-139.2', 'logps_train/chosen': '-180.85', 'loss/train': '0.69486', 'examples_per_second': '31.019', 'grad_norm': '161', 'counters/examples': 140384, 'counters/updates': 4387}
train stats after 140416 examples: {'rewards_train/chosen': '0.14389', 'rewards_train/rejected': '0.087926', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055963', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-116.91', 'loss/train': '0.70693', 'examples_per_second': '30.902', 'grad_norm': '89', 'counters/examples': 140416, 'counters/updates': 4388}
train stats after 140448 examples: {'rewards_train/chosen': '-0.010665', 'rewards_train/rejected': '0.080562', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.091227', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-162.2', 'loss/train': '0.76173', 'examples_per_second': '32.597', 'grad_norm': '111.5', 'counters/examples': 140448, 'counters/updates': 4389}
skipping logging after 140480 examples to avoid logging too frequently
train stats after 140512 examples: {'rewards_train/chosen': '0.06602', 'rewards_train/rejected': '0.12693', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06091', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-140.33', 'loss/train': '0.74196', 'examples_per_second': '32.618', 'grad_norm': '128', 'counters/examples': 140512, 'counters/updates': 4391}
train stats after 140544 examples: {'rewards_train/chosen': '0.13205', 'rewards_train/rejected': '0.026104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10594', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-155.05', 'loss/train': '0.65542', 'examples_per_second': '32.179', 'grad_norm': '83', 'counters/examples': 140544, 'counters/updates': 4392}
train stats after 140576 examples: {'rewards_train/chosen': '0.068972', 'rewards_train/rejected': '0.1411', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.072132', 'logps_train/rejected': '-159.04', 'logps_train/chosen': '-159.71', 'loss/train': '0.75291', 'examples_per_second': '30.694', 'grad_norm': '106.5', 'counters/examples': 140576, 'counters/updates': 4393}
train stats after 140608 examples: {'rewards_train/chosen': '0.025265', 'rewards_train/rejected': '0.042244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016978', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-126.64', 'loss/train': '0.71168', 'examples_per_second': '31.662', 'grad_norm': '79.5', 'counters/examples': 140608, 'counters/updates': 4394}
train stats after 140640 examples: {'rewards_train/chosen': '0.1078', 'rewards_train/rejected': '-0.042091', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14989', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-132.69', 'loss/train': '0.62946', 'examples_per_second': '30.012', 'grad_norm': '97.5', 'counters/examples': 140640, 'counters/updates': 4395}
train stats after 140672 examples: {'rewards_train/chosen': '0.10456', 'rewards_train/rejected': '0.081206', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023358', 'logps_train/rejected': '-186.7', 'logps_train/chosen': '-170.2', 'loss/train': '0.68916', 'examples_per_second': '32.432', 'grad_norm': '233', 'counters/examples': 140672, 'counters/updates': 4396}
train stats after 140704 examples: {'rewards_train/chosen': '0.18369', 'rewards_train/rejected': '0.032559', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15113', 'logps_train/rejected': '-155', 'logps_train/chosen': '-195.15', 'loss/train': '0.64161', 'examples_per_second': '30.855', 'grad_norm': '93', 'counters/examples': 140704, 'counters/updates': 4397}
train stats after 140736 examples: {'rewards_train/chosen': '0.095835', 'rewards_train/rejected': '0.037562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058273', 'logps_train/rejected': '-111.14', 'logps_train/chosen': '-148.51', 'loss/train': '0.6741', 'examples_per_second': '31.528', 'grad_norm': '80.5', 'counters/examples': 140736, 'counters/updates': 4398}
train stats after 140768 examples: {'rewards_train/chosen': '0.093687', 'rewards_train/rejected': '0.1157', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022012', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-146.32', 'loss/train': '0.71452', 'examples_per_second': '30.806', 'grad_norm': '184', 'counters/examples': 140768, 'counters/updates': 4399}
train stats after 140800 examples: {'rewards_train/chosen': '0.15237', 'rewards_train/rejected': '0.0047988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14757', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-159.12', 'loss/train': '0.63002', 'examples_per_second': '32.094', 'grad_norm': '70.5', 'counters/examples': 140800, 'counters/updates': 4400}
train stats after 140832 examples: {'rewards_train/chosen': '0.083609', 'rewards_train/rejected': '0.034889', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04872', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-181.01', 'loss/train': '0.67963', 'examples_per_second': '30.726', 'grad_norm': '167', 'counters/examples': 140832, 'counters/updates': 4401}
train stats after 140864 examples: {'rewards_train/chosen': '0.11026', 'rewards_train/rejected': '0.018013', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092251', 'logps_train/rejected': '-98.344', 'logps_train/chosen': '-147.98', 'loss/train': '0.66065', 'examples_per_second': '30.583', 'grad_norm': '62.5', 'counters/examples': 140864, 'counters/updates': 4402}
train stats after 140896 examples: {'rewards_train/chosen': '0.12711', 'rewards_train/rejected': '0.010919', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11619', 'logps_train/rejected': '-86.977', 'logps_train/chosen': '-126.66', 'loss/train': '0.64243', 'examples_per_second': '31.685', 'grad_norm': '70', 'counters/examples': 140896, 'counters/updates': 4403}
train stats after 140928 examples: {'rewards_train/chosen': '0.047748', 'rewards_train/rejected': '0.023201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024547', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-135.17', 'loss/train': '0.69213', 'examples_per_second': '32.077', 'grad_norm': '112', 'counters/examples': 140928, 'counters/updates': 4404}
train stats after 140960 examples: {'rewards_train/chosen': '0.0062949', 'rewards_train/rejected': '0.048424', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042129', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-144.77', 'loss/train': '0.7223', 'examples_per_second': '30.927', 'grad_norm': '44.75', 'counters/examples': 140960, 'counters/updates': 4405}
train stats after 140992 examples: {'rewards_train/chosen': '0.14168', 'rewards_train/rejected': '0.093977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047703', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-165.87', 'loss/train': '0.68025', 'examples_per_second': '24.198', 'grad_norm': '107', 'counters/examples': 140992, 'counters/updates': 4406}
train stats after 141024 examples: {'rewards_train/chosen': '0.064113', 'rewards_train/rejected': '0.090522', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026409', 'logps_train/rejected': '-206.89', 'logps_train/chosen': '-138.12', 'loss/train': '0.73234', 'examples_per_second': '31.496', 'grad_norm': '124.5', 'counters/examples': 141024, 'counters/updates': 4407}
train stats after 141056 examples: {'rewards_train/chosen': '0.097444', 'rewards_train/rejected': '0.040233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057212', 'logps_train/rejected': '-106.28', 'logps_train/chosen': '-158.46', 'loss/train': '0.6767', 'examples_per_second': '29.921', 'grad_norm': '79', 'counters/examples': 141056, 'counters/updates': 4408}
train stats after 141088 examples: {'rewards_train/chosen': '0.081701', 'rewards_train/rejected': '0.085984', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0042835', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-152.41', 'loss/train': '0.70989', 'examples_per_second': '25.161', 'grad_norm': '118', 'counters/examples': 141088, 'counters/updates': 4409}
skipping logging after 141120 examples to avoid logging too frequently
train stats after 141152 examples: {'rewards_train/chosen': '0.15315', 'rewards_train/rejected': '0.083671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069474', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-147.36', 'loss/train': '0.67243', 'examples_per_second': '31.294', 'grad_norm': '178', 'counters/examples': 141152, 'counters/updates': 4411}
train stats after 141184 examples: {'rewards_train/chosen': '0.047398', 'rewards_train/rejected': '0.048836', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.001438', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-132.52', 'loss/train': '0.70085', 'examples_per_second': '31.609', 'grad_norm': '83', 'counters/examples': 141184, 'counters/updates': 4412}
train stats after 141216 examples: {'rewards_train/chosen': '0.04213', 'rewards_train/rejected': '0.13725', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.095121', 'logps_train/rejected': '-133', 'logps_train/chosen': '-174.9', 'loss/train': '0.76203', 'examples_per_second': '31.495', 'grad_norm': '136', 'counters/examples': 141216, 'counters/updates': 4413}
train stats after 141248 examples: {'rewards_train/chosen': '0.1022', 'rewards_train/rejected': '-0.069695', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17189', 'logps_train/rejected': '-114.42', 'logps_train/chosen': '-150.13', 'loss/train': '0.62296', 'examples_per_second': '31.308', 'grad_norm': '82.5', 'counters/examples': 141248, 'counters/updates': 4414}
train stats after 141280 examples: {'rewards_train/chosen': '0.0068144', 'rewards_train/rejected': '0.072095', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.065281', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-127.36', 'loss/train': '0.74226', 'examples_per_second': '31.135', 'grad_norm': '57.25', 'counters/examples': 141280, 'counters/updates': 4415}
train stats after 141312 examples: {'rewards_train/chosen': '0.17403', 'rewards_train/rejected': '0.082665', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091366', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-174.01', 'loss/train': '0.71978', 'examples_per_second': '31.402', 'grad_norm': '72.5', 'counters/examples': 141312, 'counters/updates': 4416}
train stats after 141344 examples: {'rewards_train/chosen': '0.097612', 'rewards_train/rejected': '0.033262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064351', 'logps_train/rejected': '-114.21', 'logps_train/chosen': '-143.69', 'loss/train': '0.68414', 'examples_per_second': '31.248', 'grad_norm': '65', 'counters/examples': 141344, 'counters/updates': 4417}
train stats after 141376 examples: {'rewards_train/chosen': '0.079978', 'rewards_train/rejected': '0.0019622', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078016', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-133.59', 'loss/train': '0.65966', 'examples_per_second': '30.165', 'grad_norm': '58.25', 'counters/examples': 141376, 'counters/updates': 4418}
train stats after 141408 examples: {'rewards_train/chosen': '0.093268', 'rewards_train/rejected': '0.062704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030564', 'logps_train/rejected': '-126.88', 'logps_train/chosen': '-157.93', 'loss/train': '0.69159', 'examples_per_second': '31.551', 'grad_norm': '106', 'counters/examples': 141408, 'counters/updates': 4419}
train stats after 141440 examples: {'rewards_train/chosen': '0.062522', 'rewards_train/rejected': '0.00047295', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062049', 'logps_train/rejected': '-131.31', 'logps_train/chosen': '-174.43', 'loss/train': '0.67135', 'examples_per_second': '32.642', 'grad_norm': '145', 'counters/examples': 141440, 'counters/updates': 4420}
train stats after 141472 examples: {'rewards_train/chosen': '0.058008', 'rewards_train/rejected': '0.0070024', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051006', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-129.29', 'loss/train': '0.68445', 'examples_per_second': '31.203', 'grad_norm': '85.5', 'counters/examples': 141472, 'counters/updates': 4421}
train stats after 141504 examples: {'rewards_train/chosen': '0.12173', 'rewards_train/rejected': '0.11808', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0036427', 'logps_train/rejected': '-151.91', 'logps_train/chosen': '-143.14', 'loss/train': '0.70097', 'examples_per_second': '31.472', 'grad_norm': '70', 'counters/examples': 141504, 'counters/updates': 4422}
train stats after 141536 examples: {'rewards_train/chosen': '0.047975', 'rewards_train/rejected': '0.00070238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047273', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-137.43', 'loss/train': '0.68508', 'examples_per_second': '32.093', 'grad_norm': '73', 'counters/examples': 141536, 'counters/updates': 4423}
skipping logging after 141568 examples to avoid logging too frequently
train stats after 141600 examples: {'rewards_train/chosen': '0.16899', 'rewards_train/rejected': '0.03446', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13452', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-157.24', 'loss/train': '0.63701', 'examples_per_second': '30.674', 'grad_norm': '91.5', 'counters/examples': 141600, 'counters/updates': 4425}
skipping logging after 141632 examples to avoid logging too frequently
train stats after 141664 examples: {'rewards_train/chosen': '0.024148', 'rewards_train/rejected': '0.0063538', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017795', 'logps_train/rejected': '-129.96', 'logps_train/chosen': '-125.95', 'loss/train': '0.69451', 'examples_per_second': '33.644', 'grad_norm': '113', 'counters/examples': 141664, 'counters/updates': 4427}
train stats after 141696 examples: {'rewards_train/chosen': '0.19205', 'rewards_train/rejected': '0.099253', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092796', 'logps_train/rejected': '-146.66', 'logps_train/chosen': '-131.06', 'loss/train': '0.66535', 'examples_per_second': '32.467', 'grad_norm': '128', 'counters/examples': 141696, 'counters/updates': 4428}
train stats after 141728 examples: {'rewards_train/chosen': '0.071378', 'rewards_train/rejected': '0.061704', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096743', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-171.07', 'loss/train': '0.69558', 'examples_per_second': '32.165', 'grad_norm': '80.5', 'counters/examples': 141728, 'counters/updates': 4429}
train stats after 141760 examples: {'rewards_train/chosen': '-0.012721', 'rewards_train/rejected': '0.046444', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.059165', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-147.8', 'loss/train': '0.73267', 'examples_per_second': '31.694', 'grad_norm': '73.5', 'counters/examples': 141760, 'counters/updates': 4430}
train stats after 141792 examples: {'rewards_train/chosen': '0.07916', 'rewards_train/rejected': '0.10204', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02288', 'logps_train/rejected': '-142.49', 'logps_train/chosen': '-144.81', 'loss/train': '0.71549', 'examples_per_second': '31.45', 'grad_norm': '93.5', 'counters/examples': 141792, 'counters/updates': 4431}
skipping logging after 141824 examples to avoid logging too frequently
train stats after 141856 examples: {'rewards_train/chosen': '-0.027542', 'rewards_train/rejected': '0.11099', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.13853', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-151.97', 'loss/train': '0.81018', 'examples_per_second': '31.469', 'grad_norm': '120', 'counters/examples': 141856, 'counters/updates': 4433}
train stats after 141888 examples: {'rewards_train/chosen': '0.15486', 'rewards_train/rejected': '0.10327', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051586', 'logps_train/rejected': '-135.52', 'logps_train/chosen': '-199.34', 'loss/train': '0.72004', 'examples_per_second': '31.507', 'grad_norm': '592', 'counters/examples': 141888, 'counters/updates': 4434}
train stats after 141920 examples: {'rewards_train/chosen': '0.00025315', 'rewards_train/rejected': '0.080054', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0798', 'logps_train/rejected': '-167.45', 'logps_train/chosen': '-135.22', 'loss/train': '0.74645', 'examples_per_second': '30.171', 'grad_norm': '77.5', 'counters/examples': 141920, 'counters/updates': 4435}
train stats after 141952 examples: {'rewards_train/chosen': '0.050199', 'rewards_train/rejected': '0.095202', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045003', 'logps_train/rejected': '-116.79', 'logps_train/chosen': '-116.59', 'loss/train': '0.72386', 'examples_per_second': '31.657', 'grad_norm': '79', 'counters/examples': 141952, 'counters/updates': 4436}
train stats after 141984 examples: {'rewards_train/chosen': '0.083876', 'rewards_train/rejected': '0.086609', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0027337', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-171.16', 'loss/train': '0.7112', 'examples_per_second': '31.581', 'grad_norm': '156', 'counters/examples': 141984, 'counters/updates': 4437}
train stats after 142016 examples: {'rewards_train/chosen': '0.16633', 'rewards_train/rejected': '0.00558', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16075', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-159.77', 'loss/train': '0.62945', 'examples_per_second': '31.287', 'grad_norm': '97.5', 'counters/examples': 142016, 'counters/updates': 4438}
skipping logging after 142048 examples to avoid logging too frequently
train stats after 142080 examples: {'rewards_train/chosen': '0.03812', 'rewards_train/rejected': '0.067828', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029708', 'logps_train/rejected': '-133.59', 'logps_train/chosen': '-116.05', 'loss/train': '0.72578', 'examples_per_second': '31.364', 'grad_norm': '72.5', 'counters/examples': 142080, 'counters/updates': 4440}
train stats after 142112 examples: {'rewards_train/chosen': '0.070558', 'rewards_train/rejected': '0.093668', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023109', 'logps_train/rejected': '-166.44', 'logps_train/chosen': '-182.21', 'loss/train': '0.73228', 'examples_per_second': '32.05', 'grad_norm': '113', 'counters/examples': 142112, 'counters/updates': 4441}
skipping logging after 142144 examples to avoid logging too frequently
train stats after 142176 examples: {'rewards_train/chosen': '0.14662', 'rewards_train/rejected': '0.088134', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058487', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-126.44', 'loss/train': '0.6782', 'examples_per_second': '33.949', 'grad_norm': '106.5', 'counters/examples': 142176, 'counters/updates': 4443}
train stats after 142208 examples: {'rewards_train/chosen': '0.068071', 'rewards_train/rejected': '-0.0061547', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074226', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-171.19', 'loss/train': '0.67892', 'examples_per_second': '30.573', 'grad_norm': '100', 'counters/examples': 142208, 'counters/updates': 4444}
train stats after 142240 examples: {'rewards_train/chosen': '0.070594', 'rewards_train/rejected': '-0.063334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13393', 'logps_train/rejected': '-97.286', 'logps_train/chosen': '-166.1', 'loss/train': '0.64567', 'examples_per_second': '31.077', 'grad_norm': '142', 'counters/examples': 142240, 'counters/updates': 4445}
skipping logging after 142272 examples to avoid logging too frequently
train stats after 142304 examples: {'rewards_train/chosen': '0.24011', 'rewards_train/rejected': '0.10884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13127', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-171.75', 'loss/train': '0.65182', 'examples_per_second': '31.307', 'grad_norm': '140', 'counters/examples': 142304, 'counters/updates': 4447}
skipping logging after 142336 examples to avoid logging too frequently
train stats after 142368 examples: {'rewards_train/chosen': '0.060329', 'rewards_train/rejected': '0.033638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026691', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-113.78', 'loss/train': '0.68981', 'examples_per_second': '30.84', 'grad_norm': '60', 'counters/examples': 142368, 'counters/updates': 4449}
train stats after 142400 examples: {'rewards_train/chosen': '0.15819', 'rewards_train/rejected': '0.14896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0092294', 'logps_train/rejected': '-165.68', 'logps_train/chosen': '-170.75', 'loss/train': '0.70259', 'examples_per_second': '32.05', 'grad_norm': '152', 'counters/examples': 142400, 'counters/updates': 4450}
train stats after 142432 examples: {'rewards_train/chosen': '0.064302', 'rewards_train/rejected': '0.087677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.023375', 'logps_train/rejected': '-142.45', 'logps_train/chosen': '-134.8', 'loss/train': '0.7182', 'examples_per_second': '31.68', 'grad_norm': '113.5', 'counters/examples': 142432, 'counters/updates': 4451}
train stats after 142464 examples: {'rewards_train/chosen': '0.20206', 'rewards_train/rejected': '0.031091', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17097', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-142.06', 'loss/train': '0.64213', 'examples_per_second': '32.003', 'grad_norm': '56', 'counters/examples': 142464, 'counters/updates': 4452}
train stats after 142496 examples: {'rewards_train/chosen': '0.092695', 'rewards_train/rejected': '-0.018028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11072', 'logps_train/rejected': '-146.81', 'logps_train/chosen': '-165.22', 'loss/train': '0.65991', 'examples_per_second': '32.574', 'grad_norm': '97.5', 'counters/examples': 142496, 'counters/updates': 4453}
train stats after 142528 examples: {'rewards_train/chosen': '0.11548', 'rewards_train/rejected': '-0.034265', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14974', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-153.07', 'loss/train': '0.64123', 'examples_per_second': '32.972', 'grad_norm': '83', 'counters/examples': 142528, 'counters/updates': 4454}
train stats after 142560 examples: {'rewards_train/chosen': '0.10792', 'rewards_train/rejected': '-0.012389', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12031', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-152.85', 'loss/train': '0.64999', 'examples_per_second': '31.381', 'grad_norm': '50.25', 'counters/examples': 142560, 'counters/updates': 4455}
train stats after 142592 examples: {'rewards_train/chosen': '0.23971', 'rewards_train/rejected': '0.066782', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17292', 'logps_train/rejected': '-149.74', 'logps_train/chosen': '-189.79', 'loss/train': '0.63702', 'examples_per_second': '30.519', 'grad_norm': '112', 'counters/examples': 142592, 'counters/updates': 4456}
train stats after 142624 examples: {'rewards_train/chosen': '0.20171', 'rewards_train/rejected': '0.16342', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03829', 'logps_train/rejected': '-148.08', 'logps_train/chosen': '-192.59', 'loss/train': '0.70405', 'examples_per_second': '31.483', 'grad_norm': '90.5', 'counters/examples': 142624, 'counters/updates': 4457}
train stats after 142656 examples: {'rewards_train/chosen': '0.1512', 'rewards_train/rejected': '0.025551', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12565', 'logps_train/rejected': '-90.094', 'logps_train/chosen': '-149.26', 'loss/train': '0.65799', 'examples_per_second': '31.429', 'grad_norm': '63.75', 'counters/examples': 142656, 'counters/updates': 4458}
train stats after 142688 examples: {'rewards_train/chosen': '0.10468', 'rewards_train/rejected': '0.051242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053442', 'logps_train/rejected': '-126.43', 'logps_train/chosen': '-164.63', 'loss/train': '0.68558', 'examples_per_second': '31.164', 'grad_norm': '70', 'counters/examples': 142688, 'counters/updates': 4459}
skipping logging after 142720 examples to avoid logging too frequently
train stats after 142752 examples: {'rewards_train/chosen': '0.069868', 'rewards_train/rejected': '-0.015003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084871', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-120.3', 'loss/train': '0.65754', 'examples_per_second': '33.171', 'grad_norm': '176', 'counters/examples': 142752, 'counters/updates': 4461}
train stats after 142784 examples: {'rewards_train/chosen': '0.1203', 'rewards_train/rejected': '0.065753', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054545', 'logps_train/rejected': '-160.03', 'logps_train/chosen': '-130.9', 'loss/train': '0.6813', 'examples_per_second': '32.553', 'grad_norm': '81.5', 'counters/examples': 142784, 'counters/updates': 4462}
train stats after 142816 examples: {'rewards_train/chosen': '0.10589', 'rewards_train/rejected': '0.084451', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021436', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-139.52', 'loss/train': '0.69232', 'examples_per_second': '31.3', 'grad_norm': '60', 'counters/examples': 142816, 'counters/updates': 4463}
train stats after 142848 examples: {'rewards_train/chosen': '0.044073', 'rewards_train/rejected': '0.056997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012924', 'logps_train/rejected': '-116.72', 'logps_train/chosen': '-141.4', 'loss/train': '0.71879', 'examples_per_second': '31.574', 'grad_norm': '316', 'counters/examples': 142848, 'counters/updates': 4464}
train stats after 142880 examples: {'rewards_train/chosen': '0.068223', 'rewards_train/rejected': '-0.015963', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084186', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-137.76', 'loss/train': '0.6581', 'examples_per_second': '30.587', 'grad_norm': '50', 'counters/examples': 142880, 'counters/updates': 4465}
train stats after 142912 examples: {'rewards_train/chosen': '0.08088', 'rewards_train/rejected': '0.064457', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016422', 'logps_train/rejected': '-115.21', 'logps_train/chosen': '-139.51', 'loss/train': '0.69414', 'examples_per_second': '30.111', 'grad_norm': '66.5', 'counters/examples': 142912, 'counters/updates': 4466}
train stats after 142944 examples: {'rewards_train/chosen': '0.11737', 'rewards_train/rejected': '0.039176', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078199', 'logps_train/rejected': '-149.83', 'logps_train/chosen': '-159.89', 'loss/train': '0.67023', 'examples_per_second': '31.198', 'grad_norm': '119.5', 'counters/examples': 142944, 'counters/updates': 4467}
train stats after 142976 examples: {'rewards_train/chosen': '0.10739', 'rewards_train/rejected': '0.087682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.019705', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-159.88', 'loss/train': '0.69532', 'examples_per_second': '31.534', 'grad_norm': '100.5', 'counters/examples': 142976, 'counters/updates': 4468}
train stats after 143008 examples: {'rewards_train/chosen': '0.085571', 'rewards_train/rejected': '0.090453', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0048826', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-163.89', 'loss/train': '0.71345', 'examples_per_second': '32.482', 'grad_norm': '76.5', 'counters/examples': 143008, 'counters/updates': 4469}
train stats after 143040 examples: {'rewards_train/chosen': '0.20958', 'rewards_train/rejected': '0.028359', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18122', 'logps_train/rejected': '-103.4', 'logps_train/chosen': '-191.6', 'loss/train': '0.62001', 'examples_per_second': '30.026', 'grad_norm': '68', 'counters/examples': 143040, 'counters/updates': 4470}
skipping logging after 143072 examples to avoid logging too frequently
train stats after 143104 examples: {'rewards_train/chosen': '0.026988', 'rewards_train/rejected': '-0.024734', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051722', 'logps_train/rejected': '-195.5', 'logps_train/chosen': '-159.56', 'loss/train': '0.68475', 'examples_per_second': '32.451', 'grad_norm': '109', 'counters/examples': 143104, 'counters/updates': 4472}
train stats after 143136 examples: {'rewards_train/chosen': '0.048358', 'rewards_train/rejected': '0.03264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015718', 'logps_train/rejected': '-145.88', 'logps_train/chosen': '-158.36', 'loss/train': '0.6925', 'examples_per_second': '31.544', 'grad_norm': '74.5', 'counters/examples': 143136, 'counters/updates': 4473}
train stats after 143168 examples: {'rewards_train/chosen': '0.12819', 'rewards_train/rejected': '0.075651', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052535', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-141.87', 'loss/train': '0.67933', 'examples_per_second': '30.133', 'grad_norm': '286', 'counters/examples': 143168, 'counters/updates': 4474}
train stats after 143200 examples: {'rewards_train/chosen': '0.089309', 'rewards_train/rejected': '0.02483', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064479', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-142.03', 'loss/train': '0.66874', 'examples_per_second': '32.327', 'grad_norm': '53.5', 'counters/examples': 143200, 'counters/updates': 4475}
train stats after 143232 examples: {'rewards_train/chosen': '0.032759', 'rewards_train/rejected': '-0.003685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036444', 'logps_train/rejected': '-140.32', 'logps_train/chosen': '-148.44', 'loss/train': '0.68553', 'examples_per_second': '31.519', 'grad_norm': '132', 'counters/examples': 143232, 'counters/updates': 4476}
train stats after 143264 examples: {'rewards_train/chosen': '0.1212', 'rewards_train/rejected': '-0.092319', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21352', 'logps_train/rejected': '-147.79', 'logps_train/chosen': '-173', 'loss/train': '0.61284', 'examples_per_second': '31.536', 'grad_norm': '68', 'counters/examples': 143264, 'counters/updates': 4477}
train stats after 143296 examples: {'rewards_train/chosen': '0.12221', 'rewards_train/rejected': '0.064427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.057781', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-153.67', 'loss/train': '0.68325', 'examples_per_second': '30.051', 'grad_norm': '98', 'counters/examples': 143296, 'counters/updates': 4478}
train stats after 143328 examples: {'rewards_train/chosen': '0.022049', 'rewards_train/rejected': '0.018884', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.003165', 'logps_train/rejected': '-130.02', 'logps_train/chosen': '-163.95', 'loss/train': '0.69711', 'examples_per_second': '31.581', 'grad_norm': '122.5', 'counters/examples': 143328, 'counters/updates': 4479}
train stats after 143360 examples: {'rewards_train/chosen': '0.066636', 'rewards_train/rejected': '0.067412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00077607', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-191.03', 'loss/train': '0.70707', 'examples_per_second': '30.739', 'grad_norm': '104', 'counters/examples': 143360, 'counters/updates': 4480}
train stats after 143392 examples: {'rewards_train/chosen': '0.14968', 'rewards_train/rejected': '0.05762', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092056', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-145.78', 'loss/train': '0.66279', 'examples_per_second': '31.393', 'grad_norm': '70', 'counters/examples': 143392, 'counters/updates': 4481}
train stats after 143424 examples: {'rewards_train/chosen': '0.073649', 'rewards_train/rejected': '0.10835', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034696', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-162.06', 'loss/train': '0.72156', 'examples_per_second': '31.535', 'grad_norm': '63.75', 'counters/examples': 143424, 'counters/updates': 4482}
train stats after 143456 examples: {'rewards_train/chosen': '0.046239', 'rewards_train/rejected': '0.078206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.031967', 'logps_train/rejected': '-154.71', 'logps_train/chosen': '-161.47', 'loss/train': '0.72031', 'examples_per_second': '30.099', 'grad_norm': '98.5', 'counters/examples': 143456, 'counters/updates': 4483}
skipping logging after 143488 examples to avoid logging too frequently
train stats after 143520 examples: {'rewards_train/chosen': '0.035696', 'rewards_train/rejected': '0.076285', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040589', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-154.68', 'loss/train': '0.72983', 'examples_per_second': '34.371', 'grad_norm': '163', 'counters/examples': 143520, 'counters/updates': 4485}
train stats after 143552 examples: {'rewards_train/chosen': '0.14097', 'rewards_train/rejected': '0.041948', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099019', 'logps_train/rejected': '-173.05', 'logps_train/chosen': '-187.06', 'loss/train': '0.67223', 'examples_per_second': '30.922', 'grad_norm': '90.5', 'counters/examples': 143552, 'counters/updates': 4486}
train stats after 143584 examples: {'rewards_train/chosen': '0.071613', 'rewards_train/rejected': '0.083428', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011815', 'logps_train/rejected': '-106.27', 'logps_train/chosen': '-142.91', 'loss/train': '0.71367', 'examples_per_second': '30.368', 'grad_norm': '116.5', 'counters/examples': 143584, 'counters/updates': 4487}
train stats after 143616 examples: {'rewards_train/chosen': '0.10793', 'rewards_train/rejected': '-0.0022052', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11013', 'logps_train/rejected': '-106.18', 'logps_train/chosen': '-120.32', 'loss/train': '0.65968', 'examples_per_second': '31.772', 'grad_norm': '51.75', 'counters/examples': 143616, 'counters/updates': 4488}
train stats after 143648 examples: {'rewards_train/chosen': '0.15587', 'rewards_train/rejected': '0.094629', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061239', 'logps_train/rejected': '-147.34', 'logps_train/chosen': '-180.04', 'loss/train': '0.67916', 'examples_per_second': '23.584', 'grad_norm': '65', 'counters/examples': 143648, 'counters/updates': 4489}
train stats after 143680 examples: {'rewards_train/chosen': '0.072809', 'rewards_train/rejected': '0.070154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0026551', 'logps_train/rejected': '-103.6', 'logps_train/chosen': '-137.41', 'loss/train': '0.69786', 'examples_per_second': '31.719', 'grad_norm': '140', 'counters/examples': 143680, 'counters/updates': 4490}
train stats after 143712 examples: {'rewards_train/chosen': '0.071388', 'rewards_train/rejected': '0.04879', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022598', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-147.21', 'loss/train': '0.69257', 'examples_per_second': '32.689', 'grad_norm': '84.5', 'counters/examples': 143712, 'counters/updates': 4491}
train stats after 143744 examples: {'rewards_train/chosen': '0.062918', 'rewards_train/rejected': '0.085711', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.022793', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-168.76', 'loss/train': '0.72193', 'examples_per_second': '31.124', 'grad_norm': '108', 'counters/examples': 143744, 'counters/updates': 4492}
train stats after 143776 examples: {'rewards_train/chosen': '0.05806', 'rewards_train/rejected': '0.14869', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.090632', 'logps_train/rejected': '-164.16', 'logps_train/chosen': '-152.6', 'loss/train': '0.76112', 'examples_per_second': '31.304', 'grad_norm': '91.5', 'counters/examples': 143776, 'counters/updates': 4493}
skipping logging after 143808 examples to avoid logging too frequently
train stats after 143840 examples: {'rewards_train/chosen': '0.097661', 'rewards_train/rejected': '0.045132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05253', 'logps_train/rejected': '-103.64', 'logps_train/chosen': '-168.83', 'loss/train': '0.67716', 'examples_per_second': '32.252', 'grad_norm': '96.5', 'counters/examples': 143840, 'counters/updates': 4495}
train stats after 143872 examples: {'rewards_train/chosen': '0.043953', 'rewards_train/rejected': '0.060142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016189', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-154.94', 'loss/train': '0.71765', 'examples_per_second': '30.888', 'grad_norm': '63', 'counters/examples': 143872, 'counters/updates': 4496}
skipping logging after 143904 examples to avoid logging too frequently
train stats after 143936 examples: {'rewards_train/chosen': '0.074131', 'rewards_train/rejected': '0.012236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061895', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-147.32', 'loss/train': '0.67485', 'examples_per_second': '30.37', 'grad_norm': '138', 'counters/examples': 143936, 'counters/updates': 4498}
train stats after 143968 examples: {'rewards_train/chosen': '0.071048', 'rewards_train/rejected': '0.10887', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037817', 'logps_train/rejected': '-151.84', 'logps_train/chosen': '-148.91', 'loss/train': '0.72209', 'examples_per_second': '31.424', 'grad_norm': '96', 'counters/examples': 143968, 'counters/updates': 4499}
skipping logging after 144000 examples to avoid logging too frequently
Running evaluation after 144000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.21it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.89it/s]
eval after 144000: {'rewards_eval/chosen': '0.11068', 'rewards_eval/rejected': '0.042648', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.068033', 'logps_eval/rejected': '-127.72', 'logps_eval/chosen': '-149.9', 'loss/eval': '0.67414'}
skipping save for non epoch
train stats after 144032 examples: {'rewards_train/chosen': '0.14726', 'rewards_train/rejected': '0.041123', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10613', 'logps_train/rejected': '-158.39', 'logps_train/chosen': '-166.4', 'loss/train': '0.66392', 'examples_per_second': '31.444', 'grad_norm': '122.5', 'counters/examples': 144032, 'counters/updates': 4501}
skipping logging after 144064 examples to avoid logging too frequently
train stats after 144096 examples: {'rewards_train/chosen': '0.11174', 'rewards_train/rejected': '0.02072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091021', 'logps_train/rejected': '-127.45', 'logps_train/chosen': '-154.66', 'loss/train': '0.66452', 'examples_per_second': '31.848', 'grad_norm': '104.5', 'counters/examples': 144096, 'counters/updates': 4503}
skipping logging after 144128 examples to avoid logging too frequently
train stats after 144160 examples: {'rewards_train/chosen': '0.086726', 'rewards_train/rejected': '0.082592', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0041339', 'logps_train/rejected': '-147.71', 'logps_train/chosen': '-179.47', 'loss/train': '0.70647', 'examples_per_second': '31.158', 'grad_norm': '83.5', 'counters/examples': 144160, 'counters/updates': 4505}
skipping logging after 144192 examples to avoid logging too frequently
train stats after 144224 examples: {'rewards_train/chosen': '0.12742', 'rewards_train/rejected': '0.070131', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057285', 'logps_train/rejected': '-98.737', 'logps_train/chosen': '-136.31', 'loss/train': '0.67669', 'examples_per_second': '37.582', 'grad_norm': '74', 'counters/examples': 144224, 'counters/updates': 4507}
skipping logging after 144256 examples to avoid logging too frequently
train stats after 144288 examples: {'rewards_train/chosen': '0.08522', 'rewards_train/rejected': '0.1116', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026379', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-163.92', 'loss/train': '0.71809', 'examples_per_second': '36.168', 'grad_norm': '83.5', 'counters/examples': 144288, 'counters/updates': 4509}
train stats after 144320 examples: {'rewards_train/chosen': '0.14596', 'rewards_train/rejected': '0.02076', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1252', 'logps_train/rejected': '-152.76', 'logps_train/chosen': '-176.36', 'loss/train': '0.6497', 'examples_per_second': '30.295', 'grad_norm': '72.5', 'counters/examples': 144320, 'counters/updates': 4510}
train stats after 144352 examples: {'rewards_train/chosen': '0.070722', 'rewards_train/rejected': '0.032472', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03825', 'logps_train/rejected': '-105.15', 'logps_train/chosen': '-173.02', 'loss/train': '0.69152', 'examples_per_second': '31.643', 'grad_norm': '153', 'counters/examples': 144352, 'counters/updates': 4511}
train stats after 144384 examples: {'rewards_train/chosen': '0.10567', 'rewards_train/rejected': '0.063611', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042061', 'logps_train/rejected': '-127.48', 'logps_train/chosen': '-170.51', 'loss/train': '0.68711', 'examples_per_second': '31.5', 'grad_norm': '86.5', 'counters/examples': 144384, 'counters/updates': 4512}
train stats after 144416 examples: {'rewards_train/chosen': '0.074801', 'rewards_train/rejected': '0.093192', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01839', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-162.22', 'loss/train': '0.71292', 'examples_per_second': '30.366', 'grad_norm': '145', 'counters/examples': 144416, 'counters/updates': 4513}
train stats after 144448 examples: {'rewards_train/chosen': '0.12742', 'rewards_train/rejected': '0.063487', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063933', 'logps_train/rejected': '-147.06', 'logps_train/chosen': '-126.64', 'loss/train': '0.68686', 'examples_per_second': '31.268', 'grad_norm': '65.5', 'counters/examples': 144448, 'counters/updates': 4514}
train stats after 144480 examples: {'rewards_train/chosen': '0.078684', 'rewards_train/rejected': '0.095222', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.016537', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-138.46', 'loss/train': '0.70876', 'examples_per_second': '30.442', 'grad_norm': '66.5', 'counters/examples': 144480, 'counters/updates': 4515}
train stats after 144512 examples: {'rewards_train/chosen': '0.093794', 'rewards_train/rejected': '0.0084764', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085318', 'logps_train/rejected': '-146.9', 'logps_train/chosen': '-154.48', 'loss/train': '0.66853', 'examples_per_second': '30.002', 'grad_norm': '90', 'counters/examples': 144512, 'counters/updates': 4516}
train stats after 144544 examples: {'rewards_train/chosen': '0.059406', 'rewards_train/rejected': '0.013119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046286', 'logps_train/rejected': '-150.26', 'logps_train/chosen': '-152.83', 'loss/train': '0.67934', 'examples_per_second': '32.074', 'grad_norm': '274', 'counters/examples': 144544, 'counters/updates': 4517}
train stats after 144576 examples: {'rewards_train/chosen': '0.1569', 'rewards_train/rejected': '0.064934', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091968', 'logps_train/rejected': '-112.34', 'logps_train/chosen': '-168.6', 'loss/train': '0.66925', 'examples_per_second': '32.029', 'grad_norm': '85', 'counters/examples': 144576, 'counters/updates': 4518}
train stats after 144608 examples: {'rewards_train/chosen': '0.070091', 'rewards_train/rejected': '0.03347', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03662', 'logps_train/rejected': '-150.26', 'logps_train/chosen': '-188.72', 'loss/train': '0.69805', 'examples_per_second': '31.283', 'grad_norm': '63.5', 'counters/examples': 144608, 'counters/updates': 4519}
train stats after 144640 examples: {'rewards_train/chosen': '0.09273', 'rewards_train/rejected': '0.028072', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064658', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-136.39', 'loss/train': '0.67115', 'examples_per_second': '31.172', 'grad_norm': '103.5', 'counters/examples': 144640, 'counters/updates': 4520}
skipping logging after 144672 examples to avoid logging too frequently
train stats after 144704 examples: {'rewards_train/chosen': '0.12118', 'rewards_train/rejected': '0.075036', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046145', 'logps_train/rejected': '-147.18', 'logps_train/chosen': '-172.25', 'loss/train': '0.68575', 'examples_per_second': '31.36', 'grad_norm': '117.5', 'counters/examples': 144704, 'counters/updates': 4522}
train stats after 144736 examples: {'rewards_train/chosen': '0.085572', 'rewards_train/rejected': '0.095819', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.010248', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-142.65', 'loss/train': '0.71104', 'examples_per_second': '31.799', 'grad_norm': '72', 'counters/examples': 144736, 'counters/updates': 4523}
train stats after 144768 examples: {'rewards_train/chosen': '0.10497', 'rewards_train/rejected': '0.099643', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0053304', 'logps_train/rejected': '-160.07', 'logps_train/chosen': '-118.54', 'loss/train': '0.70698', 'examples_per_second': '31.495', 'grad_norm': '145', 'counters/examples': 144768, 'counters/updates': 4524}
train stats after 144800 examples: {'rewards_train/chosen': '0.083118', 'rewards_train/rejected': '0.090351', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0072323', 'logps_train/rejected': '-136.32', 'logps_train/chosen': '-137.76', 'loss/train': '0.71352', 'examples_per_second': '31.22', 'grad_norm': '114', 'counters/examples': 144800, 'counters/updates': 4525}
train stats after 144832 examples: {'rewards_train/chosen': '0.13854', 'rewards_train/rejected': '0.043336', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095209', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-196.87', 'loss/train': '0.65878', 'examples_per_second': '30.576', 'grad_norm': '67.5', 'counters/examples': 144832, 'counters/updates': 4526}
train stats after 144864 examples: {'rewards_train/chosen': '0.037691', 'rewards_train/rejected': '0.10139', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.063695', 'logps_train/rejected': '-141.94', 'logps_train/chosen': '-167.39', 'loss/train': '0.73824', 'examples_per_second': '30.111', 'grad_norm': '163', 'counters/examples': 144864, 'counters/updates': 4527}
train stats after 144896 examples: {'rewards_train/chosen': '0.1231', 'rewards_train/rejected': '0.081262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041837', 'logps_train/rejected': '-144.71', 'logps_train/chosen': '-151.16', 'loss/train': '0.69434', 'examples_per_second': '30.042', 'grad_norm': '201', 'counters/examples': 144896, 'counters/updates': 4528}
train stats after 144928 examples: {'rewards_train/chosen': '-0.0083843', 'rewards_train/rejected': '0.017029', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025413', 'logps_train/rejected': '-121.48', 'logps_train/chosen': '-148.74', 'loss/train': '0.71505', 'examples_per_second': '31.372', 'grad_norm': '138', 'counters/examples': 144928, 'counters/updates': 4529}
train stats after 144960 examples: {'rewards_train/chosen': '0.049401', 'rewards_train/rejected': '0.011565', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037836', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-179.15', 'loss/train': '0.69615', 'examples_per_second': '31.501', 'grad_norm': '186', 'counters/examples': 144960, 'counters/updates': 4530}
train stats after 144992 examples: {'rewards_train/chosen': '0.097311', 'rewards_train/rejected': '0.047073', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050238', 'logps_train/rejected': '-119.41', 'logps_train/chosen': '-167.18', 'loss/train': '0.68249', 'examples_per_second': '31.545', 'grad_norm': '124', 'counters/examples': 144992, 'counters/updates': 4531}
train stats after 145024 examples: {'rewards_train/chosen': '0.12272', 'rewards_train/rejected': '0.084063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038655', 'logps_train/rejected': '-140.3', 'logps_train/chosen': '-152.6', 'loss/train': '0.69227', 'examples_per_second': '31.538', 'grad_norm': '78.5', 'counters/examples': 145024, 'counters/updates': 4532}
train stats after 145056 examples: {'rewards_train/chosen': '0.018353', 'rewards_train/rejected': '0.018052', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.00030081', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-138.8', 'loss/train': '0.71765', 'examples_per_second': '30.538', 'grad_norm': '94.5', 'counters/examples': 145056, 'counters/updates': 4533}
skipping logging after 145088 examples to avoid logging too frequently
train stats after 145120 examples: {'rewards_train/chosen': '0.080616', 'rewards_train/rejected': '0.049642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030975', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-136.46', 'loss/train': '0.69183', 'examples_per_second': '31.483', 'grad_norm': '132', 'counters/examples': 145120, 'counters/updates': 4535}
train stats after 145152 examples: {'rewards_train/chosen': '0.12199', 'rewards_train/rejected': '0.039626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082368', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-159.88', 'loss/train': '0.66748', 'examples_per_second': '32.838', 'grad_norm': '88', 'counters/examples': 145152, 'counters/updates': 4536}
skipping logging after 145184 examples to avoid logging too frequently
train stats after 145216 examples: {'rewards_train/chosen': '0.13207', 'rewards_train/rejected': '0.11163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02044', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-154.5', 'loss/train': '0.69731', 'examples_per_second': '33.548', 'grad_norm': '82.5', 'counters/examples': 145216, 'counters/updates': 4538}
skipping logging after 145248 examples to avoid logging too frequently
train stats after 145280 examples: {'rewards_train/chosen': '0.018044', 'rewards_train/rejected': '0.052209', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034165', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-141.81', 'loss/train': '0.72661', 'examples_per_second': '32.918', 'grad_norm': '86', 'counters/examples': 145280, 'counters/updates': 4540}
train stats after 145312 examples: {'rewards_train/chosen': '0.19316', 'rewards_train/rejected': '0.016911', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17625', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-153.06', 'loss/train': '0.6241', 'examples_per_second': '31.394', 'grad_norm': '68.5', 'counters/examples': 145312, 'counters/updates': 4541}
skipping logging after 145344 examples to avoid logging too frequently
train stats after 145376 examples: {'rewards_train/chosen': '0.078048', 'rewards_train/rejected': '0.020861', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057187', 'logps_train/rejected': '-153.86', 'logps_train/chosen': '-133.62', 'loss/train': '0.67505', 'examples_per_second': '31.706', 'grad_norm': '54.75', 'counters/examples': 145376, 'counters/updates': 4543}
train stats after 145408 examples: {'rewards_train/chosen': '0.15279', 'rewards_train/rejected': '0.030828', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12197', 'logps_train/rejected': '-152.36', 'logps_train/chosen': '-178.19', 'loss/train': '0.64803', 'examples_per_second': '31.507', 'grad_norm': '129', 'counters/examples': 145408, 'counters/updates': 4544}
train stats after 145440 examples: {'rewards_train/chosen': '0.13378', 'rewards_train/rejected': '0.077224', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.056556', 'logps_train/rejected': '-147.29', 'logps_train/chosen': '-143.74', 'loss/train': '0.68384', 'examples_per_second': '32.917', 'grad_norm': '140', 'counters/examples': 145440, 'counters/updates': 4545}
train stats after 145472 examples: {'rewards_train/chosen': '0.0081211', 'rewards_train/rejected': '0.051273', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.043152', 'logps_train/rejected': '-125.1', 'logps_train/chosen': '-149.46', 'loss/train': '0.73116', 'examples_per_second': '30.875', 'grad_norm': '70.5', 'counters/examples': 145472, 'counters/updates': 4546}
train stats after 145504 examples: {'rewards_train/chosen': '0.1151', 'rewards_train/rejected': '0.00024856', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11486', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-168.19', 'loss/train': '0.65448', 'examples_per_second': '30.699', 'grad_norm': '68', 'counters/examples': 145504, 'counters/updates': 4547}
train stats after 145536 examples: {'rewards_train/chosen': '0.060756', 'rewards_train/rejected': '0.074248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.013492', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-174.69', 'loss/train': '0.71445', 'examples_per_second': '31.432', 'grad_norm': '81', 'counters/examples': 145536, 'counters/updates': 4548}
train stats after 145568 examples: {'rewards_train/chosen': '0.059436', 'rewards_train/rejected': '0.039735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019701', 'logps_train/rejected': '-119.47', 'logps_train/chosen': '-143.93', 'loss/train': '0.68945', 'examples_per_second': '31.678', 'grad_norm': '55', 'counters/examples': 145568, 'counters/updates': 4549}
train stats after 145600 examples: {'rewards_train/chosen': '0.015722', 'rewards_train/rejected': '0.10923', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.093504', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-149.04', 'loss/train': '0.75467', 'examples_per_second': '31.25', 'grad_norm': '102.5', 'counters/examples': 145600, 'counters/updates': 4550}
train stats after 145632 examples: {'rewards_train/chosen': '0.079', 'rewards_train/rejected': '0.11749', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038495', 'logps_train/rejected': '-159.35', 'logps_train/chosen': '-139.94', 'loss/train': '0.73704', 'examples_per_second': '31.524', 'grad_norm': '87.5', 'counters/examples': 145632, 'counters/updates': 4551}
train stats after 145664 examples: {'rewards_train/chosen': '0.041551', 'rewards_train/rejected': '0.05479', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013239', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-135.14', 'loss/train': '0.71158', 'examples_per_second': '33', 'grad_norm': '78', 'counters/examples': 145664, 'counters/updates': 4552}
train stats after 145696 examples: {'rewards_train/chosen': '0.062093', 'rewards_train/rejected': '0.038252', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023841', 'logps_train/rejected': '-142.51', 'logps_train/chosen': '-149.45', 'loss/train': '0.69562', 'examples_per_second': '32.421', 'grad_norm': '62.75', 'counters/examples': 145696, 'counters/updates': 4553}
train stats after 145728 examples: {'rewards_train/chosen': '0.12967', 'rewards_train/rejected': '0.040049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089624', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-154.55', 'loss/train': '0.68281', 'examples_per_second': '30.22', 'grad_norm': '101.5', 'counters/examples': 145728, 'counters/updates': 4554}
skipping logging after 145760 examples to avoid logging too frequently
train stats after 145792 examples: {'rewards_train/chosen': '0.13947', 'rewards_train/rejected': '0.044678', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094789', 'logps_train/rejected': '-155.04', 'logps_train/chosen': '-153.59', 'loss/train': '0.65753', 'examples_per_second': '32.997', 'grad_norm': '70.5', 'counters/examples': 145792, 'counters/updates': 4556}
train stats after 145824 examples: {'rewards_train/chosen': '0.037394', 'rewards_train/rejected': '0.0282', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0091941', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-128.74', 'loss/train': '0.70069', 'examples_per_second': '31.532', 'grad_norm': '80', 'counters/examples': 145824, 'counters/updates': 4557}
train stats after 145856 examples: {'rewards_train/chosen': '0.089472', 'rewards_train/rejected': '0.12025', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.03078', 'logps_train/rejected': '-153.95', 'logps_train/chosen': '-121.1', 'loss/train': '0.72126', 'examples_per_second': '30.523', 'grad_norm': '79', 'counters/examples': 145856, 'counters/updates': 4558}
train stats after 145888 examples: {'rewards_train/chosen': '0.1034', 'rewards_train/rejected': '0.03357', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.069835', 'logps_train/rejected': '-95.828', 'logps_train/chosen': '-151.07', 'loss/train': '0.67342', 'examples_per_second': '31.031', 'grad_norm': '118.5', 'counters/examples': 145888, 'counters/updates': 4559}
train stats after 145920 examples: {'rewards_train/chosen': '0.097921', 'rewards_train/rejected': '0.03187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066051', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-153.57', 'loss/train': '0.68005', 'examples_per_second': '31.558', 'grad_norm': '72.5', 'counters/examples': 145920, 'counters/updates': 4560}
train stats after 145952 examples: {'rewards_train/chosen': '0.1383', 'rewards_train/rejected': '0.10441', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03389', 'logps_train/rejected': '-150.73', 'logps_train/chosen': '-166.38', 'loss/train': '0.69431', 'examples_per_second': '30.566', 'grad_norm': '166', 'counters/examples': 145952, 'counters/updates': 4561}
skipping logging after 145984 examples to avoid logging too frequently
train stats after 146016 examples: {'rewards_train/chosen': '0.086424', 'rewards_train/rejected': '-0.021628', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10805', 'logps_train/rejected': '-85.439', 'logps_train/chosen': '-137.07', 'loss/train': '0.65424', 'examples_per_second': '34.167', 'grad_norm': '79', 'counters/examples': 146016, 'counters/updates': 4563}
train stats after 146048 examples: {'rewards_train/chosen': '0.036977', 'rewards_train/rejected': '0.012091', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024886', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-166.91', 'loss/train': '0.69704', 'examples_per_second': '30.016', 'grad_norm': '74', 'counters/examples': 146048, 'counters/updates': 4564}
train stats after 146080 examples: {'rewards_train/chosen': '0.035525', 'rewards_train/rejected': '0.035356', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00016845', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-93.759', 'loss/train': '0.70355', 'examples_per_second': '31.902', 'grad_norm': '55.75', 'counters/examples': 146080, 'counters/updates': 4565}
train stats after 146112 examples: {'rewards_train/chosen': '0.12957', 'rewards_train/rejected': '0.077807', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051767', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-164.24', 'loss/train': '0.68389', 'examples_per_second': '32.326', 'grad_norm': '60.5', 'counters/examples': 146112, 'counters/updates': 4566}
train stats after 146144 examples: {'rewards_train/chosen': '0.078205', 'rewards_train/rejected': '0.02161', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056595', 'logps_train/rejected': '-144.96', 'logps_train/chosen': '-204.24', 'loss/train': '0.67474', 'examples_per_second': '31.538', 'grad_norm': '151', 'counters/examples': 146144, 'counters/updates': 4567}
train stats after 146176 examples: {'rewards_train/chosen': '0.071895', 'rewards_train/rejected': '-0.09239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16429', 'logps_train/rejected': '-146.95', 'logps_train/chosen': '-172.74', 'loss/train': '0.63189', 'examples_per_second': '31.52', 'grad_norm': '62.5', 'counters/examples': 146176, 'counters/updates': 4568}
train stats after 146208 examples: {'rewards_train/chosen': '0.047229', 'rewards_train/rejected': '-0.056674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1039', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-146.6', 'loss/train': '0.65419', 'examples_per_second': '31.196', 'grad_norm': '107', 'counters/examples': 146208, 'counters/updates': 4569}
skipping logging after 146240 examples to avoid logging too frequently
train stats after 146272 examples: {'rewards_train/chosen': '0.12134', 'rewards_train/rejected': '-0.0067269', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12806', 'logps_train/rejected': '-131.02', 'logps_train/chosen': '-151.95', 'loss/train': '0.64166', 'examples_per_second': '31.512', 'grad_norm': '77', 'counters/examples': 146272, 'counters/updates': 4571}
train stats after 146304 examples: {'rewards_train/chosen': '0.10019', 'rewards_train/rejected': '0.082064', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.018122', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-165.62', 'loss/train': '0.69562', 'examples_per_second': '32.748', 'grad_norm': '82', 'counters/examples': 146304, 'counters/updates': 4572}
skipping logging after 146336 examples to avoid logging too frequently
train stats after 146368 examples: {'rewards_train/chosen': '0.10774', 'rewards_train/rejected': '0.014514', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093228', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-168.6', 'loss/train': '0.65865', 'examples_per_second': '29.865', 'grad_norm': '99.5', 'counters/examples': 146368, 'counters/updates': 4574}
train stats after 146400 examples: {'rewards_train/chosen': '0.15622', 'rewards_train/rejected': '0.097617', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058607', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-118.93', 'loss/train': '0.68027', 'examples_per_second': '31.128', 'grad_norm': '75.5', 'counters/examples': 146400, 'counters/updates': 4575}
skipping logging after 146432 examples to avoid logging too frequently
train stats after 146464 examples: {'rewards_train/chosen': '0.080982', 'rewards_train/rejected': '0.12161', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040631', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-172.99', 'loss/train': '0.72916', 'examples_per_second': '24.99', 'grad_norm': '92.5', 'counters/examples': 146464, 'counters/updates': 4577}
train stats after 146496 examples: {'rewards_train/chosen': '0.084314', 'rewards_train/rejected': '0.11492', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.030611', 'logps_train/rejected': '-156.66', 'logps_train/chosen': '-181.95', 'loss/train': '0.72526', 'examples_per_second': '32.505', 'grad_norm': '134', 'counters/examples': 146496, 'counters/updates': 4578}
train stats after 146528 examples: {'rewards_train/chosen': '0.085748', 'rewards_train/rejected': '-0.018187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10393', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-160.66', 'loss/train': '0.65893', 'examples_per_second': '33.103', 'grad_norm': '94.5', 'counters/examples': 146528, 'counters/updates': 4579}
train stats after 146560 examples: {'rewards_train/chosen': '0.12795', 'rewards_train/rejected': '0.10623', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021721', 'logps_train/rejected': '-125.81', 'logps_train/chosen': '-157.33', 'loss/train': '0.70526', 'examples_per_second': '23.518', 'grad_norm': '115', 'counters/examples': 146560, 'counters/updates': 4580}
train stats after 146592 examples: {'rewards_train/chosen': '0.023756', 'rewards_train/rejected': '-0.0044285', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028184', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-106.16', 'loss/train': '0.6918', 'examples_per_second': '29.969', 'grad_norm': '66', 'counters/examples': 146592, 'counters/updates': 4581}
skipping logging after 146624 examples to avoid logging too frequently
train stats after 146656 examples: {'rewards_train/chosen': '0.050062', 'rewards_train/rejected': '-0.026915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076977', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-122.45', 'loss/train': '0.6669', 'examples_per_second': '32.704', 'grad_norm': '112', 'counters/examples': 146656, 'counters/updates': 4583}
train stats after 146688 examples: {'rewards_train/chosen': '0.050105', 'rewards_train/rejected': '0.064781', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014676', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-143.76', 'loss/train': '0.70787', 'examples_per_second': '30.985', 'grad_norm': '59.75', 'counters/examples': 146688, 'counters/updates': 4584}
train stats after 146720 examples: {'rewards_train/chosen': '0.092327', 'rewards_train/rejected': '0.0037834', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088543', 'logps_train/rejected': '-178.84', 'logps_train/chosen': '-162.91', 'loss/train': '0.66671', 'examples_per_second': '31.562', 'grad_norm': '83.5', 'counters/examples': 146720, 'counters/updates': 4585}
train stats after 146752 examples: {'rewards_train/chosen': '0.10719', 'rewards_train/rejected': '0.040957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066235', 'logps_train/rejected': '-136.94', 'logps_train/chosen': '-150.63', 'loss/train': '0.68318', 'examples_per_second': '30.734', 'grad_norm': '88.5', 'counters/examples': 146752, 'counters/updates': 4586}
skipping logging after 146784 examples to avoid logging too frequently
train stats after 146816 examples: {'rewards_train/chosen': '0.033315', 'rewards_train/rejected': '0.072511', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039196', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-136.19', 'loss/train': '0.72336', 'examples_per_second': '31.892', 'grad_norm': '89', 'counters/examples': 146816, 'counters/updates': 4588}
skipping logging after 146848 examples to avoid logging too frequently
train stats after 146880 examples: {'rewards_train/chosen': '0.091195', 'rewards_train/rejected': '0.056868', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034327', 'logps_train/rejected': '-174.45', 'logps_train/chosen': '-155.9', 'loss/train': '0.69687', 'examples_per_second': '31.151', 'grad_norm': '71', 'counters/examples': 146880, 'counters/updates': 4590}
train stats after 146912 examples: {'rewards_train/chosen': '0.0178', 'rewards_train/rejected': '0.13272', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.11492', 'logps_train/rejected': '-178.37', 'logps_train/chosen': '-156.03', 'loss/train': '0.77867', 'examples_per_second': '31.401', 'grad_norm': '97', 'counters/examples': 146912, 'counters/updates': 4591}
skipping logging after 146944 examples to avoid logging too frequently
train stats after 146976 examples: {'rewards_train/chosen': '0.038055', 'rewards_train/rejected': '0.0038262', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034229', 'logps_train/rejected': '-80.512', 'logps_train/chosen': '-99.573', 'loss/train': '0.68942', 'examples_per_second': '33.402', 'grad_norm': '73.5', 'counters/examples': 146976, 'counters/updates': 4593}
train stats after 147008 examples: {'rewards_train/chosen': '0.13485', 'rewards_train/rejected': '0.04568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089167', 'logps_train/rejected': '-166', 'logps_train/chosen': '-182.94', 'loss/train': '0.65595', 'examples_per_second': '31.489', 'grad_norm': '96.5', 'counters/examples': 147008, 'counters/updates': 4594}
train stats after 147040 examples: {'rewards_train/chosen': '0.082358', 'rewards_train/rejected': '-0.008977', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091335', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-137.16', 'loss/train': '0.66449', 'examples_per_second': '30.556', 'grad_norm': '151', 'counters/examples': 147040, 'counters/updates': 4595}
train stats after 147072 examples: {'rewards_train/chosen': '0.17723', 'rewards_train/rejected': '0.043702', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13353', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-159.66', 'loss/train': '0.65316', 'examples_per_second': '31.479', 'grad_norm': '65.5', 'counters/examples': 147072, 'counters/updates': 4596}
train stats after 147104 examples: {'rewards_train/chosen': '0.081795', 'rewards_train/rejected': '0.068105', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01369', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-143.42', 'loss/train': '0.6996', 'examples_per_second': '31.603', 'grad_norm': '54.5', 'counters/examples': 147104, 'counters/updates': 4597}
train stats after 147136 examples: {'rewards_train/chosen': '0.076122', 'rewards_train/rejected': '0.0044776', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071645', 'logps_train/rejected': '-145.15', 'logps_train/chosen': '-163.03', 'loss/train': '0.66573', 'examples_per_second': '32.501', 'grad_norm': '69', 'counters/examples': 147136, 'counters/updates': 4598}
train stats after 147168 examples: {'rewards_train/chosen': '0.03504', 'rewards_train/rejected': '0.12974', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.094701', 'logps_train/rejected': '-143.51', 'logps_train/chosen': '-130.77', 'loss/train': '0.7565', 'examples_per_second': '30.981', 'grad_norm': '73.5', 'counters/examples': 147168, 'counters/updates': 4599}
train stats after 147200 examples: {'rewards_train/chosen': '0.047045', 'rewards_train/rejected': '0.072663', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025618', 'logps_train/rejected': '-142.46', 'logps_train/chosen': '-148.62', 'loss/train': '0.72362', 'examples_per_second': '30.021', 'grad_norm': '414', 'counters/examples': 147200, 'counters/updates': 4600}
train stats after 147232 examples: {'rewards_train/chosen': '0.24356', 'rewards_train/rejected': '0.094809', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14875', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-184.12', 'loss/train': '0.64007', 'examples_per_second': '31.598', 'grad_norm': '72.5', 'counters/examples': 147232, 'counters/updates': 4601}
train stats after 147264 examples: {'rewards_train/chosen': '0.080499', 'rewards_train/rejected': '6.5537e-05', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080434', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-148.4', 'loss/train': '0.66511', 'examples_per_second': '32.601', 'grad_norm': '173', 'counters/examples': 147264, 'counters/updates': 4602}
train stats after 147296 examples: {'rewards_train/chosen': '0.10401', 'rewards_train/rejected': '0.044616', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059397', 'logps_train/rejected': '-90.263', 'logps_train/chosen': '-156.91', 'loss/train': '0.68022', 'examples_per_second': '32.631', 'grad_norm': '72', 'counters/examples': 147296, 'counters/updates': 4603}
train stats after 147328 examples: {'rewards_train/chosen': '-0.00025373', 'rewards_train/rejected': '0.084694', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.084948', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-137.42', 'loss/train': '0.74648', 'examples_per_second': '31.56', 'grad_norm': '86', 'counters/examples': 147328, 'counters/updates': 4604}
train stats after 147360 examples: {'rewards_train/chosen': '0.060916', 'rewards_train/rejected': '0.070092', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0091761', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-167.97', 'loss/train': '0.70948', 'examples_per_second': '31.53', 'grad_norm': '133', 'counters/examples': 147360, 'counters/updates': 4605}
skipping logging after 147392 examples to avoid logging too frequently
train stats after 147424 examples: {'rewards_train/chosen': '0.13306', 'rewards_train/rejected': '0.13846', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0053915', 'logps_train/rejected': '-227.32', 'logps_train/chosen': '-189.3', 'loss/train': '0.72593', 'examples_per_second': '31.579', 'grad_norm': '82.5', 'counters/examples': 147424, 'counters/updates': 4607}
train stats after 147456 examples: {'rewards_train/chosen': '0.11006', 'rewards_train/rejected': '0.14957', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039517', 'logps_train/rejected': '-194.54', 'logps_train/chosen': '-197.1', 'loss/train': '0.73598', 'examples_per_second': '31.54', 'grad_norm': '124.5', 'counters/examples': 147456, 'counters/updates': 4608}
train stats after 147488 examples: {'rewards_train/chosen': '-0.0066211', 'rewards_train/rejected': '-0.0061227', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00049833', 'logps_train/rejected': '-87.866', 'logps_train/chosen': '-102.09', 'loss/train': '0.69778', 'examples_per_second': '30.722', 'grad_norm': '98', 'counters/examples': 147488, 'counters/updates': 4609}
train stats after 147520 examples: {'rewards_train/chosen': '0.15016', 'rewards_train/rejected': '0.049962', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1002', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-146.75', 'loss/train': '0.65216', 'examples_per_second': '30.435', 'grad_norm': '124.5', 'counters/examples': 147520, 'counters/updates': 4610}
skipping logging after 147552 examples to avoid logging too frequently
train stats after 147584 examples: {'rewards_train/chosen': '0.11254', 'rewards_train/rejected': '0.088589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023947', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-179.17', 'loss/train': '0.69635', 'examples_per_second': '31.509', 'grad_norm': '184', 'counters/examples': 147584, 'counters/updates': 4612}
train stats after 147616 examples: {'rewards_train/chosen': '0.11634', 'rewards_train/rejected': '0.14587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029531', 'logps_train/rejected': '-202.9', 'logps_train/chosen': '-172.42', 'loss/train': '0.72461', 'examples_per_second': '31.482', 'grad_norm': '106', 'counters/examples': 147616, 'counters/updates': 4613}
skipping logging after 147648 examples to avoid logging too frequently
train stats after 147680 examples: {'rewards_train/chosen': '0.13322', 'rewards_train/rejected': '0.018088', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11513', 'logps_train/rejected': '-131.22', 'logps_train/chosen': '-184.62', 'loss/train': '0.65291', 'examples_per_second': '36.111', 'grad_norm': '121', 'counters/examples': 147680, 'counters/updates': 4615}
train stats after 147712 examples: {'rewards_train/chosen': '-0.079256', 'rewards_train/rejected': '0.046217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.12547', 'logps_train/rejected': '-112.92', 'logps_train/chosen': '-207.65', 'loss/train': '0.81123', 'examples_per_second': '30.801', 'grad_norm': '192', 'counters/examples': 147712, 'counters/updates': 4616}
train stats after 147744 examples: {'rewards_train/chosen': '0.12608', 'rewards_train/rejected': '-0.0034489', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12953', 'logps_train/rejected': '-177.76', 'logps_train/chosen': '-201.08', 'loss/train': '0.64142', 'examples_per_second': '31.54', 'grad_norm': '82.5', 'counters/examples': 147744, 'counters/updates': 4617}
train stats after 147776 examples: {'rewards_train/chosen': '0.11413', 'rewards_train/rejected': '0.089698', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024433', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-147.96', 'loss/train': '0.68913', 'examples_per_second': '31.582', 'grad_norm': '148', 'counters/examples': 147776, 'counters/updates': 4618}
train stats after 147808 examples: {'rewards_train/chosen': '0.19623', 'rewards_train/rejected': '0.097824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098401', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-168.69', 'loss/train': '0.67762', 'examples_per_second': '32.263', 'grad_norm': '145', 'counters/examples': 147808, 'counters/updates': 4619}
train stats after 147840 examples: {'rewards_train/chosen': '0.14671', 'rewards_train/rejected': '0.065313', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081393', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-169.6', 'loss/train': '0.67287', 'examples_per_second': '31.382', 'grad_norm': '85', 'counters/examples': 147840, 'counters/updates': 4620}
train stats after 147872 examples: {'rewards_train/chosen': '0.1748', 'rewards_train/rejected': '0.055233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11956', 'logps_train/rejected': '-154.09', 'logps_train/chosen': '-150.83', 'loss/train': '0.64447', 'examples_per_second': '30.032', 'grad_norm': '119.5', 'counters/examples': 147872, 'counters/updates': 4621}
train stats after 147904 examples: {'rewards_train/chosen': '0.059857', 'rewards_train/rejected': '0.10246', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042605', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-132.61', 'loss/train': '0.72342', 'examples_per_second': '31.493', 'grad_norm': '86', 'counters/examples': 147904, 'counters/updates': 4622}
train stats after 147936 examples: {'rewards_train/chosen': '0.13388', 'rewards_train/rejected': '0.1225', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011385', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-145.04', 'loss/train': '0.69896', 'examples_per_second': '31.551', 'grad_norm': '77.5', 'counters/examples': 147936, 'counters/updates': 4623}
train stats after 147968 examples: {'rewards_train/chosen': '0.082704', 'rewards_train/rejected': '-0.077993', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1607', 'logps_train/rejected': '-129.76', 'logps_train/chosen': '-129.02', 'loss/train': '0.62479', 'examples_per_second': '32.241', 'grad_norm': '70', 'counters/examples': 147968, 'counters/updates': 4624}
train stats after 148000 examples: {'rewards_train/chosen': '0.20683', 'rewards_train/rejected': '0.019192', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18764', 'logps_train/rejected': '-134.57', 'logps_train/chosen': '-185.83', 'loss/train': '0.62404', 'examples_per_second': '31.547', 'grad_norm': '68.5', 'counters/examples': 148000, 'counters/updates': 4625}
train stats after 148032 examples: {'rewards_train/chosen': '0.21562', 'rewards_train/rejected': '0.12085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094767', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-156.64', 'loss/train': '0.66294', 'examples_per_second': '32.408', 'grad_norm': '152', 'counters/examples': 148032, 'counters/updates': 4626}
train stats after 148064 examples: {'rewards_train/chosen': '0.16389', 'rewards_train/rejected': '0.12286', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04103', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-141.21', 'loss/train': '0.68467', 'examples_per_second': '31.46', 'grad_norm': '87', 'counters/examples': 148064, 'counters/updates': 4627}
skipping logging after 148096 examples to avoid logging too frequently
train stats after 148128 examples: {'rewards_train/chosen': '0.06395', 'rewards_train/rejected': '0.12067', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.05672', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-168.23', 'loss/train': '0.74035', 'examples_per_second': '31.783', 'grad_norm': '78', 'counters/examples': 148128, 'counters/updates': 4629}
train stats after 148160 examples: {'rewards_train/chosen': '0.13528', 'rewards_train/rejected': '0.03764', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097637', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-148.22', 'loss/train': '0.6623', 'examples_per_second': '32.292', 'grad_norm': '87', 'counters/examples': 148160, 'counters/updates': 4630}
skipping logging after 148192 examples to avoid logging too frequently
train stats after 148224 examples: {'rewards_train/chosen': '0.08414', 'rewards_train/rejected': '0.049375', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034765', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-123.1', 'loss/train': '0.67987', 'examples_per_second': '33.181', 'grad_norm': '76', 'counters/examples': 148224, 'counters/updates': 4632}
train stats after 148256 examples: {'rewards_train/chosen': '0.085739', 'rewards_train/rejected': '0.090263', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0045239', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-133.53', 'loss/train': '0.70324', 'examples_per_second': '30.319', 'grad_norm': '90.5', 'counters/examples': 148256, 'counters/updates': 4633}
skipping logging after 148288 examples to avoid logging too frequently
train stats after 148320 examples: {'rewards_train/chosen': '0.078676', 'rewards_train/rejected': '0.0098474', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068828', 'logps_train/rejected': '-111.08', 'logps_train/chosen': '-141.22', 'loss/train': '0.67619', 'examples_per_second': '31.17', 'grad_norm': '55.5', 'counters/examples': 148320, 'counters/updates': 4635}
skipping logging after 148352 examples to avoid logging too frequently
train stats after 148384 examples: {'rewards_train/chosen': '0.06951', 'rewards_train/rejected': '0.044925', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.024584', 'logps_train/rejected': '-155.51', 'logps_train/chosen': '-137.2', 'loss/train': '0.69138', 'examples_per_second': '31.566', 'grad_norm': '79', 'counters/examples': 148384, 'counters/updates': 4637}
train stats after 148416 examples: {'rewards_train/chosen': '0.10221', 'rewards_train/rejected': '0.042092', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060115', 'logps_train/rejected': '-145', 'logps_train/chosen': '-155.8', 'loss/train': '0.6767', 'examples_per_second': '30.535', 'grad_norm': '63.25', 'counters/examples': 148416, 'counters/updates': 4638}
skipping logging after 148448 examples to avoid logging too frequently
train stats after 148480 examples: {'rewards_train/chosen': '0.012154', 'rewards_train/rejected': '0.099823', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.087669', 'logps_train/rejected': '-118.76', 'logps_train/chosen': '-110.16', 'loss/train': '0.75817', 'examples_per_second': '33.441', 'grad_norm': '75.5', 'counters/examples': 148480, 'counters/updates': 4640}
train stats after 148512 examples: {'rewards_train/chosen': '0.068256', 'rewards_train/rejected': '-0.019924', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08818', 'logps_train/rejected': '-155.59', 'logps_train/chosen': '-193.67', 'loss/train': '0.67078', 'examples_per_second': '31.148', 'grad_norm': '72.5', 'counters/examples': 148512, 'counters/updates': 4641}
train stats after 148544 examples: {'rewards_train/chosen': '0.13804', 'rewards_train/rejected': '0.048732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089306', 'logps_train/rejected': '-131.12', 'logps_train/chosen': '-145.64', 'loss/train': '0.66485', 'examples_per_second': '32.741', 'grad_norm': '113.5', 'counters/examples': 148544, 'counters/updates': 4642}
train stats after 148576 examples: {'rewards_train/chosen': '0.10293', 'rewards_train/rejected': '0.023495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079435', 'logps_train/rejected': '-115.15', 'logps_train/chosen': '-150.67', 'loss/train': '0.66338', 'examples_per_second': '30.427', 'grad_norm': '70', 'counters/examples': 148576, 'counters/updates': 4643}
train stats after 148608 examples: {'rewards_train/chosen': '0.1108', 'rewards_train/rejected': '0.04667', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064126', 'logps_train/rejected': '-131.68', 'logps_train/chosen': '-164.38', 'loss/train': '0.67615', 'examples_per_second': '31.619', 'grad_norm': '51.5', 'counters/examples': 148608, 'counters/updates': 4644}
train stats after 148640 examples: {'rewards_train/chosen': '-0.0028154', 'rewards_train/rejected': '-0.014916', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0121', 'logps_train/rejected': '-107.27', 'logps_train/chosen': '-146.91', 'loss/train': '0.69623', 'examples_per_second': '31.669', 'grad_norm': '61', 'counters/examples': 148640, 'counters/updates': 4645}
train stats after 148672 examples: {'rewards_train/chosen': '0.14657', 'rewards_train/rejected': '0.034481', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11209', 'logps_train/rejected': '-156.37', 'logps_train/chosen': '-150.47', 'loss/train': '0.65115', 'examples_per_second': '32.135', 'grad_norm': '53.5', 'counters/examples': 148672, 'counters/updates': 4646}
skipping logging after 148704 examples to avoid logging too frequently
train stats after 148736 examples: {'rewards_train/chosen': '0.16212', 'rewards_train/rejected': '0.050987', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11113', 'logps_train/rejected': '-128.42', 'logps_train/chosen': '-156.29', 'loss/train': '0.66415', 'examples_per_second': '34.41', 'grad_norm': '122.5', 'counters/examples': 148736, 'counters/updates': 4648}
train stats after 148768 examples: {'rewards_train/chosen': '-0.0086017', 'rewards_train/rejected': '0.0025081', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01111', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-147.57', 'loss/train': '0.70579', 'examples_per_second': '32.705', 'grad_norm': '79.5', 'counters/examples': 148768, 'counters/updates': 4649}
train stats after 148800 examples: {'rewards_train/chosen': '0.11771', 'rewards_train/rejected': '-0.072652', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19036', 'logps_train/rejected': '-105.83', 'logps_train/chosen': '-115.83', 'loss/train': '0.61212', 'examples_per_second': '32.35', 'grad_norm': '212', 'counters/examples': 148800, 'counters/updates': 4650}
train stats after 148832 examples: {'rewards_train/chosen': '0.1719', 'rewards_train/rejected': '0.10699', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064905', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-148.91', 'loss/train': '0.67567', 'examples_per_second': '31.895', 'grad_norm': '133', 'counters/examples': 148832, 'counters/updates': 4651}
train stats after 148864 examples: {'rewards_train/chosen': '0.10214', 'rewards_train/rejected': '0.043621', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058521', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-132.93', 'loss/train': '0.67967', 'examples_per_second': '30.706', 'grad_norm': '101.5', 'counters/examples': 148864, 'counters/updates': 4652}
train stats after 148896 examples: {'rewards_train/chosen': '0.099183', 'rewards_train/rejected': '0.098189', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00099397', 'logps_train/rejected': '-145.74', 'logps_train/chosen': '-140.05', 'loss/train': '0.70749', 'examples_per_second': '31.321', 'grad_norm': '78.5', 'counters/examples': 148896, 'counters/updates': 4653}
skipping logging after 148928 examples to avoid logging too frequently
train stats after 148960 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.069323', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10054', 'logps_train/rejected': '-131', 'logps_train/chosen': '-153.84', 'loss/train': '0.67409', 'examples_per_second': '30.195', 'grad_norm': '62', 'counters/examples': 148960, 'counters/updates': 4655}
train stats after 148992 examples: {'rewards_train/chosen': '0.010416', 'rewards_train/rejected': '-0.014887', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.025303', 'logps_train/rejected': '-103.19', 'logps_train/chosen': '-166.44', 'loss/train': '0.69404', 'examples_per_second': '31.02', 'grad_norm': '92', 'counters/examples': 148992, 'counters/updates': 4656}
train stats after 149024 examples: {'rewards_train/chosen': '0.10222', 'rewards_train/rejected': '0.07685', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025373', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-161.23', 'loss/train': '0.69308', 'examples_per_second': '33.172', 'grad_norm': '92.5', 'counters/examples': 149024, 'counters/updates': 4657}
train stats after 149056 examples: {'rewards_train/chosen': '0.074339', 'rewards_train/rejected': '-0.042889', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11723', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-180.82', 'loss/train': '0.65614', 'examples_per_second': '31.571', 'grad_norm': '101', 'counters/examples': 149056, 'counters/updates': 4658}
train stats after 149088 examples: {'rewards_train/chosen': '0.14317', 'rewards_train/rejected': '0.034422', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10874', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-150.94', 'loss/train': '0.65253', 'examples_per_second': '31.506', 'grad_norm': '57.75', 'counters/examples': 149088, 'counters/updates': 4659}
train stats after 149120 examples: {'rewards_train/chosen': '0.10189', 'rewards_train/rejected': '0.048027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053862', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-156.31', 'loss/train': '0.68034', 'examples_per_second': '31.556', 'grad_norm': '55.5', 'counters/examples': 149120, 'counters/updates': 4660}
train stats after 149152 examples: {'rewards_train/chosen': '0.040408', 'rewards_train/rejected': '-0.020003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060411', 'logps_train/rejected': '-129.84', 'logps_train/chosen': '-170.25', 'loss/train': '0.70127', 'examples_per_second': '32.438', 'grad_norm': '270', 'counters/examples': 149152, 'counters/updates': 4661}
skipping logging after 149184 examples to avoid logging too frequently
train stats after 149216 examples: {'rewards_train/chosen': '0.091413', 'rewards_train/rejected': '0.080225', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011188', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-129.19', 'loss/train': '0.70747', 'examples_per_second': '23.699', 'grad_norm': '57.25', 'counters/examples': 149216, 'counters/updates': 4663}
train stats after 149248 examples: {'rewards_train/chosen': '0.084838', 'rewards_train/rejected': '0.023509', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061329', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-161.67', 'loss/train': '0.67308', 'examples_per_second': '32.135', 'grad_norm': '66', 'counters/examples': 149248, 'counters/updates': 4664}
train stats after 149280 examples: {'rewards_train/chosen': '0.082705', 'rewards_train/rejected': '0.0049515', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077754', 'logps_train/rejected': '-94.863', 'logps_train/chosen': '-126.91', 'loss/train': '0.66096', 'examples_per_second': '31.809', 'grad_norm': '86.5', 'counters/examples': 149280, 'counters/updates': 4665}
train stats after 149312 examples: {'rewards_train/chosen': '0.13265', 'rewards_train/rejected': '0.039818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092827', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-137.91', 'loss/train': '0.65818', 'examples_per_second': '31.632', 'grad_norm': '95.5', 'counters/examples': 149312, 'counters/updates': 4666}
train stats after 149344 examples: {'rewards_train/chosen': '0.052024', 'rewards_train/rejected': '0.065292', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013268', 'logps_train/rejected': '-115.87', 'logps_train/chosen': '-130.44', 'loss/train': '0.70914', 'examples_per_second': '32.958', 'grad_norm': '95.5', 'counters/examples': 149344, 'counters/updates': 4667}
skipping logging after 149376 examples to avoid logging too frequently
train stats after 149408 examples: {'rewards_train/chosen': '0.054612', 'rewards_train/rejected': '0.017378', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037234', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-149.5', 'loss/train': '0.68497', 'examples_per_second': '30.532', 'grad_norm': '142', 'counters/examples': 149408, 'counters/updates': 4669}
train stats after 149440 examples: {'rewards_train/chosen': '0.096173', 'rewards_train/rejected': '0.11647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020295', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-134.55', 'loss/train': '0.72615', 'examples_per_second': '32.152', 'grad_norm': '125.5', 'counters/examples': 149440, 'counters/updates': 4670}
train stats after 149472 examples: {'rewards_train/chosen': '0.0073702', 'rewards_train/rejected': '-0.039625', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046995', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-135.18', 'loss/train': '0.67477', 'examples_per_second': '30.22', 'grad_norm': '47.5', 'counters/examples': 149472, 'counters/updates': 4671}
train stats after 149504 examples: {'rewards_train/chosen': '0.11567', 'rewards_train/rejected': '0.20218', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.086517', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-141.8', 'loss/train': '0.75206', 'examples_per_second': '31.451', 'grad_norm': '127', 'counters/examples': 149504, 'counters/updates': 4672}
train stats after 149536 examples: {'rewards_train/chosen': '-0.032788', 'rewards_train/rejected': '0.047018', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.079806', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-140.31', 'loss/train': '0.74751', 'examples_per_second': '31.56', 'grad_norm': '117', 'counters/examples': 149536, 'counters/updates': 4673}
train stats after 149568 examples: {'rewards_train/chosen': '0.073911', 'rewards_train/rejected': '0.071112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0027983', 'logps_train/rejected': '-152.12', 'logps_train/chosen': '-172.08', 'loss/train': '0.70941', 'examples_per_second': '31.151', 'grad_norm': '102.5', 'counters/examples': 149568, 'counters/updates': 4674}
train stats after 149600 examples: {'rewards_train/chosen': '0.11011', 'rewards_train/rejected': '0.097561', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012546', 'logps_train/rejected': '-133.82', 'logps_train/chosen': '-146.63', 'loss/train': '0.69937', 'examples_per_second': '31.518', 'grad_norm': '125.5', 'counters/examples': 149600, 'counters/updates': 4675}
skipping logging after 149632 examples to avoid logging too frequently
train stats after 149664 examples: {'rewards_train/chosen': '0.066478', 'rewards_train/rejected': '0.077355', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010876', 'logps_train/rejected': '-173.86', 'logps_train/chosen': '-142.91', 'loss/train': '0.71628', 'examples_per_second': '37.234', 'grad_norm': '152', 'counters/examples': 149664, 'counters/updates': 4677}
train stats after 149696 examples: {'rewards_train/chosen': '0.14279', 'rewards_train/rejected': '0.04403', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098756', 'logps_train/rejected': '-155.46', 'logps_train/chosen': '-141.58', 'loss/train': '0.65351', 'examples_per_second': '33.116', 'grad_norm': '67', 'counters/examples': 149696, 'counters/updates': 4678}
train stats after 149728 examples: {'rewards_train/chosen': '0.12289', 'rewards_train/rejected': '0.036754', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08614', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-148.4', 'loss/train': '0.66497', 'examples_per_second': '32.134', 'grad_norm': '81.5', 'counters/examples': 149728, 'counters/updates': 4679}
train stats after 149760 examples: {'rewards_train/chosen': '0.068155', 'rewards_train/rejected': '0.031413', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036742', 'logps_train/rejected': '-115.11', 'logps_train/chosen': '-162.81', 'loss/train': '0.685', 'examples_per_second': '30.042', 'grad_norm': '55.25', 'counters/examples': 149760, 'counters/updates': 4680}
train stats after 149792 examples: {'rewards_train/chosen': '0.094956', 'rewards_train/rejected': '0.047246', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.047709', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-138.6', 'loss/train': '0.67943', 'examples_per_second': '30.73', 'grad_norm': '104.5', 'counters/examples': 149792, 'counters/updates': 4681}
train stats after 149824 examples: {'rewards_train/chosen': '0.067855', 'rewards_train/rejected': '0.043472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024383', 'logps_train/rejected': '-172.79', 'logps_train/chosen': '-178.06', 'loss/train': '0.7049', 'examples_per_second': '32.09', 'grad_norm': '75.5', 'counters/examples': 149824, 'counters/updates': 4682}
train stats after 149856 examples: {'rewards_train/chosen': '0.16458', 'rewards_train/rejected': '-0.0031649', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16774', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-179.12', 'loss/train': '0.6232', 'examples_per_second': '31.218', 'grad_norm': '93', 'counters/examples': 149856, 'counters/updates': 4683}
train stats after 149888 examples: {'rewards_train/chosen': '0.17566', 'rewards_train/rejected': '0.14945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026216', 'logps_train/rejected': '-146.83', 'logps_train/chosen': '-162.36', 'loss/train': '0.69508', 'examples_per_second': '32.078', 'grad_norm': '146', 'counters/examples': 149888, 'counters/updates': 4684}
skipping logging after 149920 examples to avoid logging too frequently
train stats after 149952 examples: {'rewards_train/chosen': '0.11798', 'rewards_train/rejected': '0.045193', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072789', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-133.2', 'loss/train': '0.67676', 'examples_per_second': '33.697', 'grad_norm': '112.5', 'counters/examples': 149952, 'counters/updates': 4686}
train stats after 149984 examples: {'rewards_train/chosen': '0.18721', 'rewards_train/rejected': '-0.012006', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19922', 'logps_train/rejected': '-86.613', 'logps_train/chosen': '-181.96', 'loss/train': '0.63037', 'examples_per_second': '31.212', 'grad_norm': '47.5', 'counters/examples': 149984, 'counters/updates': 4687}
train stats after 150016 examples: {'rewards_train/chosen': '0.043347', 'rewards_train/rejected': '0.01945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023897', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-149.7', 'loss/train': '0.69585', 'examples_per_second': '30.592', 'grad_norm': '93.5', 'counters/examples': 150016, 'counters/updates': 4688}
train stats after 150048 examples: {'rewards_train/chosen': '0.063522', 'rewards_train/rejected': '0.11629', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.052767', 'logps_train/rejected': '-128.67', 'logps_train/chosen': '-147.14', 'loss/train': '0.73218', 'examples_per_second': '31.179', 'grad_norm': '91', 'counters/examples': 150048, 'counters/updates': 4689}
skipping logging after 150080 examples to avoid logging too frequently
train stats after 150112 examples: {'rewards_train/chosen': '0.010673', 'rewards_train/rejected': '0.070361', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059687', 'logps_train/rejected': '-98.632', 'logps_train/chosen': '-169.1', 'loss/train': '0.74106', 'examples_per_second': '32.978', 'grad_norm': '97.5', 'counters/examples': 150112, 'counters/updates': 4691}
train stats after 150144 examples: {'rewards_train/chosen': '0.10968', 'rewards_train/rejected': '-0.08431', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19399', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-154.4', 'loss/train': '0.6181', 'examples_per_second': '32.607', 'grad_norm': '67.5', 'counters/examples': 150144, 'counters/updates': 4692}
train stats after 150176 examples: {'rewards_train/chosen': '0.056017', 'rewards_train/rejected': '0.031177', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02484', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-128.7', 'loss/train': '0.69627', 'examples_per_second': '33.16', 'grad_norm': '48.5', 'counters/examples': 150176, 'counters/updates': 4693}
skipping logging after 150208 examples to avoid logging too frequently
train stats after 150240 examples: {'rewards_train/chosen': '0.13096', 'rewards_train/rejected': '0.038351', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09261', 'logps_train/rejected': '-165.74', 'logps_train/chosen': '-160.85', 'loss/train': '0.65842', 'examples_per_second': '31.676', 'grad_norm': '71', 'counters/examples': 150240, 'counters/updates': 4695}
train stats after 150272 examples: {'rewards_train/chosen': '0.098186', 'rewards_train/rejected': '0.018391', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079796', 'logps_train/rejected': '-120.37', 'logps_train/chosen': '-136.74', 'loss/train': '0.66558', 'examples_per_second': '30.129', 'grad_norm': '104', 'counters/examples': 150272, 'counters/updates': 4696}
train stats after 150304 examples: {'rewards_train/chosen': '0.17848', 'rewards_train/rejected': '0.085756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092721', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-159.65', 'loss/train': '0.66645', 'examples_per_second': '31.57', 'grad_norm': '71.5', 'counters/examples': 150304, 'counters/updates': 4697}
train stats after 150336 examples: {'rewards_train/chosen': '0.056075', 'rewards_train/rejected': '-0.048251', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10433', 'logps_train/rejected': '-117', 'logps_train/chosen': '-124.11', 'loss/train': '0.6761', 'examples_per_second': '31.469', 'grad_norm': '73.5', 'counters/examples': 150336, 'counters/updates': 4698}
skipping logging after 150368 examples to avoid logging too frequently
train stats after 150400 examples: {'rewards_train/chosen': '0.12187', 'rewards_train/rejected': '0.067982', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053892', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-129.7', 'loss/train': '0.69106', 'examples_per_second': '39.261', 'grad_norm': '68.5', 'counters/examples': 150400, 'counters/updates': 4700}
train stats after 150432 examples: {'rewards_train/chosen': '0.082179', 'rewards_train/rejected': '-0.021514', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10369', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-130.37', 'loss/train': '0.65595', 'examples_per_second': '30.818', 'grad_norm': '61', 'counters/examples': 150432, 'counters/updates': 4701}
train stats after 150464 examples: {'rewards_train/chosen': '0.17459', 'rewards_train/rejected': '-0.0091182', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1837', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-162.13', 'loss/train': '0.63371', 'examples_per_second': '30.49', 'grad_norm': '62.5', 'counters/examples': 150464, 'counters/updates': 4702}
train stats after 150496 examples: {'rewards_train/chosen': '0.076765', 'rewards_train/rejected': '0.08726', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010495', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-139.36', 'loss/train': '0.71472', 'examples_per_second': '31.51', 'grad_norm': '65.5', 'counters/examples': 150496, 'counters/updates': 4703}
skipping logging after 150528 examples to avoid logging too frequently
train stats after 150560 examples: {'rewards_train/chosen': '0.10738', 'rewards_train/rejected': '0.01348', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093898', 'logps_train/rejected': '-155.89', 'logps_train/chosen': '-178.25', 'loss/train': '0.65516', 'examples_per_second': '31.483', 'grad_norm': '121', 'counters/examples': 150560, 'counters/updates': 4705}
skipping logging after 150592 examples to avoid logging too frequently
train stats after 150624 examples: {'rewards_train/chosen': '0.081857', 'rewards_train/rejected': '0.14183', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.059971', 'logps_train/rejected': '-158.34', 'logps_train/chosen': '-143.79', 'loss/train': '0.7338', 'examples_per_second': '31.504', 'grad_norm': '73.5', 'counters/examples': 150624, 'counters/updates': 4707}
train stats after 150656 examples: {'rewards_train/chosen': '0.12912', 'rewards_train/rejected': '0.12848', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00064141', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-178.79', 'loss/train': '0.70601', 'examples_per_second': '31.388', 'grad_norm': '87.5', 'counters/examples': 150656, 'counters/updates': 4708}
train stats after 150688 examples: {'rewards_train/chosen': '0.18721', 'rewards_train/rejected': '0.040727', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14648', 'logps_train/rejected': '-103.85', 'logps_train/chosen': '-149.6', 'loss/train': '0.63759', 'examples_per_second': '30.715', 'grad_norm': '55.25', 'counters/examples': 150688, 'counters/updates': 4709}
train stats after 150720 examples: {'rewards_train/chosen': '0.1823', 'rewards_train/rejected': '0.18437', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0020659', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-172.22', 'loss/train': '0.72174', 'examples_per_second': '30.913', 'grad_norm': '94.5', 'counters/examples': 150720, 'counters/updates': 4710}
train stats after 150752 examples: {'rewards_train/chosen': '0.15525', 'rewards_train/rejected': '0.09035', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064896', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-165.83', 'loss/train': '0.67973', 'examples_per_second': '31.25', 'grad_norm': '182', 'counters/examples': 150752, 'counters/updates': 4711}
train stats after 150784 examples: {'rewards_train/chosen': '0.07831', 'rewards_train/rejected': '0.073044', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0052666', 'logps_train/rejected': '-164.74', 'logps_train/chosen': '-156.63', 'loss/train': '0.72491', 'examples_per_second': '31.544', 'grad_norm': '87', 'counters/examples': 150784, 'counters/updates': 4712}
train stats after 150816 examples: {'rewards_train/chosen': '0.096283', 'rewards_train/rejected': '0.10687', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01059', 'logps_train/rejected': '-153.64', 'logps_train/chosen': '-162', 'loss/train': '0.71051', 'examples_per_second': '32.003', 'grad_norm': '117.5', 'counters/examples': 150816, 'counters/updates': 4713}
train stats after 150848 examples: {'rewards_train/chosen': '0.060225', 'rewards_train/rejected': '-0.0054047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065629', 'logps_train/rejected': '-96.641', 'logps_train/chosen': '-154.62', 'loss/train': '0.6792', 'examples_per_second': '31.521', 'grad_norm': '48.5', 'counters/examples': 150848, 'counters/updates': 4714}
train stats after 150880 examples: {'rewards_train/chosen': '0.13818', 'rewards_train/rejected': '0.10469', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033494', 'logps_train/rejected': '-113.99', 'logps_train/chosen': '-151.28', 'loss/train': '0.69925', 'examples_per_second': '32.488', 'grad_norm': '77.5', 'counters/examples': 150880, 'counters/updates': 4715}
train stats after 150912 examples: {'rewards_train/chosen': '0.13312', 'rewards_train/rejected': '0.041879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091246', 'logps_train/rejected': '-115.56', 'logps_train/chosen': '-143.57', 'loss/train': '0.65743', 'examples_per_second': '30.497', 'grad_norm': '93', 'counters/examples': 150912, 'counters/updates': 4716}
train stats after 150944 examples: {'rewards_train/chosen': '0.14028', 'rewards_train/rejected': '0.057546', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082733', 'logps_train/rejected': '-151.11', 'logps_train/chosen': '-147.26', 'loss/train': '0.66538', 'examples_per_second': '32.986', 'grad_norm': '49.75', 'counters/examples': 150944, 'counters/updates': 4717}
train stats after 150976 examples: {'rewards_train/chosen': '0.0924', 'rewards_train/rejected': '0.035513', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056887', 'logps_train/rejected': '-172.34', 'logps_train/chosen': '-137.37', 'loss/train': '0.67634', 'examples_per_second': '30.416', 'grad_norm': '78.5', 'counters/examples': 150976, 'counters/updates': 4718}
skipping logging after 151008 examples to avoid logging too frequently
train stats after 151040 examples: {'rewards_train/chosen': '0.12589', 'rewards_train/rejected': '0.087912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037982', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-158.83', 'loss/train': '0.69947', 'examples_per_second': '31.099', 'grad_norm': '100', 'counters/examples': 151040, 'counters/updates': 4720}
train stats after 151072 examples: {'rewards_train/chosen': '0.077957', 'rewards_train/rejected': '0.091898', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.013941', 'logps_train/rejected': '-154.78', 'logps_train/chosen': '-129.73', 'loss/train': '0.71061', 'examples_per_second': '32.073', 'grad_norm': '84', 'counters/examples': 151072, 'counters/updates': 4721}
train stats after 151104 examples: {'rewards_train/chosen': '0.079039', 'rewards_train/rejected': '0.069535', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.009504', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-126.85', 'loss/train': '0.70505', 'examples_per_second': '31.341', 'grad_norm': '70', 'counters/examples': 151104, 'counters/updates': 4722}
train stats after 151136 examples: {'rewards_train/chosen': '0.045311', 'rewards_train/rejected': '0.079989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034677', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-140.94', 'loss/train': '0.72241', 'examples_per_second': '30.611', 'grad_norm': '67.5', 'counters/examples': 151136, 'counters/updates': 4723}
train stats after 151168 examples: {'rewards_train/chosen': '0.041793', 'rewards_train/rejected': '0.096065', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054273', 'logps_train/rejected': '-120.06', 'logps_train/chosen': '-163.63', 'loss/train': '0.72787', 'examples_per_second': '31.497', 'grad_norm': '78', 'counters/examples': 151168, 'counters/updates': 4724}
train stats after 151200 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.015258', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084757', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-148.83', 'loss/train': '0.65905', 'examples_per_second': '31.584', 'grad_norm': '57.75', 'counters/examples': 151200, 'counters/updates': 4725}
train stats after 151232 examples: {'rewards_train/chosen': '0.10305', 'rewards_train/rejected': '0.0021831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10087', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-165.31', 'loss/train': '0.66049', 'examples_per_second': '31.768', 'grad_norm': '71.5', 'counters/examples': 151232, 'counters/updates': 4726}
train stats after 151264 examples: {'rewards_train/chosen': '0.063159', 'rewards_train/rejected': '0.02859', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034569', 'logps_train/rejected': '-103.51', 'logps_train/chosen': '-128.16', 'loss/train': '0.69159', 'examples_per_second': '32.073', 'grad_norm': '69.5', 'counters/examples': 151264, 'counters/updates': 4727}
train stats after 151296 examples: {'rewards_train/chosen': '0.084945', 'rewards_train/rejected': '0.074539', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010406', 'logps_train/rejected': '-140.14', 'logps_train/chosen': '-165.42', 'loss/train': '0.70172', 'examples_per_second': '30.935', 'grad_norm': '86', 'counters/examples': 151296, 'counters/updates': 4728}
train stats after 151328 examples: {'rewards_train/chosen': '0.045382', 'rewards_train/rejected': '0.00094376', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.044439', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-150.78', 'loss/train': '0.67984', 'examples_per_second': '32.943', 'grad_norm': '86', 'counters/examples': 151328, 'counters/updates': 4729}
train stats after 151360 examples: {'rewards_train/chosen': '0.046294', 'rewards_train/rejected': '0.044942', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0013523', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-136.22', 'loss/train': '0.69811', 'examples_per_second': '31.723', 'grad_norm': '84', 'counters/examples': 151360, 'counters/updates': 4730}
train stats after 151392 examples: {'rewards_train/chosen': '0.064097', 'rewards_train/rejected': '0.082915', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018818', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-192.76', 'loss/train': '0.73467', 'examples_per_second': '30.109', 'grad_norm': '446', 'counters/examples': 151392, 'counters/updates': 4731}
train stats after 151424 examples: {'rewards_train/chosen': '0.12352', 'rewards_train/rejected': '0.11716', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0063536', 'logps_train/rejected': '-137.67', 'logps_train/chosen': '-181.87', 'loss/train': '0.70984', 'examples_per_second': '30.867', 'grad_norm': '107.5', 'counters/examples': 151424, 'counters/updates': 4732}
train stats after 151456 examples: {'rewards_train/chosen': '0.13711', 'rewards_train/rejected': '0.053552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083562', 'logps_train/rejected': '-182.71', 'logps_train/chosen': '-141.95', 'loss/train': '0.66504', 'examples_per_second': '31.547', 'grad_norm': '51.5', 'counters/examples': 151456, 'counters/updates': 4733}
train stats after 151488 examples: {'rewards_train/chosen': '0.1777', 'rewards_train/rejected': '0.060733', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11697', 'logps_train/rejected': '-144.18', 'logps_train/chosen': '-147.22', 'loss/train': '0.64956', 'examples_per_second': '31.535', 'grad_norm': '80', 'counters/examples': 151488, 'counters/updates': 4734}
train stats after 151520 examples: {'rewards_train/chosen': '0.13025', 'rewards_train/rejected': '0.048769', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081478', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-153.47', 'loss/train': '0.66444', 'examples_per_second': '33.145', 'grad_norm': '125', 'counters/examples': 151520, 'counters/updates': 4735}
train stats after 151552 examples: {'rewards_train/chosen': '0.12625', 'rewards_train/rejected': '0.024029', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10222', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-153.36', 'loss/train': '0.65333', 'examples_per_second': '31.958', 'grad_norm': '45.5', 'counters/examples': 151552, 'counters/updates': 4736}
train stats after 151584 examples: {'rewards_train/chosen': '0.079735', 'rewards_train/rejected': '0.038403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041332', 'logps_train/rejected': '-150.96', 'logps_train/chosen': '-142.45', 'loss/train': '0.68647', 'examples_per_second': '30.386', 'grad_norm': '77', 'counters/examples': 151584, 'counters/updates': 4737}
train stats after 151616 examples: {'rewards_train/chosen': '0.17562', 'rewards_train/rejected': '0.20132', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025693', 'logps_train/rejected': '-180.82', 'logps_train/chosen': '-143.54', 'loss/train': '0.74653', 'examples_per_second': '30.181', 'grad_norm': '215', 'counters/examples': 151616, 'counters/updates': 4738}
train stats after 151648 examples: {'rewards_train/chosen': '0.13426', 'rewards_train/rejected': '0.10624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028014', 'logps_train/rejected': '-128.74', 'logps_train/chosen': '-153.9', 'loss/train': '0.70412', 'examples_per_second': '32.407', 'grad_norm': '112', 'counters/examples': 151648, 'counters/updates': 4739}
train stats after 151680 examples: {'rewards_train/chosen': '0.066558', 'rewards_train/rejected': '0.044275', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022284', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-165.93', 'loss/train': '0.70128', 'examples_per_second': '32.419', 'grad_norm': '97', 'counters/examples': 151680, 'counters/updates': 4740}
skipping logging after 151712 examples to avoid logging too frequently
train stats after 151744 examples: {'rewards_train/chosen': '0.064774', 'rewards_train/rejected': '0.053743', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011031', 'logps_train/rejected': '-147.55', 'logps_train/chosen': '-152.62', 'loss/train': '0.70339', 'examples_per_second': '32.067', 'grad_norm': '85', 'counters/examples': 151744, 'counters/updates': 4742}
train stats after 151776 examples: {'rewards_train/chosen': '0.041929', 'rewards_train/rejected': '0.03676', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0051691', 'logps_train/rejected': '-131.19', 'logps_train/chosen': '-121.03', 'loss/train': '0.70253', 'examples_per_second': '31.765', 'grad_norm': '86', 'counters/examples': 151776, 'counters/updates': 4743}
train stats after 151808 examples: {'rewards_train/chosen': '0.060825', 'rewards_train/rejected': '0.027114', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033711', 'logps_train/rejected': '-110.84', 'logps_train/chosen': '-152.33', 'loss/train': '0.69365', 'examples_per_second': '31.571', 'grad_norm': '67.5', 'counters/examples': 151808, 'counters/updates': 4744}
train stats after 151840 examples: {'rewards_train/chosen': '0.1226', 'rewards_train/rejected': '-0.025872', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14847', 'logps_train/rejected': '-181.31', 'logps_train/chosen': '-189.01', 'loss/train': '0.64497', 'examples_per_second': '32.056', 'grad_norm': '82', 'counters/examples': 151840, 'counters/updates': 4745}
train stats after 151872 examples: {'rewards_train/chosen': '0.14395', 'rewards_train/rejected': '0.075532', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068419', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-138.1', 'loss/train': '0.66702', 'examples_per_second': '30.564', 'grad_norm': '75.5', 'counters/examples': 151872, 'counters/updates': 4746}
train stats after 151904 examples: {'rewards_train/chosen': '0.09424', 'rewards_train/rejected': '0.13359', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039352', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-162.56', 'loss/train': '0.73052', 'examples_per_second': '30.017', 'grad_norm': '134', 'counters/examples': 151904, 'counters/updates': 4747}
train stats after 151936 examples: {'rewards_train/chosen': '0.18227', 'rewards_train/rejected': '0.06393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11834', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-140.96', 'loss/train': '0.64829', 'examples_per_second': '28.131', 'grad_norm': '47.5', 'counters/examples': 151936, 'counters/updates': 4748}
skipping logging after 151968 examples to avoid logging too frequently
train stats after 152000 examples: {'rewards_train/chosen': '0.11082', 'rewards_train/rejected': '0.071652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039168', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-187.99', 'loss/train': '0.68453', 'examples_per_second': '32.047', 'grad_norm': '132', 'counters/examples': 152000, 'counters/updates': 4750}
train stats after 152032 examples: {'rewards_train/chosen': '0.11496', 'rewards_train/rejected': '0.059409', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055553', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-149.37', 'loss/train': '0.68311', 'examples_per_second': '24.449', 'grad_norm': '120', 'counters/examples': 152032, 'counters/updates': 4751}
train stats after 152064 examples: {'rewards_train/chosen': '0.13309', 'rewards_train/rejected': '0.10907', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024018', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-109.87', 'loss/train': '0.71736', 'examples_per_second': '31.631', 'grad_norm': '94', 'counters/examples': 152064, 'counters/updates': 4752}
train stats after 152096 examples: {'rewards_train/chosen': '0.11257', 'rewards_train/rejected': '0.078672', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0339', 'logps_train/rejected': '-134', 'logps_train/chosen': '-151.93', 'loss/train': '0.68809', 'examples_per_second': '30.481', 'grad_norm': '70', 'counters/examples': 152096, 'counters/updates': 4753}
train stats after 152128 examples: {'rewards_train/chosen': '0.043155', 'rewards_train/rejected': '0.16323', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12008', 'logps_train/rejected': '-146.88', 'logps_train/chosen': '-184.56', 'loss/train': '0.77861', 'examples_per_second': '31.55', 'grad_norm': '99', 'counters/examples': 152128, 'counters/updates': 4754}
skipping logging after 152160 examples to avoid logging too frequently
train stats after 152192 examples: {'rewards_train/chosen': '0.16584', 'rewards_train/rejected': '0.072879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092957', 'logps_train/rejected': '-161.04', 'logps_train/chosen': '-211.43', 'loss/train': '0.6684', 'examples_per_second': '30.569', 'grad_norm': '134', 'counters/examples': 152192, 'counters/updates': 4756}
train stats after 152224 examples: {'rewards_train/chosen': '0.20634', 'rewards_train/rejected': '0.080445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1259', 'logps_train/rejected': '-158.34', 'logps_train/chosen': '-158.65', 'loss/train': '0.64542', 'examples_per_second': '31.493', 'grad_norm': '190', 'counters/examples': 152224, 'counters/updates': 4757}
train stats after 152256 examples: {'rewards_train/chosen': '0.036237', 'rewards_train/rejected': '0.14905', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.11281', 'logps_train/rejected': '-139.16', 'logps_train/chosen': '-153.25', 'loss/train': '0.76045', 'examples_per_second': '30.999', 'grad_norm': '130', 'counters/examples': 152256, 'counters/updates': 4758}
train stats after 152288 examples: {'rewards_train/chosen': '0.13478', 'rewards_train/rejected': '0.066892', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.06789', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-173.92', 'loss/train': '0.6782', 'examples_per_second': '31.598', 'grad_norm': '101', 'counters/examples': 152288, 'counters/updates': 4759}
train stats after 152320 examples: {'rewards_train/chosen': '0.24682', 'rewards_train/rejected': '0.074665', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17216', 'logps_train/rejected': '-158.32', 'logps_train/chosen': '-155.09', 'loss/train': '0.63115', 'examples_per_second': '31.528', 'grad_norm': '95.5', 'counters/examples': 152320, 'counters/updates': 4760}
train stats after 152352 examples: {'rewards_train/chosen': '0.1527', 'rewards_train/rejected': '0.024512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12819', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-134.82', 'loss/train': '0.65001', 'examples_per_second': '30.732', 'grad_norm': '123.5', 'counters/examples': 152352, 'counters/updates': 4761}
train stats after 152384 examples: {'rewards_train/chosen': '0.121', 'rewards_train/rejected': '0.060877', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060119', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-141.01', 'loss/train': '0.67924', 'examples_per_second': '31.384', 'grad_norm': '122.5', 'counters/examples': 152384, 'counters/updates': 4762}
train stats after 152416 examples: {'rewards_train/chosen': '0.21186', 'rewards_train/rejected': '0.075126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13674', 'logps_train/rejected': '-144.94', 'logps_train/chosen': '-159.01', 'loss/train': '0.65137', 'examples_per_second': '31.398', 'grad_norm': '80.5', 'counters/examples': 152416, 'counters/updates': 4763}
train stats after 152448 examples: {'rewards_train/chosen': '0.042156', 'rewards_train/rejected': '-0.039298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081454', 'logps_train/rejected': '-131.48', 'logps_train/chosen': '-147.79', 'loss/train': '0.67036', 'examples_per_second': '32.288', 'grad_norm': '77', 'counters/examples': 152448, 'counters/updates': 4764}
train stats after 152480 examples: {'rewards_train/chosen': '0.13892', 'rewards_train/rejected': '0.066585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072332', 'logps_train/rejected': '-149.64', 'logps_train/chosen': '-160.48', 'loss/train': '0.67413', 'examples_per_second': '32.949', 'grad_norm': '118.5', 'counters/examples': 152480, 'counters/updates': 4765}
train stats after 152512 examples: {'rewards_train/chosen': '0.088411', 'rewards_train/rejected': '0.033481', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054931', 'logps_train/rejected': '-105.37', 'logps_train/chosen': '-159.49', 'loss/train': '0.67943', 'examples_per_second': '30.859', 'grad_norm': '87.5', 'counters/examples': 152512, 'counters/updates': 4766}
skipping logging after 152544 examples to avoid logging too frequently
train stats after 152576 examples: {'rewards_train/chosen': '0.13767', 'rewards_train/rejected': '0.070444', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067229', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-152.97', 'loss/train': '0.66667', 'examples_per_second': '33.428', 'grad_norm': '69.5', 'counters/examples': 152576, 'counters/updates': 4768}
train stats after 152608 examples: {'rewards_train/chosen': '0.039978', 'rewards_train/rejected': '0.071489', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.031511', 'logps_train/rejected': '-128.76', 'logps_train/chosen': '-181.75', 'loss/train': '0.72063', 'examples_per_second': '31.403', 'grad_norm': '97.5', 'counters/examples': 152608, 'counters/updates': 4769}
train stats after 152640 examples: {'rewards_train/chosen': '0.13877', 'rewards_train/rejected': '0.033443', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10533', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-160.05', 'loss/train': '0.65458', 'examples_per_second': '32.809', 'grad_norm': '103.5', 'counters/examples': 152640, 'counters/updates': 4770}
train stats after 152672 examples: {'rewards_train/chosen': '0.18048', 'rewards_train/rejected': '-0.019336', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.19981', 'logps_train/rejected': '-113.04', 'logps_train/chosen': '-176.01', 'loss/train': '0.63514', 'examples_per_second': '32.333', 'grad_norm': '52.25', 'counters/examples': 152672, 'counters/updates': 4771}
skipping logging after 152704 examples to avoid logging too frequently
train stats after 152736 examples: {'rewards_train/chosen': '0.086488', 'rewards_train/rejected': '0.11068', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024189', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-148.34', 'loss/train': '0.71486', 'examples_per_second': '33.427', 'grad_norm': '84', 'counters/examples': 152736, 'counters/updates': 4773}
train stats after 152768 examples: {'rewards_train/chosen': '0.061099', 'rewards_train/rejected': '0.0085522', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052547', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-134.52', 'loss/train': '0.67222', 'examples_per_second': '32.419', 'grad_norm': '71.5', 'counters/examples': 152768, 'counters/updates': 4774}
train stats after 152800 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '0.15203', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037617', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-122.11', 'loss/train': '0.72746', 'examples_per_second': '30.117', 'grad_norm': '338', 'counters/examples': 152800, 'counters/updates': 4775}
train stats after 152832 examples: {'rewards_train/chosen': '0.022672', 'rewards_train/rejected': '-0.001803', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024475', 'logps_train/rejected': '-171.21', 'logps_train/chosen': '-158.37', 'loss/train': '0.68955', 'examples_per_second': '30.046', 'grad_norm': '146', 'counters/examples': 152832, 'counters/updates': 4776}
train stats after 152864 examples: {'rewards_train/chosen': '0.20389', 'rewards_train/rejected': '0.037566', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16632', 'logps_train/rejected': '-163.4', 'logps_train/chosen': '-168.04', 'loss/train': '0.63902', 'examples_per_second': '32.376', 'grad_norm': '128', 'counters/examples': 152864, 'counters/updates': 4777}
train stats after 152896 examples: {'rewards_train/chosen': '-0.046071', 'rewards_train/rejected': '0.11202', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.15809', 'logps_train/rejected': '-155.79', 'logps_train/chosen': '-170.56', 'loss/train': '0.80125', 'examples_per_second': '31.442', 'grad_norm': '182', 'counters/examples': 152896, 'counters/updates': 4778}
train stats after 152928 examples: {'rewards_train/chosen': '0.15417', 'rewards_train/rejected': '0.064756', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089418', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-136.65', 'loss/train': '0.66472', 'examples_per_second': '31.51', 'grad_norm': '78.5', 'counters/examples': 152928, 'counters/updates': 4779}
skipping logging after 152960 examples to avoid logging too frequently
train stats after 152992 examples: {'rewards_train/chosen': '0.085197', 'rewards_train/rejected': '0.049326', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03587', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-144.67', 'loss/train': '0.7022', 'examples_per_second': '32.633', 'grad_norm': '45', 'counters/examples': 152992, 'counters/updates': 4781}
skipping logging after 153024 examples to avoid logging too frequently
train stats after 153056 examples: {'rewards_train/chosen': '0.17071', 'rewards_train/rejected': '0.098311', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072397', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-155.21', 'loss/train': '0.67735', 'examples_per_second': '30.574', 'grad_norm': '153', 'counters/examples': 153056, 'counters/updates': 4783}
skipping logging after 153088 examples to avoid logging too frequently
train stats after 153120 examples: {'rewards_train/chosen': '0.13762', 'rewards_train/rejected': '0.019321', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1183', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-162.75', 'loss/train': '0.64541', 'examples_per_second': '31.398', 'grad_norm': '238', 'counters/examples': 153120, 'counters/updates': 4785}
train stats after 153152 examples: {'rewards_train/chosen': '0.0048796', 'rewards_train/rejected': '0.053287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.048407', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-123.52', 'loss/train': '0.72897', 'examples_per_second': '32.555', 'grad_norm': '60.5', 'counters/examples': 153152, 'counters/updates': 4786}
train stats after 153184 examples: {'rewards_train/chosen': '0.095169', 'rewards_train/rejected': '0.020617', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074552', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-130.82', 'loss/train': '0.67198', 'examples_per_second': '32.797', 'grad_norm': '77.5', 'counters/examples': 153184, 'counters/updates': 4787}
skipping logging after 153216 examples to avoid logging too frequently
train stats after 153248 examples: {'rewards_train/chosen': '0.077279', 'rewards_train/rejected': '0.09469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017411', 'logps_train/rejected': '-156.17', 'logps_train/chosen': '-180.32', 'loss/train': '0.70994', 'examples_per_second': '31.334', 'grad_norm': '130', 'counters/examples': 153248, 'counters/updates': 4789}
train stats after 153280 examples: {'rewards_train/chosen': '0.047232', 'rewards_train/rejected': '0.090538', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043305', 'logps_train/rejected': '-113.42', 'logps_train/chosen': '-101.16', 'loss/train': '0.72776', 'examples_per_second': '31.502', 'grad_norm': '74', 'counters/examples': 153280, 'counters/updates': 4790}
train stats after 153312 examples: {'rewards_train/chosen': '0.09138', 'rewards_train/rejected': '0.15713', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.065746', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-157.57', 'loss/train': '0.73505', 'examples_per_second': '31.568', 'grad_norm': '109.5', 'counters/examples': 153312, 'counters/updates': 4791}
train stats after 153344 examples: {'rewards_train/chosen': '0.050605', 'rewards_train/rejected': '0.071278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020673', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-178.86', 'loss/train': '0.71802', 'examples_per_second': '30.939', 'grad_norm': '90.5', 'counters/examples': 153344, 'counters/updates': 4792}
skipping logging after 153376 examples to avoid logging too frequently
train stats after 153408 examples: {'rewards_train/chosen': '0.066627', 'rewards_train/rejected': '0.043958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022669', 'logps_train/rejected': '-110.02', 'logps_train/chosen': '-112.25', 'loss/train': '0.68934', 'examples_per_second': '35.943', 'grad_norm': '61.25', 'counters/examples': 153408, 'counters/updates': 4794}
train stats after 153440 examples: {'rewards_train/chosen': '0.076559', 'rewards_train/rejected': '-0.01268', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089239', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-165.11', 'loss/train': '0.65538', 'examples_per_second': '30.763', 'grad_norm': '69.5', 'counters/examples': 153440, 'counters/updates': 4795}
train stats after 153472 examples: {'rewards_train/chosen': '0.12426', 'rewards_train/rejected': '0.078752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045505', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-166.27', 'loss/train': '0.68789', 'examples_per_second': '31.535', 'grad_norm': '89.5', 'counters/examples': 153472, 'counters/updates': 4796}
skipping logging after 153504 examples to avoid logging too frequently
train stats after 153536 examples: {'rewards_train/chosen': '0.11546', 'rewards_train/rejected': '0.040447', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075017', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-162.44', 'loss/train': '0.67728', 'examples_per_second': '34.195', 'grad_norm': '322', 'counters/examples': 153536, 'counters/updates': 4798}
skipping logging after 153568 examples to avoid logging too frequently
train stats after 153600 examples: {'rewards_train/chosen': '0.0048152', 'rewards_train/rejected': '-0.020638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025453', 'logps_train/rejected': '-93.087', 'logps_train/chosen': '-99.074', 'loss/train': '0.68613', 'examples_per_second': '33.196', 'grad_norm': '90.5', 'counters/examples': 153600, 'counters/updates': 4800}
train stats after 153632 examples: {'rewards_train/chosen': '0.14659', 'rewards_train/rejected': '0.013974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13262', 'logps_train/rejected': '-100.88', 'logps_train/chosen': '-159.34', 'loss/train': '0.65092', 'examples_per_second': '31.147', 'grad_norm': '103', 'counters/examples': 153632, 'counters/updates': 4801}
train stats after 153664 examples: {'rewards_train/chosen': '0.041069', 'rewards_train/rejected': '0.076189', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035119', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-192.24', 'loss/train': '0.72458', 'examples_per_second': '30.017', 'grad_norm': '107.5', 'counters/examples': 153664, 'counters/updates': 4802}
train stats after 153696 examples: {'rewards_train/chosen': '0.1818', 'rewards_train/rejected': '0.13985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04195', 'logps_train/rejected': '-166.94', 'logps_train/chosen': '-226.13', 'loss/train': '0.69788', 'examples_per_second': '31.478', 'grad_norm': '152', 'counters/examples': 153696, 'counters/updates': 4803}
train stats after 153728 examples: {'rewards_train/chosen': '0.09106', 'rewards_train/rejected': '0.024994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066066', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-145.25', 'loss/train': '0.67652', 'examples_per_second': '30.551', 'grad_norm': '72', 'counters/examples': 153728, 'counters/updates': 4804}
skipping logging after 153760 examples to avoid logging too frequently
train stats after 153792 examples: {'rewards_train/chosen': '0.096379', 'rewards_train/rejected': '0.11198', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015603', 'logps_train/rejected': '-131.5', 'logps_train/chosen': '-181.05', 'loss/train': '0.70973', 'examples_per_second': '32.632', 'grad_norm': '106.5', 'counters/examples': 153792, 'counters/updates': 4806}
train stats after 153824 examples: {'rewards_train/chosen': '0.20781', 'rewards_train/rejected': '0.029948', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17787', 'logps_train/rejected': '-163.62', 'logps_train/chosen': '-149.8', 'loss/train': '0.6179', 'examples_per_second': '29.855', 'grad_norm': '87.5', 'counters/examples': 153824, 'counters/updates': 4807}
train stats after 153856 examples: {'rewards_train/chosen': '0.11637', 'rewards_train/rejected': '0.096971', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019394', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-181.36', 'loss/train': '0.70236', 'examples_per_second': '31.552', 'grad_norm': '90.5', 'counters/examples': 153856, 'counters/updates': 4808}
train stats after 153888 examples: {'rewards_train/chosen': '0.10496', 'rewards_train/rejected': '0.074487', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030473', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-171.37', 'loss/train': '0.69576', 'examples_per_second': '30.578', 'grad_norm': '89.5', 'counters/examples': 153888, 'counters/updates': 4809}
train stats after 153920 examples: {'rewards_train/chosen': '0.19287', 'rewards_train/rejected': '0.036024', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15684', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-151.04', 'loss/train': '0.62871', 'examples_per_second': '30.069', 'grad_norm': '141', 'counters/examples': 153920, 'counters/updates': 4810}
train stats after 153952 examples: {'rewards_train/chosen': '0.069892', 'rewards_train/rejected': '0.015995', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053897', 'logps_train/rejected': '-101.87', 'logps_train/chosen': '-123.11', 'loss/train': '0.6728', 'examples_per_second': '31.794', 'grad_norm': '57.5', 'counters/examples': 153952, 'counters/updates': 4811}
train stats after 153984 examples: {'rewards_train/chosen': '0.1297', 'rewards_train/rejected': '0.0070022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12269', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-170.48', 'loss/train': '0.6507', 'examples_per_second': '31.032', 'grad_norm': '149', 'counters/examples': 153984, 'counters/updates': 4812}
train stats after 154016 examples: {'rewards_train/chosen': '0.1234', 'rewards_train/rejected': '0.093918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029482', 'logps_train/rejected': '-150.89', 'logps_train/chosen': '-131.77', 'loss/train': '0.69375', 'examples_per_second': '30.069', 'grad_norm': '80.5', 'counters/examples': 154016, 'counters/updates': 4813}
train stats after 154048 examples: {'rewards_train/chosen': '0.10614', 'rewards_train/rejected': '0.1129', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.006757', 'logps_train/rejected': '-161.63', 'logps_train/chosen': '-160.14', 'loss/train': '0.71143', 'examples_per_second': '30.038', 'grad_norm': '60.5', 'counters/examples': 154048, 'counters/updates': 4814}
train stats after 154080 examples: {'rewards_train/chosen': '0.090622', 'rewards_train/rejected': '0.046213', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044409', 'logps_train/rejected': '-155.08', 'logps_train/chosen': '-159.15', 'loss/train': '0.68816', 'examples_per_second': '30.057', 'grad_norm': '75.5', 'counters/examples': 154080, 'counters/updates': 4815}
skipping logging after 154112 examples to avoid logging too frequently
train stats after 154144 examples: {'rewards_train/chosen': '0.070148', 'rewards_train/rejected': '0.012009', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058139', 'logps_train/rejected': '-158.85', 'logps_train/chosen': '-139.05', 'loss/train': '0.67632', 'examples_per_second': '31.531', 'grad_norm': '77', 'counters/examples': 154144, 'counters/updates': 4817}
skipping logging after 154176 examples to avoid logging too frequently
train stats after 154208 examples: {'rewards_train/chosen': '0.015032', 'rewards_train/rejected': '0.03714', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.022108', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-136.13', 'loss/train': '0.71522', 'examples_per_second': '30.108', 'grad_norm': '82', 'counters/examples': 154208, 'counters/updates': 4819}
train stats after 154240 examples: {'rewards_train/chosen': '0.082612', 'rewards_train/rejected': '0.15272', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.070113', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-149.59', 'loss/train': '0.74211', 'examples_per_second': '30.071', 'grad_norm': '82', 'counters/examples': 154240, 'counters/updates': 4820}
skipping logging after 154272 examples to avoid logging too frequently
train stats after 154304 examples: {'rewards_train/chosen': '0.086354', 'rewards_train/rejected': '0.028287', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058067', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-119.27', 'loss/train': '0.67702', 'examples_per_second': '34.323', 'grad_norm': '126', 'counters/examples': 154304, 'counters/updates': 4822}
train stats after 154336 examples: {'rewards_train/chosen': '0.11872', 'rewards_train/rejected': '0.029451', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.089269', 'logps_train/rejected': '-120.31', 'logps_train/chosen': '-186.33', 'loss/train': '0.65791', 'examples_per_second': '30.129', 'grad_norm': '72', 'counters/examples': 154336, 'counters/updates': 4823}
train stats after 154368 examples: {'rewards_train/chosen': '0.028261', 'rewards_train/rejected': '0.024702', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.003559', 'logps_train/rejected': '-188.12', 'logps_train/chosen': '-151.47', 'loss/train': '0.71635', 'examples_per_second': '31.517', 'grad_norm': '157', 'counters/examples': 154368, 'counters/updates': 4824}
skipping logging after 154400 examples to avoid logging too frequently
train stats after 154432 examples: {'rewards_train/chosen': '0.16226', 'rewards_train/rejected': '0.14571', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016548', 'logps_train/rejected': '-160.33', 'logps_train/chosen': '-147.12', 'loss/train': '0.69632', 'examples_per_second': '31.767', 'grad_norm': '49.75', 'counters/examples': 154432, 'counters/updates': 4826}
train stats after 154464 examples: {'rewards_train/chosen': '0.0037605', 'rewards_train/rejected': '0.044209', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.040449', 'logps_train/rejected': '-96.863', 'logps_train/chosen': '-139.35', 'loss/train': '0.7234', 'examples_per_second': '32.122', 'grad_norm': '85.5', 'counters/examples': 154464, 'counters/updates': 4827}
train stats after 154496 examples: {'rewards_train/chosen': '0.1282', 'rewards_train/rejected': '0.052787', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075418', 'logps_train/rejected': '-157.31', 'logps_train/chosen': '-173.11', 'loss/train': '0.68172', 'examples_per_second': '31.511', 'grad_norm': '84', 'counters/examples': 154496, 'counters/updates': 4828}
train stats after 154528 examples: {'rewards_train/chosen': '0.051204', 'rewards_train/rejected': '0.035015', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01619', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-158.12', 'loss/train': '0.69617', 'examples_per_second': '31.552', 'grad_norm': '135', 'counters/examples': 154528, 'counters/updates': 4829}
train stats after 154560 examples: {'rewards_train/chosen': '0.17614', 'rewards_train/rejected': '0.02929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14685', 'logps_train/rejected': '-107.22', 'logps_train/chosen': '-150.79', 'loss/train': '0.64108', 'examples_per_second': '31.796', 'grad_norm': '144', 'counters/examples': 154560, 'counters/updates': 4830}
train stats after 154592 examples: {'rewards_train/chosen': '0.13002', 'rewards_train/rejected': '0.012209', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11781', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-141.02', 'loss/train': '0.64465', 'examples_per_second': '30.588', 'grad_norm': '87.5', 'counters/examples': 154592, 'counters/updates': 4831}
train stats after 154624 examples: {'rewards_train/chosen': '0.056415', 'rewards_train/rejected': '0.07045', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014035', 'logps_train/rejected': '-152.27', 'logps_train/chosen': '-153.58', 'loss/train': '0.71373', 'examples_per_second': '31.551', 'grad_norm': '107.5', 'counters/examples': 154624, 'counters/updates': 4832}
skipping logging after 154656 examples to avoid logging too frequently
train stats after 154688 examples: {'rewards_train/chosen': '0.09342', 'rewards_train/rejected': '0.090069', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0033512', 'logps_train/rejected': '-158.03', 'logps_train/chosen': '-174.88', 'loss/train': '0.70588', 'examples_per_second': '30.801', 'grad_norm': '98', 'counters/examples': 154688, 'counters/updates': 4834}
train stats after 154720 examples: {'rewards_train/chosen': '0.067417', 'rewards_train/rejected': '0.026269', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041148', 'logps_train/rejected': '-146.1', 'logps_train/chosen': '-141.55', 'loss/train': '0.68567', 'examples_per_second': '32.501', 'grad_norm': '105.5', 'counters/examples': 154720, 'counters/updates': 4835}
train stats after 154752 examples: {'rewards_train/chosen': '0.060737', 'rewards_train/rejected': '0.0055996', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055137', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-145.77', 'loss/train': '0.68', 'examples_per_second': '31.409', 'grad_norm': '83', 'counters/examples': 154752, 'counters/updates': 4836}
train stats after 154784 examples: {'rewards_train/chosen': '0.063228', 'rewards_train/rejected': '0.046728', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0165', 'logps_train/rejected': '-144.18', 'logps_train/chosen': '-176.06', 'loss/train': '0.69842', 'examples_per_second': '23.911', 'grad_norm': '92.5', 'counters/examples': 154784, 'counters/updates': 4837}
train stats after 154816 examples: {'rewards_train/chosen': '0.13961', 'rewards_train/rejected': '0.16624', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026629', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-165.73', 'loss/train': '0.71727', 'examples_per_second': '32.566', 'grad_norm': '93', 'counters/examples': 154816, 'counters/updates': 4838}
train stats after 154848 examples: {'rewards_train/chosen': '0.020056', 'rewards_train/rejected': '0.06634', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046284', 'logps_train/rejected': '-172.4', 'logps_train/chosen': '-153.5', 'loss/train': '0.72795', 'examples_per_second': '30.984', 'grad_norm': '76.5', 'counters/examples': 154848, 'counters/updates': 4839}
train stats after 154880 examples: {'rewards_train/chosen': '0.13687', 'rewards_train/rejected': '0.063623', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073242', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-178.48', 'loss/train': '0.66798', 'examples_per_second': '31.301', 'grad_norm': '127.5', 'counters/examples': 154880, 'counters/updates': 4840}
train stats after 154912 examples: {'rewards_train/chosen': '0.044352', 'rewards_train/rejected': '0.084576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.040223', 'logps_train/rejected': '-135.75', 'logps_train/chosen': '-146.23', 'loss/train': '0.72837', 'examples_per_second': '32.216', 'grad_norm': '82', 'counters/examples': 154912, 'counters/updates': 4841}
train stats after 154944 examples: {'rewards_train/chosen': '0.12518', 'rewards_train/rejected': '0.052649', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072528', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-177.29', 'loss/train': '0.67344', 'examples_per_second': '31.418', 'grad_norm': '89', 'counters/examples': 154944, 'counters/updates': 4842}
train stats after 154976 examples: {'rewards_train/chosen': '0.089441', 'rewards_train/rejected': '0.086807', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0026345', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-126.55', 'loss/train': '0.71298', 'examples_per_second': '32.38', 'grad_norm': '330', 'counters/examples': 154976, 'counters/updates': 4843}
train stats after 155008 examples: {'rewards_train/chosen': '0.13967', 'rewards_train/rejected': '0.0079896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13168', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-148.89', 'loss/train': '0.64032', 'examples_per_second': '30.62', 'grad_norm': '109.5', 'counters/examples': 155008, 'counters/updates': 4844}
skipping logging after 155040 examples to avoid logging too frequently
train stats after 155072 examples: {'rewards_train/chosen': '0.10086', 'rewards_train/rejected': '0.042449', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058412', 'logps_train/rejected': '-169.2', 'logps_train/chosen': '-166.46', 'loss/train': '0.68548', 'examples_per_second': '32.332', 'grad_norm': '119.5', 'counters/examples': 155072, 'counters/updates': 4846}
skipping logging after 155104 examples to avoid logging too frequently
train stats after 155136 examples: {'rewards_train/chosen': '0.11193', 'rewards_train/rejected': '0.024918', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087016', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-138.94', 'loss/train': '0.66498', 'examples_per_second': '31.955', 'grad_norm': '85', 'counters/examples': 155136, 'counters/updates': 4848}
train stats after 155168 examples: {'rewards_train/chosen': '0.09751', 'rewards_train/rejected': '0.094686', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0028243', 'logps_train/rejected': '-148.94', 'logps_train/chosen': '-188.1', 'loss/train': '0.70783', 'examples_per_second': '31.593', 'grad_norm': '71', 'counters/examples': 155168, 'counters/updates': 4849}
train stats after 155200 examples: {'rewards_train/chosen': '0.1602', 'rewards_train/rejected': '0.028762', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13144', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-159.05', 'loss/train': '0.66197', 'examples_per_second': '30.105', 'grad_norm': '64', 'counters/examples': 155200, 'counters/updates': 4850}
skipping logging after 155232 examples to avoid logging too frequently
train stats after 155264 examples: {'rewards_train/chosen': '0.097476', 'rewards_train/rejected': '0.051626', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04585', 'logps_train/rejected': '-113.29', 'logps_train/chosen': '-135.91', 'loss/train': '0.67724', 'examples_per_second': '31.559', 'grad_norm': '135', 'counters/examples': 155264, 'counters/updates': 4852}
skipping logging after 155296 examples to avoid logging too frequently
train stats after 155328 examples: {'rewards_train/chosen': '0.094703', 'rewards_train/rejected': '-0.066231', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16093', 'logps_train/rejected': '-89.002', 'logps_train/chosen': '-133.91', 'loss/train': '0.62812', 'examples_per_second': '31.176', 'grad_norm': '51', 'counters/examples': 155328, 'counters/updates': 4854}
skipping logging after 155360 examples to avoid logging too frequently
train stats after 155392 examples: {'rewards_train/chosen': '0.15885', 'rewards_train/rejected': '0.050188', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10866', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-187.06', 'loss/train': '0.66739', 'examples_per_second': '31.467', 'grad_norm': '72.5', 'counters/examples': 155392, 'counters/updates': 4856}
train stats after 155424 examples: {'rewards_train/chosen': '0.1293', 'rewards_train/rejected': '0.077536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051768', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-144.91', 'loss/train': '0.68458', 'examples_per_second': '30.564', 'grad_norm': '79.5', 'counters/examples': 155424, 'counters/updates': 4857}
train stats after 155456 examples: {'rewards_train/chosen': '0.024385', 'rewards_train/rejected': '0.033485', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0090996', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-111.06', 'loss/train': '0.70632', 'examples_per_second': '30.647', 'grad_norm': '73.5', 'counters/examples': 155456, 'counters/updates': 4858}
train stats after 155488 examples: {'rewards_train/chosen': '0.05379', 'rewards_train/rejected': '0.016353', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037437', 'logps_train/rejected': '-155.64', 'logps_train/chosen': '-161.04', 'loss/train': '0.69051', 'examples_per_second': '30.562', 'grad_norm': '61.5', 'counters/examples': 155488, 'counters/updates': 4859}
train stats after 155520 examples: {'rewards_train/chosen': '0.22349', 'rewards_train/rejected': '0.025374', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19812', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-153.49', 'loss/train': '0.61655', 'examples_per_second': '29.95', 'grad_norm': '54.5', 'counters/examples': 155520, 'counters/updates': 4860}
train stats after 155552 examples: {'rewards_train/chosen': '0.097476', 'rewards_train/rejected': '0.0014561', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09602', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-141.1', 'loss/train': '0.65885', 'examples_per_second': '32.086', 'grad_norm': '72', 'counters/examples': 155552, 'counters/updates': 4861}
train stats after 155584 examples: {'rewards_train/chosen': '0.021625', 'rewards_train/rejected': '0.083436', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.061811', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-160.19', 'loss/train': '0.7389', 'examples_per_second': '32.075', 'grad_norm': '246', 'counters/examples': 155584, 'counters/updates': 4862}
train stats after 155616 examples: {'rewards_train/chosen': '0.088831', 'rewards_train/rejected': '0.04604', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042792', 'logps_train/rejected': '-159.31', 'logps_train/chosen': '-145.95', 'loss/train': '0.6845', 'examples_per_second': '31.534', 'grad_norm': '70.5', 'counters/examples': 155616, 'counters/updates': 4863}
train stats after 155648 examples: {'rewards_train/chosen': '0.19344', 'rewards_train/rejected': '0.072068', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12137', 'logps_train/rejected': '-175.75', 'logps_train/chosen': '-154.45', 'loss/train': '0.64634', 'examples_per_second': '31.54', 'grad_norm': '118', 'counters/examples': 155648, 'counters/updates': 4864}
train stats after 155680 examples: {'rewards_train/chosen': '0.12021', 'rewards_train/rejected': '0.051583', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068628', 'logps_train/rejected': '-152.32', 'logps_train/chosen': '-169.51', 'loss/train': '0.6674', 'examples_per_second': '32.402', 'grad_norm': '117', 'counters/examples': 155680, 'counters/updates': 4865}
skipping logging after 155712 examples to avoid logging too frequently
train stats after 155744 examples: {'rewards_train/chosen': '0.14749', 'rewards_train/rejected': '0.034655', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11283', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-157.25', 'loss/train': '0.6539', 'examples_per_second': '33.675', 'grad_norm': '58.5', 'counters/examples': 155744, 'counters/updates': 4867}
train stats after 155776 examples: {'rewards_train/chosen': '0.12745', 'rewards_train/rejected': '-0.0086423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13609', 'logps_train/rejected': '-154.31', 'logps_train/chosen': '-177.7', 'loss/train': '0.65697', 'examples_per_second': '32.339', 'grad_norm': '72', 'counters/examples': 155776, 'counters/updates': 4868}
train stats after 155808 examples: {'rewards_train/chosen': '0.19433', 'rewards_train/rejected': '0.068121', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12621', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-153.84', 'loss/train': '0.65921', 'examples_per_second': '31.013', 'grad_norm': '125.5', 'counters/examples': 155808, 'counters/updates': 4869}
train stats after 155840 examples: {'rewards_train/chosen': '0.087784', 'rewards_train/rejected': '-0.042774', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13056', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-141.48', 'loss/train': '0.6438', 'examples_per_second': '32.227', 'grad_norm': '90.5', 'counters/examples': 155840, 'counters/updates': 4870}
train stats after 155872 examples: {'rewards_train/chosen': '0.15305', 'rewards_train/rejected': '0.071829', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08122', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-162.79', 'loss/train': '0.66954', 'examples_per_second': '31.527', 'grad_norm': '116.5', 'counters/examples': 155872, 'counters/updates': 4871}
skipping logging after 155904 examples to avoid logging too frequently
train stats after 155936 examples: {'rewards_train/chosen': '0.093577', 'rewards_train/rejected': '0.035713', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057864', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-150.22', 'loss/train': '0.67528', 'examples_per_second': '30.879', 'grad_norm': '73', 'counters/examples': 155936, 'counters/updates': 4873}
train stats after 155968 examples: {'rewards_train/chosen': '0.21691', 'rewards_train/rejected': '0.079841', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13707', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-196.69', 'loss/train': '0.65471', 'examples_per_second': '31.549', 'grad_norm': '91', 'counters/examples': 155968, 'counters/updates': 4874}
train stats after 156000 examples: {'rewards_train/chosen': '0.13823', 'rewards_train/rejected': '0.08612', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052106', 'logps_train/rejected': '-192.07', 'logps_train/chosen': '-175.09', 'loss/train': '0.67885', 'examples_per_second': '30.588', 'grad_norm': '118.5', 'counters/examples': 156000, 'counters/updates': 4875}
Running evaluation after 156000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 156000: {'rewards_eval/chosen': '0.10958', 'rewards_eval/rejected': '0.05108', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.058504', 'logps_eval/rejected': '-127.63', 'logps_eval/chosen': '-149.91', 'loss/eval': '0.67817'}
skipping save for non epoch
train stats after 156032 examples: {'rewards_train/chosen': '0.070298', 'rewards_train/rejected': '0.082784', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012486', 'logps_train/rejected': '-121.7', 'logps_train/chosen': '-140.89', 'loss/train': '0.7111', 'examples_per_second': '33.856', 'grad_norm': '68', 'counters/examples': 156032, 'counters/updates': 4876}
train stats after 156064 examples: {'rewards_train/chosen': '0.1309', 'rewards_train/rejected': '0.072503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058398', 'logps_train/rejected': '-121.65', 'logps_train/chosen': '-138.18', 'loss/train': '0.67992', 'examples_per_second': '30.881', 'grad_norm': '116.5', 'counters/examples': 156064, 'counters/updates': 4877}
skipping logging after 156096 examples to avoid logging too frequently
train stats after 156128 examples: {'rewards_train/chosen': '0.097478', 'rewards_train/rejected': '0.045905', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051573', 'logps_train/rejected': '-137.59', 'logps_train/chosen': '-130.93', 'loss/train': '0.68742', 'examples_per_second': '32.535', 'grad_norm': '56.5', 'counters/examples': 156128, 'counters/updates': 4879}
skipping logging after 156160 examples to avoid logging too frequently
train stats after 156192 examples: {'rewards_train/chosen': '0.040778', 'rewards_train/rejected': '0.039152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0016261', 'logps_train/rejected': '-158.91', 'logps_train/chosen': '-125.65', 'loss/train': '0.70792', 'examples_per_second': '31.932', 'grad_norm': '105', 'counters/examples': 156192, 'counters/updates': 4881}
train stats after 156224 examples: {'rewards_train/chosen': '0.097961', 'rewards_train/rejected': '0.075547', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022413', 'logps_train/rejected': '-117.36', 'logps_train/chosen': '-127.79', 'loss/train': '0.70001', 'examples_per_second': '31.399', 'grad_norm': '81.5', 'counters/examples': 156224, 'counters/updates': 4882}
skipping logging after 156256 examples to avoid logging too frequently
train stats after 156288 examples: {'rewards_train/chosen': '0.047386', 'rewards_train/rejected': '0.025416', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02197', 'logps_train/rejected': '-108.19', 'logps_train/chosen': '-113.29', 'loss/train': '0.68713', 'examples_per_second': '40.27', 'grad_norm': '76', 'counters/examples': 156288, 'counters/updates': 4884}
train stats after 156320 examples: {'rewards_train/chosen': '0.051603', 'rewards_train/rejected': '-0.0076996', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059303', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-164.02', 'loss/train': '0.68435', 'examples_per_second': '31.568', 'grad_norm': '103.5', 'counters/examples': 156320, 'counters/updates': 4885}
skipping logging after 156352 examples to avoid logging too frequently
train stats after 156384 examples: {'rewards_train/chosen': '0.13256', 'rewards_train/rejected': '0.036299', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09626', 'logps_train/rejected': '-127.79', 'logps_train/chosen': '-154.72', 'loss/train': '0.67098', 'examples_per_second': '31.579', 'grad_norm': '107.5', 'counters/examples': 156384, 'counters/updates': 4887}
train stats after 156416 examples: {'rewards_train/chosen': '0.15455', 'rewards_train/rejected': '-0.004492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15904', 'logps_train/rejected': '-92.006', 'logps_train/chosen': '-156.38', 'loss/train': '0.63469', 'examples_per_second': '32.371', 'grad_norm': '197', 'counters/examples': 156416, 'counters/updates': 4888}
train stats after 156448 examples: {'rewards_train/chosen': '0.073548', 'rewards_train/rejected': '0.0083751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065173', 'logps_train/rejected': '-160.92', 'logps_train/chosen': '-171.63', 'loss/train': '0.68025', 'examples_per_second': '30.392', 'grad_norm': '103', 'counters/examples': 156448, 'counters/updates': 4889}
skipping logging after 156480 examples to avoid logging too frequently
train stats after 156512 examples: {'rewards_train/chosen': '0.074494', 'rewards_train/rejected': '0.051937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022557', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-177.75', 'loss/train': '0.69871', 'examples_per_second': '31.59', 'grad_norm': '109.5', 'counters/examples': 156512, 'counters/updates': 4891}
train stats after 156544 examples: {'rewards_train/chosen': '0.13312', 'rewards_train/rejected': '0.0051341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12798', 'logps_train/rejected': '-100.2', 'logps_train/chosen': '-192.43', 'loss/train': '0.64393', 'examples_per_second': '32.072', 'grad_norm': '85.5', 'counters/examples': 156544, 'counters/updates': 4892}
train stats after 156576 examples: {'rewards_train/chosen': '0.044371', 'rewards_train/rejected': '-0.018664', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063036', 'logps_train/rejected': '-142.03', 'logps_train/chosen': '-181.66', 'loss/train': '0.67474', 'examples_per_second': '29.907', 'grad_norm': '91', 'counters/examples': 156576, 'counters/updates': 4893}
train stats after 156608 examples: {'rewards_train/chosen': '0.089415', 'rewards_train/rejected': '0.11423', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.02481', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-170.3', 'loss/train': '0.71836', 'examples_per_second': '31.599', 'grad_norm': '147', 'counters/examples': 156608, 'counters/updates': 4894}
train stats after 156640 examples: {'rewards_train/chosen': '0.069511', 'rewards_train/rejected': '0.059141', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01037', 'logps_train/rejected': '-125.54', 'logps_train/chosen': '-148.61', 'loss/train': '0.69834', 'examples_per_second': '30.577', 'grad_norm': '72.5', 'counters/examples': 156640, 'counters/updates': 4895}
skipping logging after 156672 examples to avoid logging too frequently
train stats after 156704 examples: {'rewards_train/chosen': '0.15912', 'rewards_train/rejected': '0.019031', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14009', 'logps_train/rejected': '-157.08', 'logps_train/chosen': '-188.31', 'loss/train': '0.63613', 'examples_per_second': '29.999', 'grad_norm': '62.75', 'counters/examples': 156704, 'counters/updates': 4897}
train stats after 156736 examples: {'rewards_train/chosen': '0.08816', 'rewards_train/rejected': '0.23673', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.14857', 'logps_train/rejected': '-157.56', 'logps_train/chosen': '-159.25', 'loss/train': '0.80774', 'examples_per_second': '29.978', 'grad_norm': '174', 'counters/examples': 156736, 'counters/updates': 4898}
train stats after 156768 examples: {'rewards_train/chosen': '0.023701', 'rewards_train/rejected': '0.040858', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017157', 'logps_train/rejected': '-128.05', 'logps_train/chosen': '-135.08', 'loss/train': '0.71472', 'examples_per_second': '31.525', 'grad_norm': '98', 'counters/examples': 156768, 'counters/updates': 4899}
train stats after 156800 examples: {'rewards_train/chosen': '0.21398', 'rewards_train/rejected': '0.17961', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034372', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-204.19', 'loss/train': '0.69377', 'examples_per_second': '30.53', 'grad_norm': '86', 'counters/examples': 156800, 'counters/updates': 4900}
skipping logging after 156832 examples to avoid logging too frequently
train stats after 156864 examples: {'rewards_train/chosen': '0.084903', 'rewards_train/rejected': '0.030759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054144', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-147.67', 'loss/train': '0.68342', 'examples_per_second': '32.049', 'grad_norm': '72', 'counters/examples': 156864, 'counters/updates': 4902}
train stats after 156896 examples: {'rewards_train/chosen': '0.087712', 'rewards_train/rejected': '0.10679', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019076', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-161.27', 'loss/train': '0.7125', 'examples_per_second': '31.449', 'grad_norm': '70.5', 'counters/examples': 156896, 'counters/updates': 4903}
train stats after 156928 examples: {'rewards_train/chosen': '0.10539', 'rewards_train/rejected': '0.052762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052628', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-127.76', 'loss/train': '0.67491', 'examples_per_second': '32.687', 'grad_norm': '99.5', 'counters/examples': 156928, 'counters/updates': 4904}
skipping logging after 156960 examples to avoid logging too frequently
train stats after 156992 examples: {'rewards_train/chosen': '0.084657', 'rewards_train/rejected': '-0.0026411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087298', 'logps_train/rejected': '-85.801', 'logps_train/chosen': '-128.54', 'loss/train': '0.66124', 'examples_per_second': '34.143', 'grad_norm': '51.25', 'counters/examples': 156992, 'counters/updates': 4906}
skipping logging after 157024 examples to avoid logging too frequently
train stats after 157056 examples: {'rewards_train/chosen': '0.19769', 'rewards_train/rejected': '0.079649', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11804', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-153.94', 'loss/train': '0.64346', 'examples_per_second': '31.502', 'grad_norm': '208', 'counters/examples': 157056, 'counters/updates': 4908}
train stats after 157088 examples: {'rewards_train/chosen': '0.14378', 'rewards_train/rejected': '0.058686', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085096', 'logps_train/rejected': '-185.73', 'logps_train/chosen': '-197.42', 'loss/train': '0.6716', 'examples_per_second': '30.535', 'grad_norm': '146', 'counters/examples': 157088, 'counters/updates': 4909}
train stats after 157120 examples: {'rewards_train/chosen': '0.018478', 'rewards_train/rejected': '0.065975', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.047497', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-129.12', 'loss/train': '0.72323', 'examples_per_second': '31.526', 'grad_norm': '127.5', 'counters/examples': 157120, 'counters/updates': 4910}
train stats after 157152 examples: {'rewards_train/chosen': '0.14679', 'rewards_train/rejected': '0.14017', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0066131', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-159.83', 'loss/train': '0.72003', 'examples_per_second': '31.373', 'grad_norm': '125.5', 'counters/examples': 157152, 'counters/updates': 4911}
train stats after 157184 examples: {'rewards_train/chosen': '0.12329', 'rewards_train/rejected': '0.039388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083905', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-151.73', 'loss/train': '0.66457', 'examples_per_second': '31.528', 'grad_norm': '55.25', 'counters/examples': 157184, 'counters/updates': 4912}
train stats after 157216 examples: {'rewards_train/chosen': '0.10038', 'rewards_train/rejected': '0.11762', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017239', 'logps_train/rejected': '-159.07', 'logps_train/chosen': '-169.64', 'loss/train': '0.72045', 'examples_per_second': '30.211', 'grad_norm': '115', 'counters/examples': 157216, 'counters/updates': 4913}
train stats after 157248 examples: {'rewards_train/chosen': '0.074778', 'rewards_train/rejected': '-0.02175', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096528', 'logps_train/rejected': '-115.27', 'logps_train/chosen': '-125.55', 'loss/train': '0.65215', 'examples_per_second': '31.087', 'grad_norm': '80', 'counters/examples': 157248, 'counters/updates': 4914}
train stats after 157280 examples: {'rewards_train/chosen': '0.086004', 'rewards_train/rejected': '0.033433', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052571', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-127.78', 'loss/train': '0.676', 'examples_per_second': '30.725', 'grad_norm': '48.25', 'counters/examples': 157280, 'counters/updates': 4915}
train stats after 157312 examples: {'rewards_train/chosen': '0.10982', 'rewards_train/rejected': '0.11409', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0042726', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-152.02', 'loss/train': '0.71417', 'examples_per_second': '31.576', 'grad_norm': '83.5', 'counters/examples': 157312, 'counters/updates': 4916}
train stats after 157344 examples: {'rewards_train/chosen': '0.11518', 'rewards_train/rejected': '0.080172', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035009', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-162.06', 'loss/train': '0.68703', 'examples_per_second': '31.562', 'grad_norm': '73', 'counters/examples': 157344, 'counters/updates': 4917}
skipping logging after 157376 examples to avoid logging too frequently
train stats after 157408 examples: {'rewards_train/chosen': '0.033015', 'rewards_train/rejected': '-0.010884', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043899', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-149.34', 'loss/train': '0.68665', 'examples_per_second': '23.892', 'grad_norm': '84.5', 'counters/examples': 157408, 'counters/updates': 4919}
train stats after 157440 examples: {'rewards_train/chosen': '0.071283', 'rewards_train/rejected': '-0.0094214', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080704', 'logps_train/rejected': '-90.431', 'logps_train/chosen': '-133.25', 'loss/train': '0.66353', 'examples_per_second': '31.895', 'grad_norm': '184', 'counters/examples': 157440, 'counters/updates': 4920}
train stats after 157472 examples: {'rewards_train/chosen': '0.046568', 'rewards_train/rejected': '0.036938', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0096306', 'logps_train/rejected': '-96.725', 'logps_train/chosen': '-126.14', 'loss/train': '0.70017', 'examples_per_second': '32.95', 'grad_norm': '81', 'counters/examples': 157472, 'counters/updates': 4921}
train stats after 157504 examples: {'rewards_train/chosen': '0.13275', 'rewards_train/rejected': '0.039419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093335', 'logps_train/rejected': '-100.36', 'logps_train/chosen': '-178.29', 'loss/train': '0.66596', 'examples_per_second': '23.821', 'grad_norm': '165', 'counters/examples': 157504, 'counters/updates': 4922}
train stats after 157536 examples: {'rewards_train/chosen': '0.079497', 'rewards_train/rejected': '0.051928', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027569', 'logps_train/rejected': '-97.453', 'logps_train/chosen': '-145.67', 'loss/train': '0.70289', 'examples_per_second': '31.26', 'grad_norm': '76.5', 'counters/examples': 157536, 'counters/updates': 4923}
train stats after 157568 examples: {'rewards_train/chosen': '0.10612', 'rewards_train/rejected': '0.083628', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02249', 'logps_train/rejected': '-145.83', 'logps_train/chosen': '-155.2', 'loss/train': '0.69941', 'examples_per_second': '31.473', 'grad_norm': '110.5', 'counters/examples': 157568, 'counters/updates': 4924}
train stats after 157600 examples: {'rewards_train/chosen': '0.085852', 'rewards_train/rejected': '0.041087', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044765', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-184.02', 'loss/train': '0.69462', 'examples_per_second': '31.611', 'grad_norm': '106.5', 'counters/examples': 157600, 'counters/updates': 4925}
train stats after 157632 examples: {'rewards_train/chosen': '0.0018949', 'rewards_train/rejected': '0.089561', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.087666', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-118.77', 'loss/train': '0.75879', 'examples_per_second': '30.097', 'grad_norm': '153', 'counters/examples': 157632, 'counters/updates': 4926}
train stats after 157664 examples: {'rewards_train/chosen': '0.11549', 'rewards_train/rejected': '0.07706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038435', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-143.72', 'loss/train': '0.69054', 'examples_per_second': '30.412', 'grad_norm': '142', 'counters/examples': 157664, 'counters/updates': 4927}
train stats after 157696 examples: {'rewards_train/chosen': '-0.0018848', 'rewards_train/rejected': '0.06527', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.067154', 'logps_train/rejected': '-166.76', 'logps_train/chosen': '-154.23', 'loss/train': '0.74217', 'examples_per_second': '32.291', 'grad_norm': '83', 'counters/examples': 157696, 'counters/updates': 4928}
train stats after 157728 examples: {'rewards_train/chosen': '0.19758', 'rewards_train/rejected': '0.047127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15045', 'logps_train/rejected': '-111.37', 'logps_train/chosen': '-164.1', 'loss/train': '0.64025', 'examples_per_second': '30.297', 'grad_norm': '73', 'counters/examples': 157728, 'counters/updates': 4929}
train stats after 157760 examples: {'rewards_train/chosen': '0.063861', 'rewards_train/rejected': '-0.0030449', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066906', 'logps_train/rejected': '-140.04', 'logps_train/chosen': '-173.56', 'loss/train': '0.67358', 'examples_per_second': '31.564', 'grad_norm': '81', 'counters/examples': 157760, 'counters/updates': 4930}
train stats after 157792 examples: {'rewards_train/chosen': '0.21631', 'rewards_train/rejected': '0.037563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17874', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-172.7', 'loss/train': '0.63764', 'examples_per_second': '30.508', 'grad_norm': '94', 'counters/examples': 157792, 'counters/updates': 4931}
train stats after 157824 examples: {'rewards_train/chosen': '0.058326', 'rewards_train/rejected': '0.042384', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015942', 'logps_train/rejected': '-152.93', 'logps_train/chosen': '-178.79', 'loss/train': '0.72488', 'examples_per_second': '31.571', 'grad_norm': '61.5', 'counters/examples': 157824, 'counters/updates': 4932}
train stats after 157856 examples: {'rewards_train/chosen': '0.044303', 'rewards_train/rejected': '0.034322', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0099818', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-124.34', 'loss/train': '0.70427', 'examples_per_second': '32.014', 'grad_norm': '73', 'counters/examples': 157856, 'counters/updates': 4933}
train stats after 157888 examples: {'rewards_train/chosen': '0.034795', 'rewards_train/rejected': '0.018198', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016597', 'logps_train/rejected': '-156.4', 'logps_train/chosen': '-155.05', 'loss/train': '0.6967', 'examples_per_second': '31.741', 'grad_norm': '111', 'counters/examples': 157888, 'counters/updates': 4934}
skipping logging after 157920 examples to avoid logging too frequently
train stats after 157952 examples: {'rewards_train/chosen': '0.12343', 'rewards_train/rejected': '0.027238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096194', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-145.71', 'loss/train': '0.66565', 'examples_per_second': '31.892', 'grad_norm': '92', 'counters/examples': 157952, 'counters/updates': 4936}
train stats after 157984 examples: {'rewards_train/chosen': '0.074597', 'rewards_train/rejected': '0.14116', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.066563', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-154.32', 'loss/train': '0.74568', 'examples_per_second': '30.724', 'grad_norm': '199', 'counters/examples': 157984, 'counters/updates': 4937}
train stats after 158016 examples: {'rewards_train/chosen': '0.071232', 'rewards_train/rejected': '-0.014852', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086083', 'logps_train/rejected': '-149.98', 'logps_train/chosen': '-197.6', 'loss/train': '0.66074', 'examples_per_second': '30.165', 'grad_norm': '73.5', 'counters/examples': 158016, 'counters/updates': 4938}
skipping logging after 158048 examples to avoid logging too frequently
train stats after 158080 examples: {'rewards_train/chosen': '0.018364', 'rewards_train/rejected': '0.067576', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.049213', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-160.21', 'loss/train': '0.73651', 'examples_per_second': '33.625', 'grad_norm': '121', 'counters/examples': 158080, 'counters/updates': 4940}
train stats after 158112 examples: {'rewards_train/chosen': '0.085932', 'rewards_train/rejected': '0.031429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054504', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-133.45', 'loss/train': '0.67452', 'examples_per_second': '31.17', 'grad_norm': '95', 'counters/examples': 158112, 'counters/updates': 4941}
skipping logging after 158144 examples to avoid logging too frequently
train stats after 158176 examples: {'rewards_train/chosen': '0.034623', 'rewards_train/rejected': '0.052032', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017409', 'logps_train/rejected': '-143.61', 'logps_train/chosen': '-128.12', 'loss/train': '0.71394', 'examples_per_second': '33.245', 'grad_norm': '78', 'counters/examples': 158176, 'counters/updates': 4943}
train stats after 158208 examples: {'rewards_train/chosen': '0.11702', 'rewards_train/rejected': '0.035597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081427', 'logps_train/rejected': '-174.89', 'logps_train/chosen': '-185.09', 'loss/train': '0.68058', 'examples_per_second': '31.14', 'grad_norm': '105.5', 'counters/examples': 158208, 'counters/updates': 4944}
train stats after 158240 examples: {'rewards_train/chosen': '0.10524', 'rewards_train/rejected': '0.072527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032712', 'logps_train/rejected': '-146.95', 'logps_train/chosen': '-194.66', 'loss/train': '0.69507', 'examples_per_second': '30.879', 'grad_norm': '133', 'counters/examples': 158240, 'counters/updates': 4945}
skipping logging after 158272 examples to avoid logging too frequently
train stats after 158304 examples: {'rewards_train/chosen': '0.097779', 'rewards_train/rejected': '0.065884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031895', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-140.7', 'loss/train': '0.69174', 'examples_per_second': '30.561', 'grad_norm': '74.5', 'counters/examples': 158304, 'counters/updates': 4947}
train stats after 158336 examples: {'rewards_train/chosen': '0.092025', 'rewards_train/rejected': '0.045258', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046767', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-144.02', 'loss/train': '0.6814', 'examples_per_second': '32.279', 'grad_norm': '108', 'counters/examples': 158336, 'counters/updates': 4948}
train stats after 158368 examples: {'rewards_train/chosen': '0.11151', 'rewards_train/rejected': '0.075658', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03585', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-171.38', 'loss/train': '0.69672', 'examples_per_second': '30.651', 'grad_norm': '66.5', 'counters/examples': 158368, 'counters/updates': 4949}
train stats after 158400 examples: {'rewards_train/chosen': '0.14971', 'rewards_train/rejected': '0.141', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0087008', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-160.82', 'loss/train': '0.70115', 'examples_per_second': '30.551', 'grad_norm': '73', 'counters/examples': 158400, 'counters/updates': 4950}
train stats after 158432 examples: {'rewards_train/chosen': '0.1267', 'rewards_train/rejected': '0.066319', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06038', 'logps_train/rejected': '-147.19', 'logps_train/chosen': '-161.81', 'loss/train': '0.68733', 'examples_per_second': '31.423', 'grad_norm': '152', 'counters/examples': 158432, 'counters/updates': 4951}
train stats after 158464 examples: {'rewards_train/chosen': '0.089501', 'rewards_train/rejected': '0.072053', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017448', 'logps_train/rejected': '-166.44', 'logps_train/chosen': '-184.46', 'loss/train': '0.69996', 'examples_per_second': '30.951', 'grad_norm': '77', 'counters/examples': 158464, 'counters/updates': 4952}
train stats after 158496 examples: {'rewards_train/chosen': '0.13344', 'rewards_train/rejected': '0.072261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061176', 'logps_train/rejected': '-155.9', 'logps_train/chosen': '-174.44', 'loss/train': '0.67129', 'examples_per_second': '30.49', 'grad_norm': '104.5', 'counters/examples': 158496, 'counters/updates': 4953}
train stats after 158528 examples: {'rewards_train/chosen': '0.10783', 'rewards_train/rejected': '0.088798', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019034', 'logps_train/rejected': '-116.84', 'logps_train/chosen': '-177.93', 'loss/train': '0.6936', 'examples_per_second': '31.852', 'grad_norm': '67.5', 'counters/examples': 158528, 'counters/updates': 4954}
train stats after 158560 examples: {'rewards_train/chosen': '0.026775', 'rewards_train/rejected': '0.075141', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048366', 'logps_train/rejected': '-100.94', 'logps_train/chosen': '-144.42', 'loss/train': '0.72681', 'examples_per_second': '31.566', 'grad_norm': '66', 'counters/examples': 158560, 'counters/updates': 4955}
skipping logging after 158592 examples to avoid logging too frequently
train stats after 158624 examples: {'rewards_train/chosen': '0.21399', 'rewards_train/rejected': '0.046666', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16732', 'logps_train/rejected': '-142.14', 'logps_train/chosen': '-146.69', 'loss/train': '0.63945', 'examples_per_second': '31.288', 'grad_norm': '134', 'counters/examples': 158624, 'counters/updates': 4957}
train stats after 158656 examples: {'rewards_train/chosen': '0.032715', 'rewards_train/rejected': '0.018538', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014177', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-115.59', 'loss/train': '0.70275', 'examples_per_second': '30.884', 'grad_norm': '66', 'counters/examples': 158656, 'counters/updates': 4958}
train stats after 158688 examples: {'rewards_train/chosen': '0.11353', 'rewards_train/rejected': '0.04016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.073371', 'logps_train/rejected': '-116.69', 'logps_train/chosen': '-139.51', 'loss/train': '0.67256', 'examples_per_second': '30.968', 'grad_norm': '76', 'counters/examples': 158688, 'counters/updates': 4959}
train stats after 158720 examples: {'rewards_train/chosen': '0.1205', 'rewards_train/rejected': '0.066892', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053612', 'logps_train/rejected': '-165.34', 'logps_train/chosen': '-198.94', 'loss/train': '0.68169', 'examples_per_second': '30.523', 'grad_norm': '114.5', 'counters/examples': 158720, 'counters/updates': 4960}
train stats after 158752 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '0.053382', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048882', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-186.66', 'loss/train': '0.68872', 'examples_per_second': '31.498', 'grad_norm': '410', 'counters/examples': 158752, 'counters/updates': 4961}
train stats after 158784 examples: {'rewards_train/chosen': '0.12101', 'rewards_train/rejected': '0.10589', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015127', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-151.49', 'loss/train': '0.70291', 'examples_per_second': '30.245', 'grad_norm': '67', 'counters/examples': 158784, 'counters/updates': 4962}
train stats after 158816 examples: {'rewards_train/chosen': '0.053989', 'rewards_train/rejected': '0.11685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.06286', 'logps_train/rejected': '-129.15', 'logps_train/chosen': '-168.19', 'loss/train': '0.73964', 'examples_per_second': '31.687', 'grad_norm': '66.5', 'counters/examples': 158816, 'counters/updates': 4963}
skipping logging after 158848 examples to avoid logging too frequently
train stats after 158880 examples: {'rewards_train/chosen': '0.049226', 'rewards_train/rejected': '0.032667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01656', 'logps_train/rejected': '-141.59', 'logps_train/chosen': '-153.41', 'loss/train': '0.69967', 'examples_per_second': '31.529', 'grad_norm': '96.5', 'counters/examples': 158880, 'counters/updates': 4965}
train stats after 158912 examples: {'rewards_train/chosen': '0.022512', 'rewards_train/rejected': '0.031846', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0093344', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-150.68', 'loss/train': '0.71092', 'examples_per_second': '30.788', 'grad_norm': '100', 'counters/examples': 158912, 'counters/updates': 4966}
train stats after 158944 examples: {'rewards_train/chosen': '0.12199', 'rewards_train/rejected': '0.074952', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047037', 'logps_train/rejected': '-146.9', 'logps_train/chosen': '-154.34', 'loss/train': '0.68051', 'examples_per_second': '31.496', 'grad_norm': '81.5', 'counters/examples': 158944, 'counters/updates': 4967}
train stats after 158976 examples: {'rewards_train/chosen': '0.040224', 'rewards_train/rejected': '0.10887', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068651', 'logps_train/rejected': '-155.52', 'logps_train/chosen': '-141.45', 'loss/train': '0.75059', 'examples_per_second': '31.834', 'grad_norm': '102.5', 'counters/examples': 158976, 'counters/updates': 4968}
train stats after 159008 examples: {'rewards_train/chosen': '0.085701', 'rewards_train/rejected': '0.040666', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045034', 'logps_train/rejected': '-151.73', 'logps_train/chosen': '-185.46', 'loss/train': '0.69455', 'examples_per_second': '30.609', 'grad_norm': '100', 'counters/examples': 159008, 'counters/updates': 4969}
skipping logging after 159040 examples to avoid logging too frequently
train stats after 159072 examples: {'rewards_train/chosen': '0.065681', 'rewards_train/rejected': '0.13016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.064479', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-134.61', 'loss/train': '0.73824', 'examples_per_second': '31.114', 'grad_norm': '176', 'counters/examples': 159072, 'counters/updates': 4971}
train stats after 159104 examples: {'rewards_train/chosen': '0.011865', 'rewards_train/rejected': '0.047066', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035201', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-153.95', 'loss/train': '0.72234', 'examples_per_second': '31.405', 'grad_norm': '91.5', 'counters/examples': 159104, 'counters/updates': 4972}
train stats after 159136 examples: {'rewards_train/chosen': '0.11509', 'rewards_train/rejected': '0.077905', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037182', 'logps_train/rejected': '-146.42', 'logps_train/chosen': '-163.23', 'loss/train': '0.68874', 'examples_per_second': '29.747', 'grad_norm': '125.5', 'counters/examples': 159136, 'counters/updates': 4973}
skipping logging after 159168 examples to avoid logging too frequently
train stats after 159200 examples: {'rewards_train/chosen': '0.1106', 'rewards_train/rejected': '0.051616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058979', 'logps_train/rejected': '-157.75', 'logps_train/chosen': '-172.16', 'loss/train': '0.68051', 'examples_per_second': '31.531', 'grad_norm': '122', 'counters/examples': 159200, 'counters/updates': 4975}
skipping logging after 159232 examples to avoid logging too frequently
train stats after 159264 examples: {'rewards_train/chosen': '0.11449', 'rewards_train/rejected': '0.035476', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079011', 'logps_train/rejected': '-141.15', 'logps_train/chosen': '-169.07', 'loss/train': '0.66757', 'examples_per_second': '35.705', 'grad_norm': '72', 'counters/examples': 159264, 'counters/updates': 4977}
train stats after 159296 examples: {'rewards_train/chosen': '0.05366', 'rewards_train/rejected': '0.057132', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0034714', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-157.59', 'loss/train': '0.70392', 'examples_per_second': '31.488', 'grad_norm': '97.5', 'counters/examples': 159296, 'counters/updates': 4978}
skipping logging after 159328 examples to avoid logging too frequently
train stats after 159360 examples: {'rewards_train/chosen': '0.047899', 'rewards_train/rejected': '0.12004', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.072137', 'logps_train/rejected': '-148.34', 'logps_train/chosen': '-146.36', 'loss/train': '0.74578', 'examples_per_second': '33.42', 'grad_norm': '78.5', 'counters/examples': 159360, 'counters/updates': 4980}
train stats after 159392 examples: {'rewards_train/chosen': '0.020226', 'rewards_train/rejected': '0.088311', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.068085', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-130.98', 'loss/train': '0.74107', 'examples_per_second': '31.397', 'grad_norm': '71', 'counters/examples': 159392, 'counters/updates': 4981}
train stats after 159424 examples: {'rewards_train/chosen': '0.063812', 'rewards_train/rejected': '-0.01267', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076482', 'logps_train/rejected': '-128', 'logps_train/chosen': '-140.81', 'loss/train': '0.6722', 'examples_per_second': '30.861', 'grad_norm': '154', 'counters/examples': 159424, 'counters/updates': 4982}
train stats after 159456 examples: {'rewards_train/chosen': '0.12753', 'rewards_train/rejected': '0.034895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092637', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-155.47', 'loss/train': '0.65729', 'examples_per_second': '30.513', 'grad_norm': '45.5', 'counters/examples': 159456, 'counters/updates': 4983}
train stats after 159488 examples: {'rewards_train/chosen': '0.079216', 'rewards_train/rejected': '0.11258', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033367', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-109.68', 'loss/train': '0.72452', 'examples_per_second': '32.036', 'grad_norm': '77', 'counters/examples': 159488, 'counters/updates': 4984}
skipping logging after 159520 examples to avoid logging too frequently
train stats after 159552 examples: {'rewards_train/chosen': '0.12285', 'rewards_train/rejected': '0.065832', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057014', 'logps_train/rejected': '-159.44', 'logps_train/chosen': '-184.92', 'loss/train': '0.69121', 'examples_per_second': '30.433', 'grad_norm': '304', 'counters/examples': 159552, 'counters/updates': 4986}
train stats after 159584 examples: {'rewards_train/chosen': '0.042565', 'rewards_train/rejected': '0.068589', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026023', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-144.92', 'loss/train': '0.71871', 'examples_per_second': '31.243', 'grad_norm': '97', 'counters/examples': 159584, 'counters/updates': 4987}
train stats after 159616 examples: {'rewards_train/chosen': '0.075228', 'rewards_train/rejected': '-0.0037562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078984', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-161.07', 'loss/train': '0.66739', 'examples_per_second': '30.372', 'grad_norm': '72.5', 'counters/examples': 159616, 'counters/updates': 4988}
train stats after 159648 examples: {'rewards_train/chosen': '0.10959', 'rewards_train/rejected': '0.073371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036216', 'logps_train/rejected': '-154.05', 'logps_train/chosen': '-144.12', 'loss/train': '0.68322', 'examples_per_second': '30.018', 'grad_norm': '79', 'counters/examples': 159648, 'counters/updates': 4989}
train stats after 159680 examples: {'rewards_train/chosen': '0.052375', 'rewards_train/rejected': '0.099054', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.046679', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-140.78', 'loss/train': '0.73705', 'examples_per_second': '31.531', 'grad_norm': '106.5', 'counters/examples': 159680, 'counters/updates': 4990}
skipping logging after 159712 examples to avoid logging too frequently
train stats after 159744 examples: {'rewards_train/chosen': '0.054363', 'rewards_train/rejected': '0.019192', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035171', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-150.27', 'loss/train': '0.6991', 'examples_per_second': '31.928', 'grad_norm': '79.5', 'counters/examples': 159744, 'counters/updates': 4992}
train stats after 159776 examples: {'rewards_train/chosen': '0.12747', 'rewards_train/rejected': '0.11006', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.017403', 'logps_train/rejected': '-130.69', 'logps_train/chosen': '-163.62', 'loss/train': '0.69662', 'examples_per_second': '32.267', 'grad_norm': '124.5', 'counters/examples': 159776, 'counters/updates': 4993}
train stats after 159808 examples: {'rewards_train/chosen': '0.063798', 'rewards_train/rejected': '-0.037807', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10161', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-143.62', 'loss/train': '0.66106', 'examples_per_second': '31.458', 'grad_norm': '88', 'counters/examples': 159808, 'counters/updates': 4994}
train stats after 159840 examples: {'rewards_train/chosen': '0.049846', 'rewards_train/rejected': '0.086123', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036277', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-176.28', 'loss/train': '0.72462', 'examples_per_second': '31.501', 'grad_norm': '101.5', 'counters/examples': 159840, 'counters/updates': 4995}
train stats after 159872 examples: {'rewards_train/chosen': '0.04631', 'rewards_train/rejected': '0.054437', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0081269', 'logps_train/rejected': '-158.46', 'logps_train/chosen': '-157.2', 'loss/train': '0.70648', 'examples_per_second': '31.472', 'grad_norm': '66', 'counters/examples': 159872, 'counters/updates': 4996}
train stats after 159904 examples: {'rewards_train/chosen': '0.010233', 'rewards_train/rejected': '0.10276', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.092528', 'logps_train/rejected': '-148.79', 'logps_train/chosen': '-113.23', 'loss/train': '0.75156', 'examples_per_second': '32.49', 'grad_norm': '63.25', 'counters/examples': 159904, 'counters/updates': 4997}
train stats after 159936 examples: {'rewards_train/chosen': '0.070251', 'rewards_train/rejected': '0.020181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05007', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-142.48', 'loss/train': '0.68578', 'examples_per_second': '30.035', 'grad_norm': '76', 'counters/examples': 159936, 'counters/updates': 4998}
train stats after 159968 examples: {'rewards_train/chosen': '0.078215', 'rewards_train/rejected': '0.046232', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031983', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-124.26', 'loss/train': '0.68441', 'examples_per_second': '30.685', 'grad_norm': '243', 'counters/examples': 159968, 'counters/updates': 4999}
train stats after 160000 examples: {'rewards_train/chosen': '0.14473', 'rewards_train/rejected': '0.040822', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10391', 'logps_train/rejected': '-174.38', 'logps_train/chosen': '-176.36', 'loss/train': '0.65034', 'examples_per_second': '33.182', 'grad_norm': '71', 'counters/examples': 160000, 'counters/updates': 5000}
train stats after 160032 examples: {'rewards_train/chosen': '0.096363', 'rewards_train/rejected': '0.12183', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025471', 'logps_train/rejected': '-150.38', 'logps_train/chosen': '-158.14', 'loss/train': '0.72525', 'examples_per_second': '30.896', 'grad_norm': '101.5', 'counters/examples': 160032, 'counters/updates': 5001}
train stats after 160064 examples: {'rewards_train/chosen': '0.15362', 'rewards_train/rejected': '0.085248', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068376', 'logps_train/rejected': '-150.17', 'logps_train/chosen': '-180.06', 'loss/train': '0.69622', 'examples_per_second': '31.504', 'grad_norm': '85.5', 'counters/examples': 160064, 'counters/updates': 5002}
skipping logging after 160096 examples to avoid logging too frequently
train stats after 160128 examples: {'rewards_train/chosen': '-0.01597', 'rewards_train/rejected': '0.077513', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.093484', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-123.37', 'loss/train': '0.76037', 'examples_per_second': '30.064', 'grad_norm': '90', 'counters/examples': 160128, 'counters/updates': 5004}
train stats after 160160 examples: {'rewards_train/chosen': '0.020757', 'rewards_train/rejected': '-0.048233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06899', 'logps_train/rejected': '-130', 'logps_train/chosen': '-199.89', 'loss/train': '0.67146', 'examples_per_second': '31.442', 'grad_norm': '104', 'counters/examples': 160160, 'counters/updates': 5005}
train stats after 160192 examples: {'rewards_train/chosen': '0.041637', 'rewards_train/rejected': '0.059179', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017542', 'logps_train/rejected': '-146.47', 'logps_train/chosen': '-169.31', 'loss/train': '0.71872', 'examples_per_second': '31.495', 'grad_norm': '117.5', 'counters/examples': 160192, 'counters/updates': 5006}
train stats after 160224 examples: {'rewards_train/chosen': '0.081927', 'rewards_train/rejected': '0.03179', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050137', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-170.55', 'loss/train': '0.67903', 'examples_per_second': '32.214', 'grad_norm': '128', 'counters/examples': 160224, 'counters/updates': 5007}
train stats after 160256 examples: {'rewards_train/chosen': '0.080608', 'rewards_train/rejected': '0.10709', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026477', 'logps_train/rejected': '-146.13', 'logps_train/chosen': '-204.41', 'loss/train': '0.71979', 'examples_per_second': '31.968', 'grad_norm': '83.5', 'counters/examples': 160256, 'counters/updates': 5008}
train stats after 160288 examples: {'rewards_train/chosen': '0.1346', 'rewards_train/rejected': '0.082401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052203', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-151.72', 'loss/train': '0.67518', 'examples_per_second': '32.488', 'grad_norm': '130', 'counters/examples': 160288, 'counters/updates': 5009}
train stats after 160320 examples: {'rewards_train/chosen': '0.0034145', 'rewards_train/rejected': '0.020052', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016638', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-165.97', 'loss/train': '0.71943', 'examples_per_second': '31.34', 'grad_norm': '103.5', 'counters/examples': 160320, 'counters/updates': 5010}
train stats after 160352 examples: {'rewards_train/chosen': '0.19394', 'rewards_train/rejected': '0.014973', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17896', 'logps_train/rejected': '-164.09', 'logps_train/chosen': '-175.69', 'loss/train': '0.63781', 'examples_per_second': '23.202', 'grad_norm': '92', 'counters/examples': 160352, 'counters/updates': 5011}
train stats after 160384 examples: {'rewards_train/chosen': '0.084931', 'rewards_train/rejected': '-0.042228', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12716', 'logps_train/rejected': '-107.56', 'logps_train/chosen': '-162.64', 'loss/train': '0.64456', 'examples_per_second': '30.189', 'grad_norm': '68.5', 'counters/examples': 160384, 'counters/updates': 5012}
train stats after 160416 examples: {'rewards_train/chosen': '0.14187', 'rewards_train/rejected': '0.13985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0020166', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-153.15', 'loss/train': '0.71823', 'examples_per_second': '32.436', 'grad_norm': '66', 'counters/examples': 160416, 'counters/updates': 5013}
skipping logging after 160448 examples to avoid logging too frequently
train stats after 160480 examples: {'rewards_train/chosen': '0.038224', 'rewards_train/rejected': '0.12237', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.084143', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-165.01', 'loss/train': '0.74922', 'examples_per_second': '31.065', 'grad_norm': '203', 'counters/examples': 160480, 'counters/updates': 5015}
train stats after 160512 examples: {'rewards_train/chosen': '0.13063', 'rewards_train/rejected': '0.033165', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09747', 'logps_train/rejected': '-93.07', 'logps_train/chosen': '-134.61', 'loss/train': '0.65998', 'examples_per_second': '31.552', 'grad_norm': '95.5', 'counters/examples': 160512, 'counters/updates': 5016}
train stats after 160544 examples: {'rewards_train/chosen': '0.122', 'rewards_train/rejected': '0.063337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058661', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-150.65', 'loss/train': '0.68155', 'examples_per_second': '31.497', 'grad_norm': '79.5', 'counters/examples': 160544, 'counters/updates': 5017}
skipping logging after 160576 examples to avoid logging too frequently
train stats after 160608 examples: {'rewards_train/chosen': '0.087537', 'rewards_train/rejected': '0.10021', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012676', 'logps_train/rejected': '-133.75', 'logps_train/chosen': '-131.58', 'loss/train': '0.71967', 'examples_per_second': '31.556', 'grad_norm': '160', 'counters/examples': 160608, 'counters/updates': 5019}
train stats after 160640 examples: {'rewards_train/chosen': '0.11779', 'rewards_train/rejected': '0.17382', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.056032', 'logps_train/rejected': '-134.75', 'logps_train/chosen': '-157.2', 'loss/train': '0.7379', 'examples_per_second': '31.213', 'grad_norm': '81', 'counters/examples': 160640, 'counters/updates': 5020}
train stats after 160672 examples: {'rewards_train/chosen': '0.1722', 'rewards_train/rejected': '0.010327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16187', 'logps_train/rejected': '-144.34', 'logps_train/chosen': '-128.16', 'loss/train': '0.62818', 'examples_per_second': '31.463', 'grad_norm': '64.5', 'counters/examples': 160672, 'counters/updates': 5021}
train stats after 160704 examples: {'rewards_train/chosen': '0.061206', 'rewards_train/rejected': '0.07219', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010984', 'logps_train/rejected': '-166.14', 'logps_train/chosen': '-158.4', 'loss/train': '0.71163', 'examples_per_second': '31.279', 'grad_norm': '91.5', 'counters/examples': 160704, 'counters/updates': 5022}
train stats after 160736 examples: {'rewards_train/chosen': '0.11781', 'rewards_train/rejected': '0.086987', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030824', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-124.45', 'loss/train': '0.69223', 'examples_per_second': '30.865', 'grad_norm': '98', 'counters/examples': 160736, 'counters/updates': 5023}
train stats after 160768 examples: {'rewards_train/chosen': '0.051684', 'rewards_train/rejected': '0.0024183', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049266', 'logps_train/rejected': '-112.44', 'logps_train/chosen': '-125.16', 'loss/train': '0.67871', 'examples_per_second': '29.968', 'grad_norm': '80', 'counters/examples': 160768, 'counters/updates': 5024}
skipping logging after 160800 examples to avoid logging too frequently
train stats after 160832 examples: {'rewards_train/chosen': '0.18607', 'rewards_train/rejected': '0.054355', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13172', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-174.05', 'loss/train': '0.64617', 'examples_per_second': '36.098', 'grad_norm': '55', 'counters/examples': 160832, 'counters/updates': 5026}
skipping logging after 160864 examples to avoid logging too frequently
train stats after 160896 examples: {'rewards_train/chosen': '0.024149', 'rewards_train/rejected': '0.064015', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039866', 'logps_train/rejected': '-153.91', 'logps_train/chosen': '-160.95', 'loss/train': '0.72767', 'examples_per_second': '31.29', 'grad_norm': '101', 'counters/examples': 160896, 'counters/updates': 5028}
train stats after 160928 examples: {'rewards_train/chosen': '0.059575', 'rewards_train/rejected': '0.069557', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0099825', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-129.41', 'loss/train': '0.71623', 'examples_per_second': '31.772', 'grad_norm': '97.5', 'counters/examples': 160928, 'counters/updates': 5029}
skipping logging after 160960 examples to avoid logging too frequently
train stats after 160992 examples: {'rewards_train/chosen': '0.063128', 'rewards_train/rejected': '0.06128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0018472', 'logps_train/rejected': '-156.66', 'logps_train/chosen': '-177.64', 'loss/train': '0.70359', 'examples_per_second': '32.47', 'grad_norm': '100', 'counters/examples': 160992, 'counters/updates': 5031}
train stats after 161024 examples: {'rewards_train/chosen': '0.091835', 'rewards_train/rejected': '0.077433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014402', 'logps_train/rejected': '-148.14', 'logps_train/chosen': '-179.86', 'loss/train': '0.70837', 'examples_per_second': '31.515', 'grad_norm': '109', 'counters/examples': 161024, 'counters/updates': 5032}
train stats after 161056 examples: {'rewards_train/chosen': '0.059168', 'rewards_train/rejected': '0.0095415', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049627', 'logps_train/rejected': '-112.35', 'logps_train/chosen': '-148.91', 'loss/train': '0.68322', 'examples_per_second': '30.861', 'grad_norm': '87', 'counters/examples': 161056, 'counters/updates': 5033}
train stats after 161088 examples: {'rewards_train/chosen': '0.079654', 'rewards_train/rejected': '0.099388', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019734', 'logps_train/rejected': '-120', 'logps_train/chosen': '-148.8', 'loss/train': '0.72099', 'examples_per_second': '32.476', 'grad_norm': '102', 'counters/examples': 161088, 'counters/updates': 5034}
train stats after 161120 examples: {'rewards_train/chosen': '0.083479', 'rewards_train/rejected': '0.047504', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035975', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-162.11', 'loss/train': '0.69105', 'examples_per_second': '30.266', 'grad_norm': '70', 'counters/examples': 161120, 'counters/updates': 5035}
train stats after 161152 examples: {'rewards_train/chosen': '0.12397', 'rewards_train/rejected': '0.039566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084404', 'logps_train/rejected': '-138.32', 'logps_train/chosen': '-173.73', 'loss/train': '0.67039', 'examples_per_second': '31.8', 'grad_norm': '52.25', 'counters/examples': 161152, 'counters/updates': 5036}
skipping logging after 161184 examples to avoid logging too frequently
train stats after 161216 examples: {'rewards_train/chosen': '0.11146', 'rewards_train/rejected': '0.057195', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054268', 'logps_train/rejected': '-116', 'logps_train/chosen': '-130.8', 'loss/train': '0.67596', 'examples_per_second': '32.571', 'grad_norm': '66.5', 'counters/examples': 161216, 'counters/updates': 5038}
train stats after 161248 examples: {'rewards_train/chosen': '0.13825', 'rewards_train/rejected': '0.097521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040726', 'logps_train/rejected': '-134.25', 'logps_train/chosen': '-132.84', 'loss/train': '0.68924', 'examples_per_second': '31.553', 'grad_norm': '109', 'counters/examples': 161248, 'counters/updates': 5039}
train stats after 161280 examples: {'rewards_train/chosen': '0.10174', 'rewards_train/rejected': '0.01762', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084116', 'logps_train/rejected': '-117.72', 'logps_train/chosen': '-110.67', 'loss/train': '0.66361', 'examples_per_second': '31.76', 'grad_norm': '109.5', 'counters/examples': 161280, 'counters/updates': 5040}
train stats after 161312 examples: {'rewards_train/chosen': '-0.011129', 'rewards_train/rejected': '0.059418', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.070547', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-127.8', 'loss/train': '0.73651', 'examples_per_second': '30.554', 'grad_norm': '100', 'counters/examples': 161312, 'counters/updates': 5041}
train stats after 161344 examples: {'rewards_train/chosen': '0.0056416', 'rewards_train/rejected': '-0.0084387', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01408', 'logps_train/rejected': '-109.34', 'logps_train/chosen': '-155.89', 'loss/train': '0.69261', 'examples_per_second': '30.873', 'grad_norm': '75', 'counters/examples': 161344, 'counters/updates': 5042}
train stats after 161376 examples: {'rewards_train/chosen': '0.083848', 'rewards_train/rejected': '0.027618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05623', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-163.2', 'loss/train': '0.68405', 'examples_per_second': '32.891', 'grad_norm': '84', 'counters/examples': 161376, 'counters/updates': 5043}
train stats after 161408 examples: {'rewards_train/chosen': '0.091286', 'rewards_train/rejected': '0.035118', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056168', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-135.19', 'loss/train': '0.67963', 'examples_per_second': '32.437', 'grad_norm': '98.5', 'counters/examples': 161408, 'counters/updates': 5044}
train stats after 161440 examples: {'rewards_train/chosen': '0.13389', 'rewards_train/rejected': '0.073371', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060523', 'logps_train/rejected': '-147.97', 'logps_train/chosen': '-176.43', 'loss/train': '0.69153', 'examples_per_second': '31.36', 'grad_norm': '69', 'counters/examples': 161440, 'counters/updates': 5045}
train stats after 161472 examples: {'rewards_train/chosen': '0.082218', 'rewards_train/rejected': '0.084685', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0024667', 'logps_train/rejected': '-157.8', 'logps_train/chosen': '-169.42', 'loss/train': '0.71035', 'examples_per_second': '31.362', 'grad_norm': '87', 'counters/examples': 161472, 'counters/updates': 5046}
train stats after 161504 examples: {'rewards_train/chosen': '0.13732', 'rewards_train/rejected': '0.088371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048953', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-147.63', 'loss/train': '0.67966', 'examples_per_second': '33.249', 'grad_norm': '91', 'counters/examples': 161504, 'counters/updates': 5047}
train stats after 161536 examples: {'rewards_train/chosen': '0.12976', 'rewards_train/rejected': '0.038017', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09174', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-141.24', 'loss/train': '0.65793', 'examples_per_second': '31.974', 'grad_norm': '86.5', 'counters/examples': 161536, 'counters/updates': 5048}
train stats after 161568 examples: {'rewards_train/chosen': '0.14582', 'rewards_train/rejected': '0.085265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060551', 'logps_train/rejected': '-146.95', 'logps_train/chosen': '-158.41', 'loss/train': '0.67257', 'examples_per_second': '31.555', 'grad_norm': '74.5', 'counters/examples': 161568, 'counters/updates': 5049}
train stats after 161600 examples: {'rewards_train/chosen': '0.11127', 'rewards_train/rejected': '0.043721', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067551', 'logps_train/rejected': '-140.76', 'logps_train/chosen': '-122.19', 'loss/train': '0.66874', 'examples_per_second': '30.349', 'grad_norm': '58.75', 'counters/examples': 161600, 'counters/updates': 5050}
train stats after 161632 examples: {'rewards_train/chosen': '0.10678', 'rewards_train/rejected': '0.071357', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035424', 'logps_train/rejected': '-134.06', 'logps_train/chosen': '-164.47', 'loss/train': '0.69235', 'examples_per_second': '30.822', 'grad_norm': '105.5', 'counters/examples': 161632, 'counters/updates': 5051}
train stats after 161664 examples: {'rewards_train/chosen': '0.15825', 'rewards_train/rejected': '0.099416', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058834', 'logps_train/rejected': '-116.74', 'logps_train/chosen': '-152.98', 'loss/train': '0.68565', 'examples_per_second': '30.98', 'grad_norm': '93.5', 'counters/examples': 161664, 'counters/updates': 5052}
train stats after 161696 examples: {'rewards_train/chosen': '0.25131', 'rewards_train/rejected': '0.033635', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21767', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-171.4', 'loss/train': '0.60471', 'examples_per_second': '30.061', 'grad_norm': '197', 'counters/examples': 161696, 'counters/updates': 5053}
train stats after 161728 examples: {'rewards_train/chosen': '0.063875', 'rewards_train/rejected': '0.05298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010896', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-165.78', 'loss/train': '0.69964', 'examples_per_second': '31.422', 'grad_norm': '66', 'counters/examples': 161728, 'counters/updates': 5054}
train stats after 161760 examples: {'rewards_train/chosen': '0.17377', 'rewards_train/rejected': '-0.01851', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19228', 'logps_train/rejected': '-163.2', 'logps_train/chosen': '-185.18', 'loss/train': '0.61365', 'examples_per_second': '31.355', 'grad_norm': '81.5', 'counters/examples': 161760, 'counters/updates': 5055}
skipping logging after 161792 examples to avoid logging too frequently
train stats after 161824 examples: {'rewards_train/chosen': '0.15449', 'rewards_train/rejected': '0.023936', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13055', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-148.29', 'loss/train': '0.64345', 'examples_per_second': '35.777', 'grad_norm': '142', 'counters/examples': 161824, 'counters/updates': 5057}
train stats after 161856 examples: {'rewards_train/chosen': '0.080736', 'rewards_train/rejected': '0.058363', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022373', 'logps_train/rejected': '-154.79', 'logps_train/chosen': '-139.93', 'loss/train': '0.6922', 'examples_per_second': '30.023', 'grad_norm': '342', 'counters/examples': 161856, 'counters/updates': 5058}
train stats after 161888 examples: {'rewards_train/chosen': '0.040827', 'rewards_train/rejected': '0.020756', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020071', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-156.1', 'loss/train': '0.68987', 'examples_per_second': '31.75', 'grad_norm': '89.5', 'counters/examples': 161888, 'counters/updates': 5059}
skipping logging after 161920 examples to avoid logging too frequently
train stats after 161952 examples: {'rewards_train/chosen': '0.096235', 'rewards_train/rejected': '0.051533', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044702', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-133.21', 'loss/train': '0.69505', 'examples_per_second': '31.576', 'grad_norm': '131', 'counters/examples': 161952, 'counters/updates': 5061}
train stats after 161984 examples: {'rewards_train/chosen': '0.014784', 'rewards_train/rejected': '0.049269', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034484', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-175.87', 'loss/train': '0.72598', 'examples_per_second': '32.29', 'grad_norm': '92.5', 'counters/examples': 161984, 'counters/updates': 5062}
train stats after 162016 examples: {'rewards_train/chosen': '0.050927', 'rewards_train/rejected': '0.074567', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023639', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-141.92', 'loss/train': '0.71456', 'examples_per_second': '31.575', 'grad_norm': '133', 'counters/examples': 162016, 'counters/updates': 5063}
train stats after 162048 examples: {'rewards_train/chosen': '-0.0022948', 'rewards_train/rejected': '0.051', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.053295', 'logps_train/rejected': '-126.88', 'logps_train/chosen': '-180.6', 'loss/train': '0.73539', 'examples_per_second': '31.558', 'grad_norm': '145', 'counters/examples': 162048, 'counters/updates': 5064}
train stats after 162080 examples: {'rewards_train/chosen': '0.12935', 'rewards_train/rejected': '0.098655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030694', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-182.04', 'loss/train': '0.69271', 'examples_per_second': '31.453', 'grad_norm': '153', 'counters/examples': 162080, 'counters/updates': 5065}
train stats after 162112 examples: {'rewards_train/chosen': '0.073732', 'rewards_train/rejected': '0.084429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010697', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-160.63', 'loss/train': '0.72012', 'examples_per_second': '30.439', 'grad_norm': '129', 'counters/examples': 162112, 'counters/updates': 5066}
train stats after 162144 examples: {'rewards_train/chosen': '0.12596', 'rewards_train/rejected': '0.018455', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1075', 'logps_train/rejected': '-112.71', 'logps_train/chosen': '-134.83', 'loss/train': '0.65086', 'examples_per_second': '32.728', 'grad_norm': '97.5', 'counters/examples': 162144, 'counters/updates': 5067}
train stats after 162176 examples: {'rewards_train/chosen': '0.081732', 'rewards_train/rejected': '0.012121', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069611', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-141.82', 'loss/train': '0.6714', 'examples_per_second': '31.603', 'grad_norm': '66', 'counters/examples': 162176, 'counters/updates': 5068}
train stats after 162208 examples: {'rewards_train/chosen': '0.074506', 'rewards_train/rejected': '0.11215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037643', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-111.94', 'loss/train': '0.72361', 'examples_per_second': '30.041', 'grad_norm': '71.5', 'counters/examples': 162208, 'counters/updates': 5069}
skipping logging after 162240 examples to avoid logging too frequently
train stats after 162272 examples: {'rewards_train/chosen': '0.053808', 'rewards_train/rejected': '0.036904', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016904', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-163.12', 'loss/train': '0.69545', 'examples_per_second': '32.634', 'grad_norm': '117.5', 'counters/examples': 162272, 'counters/updates': 5071}
train stats after 162304 examples: {'rewards_train/chosen': '0.10285', 'rewards_train/rejected': '0.046381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05647', 'logps_train/rejected': '-97.919', 'logps_train/chosen': '-135.92', 'loss/train': '0.6772', 'examples_per_second': '30.38', 'grad_norm': '123', 'counters/examples': 162304, 'counters/updates': 5072}
train stats after 162336 examples: {'rewards_train/chosen': '0.0022235', 'rewards_train/rejected': '0.035435', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.033212', 'logps_train/rejected': '-107.23', 'logps_train/chosen': '-161.78', 'loss/train': '0.72646', 'examples_per_second': '32.403', 'grad_norm': '87', 'counters/examples': 162336, 'counters/updates': 5073}
skipping logging after 162368 examples to avoid logging too frequently
train stats after 162400 examples: {'rewards_train/chosen': '0.059011', 'rewards_train/rejected': '0.058154', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.00085668', 'logps_train/rejected': '-101.12', 'logps_train/chosen': '-137.18', 'loss/train': '0.70244', 'examples_per_second': '34.555', 'grad_norm': '67.5', 'counters/examples': 162400, 'counters/updates': 5075}
train stats after 162432 examples: {'rewards_train/chosen': '0.16198', 'rewards_train/rejected': '0.08263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07935', 'logps_train/rejected': '-128.56', 'logps_train/chosen': '-130.75', 'loss/train': '0.66478', 'examples_per_second': '32.07', 'grad_norm': '71.5', 'counters/examples': 162432, 'counters/updates': 5076}
skipping logging after 162464 examples to avoid logging too frequently
train stats after 162496 examples: {'rewards_train/chosen': '0.033788', 'rewards_train/rejected': '-0.001155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034943', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-148.97', 'loss/train': '0.68444', 'examples_per_second': '31.564', 'grad_norm': '116', 'counters/examples': 162496, 'counters/updates': 5078}
train stats after 162528 examples: {'rewards_train/chosen': '0.14136', 'rewards_train/rejected': '0.044226', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09713', 'logps_train/rejected': '-139.23', 'logps_train/chosen': '-148.27', 'loss/train': '0.66639', 'examples_per_second': '31.55', 'grad_norm': '76', 'counters/examples': 162528, 'counters/updates': 5079}
train stats after 162560 examples: {'rewards_train/chosen': '0.094473', 'rewards_train/rejected': '0.094848', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00037464', 'logps_train/rejected': '-150.89', 'logps_train/chosen': '-153.32', 'loss/train': '0.70777', 'examples_per_second': '31.536', 'grad_norm': '66', 'counters/examples': 162560, 'counters/updates': 5080}
train stats after 162592 examples: {'rewards_train/chosen': '0.1191', 'rewards_train/rejected': '-0.057911', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17701', 'logps_train/rejected': '-147.67', 'logps_train/chosen': '-165.38', 'loss/train': '0.62407', 'examples_per_second': '31.654', 'grad_norm': '99', 'counters/examples': 162592, 'counters/updates': 5081}
skipping logging after 162624 examples to avoid logging too frequently
train stats after 162656 examples: {'rewards_train/chosen': '0.19884', 'rewards_train/rejected': '0.1352', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063646', 'logps_train/rejected': '-155.18', 'logps_train/chosen': '-192.38', 'loss/train': '0.70482', 'examples_per_second': '31.292', 'grad_norm': '338', 'counters/examples': 162656, 'counters/updates': 5083}
train stats after 162688 examples: {'rewards_train/chosen': '0.087449', 'rewards_train/rejected': '0.094962', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.007513', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-130.5', 'loss/train': '0.71449', 'examples_per_second': '30.081', 'grad_norm': '116.5', 'counters/examples': 162688, 'counters/updates': 5084}
train stats after 162720 examples: {'rewards_train/chosen': '0.1093', 'rewards_train/rejected': '0.10806', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0012462', 'logps_train/rejected': '-155.15', 'logps_train/chosen': '-159.59', 'loss/train': '0.70854', 'examples_per_second': '31.537', 'grad_norm': '81.5', 'counters/examples': 162720, 'counters/updates': 5085}
train stats after 162752 examples: {'rewards_train/chosen': '0.087471', 'rewards_train/rejected': '0.076874', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.010598', 'logps_train/rejected': '-119.53', 'logps_train/chosen': '-112.35', 'loss/train': '0.69957', 'examples_per_second': '32.356', 'grad_norm': '86', 'counters/examples': 162752, 'counters/updates': 5086}
train stats after 162784 examples: {'rewards_train/chosen': '0.088381', 'rewards_train/rejected': '-0.023861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11224', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-206.57', 'loss/train': '0.65907', 'examples_per_second': '31.051', 'grad_norm': '115', 'counters/examples': 162784, 'counters/updates': 5087}
train stats after 162816 examples: {'rewards_train/chosen': '0.071915', 'rewards_train/rejected': '0.082239', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010324', 'logps_train/rejected': '-137.98', 'logps_train/chosen': '-136.79', 'loss/train': '0.7066', 'examples_per_second': '31.534', 'grad_norm': '85', 'counters/examples': 162816, 'counters/updates': 5088}
train stats after 162848 examples: {'rewards_train/chosen': '0.041089', 'rewards_train/rejected': '0.032835', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008254', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-131.83', 'loss/train': '0.70172', 'examples_per_second': '30.627', 'grad_norm': '99', 'counters/examples': 162848, 'counters/updates': 5089}
train stats after 162880 examples: {'rewards_train/chosen': '0.043107', 'rewards_train/rejected': '0.045575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0024679', 'logps_train/rejected': '-157.2', 'logps_train/chosen': '-161.55', 'loss/train': '0.70499', 'examples_per_second': '23.511', 'grad_norm': '79.5', 'counters/examples': 162880, 'counters/updates': 5090}
train stats after 162912 examples: {'rewards_train/chosen': '0.06212', 'rewards_train/rejected': '-0.049819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11194', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-149.34', 'loss/train': '0.65664', 'examples_per_second': '31.548', 'grad_norm': '139', 'counters/examples': 162912, 'counters/updates': 5091}
train stats after 162944 examples: {'rewards_train/chosen': '0.18245', 'rewards_train/rejected': '0.027766', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15469', 'logps_train/rejected': '-148.85', 'logps_train/chosen': '-149.58', 'loss/train': '0.64166', 'examples_per_second': '32.692', 'grad_norm': '69.5', 'counters/examples': 162944, 'counters/updates': 5092}
train stats after 162976 examples: {'rewards_train/chosen': '0.15352', 'rewards_train/rejected': '0.029269', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12425', 'logps_train/rejected': '-123.53', 'logps_train/chosen': '-161.13', 'loss/train': '0.64694', 'examples_per_second': '23.369', 'grad_norm': '68.5', 'counters/examples': 162976, 'counters/updates': 5093}
train stats after 163008 examples: {'rewards_train/chosen': '0.13437', 'rewards_train/rejected': '0.099305', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035065', 'logps_train/rejected': '-120.26', 'logps_train/chosen': '-166.82', 'loss/train': '0.69221', 'examples_per_second': '32.471', 'grad_norm': '59.25', 'counters/examples': 163008, 'counters/updates': 5094}
train stats after 163040 examples: {'rewards_train/chosen': '0.18867', 'rewards_train/rejected': '0.0035231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18515', 'logps_train/rejected': '-108.92', 'logps_train/chosen': '-160.18', 'loss/train': '0.62959', 'examples_per_second': '30.956', 'grad_norm': '65', 'counters/examples': 163040, 'counters/updates': 5095}
train stats after 163072 examples: {'rewards_train/chosen': '0.15457', 'rewards_train/rejected': '-0.012642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16721', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-135.44', 'loss/train': '0.62766', 'examples_per_second': '30.971', 'grad_norm': '69.5', 'counters/examples': 163072, 'counters/updates': 5096}
skipping logging after 163104 examples to avoid logging too frequently
train stats after 163136 examples: {'rewards_train/chosen': '0.098473', 'rewards_train/rejected': '0.071375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027098', 'logps_train/rejected': '-171.53', 'logps_train/chosen': '-180.09', 'loss/train': '0.69707', 'examples_per_second': '31.574', 'grad_norm': '95.5', 'counters/examples': 163136, 'counters/updates': 5098}
skipping logging after 163168 examples to avoid logging too frequently
train stats after 163200 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.056644', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086975', 'logps_train/rejected': '-141.55', 'logps_train/chosen': '-153.76', 'loss/train': '0.66073', 'examples_per_second': '30.082', 'grad_norm': '112', 'counters/examples': 163200, 'counters/updates': 5100}
train stats after 163232 examples: {'rewards_train/chosen': '0.056553', 'rewards_train/rejected': '0.032529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024024', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-180.63', 'loss/train': '0.69928', 'examples_per_second': '30.421', 'grad_norm': '141', 'counters/examples': 163232, 'counters/updates': 5101}
skipping logging after 163264 examples to avoid logging too frequently
train stats after 163296 examples: {'rewards_train/chosen': '0.049263', 'rewards_train/rejected': '0.091363', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0421', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-146.12', 'loss/train': '0.7209', 'examples_per_second': '31.753', 'grad_norm': '153', 'counters/examples': 163296, 'counters/updates': 5103}
train stats after 163328 examples: {'rewards_train/chosen': '0.16569', 'rewards_train/rejected': '0.058255', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10743', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-142.22', 'loss/train': '0.66612', 'examples_per_second': '31.903', 'grad_norm': '62.75', 'counters/examples': 163328, 'counters/updates': 5104}
train stats after 163360 examples: {'rewards_train/chosen': '0.21175', 'rewards_train/rejected': '0.15919', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052559', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-140.64', 'loss/train': '0.68023', 'examples_per_second': '31.551', 'grad_norm': '54', 'counters/examples': 163360, 'counters/updates': 5105}
train stats after 163392 examples: {'rewards_train/chosen': '0.097072', 'rewards_train/rejected': '0.099861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027885', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-146.42', 'loss/train': '0.71304', 'examples_per_second': '31.63', 'grad_norm': '95', 'counters/examples': 163392, 'counters/updates': 5106}
train stats after 163424 examples: {'rewards_train/chosen': '0.13379', 'rewards_train/rejected': '0.03759', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096196', 'logps_train/rejected': '-129.45', 'logps_train/chosen': '-174.29', 'loss/train': '0.65756', 'examples_per_second': '31.544', 'grad_norm': '81', 'counters/examples': 163424, 'counters/updates': 5107}
train stats after 163456 examples: {'rewards_train/chosen': '0.071357', 'rewards_train/rejected': '0.087915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016558', 'logps_train/rejected': '-132.16', 'logps_train/chosen': '-153.19', 'loss/train': '0.72099', 'examples_per_second': '31.565', 'grad_norm': '109.5', 'counters/examples': 163456, 'counters/updates': 5108}
train stats after 163488 examples: {'rewards_train/chosen': '0.12271', 'rewards_train/rejected': '0.036522', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08619', 'logps_train/rejected': '-150.12', 'logps_train/chosen': '-141.08', 'loss/train': '0.66831', 'examples_per_second': '31.286', 'grad_norm': '74', 'counters/examples': 163488, 'counters/updates': 5109}
skipping logging after 163520 examples to avoid logging too frequently
train stats after 163552 examples: {'rewards_train/chosen': '0.14487', 'rewards_train/rejected': '0.018354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12652', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-173.11', 'loss/train': '0.6455', 'examples_per_second': '32.044', 'grad_norm': '84.5', 'counters/examples': 163552, 'counters/updates': 5111}
train stats after 163584 examples: {'rewards_train/chosen': '0.028928', 'rewards_train/rejected': '0.0054668', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023461', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-138.45', 'loss/train': '0.68811', 'examples_per_second': '31.513', 'grad_norm': '90', 'counters/examples': 163584, 'counters/updates': 5112}
train stats after 163616 examples: {'rewards_train/chosen': '0.056319', 'rewards_train/rejected': '0.0039902', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052329', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-143.7', 'loss/train': '0.67266', 'examples_per_second': '32.651', 'grad_norm': '68', 'counters/examples': 163616, 'counters/updates': 5113}
train stats after 163648 examples: {'rewards_train/chosen': '0.17115', 'rewards_train/rejected': '0.019542', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1516', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-162.7', 'loss/train': '0.63419', 'examples_per_second': '31.771', 'grad_norm': '74', 'counters/examples': 163648, 'counters/updates': 5114}
train stats after 163680 examples: {'rewards_train/chosen': '0.063892', 'rewards_train/rejected': '0.11143', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047536', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-134.25', 'loss/train': '0.73871', 'examples_per_second': '31.54', 'grad_norm': '596', 'counters/examples': 163680, 'counters/updates': 5115}
train stats after 163712 examples: {'rewards_train/chosen': '0.068262', 'rewards_train/rejected': '0.061373', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0068886', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-120.89', 'loss/train': '0.70583', 'examples_per_second': '30.592', 'grad_norm': '83', 'counters/examples': 163712, 'counters/updates': 5116}
train stats after 163744 examples: {'rewards_train/chosen': '0.12693', 'rewards_train/rejected': '0.041616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085311', 'logps_train/rejected': '-123.64', 'logps_train/chosen': '-139.5', 'loss/train': '0.66762', 'examples_per_second': '30.007', 'grad_norm': '70.5', 'counters/examples': 163744, 'counters/updates': 5117}
skipping logging after 163776 examples to avoid logging too frequently
train stats after 163808 examples: {'rewards_train/chosen': '0.13785', 'rewards_train/rejected': '0.070685', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067163', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-155.88', 'loss/train': '0.67511', 'examples_per_second': '34.718', 'grad_norm': '87.5', 'counters/examples': 163808, 'counters/updates': 5119}
train stats after 163840 examples: {'rewards_train/chosen': '0.074631', 'rewards_train/rejected': '0.11294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.038306', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-143', 'loss/train': '0.72925', 'examples_per_second': '31.602', 'grad_norm': '101.5', 'counters/examples': 163840, 'counters/updates': 5120}
train stats after 163872 examples: {'rewards_train/chosen': '0.041758', 'rewards_train/rejected': '0.0064041', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035354', 'logps_train/rejected': '-154.91', 'logps_train/chosen': '-154.94', 'loss/train': '0.68463', 'examples_per_second': '31.493', 'grad_norm': '71.5', 'counters/examples': 163872, 'counters/updates': 5121}
train stats after 163904 examples: {'rewards_train/chosen': '0.12135', 'rewards_train/rejected': '-0.0061223', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12748', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-217.58', 'loss/train': '0.64781', 'examples_per_second': '29.785', 'grad_norm': '91', 'counters/examples': 163904, 'counters/updates': 5122}
train stats after 163936 examples: {'rewards_train/chosen': '0.026244', 'rewards_train/rejected': '0.023173', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0030712', 'logps_train/rejected': '-145.27', 'logps_train/chosen': '-125.82', 'loss/train': '0.69678', 'examples_per_second': '31.513', 'grad_norm': '87.5', 'counters/examples': 163936, 'counters/updates': 5123}
train stats after 163968 examples: {'rewards_train/chosen': '0.17', 'rewards_train/rejected': '0.1495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020502', 'logps_train/rejected': '-170.9', 'logps_train/chosen': '-172.67', 'loss/train': '0.68901', 'examples_per_second': '30.145', 'grad_norm': '124.5', 'counters/examples': 163968, 'counters/updates': 5124}
train stats after 164000 examples: {'rewards_train/chosen': '0.073818', 'rewards_train/rejected': '0.074854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0010358', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-136.29', 'loss/train': '0.70218', 'examples_per_second': '33.019', 'grad_norm': '152', 'counters/examples': 164000, 'counters/updates': 5125}
train stats after 164032 examples: {'rewards_train/chosen': '0.08127', 'rewards_train/rejected': '0.05881', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02246', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-119.01', 'loss/train': '0.6943', 'examples_per_second': '31.294', 'grad_norm': '68.5', 'counters/examples': 164032, 'counters/updates': 5126}
skipping logging after 164064 examples to avoid logging too frequently
train stats after 164096 examples: {'rewards_train/chosen': '0.036514', 'rewards_train/rejected': '0.022986', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013528', 'logps_train/rejected': '-143.78', 'logps_train/chosen': '-200.38', 'loss/train': '0.70685', 'examples_per_second': '30.186', 'grad_norm': '61.5', 'counters/examples': 164096, 'counters/updates': 5128}
train stats after 164128 examples: {'rewards_train/chosen': '0.078646', 'rewards_train/rejected': '0.044446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0342', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-161.05', 'loss/train': '0.68997', 'examples_per_second': '32.42', 'grad_norm': '109.5', 'counters/examples': 164128, 'counters/updates': 5129}
skipping logging after 164160 examples to avoid logging too frequently
train stats after 164192 examples: {'rewards_train/chosen': '0.052777', 'rewards_train/rejected': '0.015013', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037763', 'logps_train/rejected': '-141', 'logps_train/chosen': '-192.52', 'loss/train': '0.68334', 'examples_per_second': '29.092', 'grad_norm': '169', 'counters/examples': 164192, 'counters/updates': 5131}
train stats after 164224 examples: {'rewards_train/chosen': '0.11763', 'rewards_train/rejected': '0.028362', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089272', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-162.45', 'loss/train': '0.66217', 'examples_per_second': '31.929', 'grad_norm': '85.5', 'counters/examples': 164224, 'counters/updates': 5132}
train stats after 164256 examples: {'rewards_train/chosen': '0.13471', 'rewards_train/rejected': '0.01698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11773', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-183.43', 'loss/train': '0.64935', 'examples_per_second': '32.543', 'grad_norm': '106', 'counters/examples': 164256, 'counters/updates': 5133}
skipping logging after 164288 examples to avoid logging too frequently
train stats after 164320 examples: {'rewards_train/chosen': '0.15665', 'rewards_train/rejected': '0.054364', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10229', 'logps_train/rejected': '-112.7', 'logps_train/chosen': '-123.66', 'loss/train': '0.65331', 'examples_per_second': '32.698', 'grad_norm': '57.25', 'counters/examples': 164320, 'counters/updates': 5135}
skipping logging after 164352 examples to avoid logging too frequently
train stats after 164384 examples: {'rewards_train/chosen': '0.047689', 'rewards_train/rejected': '0.050051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0023628', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-135.34', 'loss/train': '0.70456', 'examples_per_second': '30.099', 'grad_norm': '69.5', 'counters/examples': 164384, 'counters/updates': 5137}
train stats after 164416 examples: {'rewards_train/chosen': '0.13508', 'rewards_train/rejected': '0.015105', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11998', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-148.35', 'loss/train': '0.66959', 'examples_per_second': '31.587', 'grad_norm': '56.75', 'counters/examples': 164416, 'counters/updates': 5138}
train stats after 164448 examples: {'rewards_train/chosen': '0.15277', 'rewards_train/rejected': '0.054', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098771', 'logps_train/rejected': '-152.37', 'logps_train/chosen': '-166.07', 'loss/train': '0.65882', 'examples_per_second': '30.025', 'grad_norm': '92.5', 'counters/examples': 164448, 'counters/updates': 5139}
train stats after 164480 examples: {'rewards_train/chosen': '0.16029', 'rewards_train/rejected': '0.026341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13395', 'logps_train/rejected': '-171.96', 'logps_train/chosen': '-172.04', 'loss/train': '0.64278', 'examples_per_second': '32.484', 'grad_norm': '81.5', 'counters/examples': 164480, 'counters/updates': 5140}
train stats after 164512 examples: {'rewards_train/chosen': '0.14744', 'rewards_train/rejected': '0.04251', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10493', 'logps_train/rejected': '-198.48', 'logps_train/chosen': '-204.63', 'loss/train': '0.67168', 'examples_per_second': '29.972', 'grad_norm': '109', 'counters/examples': 164512, 'counters/updates': 5141}
train stats after 164544 examples: {'rewards_train/chosen': '0.077996', 'rewards_train/rejected': '-0.021707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099703', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-153.05', 'loss/train': '0.6561', 'examples_per_second': '30.621', 'grad_norm': '69.5', 'counters/examples': 164544, 'counters/updates': 5142}
skipping logging after 164576 examples to avoid logging too frequently
train stats after 164608 examples: {'rewards_train/chosen': '0.058257', 'rewards_train/rejected': '0.014816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043441', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-143.58', 'loss/train': '0.68094', 'examples_per_second': '30.128', 'grad_norm': '67', 'counters/examples': 164608, 'counters/updates': 5144}
train stats after 164640 examples: {'rewards_train/chosen': '0.064657', 'rewards_train/rejected': '0.053785', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010873', 'logps_train/rejected': '-130.12', 'logps_train/chosen': '-132.57', 'loss/train': '0.69271', 'examples_per_second': '30.753', 'grad_norm': '76.5', 'counters/examples': 164640, 'counters/updates': 5145}
skipping logging after 164672 examples to avoid logging too frequently
train stats after 164704 examples: {'rewards_train/chosen': '0.1298', 'rewards_train/rejected': '0.059911', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069894', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-134.99', 'loss/train': '0.66689', 'examples_per_second': '31.316', 'grad_norm': '80', 'counters/examples': 164704, 'counters/updates': 5147}
train stats after 164736 examples: {'rewards_train/chosen': '0.12176', 'rewards_train/rejected': '0.036741', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.085024', 'logps_train/rejected': '-118.13', 'logps_train/chosen': '-143.49', 'loss/train': '0.65992', 'examples_per_second': '31.521', 'grad_norm': '54.5', 'counters/examples': 164736, 'counters/updates': 5148}
train stats after 164768 examples: {'rewards_train/chosen': '0.079902', 'rewards_train/rejected': '0.038459', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041444', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-156.12', 'loss/train': '0.68262', 'examples_per_second': '31.509', 'grad_norm': '59.75', 'counters/examples': 164768, 'counters/updates': 5149}
train stats after 164800 examples: {'rewards_train/chosen': '0.16758', 'rewards_train/rejected': '0.059973', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10761', 'logps_train/rejected': '-138.35', 'logps_train/chosen': '-153.48', 'loss/train': '0.6563', 'examples_per_second': '31.765', 'grad_norm': '66', 'counters/examples': 164800, 'counters/updates': 5150}
skipping logging after 164832 examples to avoid logging too frequently
train stats after 164864 examples: {'rewards_train/chosen': '0.19829', 'rewards_train/rejected': '0.042465', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15583', 'logps_train/rejected': '-165.74', 'logps_train/chosen': '-173.94', 'loss/train': '0.65297', 'examples_per_second': '31.449', 'grad_norm': '92', 'counters/examples': 164864, 'counters/updates': 5152}
train stats after 164896 examples: {'rewards_train/chosen': '0.075567', 'rewards_train/rejected': '-0.04104', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11661', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-139.91', 'loss/train': '0.64899', 'examples_per_second': '30.788', 'grad_norm': '78', 'counters/examples': 164896, 'counters/updates': 5153}
skipping logging after 164928 examples to avoid logging too frequently
train stats after 164960 examples: {'rewards_train/chosen': '0.028884', 'rewards_train/rejected': '0.043885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015002', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-130.89', 'loss/train': '0.71595', 'examples_per_second': '30.55', 'grad_norm': '85.5', 'counters/examples': 164960, 'counters/updates': 5155}
train stats after 164992 examples: {'rewards_train/chosen': '0.021388', 'rewards_train/rejected': '0.047419', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026031', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-155.6', 'loss/train': '0.72557', 'examples_per_second': '31.539', 'grad_norm': '155', 'counters/examples': 164992, 'counters/updates': 5156}
train stats after 165024 examples: {'rewards_train/chosen': '0.08472', 'rewards_train/rejected': '0.040264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044457', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-125.23', 'loss/train': '0.68268', 'examples_per_second': '30.048', 'grad_norm': '102.5', 'counters/examples': 165024, 'counters/updates': 5157}
train stats after 165056 examples: {'rewards_train/chosen': '0.048186', 'rewards_train/rejected': '0.071794', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023607', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-136.21', 'loss/train': '0.7175', 'examples_per_second': '32.45', 'grad_norm': '84.5', 'counters/examples': 165056, 'counters/updates': 5158}
train stats after 165088 examples: {'rewards_train/chosen': '0.12947', 'rewards_train/rejected': '0.0017861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12769', 'logps_train/rejected': '-144.85', 'logps_train/chosen': '-174.35', 'loss/train': '0.64852', 'examples_per_second': '31.486', 'grad_norm': '498', 'counters/examples': 165088, 'counters/updates': 5159}
train stats after 165120 examples: {'rewards_train/chosen': '0.071243', 'rewards_train/rejected': '0.032002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039241', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-136.4', 'loss/train': '0.68787', 'examples_per_second': '32.844', 'grad_norm': '63.25', 'counters/examples': 165120, 'counters/updates': 5160}
train stats after 165152 examples: {'rewards_train/chosen': '0.12796', 'rewards_train/rejected': '0.049335', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078623', 'logps_train/rejected': '-149.56', 'logps_train/chosen': '-147.46', 'loss/train': '0.66651', 'examples_per_second': '30.485', 'grad_norm': '116', 'counters/examples': 165152, 'counters/updates': 5161}
train stats after 165184 examples: {'rewards_train/chosen': '0.14442', 'rewards_train/rejected': '0.038848', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10558', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-183.6', 'loss/train': '0.65573', 'examples_per_second': '31.571', 'grad_norm': '114.5', 'counters/examples': 165184, 'counters/updates': 5162}
train stats after 165216 examples: {'rewards_train/chosen': '0.08448', 'rewards_train/rejected': '0.088252', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0037714', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-150.22', 'loss/train': '0.7079', 'examples_per_second': '31.526', 'grad_norm': '84', 'counters/examples': 165216, 'counters/updates': 5163}
skipping logging after 165248 examples to avoid logging too frequently
train stats after 165280 examples: {'rewards_train/chosen': '0.017843', 'rewards_train/rejected': '0.069016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.051173', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-133.52', 'loss/train': '0.72968', 'examples_per_second': '31.476', 'grad_norm': '68.5', 'counters/examples': 165280, 'counters/updates': 5165}
train stats after 165312 examples: {'rewards_train/chosen': '0.074447', 'rewards_train/rejected': '-0.02098', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095427', 'logps_train/rejected': '-164.02', 'logps_train/chosen': '-155.61', 'loss/train': '0.66374', 'examples_per_second': '31.254', 'grad_norm': '54.75', 'counters/examples': 165312, 'counters/updates': 5166}
train stats after 165344 examples: {'rewards_train/chosen': '0.12408', 'rewards_train/rejected': '0.0027645', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12131', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-149.76', 'loss/train': '0.65228', 'examples_per_second': '30.514', 'grad_norm': '84.5', 'counters/examples': 165344, 'counters/updates': 5167}
skipping logging after 165376 examples to avoid logging too frequently
train stats after 165408 examples: {'rewards_train/chosen': '0.073705', 'rewards_train/rejected': '-0.02038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094085', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-141.18', 'loss/train': '0.65623', 'examples_per_second': '31.088', 'grad_norm': '64.5', 'counters/examples': 165408, 'counters/updates': 5169}
train stats after 165440 examples: {'rewards_train/chosen': '0.18947', 'rewards_train/rejected': '0.067022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12245', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-153.05', 'loss/train': '0.64969', 'examples_per_second': '31.561', 'grad_norm': '64.5', 'counters/examples': 165440, 'counters/updates': 5170}
train stats after 165472 examples: {'rewards_train/chosen': '0.048462', 'rewards_train/rejected': '0.030928', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017533', 'logps_train/rejected': '-102.82', 'logps_train/chosen': '-167.79', 'loss/train': '0.69344', 'examples_per_second': '31.25', 'grad_norm': '246', 'counters/examples': 165472, 'counters/updates': 5171}
train stats after 165504 examples: {'rewards_train/chosen': '0.072108', 'rewards_train/rejected': '0.084475', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012367', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-127.46', 'loss/train': '0.72557', 'examples_per_second': '30.886', 'grad_norm': '70.5', 'counters/examples': 165504, 'counters/updates': 5172}
train stats after 165536 examples: {'rewards_train/chosen': '0.12193', 'rewards_train/rejected': '-0.0077357', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12967', 'logps_train/rejected': '-161.17', 'logps_train/chosen': '-151.89', 'loss/train': '0.64641', 'examples_per_second': '30.687', 'grad_norm': '78.5', 'counters/examples': 165536, 'counters/updates': 5173}
train stats after 165568 examples: {'rewards_train/chosen': '0.09363', 'rewards_train/rejected': '-0.0083266', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10196', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-125.19', 'loss/train': '0.65524', 'examples_per_second': '31.889', 'grad_norm': '81', 'counters/examples': 165568, 'counters/updates': 5174}
train stats after 165600 examples: {'rewards_train/chosen': '0.16887', 'rewards_train/rejected': '0.0097895', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15908', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-153.21', 'loss/train': '0.62581', 'examples_per_second': '30.17', 'grad_norm': '81.5', 'counters/examples': 165600, 'counters/updates': 5175}
skipping logging after 165632 examples to avoid logging too frequently
train stats after 165664 examples: {'rewards_train/chosen': '0.1621', 'rewards_train/rejected': '0.070775', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.091327', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-146.35', 'loss/train': '0.67806', 'examples_per_second': '37.295', 'grad_norm': '70', 'counters/examples': 165664, 'counters/updates': 5177}
train stats after 165696 examples: {'rewards_train/chosen': '0.094825', 'rewards_train/rejected': '0.057066', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037759', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-143.37', 'loss/train': '0.6919', 'examples_per_second': '30.474', 'grad_norm': '107.5', 'counters/examples': 165696, 'counters/updates': 5178}
skipping logging after 165728 examples to avoid logging too frequently
train stats after 165760 examples: {'rewards_train/chosen': '0.055293', 'rewards_train/rejected': '0.07937', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024077', 'logps_train/rejected': '-137.87', 'logps_train/chosen': '-118.01', 'loss/train': '0.72212', 'examples_per_second': '31.777', 'grad_norm': '88.5', 'counters/examples': 165760, 'counters/updates': 5180}
train stats after 165792 examples: {'rewards_train/chosen': '0.076171', 'rewards_train/rejected': '0.016925', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059246', 'logps_train/rejected': '-141.62', 'logps_train/chosen': '-150.15', 'loss/train': '0.68111', 'examples_per_second': '31.746', 'grad_norm': '75', 'counters/examples': 165792, 'counters/updates': 5181}
train stats after 165824 examples: {'rewards_train/chosen': '0.10075', 'rewards_train/rejected': '0.13846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037703', 'logps_train/rejected': '-147.69', 'logps_train/chosen': '-142.42', 'loss/train': '0.7295', 'examples_per_second': '30.81', 'grad_norm': '105.5', 'counters/examples': 165824, 'counters/updates': 5182}
train stats after 165856 examples: {'rewards_train/chosen': '0.082172', 'rewards_train/rejected': '0.054215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027956', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-181.15', 'loss/train': '0.70805', 'examples_per_second': '30.09', 'grad_norm': '94', 'counters/examples': 165856, 'counters/updates': 5183}
train stats after 165888 examples: {'rewards_train/chosen': '0.19175', 'rewards_train/rejected': '0.09115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1006', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-180.08', 'loss/train': '0.6666', 'examples_per_second': '32.362', 'grad_norm': '147', 'counters/examples': 165888, 'counters/updates': 5184}
train stats after 165920 examples: {'rewards_train/chosen': '0.15519', 'rewards_train/rejected': '0.092397', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062793', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-146.41', 'loss/train': '0.67887', 'examples_per_second': '27.918', 'grad_norm': '101.5', 'counters/examples': 165920, 'counters/updates': 5185}
train stats after 165952 examples: {'rewards_train/chosen': '0.14179', 'rewards_train/rejected': '0.031824', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10997', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-125.34', 'loss/train': '0.65419', 'examples_per_second': '31.53', 'grad_norm': '76.5', 'counters/examples': 165952, 'counters/updates': 5186}
skipping logging after 165984 examples to avoid logging too frequently
train stats after 166016 examples: {'rewards_train/chosen': '0.090373', 'rewards_train/rejected': '0.074917', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015456', 'logps_train/rejected': '-173.82', 'logps_train/chosen': '-151.68', 'loss/train': '0.70037', 'examples_per_second': '30.337', 'grad_norm': '108', 'counters/examples': 166016, 'counters/updates': 5188}
train stats after 166048 examples: {'rewards_train/chosen': '0.098839', 'rewards_train/rejected': '0.092379', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0064599', 'logps_train/rejected': '-166.54', 'logps_train/chosen': '-155.44', 'loss/train': '0.71196', 'examples_per_second': '31.342', 'grad_norm': '78', 'counters/examples': 166048, 'counters/updates': 5189}
skipping logging after 166080 examples to avoid logging too frequently
train stats after 166112 examples: {'rewards_train/chosen': '0.10408', 'rewards_train/rejected': '0.052969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051108', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-156.21', 'loss/train': '0.68439', 'examples_per_second': '33.456', 'grad_norm': '70', 'counters/examples': 166112, 'counters/updates': 5191}
train stats after 166144 examples: {'rewards_train/chosen': '0.15054', 'rewards_train/rejected': '0.11195', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038583', 'logps_train/rejected': '-158.07', 'logps_train/chosen': '-160.74', 'loss/train': '0.69777', 'examples_per_second': '31.55', 'grad_norm': '106', 'counters/examples': 166144, 'counters/updates': 5192}
train stats after 166176 examples: {'rewards_train/chosen': '0.043224', 'rewards_train/rejected': '-0.006073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049297', 'logps_train/rejected': '-139.31', 'logps_train/chosen': '-116.59', 'loss/train': '0.68408', 'examples_per_second': '31.677', 'grad_norm': '86.5', 'counters/examples': 166176, 'counters/updates': 5193}
train stats after 166208 examples: {'rewards_train/chosen': '0.1062', 'rewards_train/rejected': '0.087995', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018204', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-199.16', 'loss/train': '0.69998', 'examples_per_second': '32.415', 'grad_norm': '144', 'counters/examples': 166208, 'counters/updates': 5194}
train stats after 166240 examples: {'rewards_train/chosen': '0.10032', 'rewards_train/rejected': '0.084112', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016204', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-146.35', 'loss/train': '0.70255', 'examples_per_second': '30.356', 'grad_norm': '86', 'counters/examples': 166240, 'counters/updates': 5195}
train stats after 166272 examples: {'rewards_train/chosen': '0.092283', 'rewards_train/rejected': '0.083455', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0088281', 'logps_train/rejected': '-147.59', 'logps_train/chosen': '-168', 'loss/train': '0.70706', 'examples_per_second': '30.332', 'grad_norm': '70', 'counters/examples': 166272, 'counters/updates': 5196}
train stats after 166304 examples: {'rewards_train/chosen': '0.096929', 'rewards_train/rejected': '0.066225', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030703', 'logps_train/rejected': '-129', 'logps_train/chosen': '-112.95', 'loss/train': '0.68631', 'examples_per_second': '31.405', 'grad_norm': '61', 'counters/examples': 166304, 'counters/updates': 5197}
train stats after 166336 examples: {'rewards_train/chosen': '0.1046', 'rewards_train/rejected': '0.025568', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079035', 'logps_train/rejected': '-145.54', 'logps_train/chosen': '-137.81', 'loss/train': '0.66693', 'examples_per_second': '31.559', 'grad_norm': '101.5', 'counters/examples': 166336, 'counters/updates': 5198}
train stats after 166368 examples: {'rewards_train/chosen': '0.10792', 'rewards_train/rejected': '0.040875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067047', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-133.35', 'loss/train': '0.66822', 'examples_per_second': '30.433', 'grad_norm': '70.5', 'counters/examples': 166368, 'counters/updates': 5199}
train stats after 166400 examples: {'rewards_train/chosen': '0.087991', 'rewards_train/rejected': '-0.0052734', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093264', 'logps_train/rejected': '-132.16', 'logps_train/chosen': '-147.84', 'loss/train': '0.66233', 'examples_per_second': '31.547', 'grad_norm': '83', 'counters/examples': 166400, 'counters/updates': 5200}
train stats after 166432 examples: {'rewards_train/chosen': '0.067947', 'rewards_train/rejected': '-0.0067284', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074675', 'logps_train/rejected': '-164.37', 'logps_train/chosen': '-162.4', 'loss/train': '0.66724', 'examples_per_second': '31.566', 'grad_norm': '99.5', 'counters/examples': 166432, 'counters/updates': 5201}
train stats after 166464 examples: {'rewards_train/chosen': '0.0052828', 'rewards_train/rejected': '0.047669', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042386', 'logps_train/rejected': '-100.61', 'logps_train/chosen': '-125.49', 'loss/train': '0.72338', 'examples_per_second': '32.442', 'grad_norm': '75', 'counters/examples': 166464, 'counters/updates': 5202}
train stats after 166496 examples: {'rewards_train/chosen': '0.067306', 'rewards_train/rejected': '-0.0066749', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073981', 'logps_train/rejected': '-124.73', 'logps_train/chosen': '-173.75', 'loss/train': '0.6636', 'examples_per_second': '31.564', 'grad_norm': '87.5', 'counters/examples': 166496, 'counters/updates': 5203}
train stats after 166528 examples: {'rewards_train/chosen': '0.017554', 'rewards_train/rejected': '0.064061', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046507', 'logps_train/rejected': '-107.3', 'logps_train/chosen': '-136.82', 'loss/train': '0.72464', 'examples_per_second': '31.497', 'grad_norm': '72.5', 'counters/examples': 166528, 'counters/updates': 5204}
train stats after 166560 examples: {'rewards_train/chosen': '0.077394', 'rewards_train/rejected': '-0.024422', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10182', 'logps_train/rejected': '-127.75', 'logps_train/chosen': '-148.03', 'loss/train': '0.64737', 'examples_per_second': '31.497', 'grad_norm': '77', 'counters/examples': 166560, 'counters/updates': 5205}
train stats after 166592 examples: {'rewards_train/chosen': '0.23206', 'rewards_train/rejected': '0.039227', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19283', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-157.96', 'loss/train': '0.61915', 'examples_per_second': '30.612', 'grad_norm': '97', 'counters/examples': 166592, 'counters/updates': 5206}
train stats after 166624 examples: {'rewards_train/chosen': '0.051569', 'rewards_train/rejected': '0.017849', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03372', 'logps_train/rejected': '-99.138', 'logps_train/chosen': '-114.59', 'loss/train': '0.6834', 'examples_per_second': '31.117', 'grad_norm': '93', 'counters/examples': 166624, 'counters/updates': 5207}
train stats after 166656 examples: {'rewards_train/chosen': '0.051855', 'rewards_train/rejected': '0.073805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021949', 'logps_train/rejected': '-150.07', 'logps_train/chosen': '-145.09', 'loss/train': '0.7134', 'examples_per_second': '31.576', 'grad_norm': '104', 'counters/examples': 166656, 'counters/updates': 5208}
train stats after 166688 examples: {'rewards_train/chosen': '0.12428', 'rewards_train/rejected': '0.079412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044869', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-172.3', 'loss/train': '0.67825', 'examples_per_second': '30.138', 'grad_norm': '63', 'counters/examples': 166688, 'counters/updates': 5209}
skipping logging after 166720 examples to avoid logging too frequently
train stats after 166752 examples: {'rewards_train/chosen': '0.095687', 'rewards_train/rejected': '0.095893', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00020555', 'logps_train/rejected': '-151.36', 'logps_train/chosen': '-176.98', 'loss/train': '0.71277', 'examples_per_second': '31.457', 'grad_norm': '255', 'counters/examples': 166752, 'counters/updates': 5211}
skipping logging after 166784 examples to avoid logging too frequently
train stats after 166816 examples: {'rewards_train/chosen': '0.24736', 'rewards_train/rejected': '0.082398', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16496', 'logps_train/rejected': '-159.28', 'logps_train/chosen': '-161.53', 'loss/train': '0.65213', 'examples_per_second': '30.401', 'grad_norm': '61.75', 'counters/examples': 166816, 'counters/updates': 5213}
train stats after 166848 examples: {'rewards_train/chosen': '0.039444', 'rewards_train/rejected': '0.032652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0067921', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-99.412', 'loss/train': '0.69874', 'examples_per_second': '31.288', 'grad_norm': '52.25', 'counters/examples': 166848, 'counters/updates': 5214}
train stats after 166880 examples: {'rewards_train/chosen': '0.025073', 'rewards_train/rejected': '0.025722', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00064909', 'logps_train/rejected': '-153.73', 'logps_train/chosen': '-184.25', 'loss/train': '0.70152', 'examples_per_second': '31.494', 'grad_norm': '77.5', 'counters/examples': 166880, 'counters/updates': 5215}
skipping logging after 166912 examples to avoid logging too frequently
train stats after 166944 examples: {'rewards_train/chosen': '0.063638', 'rewards_train/rejected': '0.056286', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.007352', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-145.5', 'loss/train': '0.69513', 'examples_per_second': '30.834', 'grad_norm': '76', 'counters/examples': 166944, 'counters/updates': 5217}
train stats after 166976 examples: {'rewards_train/chosen': '0.096715', 'rewards_train/rejected': '0.018243', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078473', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-189.82', 'loss/train': '0.6779', 'examples_per_second': '31.586', 'grad_norm': '85', 'counters/examples': 166976, 'counters/updates': 5218}
train stats after 167008 examples: {'rewards_train/chosen': '0.15337', 'rewards_train/rejected': '0.077529', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07584', 'logps_train/rejected': '-143.39', 'logps_train/chosen': '-146.78', 'loss/train': '0.66287', 'examples_per_second': '31.221', 'grad_norm': '68', 'counters/examples': 167008, 'counters/updates': 5219}
skipping logging after 167040 examples to avoid logging too frequently
train stats after 167072 examples: {'rewards_train/chosen': '0.038785', 'rewards_train/rejected': '0.10688', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.068097', 'logps_train/rejected': '-144.03', 'logps_train/chosen': '-131.79', 'loss/train': '0.74256', 'examples_per_second': '30.449', 'grad_norm': '114', 'counters/examples': 167072, 'counters/updates': 5221}
skipping logging after 167104 examples to avoid logging too frequently
train stats after 167136 examples: {'rewards_train/chosen': '0.11383', 'rewards_train/rejected': '0.020906', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092923', 'logps_train/rejected': '-153.08', 'logps_train/chosen': '-148.05', 'loss/train': '0.66109', 'examples_per_second': '30.175', 'grad_norm': '78.5', 'counters/examples': 167136, 'counters/updates': 5223}
train stats after 167168 examples: {'rewards_train/chosen': '0.041096', 'rewards_train/rejected': '0.05391', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.012814', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-173.88', 'loss/train': '0.71917', 'examples_per_second': '33.136', 'grad_norm': '97.5', 'counters/examples': 167168, 'counters/updates': 5224}
skipping logging after 167200 examples to avoid logging too frequently
train stats after 167232 examples: {'rewards_train/chosen': '0.1252', 'rewards_train/rejected': '0.056416', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068787', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-145.33', 'loss/train': '0.67578', 'examples_per_second': '33.741', 'grad_norm': '88.5', 'counters/examples': 167232, 'counters/updates': 5226}
train stats after 167264 examples: {'rewards_train/chosen': '0.045632', 'rewards_train/rejected': '0.061443', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015811', 'logps_train/rejected': '-142.15', 'logps_train/chosen': '-163.53', 'loss/train': '0.71727', 'examples_per_second': '31.295', 'grad_norm': '72', 'counters/examples': 167264, 'counters/updates': 5227}
train stats after 167296 examples: {'rewards_train/chosen': '0.18194', 'rewards_train/rejected': '0.14391', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038022', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-178.18', 'loss/train': '0.68215', 'examples_per_second': '31.333', 'grad_norm': '96.5', 'counters/examples': 167296, 'counters/updates': 5228}
train stats after 167328 examples: {'rewards_train/chosen': '0.070562', 'rewards_train/rejected': '-0.011653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082215', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-152.37', 'loss/train': '0.66844', 'examples_per_second': '31.587', 'grad_norm': '88.5', 'counters/examples': 167328, 'counters/updates': 5229}
train stats after 167360 examples: {'rewards_train/chosen': '0.070353', 'rewards_train/rejected': '0.103', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032645', 'logps_train/rejected': '-154.11', 'logps_train/chosen': '-183.06', 'loss/train': '0.73066', 'examples_per_second': '31.293', 'grad_norm': '82.5', 'counters/examples': 167360, 'counters/updates': 5230}
skipping logging after 167392 examples to avoid logging too frequently
train stats after 167424 examples: {'rewards_train/chosen': '0.11004', 'rewards_train/rejected': '0.078064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031979', 'logps_train/rejected': '-155.13', 'logps_train/chosen': '-161.72', 'loss/train': '0.69904', 'examples_per_second': '35.426', 'grad_norm': '58.75', 'counters/examples': 167424, 'counters/updates': 5232}
train stats after 167456 examples: {'rewards_train/chosen': '0.14425', 'rewards_train/rejected': '0.0098035', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13444', 'logps_train/rejected': '-157.92', 'logps_train/chosen': '-174.38', 'loss/train': '0.64254', 'examples_per_second': '31.136', 'grad_norm': '77', 'counters/examples': 167456, 'counters/updates': 5233}
skipping logging after 167488 examples to avoid logging too frequently
train stats after 167520 examples: {'rewards_train/chosen': '0.090404', 'rewards_train/rejected': '0.051267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039138', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-151.8', 'loss/train': '0.687', 'examples_per_second': '31.593', 'grad_norm': '52.5', 'counters/examples': 167520, 'counters/updates': 5235}
skipping logging after 167552 examples to avoid logging too frequently
train stats after 167584 examples: {'rewards_train/chosen': '0.088307', 'rewards_train/rejected': '-0.01546', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10377', 'logps_train/rejected': '-151.31', 'logps_train/chosen': '-170.58', 'loss/train': '0.6616', 'examples_per_second': '32.586', 'grad_norm': '76', 'counters/examples': 167584, 'counters/updates': 5237}
train stats after 167616 examples: {'rewards_train/chosen': '0.063997', 'rewards_train/rejected': '-0.039468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10347', 'logps_train/rejected': '-109.91', 'logps_train/chosen': '-161.14', 'loss/train': '0.65262', 'examples_per_second': '31.792', 'grad_norm': '51.5', 'counters/examples': 167616, 'counters/updates': 5238}
skipping logging after 167648 examples to avoid logging too frequently
train stats after 167680 examples: {'rewards_train/chosen': '0.092988', 'rewards_train/rejected': '0.023146', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069843', 'logps_train/rejected': '-155.45', 'logps_train/chosen': '-147.79', 'loss/train': '0.67234', 'examples_per_second': '38.116', 'grad_norm': '70', 'counters/examples': 167680, 'counters/updates': 5240}
train stats after 167712 examples: {'rewards_train/chosen': '0.096185', 'rewards_train/rejected': '0.038544', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.057642', 'logps_train/rejected': '-105.19', 'logps_train/chosen': '-116.78', 'loss/train': '0.68268', 'examples_per_second': '31.244', 'grad_norm': '95', 'counters/examples': 167712, 'counters/updates': 5241}
train stats after 167744 examples: {'rewards_train/chosen': '0.1519', 'rewards_train/rejected': '-0.017645', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16954', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-157.8', 'loss/train': '0.62098', 'examples_per_second': '30.342', 'grad_norm': '61.75', 'counters/examples': 167744, 'counters/updates': 5242}
train stats after 167776 examples: {'rewards_train/chosen': '0.0998', 'rewards_train/rejected': '0.047544', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052255', 'logps_train/rejected': '-99.333', 'logps_train/chosen': '-91.302', 'loss/train': '0.6764', 'examples_per_second': '30.013', 'grad_norm': '48.75', 'counters/examples': 167776, 'counters/updates': 5243}
train stats after 167808 examples: {'rewards_train/chosen': '0.11589', 'rewards_train/rejected': '0.10479', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011106', 'logps_train/rejected': '-160.57', 'logps_train/chosen': '-163.46', 'loss/train': '0.70183', 'examples_per_second': '30.716', 'grad_norm': '134', 'counters/examples': 167808, 'counters/updates': 5244}
skipping logging after 167840 examples to avoid logging too frequently
train stats after 167872 examples: {'rewards_train/chosen': '0.093143', 'rewards_train/rejected': '0.10176', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0086206', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-149.02', 'loss/train': '0.71579', 'examples_per_second': '31.499', 'grad_norm': '197', 'counters/examples': 167872, 'counters/updates': 5246}
train stats after 167904 examples: {'rewards_train/chosen': '0.13753', 'rewards_train/rejected': '0.098334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039193', 'logps_train/rejected': '-111.71', 'logps_train/chosen': '-141.72', 'loss/train': '0.68666', 'examples_per_second': '31.546', 'grad_norm': '121.5', 'counters/examples': 167904, 'counters/updates': 5247}
train stats after 167936 examples: {'rewards_train/chosen': '0.033737', 'rewards_train/rejected': '0.12536', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.091622', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-142.18', 'loss/train': '0.75362', 'examples_per_second': '32.84', 'grad_norm': '101', 'counters/examples': 167936, 'counters/updates': 5248}
train stats after 167968 examples: {'rewards_train/chosen': '0.11408', 'rewards_train/rejected': '0.014493', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099588', 'logps_train/rejected': '-129.36', 'logps_train/chosen': '-145.01', 'loss/train': '0.65891', 'examples_per_second': '32.608', 'grad_norm': '83.5', 'counters/examples': 167968, 'counters/updates': 5249}
train stats after 168000 examples: {'rewards_train/chosen': '0.050644', 'rewards_train/rejected': '0.046823', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0038215', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-103.65', 'loss/train': '0.70587', 'examples_per_second': '31.646', 'grad_norm': '72.5', 'counters/examples': 168000, 'counters/updates': 5250}
Running evaluation after 168000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 168000: {'rewards_eval/chosen': '0.13822', 'rewards_eval/rejected': '0.063779', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.074438', 'logps_eval/rejected': '-127.51', 'logps_eval/chosen': '-149.62', 'loss/eval': '0.67117'}
skipping save for non epoch
train stats after 168032 examples: {'rewards_train/chosen': '0.087997', 'rewards_train/rejected': '-0.0021017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090098', 'logps_train/rejected': '-105.79', 'logps_train/chosen': '-126.74', 'loss/train': '0.66543', 'examples_per_second': '32.435', 'grad_norm': '56.25', 'counters/examples': 168032, 'counters/updates': 5251}
train stats after 168064 examples: {'rewards_train/chosen': '0.11607', 'rewards_train/rejected': '0.042241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073828', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-162.93', 'loss/train': '0.66437', 'examples_per_second': '32.271', 'grad_norm': '91.5', 'counters/examples': 168064, 'counters/updates': 5252}
train stats after 168096 examples: {'rewards_train/chosen': '0.019477', 'rewards_train/rejected': '0.076472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.056995', 'logps_train/rejected': '-114.61', 'logps_train/chosen': '-125.93', 'loss/train': '0.73757', 'examples_per_second': '31.609', 'grad_norm': '59.5', 'counters/examples': 168096, 'counters/updates': 5253}
train stats after 168128 examples: {'rewards_train/chosen': '0.15739', 'rewards_train/rejected': '0.050467', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10693', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-142.54', 'loss/train': '0.64918', 'examples_per_second': '31.026', 'grad_norm': '58.5', 'counters/examples': 168128, 'counters/updates': 5254}
train stats after 168160 examples: {'rewards_train/chosen': '0.045203', 'rewards_train/rejected': '0.029222', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015981', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-160.22', 'loss/train': '0.69535', 'examples_per_second': '31.605', 'grad_norm': '54', 'counters/examples': 168160, 'counters/updates': 5255}
train stats after 168192 examples: {'rewards_train/chosen': '0.24597', 'rewards_train/rejected': '0.062783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18318', 'logps_train/rejected': '-110.48', 'logps_train/chosen': '-156.3', 'loss/train': '0.62351', 'examples_per_second': '31.78', 'grad_norm': '78', 'counters/examples': 168192, 'counters/updates': 5256}
train stats after 168224 examples: {'rewards_train/chosen': '0.077949', 'rewards_train/rejected': '0.05651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02144', 'logps_train/rejected': '-159.42', 'logps_train/chosen': '-175.29', 'loss/train': '0.69867', 'examples_per_second': '31.367', 'grad_norm': '90.5', 'counters/examples': 168224, 'counters/updates': 5257}
train stats after 168256 examples: {'rewards_train/chosen': '0.11078', 'rewards_train/rejected': '0.033775', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077008', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-158.83', 'loss/train': '0.67183', 'examples_per_second': '31.526', 'grad_norm': '68.5', 'counters/examples': 168256, 'counters/updates': 5258}
train stats after 168288 examples: {'rewards_train/chosen': '0.15615', 'rewards_train/rejected': '0.0075876', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14856', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-156.55', 'loss/train': '0.64614', 'examples_per_second': '31.68', 'grad_norm': '83', 'counters/examples': 168288, 'counters/updates': 5259}
train stats after 168320 examples: {'rewards_train/chosen': '0.018086', 'rewards_train/rejected': '0.033374', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015288', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-117.14', 'loss/train': '0.71173', 'examples_per_second': '33.067', 'grad_norm': '119.5', 'counters/examples': 168320, 'counters/updates': 5260}
train stats after 168352 examples: {'rewards_train/chosen': '0.062196', 'rewards_train/rejected': '0.08963', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027434', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-118.69', 'loss/train': '0.72636', 'examples_per_second': '24.324', 'grad_norm': '62.75', 'counters/examples': 168352, 'counters/updates': 5261}
train stats after 168384 examples: {'rewards_train/chosen': '0.071817', 'rewards_train/rejected': '0.12171', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.049892', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-146.3', 'loss/train': '0.73643', 'examples_per_second': '32.838', 'grad_norm': '90.5', 'counters/examples': 168384, 'counters/updates': 5262}
train stats after 168416 examples: {'rewards_train/chosen': '0.056978', 'rewards_train/rejected': '0.031382', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025596', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-166.74', 'loss/train': '0.69988', 'examples_per_second': '30.123', 'grad_norm': '114', 'counters/examples': 168416, 'counters/updates': 5263}
train stats after 168448 examples: {'rewards_train/chosen': '0.19955', 'rewards_train/rejected': '0.13013', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069417', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-157.66', 'loss/train': '0.68475', 'examples_per_second': '24.306', 'grad_norm': '116.5', 'counters/examples': 168448, 'counters/updates': 5264}
train stats after 168480 examples: {'rewards_train/chosen': '0.10138', 'rewards_train/rejected': '0.079096', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022284', 'logps_train/rejected': '-91.352', 'logps_train/chosen': '-117.06', 'loss/train': '0.69527', 'examples_per_second': '32.999', 'grad_norm': '90.5', 'counters/examples': 168480, 'counters/updates': 5265}
train stats after 168512 examples: {'rewards_train/chosen': '0.071942', 'rewards_train/rejected': '0.12274', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.050796', 'logps_train/rejected': '-128.26', 'logps_train/chosen': '-169.79', 'loss/train': '0.73558', 'examples_per_second': '30.168', 'grad_norm': '193', 'counters/examples': 168512, 'counters/updates': 5266}
train stats after 168544 examples: {'rewards_train/chosen': '0.11472', 'rewards_train/rejected': '0.067413', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047303', 'logps_train/rejected': '-174.69', 'logps_train/chosen': '-136.74', 'loss/train': '0.68721', 'examples_per_second': '31.61', 'grad_norm': '131', 'counters/examples': 168544, 'counters/updates': 5267}
train stats after 168576 examples: {'rewards_train/chosen': '0.10194', 'rewards_train/rejected': '-0.0064491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10839', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-152.41', 'loss/train': '0.65034', 'examples_per_second': '31.352', 'grad_norm': '61.5', 'counters/examples': 168576, 'counters/updates': 5268}
train stats after 168608 examples: {'rewards_train/chosen': '0.093713', 'rewards_train/rejected': '0.023326', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070387', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-136.95', 'loss/train': '0.67712', 'examples_per_second': '30.904', 'grad_norm': '412', 'counters/examples': 168608, 'counters/updates': 5269}
train stats after 168640 examples: {'rewards_train/chosen': '0.10342', 'rewards_train/rejected': '0.092468', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010948', 'logps_train/rejected': '-105.2', 'logps_train/chosen': '-118.93', 'loss/train': '0.69646', 'examples_per_second': '30.795', 'grad_norm': '172', 'counters/examples': 168640, 'counters/updates': 5270}
train stats after 168672 examples: {'rewards_train/chosen': '0.15357', 'rewards_train/rejected': '0.10602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047544', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-164.16', 'loss/train': '0.6823', 'examples_per_second': '31.601', 'grad_norm': '91', 'counters/examples': 168672, 'counters/updates': 5271}
train stats after 168704 examples: {'rewards_train/chosen': '0.097555', 'rewards_train/rejected': '0.058205', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.03935', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-143.86', 'loss/train': '0.69104', 'examples_per_second': '31.641', 'grad_norm': '236', 'counters/examples': 168704, 'counters/updates': 5272}
train stats after 168736 examples: {'rewards_train/chosen': '0.0821', 'rewards_train/rejected': '-0.034897', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.117', 'logps_train/rejected': '-176.42', 'logps_train/chosen': '-211.51', 'loss/train': '0.65407', 'examples_per_second': '31.333', 'grad_norm': '151', 'counters/examples': 168736, 'counters/updates': 5273}
train stats after 168768 examples: {'rewards_train/chosen': '0.10551', 'rewards_train/rejected': '0.0090563', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.096452', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-142.1', 'loss/train': '0.67043', 'examples_per_second': '31.995', 'grad_norm': '103', 'counters/examples': 168768, 'counters/updates': 5274}
skipping logging after 168800 examples to avoid logging too frequently
train stats after 168832 examples: {'rewards_train/chosen': '0.027032', 'rewards_train/rejected': '0.13017', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.10314', 'logps_train/rejected': '-192.72', 'logps_train/chosen': '-165.52', 'loss/train': '0.76273', 'examples_per_second': '30.429', 'grad_norm': '143', 'counters/examples': 168832, 'counters/updates': 5276}
train stats after 168864 examples: {'rewards_train/chosen': '0.16699', 'rewards_train/rejected': '0.054813', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11217', 'logps_train/rejected': '-155.58', 'logps_train/chosen': '-131.59', 'loss/train': '0.65885', 'examples_per_second': '32.389', 'grad_norm': '56.75', 'counters/examples': 168864, 'counters/updates': 5277}
train stats after 168896 examples: {'rewards_train/chosen': '-0.022442', 'rewards_train/rejected': '0.099435', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.12188', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-137.7', 'loss/train': '0.76353', 'examples_per_second': '31.295', 'grad_norm': '75.5', 'counters/examples': 168896, 'counters/updates': 5278}
train stats after 168928 examples: {'rewards_train/chosen': '0.044542', 'rewards_train/rejected': '0.021357', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023185', 'logps_train/rejected': '-83.896', 'logps_train/chosen': '-107.18', 'loss/train': '0.6878', 'examples_per_second': '31.625', 'grad_norm': '95', 'counters/examples': 168928, 'counters/updates': 5279}
train stats after 168960 examples: {'rewards_train/chosen': '0.22386', 'rewards_train/rejected': '0.093895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12997', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-164.56', 'loss/train': '0.65264', 'examples_per_second': '31.646', 'grad_norm': '205', 'counters/examples': 168960, 'counters/updates': 5280}
train stats after 168992 examples: {'rewards_train/chosen': '0.18297', 'rewards_train/rejected': '0.040619', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14236', 'logps_train/rejected': '-139.78', 'logps_train/chosen': '-131.71', 'loss/train': '0.64725', 'examples_per_second': '31.758', 'grad_norm': '102', 'counters/examples': 168992, 'counters/updates': 5281}
train stats after 169024 examples: {'rewards_train/chosen': '0.089296', 'rewards_train/rejected': '0.048331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040965', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-159.37', 'loss/train': '0.68281', 'examples_per_second': '32.022', 'grad_norm': '105', 'counters/examples': 169024, 'counters/updates': 5282}
train stats after 169056 examples: {'rewards_train/chosen': '0.11035', 'rewards_train/rejected': '0.10297', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0073751', 'logps_train/rejected': '-159.89', 'logps_train/chosen': '-194.25', 'loss/train': '0.70973', 'examples_per_second': '31.989', 'grad_norm': '100.5', 'counters/examples': 169056, 'counters/updates': 5283}
skipping logging after 169088 examples to avoid logging too frequently
train stats after 169120 examples: {'rewards_train/chosen': '0.092466', 'rewards_train/rejected': '0.032073', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060392', 'logps_train/rejected': '-137.18', 'logps_train/chosen': '-150.7', 'loss/train': '0.67311', 'examples_per_second': '30.893', 'grad_norm': '88.5', 'counters/examples': 169120, 'counters/updates': 5285}
train stats after 169152 examples: {'rewards_train/chosen': '0.060358', 'rewards_train/rejected': '0.027385', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032973', 'logps_train/rejected': '-137.39', 'logps_train/chosen': '-148.79', 'loss/train': '0.69347', 'examples_per_second': '32.405', 'grad_norm': '69.5', 'counters/examples': 169152, 'counters/updates': 5286}
train stats after 169184 examples: {'rewards_train/chosen': '0.14015', 'rewards_train/rejected': '0.11298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027173', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-174.92', 'loss/train': '0.69504', 'examples_per_second': '31.657', 'grad_norm': '72', 'counters/examples': 169184, 'counters/updates': 5287}
train stats after 169216 examples: {'rewards_train/chosen': '0.047594', 'rewards_train/rejected': '0.18733', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.13974', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-128.41', 'loss/train': '0.78924', 'examples_per_second': '31.352', 'grad_norm': '102', 'counters/examples': 169216, 'counters/updates': 5288}
train stats after 169248 examples: {'rewards_train/chosen': '0.12768', 'rewards_train/rejected': '0.12582', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0018598', 'logps_train/rejected': '-184.71', 'logps_train/chosen': '-167.92', 'loss/train': '0.71607', 'examples_per_second': '31.775', 'grad_norm': '81', 'counters/examples': 169248, 'counters/updates': 5289}
skipping logging after 169280 examples to avoid logging too frequently
train stats after 169312 examples: {'rewards_train/chosen': '0.045294', 'rewards_train/rejected': '0.054718', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0094238', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-151.96', 'loss/train': '0.7131', 'examples_per_second': '31.537', 'grad_norm': '86.5', 'counters/examples': 169312, 'counters/updates': 5291}
train stats after 169344 examples: {'rewards_train/chosen': '0.16209', 'rewards_train/rejected': '0.079134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082954', 'logps_train/rejected': '-167.42', 'logps_train/chosen': '-190.62', 'loss/train': '0.66921', 'examples_per_second': '32.375', 'grad_norm': '94.5', 'counters/examples': 169344, 'counters/updates': 5292}
train stats after 169376 examples: {'rewards_train/chosen': '0.08209', 'rewards_train/rejected': '-0.0023768', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084467', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-135.9', 'loss/train': '0.67025', 'examples_per_second': '31.168', 'grad_norm': '92', 'counters/examples': 169376, 'counters/updates': 5293}
train stats after 169408 examples: {'rewards_train/chosen': '0.11322', 'rewards_train/rejected': '0.10403', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0091924', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-123.1', 'loss/train': '0.70861', 'examples_per_second': '30.702', 'grad_norm': '60', 'counters/examples': 169408, 'counters/updates': 5294}
train stats after 169440 examples: {'rewards_train/chosen': '0.14526', 'rewards_train/rejected': '0.037112', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10815', 'logps_train/rejected': '-155.91', 'logps_train/chosen': '-161.07', 'loss/train': '0.67235', 'examples_per_second': '31.806', 'grad_norm': '154', 'counters/examples': 169440, 'counters/updates': 5295}
skipping logging after 169472 examples to avoid logging too frequently
train stats after 169504 examples: {'rewards_train/chosen': '0.044089', 'rewards_train/rejected': '0.12779', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.083698', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-155.28', 'loss/train': '0.75454', 'examples_per_second': '30.903', 'grad_norm': '92.5', 'counters/examples': 169504, 'counters/updates': 5297}
train stats after 169536 examples: {'rewards_train/chosen': '0.015732', 'rewards_train/rejected': '0.054388', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.038656', 'logps_train/rejected': '-149.13', 'logps_train/chosen': '-118.34', 'loss/train': '0.72339', 'examples_per_second': '31.601', 'grad_norm': '89', 'counters/examples': 169536, 'counters/updates': 5298}
train stats after 169568 examples: {'rewards_train/chosen': '0.13365', 'rewards_train/rejected': '0.1652', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03155', 'logps_train/rejected': '-145.82', 'logps_train/chosen': '-164.91', 'loss/train': '0.73028', 'examples_per_second': '31.62', 'grad_norm': '92', 'counters/examples': 169568, 'counters/updates': 5299}
train stats after 169600 examples: {'rewards_train/chosen': '0.018125', 'rewards_train/rejected': '0.1908', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.17267', 'logps_train/rejected': '-162.02', 'logps_train/chosen': '-122.43', 'loss/train': '0.79967', 'examples_per_second': '32.884', 'grad_norm': '102', 'counters/examples': 169600, 'counters/updates': 5300}
skipping logging after 169632 examples to avoid logging too frequently
train stats after 169664 examples: {'rewards_train/chosen': '0.15125', 'rewards_train/rejected': '0.089671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061575', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-155.46', 'loss/train': '0.69708', 'examples_per_second': '37.424', 'grad_norm': '77.5', 'counters/examples': 169664, 'counters/updates': 5302}
train stats after 169696 examples: {'rewards_train/chosen': '0.10635', 'rewards_train/rejected': '0.034158', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072194', 'logps_train/rejected': '-148.17', 'logps_train/chosen': '-149.16', 'loss/train': '0.66987', 'examples_per_second': '32.68', 'grad_norm': '79', 'counters/examples': 169696, 'counters/updates': 5303}
train stats after 169728 examples: {'rewards_train/chosen': '0.080395', 'rewards_train/rejected': '-0.043363', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12376', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-158.03', 'loss/train': '0.65622', 'examples_per_second': '31.543', 'grad_norm': '108.5', 'counters/examples': 169728, 'counters/updates': 5304}
train stats after 169760 examples: {'rewards_train/chosen': '0.036089', 'rewards_train/rejected': '-0.034774', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070863', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-134.31', 'loss/train': '0.67471', 'examples_per_second': '31.078', 'grad_norm': '69.5', 'counters/examples': 169760, 'counters/updates': 5305}
train stats after 169792 examples: {'rewards_train/chosen': '0.13747', 'rewards_train/rejected': '0.085537', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051932', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-146.14', 'loss/train': '0.68031', 'examples_per_second': '33', 'grad_norm': '109', 'counters/examples': 169792, 'counters/updates': 5306}
skipping logging after 169824 examples to avoid logging too frequently
train stats after 169856 examples: {'rewards_train/chosen': '0.14729', 'rewards_train/rejected': '0.070783', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076505', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-149.58', 'loss/train': '0.67254', 'examples_per_second': '31.396', 'grad_norm': '153', 'counters/examples': 169856, 'counters/updates': 5308}
skipping logging after 169888 examples to avoid logging too frequently
train stats after 169920 examples: {'rewards_train/chosen': '0.17099', 'rewards_train/rejected': '0.030429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14057', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-155.25', 'loss/train': '0.63489', 'examples_per_second': '30.883', 'grad_norm': '80', 'counters/examples': 169920, 'counters/updates': 5310}
skipping logging after 169952 examples to avoid logging too frequently
train stats after 169984 examples: {'rewards_train/chosen': '0.15977', 'rewards_train/rejected': '0.055862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10391', 'logps_train/rejected': '-126.2', 'logps_train/chosen': '-146.52', 'loss/train': '0.65781', 'examples_per_second': '32.891', 'grad_norm': '71.5', 'counters/examples': 169984, 'counters/updates': 5312}
train stats after 170016 examples: {'rewards_train/chosen': '0.11053', 'rewards_train/rejected': '0.068681', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041846', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-166.28', 'loss/train': '0.68388', 'examples_per_second': '30.695', 'grad_norm': '131', 'counters/examples': 170016, 'counters/updates': 5313}
skipping logging after 170048 examples to avoid logging too frequently
train stats after 170080 examples: {'rewards_train/chosen': '0.075978', 'rewards_train/rejected': '0.095075', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019097', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-87.416', 'loss/train': '0.71712', 'examples_per_second': '35.355', 'grad_norm': '99', 'counters/examples': 170080, 'counters/updates': 5315}
train stats after 170112 examples: {'rewards_train/chosen': '-0.014877', 'rewards_train/rejected': '-0.024367', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00949', 'logps_train/rejected': '-151.37', 'logps_train/chosen': '-150.68', 'loss/train': '0.697', 'examples_per_second': '30.283', 'grad_norm': '104.5', 'counters/examples': 170112, 'counters/updates': 5316}
train stats after 170144 examples: {'rewards_train/chosen': '-0.042921', 'rewards_train/rejected': '0.054089', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.09701', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-161.99', 'loss/train': '0.79102', 'examples_per_second': '30.841', 'grad_norm': '91', 'counters/examples': 170144, 'counters/updates': 5317}
train stats after 170176 examples: {'rewards_train/chosen': '0.17066', 'rewards_train/rejected': '0.033666', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13699', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-174.7', 'loss/train': '0.64362', 'examples_per_second': '31.645', 'grad_norm': '90', 'counters/examples': 170176, 'counters/updates': 5318}
train stats after 170208 examples: {'rewards_train/chosen': '0.077023', 'rewards_train/rejected': '0.067021', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.010002', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-141.79', 'loss/train': '0.70346', 'examples_per_second': '31.57', 'grad_norm': '69', 'counters/examples': 170208, 'counters/updates': 5319}
skipping logging after 170240 examples to avoid logging too frequently
train stats after 170272 examples: {'rewards_train/chosen': '0.15759', 'rewards_train/rejected': '0.010646', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14694', 'logps_train/rejected': '-101.13', 'logps_train/chosen': '-149.65', 'loss/train': '0.63544', 'examples_per_second': '34.334', 'grad_norm': '62.5', 'counters/examples': 170272, 'counters/updates': 5321}
train stats after 170304 examples: {'rewards_train/chosen': '0.13135', 'rewards_train/rejected': '0.10893', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022413', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-158.02', 'loss/train': '0.69262', 'examples_per_second': '31.786', 'grad_norm': '106', 'counters/examples': 170304, 'counters/updates': 5322}
skipping logging after 170336 examples to avoid logging too frequently
train stats after 170368 examples: {'rewards_train/chosen': '0.14646', 'rewards_train/rejected': '0.076276', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070179', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-141.9', 'loss/train': '0.66459', 'examples_per_second': '31.747', 'grad_norm': '53', 'counters/examples': 170368, 'counters/updates': 5324}
train stats after 170400 examples: {'rewards_train/chosen': '0.11406', 'rewards_train/rejected': '0.015727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098334', 'logps_train/rejected': '-135.49', 'logps_train/chosen': '-150.24', 'loss/train': '0.65727', 'examples_per_second': '31.771', 'grad_norm': '139', 'counters/examples': 170400, 'counters/updates': 5325}
train stats after 170432 examples: {'rewards_train/chosen': '0.22971', 'rewards_train/rejected': '0.14998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079728', 'logps_train/rejected': '-146.03', 'logps_train/chosen': '-146.91', 'loss/train': '0.68381', 'examples_per_second': '31.542', 'grad_norm': '171', 'counters/examples': 170432, 'counters/updates': 5326}
train stats after 170464 examples: {'rewards_train/chosen': '0.067981', 'rewards_train/rejected': '0.041821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02616', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-156.87', 'loss/train': '0.68949', 'examples_per_second': '30.712', 'grad_norm': '78', 'counters/examples': 170464, 'counters/updates': 5327}
train stats after 170496 examples: {'rewards_train/chosen': '0.041464', 'rewards_train/rejected': '0.10661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.065141', 'logps_train/rejected': '-136.81', 'logps_train/chosen': '-129.46', 'loss/train': '0.74757', 'examples_per_second': '31.527', 'grad_norm': '97', 'counters/examples': 170496, 'counters/updates': 5328}
train stats after 170528 examples: {'rewards_train/chosen': '0.11349', 'rewards_train/rejected': '0.050154', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063333', 'logps_train/rejected': '-109.56', 'logps_train/chosen': '-150.04', 'loss/train': '0.67292', 'examples_per_second': '31.457', 'grad_norm': '89.5', 'counters/examples': 170528, 'counters/updates': 5329}
train stats after 170560 examples: {'rewards_train/chosen': '0.021076', 'rewards_train/rejected': '0.047225', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026149', 'logps_train/rejected': '-101.13', 'logps_train/chosen': '-155.9', 'loss/train': '0.71595', 'examples_per_second': '31.645', 'grad_norm': '54.75', 'counters/examples': 170560, 'counters/updates': 5330}
train stats after 170592 examples: {'rewards_train/chosen': '0.14316', 'rewards_train/rejected': '0.015594', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12757', 'logps_train/rejected': '-135.93', 'logps_train/chosen': '-149.31', 'loss/train': '0.64728', 'examples_per_second': '31.654', 'grad_norm': '103.5', 'counters/examples': 170592, 'counters/updates': 5331}
train stats after 170624 examples: {'rewards_train/chosen': '0.09676', 'rewards_train/rejected': '0.024927', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071833', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-128.79', 'loss/train': '0.666', 'examples_per_second': '31.727', 'grad_norm': '129', 'counters/examples': 170624, 'counters/updates': 5332}
train stats after 170656 examples: {'rewards_train/chosen': '0.071059', 'rewards_train/rejected': '0.1374', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066341', 'logps_train/rejected': '-159.08', 'logps_train/chosen': '-137.41', 'loss/train': '0.74568', 'examples_per_second': '32.233', 'grad_norm': '82', 'counters/examples': 170656, 'counters/updates': 5333}
train stats after 170688 examples: {'rewards_train/chosen': '0.036726', 'rewards_train/rejected': '0.054366', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01764', 'logps_train/rejected': '-106.85', 'logps_train/chosen': '-139.27', 'loss/train': '0.71263', 'examples_per_second': '32.61', 'grad_norm': '73', 'counters/examples': 170688, 'counters/updates': 5334}
train stats after 170720 examples: {'rewards_train/chosen': '0.041005', 'rewards_train/rejected': '0.029627', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011377', 'logps_train/rejected': '-157.12', 'logps_train/chosen': '-163.52', 'loss/train': '0.69739', 'examples_per_second': '31.601', 'grad_norm': '92', 'counters/examples': 170720, 'counters/updates': 5335}
train stats after 170752 examples: {'rewards_train/chosen': '0.092798', 'rewards_train/rejected': '0.15668', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06388', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-149.16', 'loss/train': '0.73395', 'examples_per_second': '31.652', 'grad_norm': '81', 'counters/examples': 170752, 'counters/updates': 5336}
train stats after 170784 examples: {'rewards_train/chosen': '0.063431', 'rewards_train/rejected': '0.044783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018649', 'logps_train/rejected': '-173.21', 'logps_train/chosen': '-149.81', 'loss/train': '0.69519', 'examples_per_second': '31.193', 'grad_norm': '96.5', 'counters/examples': 170784, 'counters/updates': 5337}
train stats after 170816 examples: {'rewards_train/chosen': '0.12813', 'rewards_train/rejected': '0.041653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086477', 'logps_train/rejected': '-138.96', 'logps_train/chosen': '-122.67', 'loss/train': '0.66902', 'examples_per_second': '31.349', 'grad_norm': '77', 'counters/examples': 170816, 'counters/updates': 5338}
skipping logging after 170848 examples to avoid logging too frequently
train stats after 170880 examples: {'rewards_train/chosen': '0.12208', 'rewards_train/rejected': '0.05042', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071659', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-127.61', 'loss/train': '0.6681', 'examples_per_second': '44.176', 'grad_norm': '73.5', 'counters/examples': 170880, 'counters/updates': 5340}
train stats after 170912 examples: {'rewards_train/chosen': '0.13036', 'rewards_train/rejected': '0.08493', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045432', 'logps_train/rejected': '-144', 'logps_train/chosen': '-144', 'loss/train': '0.68695', 'examples_per_second': '32.504', 'grad_norm': '87.5', 'counters/examples': 170912, 'counters/updates': 5341}
train stats after 170944 examples: {'rewards_train/chosen': '0.12019', 'rewards_train/rejected': '0.14131', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.021127', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-121.21', 'loss/train': '0.71523', 'examples_per_second': '32.467', 'grad_norm': '314', 'counters/examples': 170944, 'counters/updates': 5342}
train stats after 170976 examples: {'rewards_train/chosen': '-0.026434', 'rewards_train/rejected': '0.05485', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.081284', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-131.26', 'loss/train': '0.74583', 'examples_per_second': '31.258', 'grad_norm': '78.5', 'counters/examples': 170976, 'counters/updates': 5343}
train stats after 171008 examples: {'rewards_train/chosen': '0.20784', 'rewards_train/rejected': '0.062097', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14574', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-134.89', 'loss/train': '0.63404', 'examples_per_second': '31.826', 'grad_norm': '76.5', 'counters/examples': 171008, 'counters/updates': 5344}
train stats after 171040 examples: {'rewards_train/chosen': '0.20458', 'rewards_train/rejected': '0.09322', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11136', 'logps_train/rejected': '-202.19', 'logps_train/chosen': '-187.2', 'loss/train': '0.6496', 'examples_per_second': '31.613', 'grad_norm': '121', 'counters/examples': 171040, 'counters/updates': 5345}
train stats after 171072 examples: {'rewards_train/chosen': '0.20628', 'rewards_train/rejected': '0.074273', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.132', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-172.17', 'loss/train': '0.64116', 'examples_per_second': '31.741', 'grad_norm': '71', 'counters/examples': 171072, 'counters/updates': 5346}
skipping logging after 171104 examples to avoid logging too frequently
train stats after 171136 examples: {'rewards_train/chosen': '0.052836', 'rewards_train/rejected': '0.078362', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025527', 'logps_train/rejected': '-126.64', 'logps_train/chosen': '-154.31', 'loss/train': '0.71707', 'examples_per_second': '31.679', 'grad_norm': '356', 'counters/examples': 171136, 'counters/updates': 5348}
train stats after 171168 examples: {'rewards_train/chosen': '-0.00069629', 'rewards_train/rejected': '0.071901', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.072597', 'logps_train/rejected': '-115.64', 'logps_train/chosen': '-112.77', 'loss/train': '0.74138', 'examples_per_second': '33.106', 'grad_norm': '115.5', 'counters/examples': 171168, 'counters/updates': 5349}
train stats after 171200 examples: {'rewards_train/chosen': '0.13468', 'rewards_train/rejected': '0.07958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055095', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-165.62', 'loss/train': '0.68121', 'examples_per_second': '31.682', 'grad_norm': '82.5', 'counters/examples': 171200, 'counters/updates': 5350}
train stats after 171232 examples: {'rewards_train/chosen': '0.15615', 'rewards_train/rejected': '0.083348', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072805', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-166.12', 'loss/train': '0.66641', 'examples_per_second': '31.653', 'grad_norm': '201', 'counters/examples': 171232, 'counters/updates': 5351}
train stats after 171264 examples: {'rewards_train/chosen': '0.14877', 'rewards_train/rejected': '0.0391', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10967', 'logps_train/rejected': '-181.24', 'logps_train/chosen': '-173.03', 'loss/train': '0.66293', 'examples_per_second': '32.086', 'grad_norm': '146', 'counters/examples': 171264, 'counters/updates': 5352}
train stats after 171296 examples: {'rewards_train/chosen': '0.085716', 'rewards_train/rejected': '0.10042', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014701', 'logps_train/rejected': '-106.29', 'logps_train/chosen': '-125.92', 'loss/train': '0.71841', 'examples_per_second': '32.191', 'grad_norm': '78.5', 'counters/examples': 171296, 'counters/updates': 5353}
train stats after 171328 examples: {'rewards_train/chosen': '0.079131', 'rewards_train/rejected': '0.21661', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.13748', 'logps_train/rejected': '-146.83', 'logps_train/chosen': '-143.47', 'loss/train': '0.77858', 'examples_per_second': '30.425', 'grad_norm': '92', 'counters/examples': 171328, 'counters/updates': 5354}
train stats after 171360 examples: {'rewards_train/chosen': '0.13098', 'rewards_train/rejected': '0.036819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094164', 'logps_train/rejected': '-148.81', 'logps_train/chosen': '-169.03', 'loss/train': '0.67151', 'examples_per_second': '31.79', 'grad_norm': '110.5', 'counters/examples': 171360, 'counters/updates': 5355}
skipping logging after 171392 examples to avoid logging too frequently
train stats after 171424 examples: {'rewards_train/chosen': '0.095821', 'rewards_train/rejected': '0.074857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020963', 'logps_train/rejected': '-145.52', 'logps_train/chosen': '-177.6', 'loss/train': '0.6952', 'examples_per_second': '31.396', 'grad_norm': '76', 'counters/examples': 171424, 'counters/updates': 5357}
skipping logging after 171456 examples to avoid logging too frequently
train stats after 171488 examples: {'rewards_train/chosen': '0.090857', 'rewards_train/rejected': '0.036122', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054735', 'logps_train/rejected': '-202.52', 'logps_train/chosen': '-211.23', 'loss/train': '0.67974', 'examples_per_second': '24.525', 'grad_norm': '199', 'counters/examples': 171488, 'counters/updates': 5359}
train stats after 171520 examples: {'rewards_train/chosen': '0.14543', 'rewards_train/rejected': '0.068514', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076914', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-145.39', 'loss/train': '0.66773', 'examples_per_second': '30.114', 'grad_norm': '83', 'counters/examples': 171520, 'counters/updates': 5360}
train stats after 171552 examples: {'rewards_train/chosen': '0.15073', 'rewards_train/rejected': '0.058708', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092025', 'logps_train/rejected': '-159.5', 'logps_train/chosen': '-145.04', 'loss/train': '0.67076', 'examples_per_second': '30.642', 'grad_norm': '92', 'counters/examples': 171552, 'counters/updates': 5361}
train stats after 171584 examples: {'rewards_train/chosen': '0.087289', 'rewards_train/rejected': '0.059077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028211', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-178.23', 'loss/train': '0.70513', 'examples_per_second': '30.317', 'grad_norm': '126', 'counters/examples': 171584, 'counters/updates': 5362}
train stats after 171616 examples: {'rewards_train/chosen': '0.13159', 'rewards_train/rejected': '0.12438', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0072084', 'logps_train/rejected': '-171.84', 'logps_train/chosen': '-133.33', 'loss/train': '0.70445', 'examples_per_second': '30.7', 'grad_norm': '97', 'counters/examples': 171616, 'counters/updates': 5363}
train stats after 171648 examples: {'rewards_train/chosen': '0.10782', 'rewards_train/rejected': '-0.0037872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1116', 'logps_train/rejected': '-143.78', 'logps_train/chosen': '-179.83', 'loss/train': '0.65466', 'examples_per_second': '31.865', 'grad_norm': '59.75', 'counters/examples': 171648, 'counters/updates': 5364}
train stats after 171680 examples: {'rewards_train/chosen': '0.1675', 'rewards_train/rejected': '0.068723', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098773', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-157.54', 'loss/train': '0.67618', 'examples_per_second': '31.686', 'grad_norm': '104', 'counters/examples': 171680, 'counters/updates': 5365}
train stats after 171712 examples: {'rewards_train/chosen': '0.013217', 'rewards_train/rejected': '0.071189', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.057972', 'logps_train/rejected': '-111.42', 'logps_train/chosen': '-143.55', 'loss/train': '0.732', 'examples_per_second': '32.775', 'grad_norm': '61.5', 'counters/examples': 171712, 'counters/updates': 5366}
skipping logging after 171744 examples to avoid logging too frequently
train stats after 171776 examples: {'rewards_train/chosen': '0.18041', 'rewards_train/rejected': '0.065771', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11464', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-143.22', 'loss/train': '0.66625', 'examples_per_second': '31.638', 'grad_norm': '133', 'counters/examples': 171776, 'counters/updates': 5368}
train stats after 171808 examples: {'rewards_train/chosen': '0.095375', 'rewards_train/rejected': '0.10433', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0089539', 'logps_train/rejected': '-169.51', 'logps_train/chosen': '-168.29', 'loss/train': '0.71024', 'examples_per_second': '32.426', 'grad_norm': '115.5', 'counters/examples': 171808, 'counters/updates': 5369}
train stats after 171840 examples: {'rewards_train/chosen': '0.11456', 'rewards_train/rejected': '0.072153', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042402', 'logps_train/rejected': '-184.15', 'logps_train/chosen': '-202.67', 'loss/train': '0.68973', 'examples_per_second': '32.268', 'grad_norm': '246', 'counters/examples': 171840, 'counters/updates': 5370}
train stats after 171872 examples: {'rewards_train/chosen': '0.046448', 'rewards_train/rejected': '-0.014042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060489', 'logps_train/rejected': '-147.46', 'logps_train/chosen': '-181.5', 'loss/train': '0.68763', 'examples_per_second': '30.681', 'grad_norm': '68.5', 'counters/examples': 171872, 'counters/updates': 5371}
train stats after 171904 examples: {'rewards_train/chosen': '0.11466', 'rewards_train/rejected': '0.035328', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079337', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-162.64', 'loss/train': '0.67281', 'examples_per_second': '31.993', 'grad_norm': '92', 'counters/examples': 171904, 'counters/updates': 5372}
train stats after 171936 examples: {'rewards_train/chosen': '0.042465', 'rewards_train/rejected': '-0.031057', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073523', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-203.09', 'loss/train': '0.66351', 'examples_per_second': '31.652', 'grad_norm': '116', 'counters/examples': 171936, 'counters/updates': 5373}
train stats after 171968 examples: {'rewards_train/chosen': '0.11174', 'rewards_train/rejected': '0.042583', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069154', 'logps_train/rejected': '-100.57', 'logps_train/chosen': '-133.76', 'loss/train': '0.67391', 'examples_per_second': '31.669', 'grad_norm': '108.5', 'counters/examples': 171968, 'counters/updates': 5374}
train stats after 172000 examples: {'rewards_train/chosen': '0.079913', 'rewards_train/rejected': '0.0088936', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.071019', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-156.25', 'loss/train': '0.6854', 'examples_per_second': '31.555', 'grad_norm': '55.5', 'counters/examples': 172000, 'counters/updates': 5375}
train stats after 172032 examples: {'rewards_train/chosen': '0.13373', 'rewards_train/rejected': '0.061114', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072614', 'logps_train/rejected': '-131.24', 'logps_train/chosen': '-185.96', 'loss/train': '0.67016', 'examples_per_second': '32.588', 'grad_norm': '115', 'counters/examples': 172032, 'counters/updates': 5376}
train stats after 172064 examples: {'rewards_train/chosen': '0.13104', 'rewards_train/rejected': '0.16041', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029369', 'logps_train/rejected': '-163.71', 'logps_train/chosen': '-150.08', 'loss/train': '0.72867', 'examples_per_second': '31.627', 'grad_norm': '92.5', 'counters/examples': 172064, 'counters/updates': 5377}
train stats after 172096 examples: {'rewards_train/chosen': '0.081489', 'rewards_train/rejected': '0.02068', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06081', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-143.05', 'loss/train': '0.67902', 'examples_per_second': '31.644', 'grad_norm': '86.5', 'counters/examples': 172096, 'counters/updates': 5378}
train stats after 172128 examples: {'rewards_train/chosen': '0.21597', 'rewards_train/rejected': '0.14024', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075726', 'logps_train/rejected': '-153.17', 'logps_train/chosen': '-184.78', 'loss/train': '0.67537', 'examples_per_second': '32.494', 'grad_norm': '96', 'counters/examples': 172128, 'counters/updates': 5379}
train stats after 172160 examples: {'rewards_train/chosen': '0.10637', 'rewards_train/rejected': '-0.037078', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14344', 'logps_train/rejected': '-147.8', 'logps_train/chosen': '-147.07', 'loss/train': '0.63401', 'examples_per_second': '30.086', 'grad_norm': '109', 'counters/examples': 172160, 'counters/updates': 5380}
train stats after 172192 examples: {'rewards_train/chosen': '0.16433', 'rewards_train/rejected': '0.043725', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12061', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-160.29', 'loss/train': '0.6592', 'examples_per_second': '31.802', 'grad_norm': '67', 'counters/examples': 172192, 'counters/updates': 5381}
train stats after 172224 examples: {'rewards_train/chosen': '0.071377', 'rewards_train/rejected': '-0.017689', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089066', 'logps_train/rejected': '-142.21', 'logps_train/chosen': '-152.38', 'loss/train': '0.65799', 'examples_per_second': '32.994', 'grad_norm': '62', 'counters/examples': 172224, 'counters/updates': 5382}
train stats after 172256 examples: {'rewards_train/chosen': '0.094082', 'rewards_train/rejected': '-0.023785', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11787', 'logps_train/rejected': '-154.42', 'logps_train/chosen': '-125.31', 'loss/train': '0.64577', 'examples_per_second': '31.636', 'grad_norm': '95', 'counters/examples': 172256, 'counters/updates': 5383}
train stats after 172288 examples: {'rewards_train/chosen': '0.060105', 'rewards_train/rejected': '0.022677', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037428', 'logps_train/rejected': '-157.58', 'logps_train/chosen': '-153.11', 'loss/train': '0.69855', 'examples_per_second': '31.629', 'grad_norm': '101.5', 'counters/examples': 172288, 'counters/updates': 5384}
train stats after 172320 examples: {'rewards_train/chosen': '0.10801', 'rewards_train/rejected': '0.061308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046701', 'logps_train/rejected': '-146.76', 'logps_train/chosen': '-175.48', 'loss/train': '0.68941', 'examples_per_second': '32.403', 'grad_norm': '83', 'counters/examples': 172320, 'counters/updates': 5385}
skipping logging after 172352 examples to avoid logging too frequently
train stats after 172384 examples: {'rewards_train/chosen': '-0.0059629', 'rewards_train/rejected': '0.00071517', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0066781', 'logps_train/rejected': '-137.1', 'logps_train/chosen': '-124.62', 'loss/train': '0.7046', 'examples_per_second': '36.792', 'grad_norm': '79', 'counters/examples': 172384, 'counters/updates': 5387}
train stats after 172416 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.076772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038047', 'logps_train/rejected': '-151.82', 'logps_train/chosen': '-156.7', 'loss/train': '0.69222', 'examples_per_second': '31.121', 'grad_norm': '70', 'counters/examples': 172416, 'counters/updates': 5388}
skipping logging after 172448 examples to avoid logging too frequently
train stats after 172480 examples: {'rewards_train/chosen': '0.18485', 'rewards_train/rejected': '0.099497', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085353', 'logps_train/rejected': '-137.72', 'logps_train/chosen': '-159.61', 'loss/train': '0.68956', 'examples_per_second': '31.159', 'grad_norm': '76.5', 'counters/examples': 172480, 'counters/updates': 5390}
train stats after 172512 examples: {'rewards_train/chosen': '0.15529', 'rewards_train/rejected': '0.06479', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090497', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-199.62', 'loss/train': '0.66593', 'examples_per_second': '30.671', 'grad_norm': '80', 'counters/examples': 172512, 'counters/updates': 5391}
train stats after 172544 examples: {'rewards_train/chosen': '0.10932', 'rewards_train/rejected': '0.071212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038108', 'logps_train/rejected': '-107.51', 'logps_train/chosen': '-136.21', 'loss/train': '0.67989', 'examples_per_second': '31.36', 'grad_norm': '84.5', 'counters/examples': 172544, 'counters/updates': 5392}
train stats after 172576 examples: {'rewards_train/chosen': '0.034849', 'rewards_train/rejected': '-0.034762', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069611', 'logps_train/rejected': '-180.55', 'logps_train/chosen': '-163', 'loss/train': '0.66866', 'examples_per_second': '30.119', 'grad_norm': '79.5', 'counters/examples': 172576, 'counters/updates': 5393}
train stats after 172608 examples: {'rewards_train/chosen': '0.054676', 'rewards_train/rejected': '0.07787', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023194', 'logps_train/rejected': '-99.419', 'logps_train/chosen': '-145.08', 'loss/train': '0.71522', 'examples_per_second': '30.135', 'grad_norm': '61', 'counters/examples': 172608, 'counters/updates': 5394}
skipping logging after 172640 examples to avoid logging too frequently
train stats after 172672 examples: {'rewards_train/chosen': '0.087656', 'rewards_train/rejected': '0.065271', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022385', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-155.58', 'loss/train': '0.68971', 'examples_per_second': '32.812', 'grad_norm': '113.5', 'counters/examples': 172672, 'counters/updates': 5396}
train stats after 172704 examples: {'rewards_train/chosen': '0.13567', 'rewards_train/rejected': '0.095732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039942', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-178.68', 'loss/train': '0.68899', 'examples_per_second': '31.622', 'grad_norm': '83.5', 'counters/examples': 172704, 'counters/updates': 5397}
train stats after 172736 examples: {'rewards_train/chosen': '0.019924', 'rewards_train/rejected': '0.071467', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.051543', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-162.18', 'loss/train': '0.73743', 'examples_per_second': '31.636', 'grad_norm': '112.5', 'counters/examples': 172736, 'counters/updates': 5398}
train stats after 172768 examples: {'rewards_train/chosen': '0.098895', 'rewards_train/rejected': '-0.012869', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.11176', 'logps_train/rejected': '-109.58', 'logps_train/chosen': '-139.91', 'loss/train': '0.64512', 'examples_per_second': '31.627', 'grad_norm': '90.5', 'counters/examples': 172768, 'counters/updates': 5399}
train stats after 172800 examples: {'rewards_train/chosen': '0.092641', 'rewards_train/rejected': '0.01642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076221', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-140.46', 'loss/train': '0.67155', 'examples_per_second': '30.653', 'grad_norm': '87.5', 'counters/examples': 172800, 'counters/updates': 5400}
train stats after 172832 examples: {'rewards_train/chosen': '0.18853', 'rewards_train/rejected': '0.083832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1047', 'logps_train/rejected': '-169.68', 'logps_train/chosen': '-163.68', 'loss/train': '0.6569', 'examples_per_second': '30.544', 'grad_norm': '132', 'counters/examples': 172832, 'counters/updates': 5401}
train stats after 172864 examples: {'rewards_train/chosen': '0.056372', 'rewards_train/rejected': '0.14796', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.091587', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-131.33', 'loss/train': '0.74817', 'examples_per_second': '32.466', 'grad_norm': '129', 'counters/examples': 172864, 'counters/updates': 5402}
train stats after 172896 examples: {'rewards_train/chosen': '0.14154', 'rewards_train/rejected': '0.17191', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.030369', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-161.14', 'loss/train': '0.72842', 'examples_per_second': '31.632', 'grad_norm': '211', 'counters/examples': 172896, 'counters/updates': 5403}
train stats after 172928 examples: {'rewards_train/chosen': '0.089895', 'rewards_train/rejected': '0.087573', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0023223', 'logps_train/rejected': '-147.25', 'logps_train/chosen': '-137.38', 'loss/train': '0.70367', 'examples_per_second': '31.394', 'grad_norm': '114.5', 'counters/examples': 172928, 'counters/updates': 5404}
train stats after 172960 examples: {'rewards_train/chosen': '0.096987', 'rewards_train/rejected': '0.014699', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082288', 'logps_train/rejected': '-130.35', 'logps_train/chosen': '-147.73', 'loss/train': '0.66493', 'examples_per_second': '30.109', 'grad_norm': '52.5', 'counters/examples': 172960, 'counters/updates': 5405}
skipping logging after 172992 examples to avoid logging too frequently
train stats after 173024 examples: {'rewards_train/chosen': '0.080822', 'rewards_train/rejected': '0.0015007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079322', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-194.35', 'loss/train': '0.67135', 'examples_per_second': '31.641', 'grad_norm': '73', 'counters/examples': 173024, 'counters/updates': 5407}
train stats after 173056 examples: {'rewards_train/chosen': '0.052485', 'rewards_train/rejected': '0.052842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00035744', 'logps_train/rejected': '-162.6', 'logps_train/chosen': '-151.63', 'loss/train': '0.70637', 'examples_per_second': '30.044', 'grad_norm': '77', 'counters/examples': 173056, 'counters/updates': 5408}
skipping logging after 173088 examples to avoid logging too frequently
train stats after 173120 examples: {'rewards_train/chosen': '0.088389', 'rewards_train/rejected': '0.07866', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0097293', 'logps_train/rejected': '-151.67', 'logps_train/chosen': '-148.01', 'loss/train': '0.70906', 'examples_per_second': '33.465', 'grad_norm': '91', 'counters/examples': 173120, 'counters/updates': 5410}
train stats after 173152 examples: {'rewards_train/chosen': '0.068896', 'rewards_train/rejected': '0.070433', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0015371', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-135.53', 'loss/train': '0.70466', 'examples_per_second': '32.041', 'grad_norm': '60.5', 'counters/examples': 173152, 'counters/updates': 5411}
train stats after 173184 examples: {'rewards_train/chosen': '0.12033', 'rewards_train/rejected': '0.082247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038081', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-146.56', 'loss/train': '0.6916', 'examples_per_second': '31.09', 'grad_norm': '91', 'counters/examples': 173184, 'counters/updates': 5412}
train stats after 173216 examples: {'rewards_train/chosen': '0.10709', 'rewards_train/rejected': '0.038836', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068254', 'logps_train/rejected': '-104.33', 'logps_train/chosen': '-124.49', 'loss/train': '0.66942', 'examples_per_second': '30.087', 'grad_norm': '112', 'counters/examples': 173216, 'counters/updates': 5413}
train stats after 173248 examples: {'rewards_train/chosen': '0.18735', 'rewards_train/rejected': '0.021628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16573', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-145.66', 'loss/train': '0.63876', 'examples_per_second': '32.281', 'grad_norm': '72.5', 'counters/examples': 173248, 'counters/updates': 5414}
train stats after 173280 examples: {'rewards_train/chosen': '0.15499', 'rewards_train/rejected': '0.038626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11636', 'logps_train/rejected': '-142.26', 'logps_train/chosen': '-174.99', 'loss/train': '0.65447', 'examples_per_second': '30.16', 'grad_norm': '226', 'counters/examples': 173280, 'counters/updates': 5415}
train stats after 173312 examples: {'rewards_train/chosen': '0.04662', 'rewards_train/rejected': '0.068064', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021444', 'logps_train/rejected': '-167.84', 'logps_train/chosen': '-189.69', 'loss/train': '0.73376', 'examples_per_second': '31.047', 'grad_norm': '143', 'counters/examples': 173312, 'counters/updates': 5416}
skipping logging after 173344 examples to avoid logging too frequently
train stats after 173376 examples: {'rewards_train/chosen': '0.12172', 'rewards_train/rejected': '0.073061', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04866', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-197.76', 'loss/train': '0.68196', 'examples_per_second': '31.59', 'grad_norm': '73.5', 'counters/examples': 173376, 'counters/updates': 5418}
train stats after 173408 examples: {'rewards_train/chosen': '0.14348', 'rewards_train/rejected': '0.049516', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093961', 'logps_train/rejected': '-147.23', 'logps_train/chosen': '-170.79', 'loss/train': '0.65787', 'examples_per_second': '32.451', 'grad_norm': '69', 'counters/examples': 173408, 'counters/updates': 5419}
skipping logging after 173440 examples to avoid logging too frequently
train stats after 173472 examples: {'rewards_train/chosen': '0.20297', 'rewards_train/rejected': '0.086271', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11669', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-171.39', 'loss/train': '0.66', 'examples_per_second': '31.621', 'grad_norm': '79.5', 'counters/examples': 173472, 'counters/updates': 5421}
train stats after 173504 examples: {'rewards_train/chosen': '0.10028', 'rewards_train/rejected': '0.068306', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031971', 'logps_train/rejected': '-148.51', 'logps_train/chosen': '-174.63', 'loss/train': '0.69035', 'examples_per_second': '32.103', 'grad_norm': '66', 'counters/examples': 173504, 'counters/updates': 5422}
skipping logging after 173536 examples to avoid logging too frequently
train stats after 173568 examples: {'rewards_train/chosen': '0.027278', 'rewards_train/rejected': '0.085883', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.058605', 'logps_train/rejected': '-146.53', 'logps_train/chosen': '-148.03', 'loss/train': '0.73515', 'examples_per_second': '30.979', 'grad_norm': '69', 'counters/examples': 173568, 'counters/updates': 5424}
train stats after 173600 examples: {'rewards_train/chosen': '0.014527', 'rewards_train/rejected': '0.043312', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028784', 'logps_train/rejected': '-150.38', 'logps_train/chosen': '-164.7', 'loss/train': '0.73348', 'examples_per_second': '32.648', 'grad_norm': '94', 'counters/examples': 173600, 'counters/updates': 5425}
skipping logging after 173632 examples to avoid logging too frequently
train stats after 173664 examples: {'rewards_train/chosen': '0.26871', 'rewards_train/rejected': '0.042701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22601', 'logps_train/rejected': '-169.14', 'logps_train/chosen': '-183.61', 'loss/train': '0.61441', 'examples_per_second': '29.829', 'grad_norm': '63.75', 'counters/examples': 173664, 'counters/updates': 5427}
train stats after 173696 examples: {'rewards_train/chosen': '-0.0045037', 'rewards_train/rejected': '0.027073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031577', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-146.84', 'loss/train': '0.72052', 'examples_per_second': '30.692', 'grad_norm': '109', 'counters/examples': 173696, 'counters/updates': 5428}
train stats after 173728 examples: {'rewards_train/chosen': '0.13915', 'rewards_train/rejected': '0.21462', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.075463', 'logps_train/rejected': '-159.17', 'logps_train/chosen': '-140.33', 'loss/train': '0.76304', 'examples_per_second': '31.233', 'grad_norm': '197', 'counters/examples': 173728, 'counters/updates': 5429}
train stats after 173760 examples: {'rewards_train/chosen': '0.14826', 'rewards_train/rejected': '0.12225', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02601', 'logps_train/rejected': '-152.17', 'logps_train/chosen': '-135.01', 'loss/train': '0.69652', 'examples_per_second': '31.274', 'grad_norm': '119.5', 'counters/examples': 173760, 'counters/updates': 5430}
train stats after 173792 examples: {'rewards_train/chosen': '0.084044', 'rewards_train/rejected': '0.099805', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015761', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-127.43', 'loss/train': '0.71078', 'examples_per_second': '31.544', 'grad_norm': '132', 'counters/examples': 173792, 'counters/updates': 5431}
train stats after 173824 examples: {'rewards_train/chosen': '0.23106', 'rewards_train/rejected': '0.0093004', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22176', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-162.26', 'loss/train': '0.61581', 'examples_per_second': '25.07', 'grad_norm': '56.75', 'counters/examples': 173824, 'counters/updates': 5432}
train stats after 173856 examples: {'rewards_train/chosen': '0.20977', 'rewards_train/rejected': '0.06338', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14639', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-147.12', 'loss/train': '0.63072', 'examples_per_second': '31.46', 'grad_norm': '163', 'counters/examples': 173856, 'counters/updates': 5433}
train stats after 173888 examples: {'rewards_train/chosen': '0.11232', 'rewards_train/rejected': '0.029287', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083029', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-163.75', 'loss/train': '0.66927', 'examples_per_second': '31.325', 'grad_norm': '225', 'counters/examples': 173888, 'counters/updates': 5434}
train stats after 173920 examples: {'rewards_train/chosen': '0.17982', 'rewards_train/rejected': '0.091499', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088323', 'logps_train/rejected': '-164.04', 'logps_train/chosen': '-204.35', 'loss/train': '0.66253', 'examples_per_second': '24.848', 'grad_norm': '100', 'counters/examples': 173920, 'counters/updates': 5435}
train stats after 173952 examples: {'rewards_train/chosen': '0.034114', 'rewards_train/rejected': '0.0936', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.059486', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-159.27', 'loss/train': '0.74469', 'examples_per_second': '32.167', 'grad_norm': '132', 'counters/examples': 173952, 'counters/updates': 5436}
train stats after 173984 examples: {'rewards_train/chosen': '0.17509', 'rewards_train/rejected': '0.15758', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017509', 'logps_train/rejected': '-129.87', 'logps_train/chosen': '-155.27', 'loss/train': '0.6921', 'examples_per_second': '30.821', 'grad_norm': '90', 'counters/examples': 173984, 'counters/updates': 5437}
train stats after 174016 examples: {'rewards_train/chosen': '0.10372', 'rewards_train/rejected': '0.022684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081036', 'logps_train/rejected': '-170.65', 'logps_train/chosen': '-191.02', 'loss/train': '0.66962', 'examples_per_second': '32.571', 'grad_norm': '81.5', 'counters/examples': 174016, 'counters/updates': 5438}
train stats after 174048 examples: {'rewards_train/chosen': '0.1657', 'rewards_train/rejected': '0.031911', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13379', 'logps_train/rejected': '-129.31', 'logps_train/chosen': '-135.91', 'loss/train': '0.63943', 'examples_per_second': '31.633', 'grad_norm': '56.75', 'counters/examples': 174048, 'counters/updates': 5439}
train stats after 174080 examples: {'rewards_train/chosen': '0.08352', 'rewards_train/rejected': '-0.047592', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13111', 'logps_train/rejected': '-150.76', 'logps_train/chosen': '-170.09', 'loss/train': '0.63694', 'examples_per_second': '31.564', 'grad_norm': '111', 'counters/examples': 174080, 'counters/updates': 5440}
train stats after 174112 examples: {'rewards_train/chosen': '0.097104', 'rewards_train/rejected': '-0.061591', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15869', 'logps_train/rejected': '-153.54', 'logps_train/chosen': '-124.29', 'loss/train': '0.642', 'examples_per_second': '31.622', 'grad_norm': '111.5', 'counters/examples': 174112, 'counters/updates': 5441}
train stats after 174144 examples: {'rewards_train/chosen': '0.14431', 'rewards_train/rejected': '0.09994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044371', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-138.99', 'loss/train': '0.68273', 'examples_per_second': '30.191', 'grad_norm': '81.5', 'counters/examples': 174144, 'counters/updates': 5442}
train stats after 174176 examples: {'rewards_train/chosen': '0.11446', 'rewards_train/rejected': '-0.050079', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16454', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-144.49', 'loss/train': '0.644', 'examples_per_second': '32.13', 'grad_norm': '83', 'counters/examples': 174176, 'counters/updates': 5443}
skipping logging after 174208 examples to avoid logging too frequently
train stats after 174240 examples: {'rewards_train/chosen': '0.14544', 'rewards_train/rejected': '0.040129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10531', 'logps_train/rejected': '-110.77', 'logps_train/chosen': '-162.86', 'loss/train': '0.65245', 'examples_per_second': '31.581', 'grad_norm': '87.5', 'counters/examples': 174240, 'counters/updates': 5445}
train stats after 174272 examples: {'rewards_train/chosen': '0.14103', 'rewards_train/rejected': '0.016925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12411', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-142.19', 'loss/train': '0.64982', 'examples_per_second': '31.572', 'grad_norm': '62.75', 'counters/examples': 174272, 'counters/updates': 5446}
train stats after 174304 examples: {'rewards_train/chosen': '0.16189', 'rewards_train/rejected': '0.038114', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12378', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-205.38', 'loss/train': '0.65471', 'examples_per_second': '31.006', 'grad_norm': '116.5', 'counters/examples': 174304, 'counters/updates': 5447}
train stats after 174336 examples: {'rewards_train/chosen': '0.08541', 'rewards_train/rejected': '0.036612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048798', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-149.79', 'loss/train': '0.69717', 'examples_per_second': '32.458', 'grad_norm': '107.5', 'counters/examples': 174336, 'counters/updates': 5448}
train stats after 174368 examples: {'rewards_train/chosen': '0.15856', 'rewards_train/rejected': '0.16036', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0018002', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-182.79', 'loss/train': '0.70245', 'examples_per_second': '31.609', 'grad_norm': '69', 'counters/examples': 174368, 'counters/updates': 5449}
train stats after 174400 examples: {'rewards_train/chosen': '0.09657', 'rewards_train/rejected': '0.0067052', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089865', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-138.52', 'loss/train': '0.66255', 'examples_per_second': '31.838', 'grad_norm': '64', 'counters/examples': 174400, 'counters/updates': 5450}
train stats after 174432 examples: {'rewards_train/chosen': '0.14379', 'rewards_train/rejected': '0.033934', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10986', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-161.15', 'loss/train': '0.66098', 'examples_per_second': '30.974', 'grad_norm': '92.5', 'counters/examples': 174432, 'counters/updates': 5451}
train stats after 174464 examples: {'rewards_train/chosen': '0.09813', 'rewards_train/rejected': '0.1358', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037675', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-140.67', 'loss/train': '0.7231', 'examples_per_second': '31.49', 'grad_norm': '128', 'counters/examples': 174464, 'counters/updates': 5452}
train stats after 174496 examples: {'rewards_train/chosen': '-0.049011', 'rewards_train/rejected': '0.073342', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.12235', 'logps_train/rejected': '-97.375', 'logps_train/chosen': '-137.87', 'loss/train': '0.778', 'examples_per_second': '32.521', 'grad_norm': '187', 'counters/examples': 174496, 'counters/updates': 5453}
skipping logging after 174528 examples to avoid logging too frequently
train stats after 174560 examples: {'rewards_train/chosen': '0.097638', 'rewards_train/rejected': '0.055106', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042532', 'logps_train/rejected': '-132.69', 'logps_train/chosen': '-147.03', 'loss/train': '0.68988', 'examples_per_second': '33.969', 'grad_norm': '62.25', 'counters/examples': 174560, 'counters/updates': 5455}
train stats after 174592 examples: {'rewards_train/chosen': '0.15542', 'rewards_train/rejected': '0.048524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10689', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-144.7', 'loss/train': '0.65594', 'examples_per_second': '31.607', 'grad_norm': '67.5', 'counters/examples': 174592, 'counters/updates': 5456}
train stats after 174624 examples: {'rewards_train/chosen': '0.035521', 'rewards_train/rejected': '-0.0034512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038972', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-120.8', 'loss/train': '0.68209', 'examples_per_second': '31.044', 'grad_norm': '69.5', 'counters/examples': 174624, 'counters/updates': 5457}
train stats after 174656 examples: {'rewards_train/chosen': '0.092727', 'rewards_train/rejected': '-0.0029527', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09568', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-190.32', 'loss/train': '0.66039', 'examples_per_second': '30.224', 'grad_norm': '78', 'counters/examples': 174656, 'counters/updates': 5458}
train stats after 174688 examples: {'rewards_train/chosen': '0.13627', 'rewards_train/rejected': '-0.0043096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14058', 'logps_train/rejected': '-93.602', 'logps_train/chosen': '-141.32', 'loss/train': '0.6361', 'examples_per_second': '31.554', 'grad_norm': '55', 'counters/examples': 174688, 'counters/updates': 5459}
train stats after 174720 examples: {'rewards_train/chosen': '0.084569', 'rewards_train/rejected': '0.09094', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0063711', 'logps_train/rejected': '-147.54', 'logps_train/chosen': '-124.92', 'loss/train': '0.70664', 'examples_per_second': '31.534', 'grad_norm': '64', 'counters/examples': 174720, 'counters/updates': 5460}
train stats after 174752 examples: {'rewards_train/chosen': '0.020829', 'rewards_train/rejected': '0.055608', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034779', 'logps_train/rejected': '-132.22', 'logps_train/chosen': '-120.32', 'loss/train': '0.72267', 'examples_per_second': '32.539', 'grad_norm': '81', 'counters/examples': 174752, 'counters/updates': 5461}
train stats after 174784 examples: {'rewards_train/chosen': '0.084526', 'rewards_train/rejected': '0.099359', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014833', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-157.92', 'loss/train': '0.70633', 'examples_per_second': '31.041', 'grad_norm': '60.25', 'counters/examples': 174784, 'counters/updates': 5462}
train stats after 174816 examples: {'rewards_train/chosen': '0.071869', 'rewards_train/rejected': '0.12984', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.057973', 'logps_train/rejected': '-135.86', 'logps_train/chosen': '-152.93', 'loss/train': '0.7426', 'examples_per_second': '31.347', 'grad_norm': '174', 'counters/examples': 174816, 'counters/updates': 5463}
train stats after 174848 examples: {'rewards_train/chosen': '0.044212', 'rewards_train/rejected': '0.05935', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015137', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-128.92', 'loss/train': '0.7185', 'examples_per_second': '31.275', 'grad_norm': '115.5', 'counters/examples': 174848, 'counters/updates': 5464}
train stats after 174880 examples: {'rewards_train/chosen': '0.18306', 'rewards_train/rejected': '0.086851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09621', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-164.77', 'loss/train': '0.66131', 'examples_per_second': '31.321', 'grad_norm': '57', 'counters/examples': 174880, 'counters/updates': 5465}
train stats after 174912 examples: {'rewards_train/chosen': '0.073143', 'rewards_train/rejected': '0.061547', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011597', 'logps_train/rejected': '-111.99', 'logps_train/chosen': '-116.5', 'loss/train': '0.70387', 'examples_per_second': '30.375', 'grad_norm': '89', 'counters/examples': 174912, 'counters/updates': 5466}
train stats after 174944 examples: {'rewards_train/chosen': '0.087569', 'rewards_train/rejected': '0.078404', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0091647', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-171.85', 'loss/train': '0.69592', 'examples_per_second': '30.5', 'grad_norm': '121', 'counters/examples': 174944, 'counters/updates': 5467}
train stats after 174976 examples: {'rewards_train/chosen': '0.052798', 'rewards_train/rejected': '0.065596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.012797', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-160.23', 'loss/train': '0.71817', 'examples_per_second': '31.784', 'grad_norm': '104', 'counters/examples': 174976, 'counters/updates': 5468}
train stats after 175008 examples: {'rewards_train/chosen': '0.10663', 'rewards_train/rejected': '0.071044', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035583', 'logps_train/rejected': '-165.15', 'logps_train/chosen': '-177.02', 'loss/train': '0.68973', 'examples_per_second': '30.647', 'grad_norm': '154', 'counters/examples': 175008, 'counters/updates': 5469}
train stats after 175040 examples: {'rewards_train/chosen': '0.1796', 'rewards_train/rejected': '0.17962', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-1.8397e-05', 'logps_train/rejected': '-131.01', 'logps_train/chosen': '-158.76', 'loss/train': '0.72927', 'examples_per_second': '32.513', 'grad_norm': '89', 'counters/examples': 175040, 'counters/updates': 5470}
train stats after 175072 examples: {'rewards_train/chosen': '0.18022', 'rewards_train/rejected': '0.12384', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056383', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-139.18', 'loss/train': '0.67858', 'examples_per_second': '30.502', 'grad_norm': '126.5', 'counters/examples': 175072, 'counters/updates': 5471}
train stats after 175104 examples: {'rewards_train/chosen': '0.096949', 'rewards_train/rejected': '0.043639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05331', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-172.92', 'loss/train': '0.68199', 'examples_per_second': '30.804', 'grad_norm': '130', 'counters/examples': 175104, 'counters/updates': 5472}
train stats after 175136 examples: {'rewards_train/chosen': '0.041918', 'rewards_train/rejected': '0.067603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025685', 'logps_train/rejected': '-161.83', 'logps_train/chosen': '-164.49', 'loss/train': '0.72245', 'examples_per_second': '31.469', 'grad_norm': '67.5', 'counters/examples': 175136, 'counters/updates': 5473}
train stats after 175168 examples: {'rewards_train/chosen': '0.10144', 'rewards_train/rejected': '-0.048204', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14964', 'logps_train/rejected': '-133.64', 'logps_train/chosen': '-154.23', 'loss/train': '0.63465', 'examples_per_second': '31.294', 'grad_norm': '116', 'counters/examples': 175168, 'counters/updates': 5474}
train stats after 175200 examples: {'rewards_train/chosen': '0.088393', 'rewards_train/rejected': '0.025683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06271', 'logps_train/rejected': '-159.67', 'logps_train/chosen': '-147.99', 'loss/train': '0.67772', 'examples_per_second': '31.596', 'grad_norm': '81.5', 'counters/examples': 175200, 'counters/updates': 5475}
train stats after 175232 examples: {'rewards_train/chosen': '0.050368', 'rewards_train/rejected': '0.075004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024636', 'logps_train/rejected': '-147.46', 'logps_train/chosen': '-150.25', 'loss/train': '0.71518', 'examples_per_second': '31.745', 'grad_norm': '102.5', 'counters/examples': 175232, 'counters/updates': 5476}
skipping logging after 175264 examples to avoid logging too frequently
train stats after 175296 examples: {'rewards_train/chosen': '0.082355', 'rewards_train/rejected': '0.081165', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0011905', 'logps_train/rejected': '-110.45', 'logps_train/chosen': '-130.57', 'loss/train': '0.7076', 'examples_per_second': '31.556', 'grad_norm': '104.5', 'counters/examples': 175296, 'counters/updates': 5478}
train stats after 175328 examples: {'rewards_train/chosen': '0.12809', 'rewards_train/rejected': '0.03073', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097358', 'logps_train/rejected': '-173.43', 'logps_train/chosen': '-151.07', 'loss/train': '0.65796', 'examples_per_second': '31.383', 'grad_norm': '83.5', 'counters/examples': 175328, 'counters/updates': 5479}
train stats after 175360 examples: {'rewards_train/chosen': '0.094307', 'rewards_train/rejected': '-0.0060841', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10039', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-189.84', 'loss/train': '0.65889', 'examples_per_second': '30.733', 'grad_norm': '122.5', 'counters/examples': 175360, 'counters/updates': 5480}
train stats after 175392 examples: {'rewards_train/chosen': '0.092511', 'rewards_train/rejected': '0.094618', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0021071', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-143.64', 'loss/train': '0.7051', 'examples_per_second': '30.758', 'grad_norm': '86', 'counters/examples': 175392, 'counters/updates': 5481}
train stats after 175424 examples: {'rewards_train/chosen': '0.1178', 'rewards_train/rejected': '0.12338', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0055844', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-134.12', 'loss/train': '0.71088', 'examples_per_second': '31.505', 'grad_norm': '65', 'counters/examples': 175424, 'counters/updates': 5482}
train stats after 175456 examples: {'rewards_train/chosen': '0.21307', 'rewards_train/rejected': '0.040168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17291', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-151.87', 'loss/train': '0.64465', 'examples_per_second': '31.544', 'grad_norm': '45.5', 'counters/examples': 175456, 'counters/updates': 5483}
train stats after 175488 examples: {'rewards_train/chosen': '0.17595', 'rewards_train/rejected': '0.1553', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020655', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-136.35', 'loss/train': '0.70043', 'examples_per_second': '30.009', 'grad_norm': '93', 'counters/examples': 175488, 'counters/updates': 5484}
train stats after 175520 examples: {'rewards_train/chosen': '0.063836', 'rewards_train/rejected': '0.072749', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0089128', 'logps_train/rejected': '-123.04', 'logps_train/chosen': '-155.22', 'loss/train': '0.71053', 'examples_per_second': '30.825', 'grad_norm': '86', 'counters/examples': 175520, 'counters/updates': 5485}
train stats after 175552 examples: {'rewards_train/chosen': '0.079282', 'rewards_train/rejected': '0.091988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012707', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-124.17', 'loss/train': '0.71602', 'examples_per_second': '31.031', 'grad_norm': '219', 'counters/examples': 175552, 'counters/updates': 5486}
train stats after 175584 examples: {'rewards_train/chosen': '0.043627', 'rewards_train/rejected': '0.055147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.01152', 'logps_train/rejected': '-134.57', 'logps_train/chosen': '-149.95', 'loss/train': '0.70802', 'examples_per_second': '30.068', 'grad_norm': '86', 'counters/examples': 175584, 'counters/updates': 5487}
skipping logging after 175616 examples to avoid logging too frequently
train stats after 175648 examples: {'rewards_train/chosen': '0.032334', 'rewards_train/rejected': '0.068617', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036283', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-137.28', 'loss/train': '0.72509', 'examples_per_second': '30.584', 'grad_norm': '177', 'counters/examples': 175648, 'counters/updates': 5489}
train stats after 175680 examples: {'rewards_train/chosen': '0.10411', 'rewards_train/rejected': '0.11712', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013015', 'logps_train/rejected': '-169.29', 'logps_train/chosen': '-171.76', 'loss/train': '0.71429', 'examples_per_second': '31.542', 'grad_norm': '97', 'counters/examples': 175680, 'counters/updates': 5490}
train stats after 175712 examples: {'rewards_train/chosen': '0.18346', 'rewards_train/rejected': '0.068293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11517', 'logps_train/rejected': '-150.13', 'logps_train/chosen': '-169.18', 'loss/train': '0.65164', 'examples_per_second': '32.617', 'grad_norm': '112', 'counters/examples': 175712, 'counters/updates': 5491}
train stats after 175744 examples: {'rewards_train/chosen': '0.086387', 'rewards_train/rejected': '0.047962', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038425', 'logps_train/rejected': '-99.889', 'logps_train/chosen': '-172.63', 'loss/train': '0.68316', 'examples_per_second': '30.285', 'grad_norm': '128', 'counters/examples': 175744, 'counters/updates': 5492}
train stats after 175776 examples: {'rewards_train/chosen': '0.096271', 'rewards_train/rejected': '0.023341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07293', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-163.23', 'loss/train': '0.67007', 'examples_per_second': '31.572', 'grad_norm': '66', 'counters/examples': 175776, 'counters/updates': 5493}
train stats after 175808 examples: {'rewards_train/chosen': '0.068729', 'rewards_train/rejected': '0.082248', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013519', 'logps_train/rejected': '-167.67', 'logps_train/chosen': '-160.41', 'loss/train': '0.70989', 'examples_per_second': '31.381', 'grad_norm': '98.5', 'counters/examples': 175808, 'counters/updates': 5494}
train stats after 175840 examples: {'rewards_train/chosen': '0.14112', 'rewards_train/rejected': '0.12431', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016816', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-133.07', 'loss/train': '0.69876', 'examples_per_second': '30.821', 'grad_norm': '85', 'counters/examples': 175840, 'counters/updates': 5495}
train stats after 175872 examples: {'rewards_train/chosen': '0.080305', 'rewards_train/rejected': '0.029845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05046', 'logps_train/rejected': '-160.03', 'logps_train/chosen': '-132.83', 'loss/train': '0.68971', 'examples_per_second': '31.981', 'grad_norm': '69', 'counters/examples': 175872, 'counters/updates': 5496}
train stats after 175904 examples: {'rewards_train/chosen': '0.10201', 'rewards_train/rejected': '0.082964', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019048', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-135.11', 'loss/train': '0.69508', 'examples_per_second': '30.569', 'grad_norm': '89', 'counters/examples': 175904, 'counters/updates': 5497}
train stats after 175936 examples: {'rewards_train/chosen': '0.042467', 'rewards_train/rejected': '0.013836', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028631', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-160.14', 'loss/train': '0.69592', 'examples_per_second': '31.526', 'grad_norm': '68', 'counters/examples': 175936, 'counters/updates': 5498}
train stats after 175968 examples: {'rewards_train/chosen': '0.21718', 'rewards_train/rejected': '0.065522', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15166', 'logps_train/rejected': '-134.57', 'logps_train/chosen': '-156.32', 'loss/train': '0.63318', 'examples_per_second': '31.562', 'grad_norm': '80.5', 'counters/examples': 175968, 'counters/updates': 5499}
train stats after 176000 examples: {'rewards_train/chosen': '0.12568', 'rewards_train/rejected': '0.11695', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087371', 'logps_train/rejected': '-164.03', 'logps_train/chosen': '-134.72', 'loss/train': '0.70467', 'examples_per_second': '31.854', 'grad_norm': '164', 'counters/examples': 176000, 'counters/updates': 5500}
skipping logging after 176032 examples to avoid logging too frequently
train stats after 176064 examples: {'rewards_train/chosen': '0.14256', 'rewards_train/rejected': '0.028414', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11414', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-160.79', 'loss/train': '0.66849', 'examples_per_second': '34.398', 'grad_norm': '62.25', 'counters/examples': 176064, 'counters/updates': 5502}
train stats after 176096 examples: {'rewards_train/chosen': '0.16189', 'rewards_train/rejected': '0.045242', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11664', 'logps_train/rejected': '-151.66', 'logps_train/chosen': '-176.57', 'loss/train': '0.64599', 'examples_per_second': '31.516', 'grad_norm': '61', 'counters/examples': 176096, 'counters/updates': 5503}
train stats after 176128 examples: {'rewards_train/chosen': '0.14944', 'rewards_train/rejected': '0.032434', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11701', 'logps_train/rejected': '-106.62', 'logps_train/chosen': '-140.3', 'loss/train': '0.65089', 'examples_per_second': '31.454', 'grad_norm': '217', 'counters/examples': 176128, 'counters/updates': 5504}
train stats after 176160 examples: {'rewards_train/chosen': '0.14471', 'rewards_train/rejected': '0.10267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042042', 'logps_train/rejected': '-149.68', 'logps_train/chosen': '-185.55', 'loss/train': '0.68351', 'examples_per_second': '31.29', 'grad_norm': '64.5', 'counters/examples': 176160, 'counters/updates': 5505}
train stats after 176192 examples: {'rewards_train/chosen': '0.10561', 'rewards_train/rejected': '-0.0081003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11371', 'logps_train/rejected': '-164.78', 'logps_train/chosen': '-161.32', 'loss/train': '0.65106', 'examples_per_second': '30.797', 'grad_norm': '71.5', 'counters/examples': 176192, 'counters/updates': 5506}
train stats after 176224 examples: {'rewards_train/chosen': '0.096848', 'rewards_train/rejected': '-0.0023922', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09924', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-137.83', 'loss/train': '0.65549', 'examples_per_second': '31.389', 'grad_norm': '61.25', 'counters/examples': 176224, 'counters/updates': 5507}
train stats after 176256 examples: {'rewards_train/chosen': '0.11396', 'rewards_train/rejected': '0.056089', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057871', 'logps_train/rejected': '-130.37', 'logps_train/chosen': '-134.66', 'loss/train': '0.67922', 'examples_per_second': '31.902', 'grad_norm': '109', 'counters/examples': 176256, 'counters/updates': 5508}
train stats after 176288 examples: {'rewards_train/chosen': '0.099749', 'rewards_train/rejected': '0.056965', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042784', 'logps_train/rejected': '-156.06', 'logps_train/chosen': '-152.23', 'loss/train': '0.6804', 'examples_per_second': '32.075', 'grad_norm': '80.5', 'counters/examples': 176288, 'counters/updates': 5509}
train stats after 176320 examples: {'rewards_train/chosen': '0.02339', 'rewards_train/rejected': '0.057472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034082', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-168.14', 'loss/train': '0.72277', 'examples_per_second': '30.151', 'grad_norm': '117', 'counters/examples': 176320, 'counters/updates': 5510}
train stats after 176352 examples: {'rewards_train/chosen': '0.21629', 'rewards_train/rejected': '0.0068603', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20943', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-186.35', 'loss/train': '0.61527', 'examples_per_second': '32.302', 'grad_norm': '118', 'counters/examples': 176352, 'counters/updates': 5511}
train stats after 176384 examples: {'rewards_train/chosen': '0.15998', 'rewards_train/rejected': '0.008625', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15135', 'logps_train/rejected': '-104.93', 'logps_train/chosen': '-158.06', 'loss/train': '0.63363', 'examples_per_second': '31.464', 'grad_norm': '76.5', 'counters/examples': 176384, 'counters/updates': 5512}
skipping logging after 176416 examples to avoid logging too frequently
train stats after 176448 examples: {'rewards_train/chosen': '0.11074', 'rewards_train/rejected': '0.046565', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064173', 'logps_train/rejected': '-142.62', 'logps_train/chosen': '-183.26', 'loss/train': '0.67571', 'examples_per_second': '31.478', 'grad_norm': '148', 'counters/examples': 176448, 'counters/updates': 5514}
skipping logging after 176480 examples to avoid logging too frequently
train stats after 176512 examples: {'rewards_train/chosen': '0.070826', 'rewards_train/rejected': '0.13547', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.064642', 'logps_train/rejected': '-142.4', 'logps_train/chosen': '-146.92', 'loss/train': '0.73496', 'examples_per_second': '32.302', 'grad_norm': '78', 'counters/examples': 176512, 'counters/updates': 5516}
train stats after 176544 examples: {'rewards_train/chosen': '0.095515', 'rewards_train/rejected': '0.071582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023933', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-190.29', 'loss/train': '0.70916', 'examples_per_second': '31.758', 'grad_norm': '79', 'counters/examples': 176544, 'counters/updates': 5517}
skipping logging after 176576 examples to avoid logging too frequently
train stats after 176608 examples: {'rewards_train/chosen': '0.096878', 'rewards_train/rejected': '0.10771', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010834', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-126.21', 'loss/train': '0.70696', 'examples_per_second': '32.427', 'grad_norm': '82', 'counters/examples': 176608, 'counters/updates': 5519}
train stats after 176640 examples: {'rewards_train/chosen': '0.15715', 'rewards_train/rejected': '-0.02719', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18434', 'logps_train/rejected': '-99.337', 'logps_train/chosen': '-144.69', 'loss/train': '0.62328', 'examples_per_second': '31.581', 'grad_norm': '91', 'counters/examples': 176640, 'counters/updates': 5520}
train stats after 176672 examples: {'rewards_train/chosen': '0.074712', 'rewards_train/rejected': '0.073809', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.00090244', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-134.74', 'loss/train': '0.70374', 'examples_per_second': '30.036', 'grad_norm': '66', 'counters/examples': 176672, 'counters/updates': 5521}
train stats after 176704 examples: {'rewards_train/chosen': '0.016636', 'rewards_train/rejected': '0.051087', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034452', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-165.13', 'loss/train': '0.71809', 'examples_per_second': '30.36', 'grad_norm': '158', 'counters/examples': 176704, 'counters/updates': 5522}
skipping logging after 176736 examples to avoid logging too frequently
train stats after 176768 examples: {'rewards_train/chosen': '0.072607', 'rewards_train/rejected': '0.047683', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024924', 'logps_train/rejected': '-91.654', 'logps_train/chosen': '-131.43', 'loss/train': '0.69295', 'examples_per_second': '32.758', 'grad_norm': '72.5', 'counters/examples': 176768, 'counters/updates': 5524}
train stats after 176800 examples: {'rewards_train/chosen': '0.14851', 'rewards_train/rejected': '0.051655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096859', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-168.14', 'loss/train': '0.65785', 'examples_per_second': '30.902', 'grad_norm': '67', 'counters/examples': 176800, 'counters/updates': 5525}
train stats after 176832 examples: {'rewards_train/chosen': '0.13938', 'rewards_train/rejected': '0.10729', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032095', 'logps_train/rejected': '-164.82', 'logps_train/chosen': '-173.09', 'loss/train': '0.69453', 'examples_per_second': '30.064', 'grad_norm': '136', 'counters/examples': 176832, 'counters/updates': 5526}
skipping logging after 176864 examples to avoid logging too frequently
train stats after 176896 examples: {'rewards_train/chosen': '0.090404', 'rewards_train/rejected': '0.13384', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.043432', 'logps_train/rejected': '-122.87', 'logps_train/chosen': '-135.64', 'loss/train': '0.72991', 'examples_per_second': '31.522', 'grad_norm': '91', 'counters/examples': 176896, 'counters/updates': 5528}
train stats after 176928 examples: {'rewards_train/chosen': '0.17564', 'rewards_train/rejected': '0.06253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11311', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-158.55', 'loss/train': '0.6493', 'examples_per_second': '31.616', 'grad_norm': '78', 'counters/examples': 176928, 'counters/updates': 5529}
train stats after 176960 examples: {'rewards_train/chosen': '-0.0094473', 'rewards_train/rejected': '0.016231', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025679', 'logps_train/rejected': '-104.93', 'logps_train/chosen': '-124.13', 'loss/train': '0.71662', 'examples_per_second': '33.004', 'grad_norm': '137', 'counters/examples': 176960, 'counters/updates': 5530}
train stats after 176992 examples: {'rewards_train/chosen': '0.20058', 'rewards_train/rejected': '0.09545', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10513', 'logps_train/rejected': '-138.63', 'logps_train/chosen': '-163.15', 'loss/train': '0.65094', 'examples_per_second': '31.34', 'grad_norm': '91', 'counters/examples': 176992, 'counters/updates': 5531}
skipping logging after 177024 examples to avoid logging too frequently
train stats after 177056 examples: {'rewards_train/chosen': '0.16387', 'rewards_train/rejected': '0.032919', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13095', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-170.07', 'loss/train': '0.64902', 'examples_per_second': '24.736', 'grad_norm': '124', 'counters/examples': 177056, 'counters/updates': 5533}
train stats after 177088 examples: {'rewards_train/chosen': '0.0752', 'rewards_train/rejected': '0.15665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.081452', 'logps_train/rejected': '-163.73', 'logps_train/chosen': '-132.59', 'loss/train': '0.75758', 'examples_per_second': '30.124', 'grad_norm': '80', 'counters/examples': 177088, 'counters/updates': 5534}
train stats after 177120 examples: {'rewards_train/chosen': '0.10578', 'rewards_train/rejected': '0.10428', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0014994', 'logps_train/rejected': '-164.48', 'logps_train/chosen': '-160.6', 'loss/train': '0.71656', 'examples_per_second': '31.522', 'grad_norm': '116', 'counters/examples': 177120, 'counters/updates': 5535}
train stats after 177152 examples: {'rewards_train/chosen': '0.1286', 'rewards_train/rejected': '0.078697', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049898', 'logps_train/rejected': '-153.99', 'logps_train/chosen': '-146.17', 'loss/train': '0.6823', 'examples_per_second': '30.212', 'grad_norm': '91', 'counters/examples': 177152, 'counters/updates': 5536}
train stats after 177184 examples: {'rewards_train/chosen': '0.17458', 'rewards_train/rejected': '0.075735', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098842', 'logps_train/rejected': '-167.03', 'logps_train/chosen': '-206.71', 'loss/train': '0.65646', 'examples_per_second': '30.65', 'grad_norm': '180', 'counters/examples': 177184, 'counters/updates': 5537}
train stats after 177216 examples: {'rewards_train/chosen': '0.061483', 'rewards_train/rejected': '0.01481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046673', 'logps_train/rejected': '-122.95', 'logps_train/chosen': '-161.73', 'loss/train': '0.68303', 'examples_per_second': '31.122', 'grad_norm': '105.5', 'counters/examples': 177216, 'counters/updates': 5538}
skipping logging after 177248 examples to avoid logging too frequently
train stats after 177280 examples: {'rewards_train/chosen': '0.076614', 'rewards_train/rejected': '0.17788', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.10127', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-145.96', 'loss/train': '0.75854', 'examples_per_second': '31.713', 'grad_norm': '152', 'counters/examples': 177280, 'counters/updates': 5540}
train stats after 177312 examples: {'rewards_train/chosen': '0.073538', 'rewards_train/rejected': '0.072559', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00097957', 'logps_train/rejected': '-99.159', 'logps_train/chosen': '-144.55', 'loss/train': '0.70411', 'examples_per_second': '32.802', 'grad_norm': '107', 'counters/examples': 177312, 'counters/updates': 5541}
train stats after 177344 examples: {'rewards_train/chosen': '0.22628', 'rewards_train/rejected': '0.092113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13416', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-163.34', 'loss/train': '0.64348', 'examples_per_second': '30.563', 'grad_norm': '73', 'counters/examples': 177344, 'counters/updates': 5542}
train stats after 177376 examples: {'rewards_train/chosen': '0.24775', 'rewards_train/rejected': '0.022471', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22528', 'logps_train/rejected': '-156.8', 'logps_train/chosen': '-217.93', 'loss/train': '0.61606', 'examples_per_second': '30.454', 'grad_norm': '72.5', 'counters/examples': 177376, 'counters/updates': 5543}
train stats after 177408 examples: {'rewards_train/chosen': '0.034393', 'rewards_train/rejected': '0.10465', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.070259', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-119.21', 'loss/train': '0.74333', 'examples_per_second': '33.196', 'grad_norm': '91', 'counters/examples': 177408, 'counters/updates': 5544}
train stats after 177440 examples: {'rewards_train/chosen': '0.078504', 'rewards_train/rejected': '0.023388', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055116', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-141.08', 'loss/train': '0.67385', 'examples_per_second': '30.481', 'grad_norm': '94.5', 'counters/examples': 177440, 'counters/updates': 5545}
train stats after 177472 examples: {'rewards_train/chosen': '0.079151', 'rewards_train/rejected': '-0.018909', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098059', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-170.4', 'loss/train': '0.65198', 'examples_per_second': '31.514', 'grad_norm': '93', 'counters/examples': 177472, 'counters/updates': 5546}
train stats after 177504 examples: {'rewards_train/chosen': '0.1839', 'rewards_train/rejected': '-0.012709', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19661', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-160.34', 'loss/train': '0.62313', 'examples_per_second': '31.5', 'grad_norm': '51', 'counters/examples': 177504, 'counters/updates': 5547}
skipping logging after 177536 examples to avoid logging too frequently
train stats after 177568 examples: {'rewards_train/chosen': '0.069668', 'rewards_train/rejected': '0.093857', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.024189', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-115.51', 'loss/train': '0.7153', 'examples_per_second': '33.131', 'grad_norm': '68.5', 'counters/examples': 177568, 'counters/updates': 5549}
train stats after 177600 examples: {'rewards_train/chosen': '0.045535', 'rewards_train/rejected': '0.015171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030364', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-145.67', 'loss/train': '0.68401', 'examples_per_second': '30.439', 'grad_norm': '105.5', 'counters/examples': 177600, 'counters/updates': 5550}
train stats after 177632 examples: {'rewards_train/chosen': '0.075076', 'rewards_train/rejected': '-0.039224', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1143', 'logps_train/rejected': '-149.95', 'logps_train/chosen': '-144.04', 'loss/train': '0.65194', 'examples_per_second': '32.064', 'grad_norm': '54', 'counters/examples': 177632, 'counters/updates': 5551}
train stats after 177664 examples: {'rewards_train/chosen': '0.080606', 'rewards_train/rejected': '0.04824', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032366', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-142.88', 'loss/train': '0.68906', 'examples_per_second': '31.713', 'grad_norm': '104', 'counters/examples': 177664, 'counters/updates': 5552}
train stats after 177696 examples: {'rewards_train/chosen': '0.10569', 'rewards_train/rejected': '0.06693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038758', 'logps_train/rejected': '-115.63', 'logps_train/chosen': '-182.25', 'loss/train': '0.6903', 'examples_per_second': '30.317', 'grad_norm': '82', 'counters/examples': 177696, 'counters/updates': 5553}
train stats after 177728 examples: {'rewards_train/chosen': '0.084795', 'rewards_train/rejected': '0.035942', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048852', 'logps_train/rejected': '-153.48', 'logps_train/chosen': '-170.38', 'loss/train': '0.6825', 'examples_per_second': '30.668', 'grad_norm': '116.5', 'counters/examples': 177728, 'counters/updates': 5554}
train stats after 177760 examples: {'rewards_train/chosen': '0.047933', 'rewards_train/rejected': '-0.026034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073967', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-154.27', 'loss/train': '0.66515', 'examples_per_second': '32.156', 'grad_norm': '49.25', 'counters/examples': 177760, 'counters/updates': 5555}
train stats after 177792 examples: {'rewards_train/chosen': '0.18129', 'rewards_train/rejected': '0.068208', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11308', 'logps_train/rejected': '-149.41', 'logps_train/chosen': '-171.97', 'loss/train': '0.65754', 'examples_per_second': '31.504', 'grad_norm': '78', 'counters/examples': 177792, 'counters/updates': 5556}
train stats after 177824 examples: {'rewards_train/chosen': '0.016475', 'rewards_train/rejected': '-0.010534', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027009', 'logps_train/rejected': '-134.85', 'logps_train/chosen': '-145.3', 'loss/train': '0.68901', 'examples_per_second': '31.512', 'grad_norm': '86.5', 'counters/examples': 177824, 'counters/updates': 5557}
train stats after 177856 examples: {'rewards_train/chosen': '0.081732', 'rewards_train/rejected': '0.029548', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052184', 'logps_train/rejected': '-178.61', 'logps_train/chosen': '-160.58', 'loss/train': '0.68428', 'examples_per_second': '31.512', 'grad_norm': '74', 'counters/examples': 177856, 'counters/updates': 5558}
train stats after 177888 examples: {'rewards_train/chosen': '0.12156', 'rewards_train/rejected': '0.10603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01553', 'logps_train/rejected': '-176.45', 'logps_train/chosen': '-168.08', 'loss/train': '0.70389', 'examples_per_second': '30.872', 'grad_norm': '125.5', 'counters/examples': 177888, 'counters/updates': 5559}
train stats after 177920 examples: {'rewards_train/chosen': '0.1532', 'rewards_train/rejected': '0.091594', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061607', 'logps_train/rejected': '-119.91', 'logps_train/chosen': '-142.38', 'loss/train': '0.67819', 'examples_per_second': '31.515', 'grad_norm': '73', 'counters/examples': 177920, 'counters/updates': 5560}
train stats after 177952 examples: {'rewards_train/chosen': '0.033281', 'rewards_train/rejected': '0.061536', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028255', 'logps_train/rejected': '-96.203', 'logps_train/chosen': '-135.85', 'loss/train': '0.72145', 'examples_per_second': '31.557', 'grad_norm': '56.75', 'counters/examples': 177952, 'counters/updates': 5561}
skipping logging after 177984 examples to avoid logging too frequently
train stats after 178016 examples: {'rewards_train/chosen': '0.062523', 'rewards_train/rejected': '0.099384', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036861', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-160.42', 'loss/train': '0.72294', 'examples_per_second': '35.924', 'grad_norm': '174', 'counters/examples': 178016, 'counters/updates': 5563}
train stats after 178048 examples: {'rewards_train/chosen': '0.14652', 'rewards_train/rejected': '0.021202', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12532', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-183.72', 'loss/train': '0.65889', 'examples_per_second': '30.411', 'grad_norm': '54.75', 'counters/examples': 178048, 'counters/updates': 5564}
train stats after 178080 examples: {'rewards_train/chosen': '-0.0186', 'rewards_train/rejected': '0.057269', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.07587', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-140.93', 'loss/train': '0.74291', 'examples_per_second': '31.161', 'grad_norm': '68.5', 'counters/examples': 178080, 'counters/updates': 5565}
train stats after 178112 examples: {'rewards_train/chosen': '0.13694', 'rewards_train/rejected': '0.072274', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064666', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-146.91', 'loss/train': '0.67108', 'examples_per_second': '30.542', 'grad_norm': '274', 'counters/examples': 178112, 'counters/updates': 5566}
train stats after 178144 examples: {'rewards_train/chosen': '0.063568', 'rewards_train/rejected': '0.10167', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.038102', 'logps_train/rejected': '-149.69', 'logps_train/chosen': '-150.72', 'loss/train': '0.72895', 'examples_per_second': '33.119', 'grad_norm': '76.5', 'counters/examples': 178144, 'counters/updates': 5567}
train stats after 178176 examples: {'rewards_train/chosen': '0.15742', 'rewards_train/rejected': '-0.058363', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21578', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-115.59', 'loss/train': '0.60874', 'examples_per_second': '32.146', 'grad_norm': '64.5', 'counters/examples': 178176, 'counters/updates': 5568}
train stats after 178208 examples: {'rewards_train/chosen': '0.16043', 'rewards_train/rejected': '0.061833', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0986', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-167.46', 'loss/train': '0.65535', 'examples_per_second': '31.278', 'grad_norm': '83.5', 'counters/examples': 178208, 'counters/updates': 5569}
train stats after 178240 examples: {'rewards_train/chosen': '0.05458', 'rewards_train/rejected': '0.036482', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018099', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-158.77', 'loss/train': '0.6913', 'examples_per_second': '30.107', 'grad_norm': '100.5', 'counters/examples': 178240, 'counters/updates': 5570}
train stats after 178272 examples: {'rewards_train/chosen': '0.086764', 'rewards_train/rejected': '0.076794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0099703', 'logps_train/rejected': '-125.03', 'logps_train/chosen': '-135.08', 'loss/train': '0.7102', 'examples_per_second': '33.084', 'grad_norm': '47.5', 'counters/examples': 178272, 'counters/updates': 5571}
train stats after 178304 examples: {'rewards_train/chosen': '0.10268', 'rewards_train/rejected': '0.04457', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.058108', 'logps_train/rejected': '-108.67', 'logps_train/chosen': '-136.6', 'loss/train': '0.67683', 'examples_per_second': '31.342', 'grad_norm': '61.25', 'counters/examples': 178304, 'counters/updates': 5572}
skipping logging after 178336 examples to avoid logging too frequently
train stats after 178368 examples: {'rewards_train/chosen': '0.11535', 'rewards_train/rejected': '0.069565', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045785', 'logps_train/rejected': '-93.678', 'logps_train/chosen': '-144.92', 'loss/train': '0.68118', 'examples_per_second': '30.936', 'grad_norm': '77.5', 'counters/examples': 178368, 'counters/updates': 5574}
train stats after 178400 examples: {'rewards_train/chosen': '0.042609', 'rewards_train/rejected': '0.086672', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.044063', 'logps_train/rejected': '-119.79', 'logps_train/chosen': '-128.2', 'loss/train': '0.72688', 'examples_per_second': '32.953', 'grad_norm': '74', 'counters/examples': 178400, 'counters/updates': 5575}
skipping logging after 178432 examples to avoid logging too frequently
train stats after 178464 examples: {'rewards_train/chosen': '0.10003', 'rewards_train/rejected': '0.032652', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.067377', 'logps_train/rejected': '-94.821', 'logps_train/chosen': '-160.69', 'loss/train': '0.68552', 'examples_per_second': '30.51', 'grad_norm': '182', 'counters/examples': 178464, 'counters/updates': 5577}
train stats after 178496 examples: {'rewards_train/chosen': '0.071771', 'rewards_train/rejected': '0.074829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0030582', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-165.87', 'loss/train': '0.70349', 'examples_per_second': '30.845', 'grad_norm': '131', 'counters/examples': 178496, 'counters/updates': 5578}
train stats after 178528 examples: {'rewards_train/chosen': '0.15395', 'rewards_train/rejected': '0.13334', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020611', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-146.49', 'loss/train': '0.69289', 'examples_per_second': '31.13', 'grad_norm': '69', 'counters/examples': 178528, 'counters/updates': 5579}
skipping logging after 178560 examples to avoid logging too frequently
train stats after 178592 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '-0.021317', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13572', 'logps_train/rejected': '-89.745', 'logps_train/chosen': '-134.16', 'loss/train': '0.64517', 'examples_per_second': '30.263', 'grad_norm': '75', 'counters/examples': 178592, 'counters/updates': 5581}
train stats after 178624 examples: {'rewards_train/chosen': '0.077715', 'rewards_train/rejected': '0.063902', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013814', 'logps_train/rejected': '-98.457', 'logps_train/chosen': '-107.26', 'loss/train': '0.70933', 'examples_per_second': '33.269', 'grad_norm': '111.5', 'counters/examples': 178624, 'counters/updates': 5582}
train stats after 178656 examples: {'rewards_train/chosen': '0.12976', 'rewards_train/rejected': '0.057911', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071848', 'logps_train/rejected': '-149.17', 'logps_train/chosen': '-144.05', 'loss/train': '0.67268', 'examples_per_second': '31.02', 'grad_norm': '77', 'counters/examples': 178656, 'counters/updates': 5583}
skipping logging after 178688 examples to avoid logging too frequently
train stats after 178720 examples: {'rewards_train/chosen': '0.0855', 'rewards_train/rejected': '0.0288', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056699', 'logps_train/rejected': '-95.901', 'logps_train/chosen': '-131.81', 'loss/train': '0.67145', 'examples_per_second': '33.006', 'grad_norm': '53.25', 'counters/examples': 178720, 'counters/updates': 5585}
skipping logging after 178752 examples to avoid logging too frequently
train stats after 178784 examples: {'rewards_train/chosen': '-0.002679', 'rewards_train/rejected': '-0.01301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.010331', 'logps_train/rejected': '-153.99', 'logps_train/chosen': '-202.76', 'loss/train': '0.7028', 'examples_per_second': '31.517', 'grad_norm': '73', 'counters/examples': 178784, 'counters/updates': 5587}
skipping logging after 178816 examples to avoid logging too frequently
train stats after 178848 examples: {'rewards_train/chosen': '0.08556', 'rewards_train/rejected': '0.058591', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026968', 'logps_train/rejected': '-145.05', 'logps_train/chosen': '-140.26', 'loss/train': '0.68742', 'examples_per_second': '31.288', 'grad_norm': '67', 'counters/examples': 178848, 'counters/updates': 5589}
train stats after 178880 examples: {'rewards_train/chosen': '0.077505', 'rewards_train/rejected': '0.11417', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036669', 'logps_train/rejected': '-184.68', 'logps_train/chosen': '-144.25', 'loss/train': '0.72095', 'examples_per_second': '31.277', 'grad_norm': '107', 'counters/examples': 178880, 'counters/updates': 5590}
train stats after 178912 examples: {'rewards_train/chosen': '0.065011', 'rewards_train/rejected': '0.13472', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.06971', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-157.62', 'loss/train': '0.74258', 'examples_per_second': '31.543', 'grad_norm': '94', 'counters/examples': 178912, 'counters/updates': 5591}
train stats after 178944 examples: {'rewards_train/chosen': '0.13469', 'rewards_train/rejected': '0.16776', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033062', 'logps_train/rejected': '-151.2', 'logps_train/chosen': '-149.43', 'loss/train': '0.72392', 'examples_per_second': '30.863', 'grad_norm': '60.75', 'counters/examples': 178944, 'counters/updates': 5592}
train stats after 178976 examples: {'rewards_train/chosen': '0.13548', 'rewards_train/rejected': '0.028906', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10657', 'logps_train/rejected': '-118.48', 'logps_train/chosen': '-139.67', 'loss/train': '0.65822', 'examples_per_second': '30.908', 'grad_norm': '74.5', 'counters/examples': 178976, 'counters/updates': 5593}
train stats after 179008 examples: {'rewards_train/chosen': '0.12084', 'rewards_train/rejected': '0.014342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1065', 'logps_train/rejected': '-148.85', 'logps_train/chosen': '-134.17', 'loss/train': '0.64975', 'examples_per_second': '30.075', 'grad_norm': '73', 'counters/examples': 179008, 'counters/updates': 5594}
train stats after 179040 examples: {'rewards_train/chosen': '0.02565', 'rewards_train/rejected': '0.041539', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.015889', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-128.91', 'loss/train': '0.71043', 'examples_per_second': '31.166', 'grad_norm': '63.5', 'counters/examples': 179040, 'counters/updates': 5595}
train stats after 179072 examples: {'rewards_train/chosen': '0.13728', 'rewards_train/rejected': '0.062767', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074516', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-148.56', 'loss/train': '0.67623', 'examples_per_second': '32.225', 'grad_norm': '145', 'counters/examples': 179072, 'counters/updates': 5596}
train stats after 179104 examples: {'rewards_train/chosen': '0.019687', 'rewards_train/rejected': '0.046124', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026437', 'logps_train/rejected': '-104.28', 'logps_train/chosen': '-162.35', 'loss/train': '0.72089', 'examples_per_second': '33.026', 'grad_norm': '120.5', 'counters/examples': 179104, 'counters/updates': 5597}
skipping logging after 179136 examples to avoid logging too frequently
train stats after 179168 examples: {'rewards_train/chosen': '0.053197', 'rewards_train/rejected': '0.097273', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044076', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-143.31', 'loss/train': '0.72448', 'examples_per_second': '32.975', 'grad_norm': '68.5', 'counters/examples': 179168, 'counters/updates': 5599}
train stats after 179200 examples: {'rewards_train/chosen': '0.11893', 'rewards_train/rejected': '0.047742', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071187', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-166.85', 'loss/train': '0.67782', 'examples_per_second': '31.259', 'grad_norm': '88.5', 'counters/examples': 179200, 'counters/updates': 5600}
skipping logging after 179232 examples to avoid logging too frequently
train stats after 179264 examples: {'rewards_train/chosen': '0.15169', 'rewards_train/rejected': '0.11208', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039607', 'logps_train/rejected': '-174.08', 'logps_train/chosen': '-171.48', 'loss/train': '0.69738', 'examples_per_second': '30.585', 'grad_norm': '103.5', 'counters/examples': 179264, 'counters/updates': 5602}
train stats after 179296 examples: {'rewards_train/chosen': '0.13731', 'rewards_train/rejected': '0.041822', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095492', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-163.45', 'loss/train': '0.66278', 'examples_per_second': '24.257', 'grad_norm': '107', 'counters/examples': 179296, 'counters/updates': 5603}
train stats after 179328 examples: {'rewards_train/chosen': '0.19693', 'rewards_train/rejected': '0.083453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11348', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-144.37', 'loss/train': '0.65543', 'examples_per_second': '30.513', 'grad_norm': '88', 'counters/examples': 179328, 'counters/updates': 5604}
train stats after 179360 examples: {'rewards_train/chosen': '0.084', 'rewards_train/rejected': '-0.02976', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11376', 'logps_train/rejected': '-154.13', 'logps_train/chosen': '-127.07', 'loss/train': '0.64839', 'examples_per_second': '32.137', 'grad_norm': '79', 'counters/examples': 179360, 'counters/updates': 5605}
train stats after 179392 examples: {'rewards_train/chosen': '0.16344', 'rewards_train/rejected': '0.050556', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11288', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-144.4', 'loss/train': '0.65143', 'examples_per_second': '25.593', 'grad_norm': '51', 'counters/examples': 179392, 'counters/updates': 5606}
skipping logging after 179424 examples to avoid logging too frequently
train stats after 179456 examples: {'rewards_train/chosen': '0.061504', 'rewards_train/rejected': '-0.013831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075334', 'logps_train/rejected': '-110.22', 'logps_train/chosen': '-122.19', 'loss/train': '0.66692', 'examples_per_second': '41.652', 'grad_norm': '68.5', 'counters/examples': 179456, 'counters/updates': 5608}
train stats after 179488 examples: {'rewards_train/chosen': '0.051925', 'rewards_train/rejected': '0.044026', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0078987', 'logps_train/rejected': '-147.37', 'logps_train/chosen': '-138.91', 'loss/train': '0.69994', 'examples_per_second': '31.53', 'grad_norm': '106.5', 'counters/examples': 179488, 'counters/updates': 5609}
train stats after 179520 examples: {'rewards_train/chosen': '0.086558', 'rewards_train/rejected': '0.041035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045523', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-169.08', 'loss/train': '0.68211', 'examples_per_second': '31.271', 'grad_norm': '169', 'counters/examples': 179520, 'counters/updates': 5610}
train stats after 179552 examples: {'rewards_train/chosen': '0.069188', 'rewards_train/rejected': '0.088255', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019067', 'logps_train/rejected': '-145.45', 'logps_train/chosen': '-176.51', 'loss/train': '0.72538', 'examples_per_second': '30.013', 'grad_norm': '86', 'counters/examples': 179552, 'counters/updates': 5611}
train stats after 179584 examples: {'rewards_train/chosen': '0.13432', 'rewards_train/rejected': '0.024335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10998', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-162.57', 'loss/train': '0.65138', 'examples_per_second': '32.013', 'grad_norm': '92.5', 'counters/examples': 179584, 'counters/updates': 5612}
skipping logging after 179616 examples to avoid logging too frequently
train stats after 179648 examples: {'rewards_train/chosen': '0.16545', 'rewards_train/rejected': '0.15552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0099311', 'logps_train/rejected': '-178.08', 'logps_train/chosen': '-192.7', 'loss/train': '0.70166', 'examples_per_second': '33.32', 'grad_norm': '85.5', 'counters/examples': 179648, 'counters/updates': 5614}
train stats after 179680 examples: {'rewards_train/chosen': '0.11832', 'rewards_train/rejected': '0.12279', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0044698', 'logps_train/rejected': '-123', 'logps_train/chosen': '-122.29', 'loss/train': '0.70958', 'examples_per_second': '30.725', 'grad_norm': '117', 'counters/examples': 179680, 'counters/updates': 5615}
train stats after 179712 examples: {'rewards_train/chosen': '0.21522', 'rewards_train/rejected': '0.17651', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038717', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-165.31', 'loss/train': '0.70107', 'examples_per_second': '29.856', 'grad_norm': '87.5', 'counters/examples': 179712, 'counters/updates': 5616}
skipping logging after 179744 examples to avoid logging too frequently
train stats after 179776 examples: {'rewards_train/chosen': '0.060783', 'rewards_train/rejected': '0.056167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0046169', 'logps_train/rejected': '-152.02', 'logps_train/chosen': '-185.21', 'loss/train': '0.70224', 'examples_per_second': '31.499', 'grad_norm': '181', 'counters/examples': 179776, 'counters/updates': 5618}
train stats after 179808 examples: {'rewards_train/chosen': '0.09997', 'rewards_train/rejected': '0.099796', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0001747', 'logps_train/rejected': '-141.5', 'logps_train/chosen': '-147.2', 'loss/train': '0.72897', 'examples_per_second': '31.541', 'grad_norm': '79.5', 'counters/examples': 179808, 'counters/updates': 5619}
train stats after 179840 examples: {'rewards_train/chosen': '0.084158', 'rewards_train/rejected': '0.022339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061819', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-140.39', 'loss/train': '0.67588', 'examples_per_second': '31.42', 'grad_norm': '272', 'counters/examples': 179840, 'counters/updates': 5620}
skipping logging after 179872 examples to avoid logging too frequently
train stats after 179904 examples: {'rewards_train/chosen': '0.032245', 'rewards_train/rejected': '0.077986', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045741', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-119.49', 'loss/train': '0.72631', 'examples_per_second': '33.294', 'grad_norm': '185', 'counters/examples': 179904, 'counters/updates': 5622}
train stats after 179936 examples: {'rewards_train/chosen': '0.053339', 'rewards_train/rejected': '0.0088134', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044526', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-146.72', 'loss/train': '0.67836', 'examples_per_second': '31.519', 'grad_norm': '134', 'counters/examples': 179936, 'counters/updates': 5623}
skipping logging after 179968 examples to avoid logging too frequently
train stats after 180000 examples: {'rewards_train/chosen': '0.10833', 'rewards_train/rejected': '0.0056324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1027', 'logps_train/rejected': '-137.49', 'logps_train/chosen': '-149.27', 'loss/train': '0.66097', 'examples_per_second': '31.501', 'grad_norm': '145', 'counters/examples': 180000, 'counters/updates': 5625}
Running evaluation after 180000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.92it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.96it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.88it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.81it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.85it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.88it/s]
eval after 180000: {'rewards_eval/chosen': '0.11119', 'rewards_eval/rejected': '0.053155', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.058033', 'logps_eval/rejected': '-127.61', 'logps_eval/chosen': '-149.89', 'loss/eval': '0.6775'}
skipping save for non epoch
train stats after 180032 examples: {'rewards_train/chosen': '0.10898', 'rewards_train/rejected': '0.020165', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088817', 'logps_train/rejected': '-158.47', 'logps_train/chosen': '-159.36', 'loss/train': '0.6603', 'examples_per_second': '31.881', 'grad_norm': '103.5', 'counters/examples': 180032, 'counters/updates': 5626}
skipping logging after 180064 examples to avoid logging too frequently
train stats after 180096 examples: {'rewards_train/chosen': '0.044684', 'rewards_train/rejected': '-0.0025994', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047283', 'logps_train/rejected': '-128.76', 'logps_train/chosen': '-167.52', 'loss/train': '0.67503', 'examples_per_second': '33.212', 'grad_norm': '107', 'counters/examples': 180096, 'counters/updates': 5628}
train stats after 180128 examples: {'rewards_train/chosen': '0.049133', 'rewards_train/rejected': '0.090334', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041201', 'logps_train/rejected': '-110.69', 'logps_train/chosen': '-135.51', 'loss/train': '0.71954', 'examples_per_second': '30.044', 'grad_norm': '134', 'counters/examples': 180128, 'counters/updates': 5629}
skipping logging after 180160 examples to avoid logging too frequently
train stats after 180192 examples: {'rewards_train/chosen': '-0.043499', 'rewards_train/rejected': '0.014732', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.058232', 'logps_train/rejected': '-131.59', 'logps_train/chosen': '-183.14', 'loss/train': '0.74386', 'examples_per_second': '31.573', 'grad_norm': '130', 'counters/examples': 180192, 'counters/updates': 5631}
train stats after 180224 examples: {'rewards_train/chosen': '0.10189', 'rewards_train/rejected': '0.16695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.065058', 'logps_train/rejected': '-183.03', 'logps_train/chosen': '-145.91', 'loss/train': '0.74396', 'examples_per_second': '30.849', 'grad_norm': '102.5', 'counters/examples': 180224, 'counters/updates': 5632}
train stats after 180256 examples: {'rewards_train/chosen': '0.11985', 'rewards_train/rejected': '0.046075', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073775', 'logps_train/rejected': '-106.01', 'logps_train/chosen': '-194.77', 'loss/train': '0.66908', 'examples_per_second': '29.937', 'grad_norm': '80', 'counters/examples': 180256, 'counters/updates': 5633}
train stats after 180288 examples: {'rewards_train/chosen': '0.07804', 'rewards_train/rejected': '0.0099344', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068106', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-148.06', 'loss/train': '0.67128', 'examples_per_second': '31.04', 'grad_norm': '138', 'counters/examples': 180288, 'counters/updates': 5634}
train stats after 180320 examples: {'rewards_train/chosen': '0.094406', 'rewards_train/rejected': '0.014787', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079618', 'logps_train/rejected': '-123.33', 'logps_train/chosen': '-176.28', 'loss/train': '0.67979', 'examples_per_second': '31.81', 'grad_norm': '67.5', 'counters/examples': 180320, 'counters/updates': 5635}
train stats after 180352 examples: {'rewards_train/chosen': '0.082064', 'rewards_train/rejected': '0.064407', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017656', 'logps_train/rejected': '-163.8', 'logps_train/chosen': '-152.3', 'loss/train': '0.69549', 'examples_per_second': '31.847', 'grad_norm': '139', 'counters/examples': 180352, 'counters/updates': 5636}
skipping logging after 180384 examples to avoid logging too frequently
train stats after 180416 examples: {'rewards_train/chosen': '0.088901', 'rewards_train/rejected': '0.045033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043868', 'logps_train/rejected': '-104.32', 'logps_train/chosen': '-123.53', 'loss/train': '0.68317', 'examples_per_second': '31.625', 'grad_norm': '161', 'counters/examples': 180416, 'counters/updates': 5638}
train stats after 180448 examples: {'rewards_train/chosen': '0.18418', 'rewards_train/rejected': '0.065033', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11915', 'logps_train/rejected': '-131.47', 'logps_train/chosen': '-116.61', 'loss/train': '0.65776', 'examples_per_second': '31.63', 'grad_norm': '76', 'counters/examples': 180448, 'counters/updates': 5639}
train stats after 180480 examples: {'rewards_train/chosen': '0.068394', 'rewards_train/rejected': '0.079392', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.010998', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-127.21', 'loss/train': '0.70596', 'examples_per_second': '31.181', 'grad_norm': '160', 'counters/examples': 180480, 'counters/updates': 5640}
skipping logging after 180512 examples to avoid logging too frequently
train stats after 180544 examples: {'rewards_train/chosen': '0.096622', 'rewards_train/rejected': '0.083456', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013165', 'logps_train/rejected': '-103.49', 'logps_train/chosen': '-112.96', 'loss/train': '0.69246', 'examples_per_second': '34.813', 'grad_norm': '124', 'counters/examples': 180544, 'counters/updates': 5642}
train stats after 180576 examples: {'rewards_train/chosen': '0.1156', 'rewards_train/rejected': '0.058305', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057295', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-174.22', 'loss/train': '0.69168', 'examples_per_second': '30.38', 'grad_norm': '61.5', 'counters/examples': 180576, 'counters/updates': 5643}
train stats after 180608 examples: {'rewards_train/chosen': '0.095455', 'rewards_train/rejected': '0.030187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065267', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-186.37', 'loss/train': '0.6721', 'examples_per_second': '30.058', 'grad_norm': '101.5', 'counters/examples': 180608, 'counters/updates': 5644}
train stats after 180640 examples: {'rewards_train/chosen': '0.056169', 'rewards_train/rejected': '0.022711', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033458', 'logps_train/rejected': '-112.4', 'logps_train/chosen': '-130.65', 'loss/train': '0.68343', 'examples_per_second': '31.306', 'grad_norm': '83.5', 'counters/examples': 180640, 'counters/updates': 5645}
train stats after 180672 examples: {'rewards_train/chosen': '0.14765', 'rewards_train/rejected': '0.048782', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098865', 'logps_train/rejected': '-100.72', 'logps_train/chosen': '-151.14', 'loss/train': '0.65828', 'examples_per_second': '32.451', 'grad_norm': '146', 'counters/examples': 180672, 'counters/updates': 5646}
train stats after 180704 examples: {'rewards_train/chosen': '0.031773', 'rewards_train/rejected': '0.033437', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0016641', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-137.3', 'loss/train': '0.70749', 'examples_per_second': '31.588', 'grad_norm': '71', 'counters/examples': 180704, 'counters/updates': 5647}
train stats after 180736 examples: {'rewards_train/chosen': '0.041192', 'rewards_train/rejected': '0.040797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00039484', 'logps_train/rejected': '-139.74', 'logps_train/chosen': '-141.41', 'loss/train': '0.70843', 'examples_per_second': '31.177', 'grad_norm': '72.5', 'counters/examples': 180736, 'counters/updates': 5648}
train stats after 180768 examples: {'rewards_train/chosen': '0.16662', 'rewards_train/rejected': '0.052707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11392', 'logps_train/rejected': '-115.36', 'logps_train/chosen': '-145.37', 'loss/train': '0.65571', 'examples_per_second': '30.741', 'grad_norm': '98', 'counters/examples': 180768, 'counters/updates': 5649}
skipping logging after 180800 examples to avoid logging too frequently
train stats after 180832 examples: {'rewards_train/chosen': '0.11126', 'rewards_train/rejected': '0.066697', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044561', 'logps_train/rejected': '-151.93', 'logps_train/chosen': '-172.45', 'loss/train': '0.69192', 'examples_per_second': '31.96', 'grad_norm': '92.5', 'counters/examples': 180832, 'counters/updates': 5651}
train stats after 180864 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.084745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.016853', 'logps_train/rejected': '-122.74', 'logps_train/chosen': '-178.51', 'loss/train': '0.69879', 'examples_per_second': '31.212', 'grad_norm': '108', 'counters/examples': 180864, 'counters/updates': 5652}
train stats after 180896 examples: {'rewards_train/chosen': '0.18126', 'rewards_train/rejected': '0.12002', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.06124', 'logps_train/rejected': '-158.79', 'logps_train/chosen': '-170.58', 'loss/train': '0.67938', 'examples_per_second': '30.453', 'grad_norm': '189', 'counters/examples': 180896, 'counters/updates': 5653}
train stats after 180928 examples: {'rewards_train/chosen': '0.13855', 'rewards_train/rejected': '0.097774', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.040771', 'logps_train/rejected': '-161.35', 'logps_train/chosen': '-165.11', 'loss/train': '0.69396', 'examples_per_second': '30.312', 'grad_norm': '205', 'counters/examples': 180928, 'counters/updates': 5654}
train stats after 180960 examples: {'rewards_train/chosen': '0.12627', 'rewards_train/rejected': '0.049964', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076307', 'logps_train/rejected': '-134.77', 'logps_train/chosen': '-132.62', 'loss/train': '0.67123', 'examples_per_second': '32.612', 'grad_norm': '94.5', 'counters/examples': 180960, 'counters/updates': 5655}
train stats after 180992 examples: {'rewards_train/chosen': '0.11684', 'rewards_train/rejected': '0.021965', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.094871', 'logps_train/rejected': '-154.76', 'logps_train/chosen': '-178.18', 'loss/train': '0.65374', 'examples_per_second': '31.587', 'grad_norm': '99.5', 'counters/examples': 180992, 'counters/updates': 5656}
train stats after 181024 examples: {'rewards_train/chosen': '0.12661', 'rewards_train/rejected': '0.065932', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060679', 'logps_train/rejected': '-113.79', 'logps_train/chosen': '-152.87', 'loss/train': '0.67304', 'examples_per_second': '30.186', 'grad_norm': '72.5', 'counters/examples': 181024, 'counters/updates': 5657}
train stats after 181056 examples: {'rewards_train/chosen': '0.045016', 'rewards_train/rejected': '0.051373', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0063569', 'logps_train/rejected': '-126.76', 'logps_train/chosen': '-165.81', 'loss/train': '0.70692', 'examples_per_second': '32.907', 'grad_norm': '57.25', 'counters/examples': 181056, 'counters/updates': 5658}
train stats after 181088 examples: {'rewards_train/chosen': '0.15398', 'rewards_train/rejected': '0.054409', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09957', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-188.93', 'loss/train': '0.65791', 'examples_per_second': '30.56', 'grad_norm': '79.5', 'counters/examples': 181088, 'counters/updates': 5659}
train stats after 181120 examples: {'rewards_train/chosen': '0.12858', 'rewards_train/rejected': '9.748e-05', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12848', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-168.19', 'loss/train': '0.64844', 'examples_per_second': '30.931', 'grad_norm': '115.5', 'counters/examples': 181120, 'counters/updates': 5660}
train stats after 181152 examples: {'rewards_train/chosen': '0.11967', 'rewards_train/rejected': '0.10051', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.019159', 'logps_train/rejected': '-145.88', 'logps_train/chosen': '-183.94', 'loss/train': '0.70635', 'examples_per_second': '31.476', 'grad_norm': '103', 'counters/examples': 181152, 'counters/updates': 5661}
train stats after 181184 examples: {'rewards_train/chosen': '0.11103', 'rewards_train/rejected': '0.035195', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075837', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-145.85', 'loss/train': '0.67319', 'examples_per_second': '30.01', 'grad_norm': '77', 'counters/examples': 181184, 'counters/updates': 5662}
train stats after 181216 examples: {'rewards_train/chosen': '0.20052', 'rewards_train/rejected': '0.054066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14645', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-180.59', 'loss/train': '0.63501', 'examples_per_second': '30.163', 'grad_norm': '68', 'counters/examples': 181216, 'counters/updates': 5663}
train stats after 181248 examples: {'rewards_train/chosen': '0.21188', 'rewards_train/rejected': '0.11899', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.092891', 'logps_train/rejected': '-154.01', 'logps_train/chosen': '-178.04', 'loss/train': '0.6734', 'examples_per_second': '30.026', 'grad_norm': '84.5', 'counters/examples': 181248, 'counters/updates': 5664}
train stats after 181280 examples: {'rewards_train/chosen': '0.13745', 'rewards_train/rejected': '0.026466', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11098', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-211.8', 'loss/train': '0.6478', 'examples_per_second': '32.738', 'grad_norm': '78.5', 'counters/examples': 181280, 'counters/updates': 5665}
train stats after 181312 examples: {'rewards_train/chosen': '0.0048251', 'rewards_train/rejected': '0.015807', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010982', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-131.45', 'loss/train': '0.71485', 'examples_per_second': '33.227', 'grad_norm': '113.5', 'counters/examples': 181312, 'counters/updates': 5666}
skipping logging after 181344 examples to avoid logging too frequently
train stats after 181376 examples: {'rewards_train/chosen': '0.050824', 'rewards_train/rejected': '0.045703', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0051207', 'logps_train/rejected': '-164.49', 'logps_train/chosen': '-126.51', 'loss/train': '0.70324', 'examples_per_second': '30.411', 'grad_norm': '80', 'counters/examples': 181376, 'counters/updates': 5668}
skipping logging after 181408 examples to avoid logging too frequently
train stats after 181440 examples: {'rewards_train/chosen': '0.11585', 'rewards_train/rejected': '-0.011727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12757', 'logps_train/rejected': '-148.11', 'logps_train/chosen': '-183.24', 'loss/train': '0.65105', 'examples_per_second': '34.814', 'grad_norm': '185', 'counters/examples': 181440, 'counters/updates': 5670}
train stats after 181472 examples: {'rewards_train/chosen': '0.13981', 'rewards_train/rejected': '0.10449', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035321', 'logps_train/rejected': '-141.62', 'logps_train/chosen': '-182.05', 'loss/train': '0.70079', 'examples_per_second': '30.583', 'grad_norm': '85', 'counters/examples': 181472, 'counters/updates': 5671}
train stats after 181504 examples: {'rewards_train/chosen': '0.088226', 'rewards_train/rejected': '0.049963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038263', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-141.99', 'loss/train': '0.68363', 'examples_per_second': '32.237', 'grad_norm': '102.5', 'counters/examples': 181504, 'counters/updates': 5672}
train stats after 181536 examples: {'rewards_train/chosen': '0.25416', 'rewards_train/rejected': '0.024333', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22983', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-183.39', 'loss/train': '0.60274', 'examples_per_second': '31.704', 'grad_norm': '56.75', 'counters/examples': 181536, 'counters/updates': 5673}
train stats after 181568 examples: {'rewards_train/chosen': '0.11902', 'rewards_train/rejected': '0.017473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10154', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-166.47', 'loss/train': '0.65355', 'examples_per_second': '31.122', 'grad_norm': '85', 'counters/examples': 181568, 'counters/updates': 5674}
train stats after 181600 examples: {'rewards_train/chosen': '0.13903', 'rewards_train/rejected': '0.088285', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050749', 'logps_train/rejected': '-166.66', 'logps_train/chosen': '-145.16', 'loss/train': '0.68246', 'examples_per_second': '30.595', 'grad_norm': '111', 'counters/examples': 181600, 'counters/updates': 5675}
train stats after 181632 examples: {'rewards_train/chosen': '0.17576', 'rewards_train/rejected': '0.060417', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11535', 'logps_train/rejected': '-103.25', 'logps_train/chosen': '-123.84', 'loss/train': '0.64664', 'examples_per_second': '32.731', 'grad_norm': '76.5', 'counters/examples': 181632, 'counters/updates': 5676}
train stats after 181664 examples: {'rewards_train/chosen': '0.12211', 'rewards_train/rejected': '0.071054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051061', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-166.14', 'loss/train': '0.70439', 'examples_per_second': '31.579', 'grad_norm': '125.5', 'counters/examples': 181664, 'counters/updates': 5677}
train stats after 181696 examples: {'rewards_train/chosen': '0.11566', 'rewards_train/rejected': '0.059853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055804', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-159.76', 'loss/train': '0.679', 'examples_per_second': '31.044', 'grad_norm': '104.5', 'counters/examples': 181696, 'counters/updates': 5678}
train stats after 181728 examples: {'rewards_train/chosen': '0.12527', 'rewards_train/rejected': '0.10955', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015726', 'logps_train/rejected': '-176.72', 'logps_train/chosen': '-191.05', 'loss/train': '0.70724', 'examples_per_second': '31.556', 'grad_norm': '83.5', 'counters/examples': 181728, 'counters/updates': 5679}
train stats after 181760 examples: {'rewards_train/chosen': '0.091689', 'rewards_train/rejected': '-0.015691', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10738', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-163.31', 'loss/train': '0.66637', 'examples_per_second': '31.579', 'grad_norm': '73.5', 'counters/examples': 181760, 'counters/updates': 5680}
skipping logging after 181792 examples to avoid logging too frequently
train stats after 181824 examples: {'rewards_train/chosen': '0.079062', 'rewards_train/rejected': '0.040396', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038666', 'logps_train/rejected': '-97.223', 'logps_train/chosen': '-117.88', 'loss/train': '0.68115', 'examples_per_second': '31.787', 'grad_norm': '268', 'counters/examples': 181824, 'counters/updates': 5682}
train stats after 181856 examples: {'rewards_train/chosen': '0.22654', 'rewards_train/rejected': '0.042116', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18442', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-163.21', 'loss/train': '0.61769', 'examples_per_second': '32.35', 'grad_norm': '65', 'counters/examples': 181856, 'counters/updates': 5683}
train stats after 181888 examples: {'rewards_train/chosen': '0.24041', 'rewards_train/rejected': '0.065519', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17489', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-172.81', 'loss/train': '0.6251', 'examples_per_second': '31.904', 'grad_norm': '97.5', 'counters/examples': 181888, 'counters/updates': 5684}
skipping logging after 181920 examples to avoid logging too frequently
train stats after 181952 examples: {'rewards_train/chosen': '0.074273', 'rewards_train/rejected': '0.064809', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0094639', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-109.56', 'loss/train': '0.70252', 'examples_per_second': '35.461', 'grad_norm': '60.5', 'counters/examples': 181952, 'counters/updates': 5686}
train stats after 181984 examples: {'rewards_train/chosen': '0.10654', 'rewards_train/rejected': '0.042422', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.064117', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-142.19', 'loss/train': '0.69311', 'examples_per_second': '31.589', 'grad_norm': '101', 'counters/examples': 181984, 'counters/updates': 5687}
skipping logging after 182016 examples to avoid logging too frequently
train stats after 182048 examples: {'rewards_train/chosen': '0.039598', 'rewards_train/rejected': '0.023229', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016369', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-140.5', 'loss/train': '0.69571', 'examples_per_second': '33.932', 'grad_norm': '103', 'counters/examples': 182048, 'counters/updates': 5689}
train stats after 182080 examples: {'rewards_train/chosen': '0.094113', 'rewards_train/rejected': '0.067154', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026959', 'logps_train/rejected': '-176.09', 'logps_train/chosen': '-168.73', 'loss/train': '0.69365', 'examples_per_second': '31.425', 'grad_norm': '164', 'counters/examples': 182080, 'counters/updates': 5690}
skipping logging after 182112 examples to avoid logging too frequently
train stats after 182144 examples: {'rewards_train/chosen': '0.048452', 'rewards_train/rejected': '0.055221', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0067688', 'logps_train/rejected': '-110.83', 'logps_train/chosen': '-149.72', 'loss/train': '0.73107', 'examples_per_second': '33.347', 'grad_norm': '96', 'counters/examples': 182144, 'counters/updates': 5692}
skipping logging after 182176 examples to avoid logging too frequently
train stats after 182208 examples: {'rewards_train/chosen': '0.1094', 'rewards_train/rejected': '0.12653', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017126', 'logps_train/rejected': '-161.5', 'logps_train/chosen': '-167', 'loss/train': '0.7225', 'examples_per_second': '31.279', 'grad_norm': '97.5', 'counters/examples': 182208, 'counters/updates': 5694}
skipping logging after 182240 examples to avoid logging too frequently
train stats after 182272 examples: {'rewards_train/chosen': '0.065323', 'rewards_train/rejected': '0.077451', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012128', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-165.66', 'loss/train': '0.71594', 'examples_per_second': '31.03', 'grad_norm': '96', 'counters/examples': 182272, 'counters/updates': 5696}
train stats after 182304 examples: {'rewards_train/chosen': '0.095137', 'rewards_train/rejected': '0.068996', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026141', 'logps_train/rejected': '-103.63', 'logps_train/chosen': '-142.67', 'loss/train': '0.68905', 'examples_per_second': '31.787', 'grad_norm': '118.5', 'counters/examples': 182304, 'counters/updates': 5697}
train stats after 182336 examples: {'rewards_train/chosen': '0.11638', 'rewards_train/rejected': '0.13012', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013742', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-169.32', 'loss/train': '0.71703', 'examples_per_second': '31.819', 'grad_norm': '74', 'counters/examples': 182336, 'counters/updates': 5698}
train stats after 182368 examples: {'rewards_train/chosen': '0.080434', 'rewards_train/rejected': '0.014192', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066242', 'logps_train/rejected': '-142.84', 'logps_train/chosen': '-143.45', 'loss/train': '0.66884', 'examples_per_second': '30.596', 'grad_norm': '75', 'counters/examples': 182368, 'counters/updates': 5699}
train stats after 182400 examples: {'rewards_train/chosen': '0.18506', 'rewards_train/rejected': '0.067836', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11723', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-179.06', 'loss/train': '0.6511', 'examples_per_second': '31.543', 'grad_norm': '81.5', 'counters/examples': 182400, 'counters/updates': 5700}
train stats after 182432 examples: {'rewards_train/chosen': '0.10107', 'rewards_train/rejected': '0.080269', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020796', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-119.65', 'loss/train': '0.69638', 'examples_per_second': '32.79', 'grad_norm': '53.25', 'counters/examples': 182432, 'counters/updates': 5701}
train stats after 182464 examples: {'rewards_train/chosen': '0.1066', 'rewards_train/rejected': '0.10056', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0060358', 'logps_train/rejected': '-115.62', 'logps_train/chosen': '-157.22', 'loss/train': '0.71165', 'examples_per_second': '31.383', 'grad_norm': '88', 'counters/examples': 182464, 'counters/updates': 5702}
train stats after 182496 examples: {'rewards_train/chosen': '0.075959', 'rewards_train/rejected': '0.07754', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0015806', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-132.87', 'loss/train': '0.70826', 'examples_per_second': '31.636', 'grad_norm': '205', 'counters/examples': 182496, 'counters/updates': 5703}
train stats after 182528 examples: {'rewards_train/chosen': '0.048782', 'rewards_train/rejected': '0.045847', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.002935', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-123.16', 'loss/train': '0.70519', 'examples_per_second': '31.411', 'grad_norm': '113', 'counters/examples': 182528, 'counters/updates': 5704}
skipping logging after 182560 examples to avoid logging too frequently
train stats after 182592 examples: {'rewards_train/chosen': '0.1109', 'rewards_train/rejected': '0.074637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036266', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-135.68', 'loss/train': '0.69494', 'examples_per_second': '33.037', 'grad_norm': '336', 'counters/examples': 182592, 'counters/updates': 5706}
train stats after 182624 examples: {'rewards_train/chosen': '0.073041', 'rewards_train/rejected': '0.089283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016242', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-134.6', 'loss/train': '0.71856', 'examples_per_second': '24.723', 'grad_norm': '120.5', 'counters/examples': 182624, 'counters/updates': 5707}
skipping logging after 182656 examples to avoid logging too frequently
train stats after 182688 examples: {'rewards_train/chosen': '0.017127', 'rewards_train/rejected': '0.0012274', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0159', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-142.64', 'loss/train': '0.69091', 'examples_per_second': '31.077', 'grad_norm': '69', 'counters/examples': 182688, 'counters/updates': 5709}
train stats after 182720 examples: {'rewards_train/chosen': '0.095529', 'rewards_train/rejected': '0.0064929', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089036', 'logps_train/rejected': '-147.87', 'logps_train/chosen': '-184.07', 'loss/train': '0.65951', 'examples_per_second': '30.575', 'grad_norm': '119.5', 'counters/examples': 182720, 'counters/updates': 5710}
train stats after 182752 examples: {'rewards_train/chosen': '0.10447', 'rewards_train/rejected': '0.076186', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028279', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-132.4', 'loss/train': '0.6907', 'examples_per_second': '31.605', 'grad_norm': '85', 'counters/examples': 182752, 'counters/updates': 5711}
train stats after 182784 examples: {'rewards_train/chosen': '0.11741', 'rewards_train/rejected': '0.047011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070398', 'logps_train/rejected': '-96.605', 'logps_train/chosen': '-151.3', 'loss/train': '0.66662', 'examples_per_second': '31.727', 'grad_norm': '141', 'counters/examples': 182784, 'counters/updates': 5712}
train stats after 182816 examples: {'rewards_train/chosen': '0.13892', 'rewards_train/rejected': '0.027427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1115', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-161.94', 'loss/train': '0.65167', 'examples_per_second': '30.524', 'grad_norm': '70.5', 'counters/examples': 182816, 'counters/updates': 5713}
train stats after 182848 examples: {'rewards_train/chosen': '0.096299', 'rewards_train/rejected': '0.0073397', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08896', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-125.38', 'loss/train': '0.65898', 'examples_per_second': '31.552', 'grad_norm': '85.5', 'counters/examples': 182848, 'counters/updates': 5714}
train stats after 182880 examples: {'rewards_train/chosen': '0.14856', 'rewards_train/rejected': '0.094673', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053888', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-137.66', 'loss/train': '0.67912', 'examples_per_second': '30.015', 'grad_norm': '80', 'counters/examples': 182880, 'counters/updates': 5715}
train stats after 182912 examples: {'rewards_train/chosen': '0.063267', 'rewards_train/rejected': '0.12212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.058858', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-158.17', 'loss/train': '0.74196', 'examples_per_second': '32.983', 'grad_norm': '88.5', 'counters/examples': 182912, 'counters/updates': 5716}
train stats after 182944 examples: {'rewards_train/chosen': '0.17828', 'rewards_train/rejected': '0.060484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1178', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-180.3', 'loss/train': '0.65192', 'examples_per_second': '30.145', 'grad_norm': '76.5', 'counters/examples': 182944, 'counters/updates': 5717}
train stats after 182976 examples: {'rewards_train/chosen': '0.14625', 'rewards_train/rejected': '0.068029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078218', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-149.09', 'loss/train': '0.66954', 'examples_per_second': '31.542', 'grad_norm': '57.25', 'counters/examples': 182976, 'counters/updates': 5718}
skipping logging after 183008 examples to avoid logging too frequently
train stats after 183040 examples: {'rewards_train/chosen': '0.16335', 'rewards_train/rejected': '0.13069', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032665', 'logps_train/rejected': '-121.11', 'logps_train/chosen': '-179.7', 'loss/train': '0.70281', 'examples_per_second': '34.607', 'grad_norm': '70', 'counters/examples': 183040, 'counters/updates': 5720}
skipping logging after 183072 examples to avoid logging too frequently
train stats after 183104 examples: {'rewards_train/chosen': '0.09991', 'rewards_train/rejected': '0.07566', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.02425', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-139.02', 'loss/train': '0.69172', 'examples_per_second': '32.499', 'grad_norm': '312', 'counters/examples': 183104, 'counters/updates': 5722}
train stats after 183136 examples: {'rewards_train/chosen': '0.10985', 'rewards_train/rejected': '0.017508', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.092343', 'logps_train/rejected': '-97.088', 'logps_train/chosen': '-129.78', 'loss/train': '0.65273', 'examples_per_second': '30.08', 'grad_norm': '78.5', 'counters/examples': 183136, 'counters/updates': 5723}
train stats after 183168 examples: {'rewards_train/chosen': '0.12848', 'rewards_train/rejected': '0.064893', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063591', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-182.54', 'loss/train': '0.68037', 'examples_per_second': '30.183', 'grad_norm': '94', 'counters/examples': 183168, 'counters/updates': 5724}
train stats after 183200 examples: {'rewards_train/chosen': '0.24087', 'rewards_train/rejected': '0.086549', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15432', 'logps_train/rejected': '-144.11', 'logps_train/chosen': '-165.88', 'loss/train': '0.6421', 'examples_per_second': '30.913', 'grad_norm': '67.5', 'counters/examples': 183200, 'counters/updates': 5725}
train stats after 183232 examples: {'rewards_train/chosen': '0.029371', 'rewards_train/rejected': '0.085444', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.056073', 'logps_train/rejected': '-136.98', 'logps_train/chosen': '-166.12', 'loss/train': '0.73297', 'examples_per_second': '31.646', 'grad_norm': '81.5', 'counters/examples': 183232, 'counters/updates': 5726}
train stats after 183264 examples: {'rewards_train/chosen': '0.13882', 'rewards_train/rejected': '-0.063645', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20246', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-150.37', 'loss/train': '0.61262', 'examples_per_second': '31.626', 'grad_norm': '81', 'counters/examples': 183264, 'counters/updates': 5727}
train stats after 183296 examples: {'rewards_train/chosen': '0.088935', 'rewards_train/rejected': '0.072998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015937', 'logps_train/rejected': '-133.33', 'logps_train/chosen': '-156.74', 'loss/train': '0.69629', 'examples_per_second': '31.559', 'grad_norm': '70', 'counters/examples': 183296, 'counters/updates': 5728}
skipping logging after 183328 examples to avoid logging too frequently
train stats after 183360 examples: {'rewards_train/chosen': '0.14709', 'rewards_train/rejected': '0.026184', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12091', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-165.08', 'loss/train': '0.6554', 'examples_per_second': '32.131', 'grad_norm': '76.5', 'counters/examples': 183360, 'counters/updates': 5730}
train stats after 183392 examples: {'rewards_train/chosen': '0.031275', 'rewards_train/rejected': '0.083829', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.052554', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-139.76', 'loss/train': '0.75863', 'examples_per_second': '31.675', 'grad_norm': '142', 'counters/examples': 183392, 'counters/updates': 5731}
train stats after 183424 examples: {'rewards_train/chosen': '0.010378', 'rewards_train/rejected': '0.05622', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045842', 'logps_train/rejected': '-129.62', 'logps_train/chosen': '-188.51', 'loss/train': '0.75053', 'examples_per_second': '31.19', 'grad_norm': '1000', 'counters/examples': 183424, 'counters/updates': 5732}
train stats after 183456 examples: {'rewards_train/chosen': '0.10485', 'rewards_train/rejected': '-0.01348', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11832', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-182.68', 'loss/train': '0.64782', 'examples_per_second': '32.504', 'grad_norm': '98', 'counters/examples': 183456, 'counters/updates': 5733}
train stats after 183488 examples: {'rewards_train/chosen': '0.034207', 'rewards_train/rejected': '-0.013367', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047574', 'logps_train/rejected': '-145.52', 'logps_train/chosen': '-176.51', 'loss/train': '0.69231', 'examples_per_second': '31.968', 'grad_norm': '260', 'counters/examples': 183488, 'counters/updates': 5734}
train stats after 183520 examples: {'rewards_train/chosen': '0.19341', 'rewards_train/rejected': '0.14414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049262', 'logps_train/rejected': '-149.47', 'logps_train/chosen': '-187.94', 'loss/train': '0.68101', 'examples_per_second': '31.272', 'grad_norm': '71', 'counters/examples': 183520, 'counters/updates': 5735}
skipping logging after 183552 examples to avoid logging too frequently
train stats after 183584 examples: {'rewards_train/chosen': '0.10054', 'rewards_train/rejected': '0.024831', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075712', 'logps_train/rejected': '-136.1', 'logps_train/chosen': '-168.16', 'loss/train': '0.66603', 'examples_per_second': '31.654', 'grad_norm': '77', 'counters/examples': 183584, 'counters/updates': 5737}
train stats after 183616 examples: {'rewards_train/chosen': '0.094608', 'rewards_train/rejected': '-0.031159', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12577', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-167.88', 'loss/train': '0.64215', 'examples_per_second': '31.659', 'grad_norm': '61', 'counters/examples': 183616, 'counters/updates': 5738}
train stats after 183648 examples: {'rewards_train/chosen': '0.16566', 'rewards_train/rejected': '0.058049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10762', 'logps_train/rejected': '-116.1', 'logps_train/chosen': '-155.03', 'loss/train': '0.65324', 'examples_per_second': '31.616', 'grad_norm': '64', 'counters/examples': 183648, 'counters/updates': 5739}
train stats after 183680 examples: {'rewards_train/chosen': '0.15098', 'rewards_train/rejected': '0.038717', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11226', 'logps_train/rejected': '-130.81', 'logps_train/chosen': '-150.64', 'loss/train': '0.66311', 'examples_per_second': '31.638', 'grad_norm': '86.5', 'counters/examples': 183680, 'counters/updates': 5740}
train stats after 183712 examples: {'rewards_train/chosen': '0.13571', 'rewards_train/rejected': '0.12997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0057375', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-125.95', 'loss/train': '0.71644', 'examples_per_second': '30.484', 'grad_norm': '150', 'counters/examples': 183712, 'counters/updates': 5741}
train stats after 183744 examples: {'rewards_train/chosen': '0.11952', 'rewards_train/rejected': '0.072544', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04698', 'logps_train/rejected': '-153.23', 'logps_train/chosen': '-130.83', 'loss/train': '0.68503', 'examples_per_second': '31.817', 'grad_norm': '134', 'counters/examples': 183744, 'counters/updates': 5742}
skipping logging after 183776 examples to avoid logging too frequently
train stats after 183808 examples: {'rewards_train/chosen': '0.10474', 'rewards_train/rejected': '0.049996', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054745', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-119.55', 'loss/train': '0.68271', 'examples_per_second': '32.69', 'grad_norm': '69.5', 'counters/examples': 183808, 'counters/updates': 5744}
train stats after 183840 examples: {'rewards_train/chosen': '0.058105', 'rewards_train/rejected': '-0.017379', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075484', 'logps_train/rejected': '-166.61', 'logps_train/chosen': '-175.63', 'loss/train': '0.66396', 'examples_per_second': '31.34', 'grad_norm': '86.5', 'counters/examples': 183840, 'counters/updates': 5745}
train stats after 183872 examples: {'rewards_train/chosen': '0.13522', 'rewards_train/rejected': '0.024434', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11078', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-124.64', 'loss/train': '0.65033', 'examples_per_second': '31.998', 'grad_norm': '54.25', 'counters/examples': 183872, 'counters/updates': 5746}
train stats after 183904 examples: {'rewards_train/chosen': '0.14186', 'rewards_train/rejected': '-0.00081617', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14268', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-172.85', 'loss/train': '0.63573', 'examples_per_second': '30.644', 'grad_norm': '64', 'counters/examples': 183904, 'counters/updates': 5747}
train stats after 183936 examples: {'rewards_train/chosen': '0.057808', 'rewards_train/rejected': '0.083129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025321', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-171.56', 'loss/train': '0.72895', 'examples_per_second': '32.174', 'grad_norm': '79.5', 'counters/examples': 183936, 'counters/updates': 5748}
skipping logging after 183968 examples to avoid logging too frequently
train stats after 184000 examples: {'rewards_train/chosen': '0.10766', 'rewards_train/rejected': '-0.054802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16246', 'logps_train/rejected': '-142.29', 'logps_train/chosen': '-133.21', 'loss/train': '0.64274', 'examples_per_second': '30.176', 'grad_norm': '141', 'counters/examples': 184000, 'counters/updates': 5750}
train stats after 184032 examples: {'rewards_train/chosen': '0.10571', 'rewards_train/rejected': '0.030596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075118', 'logps_train/rejected': '-154.23', 'logps_train/chosen': '-190.19', 'loss/train': '0.67403', 'examples_per_second': '30.833', 'grad_norm': '101', 'counters/examples': 184032, 'counters/updates': 5751}
train stats after 184064 examples: {'rewards_train/chosen': '0.11934', 'rewards_train/rejected': '0.096002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023333', 'logps_train/rejected': '-156.66', 'logps_train/chosen': '-158.06', 'loss/train': '0.70028', 'examples_per_second': '30.118', 'grad_norm': '103', 'counters/examples': 184064, 'counters/updates': 5752}
train stats after 184096 examples: {'rewards_train/chosen': '0.1413', 'rewards_train/rejected': '0.042368', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098929', 'logps_train/rejected': '-161.91', 'logps_train/chosen': '-207.41', 'loss/train': '0.66189', 'examples_per_second': '31.614', 'grad_norm': '92.5', 'counters/examples': 184096, 'counters/updates': 5753}
train stats after 184128 examples: {'rewards_train/chosen': '0.11817', 'rewards_train/rejected': '0.03562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082554', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-139.16', 'loss/train': '0.66871', 'examples_per_second': '31.34', 'grad_norm': '202', 'counters/examples': 184128, 'counters/updates': 5754}
train stats after 184160 examples: {'rewards_train/chosen': '0.12234', 'rewards_train/rejected': '0.073978', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048359', 'logps_train/rejected': '-150.68', 'logps_train/chosen': '-129.85', 'loss/train': '0.69153', 'examples_per_second': '31.408', 'grad_norm': '103', 'counters/examples': 184160, 'counters/updates': 5755}
train stats after 184192 examples: {'rewards_train/chosen': '0.093052', 'rewards_train/rejected': '0.048609', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044443', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-175.24', 'loss/train': '0.6866', 'examples_per_second': '31.805', 'grad_norm': '129', 'counters/examples': 184192, 'counters/updates': 5756}
skipping logging after 184224 examples to avoid logging too frequently
train stats after 184256 examples: {'rewards_train/chosen': '0.14769', 'rewards_train/rejected': '0.0032859', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1444', 'logps_train/rejected': '-178.18', 'logps_train/chosen': '-154.35', 'loss/train': '0.64677', 'examples_per_second': '31.12', 'grad_norm': '60', 'counters/examples': 184256, 'counters/updates': 5758}
train stats after 184288 examples: {'rewards_train/chosen': '0.094095', 'rewards_train/rejected': '0.15339', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.059297', 'logps_train/rejected': '-184.69', 'logps_train/chosen': '-142.39', 'loss/train': '0.73962', 'examples_per_second': '32.184', 'grad_norm': '97.5', 'counters/examples': 184288, 'counters/updates': 5759}
skipping logging after 184320 examples to avoid logging too frequently
train stats after 184352 examples: {'rewards_train/chosen': '0.182', 'rewards_train/rejected': '0.086581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.095421', 'logps_train/rejected': '-124.39', 'logps_train/chosen': '-131.84', 'loss/train': '0.66521', 'examples_per_second': '29.774', 'grad_norm': '92', 'counters/examples': 184352, 'counters/updates': 5761}
skipping logging after 184384 examples to avoid logging too frequently
train stats after 184416 examples: {'rewards_train/chosen': '0.075508', 'rewards_train/rejected': '0.10798', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032468', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-130.53', 'loss/train': '0.72641', 'examples_per_second': '36.278', 'grad_norm': '58.5', 'counters/examples': 184416, 'counters/updates': 5763}
train stats after 184448 examples: {'rewards_train/chosen': '0.065322', 'rewards_train/rejected': '0.062234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0030882', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-142.49', 'loss/train': '0.70277', 'examples_per_second': '31.843', 'grad_norm': '64', 'counters/examples': 184448, 'counters/updates': 5764}
skipping logging after 184480 examples to avoid logging too frequently
train stats after 184512 examples: {'rewards_train/chosen': '0.17914', 'rewards_train/rejected': '0.0897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089437', 'logps_train/rejected': '-108.46', 'logps_train/chosen': '-157.43', 'loss/train': '0.66063', 'examples_per_second': '30.284', 'grad_norm': '104', 'counters/examples': 184512, 'counters/updates': 5766}
train stats after 184544 examples: {'rewards_train/chosen': '0.081849', 'rewards_train/rejected': '0.04696', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034889', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-133.8', 'loss/train': '0.69907', 'examples_per_second': '31.669', 'grad_norm': '62.75', 'counters/examples': 184544, 'counters/updates': 5767}
train stats after 184576 examples: {'rewards_train/chosen': '0.087623', 'rewards_train/rejected': '0.03445', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053173', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-159.78', 'loss/train': '0.67943', 'examples_per_second': '30.682', 'grad_norm': '106', 'counters/examples': 184576, 'counters/updates': 5768}
train stats after 184608 examples: {'rewards_train/chosen': '0.12542', 'rewards_train/rejected': '0.16208', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.036664', 'logps_train/rejected': '-150.27', 'logps_train/chosen': '-167.79', 'loss/train': '0.74674', 'examples_per_second': '31.667', 'grad_norm': '132', 'counters/examples': 184608, 'counters/updates': 5769}
skipping logging after 184640 examples to avoid logging too frequently
train stats after 184672 examples: {'rewards_train/chosen': '0.12406', 'rewards_train/rejected': '0.03512', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088945', 'logps_train/rejected': '-168.55', 'logps_train/chosen': '-159.36', 'loss/train': '0.66262', 'examples_per_second': '30.203', 'grad_norm': '97', 'counters/examples': 184672, 'counters/updates': 5771}
train stats after 184704 examples: {'rewards_train/chosen': '0.07548', 'rewards_train/rejected': '-0.014712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090192', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-120.65', 'loss/train': '0.66005', 'examples_per_second': '30.342', 'grad_norm': '122.5', 'counters/examples': 184704, 'counters/updates': 5772}
skipping logging after 184736 examples to avoid logging too frequently
train stats after 184768 examples: {'rewards_train/chosen': '0.093713', 'rewards_train/rejected': '0.045947', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047766', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-145.47', 'loss/train': '0.68033', 'examples_per_second': '25.487', 'grad_norm': '233', 'counters/examples': 184768, 'counters/updates': 5774}
train stats after 184800 examples: {'rewards_train/chosen': '0.12082', 'rewards_train/rejected': '0.027129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093691', 'logps_train/rejected': '-138.19', 'logps_train/chosen': '-136.05', 'loss/train': '0.67231', 'examples_per_second': '32.713', 'grad_norm': '114.5', 'counters/examples': 184800, 'counters/updates': 5775}
train stats after 184832 examples: {'rewards_train/chosen': '0.13127', 'rewards_train/rejected': '0.023459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10781', 'logps_train/rejected': '-156.62', 'logps_train/chosen': '-153.71', 'loss/train': '0.65472', 'examples_per_second': '30.155', 'grad_norm': '79', 'counters/examples': 184832, 'counters/updates': 5776}
train stats after 184864 examples: {'rewards_train/chosen': '0.097486', 'rewards_train/rejected': '0.055953', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041532', 'logps_train/rejected': '-158.81', 'logps_train/chosen': '-154.17', 'loss/train': '0.6905', 'examples_per_second': '26.172', 'grad_norm': '56.5', 'counters/examples': 184864, 'counters/updates': 5777}
train stats after 184896 examples: {'rewards_train/chosen': '0.093784', 'rewards_train/rejected': '0.049356', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044428', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-175.59', 'loss/train': '0.68194', 'examples_per_second': '30.949', 'grad_norm': '108', 'counters/examples': 184896, 'counters/updates': 5778}
train stats after 184928 examples: {'rewards_train/chosen': '0.075695', 'rewards_train/rejected': '0.028824', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046871', 'logps_train/rejected': '-119.12', 'logps_train/chosen': '-134.23', 'loss/train': '0.69332', 'examples_per_second': '31.116', 'grad_norm': '115.5', 'counters/examples': 184928, 'counters/updates': 5779}
skipping logging after 184960 examples to avoid logging too frequently
train stats after 184992 examples: {'rewards_train/chosen': '0.11661', 'rewards_train/rejected': '0.11059', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0060114', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-176.45', 'loss/train': '0.69981', 'examples_per_second': '31.819', 'grad_norm': '101', 'counters/examples': 184992, 'counters/updates': 5781}
skipping logging after 185024 examples to avoid logging too frequently
train stats after 185056 examples: {'rewards_train/chosen': '0.15667', 'rewards_train/rejected': '0.029706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12696', 'logps_train/rejected': '-151.47', 'logps_train/chosen': '-141.4', 'loss/train': '0.64868', 'examples_per_second': '31.308', 'grad_norm': '128', 'counters/examples': 185056, 'counters/updates': 5783}
skipping logging after 185088 examples to avoid logging too frequently
train stats after 185120 examples: {'rewards_train/chosen': '-0.0070521', 'rewards_train/rejected': '-0.025269', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018217', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-167.27', 'loss/train': '0.69504', 'examples_per_second': '30.448', 'grad_norm': '81.5', 'counters/examples': 185120, 'counters/updates': 5785}
skipping logging after 185152 examples to avoid logging too frequently
train stats after 185184 examples: {'rewards_train/chosen': '0.12752', 'rewards_train/rejected': '0.079934', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047588', 'logps_train/rejected': '-124.11', 'logps_train/chosen': '-136.44', 'loss/train': '0.67968', 'examples_per_second': '30.636', 'grad_norm': '175', 'counters/examples': 185184, 'counters/updates': 5787}
train stats after 185216 examples: {'rewards_train/chosen': '0.054578', 'rewards_train/rejected': '0.069264', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014686', 'logps_train/rejected': '-136.84', 'logps_train/chosen': '-150.72', 'loss/train': '0.72623', 'examples_per_second': '31.251', 'grad_norm': '137', 'counters/examples': 185216, 'counters/updates': 5788}
train stats after 185248 examples: {'rewards_train/chosen': '0.11704', 'rewards_train/rejected': '0.011371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10567', 'logps_train/rejected': '-126.43', 'logps_train/chosen': '-144.59', 'loss/train': '0.64929', 'examples_per_second': '31.04', 'grad_norm': '57.5', 'counters/examples': 185248, 'counters/updates': 5789}
skipping logging after 185280 examples to avoid logging too frequently
train stats after 185312 examples: {'rewards_train/chosen': '0.11461', 'rewards_train/rejected': '0.073779', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040829', 'logps_train/rejected': '-149.72', 'logps_train/chosen': '-157.17', 'loss/train': '0.68591', 'examples_per_second': '32.085', 'grad_norm': '79', 'counters/examples': 185312, 'counters/updates': 5791}
train stats after 185344 examples: {'rewards_train/chosen': '0.11468', 'rewards_train/rejected': '0.089602', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025081', 'logps_train/rejected': '-158.06', 'logps_train/chosen': '-137.09', 'loss/train': '0.69276', 'examples_per_second': '30.285', 'grad_norm': '81', 'counters/examples': 185344, 'counters/updates': 5792}
skipping logging after 185376 examples to avoid logging too frequently
train stats after 185408 examples: {'rewards_train/chosen': '0.145', 'rewards_train/rejected': '0.15998', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.014982', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-143.08', 'loss/train': '0.73093', 'examples_per_second': '31.372', 'grad_norm': '88.5', 'counters/examples': 185408, 'counters/updates': 5794}
train stats after 185440 examples: {'rewards_train/chosen': '0.13271', 'rewards_train/rejected': '0.00050446', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1322', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-179.04', 'loss/train': '0.65026', 'examples_per_second': '30.126', 'grad_norm': '88', 'counters/examples': 185440, 'counters/updates': 5795}
skipping logging after 185472 examples to avoid logging too frequently
train stats after 185504 examples: {'rewards_train/chosen': '0.17422', 'rewards_train/rejected': '0.097036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077188', 'logps_train/rejected': '-134.01', 'logps_train/chosen': '-176.63', 'loss/train': '0.66544', 'examples_per_second': '31.822', 'grad_norm': '352', 'counters/examples': 185504, 'counters/updates': 5797}
train stats after 185536 examples: {'rewards_train/chosen': '0.054384', 'rewards_train/rejected': '0.065978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011594', 'logps_train/rejected': '-158.14', 'logps_train/chosen': '-161.26', 'loss/train': '0.72434', 'examples_per_second': '32.006', 'grad_norm': '75.5', 'counters/examples': 185536, 'counters/updates': 5798}
train stats after 185568 examples: {'rewards_train/chosen': '0.097737', 'rewards_train/rejected': '0.077587', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020149', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-150.57', 'loss/train': '0.69838', 'examples_per_second': '31.026', 'grad_norm': '102', 'counters/examples': 185568, 'counters/updates': 5799}
skipping logging after 185600 examples to avoid logging too frequently
train stats after 185632 examples: {'rewards_train/chosen': '0.056978', 'rewards_train/rejected': '-0.0089843', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065963', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-152.24', 'loss/train': '0.67317', 'examples_per_second': '33.504', 'grad_norm': '80.5', 'counters/examples': 185632, 'counters/updates': 5801}
skipping logging after 185664 examples to avoid logging too frequently
train stats after 185696 examples: {'rewards_train/chosen': '0.12481', 'rewards_train/rejected': '0.073338', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051475', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-175.47', 'loss/train': '0.68169', 'examples_per_second': '34.716', 'grad_norm': '87', 'counters/examples': 185696, 'counters/updates': 5803}
train stats after 185728 examples: {'rewards_train/chosen': '0.13014', 'rewards_train/rejected': '0.066298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063842', 'logps_train/rejected': '-114.4', 'logps_train/chosen': '-151.55', 'loss/train': '0.67308', 'examples_per_second': '31.401', 'grad_norm': '106', 'counters/examples': 185728, 'counters/updates': 5804}
train stats after 185760 examples: {'rewards_train/chosen': '0.091553', 'rewards_train/rejected': '0.043588', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047965', 'logps_train/rejected': '-149.81', 'logps_train/chosen': '-174.9', 'loss/train': '0.67823', 'examples_per_second': '31.827', 'grad_norm': '94.5', 'counters/examples': 185760, 'counters/updates': 5805}
train stats after 185792 examples: {'rewards_train/chosen': '0.15518', 'rewards_train/rejected': '0.049937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10524', 'logps_train/rejected': '-137.93', 'logps_train/chosen': '-148.16', 'loss/train': '0.65331', 'examples_per_second': '31.58', 'grad_norm': '82', 'counters/examples': 185792, 'counters/updates': 5806}
train stats after 185824 examples: {'rewards_train/chosen': '0.19233', 'rewards_train/rejected': '0.092611', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099716', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-120.73', 'loss/train': '0.65937', 'examples_per_second': '30.636', 'grad_norm': '54.75', 'counters/examples': 185824, 'counters/updates': 5807}
train stats after 185856 examples: {'rewards_train/chosen': '0.067639', 'rewards_train/rejected': '-0.0084653', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076104', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-182.77', 'loss/train': '0.66962', 'examples_per_second': '30.097', 'grad_norm': '127', 'counters/examples': 185856, 'counters/updates': 5808}
train stats after 185888 examples: {'rewards_train/chosen': '0.16793', 'rewards_train/rejected': '0.0065563', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16138', 'logps_train/rejected': '-111.87', 'logps_train/chosen': '-163.86', 'loss/train': '0.63364', 'examples_per_second': '31.489', 'grad_norm': '104.5', 'counters/examples': 185888, 'counters/updates': 5809}
train stats after 185920 examples: {'rewards_train/chosen': '0.078958', 'rewards_train/rejected': '0.001607', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077351', 'logps_train/rejected': '-152.07', 'logps_train/chosen': '-121.45', 'loss/train': '0.67157', 'examples_per_second': '31.508', 'grad_norm': '55.75', 'counters/examples': 185920, 'counters/updates': 5810}
train stats after 185952 examples: {'rewards_train/chosen': '0.12538', 'rewards_train/rejected': '0.040877', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084508', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-169.47', 'loss/train': '0.66145', 'examples_per_second': '30.299', 'grad_norm': '99', 'counters/examples': 185952, 'counters/updates': 5811}
train stats after 185984 examples: {'rewards_train/chosen': '0.025993', 'rewards_train/rejected': '0.11398', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.08799', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-165.79', 'loss/train': '0.76091', 'examples_per_second': '33.057', 'grad_norm': '75', 'counters/examples': 185984, 'counters/updates': 5812}
train stats after 186016 examples: {'rewards_train/chosen': '0.064856', 'rewards_train/rejected': '-0.0090738', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07393', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-214.98', 'loss/train': '0.67824', 'examples_per_second': '31.664', 'grad_norm': '76', 'counters/examples': 186016, 'counters/updates': 5813}
train stats after 186048 examples: {'rewards_train/chosen': '0.1569', 'rewards_train/rejected': '0.016127', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14077', 'logps_train/rejected': '-153.06', 'logps_train/chosen': '-164.67', 'loss/train': '0.64593', 'examples_per_second': '31.631', 'grad_norm': '188', 'counters/examples': 186048, 'counters/updates': 5814}
train stats after 186080 examples: {'rewards_train/chosen': '0.041906', 'rewards_train/rejected': '-0.02084', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062746', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-176.24', 'loss/train': '0.68831', 'examples_per_second': '32.567', 'grad_norm': '82', 'counters/examples': 186080, 'counters/updates': 5815}
skipping logging after 186112 examples to avoid logging too frequently
train stats after 186144 examples: {'rewards_train/chosen': '0.064151', 'rewards_train/rejected': '0.042407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021744', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-108.21', 'loss/train': '0.69336', 'examples_per_second': '38.423', 'grad_norm': '55.75', 'counters/examples': 186144, 'counters/updates': 5817}
train stats after 186176 examples: {'rewards_train/chosen': '0.061078', 'rewards_train/rejected': '0.05041', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010667', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-129.88', 'loss/train': '0.69942', 'examples_per_second': '30.407', 'grad_norm': '260', 'counters/examples': 186176, 'counters/updates': 5818}
train stats after 186208 examples: {'rewards_train/chosen': '0.17967', 'rewards_train/rejected': '0.024882', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15479', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-144.97', 'loss/train': '0.63512', 'examples_per_second': '32.116', 'grad_norm': '58.25', 'counters/examples': 186208, 'counters/updates': 5819}
train stats after 186240 examples: {'rewards_train/chosen': '0.16357', 'rewards_train/rejected': '0.14362', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01995', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-129.16', 'loss/train': '0.69995', 'examples_per_second': '31.586', 'grad_norm': '136', 'counters/examples': 186240, 'counters/updates': 5820}
train stats after 186272 examples: {'rewards_train/chosen': '0.10472', 'rewards_train/rejected': '0.039213', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065508', 'logps_train/rejected': '-150.19', 'logps_train/chosen': '-162.49', 'loss/train': '0.676', 'examples_per_second': '32.188', 'grad_norm': '77.5', 'counters/examples': 186272, 'counters/updates': 5821}
skipping logging after 186304 examples to avoid logging too frequently
train stats after 186336 examples: {'rewards_train/chosen': '0.071405', 'rewards_train/rejected': '0.126', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.05459', 'logps_train/rejected': '-114.67', 'logps_train/chosen': '-172.05', 'loss/train': '0.73255', 'examples_per_second': '32.601', 'grad_norm': '108', 'counters/examples': 186336, 'counters/updates': 5823}
train stats after 186368 examples: {'rewards_train/chosen': '0.16567', 'rewards_train/rejected': '0.084869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080804', 'logps_train/rejected': '-108.29', 'logps_train/chosen': '-176.51', 'loss/train': '0.66313', 'examples_per_second': '31.618', 'grad_norm': '82', 'counters/examples': 186368, 'counters/updates': 5824}
train stats after 186400 examples: {'rewards_train/chosen': '-0.011193', 'rewards_train/rejected': '0.028418', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.039611', 'logps_train/rejected': '-91.412', 'logps_train/chosen': '-107.25', 'loss/train': '0.73248', 'examples_per_second': '32.196', 'grad_norm': '112.5', 'counters/examples': 186400, 'counters/updates': 5825}
train stats after 186432 examples: {'rewards_train/chosen': '0.10547', 'rewards_train/rejected': '0.066248', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039218', 'logps_train/rejected': '-138.4', 'logps_train/chosen': '-172.31', 'loss/train': '0.69603', 'examples_per_second': '32.044', 'grad_norm': '70.5', 'counters/examples': 186432, 'counters/updates': 5826}
train stats after 186464 examples: {'rewards_train/chosen': '-0.01046', 'rewards_train/rejected': '0.025589', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036048', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-149.74', 'loss/train': '0.72301', 'examples_per_second': '31.604', 'grad_norm': '113', 'counters/examples': 186464, 'counters/updates': 5827}
train stats after 186496 examples: {'rewards_train/chosen': '0.14745', 'rewards_train/rejected': '-0.0048764', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15233', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-212.95', 'loss/train': '0.63516', 'examples_per_second': '30.546', 'grad_norm': '79', 'counters/examples': 186496, 'counters/updates': 5828}
train stats after 186528 examples: {'rewards_train/chosen': '0.019747', 'rewards_train/rejected': '0.016667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0030801', 'logps_train/rejected': '-157.5', 'logps_train/chosen': '-160.33', 'loss/train': '0.7023', 'examples_per_second': '31.488', 'grad_norm': '207', 'counters/examples': 186528, 'counters/updates': 5829}
train stats after 186560 examples: {'rewards_train/chosen': '0.043756', 'rewards_train/rejected': '0.019092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024664', 'logps_train/rejected': '-108.51', 'logps_train/chosen': '-135.89', 'loss/train': '0.68684', 'examples_per_second': '32.581', 'grad_norm': '78.5', 'counters/examples': 186560, 'counters/updates': 5830}
train stats after 186592 examples: {'rewards_train/chosen': '0.15322', 'rewards_train/rejected': '-0.012419', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16564', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-163.13', 'loss/train': '0.63021', 'examples_per_second': '30.205', 'grad_norm': '137', 'counters/examples': 186592, 'counters/updates': 5831}
train stats after 186624 examples: {'rewards_train/chosen': '0.13575', 'rewards_train/rejected': '0.08562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050133', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-167.38', 'loss/train': '0.67774', 'examples_per_second': '31.673', 'grad_norm': '167', 'counters/examples': 186624, 'counters/updates': 5832}
train stats after 186656 examples: {'rewards_train/chosen': '0.099183', 'rewards_train/rejected': '-0.0175', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11668', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-150.64', 'loss/train': '0.64769', 'examples_per_second': '31.459', 'grad_norm': '70', 'counters/examples': 186656, 'counters/updates': 5833}
train stats after 186688 examples: {'rewards_train/chosen': '0.14363', 'rewards_train/rejected': '0.097085', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.046541', 'logps_train/rejected': '-141.54', 'logps_train/chosen': '-160.1', 'loss/train': '0.68592', 'examples_per_second': '30.396', 'grad_norm': '75.5', 'counters/examples': 186688, 'counters/updates': 5834}
train stats after 186720 examples: {'rewards_train/chosen': '0.046649', 'rewards_train/rejected': '-0.0056318', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052281', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-122.19', 'loss/train': '0.67806', 'examples_per_second': '31.514', 'grad_norm': '61.5', 'counters/examples': 186720, 'counters/updates': 5835}
train stats after 186752 examples: {'rewards_train/chosen': '0.11835', 'rewards_train/rejected': '0.080623', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037724', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-172.23', 'loss/train': '0.68807', 'examples_per_second': '32.167', 'grad_norm': '67', 'counters/examples': 186752, 'counters/updates': 5836}
train stats after 186784 examples: {'rewards_train/chosen': '0.19978', 'rewards_train/rejected': '0.084767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11501', 'logps_train/rejected': '-141.92', 'logps_train/chosen': '-196.44', 'loss/train': '0.66702', 'examples_per_second': '31.386', 'grad_norm': '82.5', 'counters/examples': 186784, 'counters/updates': 5837}
train stats after 186816 examples: {'rewards_train/chosen': '0.13508', 'rewards_train/rejected': '0.081071', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054008', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-166.42', 'loss/train': '0.68473', 'examples_per_second': '31.854', 'grad_norm': '85', 'counters/examples': 186816, 'counters/updates': 5838}
train stats after 186848 examples: {'rewards_train/chosen': '0.12075', 'rewards_train/rejected': '0.065512', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055242', 'logps_train/rejected': '-116.34', 'logps_train/chosen': '-129.96', 'loss/train': '0.68993', 'examples_per_second': '29.999', 'grad_norm': '150', 'counters/examples': 186848, 'counters/updates': 5839}
train stats after 186880 examples: {'rewards_train/chosen': '0.061906', 'rewards_train/rejected': '0.11386', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.051951', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-127.78', 'loss/train': '0.75657', 'examples_per_second': '32.986', 'grad_norm': '344', 'counters/examples': 186880, 'counters/updates': 5840}
train stats after 186912 examples: {'rewards_train/chosen': '0.11535', 'rewards_train/rejected': '0.12608', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010732', 'logps_train/rejected': '-145.3', 'logps_train/chosen': '-175.56', 'loss/train': '0.72043', 'examples_per_second': '30.929', 'grad_norm': '117', 'counters/examples': 186912, 'counters/updates': 5841}
skipping logging after 186944 examples to avoid logging too frequently
train stats after 186976 examples: {'rewards_train/chosen': '0.014288', 'rewards_train/rejected': '0.059025', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044737', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-139.77', 'loss/train': '0.72834', 'examples_per_second': '33.919', 'grad_norm': '118', 'counters/examples': 186976, 'counters/updates': 5843}
train stats after 187008 examples: {'rewards_train/chosen': '0.054628', 'rewards_train/rejected': '0.037594', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017034', 'logps_train/rejected': '-139.56', 'logps_train/chosen': '-134.4', 'loss/train': '0.70341', 'examples_per_second': '30.45', 'grad_norm': '72.5', 'counters/examples': 187008, 'counters/updates': 5844}
train stats after 187040 examples: {'rewards_train/chosen': '0.055733', 'rewards_train/rejected': '-0.013603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069336', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-150.91', 'loss/train': '0.67101', 'examples_per_second': '31.58', 'grad_norm': '104.5', 'counters/examples': 187040, 'counters/updates': 5845}
train stats after 187072 examples: {'rewards_train/chosen': '0.15354', 'rewards_train/rejected': '0.024315', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12923', 'logps_train/rejected': '-93.028', 'logps_train/chosen': '-134.42', 'loss/train': '0.64385', 'examples_per_second': '32.81', 'grad_norm': '66.5', 'counters/examples': 187072, 'counters/updates': 5846}
train stats after 187104 examples: {'rewards_train/chosen': '0.1022', 'rewards_train/rejected': '0.095787', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0064092', 'logps_train/rejected': '-100.66', 'logps_train/chosen': '-149.44', 'loss/train': '0.70242', 'examples_per_second': '30.119', 'grad_norm': '57.75', 'counters/examples': 187104, 'counters/updates': 5847}
train stats after 187136 examples: {'rewards_train/chosen': '0.14381', 'rewards_train/rejected': '0.028045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11577', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-148', 'loss/train': '0.65437', 'examples_per_second': '30.739', 'grad_norm': '74.5', 'counters/examples': 187136, 'counters/updates': 5848}
train stats after 187168 examples: {'rewards_train/chosen': '0.15068', 'rewards_train/rejected': '0.026568', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12411', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-139.34', 'loss/train': '0.64437', 'examples_per_second': '31.64', 'grad_norm': '69.5', 'counters/examples': 187168, 'counters/updates': 5849}
train stats after 187200 examples: {'rewards_train/chosen': '0.17271', 'rewards_train/rejected': '0.046923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12579', 'logps_train/rejected': '-131.9', 'logps_train/chosen': '-153.36', 'loss/train': '0.64631', 'examples_per_second': '31.935', 'grad_norm': '88.5', 'counters/examples': 187200, 'counters/updates': 5850}
train stats after 187232 examples: {'rewards_train/chosen': '0.06586', 'rewards_train/rejected': '-0.00048596', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066346', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-134.9', 'loss/train': '0.67809', 'examples_per_second': '31.251', 'grad_norm': '80', 'counters/examples': 187232, 'counters/updates': 5851}
skipping logging after 187264 examples to avoid logging too frequently
train stats after 187296 examples: {'rewards_train/chosen': '0.13813', 'rewards_train/rejected': '0.056696', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081435', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-155.91', 'loss/train': '0.66685', 'examples_per_second': '35.81', 'grad_norm': '101.5', 'counters/examples': 187296, 'counters/updates': 5853}
train stats after 187328 examples: {'rewards_train/chosen': '0.20119', 'rewards_train/rejected': '-0.035597', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23679', 'logps_train/rejected': '-120.1', 'logps_train/chosen': '-145.42', 'loss/train': '0.60169', 'examples_per_second': '31.471', 'grad_norm': '63.25', 'counters/examples': 187328, 'counters/updates': 5854}
train stats after 187360 examples: {'rewards_train/chosen': '0.14937', 'rewards_train/rejected': '0.0017638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1476', 'logps_train/rejected': '-118.92', 'logps_train/chosen': '-150.62', 'loss/train': '0.63624', 'examples_per_second': '31.578', 'grad_norm': '83', 'counters/examples': 187360, 'counters/updates': 5855}
train stats after 187392 examples: {'rewards_train/chosen': '0.15607', 'rewards_train/rejected': '-0.024719', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18079', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-154.93', 'loss/train': '0.61924', 'examples_per_second': '32.768', 'grad_norm': '52.75', 'counters/examples': 187392, 'counters/updates': 5856}
train stats after 187424 examples: {'rewards_train/chosen': '0.03792', 'rewards_train/rejected': '0.083714', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.045794', 'logps_train/rejected': '-148.58', 'logps_train/chosen': '-162.22', 'loss/train': '0.73336', 'examples_per_second': '32.031', 'grad_norm': '276', 'counters/examples': 187424, 'counters/updates': 5857}
train stats after 187456 examples: {'rewards_train/chosen': '0.12521', 'rewards_train/rejected': '0.089542', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035664', 'logps_train/rejected': '-111.98', 'logps_train/chosen': '-147.23', 'loss/train': '0.68797', 'examples_per_second': '30.121', 'grad_norm': '69', 'counters/examples': 187456, 'counters/updates': 5858}
train stats after 187488 examples: {'rewards_train/chosen': '0.089654', 'rewards_train/rejected': '0.062008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.027646', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-142.84', 'loss/train': '0.68823', 'examples_per_second': '30.549', 'grad_norm': '67', 'counters/examples': 187488, 'counters/updates': 5859}
train stats after 187520 examples: {'rewards_train/chosen': '0.079948', 'rewards_train/rejected': '0.11147', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031522', 'logps_train/rejected': '-214.91', 'logps_train/chosen': '-175.47', 'loss/train': '0.73904', 'examples_per_second': '31.352', 'grad_norm': '145', 'counters/examples': 187520, 'counters/updates': 5860}
train stats after 187552 examples: {'rewards_train/chosen': '0.27519', 'rewards_train/rejected': '0.12854', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14665', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-150.82', 'loss/train': '0.6494', 'examples_per_second': '29.771', 'grad_norm': '84', 'counters/examples': 187552, 'counters/updates': 5861}
train stats after 187584 examples: {'rewards_train/chosen': '0.086289', 'rewards_train/rejected': '0.061683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024606', 'logps_train/rejected': '-139.66', 'logps_train/chosen': '-136.74', 'loss/train': '0.69044', 'examples_per_second': '32.486', 'grad_norm': '118', 'counters/examples': 187584, 'counters/updates': 5862}
train stats after 187616 examples: {'rewards_train/chosen': '-0.011599', 'rewards_train/rejected': '0.021676', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.033275', 'logps_train/rejected': '-166.53', 'logps_train/chosen': '-150.06', 'loss/train': '0.72224', 'examples_per_second': '31.507', 'grad_norm': '294', 'counters/examples': 187616, 'counters/updates': 5863}
skipping logging after 187648 examples to avoid logging too frequently
train stats after 187680 examples: {'rewards_train/chosen': '0.14262', 'rewards_train/rejected': '0.025379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11724', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-161.54', 'loss/train': '0.6558', 'examples_per_second': '32.733', 'grad_norm': '74', 'counters/examples': 187680, 'counters/updates': 5865}
train stats after 187712 examples: {'rewards_train/chosen': '0.17469', 'rewards_train/rejected': '0.17568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00098396', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-134.67', 'loss/train': '0.71866', 'examples_per_second': '31.603', 'grad_norm': '121', 'counters/examples': 187712, 'counters/updates': 5866}
train stats after 187744 examples: {'rewards_train/chosen': '0.11755', 'rewards_train/rejected': '0.076686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040862', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-146.54', 'loss/train': '0.68704', 'examples_per_second': '30.732', 'grad_norm': '82.5', 'counters/examples': 187744, 'counters/updates': 5867}
train stats after 187776 examples: {'rewards_train/chosen': '0.092164', 'rewards_train/rejected': '0.098186', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0060218', 'logps_train/rejected': '-148.6', 'logps_train/chosen': '-150.72', 'loss/train': '0.7323', 'examples_per_second': '30.525', 'grad_norm': '114', 'counters/examples': 187776, 'counters/updates': 5868}
train stats after 187808 examples: {'rewards_train/chosen': '0.092338', 'rewards_train/rejected': '0.063087', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029251', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-141.46', 'loss/train': '0.68463', 'examples_per_second': '31.551', 'grad_norm': '60.75', 'counters/examples': 187808, 'counters/updates': 5869}
train stats after 187840 examples: {'rewards_train/chosen': '0.081135', 'rewards_train/rejected': '-0.022065', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1032', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-141.14', 'loss/train': '0.65154', 'examples_per_second': '30.263', 'grad_norm': '57.5', 'counters/examples': 187840, 'counters/updates': 5870}
train stats after 187872 examples: {'rewards_train/chosen': '0.2263', 'rewards_train/rejected': '0.08499', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14131', 'logps_train/rejected': '-120.44', 'logps_train/chosen': '-185.4', 'loss/train': '0.6382', 'examples_per_second': '31.538', 'grad_norm': '85.5', 'counters/examples': 187872, 'counters/updates': 5871}
train stats after 187904 examples: {'rewards_train/chosen': '0.037506', 'rewards_train/rejected': '0.077632', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040126', 'logps_train/rejected': '-161.46', 'logps_train/chosen': '-155.81', 'loss/train': '0.72475', 'examples_per_second': '31.147', 'grad_norm': '63.75', 'counters/examples': 187904, 'counters/updates': 5872}
train stats after 187936 examples: {'rewards_train/chosen': '0.067946', 'rewards_train/rejected': '0.0074862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06046', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-146.23', 'loss/train': '0.67217', 'examples_per_second': '30.067', 'grad_norm': '118.5', 'counters/examples': 187936, 'counters/updates': 5873}
train stats after 187968 examples: {'rewards_train/chosen': '0.038705', 'rewards_train/rejected': '-0.011184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049889', 'logps_train/rejected': '-155.19', 'logps_train/chosen': '-149.8', 'loss/train': '0.6816', 'examples_per_second': '32.525', 'grad_norm': '62', 'counters/examples': 187968, 'counters/updates': 5874}
train stats after 188000 examples: {'rewards_train/chosen': '0.15824', 'rewards_train/rejected': '0.0087325', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14951', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-146.76', 'loss/train': '0.6396', 'examples_per_second': '31.571', 'grad_norm': '57.5', 'counters/examples': 188000, 'counters/updates': 5875}
train stats after 188032 examples: {'rewards_train/chosen': '0.11576', 'rewards_train/rejected': '0.02376', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092003', 'logps_train/rejected': '-164.73', 'logps_train/chosen': '-190.36', 'loss/train': '0.66924', 'examples_per_second': '30.599', 'grad_norm': '154', 'counters/examples': 188032, 'counters/updates': 5876}
skipping logging after 188064 examples to avoid logging too frequently
train stats after 188096 examples: {'rewards_train/chosen': '0.10925', 'rewards_train/rejected': '0.018582', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090672', 'logps_train/rejected': '-138.79', 'logps_train/chosen': '-119.7', 'loss/train': '0.66982', 'examples_per_second': '37.979', 'grad_norm': '60.75', 'counters/examples': 188096, 'counters/updates': 5878}
train stats after 188128 examples: {'rewards_train/chosen': '0.069457', 'rewards_train/rejected': '0.011979', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057477', 'logps_train/rejected': '-163.74', 'logps_train/chosen': '-186.5', 'loss/train': '0.68253', 'examples_per_second': '31.43', 'grad_norm': '108', 'counters/examples': 188128, 'counters/updates': 5879}
train stats after 188160 examples: {'rewards_train/chosen': '0.13138', 'rewards_train/rejected': '-0.025538', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15692', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-142.55', 'loss/train': '0.63166', 'examples_per_second': '30.116', 'grad_norm': '67', 'counters/examples': 188160, 'counters/updates': 5880}
train stats after 188192 examples: {'rewards_train/chosen': '0.15215', 'rewards_train/rejected': '0.026923', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12522', 'logps_train/rejected': '-112.44', 'logps_train/chosen': '-154.1', 'loss/train': '0.64264', 'examples_per_second': '24.211', 'grad_norm': '59.75', 'counters/examples': 188192, 'counters/updates': 5881}
skipping logging after 188224 examples to avoid logging too frequently
train stats after 188256 examples: {'rewards_train/chosen': '0.1228', 'rewards_train/rejected': '0.050629', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072173', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-126.26', 'loss/train': '0.66786', 'examples_per_second': '31.378', 'grad_norm': '71', 'counters/examples': 188256, 'counters/updates': 5883}
skipping logging after 188288 examples to avoid logging too frequently
train stats after 188320 examples: {'rewards_train/chosen': '0.02436', 'rewards_train/rejected': '0.067878', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043518', 'logps_train/rejected': '-149.57', 'logps_train/chosen': '-162.81', 'loss/train': '0.73844', 'examples_per_second': '31.503', 'grad_norm': '89.5', 'counters/examples': 188320, 'counters/updates': 5885}
skipping logging after 188352 examples to avoid logging too frequently
train stats after 188384 examples: {'rewards_train/chosen': '0.14064', 'rewards_train/rejected': '0.038521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10212', 'logps_train/rejected': '-141.81', 'logps_train/chosen': '-133.62', 'loss/train': '0.66058', 'examples_per_second': '31.581', 'grad_norm': '75.5', 'counters/examples': 188384, 'counters/updates': 5887}
train stats after 188416 examples: {'rewards_train/chosen': '0.068579', 'rewards_train/rejected': '0.092137', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023559', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-112.75', 'loss/train': '0.71142', 'examples_per_second': '31.59', 'grad_norm': '67.5', 'counters/examples': 188416, 'counters/updates': 5888}
train stats after 188448 examples: {'rewards_train/chosen': '0.078142', 'rewards_train/rejected': '0.061172', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01697', 'logps_train/rejected': '-118.28', 'logps_train/chosen': '-109.22', 'loss/train': '0.70388', 'examples_per_second': '31.612', 'grad_norm': '94.5', 'counters/examples': 188448, 'counters/updates': 5889}
train stats after 188480 examples: {'rewards_train/chosen': '0.087998', 'rewards_train/rejected': '0.063907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024091', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-158.85', 'loss/train': '0.69189', 'examples_per_second': '30.632', 'grad_norm': '108.5', 'counters/examples': 188480, 'counters/updates': 5890}
train stats after 188512 examples: {'rewards_train/chosen': '0.074611', 'rewards_train/rejected': '0.098014', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023403', 'logps_train/rejected': '-161.47', 'logps_train/chosen': '-180.23', 'loss/train': '0.72248', 'examples_per_second': '31.216', 'grad_norm': '86.5', 'counters/examples': 188512, 'counters/updates': 5891}
train stats after 188544 examples: {'rewards_train/chosen': '0.019054', 'rewards_train/rejected': '0.025129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.006075', 'logps_train/rejected': '-147.35', 'logps_train/chosen': '-168.8', 'loss/train': '0.72286', 'examples_per_second': '31.559', 'grad_norm': '169', 'counters/examples': 188544, 'counters/updates': 5892}
skipping logging after 188576 examples to avoid logging too frequently
train stats after 188608 examples: {'rewards_train/chosen': '0.088931', 'rewards_train/rejected': '0.15348', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.064553', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-150.78', 'loss/train': '0.74017', 'examples_per_second': '31.16', 'grad_norm': '78.5', 'counters/examples': 188608, 'counters/updates': 5894}
train stats after 188640 examples: {'rewards_train/chosen': '0.065674', 'rewards_train/rejected': '0.018863', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046811', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-138.75', 'loss/train': '0.6781', 'examples_per_second': '31.375', 'grad_norm': '144', 'counters/examples': 188640, 'counters/updates': 5895}
train stats after 188672 examples: {'rewards_train/chosen': '0.1141', 'rewards_train/rejected': '0.037899', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076202', 'logps_train/rejected': '-111.58', 'logps_train/chosen': '-160.76', 'loss/train': '0.66517', 'examples_per_second': '32.03', 'grad_norm': '107', 'counters/examples': 188672, 'counters/updates': 5896}
train stats after 188704 examples: {'rewards_train/chosen': '0.12382', 'rewards_train/rejected': '0.064123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059692', 'logps_train/rejected': '-114.36', 'logps_train/chosen': '-149.16', 'loss/train': '0.67564', 'examples_per_second': '30.263', 'grad_norm': '105', 'counters/examples': 188704, 'counters/updates': 5897}
train stats after 188736 examples: {'rewards_train/chosen': '0.10708', 'rewards_train/rejected': '0.059066', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.048009', 'logps_train/rejected': '-121.9', 'logps_train/chosen': '-124.35', 'loss/train': '0.68916', 'examples_per_second': '30.38', 'grad_norm': '490', 'counters/examples': 188736, 'counters/updates': 5898}
skipping logging after 188768 examples to avoid logging too frequently
train stats after 188800 examples: {'rewards_train/chosen': '0.15096', 'rewards_train/rejected': '0.00071777', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15025', 'logps_train/rejected': '-106.46', 'logps_train/chosen': '-155.35', 'loss/train': '0.62957', 'examples_per_second': '34.33', 'grad_norm': '77', 'counters/examples': 188800, 'counters/updates': 5900}
skipping logging after 188832 examples to avoid logging too frequently
train stats after 188864 examples: {'rewards_train/chosen': '0.095596', 'rewards_train/rejected': '0.013985', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081612', 'logps_train/rejected': '-97.642', 'logps_train/chosen': '-160.38', 'loss/train': '0.66565', 'examples_per_second': '32.446', 'grad_norm': '76.5', 'counters/examples': 188864, 'counters/updates': 5902}
train stats after 188896 examples: {'rewards_train/chosen': '0.11185', 'rewards_train/rejected': '0.041064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070787', 'logps_train/rejected': '-97.942', 'logps_train/chosen': '-127.62', 'loss/train': '0.67018', 'examples_per_second': '31.932', 'grad_norm': '82.5', 'counters/examples': 188896, 'counters/updates': 5903}
train stats after 188928 examples: {'rewards_train/chosen': '0.26103', 'rewards_train/rejected': '0.1418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11924', 'logps_train/rejected': '-152.11', 'logps_train/chosen': '-183.49', 'loss/train': '0.66473', 'examples_per_second': '32.448', 'grad_norm': '75', 'counters/examples': 188928, 'counters/updates': 5904}
train stats after 188960 examples: {'rewards_train/chosen': '0.10101', 'rewards_train/rejected': '0.061943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03907', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-133.21', 'loss/train': '0.69561', 'examples_per_second': '30.332', 'grad_norm': '76', 'counters/examples': 188960, 'counters/updates': 5905}
train stats after 188992 examples: {'rewards_train/chosen': '0.105', 'rewards_train/rejected': '0.007926', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097076', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-194.07', 'loss/train': '0.66259', 'examples_per_second': '29.957', 'grad_norm': '59', 'counters/examples': 188992, 'counters/updates': 5906}
train stats after 189024 examples: {'rewards_train/chosen': '0.15204', 'rewards_train/rejected': '0.032585', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11946', 'logps_train/rejected': '-165.05', 'logps_train/chosen': '-172.33', 'loss/train': '0.64604', 'examples_per_second': '30.154', 'grad_norm': '124.5', 'counters/examples': 189024, 'counters/updates': 5907}
train stats after 189056 examples: {'rewards_train/chosen': '0.071763', 'rewards_train/rejected': '0.134', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06224', 'logps_train/rejected': '-135.24', 'logps_train/chosen': '-138.08', 'loss/train': '0.73238', 'examples_per_second': '29.939', 'grad_norm': '84', 'counters/examples': 189056, 'counters/updates': 5908}
train stats after 189088 examples: {'rewards_train/chosen': '0.12801', 'rewards_train/rejected': '0.039773', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088233', 'logps_train/rejected': '-128.9', 'logps_train/chosen': '-163.1', 'loss/train': '0.66395', 'examples_per_second': '32.385', 'grad_norm': '98.5', 'counters/examples': 189088, 'counters/updates': 5909}
train stats after 189120 examples: {'rewards_train/chosen': '0.10117', 'rewards_train/rejected': '0.061161', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040012', 'logps_train/rejected': '-96.962', 'logps_train/chosen': '-95.628', 'loss/train': '0.6851', 'examples_per_second': '31.548', 'grad_norm': '68.5', 'counters/examples': 189120, 'counters/updates': 5910}
train stats after 189152 examples: {'rewards_train/chosen': '0.11671', 'rewards_train/rejected': '0.046785', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069922', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-142.33', 'loss/train': '0.67367', 'examples_per_second': '31.512', 'grad_norm': '76.5', 'counters/examples': 189152, 'counters/updates': 5911}
train stats after 189184 examples: {'rewards_train/chosen': '0.12351', 'rewards_train/rejected': '0.046041', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077471', 'logps_train/rejected': '-123.99', 'logps_train/chosen': '-130.6', 'loss/train': '0.67248', 'examples_per_second': '31.498', 'grad_norm': '102', 'counters/examples': 189184, 'counters/updates': 5912}
train stats after 189216 examples: {'rewards_train/chosen': '0.11315', 'rewards_train/rejected': '0.0089982', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-171.49', 'loss/train': '0.65201', 'examples_per_second': '31.539', 'grad_norm': '94', 'counters/examples': 189216, 'counters/updates': 5913}
train stats after 189248 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.11705', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0099089', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-142.15', 'loss/train': '0.70148', 'examples_per_second': '32.215', 'grad_norm': '55.75', 'counters/examples': 189248, 'counters/updates': 5914}
train stats after 189280 examples: {'rewards_train/chosen': '0.11235', 'rewards_train/rejected': '0.092', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020348', 'logps_train/rejected': '-150.1', 'logps_train/chosen': '-150', 'loss/train': '0.68944', 'examples_per_second': '31.545', 'grad_norm': '79.5', 'counters/examples': 189280, 'counters/updates': 5915}
train stats after 189312 examples: {'rewards_train/chosen': '0.099939', 'rewards_train/rejected': '0.08433', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.015609', 'logps_train/rejected': '-191.19', 'logps_train/chosen': '-133.31', 'loss/train': '0.70008', 'examples_per_second': '31.425', 'grad_norm': '102', 'counters/examples': 189312, 'counters/updates': 5916}
skipping logging after 189344 examples to avoid logging too frequently
train stats after 189376 examples: {'rewards_train/chosen': '0.095525', 'rewards_train/rejected': '0.061193', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034332', 'logps_train/rejected': '-111.55', 'logps_train/chosen': '-142.23', 'loss/train': '0.68797', 'examples_per_second': '35.924', 'grad_norm': '68', 'counters/examples': 189376, 'counters/updates': 5918}
skipping logging after 189408 examples to avoid logging too frequently
train stats after 189440 examples: {'rewards_train/chosen': '0.0294', 'rewards_train/rejected': '-0.0010835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030484', 'logps_train/rejected': '-103.6', 'logps_train/chosen': '-135.79', 'loss/train': '0.68812', 'examples_per_second': '35.903', 'grad_norm': '75.5', 'counters/examples': 189440, 'counters/updates': 5920}
train stats after 189472 examples: {'rewards_train/chosen': '0.11901', 'rewards_train/rejected': '0.077896', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041118', 'logps_train/rejected': '-138.3', 'logps_train/chosen': '-179.8', 'loss/train': '0.68388', 'examples_per_second': '31.557', 'grad_norm': '82.5', 'counters/examples': 189472, 'counters/updates': 5921}
train stats after 189504 examples: {'rewards_train/chosen': '-0.0016571', 'rewards_train/rejected': '0.090024', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.091681', 'logps_train/rejected': '-154.86', 'logps_train/chosen': '-139.01', 'loss/train': '0.7499', 'examples_per_second': '31.605', 'grad_norm': '120.5', 'counters/examples': 189504, 'counters/updates': 5922}
skipping logging after 189536 examples to avoid logging too frequently
train stats after 189568 examples: {'rewards_train/chosen': '0.11339', 'rewards_train/rejected': '0.026529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086857', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-169.57', 'loss/train': '0.66032', 'examples_per_second': '33.095', 'grad_norm': '61.25', 'counters/examples': 189568, 'counters/updates': 5924}
train stats after 189600 examples: {'rewards_train/chosen': '0.24745', 'rewards_train/rejected': '0.03901', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20844', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-219.97', 'loss/train': '0.60983', 'examples_per_second': '30.115', 'grad_norm': '90', 'counters/examples': 189600, 'counters/updates': 5925}
train stats after 189632 examples: {'rewards_train/chosen': '0.094164', 'rewards_train/rejected': '0.0013696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092794', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-163.67', 'loss/train': '0.65929', 'examples_per_second': '31.759', 'grad_norm': '61.5', 'counters/examples': 189632, 'counters/updates': 5926}
train stats after 189664 examples: {'rewards_train/chosen': '0.205', 'rewards_train/rejected': '0.091318', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11368', 'logps_train/rejected': '-137.25', 'logps_train/chosen': '-145.75', 'loss/train': '0.64734', 'examples_per_second': '31.146', 'grad_norm': '116', 'counters/examples': 189664, 'counters/updates': 5927}
train stats after 189696 examples: {'rewards_train/chosen': '0.13652', 'rewards_train/rejected': '0.14815', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011627', 'logps_train/rejected': '-159.57', 'logps_train/chosen': '-167.33', 'loss/train': '0.72562', 'examples_per_second': '30.856', 'grad_norm': '98.5', 'counters/examples': 189696, 'counters/updates': 5928}
train stats after 189728 examples: {'rewards_train/chosen': '0.10886', 'rewards_train/rejected': '0.028371', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080486', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-136.4', 'loss/train': '0.66564', 'examples_per_second': '30.057', 'grad_norm': '70.5', 'counters/examples': 189728, 'counters/updates': 5929}
train stats after 189760 examples: {'rewards_train/chosen': '0.070042', 'rewards_train/rejected': '0.060639', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0094029', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-125.61', 'loss/train': '0.70651', 'examples_per_second': '30.471', 'grad_norm': '77', 'counters/examples': 189760, 'counters/updates': 5930}
train stats after 189792 examples: {'rewards_train/chosen': '0.058229', 'rewards_train/rejected': '0.073076', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014847', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-115.83', 'loss/train': '0.71046', 'examples_per_second': '31.012', 'grad_norm': '61.75', 'counters/examples': 189792, 'counters/updates': 5931}
train stats after 189824 examples: {'rewards_train/chosen': '0.089918', 'rewards_train/rejected': '0.026771', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063147', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-173.52', 'loss/train': '0.6775', 'examples_per_second': '31.867', 'grad_norm': '79', 'counters/examples': 189824, 'counters/updates': 5932}
train stats after 189856 examples: {'rewards_train/chosen': '0.20163', 'rewards_train/rejected': '0.18313', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018502', 'logps_train/rejected': '-161.53', 'logps_train/chosen': '-123.28', 'loss/train': '0.70941', 'examples_per_second': '30.441', 'grad_norm': '89.5', 'counters/examples': 189856, 'counters/updates': 5933}
train stats after 189888 examples: {'rewards_train/chosen': '0.058818', 'rewards_train/rejected': '0.064671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0058533', 'logps_train/rejected': '-128.3', 'logps_train/chosen': '-148.13', 'loss/train': '0.70585', 'examples_per_second': '31.238', 'grad_norm': '85.5', 'counters/examples': 189888, 'counters/updates': 5934}
train stats after 189920 examples: {'rewards_train/chosen': '0.10637', 'rewards_train/rejected': '0.068911', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037462', 'logps_train/rejected': '-168.28', 'logps_train/chosen': '-160.74', 'loss/train': '0.68094', 'examples_per_second': '31.909', 'grad_norm': '64.5', 'counters/examples': 189920, 'counters/updates': 5935}
train stats after 189952 examples: {'rewards_train/chosen': '0.058772', 'rewards_train/rejected': '0.074482', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01571', 'logps_train/rejected': '-142.4', 'logps_train/chosen': '-178.16', 'loss/train': '0.72913', 'examples_per_second': '31.115', 'grad_norm': '118.5', 'counters/examples': 189952, 'counters/updates': 5936}
train stats after 189984 examples: {'rewards_train/chosen': '0.042748', 'rewards_train/rejected': '-0.0040872', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046835', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-158.53', 'loss/train': '0.67875', 'examples_per_second': '31.567', 'grad_norm': '130', 'counters/examples': 189984, 'counters/updates': 5937}
train stats after 190016 examples: {'rewards_train/chosen': '0.14377', 'rewards_train/rejected': '0.057737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086038', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-176.83', 'loss/train': '0.66683', 'examples_per_second': '31.38', 'grad_norm': '93', 'counters/examples': 190016, 'counters/updates': 5938}
train stats after 190048 examples: {'rewards_train/chosen': '0.059673', 'rewards_train/rejected': '0.047518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012156', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-124.25', 'loss/train': '0.70914', 'examples_per_second': '31.213', 'grad_norm': '90.5', 'counters/examples': 190048, 'counters/updates': 5939}
train stats after 190080 examples: {'rewards_train/chosen': '0.1533', 'rewards_train/rejected': '0.22328', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.069985', 'logps_train/rejected': '-177.4', 'logps_train/chosen': '-191.26', 'loss/train': '0.74497', 'examples_per_second': '31.171', 'grad_norm': '532', 'counters/examples': 190080, 'counters/updates': 5940}
train stats after 190112 examples: {'rewards_train/chosen': '0.14255', 'rewards_train/rejected': '0.098692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043854', 'logps_train/rejected': '-148.3', 'logps_train/chosen': '-134.95', 'loss/train': '0.68698', 'examples_per_second': '30.557', 'grad_norm': '81', 'counters/examples': 190112, 'counters/updates': 5941}
train stats after 190144 examples: {'rewards_train/chosen': '0.052964', 'rewards_train/rejected': '0.039911', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013053', 'logps_train/rejected': '-113', 'logps_train/chosen': '-145.26', 'loss/train': '0.70686', 'examples_per_second': '32.981', 'grad_norm': '52', 'counters/examples': 190144, 'counters/updates': 5942}
skipping logging after 190176 examples to avoid logging too frequently
train stats after 190208 examples: {'rewards_train/chosen': '0.099398', 'rewards_train/rejected': '0.021287', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07811', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-140.75', 'loss/train': '0.66562', 'examples_per_second': '30.177', 'grad_norm': '112', 'counters/examples': 190208, 'counters/updates': 5944}
train stats after 190240 examples: {'rewards_train/chosen': '0.15699', 'rewards_train/rejected': '0.11412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042871', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-146.78', 'loss/train': '0.68605', 'examples_per_second': '24.206', 'grad_norm': '210', 'counters/examples': 190240, 'counters/updates': 5945}
train stats after 190272 examples: {'rewards_train/chosen': '0.14055', 'rewards_train/rejected': '0.039478', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10107', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-129.46', 'loss/train': '0.65979', 'examples_per_second': '31.161', 'grad_norm': '79.5', 'counters/examples': 190272, 'counters/updates': 5946}
train stats after 190304 examples: {'rewards_train/chosen': '0.11081', 'rewards_train/rejected': '0.075984', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034821', 'logps_train/rejected': '-162.11', 'logps_train/chosen': '-163.86', 'loss/train': '0.69097', 'examples_per_second': '31.111', 'grad_norm': '99', 'counters/examples': 190304, 'counters/updates': 5947}
train stats after 190336 examples: {'rewards_train/chosen': '0.075199', 'rewards_train/rejected': '0.01356', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061638', 'logps_train/rejected': '-147.17', 'logps_train/chosen': '-148.61', 'loss/train': '0.67354', 'examples_per_second': '23.923', 'grad_norm': '85', 'counters/examples': 190336, 'counters/updates': 5948}
train stats after 190368 examples: {'rewards_train/chosen': '0.0096029', 'rewards_train/rejected': '0.041739', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.032136', 'logps_train/rejected': '-112.21', 'logps_train/chosen': '-135.6', 'loss/train': '0.71952', 'examples_per_second': '32.42', 'grad_norm': '60.75', 'counters/examples': 190368, 'counters/updates': 5949}
train stats after 190400 examples: {'rewards_train/chosen': '0.13292', 'rewards_train/rejected': '0.094164', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038754', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-185.08', 'loss/train': '0.68475', 'examples_per_second': '31.173', 'grad_norm': '110', 'counters/examples': 190400, 'counters/updates': 5950}
train stats after 190432 examples: {'rewards_train/chosen': '0.10165', 'rewards_train/rejected': '0.039889', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061765', 'logps_train/rejected': '-116.74', 'logps_train/chosen': '-129.91', 'loss/train': '0.67254', 'examples_per_second': '33.113', 'grad_norm': '57.5', 'counters/examples': 190432, 'counters/updates': 5951}
train stats after 190464 examples: {'rewards_train/chosen': '0.15053', 'rewards_train/rejected': '0.0017465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14879', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-184.75', 'loss/train': '0.63271', 'examples_per_second': '30.541', 'grad_norm': '191', 'counters/examples': 190464, 'counters/updates': 5952}
train stats after 190496 examples: {'rewards_train/chosen': '0.046578', 'rewards_train/rejected': '0.088392', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041814', 'logps_train/rejected': '-127.29', 'logps_train/chosen': '-136.2', 'loss/train': '0.73272', 'examples_per_second': '30.059', 'grad_norm': '370', 'counters/examples': 190496, 'counters/updates': 5953}
train stats after 190528 examples: {'rewards_train/chosen': '0.10928', 'rewards_train/rejected': '0.032478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076801', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-121.25', 'loss/train': '0.66842', 'examples_per_second': '32.625', 'grad_norm': '63.5', 'counters/examples': 190528, 'counters/updates': 5954}
train stats after 190560 examples: {'rewards_train/chosen': '0.060103', 'rewards_train/rejected': '-0.06462', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12472', 'logps_train/rejected': '-185.28', 'logps_train/chosen': '-158.98', 'loss/train': '0.64095', 'examples_per_second': '30.613', 'grad_norm': '109', 'counters/examples': 190560, 'counters/updates': 5955}
train stats after 190592 examples: {'rewards_train/chosen': '0.046045', 'rewards_train/rejected': '-0.019849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065894', 'logps_train/rejected': '-103.17', 'logps_train/chosen': '-141.24', 'loss/train': '0.66588', 'examples_per_second': '32.032', 'grad_norm': '50.5', 'counters/examples': 190592, 'counters/updates': 5956}
train stats after 190624 examples: {'rewards_train/chosen': '0.31119', 'rewards_train/rejected': '0.17109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14009', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-183.21', 'loss/train': '0.65327', 'examples_per_second': '31.915', 'grad_norm': '122', 'counters/examples': 190624, 'counters/updates': 5957}
train stats after 190656 examples: {'rewards_train/chosen': '0.0836', 'rewards_train/rejected': '0.042901', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.040699', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-176.61', 'loss/train': '0.69386', 'examples_per_second': '32.493', 'grad_norm': '123', 'counters/examples': 190656, 'counters/updates': 5958}
train stats after 190688 examples: {'rewards_train/chosen': '0.069607', 'rewards_train/rejected': '-0.0056824', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07529', 'logps_train/rejected': '-143.02', 'logps_train/chosen': '-143.52', 'loss/train': '0.66574', 'examples_per_second': '31.562', 'grad_norm': '89.5', 'counters/examples': 190688, 'counters/updates': 5959}
train stats after 190720 examples: {'rewards_train/chosen': '0.087861', 'rewards_train/rejected': '0.0091728', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078689', 'logps_train/rejected': '-105.52', 'logps_train/chosen': '-166.53', 'loss/train': '0.67342', 'examples_per_second': '30.642', 'grad_norm': '153', 'counters/examples': 190720, 'counters/updates': 5960}
train stats after 190752 examples: {'rewards_train/chosen': '0.094868', 'rewards_train/rejected': '0.10746', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012591', 'logps_train/rejected': '-159.29', 'logps_train/chosen': '-195.13', 'loss/train': '0.72645', 'examples_per_second': '31.548', 'grad_norm': '78', 'counters/examples': 190752, 'counters/updates': 5961}
train stats after 190784 examples: {'rewards_train/chosen': '0.17798', 'rewards_train/rejected': '0.08158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096398', 'logps_train/rejected': '-149.48', 'logps_train/chosen': '-193.32', 'loss/train': '0.6621', 'examples_per_second': '31.142', 'grad_norm': '73.5', 'counters/examples': 190784, 'counters/updates': 5962}
train stats after 190816 examples: {'rewards_train/chosen': '0.085715', 'rewards_train/rejected': '0.13682', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.051108', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-164.21', 'loss/train': '0.77784', 'examples_per_second': '31.484', 'grad_norm': '116.5', 'counters/examples': 190816, 'counters/updates': 5963}
train stats after 190848 examples: {'rewards_train/chosen': '0.20287', 'rewards_train/rejected': '0.20066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0022065', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-160.28', 'loss/train': '0.73331', 'examples_per_second': '32.138', 'grad_norm': '510', 'counters/examples': 190848, 'counters/updates': 5964}
train stats after 190880 examples: {'rewards_train/chosen': '0.10386', 'rewards_train/rejected': '0.02806', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075805', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-178.26', 'loss/train': '0.66945', 'examples_per_second': '33.046', 'grad_norm': '60.5', 'counters/examples': 190880, 'counters/updates': 5965}
train stats after 190912 examples: {'rewards_train/chosen': '0.093241', 'rewards_train/rejected': '0.020075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073166', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-129.85', 'loss/train': '0.67146', 'examples_per_second': '30.45', 'grad_norm': '145', 'counters/examples': 190912, 'counters/updates': 5966}
skipping logging after 190944 examples to avoid logging too frequently
train stats after 190976 examples: {'rewards_train/chosen': '0.05931', 'rewards_train/rejected': '0.081747', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022438', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-140.8', 'loss/train': '0.72038', 'examples_per_second': '31.634', 'grad_norm': '79', 'counters/examples': 190976, 'counters/updates': 5968}
train stats after 191008 examples: {'rewards_train/chosen': '0.17785', 'rewards_train/rejected': '0.058533', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11932', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-209.03', 'loss/train': '0.65355', 'examples_per_second': '30.813', 'grad_norm': '115', 'counters/examples': 191008, 'counters/updates': 5969}
train stats after 191040 examples: {'rewards_train/chosen': '0.11614', 'rewards_train/rejected': '-0.07906', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1952', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-164.79', 'loss/train': '0.61767', 'examples_per_second': '31.583', 'grad_norm': '80', 'counters/examples': 191040, 'counters/updates': 5970}
train stats after 191072 examples: {'rewards_train/chosen': '0.056579', 'rewards_train/rejected': '0.018842', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037736', 'logps_train/rejected': '-125.41', 'logps_train/chosen': '-100.95', 'loss/train': '0.68559', 'examples_per_second': '31.813', 'grad_norm': '75.5', 'counters/examples': 191072, 'counters/updates': 5971}
train stats after 191104 examples: {'rewards_train/chosen': '0.18804', 'rewards_train/rejected': '0.10724', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080802', 'logps_train/rejected': '-141.88', 'logps_train/chosen': '-160.71', 'loss/train': '0.66384', 'examples_per_second': '30.111', 'grad_norm': '77.5', 'counters/examples': 191104, 'counters/updates': 5972}
train stats after 191136 examples: {'rewards_train/chosen': '0.050084', 'rewards_train/rejected': '-0.07599', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12607', 'logps_train/rejected': '-145.16', 'logps_train/chosen': '-155.27', 'loss/train': '0.64787', 'examples_per_second': '31.561', 'grad_norm': '209', 'counters/examples': 191136, 'counters/updates': 5973}
skipping logging after 191168 examples to avoid logging too frequently
train stats after 191200 examples: {'rewards_train/chosen': '0.12032', 'rewards_train/rejected': '0.09793', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022387', 'logps_train/rejected': '-161.67', 'logps_train/chosen': '-151.41', 'loss/train': '0.69089', 'examples_per_second': '31.567', 'grad_norm': '71.5', 'counters/examples': 191200, 'counters/updates': 5975}
train stats after 191232 examples: {'rewards_train/chosen': '0.1445', 'rewards_train/rejected': '0.039816', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10469', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-187.54', 'loss/train': '0.66112', 'examples_per_second': '29.909', 'grad_norm': '127', 'counters/examples': 191232, 'counters/updates': 5976}
train stats after 191264 examples: {'rewards_train/chosen': '0.0951', 'rewards_train/rejected': '0.098106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0030054', 'logps_train/rejected': '-147.56', 'logps_train/chosen': '-165.53', 'loss/train': '0.70805', 'examples_per_second': '30.378', 'grad_norm': '105.5', 'counters/examples': 191264, 'counters/updates': 5977}
train stats after 191296 examples: {'rewards_train/chosen': '0.11447', 'rewards_train/rejected': '0.08687', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0276', 'logps_train/rejected': '-130.12', 'logps_train/chosen': '-153.25', 'loss/train': '0.68986', 'examples_per_second': '31.373', 'grad_norm': '81', 'counters/examples': 191296, 'counters/updates': 5978}
train stats after 191328 examples: {'rewards_train/chosen': '0.12593', 'rewards_train/rejected': '0.090784', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035149', 'logps_train/rejected': '-156.41', 'logps_train/chosen': '-144.55', 'loss/train': '0.69088', 'examples_per_second': '30.44', 'grad_norm': '59.75', 'counters/examples': 191328, 'counters/updates': 5979}
train stats after 191360 examples: {'rewards_train/chosen': '0.095917', 'rewards_train/rejected': '0.15095', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.055034', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-168.05', 'loss/train': '0.73367', 'examples_per_second': '31.491', 'grad_norm': '89', 'counters/examples': 191360, 'counters/updates': 5980}
train stats after 191392 examples: {'rewards_train/chosen': '0.15331', 'rewards_train/rejected': '0.034527', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11879', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-159.22', 'loss/train': '0.64932', 'examples_per_second': '31.025', 'grad_norm': '88', 'counters/examples': 191392, 'counters/updates': 5981}
train stats after 191424 examples: {'rewards_train/chosen': '0.040574', 'rewards_train/rejected': '0.0052089', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035365', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-150.3', 'loss/train': '0.68839', 'examples_per_second': '30.653', 'grad_norm': '57', 'counters/examples': 191424, 'counters/updates': 5982}
train stats after 191456 examples: {'rewards_train/chosen': '0.099299', 'rewards_train/rejected': '0.00019265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099106', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-151.9', 'loss/train': '0.65804', 'examples_per_second': '30.175', 'grad_norm': '163', 'counters/examples': 191456, 'counters/updates': 5983}
train stats after 191488 examples: {'rewards_train/chosen': '0.16336', 'rewards_train/rejected': '0.082347', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.081016', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-139.85', 'loss/train': '0.6716', 'examples_per_second': '30.138', 'grad_norm': '77.5', 'counters/examples': 191488, 'counters/updates': 5984}
train stats after 191520 examples: {'rewards_train/chosen': '0.12369', 'rewards_train/rejected': '0.020044', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10365', 'logps_train/rejected': '-153.41', 'logps_train/chosen': '-169.87', 'loss/train': '0.65391', 'examples_per_second': '31.575', 'grad_norm': '164', 'counters/examples': 191520, 'counters/updates': 5985}
train stats after 191552 examples: {'rewards_train/chosen': '0.11827', 'rewards_train/rejected': '0.026066', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092205', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-143.21', 'loss/train': '0.65864', 'examples_per_second': '31.503', 'grad_norm': '99.5', 'counters/examples': 191552, 'counters/updates': 5986}
train stats after 191584 examples: {'rewards_train/chosen': '0.17617', 'rewards_train/rejected': '0.10907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067094', 'logps_train/rejected': '-158.55', 'logps_train/chosen': '-166', 'loss/train': '0.67357', 'examples_per_second': '31.444', 'grad_norm': '82.5', 'counters/examples': 191584, 'counters/updates': 5987}
train stats after 191616 examples: {'rewards_train/chosen': '0.098825', 'rewards_train/rejected': '0.060906', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037919', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-169.33', 'loss/train': '0.68701', 'examples_per_second': '30.738', 'grad_norm': '122.5', 'counters/examples': 191616, 'counters/updates': 5988}
train stats after 191648 examples: {'rewards_train/chosen': '0.099556', 'rewards_train/rejected': '0.074976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024581', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-133.32', 'loss/train': '0.69095', 'examples_per_second': '31.087', 'grad_norm': '51.25', 'counters/examples': 191648, 'counters/updates': 5989}
skipping logging after 191680 examples to avoid logging too frequently
train stats after 191712 examples: {'rewards_train/chosen': '0.1862', 'rewards_train/rejected': '0.17063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015566', 'logps_train/rejected': '-174.35', 'logps_train/chosen': '-187.79', 'loss/train': '0.69885', 'examples_per_second': '31.666', 'grad_norm': '116.5', 'counters/examples': 191712, 'counters/updates': 5991}
train stats after 191744 examples: {'rewards_train/chosen': '0.12446', 'rewards_train/rejected': '0.080603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043862', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-171.08', 'loss/train': '0.69161', 'examples_per_second': '30.709', 'grad_norm': '63.5', 'counters/examples': 191744, 'counters/updates': 5992}
skipping logging after 191776 examples to avoid logging too frequently
train stats after 191808 examples: {'rewards_train/chosen': '-0.0087193', 'rewards_train/rejected': '0.048375', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.057095', 'logps_train/rejected': '-88.732', 'logps_train/chosen': '-120.22', 'loss/train': '0.73446', 'examples_per_second': '31.448', 'grad_norm': '58.5', 'counters/examples': 191808, 'counters/updates': 5994}
skipping logging after 191840 examples to avoid logging too frequently
train stats after 191872 examples: {'rewards_train/chosen': '0.072167', 'rewards_train/rejected': '0.063587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0085796', 'logps_train/rejected': '-169.06', 'logps_train/chosen': '-171.8', 'loss/train': '0.71296', 'examples_per_second': '30.192', 'grad_norm': '200', 'counters/examples': 191872, 'counters/updates': 5996}
skipping logging after 191904 examples to avoid logging too frequently
train stats after 191936 examples: {'rewards_train/chosen': '0.087176', 'rewards_train/rejected': '-0.0086761', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095852', 'logps_train/rejected': '-144.25', 'logps_train/chosen': '-178.75', 'loss/train': '0.65936', 'examples_per_second': '31.667', 'grad_norm': '217', 'counters/examples': 191936, 'counters/updates': 5998}
train stats after 191968 examples: {'rewards_train/chosen': '0.076865', 'rewards_train/rejected': '0.067146', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0097181', 'logps_train/rejected': '-146.27', 'logps_train/chosen': '-154.2', 'loss/train': '0.70046', 'examples_per_second': '30.285', 'grad_norm': '154', 'counters/examples': 191968, 'counters/updates': 5999}
train stats after 192000 examples: {'rewards_train/chosen': '0.16485', 'rewards_train/rejected': '0.082671', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.082179', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-158.58', 'loss/train': '0.65915', 'examples_per_second': '31.466', 'grad_norm': '93.5', 'counters/examples': 192000, 'counters/updates': 6000}
Running evaluation after 192000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.90it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]
eval after 192000: {'rewards_eval/chosen': '0.12967', 'rewards_eval/rejected': '0.055787', 'rewards_eval/accuracies': '0.53906', 'rewards_eval/margins': '0.073884', 'logps_eval/rejected': '-127.59', 'logps_eval/chosen': '-149.71', 'loss/eval': '0.67444'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716/step-192000...
writing checkpoint to .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716/step-192000/policy.pt...
train stats after 192032 examples: {'rewards_train/chosen': '0.066049', 'rewards_train/rejected': '0.057359', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00869', 'logps_train/rejected': '-118.81', 'logps_train/chosen': '-136.63', 'loss/train': '0.6977', 'examples_per_second': '24.3', 'grad_norm': '89.5', 'counters/examples': 192032, 'counters/updates': 6001}
train stats after 192064 examples: {'rewards_train/chosen': '0.10216', 'rewards_train/rejected': '0.02877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073393', 'logps_train/rejected': '-141', 'logps_train/chosen': '-173.98', 'loss/train': '0.67696', 'examples_per_second': '30.118', 'grad_norm': '121', 'counters/examples': 192064, 'counters/updates': 6002}
train stats after 192096 examples: {'rewards_train/chosen': '0.15919', 'rewards_train/rejected': '0.046572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11262', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-199.16', 'loss/train': '0.6545', 'examples_per_second': '31.841', 'grad_norm': '79', 'counters/examples': 192096, 'counters/updates': 6003}
skipping logging after 192128 examples to avoid logging too frequently
train stats after 192160 examples: {'rewards_train/chosen': '0.11811', 'rewards_train/rejected': '0.020545', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097562', 'logps_train/rejected': '-163.86', 'logps_train/chosen': '-161.61', 'loss/train': '0.6589', 'examples_per_second': '31.6', 'grad_norm': '86.5', 'counters/examples': 192160, 'counters/updates': 6005}
train stats after 192192 examples: {'rewards_train/chosen': '0.042831', 'rewards_train/rejected': '0.072216', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029385', 'logps_train/rejected': '-104.95', 'logps_train/chosen': '-186.82', 'loss/train': '0.71988', 'examples_per_second': '31.625', 'grad_norm': '97', 'counters/examples': 192192, 'counters/updates': 6006}
skipping logging after 192224 examples to avoid logging too frequently
train stats after 192256 examples: {'rewards_train/chosen': '0.034602', 'rewards_train/rejected': '0.074937', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040335', 'logps_train/rejected': '-122.85', 'logps_train/chosen': '-142.5', 'loss/train': '0.72514', 'examples_per_second': '31.586', 'grad_norm': '74.5', 'counters/examples': 192256, 'counters/updates': 6008}
train stats after 192288 examples: {'rewards_train/chosen': '0.21384', 'rewards_train/rejected': '0.031263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18258', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-163.58', 'loss/train': '0.62628', 'examples_per_second': '31.293', 'grad_norm': '140', 'counters/examples': 192288, 'counters/updates': 6009}
train stats after 192320 examples: {'rewards_train/chosen': '0.084756', 'rewards_train/rejected': '0.022757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061998', 'logps_train/rejected': '-158.58', 'logps_train/chosen': '-125.34', 'loss/train': '0.68913', 'examples_per_second': '31.588', 'grad_norm': '253', 'counters/examples': 192320, 'counters/updates': 6010}
skipping logging after 192352 examples to avoid logging too frequently
train stats after 192384 examples: {'rewards_train/chosen': '0.04814', 'rewards_train/rejected': '0.03067', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01747', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-137.55', 'loss/train': '0.69296', 'examples_per_second': '30.489', 'grad_norm': '123', 'counters/examples': 192384, 'counters/updates': 6012}
train stats after 192416 examples: {'rewards_train/chosen': '0.087067', 'rewards_train/rejected': '0.052803', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034265', 'logps_train/rejected': '-159.83', 'logps_train/chosen': '-150.54', 'loss/train': '0.71528', 'examples_per_second': '31.081', 'grad_norm': '117', 'counters/examples': 192416, 'counters/updates': 6013}
train stats after 192448 examples: {'rewards_train/chosen': '0.10887', 'rewards_train/rejected': '0.081634', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027234', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-134.42', 'loss/train': '0.69376', 'examples_per_second': '32.429', 'grad_norm': '75.5', 'counters/examples': 192448, 'counters/updates': 6014}
train stats after 192480 examples: {'rewards_train/chosen': '0.15692', 'rewards_train/rejected': '-0.062395', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21931', 'logps_train/rejected': '-171.87', 'logps_train/chosen': '-173.77', 'loss/train': '0.60657', 'examples_per_second': '30.093', 'grad_norm': '86', 'counters/examples': 192480, 'counters/updates': 6015}
train stats after 192512 examples: {'rewards_train/chosen': '0.030192', 'rewards_train/rejected': '0.066866', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036673', 'logps_train/rejected': '-171.9', 'logps_train/chosen': '-138.94', 'loss/train': '0.72189', 'examples_per_second': '30.712', 'grad_norm': '95', 'counters/examples': 192512, 'counters/updates': 6016}
train stats after 192544 examples: {'rewards_train/chosen': '0.079919', 'rewards_train/rejected': '0.072999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0069199', 'logps_train/rejected': '-153.89', 'logps_train/chosen': '-177.08', 'loss/train': '0.69692', 'examples_per_second': '31.949', 'grad_norm': '97', 'counters/examples': 192544, 'counters/updates': 6017}
train stats after 192576 examples: {'rewards_train/chosen': '0.072234', 'rewards_train/rejected': '0.078635', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0064004', 'logps_train/rejected': '-107.86', 'logps_train/chosen': '-140.25', 'loss/train': '0.70805', 'examples_per_second': '30.811', 'grad_norm': '122', 'counters/examples': 192576, 'counters/updates': 6018}
train stats after 192608 examples: {'rewards_train/chosen': '0.14179', 'rewards_train/rejected': '0.07929', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062499', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-172.2', 'loss/train': '0.67707', 'examples_per_second': '31.512', 'grad_norm': '243', 'counters/examples': 192608, 'counters/updates': 6019}
train stats after 192640 examples: {'rewards_train/chosen': '0.1478', 'rewards_train/rejected': '0.029339', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11846', 'logps_train/rejected': '-178.22', 'logps_train/chosen': '-196.82', 'loss/train': '0.64843', 'examples_per_second': '30.174', 'grad_norm': '99.5', 'counters/examples': 192640, 'counters/updates': 6020}
train stats after 192672 examples: {'rewards_train/chosen': '0.056122', 'rewards_train/rejected': '0.14542', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.089302', 'logps_train/rejected': '-161.19', 'logps_train/chosen': '-173.89', 'loss/train': '0.75047', 'examples_per_second': '30.045', 'grad_norm': '147', 'counters/examples': 192672, 'counters/updates': 6021}
train stats after 192704 examples: {'rewards_train/chosen': '0.13089', 'rewards_train/rejected': '-0.0069948', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13788', 'logps_train/rejected': '-96.516', 'logps_train/chosen': '-156.62', 'loss/train': '0.64158', 'examples_per_second': '32.726', 'grad_norm': '71', 'counters/examples': 192704, 'counters/updates': 6022}
train stats after 192736 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '0.021948', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099238', 'logps_train/rejected': '-120.53', 'logps_train/chosen': '-144.62', 'loss/train': '0.66004', 'examples_per_second': '31.404', 'grad_norm': '78', 'counters/examples': 192736, 'counters/updates': 6023}
train stats after 192768 examples: {'rewards_train/chosen': '0.080878', 'rewards_train/rejected': '0.019519', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061359', 'logps_train/rejected': '-113.35', 'logps_train/chosen': '-168.55', 'loss/train': '0.67461', 'examples_per_second': '31.216', 'grad_norm': '81', 'counters/examples': 192768, 'counters/updates': 6024}
train stats after 192800 examples: {'rewards_train/chosen': '0.16954', 'rewards_train/rejected': '0.041935', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1276', 'logps_train/rejected': '-109.86', 'logps_train/chosen': '-146.93', 'loss/train': '0.65105', 'examples_per_second': '31.525', 'grad_norm': '56', 'counters/examples': 192800, 'counters/updates': 6025}
train stats after 192832 examples: {'rewards_train/chosen': '0.084103', 'rewards_train/rejected': '0.078991', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0051126', 'logps_train/rejected': '-171.31', 'logps_train/chosen': '-165.01', 'loss/train': '0.712', 'examples_per_second': '31.185', 'grad_norm': '123.5', 'counters/examples': 192832, 'counters/updates': 6026}
train stats after 192864 examples: {'rewards_train/chosen': '0.13784', 'rewards_train/rejected': '0.012681', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12516', 'logps_train/rejected': '-93.65', 'logps_train/chosen': '-158.96', 'loss/train': '0.64731', 'examples_per_second': '31.765', 'grad_norm': '89', 'counters/examples': 192864, 'counters/updates': 6027}
train stats after 192896 examples: {'rewards_train/chosen': '0.20457', 'rewards_train/rejected': '0.066352', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13822', 'logps_train/rejected': '-162.76', 'logps_train/chosen': '-209.97', 'loss/train': '0.64276', 'examples_per_second': '30.63', 'grad_norm': '78', 'counters/examples': 192896, 'counters/updates': 6028}
train stats after 192928 examples: {'rewards_train/chosen': '0.077767', 'rewards_train/rejected': '-0.022402', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10017', 'logps_train/rejected': '-111.05', 'logps_train/chosen': '-118.63', 'loss/train': '0.65233', 'examples_per_second': '31.912', 'grad_norm': '126.5', 'counters/examples': 192928, 'counters/updates': 6029}
train stats after 192960 examples: {'rewards_train/chosen': '0.020927', 'rewards_train/rejected': '0.022949', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0020219', 'logps_train/rejected': '-89.876', 'logps_train/chosen': '-115.57', 'loss/train': '0.70743', 'examples_per_second': '31.208', 'grad_norm': '71.5', 'counters/examples': 192960, 'counters/updates': 6030}
train stats after 192992 examples: {'rewards_train/chosen': '0.1253', 'rewards_train/rejected': '0.076783', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048515', 'logps_train/rejected': '-177.66', 'logps_train/chosen': '-170.14', 'loss/train': '0.70116', 'examples_per_second': '31.638', 'grad_norm': '82.5', 'counters/examples': 192992, 'counters/updates': 6031}
train stats after 193024 examples: {'rewards_train/chosen': '0.12605', 'rewards_train/rejected': '0.065313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060734', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-138.14', 'loss/train': '0.68598', 'examples_per_second': '31.624', 'grad_norm': '70', 'counters/examples': 193024, 'counters/updates': 6032}
skipping logging after 193056 examples to avoid logging too frequently
train stats after 193088 examples: {'rewards_train/chosen': '0.14834', 'rewards_train/rejected': '0.034794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11355', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-145.86', 'loss/train': '0.65033', 'examples_per_second': '31.587', 'grad_norm': '59', 'counters/examples': 193088, 'counters/updates': 6034}
train stats after 193120 examples: {'rewards_train/chosen': '0.078347', 'rewards_train/rejected': '0.098655', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.020308', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-117.36', 'loss/train': '0.71004', 'examples_per_second': '31.192', 'grad_norm': '62.25', 'counters/examples': 193120, 'counters/updates': 6035}
skipping logging after 193152 examples to avoid logging too frequently
train stats after 193184 examples: {'rewards_train/chosen': '0.13113', 'rewards_train/rejected': '0.104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027134', 'logps_train/rejected': '-104.27', 'logps_train/chosen': '-151.68', 'loss/train': '0.7061', 'examples_per_second': '32.399', 'grad_norm': '61.25', 'counters/examples': 193184, 'counters/updates': 6037}
train stats after 193216 examples: {'rewards_train/chosen': '0.1489', 'rewards_train/rejected': '0.067551', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081349', 'logps_train/rejected': '-135.54', 'logps_train/chosen': '-133.17', 'loss/train': '0.66338', 'examples_per_second': '32.831', 'grad_norm': '128', 'counters/examples': 193216, 'counters/updates': 6038}
train stats after 193248 examples: {'rewards_train/chosen': '0.12739', 'rewards_train/rejected': '0.1009', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.026484', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-136.46', 'loss/train': '0.69751', 'examples_per_second': '31.193', 'grad_norm': '96.5', 'counters/examples': 193248, 'counters/updates': 6039}
train stats after 193280 examples: {'rewards_train/chosen': '0.027181', 'rewards_train/rejected': '0.020395', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0067851', 'logps_train/rejected': '-181.08', 'logps_train/chosen': '-155.29', 'loss/train': '0.72481', 'examples_per_second': '30.655', 'grad_norm': '108', 'counters/examples': 193280, 'counters/updates': 6040}
train stats after 193312 examples: {'rewards_train/chosen': '0.093915', 'rewards_train/rejected': '0.13039', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.036476', 'logps_train/rejected': '-166.57', 'logps_train/chosen': '-144.09', 'loss/train': '0.72207', 'examples_per_second': '31.668', 'grad_norm': '296', 'counters/examples': 193312, 'counters/updates': 6041}
train stats after 193344 examples: {'rewards_train/chosen': '0.22668', 'rewards_train/rejected': '0.075852', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15083', 'logps_train/rejected': '-167.8', 'logps_train/chosen': '-162.71', 'loss/train': '0.65308', 'examples_per_second': '30.112', 'grad_norm': '106.5', 'counters/examples': 193344, 'counters/updates': 6042}
train stats after 193376 examples: {'rewards_train/chosen': '0.068603', 'rewards_train/rejected': '0.052472', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016131', 'logps_train/rejected': '-155.7', 'logps_train/chosen': '-153.77', 'loss/train': '0.69655', 'examples_per_second': '31.481', 'grad_norm': '69.5', 'counters/examples': 193376, 'counters/updates': 6043}
skipping logging after 193408 examples to avoid logging too frequently
train stats after 193440 examples: {'rewards_train/chosen': '0.054324', 'rewards_train/rejected': '-0.015247', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069572', 'logps_train/rejected': '-146.5', 'logps_train/chosen': '-160.75', 'loss/train': '0.67511', 'examples_per_second': '31.266', 'grad_norm': '96', 'counters/examples': 193440, 'counters/updates': 6045}
train stats after 193472 examples: {'rewards_train/chosen': '0.13038', 'rewards_train/rejected': '0.046039', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084345', 'logps_train/rejected': '-159.91', 'logps_train/chosen': '-150.95', 'loss/train': '0.6722', 'examples_per_second': '31.028', 'grad_norm': '79', 'counters/examples': 193472, 'counters/updates': 6046}
train stats after 193504 examples: {'rewards_train/chosen': '0.11942', 'rewards_train/rejected': '0.021673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097747', 'logps_train/rejected': '-102.85', 'logps_train/chosen': '-114.77', 'loss/train': '0.65153', 'examples_per_second': '31.6', 'grad_norm': '45.75', 'counters/examples': 193504, 'counters/updates': 6047}
skipping logging after 193536 examples to avoid logging too frequently
train stats after 193568 examples: {'rewards_train/chosen': '0.089407', 'rewards_train/rejected': '-0.03642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12583', 'logps_train/rejected': '-132.97', 'logps_train/chosen': '-127.62', 'loss/train': '0.64462', 'examples_per_second': '31.63', 'grad_norm': '57.25', 'counters/examples': 193568, 'counters/updates': 6049}
train stats after 193600 examples: {'rewards_train/chosen': '0.11098', 'rewards_train/rejected': '0.031898', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079084', 'logps_train/rejected': '-149.47', 'logps_train/chosen': '-158.31', 'loss/train': '0.67395', 'examples_per_second': '30.539', 'grad_norm': '64.5', 'counters/examples': 193600, 'counters/updates': 6050}
train stats after 193632 examples: {'rewards_train/chosen': '0.18654', 'rewards_train/rejected': '-0.015788', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20233', 'logps_train/rejected': '-148.24', 'logps_train/chosen': '-164.58', 'loss/train': '0.61401', 'examples_per_second': '32.549', 'grad_norm': '99', 'counters/examples': 193632, 'counters/updates': 6051}
train stats after 193664 examples: {'rewards_train/chosen': '0.1181', 'rewards_train/rejected': '0.026459', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091638', 'logps_train/rejected': '-105.62', 'logps_train/chosen': '-110.44', 'loss/train': '0.66606', 'examples_per_second': '30.744', 'grad_norm': '93.5', 'counters/examples': 193664, 'counters/updates': 6052}
train stats after 193696 examples: {'rewards_train/chosen': '0.064063', 'rewards_train/rejected': '0.13904', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.074976', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-112.91', 'loss/train': '0.745', 'examples_per_second': '32.304', 'grad_norm': '81.5', 'counters/examples': 193696, 'counters/updates': 6053}
train stats after 193728 examples: {'rewards_train/chosen': '0.10743', 'rewards_train/rejected': '0.020723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086705', 'logps_train/rejected': '-129.93', 'logps_train/chosen': '-195.74', 'loss/train': '0.66464', 'examples_per_second': '31.559', 'grad_norm': '127.5', 'counters/examples': 193728, 'counters/updates': 6054}
train stats after 193760 examples: {'rewards_train/chosen': '0.061322', 'rewards_train/rejected': '-0.026856', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088178', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-146.74', 'loss/train': '0.66322', 'examples_per_second': '31.62', 'grad_norm': '129', 'counters/examples': 193760, 'counters/updates': 6055}
train stats after 193792 examples: {'rewards_train/chosen': '0.093844', 'rewards_train/rejected': '0.054284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03956', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-160.71', 'loss/train': '0.68293', 'examples_per_second': '32.269', 'grad_norm': '191', 'counters/examples': 193792, 'counters/updates': 6056}
train stats after 193824 examples: {'rewards_train/chosen': '0.033355', 'rewards_train/rejected': '0.14204', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.10868', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-149.74', 'loss/train': '0.75826', 'examples_per_second': '30.277', 'grad_norm': '109', 'counters/examples': 193824, 'counters/updates': 6057}
train stats after 193856 examples: {'rewards_train/chosen': '0.21734', 'rewards_train/rejected': '0.092631', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1247', 'logps_train/rejected': '-157.44', 'logps_train/chosen': '-180.47', 'loss/train': '0.64609', 'examples_per_second': '31.476', 'grad_norm': '95.5', 'counters/examples': 193856, 'counters/updates': 6058}
train stats after 193888 examples: {'rewards_train/chosen': '0.090688', 'rewards_train/rejected': '0.097261', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0065725', 'logps_train/rejected': '-106.68', 'logps_train/chosen': '-151.7', 'loss/train': '0.70871', 'examples_per_second': '27.268', 'grad_norm': '215', 'counters/examples': 193888, 'counters/updates': 6059}
skipping logging after 193920 examples to avoid logging too frequently
train stats after 193952 examples: {'rewards_train/chosen': '0.093865', 'rewards_train/rejected': '0.060124', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033741', 'logps_train/rejected': '-156.64', 'logps_train/chosen': '-135.27', 'loss/train': '0.6856', 'examples_per_second': '31.559', 'grad_norm': '74.5', 'counters/examples': 193952, 'counters/updates': 6061}
train stats after 193984 examples: {'rewards_train/chosen': '0.063566', 'rewards_train/rejected': '-0.025353', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088919', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-157.07', 'loss/train': '0.6645', 'examples_per_second': '32.699', 'grad_norm': '161', 'counters/examples': 193984, 'counters/updates': 6062}
skipping logging after 194016 examples to avoid logging too frequently
train stats after 194048 examples: {'rewards_train/chosen': '0.054432', 'rewards_train/rejected': '0.063646', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009214', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-163.05', 'loss/train': '0.71474', 'examples_per_second': '34.263', 'grad_norm': '80', 'counters/examples': 194048, 'counters/updates': 6064}
train stats after 194080 examples: {'rewards_train/chosen': '0.21036', 'rewards_train/rejected': '0.096672', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11369', 'logps_train/rejected': '-134.29', 'logps_train/chosen': '-137.65', 'loss/train': '0.65725', 'examples_per_second': '30.125', 'grad_norm': '54.25', 'counters/examples': 194080, 'counters/updates': 6065}
train stats after 194112 examples: {'rewards_train/chosen': '0.08881', 'rewards_train/rejected': '0.13907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.050263', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-152.75', 'loss/train': '0.73165', 'examples_per_second': '30.903', 'grad_norm': '93.5', 'counters/examples': 194112, 'counters/updates': 6066}
train stats after 194144 examples: {'rewards_train/chosen': '0.096668', 'rewards_train/rejected': '0.042349', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.054319', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-133.42', 'loss/train': '0.67877', 'examples_per_second': '31.731', 'grad_norm': '60.5', 'counters/examples': 194144, 'counters/updates': 6067}
train stats after 194176 examples: {'rewards_train/chosen': '0.13052', 'rewards_train/rejected': '0.073309', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057211', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-180.89', 'loss/train': '0.67338', 'examples_per_second': '31.489', 'grad_norm': '70', 'counters/examples': 194176, 'counters/updates': 6068}
skipping logging after 194208 examples to avoid logging too frequently
train stats after 194240 examples: {'rewards_train/chosen': '0.12607', 'rewards_train/rejected': '-0.043109', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16918', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-138.71', 'loss/train': '0.62084', 'examples_per_second': '30.415', 'grad_norm': '61.5', 'counters/examples': 194240, 'counters/updates': 6070}
train stats after 194272 examples: {'rewards_train/chosen': '0.1481', 'rewards_train/rejected': '0.076311', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07179', 'logps_train/rejected': '-128.4', 'logps_train/chosen': '-172.72', 'loss/train': '0.68597', 'examples_per_second': '32.583', 'grad_norm': '77', 'counters/examples': 194272, 'counters/updates': 6071}
skipping logging after 194304 examples to avoid logging too frequently
train stats after 194336 examples: {'rewards_train/chosen': '0.047302', 'rewards_train/rejected': '0.10907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.061766', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-146.07', 'loss/train': '0.7387', 'examples_per_second': '30.115', 'grad_norm': '66.5', 'counters/examples': 194336, 'counters/updates': 6073}
skipping logging after 194368 examples to avoid logging too frequently
train stats after 194400 examples: {'rewards_train/chosen': '0.054187', 'rewards_train/rejected': '0.092054', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037867', 'logps_train/rejected': '-160.58', 'logps_train/chosen': '-186.23', 'loss/train': '0.73148', 'examples_per_second': '30.106', 'grad_norm': '135', 'counters/examples': 194400, 'counters/updates': 6075}
train stats after 194432 examples: {'rewards_train/chosen': '0.10262', 'rewards_train/rejected': '0.067365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035254', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-172.07', 'loss/train': '0.68979', 'examples_per_second': '31.545', 'grad_norm': '109.5', 'counters/examples': 194432, 'counters/updates': 6076}
skipping logging after 194464 examples to avoid logging too frequently
train stats after 194496 examples: {'rewards_train/chosen': '0.09704', 'rewards_train/rejected': '0.050267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046773', 'logps_train/rejected': '-96.39', 'logps_train/chosen': '-121.27', 'loss/train': '0.67463', 'examples_per_second': '33.165', 'grad_norm': '58.5', 'counters/examples': 194496, 'counters/updates': 6078}
train stats after 194528 examples: {'rewards_train/chosen': '0.13474', 'rewards_train/rejected': '0.2023', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.067568', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-146.44', 'loss/train': '0.73689', 'examples_per_second': '32.177', 'grad_norm': '108.5', 'counters/examples': 194528, 'counters/updates': 6079}
train stats after 194560 examples: {'rewards_train/chosen': '0.11865', 'rewards_train/rejected': '0.043944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074704', 'logps_train/rejected': '-147.18', 'logps_train/chosen': '-170.56', 'loss/train': '0.67661', 'examples_per_second': '31.323', 'grad_norm': '65', 'counters/examples': 194560, 'counters/updates': 6080}
train stats after 194592 examples: {'rewards_train/chosen': '0.13081', 'rewards_train/rejected': '0.029998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10081', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-168.23', 'loss/train': '0.65686', 'examples_per_second': '31.662', 'grad_norm': '70', 'counters/examples': 194592, 'counters/updates': 6081}
train stats after 194624 examples: {'rewards_train/chosen': '0.033298', 'rewards_train/rejected': '-0.025852', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05915', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-139.15', 'loss/train': '0.67885', 'examples_per_second': '31.089', 'grad_norm': '82', 'counters/examples': 194624, 'counters/updates': 6082}
train stats after 194656 examples: {'rewards_train/chosen': '0.095139', 'rewards_train/rejected': '0.016289', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078851', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-116.22', 'loss/train': '0.66615', 'examples_per_second': '30.927', 'grad_norm': '77.5', 'counters/examples': 194656, 'counters/updates': 6083}
train stats after 194688 examples: {'rewards_train/chosen': '0.18525', 'rewards_train/rejected': '0.04035', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1449', 'logps_train/rejected': '-150.55', 'logps_train/chosen': '-190.04', 'loss/train': '0.6568', 'examples_per_second': '30.479', 'grad_norm': '106', 'counters/examples': 194688, 'counters/updates': 6084}
train stats after 194720 examples: {'rewards_train/chosen': '0.19468', 'rewards_train/rejected': '0.047081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1476', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-160.08', 'loss/train': '0.64546', 'examples_per_second': '31.645', 'grad_norm': '179', 'counters/examples': 194720, 'counters/updates': 6085}
train stats after 194752 examples: {'rewards_train/chosen': '0.081104', 'rewards_train/rejected': '0.082286', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011819', 'logps_train/rejected': '-164.93', 'logps_train/chosen': '-167.46', 'loss/train': '0.7187', 'examples_per_second': '30.716', 'grad_norm': '170', 'counters/examples': 194752, 'counters/updates': 6086}
train stats after 194784 examples: {'rewards_train/chosen': '0.084769', 'rewards_train/rejected': '0.031431', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053338', 'logps_train/rejected': '-148.96', 'logps_train/chosen': '-127.42', 'loss/train': '0.6761', 'examples_per_second': '31.24', 'grad_norm': '73', 'counters/examples': 194784, 'counters/updates': 6087}
train stats after 194816 examples: {'rewards_train/chosen': '0.13781', 'rewards_train/rejected': '-0.0095584', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14737', 'logps_train/rejected': '-136.65', 'logps_train/chosen': '-171.13', 'loss/train': '0.64183', 'examples_per_second': '31.648', 'grad_norm': '76.5', 'counters/examples': 194816, 'counters/updates': 6088}
train stats after 194848 examples: {'rewards_train/chosen': '0.17134', 'rewards_train/rejected': '0.0013025', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17003', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-196.34', 'loss/train': '0.6323', 'examples_per_second': '31.647', 'grad_norm': '147', 'counters/examples': 194848, 'counters/updates': 6089}
train stats after 194880 examples: {'rewards_train/chosen': '0.21423', 'rewards_train/rejected': '0.11329', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10094', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-173.64', 'loss/train': '0.66059', 'examples_per_second': '30.266', 'grad_norm': '102.5', 'counters/examples': 194880, 'counters/updates': 6090}
train stats after 194912 examples: {'rewards_train/chosen': '0.073887', 'rewards_train/rejected': '0.025545', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048342', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-156.96', 'loss/train': '0.68072', 'examples_per_second': '31.613', 'grad_norm': '91', 'counters/examples': 194912, 'counters/updates': 6091}
skipping logging after 194944 examples to avoid logging too frequently
train stats after 194976 examples: {'rewards_train/chosen': '0.092048', 'rewards_train/rejected': '0.10169', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0096416', 'logps_train/rejected': '-101.38', 'logps_train/chosen': '-144.29', 'loss/train': '0.71237', 'examples_per_second': '35.977', 'grad_norm': '70', 'counters/examples': 194976, 'counters/updates': 6093}
train stats after 195008 examples: {'rewards_train/chosen': '0.14', 'rewards_train/rejected': '0.050751', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089248', 'logps_train/rejected': '-111.11', 'logps_train/chosen': '-136.51', 'loss/train': '0.65822', 'examples_per_second': '30.592', 'grad_norm': '81', 'counters/examples': 195008, 'counters/updates': 6094}
skipping logging after 195040 examples to avoid logging too frequently
train stats after 195072 examples: {'rewards_train/chosen': '0.13551', 'rewards_train/rejected': '0.067072', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068443', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-161.88', 'loss/train': '0.67893', 'examples_per_second': '32.339', 'grad_norm': '115', 'counters/examples': 195072, 'counters/updates': 6096}
train stats after 195104 examples: {'rewards_train/chosen': '0.11129', 'rewards_train/rejected': '0.083042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028244', 'logps_train/rejected': '-145.85', 'logps_train/chosen': '-186.24', 'loss/train': '0.68829', 'examples_per_second': '31.28', 'grad_norm': '80.5', 'counters/examples': 195104, 'counters/updates': 6097}
train stats after 195136 examples: {'rewards_train/chosen': '0.14998', 'rewards_train/rejected': '0.10188', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048094', 'logps_train/rejected': '-121.83', 'logps_train/chosen': '-130.94', 'loss/train': '0.68046', 'examples_per_second': '31.064', 'grad_norm': '125.5', 'counters/examples': 195136, 'counters/updates': 6098}
skipping logging after 195168 examples to avoid logging too frequently
train stats after 195200 examples: {'rewards_train/chosen': '0.23805', 'rewards_train/rejected': '0.003058', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23499', 'logps_train/rejected': '-117.34', 'logps_train/chosen': '-161.29', 'loss/train': '0.59985', 'examples_per_second': '31.523', 'grad_norm': '102.5', 'counters/examples': 195200, 'counters/updates': 6100}
train stats after 195232 examples: {'rewards_train/chosen': '0.10411', 'rewards_train/rejected': '0.034232', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069877', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-127.39', 'loss/train': '0.67893', 'examples_per_second': '31.503', 'grad_norm': '95.5', 'counters/examples': 195232, 'counters/updates': 6101}
train stats after 195264 examples: {'rewards_train/chosen': '0.18861', 'rewards_train/rejected': '0.13987', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048736', 'logps_train/rejected': '-147.25', 'logps_train/chosen': '-158.89', 'loss/train': '0.68587', 'examples_per_second': '30.499', 'grad_norm': '106', 'counters/examples': 195264, 'counters/updates': 6102}
train stats after 195296 examples: {'rewards_train/chosen': '0.11803', 'rewards_train/rejected': '0.07726', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.040767', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-139.69', 'loss/train': '0.68252', 'examples_per_second': '29.642', 'grad_norm': '78.5', 'counters/examples': 195296, 'counters/updates': 6103}
train stats after 195328 examples: {'rewards_train/chosen': '0.1617', 'rewards_train/rejected': '0.070689', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091008', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-124.73', 'loss/train': '0.66322', 'examples_per_second': '29.972', 'grad_norm': '72', 'counters/examples': 195328, 'counters/updates': 6104}
train stats after 195360 examples: {'rewards_train/chosen': '0.11021', 'rewards_train/rejected': '0.031943', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078268', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-148.65', 'loss/train': '0.6651', 'examples_per_second': '32.525', 'grad_norm': '65.5', 'counters/examples': 195360, 'counters/updates': 6105}
train stats after 195392 examples: {'rewards_train/chosen': '0.045291', 'rewards_train/rejected': '0.087245', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.041954', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-124.6', 'loss/train': '0.7324', 'examples_per_second': '31.32', 'grad_norm': '107.5', 'counters/examples': 195392, 'counters/updates': 6106}
train stats after 195424 examples: {'rewards_train/chosen': '0.13953', 'rewards_train/rejected': '-0.01147', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.151', 'logps_train/rejected': '-107.39', 'logps_train/chosen': '-139.14', 'loss/train': '0.63476', 'examples_per_second': '31.688', 'grad_norm': '144', 'counters/examples': 195424, 'counters/updates': 6107}
train stats after 195456 examples: {'rewards_train/chosen': '0.090133', 'rewards_train/rejected': '0.066265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.023867', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-170.21', 'loss/train': '0.69944', 'examples_per_second': '30.516', 'grad_norm': '104.5', 'counters/examples': 195456, 'counters/updates': 6108}
train stats after 195488 examples: {'rewards_train/chosen': '0.24775', 'rewards_train/rejected': '0.061786', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18597', 'logps_train/rejected': '-159.72', 'logps_train/chosen': '-195.12', 'loss/train': '0.63486', 'examples_per_second': '30.039', 'grad_norm': '64', 'counters/examples': 195488, 'counters/updates': 6109}
train stats after 195520 examples: {'rewards_train/chosen': '0.12004', 'rewards_train/rejected': '0.13316', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.013118', 'logps_train/rejected': '-173.62', 'logps_train/chosen': '-159.6', 'loss/train': '0.70756', 'examples_per_second': '31.558', 'grad_norm': '104', 'counters/examples': 195520, 'counters/updates': 6110}
train stats after 195552 examples: {'rewards_train/chosen': '0.10051', 'rewards_train/rejected': '0.088926', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011586', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-161.43', 'loss/train': '0.69598', 'examples_per_second': '31.273', 'grad_norm': '82', 'counters/examples': 195552, 'counters/updates': 6111}
train stats after 195584 examples: {'rewards_train/chosen': '0.15244', 'rewards_train/rejected': '0.13353', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018917', 'logps_train/rejected': '-169.72', 'logps_train/chosen': '-174.52', 'loss/train': '0.71348', 'examples_per_second': '32.653', 'grad_norm': '95', 'counters/examples': 195584, 'counters/updates': 6112}
train stats after 195616 examples: {'rewards_train/chosen': '0.12046', 'rewards_train/rejected': '0.091897', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02856', 'logps_train/rejected': '-149.96', 'logps_train/chosen': '-158.17', 'loss/train': '0.70308', 'examples_per_second': '32.833', 'grad_norm': '82', 'counters/examples': 195616, 'counters/updates': 6113}
train stats after 195648 examples: {'rewards_train/chosen': '0.10989', 'rewards_train/rejected': '0.057494', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052391', 'logps_train/rejected': '-116.23', 'logps_train/chosen': '-153.25', 'loss/train': '0.6854', 'examples_per_second': '30.925', 'grad_norm': '58.5', 'counters/examples': 195648, 'counters/updates': 6114}
train stats after 195680 examples: {'rewards_train/chosen': '0.088153', 'rewards_train/rejected': '0.14216', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.054004', 'logps_train/rejected': '-163.24', 'logps_train/chosen': '-157.56', 'loss/train': '0.73094', 'examples_per_second': '31.525', 'grad_norm': '78', 'counters/examples': 195680, 'counters/updates': 6115}
train stats after 195712 examples: {'rewards_train/chosen': '0.10094', 'rewards_train/rejected': '0.091403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0095367', 'logps_train/rejected': '-144.64', 'logps_train/chosen': '-185.38', 'loss/train': '0.70377', 'examples_per_second': '30.156', 'grad_norm': '104.5', 'counters/examples': 195712, 'counters/updates': 6116}
train stats after 195744 examples: {'rewards_train/chosen': '0.0054262', 'rewards_train/rejected': '0.016363', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010937', 'logps_train/rejected': '-108.45', 'logps_train/chosen': '-141.67', 'loss/train': '0.71134', 'examples_per_second': '30.296', 'grad_norm': '109.5', 'counters/examples': 195744, 'counters/updates': 6117}
train stats after 195776 examples: {'rewards_train/chosen': '0.19628', 'rewards_train/rejected': '0.047134', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14915', 'logps_train/rejected': '-153.4', 'logps_train/chosen': '-150.68', 'loss/train': '0.63734', 'examples_per_second': '30.476', 'grad_norm': '80.5', 'counters/examples': 195776, 'counters/updates': 6118}
train stats after 195808 examples: {'rewards_train/chosen': '0.10177', 'rewards_train/rejected': '0.14476', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04299', 'logps_train/rejected': '-159.91', 'logps_train/chosen': '-147.24', 'loss/train': '0.72731', 'examples_per_second': '30.191', 'grad_norm': '142', 'counters/examples': 195808, 'counters/updates': 6119}
train stats after 195840 examples: {'rewards_train/chosen': '0.086295', 'rewards_train/rejected': '0.030427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055869', 'logps_train/rejected': '-101.15', 'logps_train/chosen': '-119.08', 'loss/train': '0.67155', 'examples_per_second': '31.086', 'grad_norm': '57.5', 'counters/examples': 195840, 'counters/updates': 6120}
train stats after 195872 examples: {'rewards_train/chosen': '0.073239', 'rewards_train/rejected': '0.039383', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033856', 'logps_train/rejected': '-142.36', 'logps_train/chosen': '-183.66', 'loss/train': '0.6973', 'examples_per_second': '31.577', 'grad_norm': '160', 'counters/examples': 195872, 'counters/updates': 6121}
train stats after 195904 examples: {'rewards_train/chosen': '0.15641', 'rewards_train/rejected': '0.036234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12018', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-162.07', 'loss/train': '0.65434', 'examples_per_second': '31.575', 'grad_norm': '70', 'counters/examples': 195904, 'counters/updates': 6122}
train stats after 195936 examples: {'rewards_train/chosen': '0.083912', 'rewards_train/rejected': '0.062466', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021446', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-156.6', 'loss/train': '0.69531', 'examples_per_second': '31.864', 'grad_norm': '72.5', 'counters/examples': 195936, 'counters/updates': 6123}
skipping logging after 195968 examples to avoid logging too frequently
train stats after 196000 examples: {'rewards_train/chosen': '0.12953', 'rewards_train/rejected': '0.020912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10862', 'logps_train/rejected': '-102.38', 'logps_train/chosen': '-101.53', 'loss/train': '0.64838', 'examples_per_second': '34.185', 'grad_norm': '72.5', 'counters/examples': 196000, 'counters/updates': 6125}
skipping logging after 196032 examples to avoid logging too frequently
train stats after 196064 examples: {'rewards_train/chosen': '0.096355', 'rewards_train/rejected': '0.064974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031381', 'logps_train/rejected': '-117.9', 'logps_train/chosen': '-134.64', 'loss/train': '0.68842', 'examples_per_second': '31.24', 'grad_norm': '84.5', 'counters/examples': 196064, 'counters/updates': 6127}
train stats after 196096 examples: {'rewards_train/chosen': '0.10915', 'rewards_train/rejected': '0.064659', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044491', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-149.9', 'loss/train': '0.68797', 'examples_per_second': '31.046', 'grad_norm': '59.5', 'counters/examples': 196096, 'counters/updates': 6128}
skipping logging after 196128 examples to avoid logging too frequently
train stats after 196160 examples: {'rewards_train/chosen': '0.19037', 'rewards_train/rejected': '0.038507', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15187', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-181.3', 'loss/train': '0.63625', 'examples_per_second': '33.57', 'grad_norm': '74.5', 'counters/examples': 196160, 'counters/updates': 6130}
train stats after 196192 examples: {'rewards_train/chosen': '0.14963', 'rewards_train/rejected': '0.10094', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048695', 'logps_train/rejected': '-157.8', 'logps_train/chosen': '-171.44', 'loss/train': '0.68374', 'examples_per_second': '31.593', 'grad_norm': '94', 'counters/examples': 196192, 'counters/updates': 6131}
train stats after 196224 examples: {'rewards_train/chosen': '0.083113', 'rewards_train/rejected': '-0.038662', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12178', 'logps_train/rejected': '-162.36', 'logps_train/chosen': '-138.87', 'loss/train': '0.64994', 'examples_per_second': '32.486', 'grad_norm': '89.5', 'counters/examples': 196224, 'counters/updates': 6132}
train stats after 196256 examples: {'rewards_train/chosen': '0.1419', 'rewards_train/rejected': '0.087984', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053913', 'logps_train/rejected': '-139.62', 'logps_train/chosen': '-149.4', 'loss/train': '0.67482', 'examples_per_second': '32.934', 'grad_norm': '64.5', 'counters/examples': 196256, 'counters/updates': 6133}
train stats after 196288 examples: {'rewards_train/chosen': '0.1438', 'rewards_train/rejected': '0.11849', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.025302', 'logps_train/rejected': '-140.82', 'logps_train/chosen': '-154.88', 'loss/train': '0.70801', 'examples_per_second': '30.422', 'grad_norm': '133', 'counters/examples': 196288, 'counters/updates': 6134}
train stats after 196320 examples: {'rewards_train/chosen': '0.097237', 'rewards_train/rejected': '0.1159', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018665', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-105.61', 'loss/train': '0.71369', 'examples_per_second': '32.332', 'grad_norm': '71.5', 'counters/examples': 196320, 'counters/updates': 6135}
train stats after 196352 examples: {'rewards_train/chosen': '0.11241', 'rewards_train/rejected': '0.02805', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084365', 'logps_train/rejected': '-79.539', 'logps_train/chosen': '-137.63', 'loss/train': '0.6645', 'examples_per_second': '32.699', 'grad_norm': '61.5', 'counters/examples': 196352, 'counters/updates': 6136}
skipping logging after 196384 examples to avoid logging too frequently
train stats after 196416 examples: {'rewards_train/chosen': '0.15864', 'rewards_train/rejected': '0.074382', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084256', 'logps_train/rejected': '-148.18', 'logps_train/chosen': '-129.25', 'loss/train': '0.66022', 'examples_per_second': '31.6', 'grad_norm': '160', 'counters/examples': 196416, 'counters/updates': 6138}
train stats after 196448 examples: {'rewards_train/chosen': '0.074729', 'rewards_train/rejected': '0.15942', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.084689', 'logps_train/rejected': '-172.89', 'logps_train/chosen': '-159.64', 'loss/train': '0.75397', 'examples_per_second': '31.06', 'grad_norm': '274', 'counters/examples': 196448, 'counters/updates': 6139}
train stats after 196480 examples: {'rewards_train/chosen': '0.098444', 'rewards_train/rejected': '0.055475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042969', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-133.08', 'loss/train': '0.68131', 'examples_per_second': '30.698', 'grad_norm': '80.5', 'counters/examples': 196480, 'counters/updates': 6140}
train stats after 196512 examples: {'rewards_train/chosen': '0.1257', 'rewards_train/rejected': '0.075067', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050628', 'logps_train/rejected': '-168.95', 'logps_train/chosen': '-149.68', 'loss/train': '0.68965', 'examples_per_second': '32.809', 'grad_norm': '71', 'counters/examples': 196512, 'counters/updates': 6141}
train stats after 196544 examples: {'rewards_train/chosen': '0.039807', 'rewards_train/rejected': '0.042897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0030894', 'logps_train/rejected': '-169.4', 'logps_train/chosen': '-139.94', 'loss/train': '0.70868', 'examples_per_second': '31.268', 'grad_norm': '88', 'counters/examples': 196544, 'counters/updates': 6142}
train stats after 196576 examples: {'rewards_train/chosen': '0.055619', 'rewards_train/rejected': '0.033071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022547', 'logps_train/rejected': '-152.56', 'logps_train/chosen': '-162.5', 'loss/train': '0.69628', 'examples_per_second': '31.506', 'grad_norm': '93', 'counters/examples': 196576, 'counters/updates': 6143}
train stats after 196608 examples: {'rewards_train/chosen': '0.082182', 'rewards_train/rejected': '0.09589', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013708', 'logps_train/rejected': '-148.68', 'logps_train/chosen': '-165.97', 'loss/train': '0.71286', 'examples_per_second': '31.485', 'grad_norm': '75.5', 'counters/examples': 196608, 'counters/updates': 6144}
train stats after 196640 examples: {'rewards_train/chosen': '0.1213', 'rewards_train/rejected': '0.042693', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078604', 'logps_train/rejected': '-157.97', 'logps_train/chosen': '-174.39', 'loss/train': '0.6841', 'examples_per_second': '31.534', 'grad_norm': '112', 'counters/examples': 196640, 'counters/updates': 6145}
train stats after 196672 examples: {'rewards_train/chosen': '0.11367', 'rewards_train/rejected': '0.011716', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10196', 'logps_train/rejected': '-128.03', 'logps_train/chosen': '-169.47', 'loss/train': '0.66134', 'examples_per_second': '31.707', 'grad_norm': '109', 'counters/examples': 196672, 'counters/updates': 6146}
train stats after 196704 examples: {'rewards_train/chosen': '0.044775', 'rewards_train/rejected': '0.053628', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.008853', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-140.68', 'loss/train': '0.70964', 'examples_per_second': '32.691', 'grad_norm': '77', 'counters/examples': 196704, 'counters/updates': 6147}
train stats after 196736 examples: {'rewards_train/chosen': '0.099061', 'rewards_train/rejected': '0.079532', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019529', 'logps_train/rejected': '-166.45', 'logps_train/chosen': '-177.12', 'loss/train': '0.69384', 'examples_per_second': '24.448', 'grad_norm': '195', 'counters/examples': 196736, 'counters/updates': 6148}
train stats after 196768 examples: {'rewards_train/chosen': '0.035955', 'rewards_train/rejected': '0.050093', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014138', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-148.37', 'loss/train': '0.71082', 'examples_per_second': '30.092', 'grad_norm': '95', 'counters/examples': 196768, 'counters/updates': 6149}
skipping logging after 196800 examples to avoid logging too frequently
train stats after 196832 examples: {'rewards_train/chosen': '0.13753', 'rewards_train/rejected': '0.084159', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053367', 'logps_train/rejected': '-135.45', 'logps_train/chosen': '-144.55', 'loss/train': '0.67814', 'examples_per_second': '24.142', 'grad_norm': '61', 'counters/examples': 196832, 'counters/updates': 6151}
train stats after 196864 examples: {'rewards_train/chosen': '0.0059489', 'rewards_train/rejected': '0.0052475', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0007014', 'logps_train/rejected': '-151.07', 'logps_train/chosen': '-169.38', 'loss/train': '0.71338', 'examples_per_second': '30.595', 'grad_norm': '118', 'counters/examples': 196864, 'counters/updates': 6152}
skipping logging after 196896 examples to avoid logging too frequently
train stats after 196928 examples: {'rewards_train/chosen': '0.026943', 'rewards_train/rejected': '0.078215', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.051272', 'logps_train/rejected': '-124.88', 'logps_train/chosen': '-145.83', 'loss/train': '0.73425', 'examples_per_second': '31.64', 'grad_norm': '171', 'counters/examples': 196928, 'counters/updates': 6154}
train stats after 196960 examples: {'rewards_train/chosen': '0.090257', 'rewards_train/rejected': '0.062234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028022', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-176.89', 'loss/train': '0.69848', 'examples_per_second': '31.558', 'grad_norm': '132', 'counters/examples': 196960, 'counters/updates': 6155}
train stats after 196992 examples: {'rewards_train/chosen': '0.11734', 'rewards_train/rejected': '0.074827', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042517', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-149.59', 'loss/train': '0.68946', 'examples_per_second': '32.013', 'grad_norm': '334', 'counters/examples': 196992, 'counters/updates': 6156}
train stats after 197024 examples: {'rewards_train/chosen': '0.14596', 'rewards_train/rejected': '0.036165', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10979', 'logps_train/rejected': '-149.88', 'logps_train/chosen': '-168.99', 'loss/train': '0.65315', 'examples_per_second': '30.586', 'grad_norm': '72.5', 'counters/examples': 197024, 'counters/updates': 6157}
train stats after 197056 examples: {'rewards_train/chosen': '0.17057', 'rewards_train/rejected': '-0.025148', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19572', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-167.53', 'loss/train': '0.61323', 'examples_per_second': '30.521', 'grad_norm': '584', 'counters/examples': 197056, 'counters/updates': 6158}
train stats after 197088 examples: {'rewards_train/chosen': '0.092642', 'rewards_train/rejected': '0.0051949', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087447', 'logps_train/rejected': '-81.472', 'logps_train/chosen': '-149.1', 'loss/train': '0.6591', 'examples_per_second': '31.535', 'grad_norm': '78.5', 'counters/examples': 197088, 'counters/updates': 6159}
train stats after 197120 examples: {'rewards_train/chosen': '0.043592', 'rewards_train/rejected': '0.027423', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01617', 'logps_train/rejected': '-172.82', 'logps_train/chosen': '-132.25', 'loss/train': '0.70128', 'examples_per_second': '31.28', 'grad_norm': '62', 'counters/examples': 197120, 'counters/updates': 6160}
train stats after 197152 examples: {'rewards_train/chosen': '0.041688', 'rewards_train/rejected': '0.038441', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.003247', 'logps_train/rejected': '-103.17', 'logps_train/chosen': '-112.73', 'loss/train': '0.70088', 'examples_per_second': '31.139', 'grad_norm': '72.5', 'counters/examples': 197152, 'counters/updates': 6161}
train stats after 197184 examples: {'rewards_train/chosen': '0.089582', 'rewards_train/rejected': '0.04571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043871', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-112.28', 'loss/train': '0.6784', 'examples_per_second': '30.422', 'grad_norm': '124', 'counters/examples': 197184, 'counters/updates': 6162}
train stats after 197216 examples: {'rewards_train/chosen': '0.083868', 'rewards_train/rejected': '0.017214', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.066654', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-151.57', 'loss/train': '0.67236', 'examples_per_second': '31.291', 'grad_norm': '65.5', 'counters/examples': 197216, 'counters/updates': 6163}
train stats after 197248 examples: {'rewards_train/chosen': '0.16936', 'rewards_train/rejected': '0.08269', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086672', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-176.79', 'loss/train': '0.6604', 'examples_per_second': '29.931', 'grad_norm': '143', 'counters/examples': 197248, 'counters/updates': 6164}
train stats after 197280 examples: {'rewards_train/chosen': '0.13394', 'rewards_train/rejected': '-0.0037777', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13771', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-154.21', 'loss/train': '0.65108', 'examples_per_second': '31.093', 'grad_norm': '69.5', 'counters/examples': 197280, 'counters/updates': 6165}
train stats after 197312 examples: {'rewards_train/chosen': '0.10851', 'rewards_train/rejected': '0.075455', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033054', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-161.23', 'loss/train': '0.69186', 'examples_per_second': '30.058', 'grad_norm': '104.5', 'counters/examples': 197312, 'counters/updates': 6166}
train stats after 197344 examples: {'rewards_train/chosen': '0.097976', 'rewards_train/rejected': '0.04867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049306', 'logps_train/rejected': '-116.62', 'logps_train/chosen': '-139.11', 'loss/train': '0.68382', 'examples_per_second': '31.536', 'grad_norm': '110', 'counters/examples': 197344, 'counters/updates': 6167}
train stats after 197376 examples: {'rewards_train/chosen': '0.053301', 'rewards_train/rejected': '0.0025758', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050725', 'logps_train/rejected': '-139.6', 'logps_train/chosen': '-149.54', 'loss/train': '0.67844', 'examples_per_second': '31.347', 'grad_norm': '54', 'counters/examples': 197376, 'counters/updates': 6168}
train stats after 197408 examples: {'rewards_train/chosen': '0.11654', 'rewards_train/rejected': '0.05916', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057375', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-145.43', 'loss/train': '0.67369', 'examples_per_second': '30.226', 'grad_norm': '58.5', 'counters/examples': 197408, 'counters/updates': 6169}
train stats after 197440 examples: {'rewards_train/chosen': '0.13782', 'rewards_train/rejected': '0.091081', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046737', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-140.69', 'loss/train': '0.6846', 'examples_per_second': '31.149', 'grad_norm': '79', 'counters/examples': 197440, 'counters/updates': 6170}
skipping logging after 197472 examples to avoid logging too frequently
train stats after 197504 examples: {'rewards_train/chosen': '0.096735', 'rewards_train/rejected': '-0.039711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13645', 'logps_train/rejected': '-140.62', 'logps_train/chosen': '-137.08', 'loss/train': '0.63759', 'examples_per_second': '34.479', 'grad_norm': '95', 'counters/examples': 197504, 'counters/updates': 6172}
skipping logging after 197536 examples to avoid logging too frequently
train stats after 197568 examples: {'rewards_train/chosen': '0.23419', 'rewards_train/rejected': '0.047273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18692', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-174.15', 'loss/train': '0.62552', 'examples_per_second': '31.641', 'grad_norm': '82', 'counters/examples': 197568, 'counters/updates': 6174}
train stats after 197600 examples: {'rewards_train/chosen': '0.10416', 'rewards_train/rejected': '0.090828', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01333', 'logps_train/rejected': '-135.31', 'logps_train/chosen': '-159.32', 'loss/train': '0.6994', 'examples_per_second': '32.391', 'grad_norm': '89', 'counters/examples': 197600, 'counters/updates': 6175}
train stats after 197632 examples: {'rewards_train/chosen': '0.007047', 'rewards_train/rejected': '0.032258', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025211', 'logps_train/rejected': '-157.81', 'logps_train/chosen': '-150.53', 'loss/train': '0.71615', 'examples_per_second': '30.089', 'grad_norm': '70.5', 'counters/examples': 197632, 'counters/updates': 6176}
train stats after 197664 examples: {'rewards_train/chosen': '0.018779', 'rewards_train/rejected': '0.053799', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.03502', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-139.04', 'loss/train': '0.72617', 'examples_per_second': '30.839', 'grad_norm': '98', 'counters/examples': 197664, 'counters/updates': 6177}
train stats after 197696 examples: {'rewards_train/chosen': '0.13217', 'rewards_train/rejected': '-0.026825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15899', 'logps_train/rejected': '-133.63', 'logps_train/chosen': '-145.99', 'loss/train': '0.63106', 'examples_per_second': '31.534', 'grad_norm': '86', 'counters/examples': 197696, 'counters/updates': 6178}
train stats after 197728 examples: {'rewards_train/chosen': '0.057094', 'rewards_train/rejected': '0.0094247', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.047669', 'logps_train/rejected': '-159.1', 'logps_train/chosen': '-131.41', 'loss/train': '0.68479', 'examples_per_second': '31.5', 'grad_norm': '98', 'counters/examples': 197728, 'counters/updates': 6179}
train stats after 197760 examples: {'rewards_train/chosen': '0.12115', 'rewards_train/rejected': '0.0069265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11422', 'logps_train/rejected': '-146.04', 'logps_train/chosen': '-154.38', 'loss/train': '0.6575', 'examples_per_second': '31.492', 'grad_norm': '115.5', 'counters/examples': 197760, 'counters/updates': 6180}
train stats after 197792 examples: {'rewards_train/chosen': '-0.021449', 'rewards_train/rejected': '0.0042064', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025655', 'logps_train/rejected': '-152.51', 'logps_train/chosen': '-140.89', 'loss/train': '0.71592', 'examples_per_second': '31.581', 'grad_norm': '100', 'counters/examples': 197792, 'counters/updates': 6181}
train stats after 197824 examples: {'rewards_train/chosen': '0.12629', 'rewards_train/rejected': '0.070087', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056208', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-122.3', 'loss/train': '0.67199', 'examples_per_second': '31.295', 'grad_norm': '66.5', 'counters/examples': 197824, 'counters/updates': 6182}
train stats after 197856 examples: {'rewards_train/chosen': '0.072472', 'rewards_train/rejected': '-0.019764', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092235', 'logps_train/rejected': '-157.59', 'logps_train/chosen': '-160.7', 'loss/train': '0.65431', 'examples_per_second': '32.274', 'grad_norm': '85', 'counters/examples': 197856, 'counters/updates': 6183}
train stats after 197888 examples: {'rewards_train/chosen': '0.16755', 'rewards_train/rejected': '0.086142', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081406', 'logps_train/rejected': '-162.89', 'logps_train/chosen': '-159.41', 'loss/train': '0.66376', 'examples_per_second': '30.855', 'grad_norm': '95', 'counters/examples': 197888, 'counters/updates': 6184}
skipping logging after 197920 examples to avoid logging too frequently
train stats after 197952 examples: {'rewards_train/chosen': '0.066071', 'rewards_train/rejected': '0.14606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.079991', 'logps_train/rejected': '-144.37', 'logps_train/chosen': '-177.04', 'loss/train': '0.75987', 'examples_per_second': '33.491', 'grad_norm': '98', 'counters/examples': 197952, 'counters/updates': 6186}
train stats after 197984 examples: {'rewards_train/chosen': '0.15672', 'rewards_train/rejected': '0.016105', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14061', 'logps_train/rejected': '-105.95', 'logps_train/chosen': '-134.5', 'loss/train': '0.64172', 'examples_per_second': '29.978', 'grad_norm': '46.5', 'counters/examples': 197984, 'counters/updates': 6187}
train stats after 198016 examples: {'rewards_train/chosen': '0.12818', 'rewards_train/rejected': '0.0084442', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11974', 'logps_train/rejected': '-117.71', 'logps_train/chosen': '-191.61', 'loss/train': '0.65534', 'examples_per_second': '31.522', 'grad_norm': '72', 'counters/examples': 198016, 'counters/updates': 6188}
skipping logging after 198048 examples to avoid logging too frequently
train stats after 198080 examples: {'rewards_train/chosen': '0.074747', 'rewards_train/rejected': '0.10286', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028116', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-135.76', 'loss/train': '0.7249', 'examples_per_second': '31.178', 'grad_norm': '114', 'counters/examples': 198080, 'counters/updates': 6190}
train stats after 198112 examples: {'rewards_train/chosen': '0.086012', 'rewards_train/rejected': '0.11451', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028499', 'logps_train/rejected': '-148.27', 'logps_train/chosen': '-170.39', 'loss/train': '0.72081', 'examples_per_second': '31.332', 'grad_norm': '119.5', 'counters/examples': 198112, 'counters/updates': 6191}
train stats after 198144 examples: {'rewards_train/chosen': '0.078073', 'rewards_train/rejected': '0.0079334', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.07014', 'logps_train/rejected': '-100.26', 'logps_train/chosen': '-141.66', 'loss/train': '0.66764', 'examples_per_second': '31.579', 'grad_norm': '60.5', 'counters/examples': 198144, 'counters/updates': 6192}
train stats after 198176 examples: {'rewards_train/chosen': '0.1244', 'rewards_train/rejected': '0.049495', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074907', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-191.78', 'loss/train': '0.66443', 'examples_per_second': '31.536', 'grad_norm': '87', 'counters/examples': 198176, 'counters/updates': 6193}
train stats after 198208 examples: {'rewards_train/chosen': '0.13051', 'rewards_train/rejected': '0.037775', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092732', 'logps_train/rejected': '-99.971', 'logps_train/chosen': '-171.81', 'loss/train': '0.66132', 'examples_per_second': '32.3', 'grad_norm': '80', 'counters/examples': 198208, 'counters/updates': 6194}
skipping logging after 198240 examples to avoid logging too frequently
train stats after 198272 examples: {'rewards_train/chosen': '0.023958', 'rewards_train/rejected': '0.052794', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028835', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-156.88', 'loss/train': '0.71709', 'examples_per_second': '32.982', 'grad_norm': '83.5', 'counters/examples': 198272, 'counters/updates': 6196}
train stats after 198304 examples: {'rewards_train/chosen': '0.071156', 'rewards_train/rejected': '0.10393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.032777', 'logps_train/rejected': '-107.27', 'logps_train/chosen': '-121.63', 'loss/train': '0.73047', 'examples_per_second': '31.521', 'grad_norm': '72', 'counters/examples': 198304, 'counters/updates': 6197}
train stats after 198336 examples: {'rewards_train/chosen': '0.13137', 'rewards_train/rejected': '0.044109', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087266', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-131.5', 'loss/train': '0.65876', 'examples_per_second': '30.606', 'grad_norm': '77', 'counters/examples': 198336, 'counters/updates': 6198}
train stats after 198368 examples: {'rewards_train/chosen': '0.074609', 'rewards_train/rejected': '0.11462', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.040014', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-150.71', 'loss/train': '0.732', 'examples_per_second': '31.419', 'grad_norm': '140', 'counters/examples': 198368, 'counters/updates': 6199}
train stats after 198400 examples: {'rewards_train/chosen': '0.22917', 'rewards_train/rejected': '0.092972', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1362', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-165.8', 'loss/train': '0.64902', 'examples_per_second': '30.702', 'grad_norm': '52.25', 'counters/examples': 198400, 'counters/updates': 6200}
train stats after 198432 examples: {'rewards_train/chosen': '0.085186', 'rewards_train/rejected': '-0.013392', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098578', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-154.51', 'loss/train': '0.65581', 'examples_per_second': '31.58', 'grad_norm': '72.5', 'counters/examples': 198432, 'counters/updates': 6201}
train stats after 198464 examples: {'rewards_train/chosen': '0.011452', 'rewards_train/rejected': '0.021955', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010504', 'logps_train/rejected': '-103.61', 'logps_train/chosen': '-134.93', 'loss/train': '0.71113', 'examples_per_second': '32.611', 'grad_norm': '70.5', 'counters/examples': 198464, 'counters/updates': 6202}
train stats after 198496 examples: {'rewards_train/chosen': '0.092943', 'rewards_train/rejected': '0.096871', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0039279', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-150.67', 'loss/train': '0.71209', 'examples_per_second': '30.979', 'grad_norm': '75', 'counters/examples': 198496, 'counters/updates': 6203}
train stats after 198528 examples: {'rewards_train/chosen': '0.1142', 'rewards_train/rejected': '0.059601', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054604', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-122.76', 'loss/train': '0.68157', 'examples_per_second': '31.4', 'grad_norm': '124', 'counters/examples': 198528, 'counters/updates': 6204}
train stats after 198560 examples: {'rewards_train/chosen': '0.055965', 'rewards_train/rejected': '0.050154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0058116', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-132.83', 'loss/train': '0.70166', 'examples_per_second': '31.786', 'grad_norm': '83', 'counters/examples': 198560, 'counters/updates': 6205}
train stats after 198592 examples: {'rewards_train/chosen': '0.073264', 'rewards_train/rejected': '0.046914', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02635', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-177.25', 'loss/train': '0.69085', 'examples_per_second': '30.428', 'grad_norm': '111', 'counters/examples': 198592, 'counters/updates': 6206}
train stats after 198624 examples: {'rewards_train/chosen': '0.091835', 'rewards_train/rejected': '0.010858', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080977', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-135.17', 'loss/train': '0.66644', 'examples_per_second': '32.291', 'grad_norm': '128', 'counters/examples': 198624, 'counters/updates': 6207}
train stats after 198656 examples: {'rewards_train/chosen': '0.065151', 'rewards_train/rejected': '0.097755', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.032604', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-162.04', 'loss/train': '0.72275', 'examples_per_second': '31.195', 'grad_norm': '61.5', 'counters/examples': 198656, 'counters/updates': 6208}
train stats after 198688 examples: {'rewards_train/chosen': '0.16037', 'rewards_train/rejected': '0.070739', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089632', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-149.89', 'loss/train': '0.6615', 'examples_per_second': '30.372', 'grad_norm': '110.5', 'counters/examples': 198688, 'counters/updates': 6209}
train stats after 198720 examples: {'rewards_train/chosen': '0.073545', 'rewards_train/rejected': '0.12062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04708', 'logps_train/rejected': '-145.53', 'logps_train/chosen': '-183.14', 'loss/train': '0.74799', 'examples_per_second': '31.521', 'grad_norm': '166', 'counters/examples': 198720, 'counters/updates': 6210}
train stats after 198752 examples: {'rewards_train/chosen': '0.09062', 'rewards_train/rejected': '0.078319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012301', 'logps_train/rejected': '-145.68', 'logps_train/chosen': '-150.97', 'loss/train': '0.70143', 'examples_per_second': '31.413', 'grad_norm': '114', 'counters/examples': 198752, 'counters/updates': 6211}
skipping logging after 198784 examples to avoid logging too frequently
train stats after 198816 examples: {'rewards_train/chosen': '0.034368', 'rewards_train/rejected': '0.034985', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00061675', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-185.01', 'loss/train': '0.70688', 'examples_per_second': '32.871', 'grad_norm': '96', 'counters/examples': 198816, 'counters/updates': 6213}
train stats after 198848 examples: {'rewards_train/chosen': '0.14552', 'rewards_train/rejected': '0.055148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090371', 'logps_train/rejected': '-150.2', 'logps_train/chosen': '-142.7', 'loss/train': '0.66015', 'examples_per_second': '31.41', 'grad_norm': '101.5', 'counters/examples': 198848, 'counters/updates': 6214}
train stats after 198880 examples: {'rewards_train/chosen': '0.14121', 'rewards_train/rejected': '0.086255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.054958', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-150.53', 'loss/train': '0.68377', 'examples_per_second': '30.895', 'grad_norm': '103', 'counters/examples': 198880, 'counters/updates': 6215}
train stats after 198912 examples: {'rewards_train/chosen': '0.099788', 'rewards_train/rejected': '-0.0018122', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1016', 'logps_train/rejected': '-108.76', 'logps_train/chosen': '-145.27', 'loss/train': '0.65334', 'examples_per_second': '31.461', 'grad_norm': '276', 'counters/examples': 198912, 'counters/updates': 6216}
train stats after 198944 examples: {'rewards_train/chosen': '0.091722', 'rewards_train/rejected': '0.11319', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021464', 'logps_train/rejected': '-145.82', 'logps_train/chosen': '-129.29', 'loss/train': '0.71691', 'examples_per_second': '33.159', 'grad_norm': '147', 'counters/examples': 198944, 'counters/updates': 6217}
train stats after 198976 examples: {'rewards_train/chosen': '0.14368', 'rewards_train/rejected': '0.056856', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086821', 'logps_train/rejected': '-113.68', 'logps_train/chosen': '-124.94', 'loss/train': '0.6645', 'examples_per_second': '31.005', 'grad_norm': '61', 'counters/examples': 198976, 'counters/updates': 6218}
train stats after 199008 examples: {'rewards_train/chosen': '0.057923', 'rewards_train/rejected': '0.091933', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03401', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-148.07', 'loss/train': '0.73106', 'examples_per_second': '32.352', 'grad_norm': '76', 'counters/examples': 199008, 'counters/updates': 6219}
train stats after 199040 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '-0.0097402', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12461', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-198.44', 'loss/train': '0.64995', 'examples_per_second': '31.036', 'grad_norm': '71.5', 'counters/examples': 199040, 'counters/updates': 6220}
train stats after 199072 examples: {'rewards_train/chosen': '0.15211', 'rewards_train/rejected': '0.023014', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1291', 'logps_train/rejected': '-141.06', 'logps_train/chosen': '-204.34', 'loss/train': '0.65364', 'examples_per_second': '31.357', 'grad_norm': '68', 'counters/examples': 199072, 'counters/updates': 6221}
train stats after 199104 examples: {'rewards_train/chosen': '0.11699', 'rewards_train/rejected': '0.065501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051494', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-189.23', 'loss/train': '0.69181', 'examples_per_second': '31.052', 'grad_norm': '82', 'counters/examples': 199104, 'counters/updates': 6222}
skipping logging after 199136 examples to avoid logging too frequently
train stats after 199168 examples: {'rewards_train/chosen': '0.12256', 'rewards_train/rejected': '0.044843', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.07772', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-149.76', 'loss/train': '0.66823', 'examples_per_second': '34.963', 'grad_norm': '78.5', 'counters/examples': 199168, 'counters/updates': 6224}
train stats after 199200 examples: {'rewards_train/chosen': '0.08399', 'rewards_train/rejected': '0.0091529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074837', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-125.4', 'loss/train': '0.67191', 'examples_per_second': '31.277', 'grad_norm': '53.5', 'counters/examples': 199200, 'counters/updates': 6225}
skipping logging after 199232 examples to avoid logging too frequently
train stats after 199264 examples: {'rewards_train/chosen': '0.089871', 'rewards_train/rejected': '0.006759', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083112', 'logps_train/rejected': '-114.76', 'logps_train/chosen': '-159.22', 'loss/train': '0.66083', 'examples_per_second': '30.911', 'grad_norm': '71.5', 'counters/examples': 199264, 'counters/updates': 6227}
skipping logging after 199296 examples to avoid logging too frequently
train stats after 199328 examples: {'rewards_train/chosen': '0.027797', 'rewards_train/rejected': '0.014819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012977', 'logps_train/rejected': '-176.93', 'logps_train/chosen': '-182.88', 'loss/train': '0.70352', 'examples_per_second': '31.436', 'grad_norm': '152', 'counters/examples': 199328, 'counters/updates': 6229}
train stats after 199360 examples: {'rewards_train/chosen': '0.076934', 'rewards_train/rejected': '0.082755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0058209', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-142.81', 'loss/train': '0.70565', 'examples_per_second': '30.62', 'grad_norm': '83.5', 'counters/examples': 199360, 'counters/updates': 6230}
train stats after 199392 examples: {'rewards_train/chosen': '0.10623', 'rewards_train/rejected': '0.083802', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022425', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-130.39', 'loss/train': '0.6915', 'examples_per_second': '29.967', 'grad_norm': '118.5', 'counters/examples': 199392, 'counters/updates': 6231}
skipping logging after 199424 examples to avoid logging too frequently
train stats after 199456 examples: {'rewards_train/chosen': '0.10991', 'rewards_train/rejected': '-0.018116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12803', 'logps_train/rejected': '-98.657', 'logps_train/chosen': '-120.84', 'loss/train': '0.6456', 'examples_per_second': '33.838', 'grad_norm': '75.5', 'counters/examples': 199456, 'counters/updates': 6233}
train stats after 199488 examples: {'rewards_train/chosen': '0.15457', 'rewards_train/rejected': '0.10813', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046433', 'logps_train/rejected': '-144.97', 'logps_train/chosen': '-130.64', 'loss/train': '0.68289', 'examples_per_second': '32.797', 'grad_norm': '54.75', 'counters/examples': 199488, 'counters/updates': 6234}
train stats after 199520 examples: {'rewards_train/chosen': '0.064214', 'rewards_train/rejected': '0.0080635', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056151', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-167.26', 'loss/train': '0.67197', 'examples_per_second': '31.459', 'grad_norm': '53.5', 'counters/examples': 199520, 'counters/updates': 6235}
train stats after 199552 examples: {'rewards_train/chosen': '0.10991', 'rewards_train/rejected': '0.076156', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033756', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-175', 'loss/train': '0.68846', 'examples_per_second': '30.572', 'grad_norm': '136', 'counters/examples': 199552, 'counters/updates': 6236}
train stats after 199584 examples: {'rewards_train/chosen': '0.11773', 'rewards_train/rejected': '0.16386', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046137', 'logps_train/rejected': '-136.13', 'logps_train/chosen': '-136.18', 'loss/train': '0.73012', 'examples_per_second': '31.08', 'grad_norm': '123.5', 'counters/examples': 199584, 'counters/updates': 6237}
train stats after 199616 examples: {'rewards_train/chosen': '0.15241', 'rewards_train/rejected': '0.080822', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071583', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-150.09', 'loss/train': '0.66831', 'examples_per_second': '32.754', 'grad_norm': '65', 'counters/examples': 199616, 'counters/updates': 6238}
skipping logging after 199648 examples to avoid logging too frequently
train stats after 199680 examples: {'rewards_train/chosen': '0.14716', 'rewards_train/rejected': '-0.011748', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15891', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-143.78', 'loss/train': '0.62494', 'examples_per_second': '31.813', 'grad_norm': '45', 'counters/examples': 199680, 'counters/updates': 6240}
train stats after 199712 examples: {'rewards_train/chosen': '0.08514', 'rewards_train/rejected': '0.069919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015221', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-162.87', 'loss/train': '0.69924', 'examples_per_second': '31.248', 'grad_norm': '81', 'counters/examples': 199712, 'counters/updates': 6241}
train stats after 199744 examples: {'rewards_train/chosen': '0.098778', 'rewards_train/rejected': '0.059856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038922', 'logps_train/rejected': '-144.1', 'logps_train/chosen': '-155.31', 'loss/train': '0.68966', 'examples_per_second': '31.503', 'grad_norm': '66.5', 'counters/examples': 199744, 'counters/updates': 6242}
train stats after 199776 examples: {'rewards_train/chosen': '0.1566', 'rewards_train/rejected': '0.10878', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047828', 'logps_train/rejected': '-149.18', 'logps_train/chosen': '-175.8', 'loss/train': '0.68683', 'examples_per_second': '31.254', 'grad_norm': '66.5', 'counters/examples': 199776, 'counters/updates': 6243}
train stats after 199808 examples: {'rewards_train/chosen': '0.065664', 'rewards_train/rejected': '0.019338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046325', 'logps_train/rejected': '-118.79', 'logps_train/chosen': '-149.04', 'loss/train': '0.6803', 'examples_per_second': '31.355', 'grad_norm': '63.25', 'counters/examples': 199808, 'counters/updates': 6244}
train stats after 199840 examples: {'rewards_train/chosen': '0.13059', 'rewards_train/rejected': '0.064985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065602', 'logps_train/rejected': '-141.35', 'logps_train/chosen': '-130.82', 'loss/train': '0.67616', 'examples_per_second': '31.148', 'grad_norm': '124', 'counters/examples': 199840, 'counters/updates': 6245}
skipping logging after 199872 examples to avoid logging too frequently
train stats after 199904 examples: {'rewards_train/chosen': '0.086539', 'rewards_train/rejected': '0.011152', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075386', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-150.11', 'loss/train': '0.66876', 'examples_per_second': '30.065', 'grad_norm': '60.75', 'counters/examples': 199904, 'counters/updates': 6247}
train stats after 199936 examples: {'rewards_train/chosen': '0.13164', 'rewards_train/rejected': '0.064402', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067238', 'logps_train/rejected': '-138.81', 'logps_train/chosen': '-154.06', 'loss/train': '0.67019', 'examples_per_second': '31.494', 'grad_norm': '80', 'counters/examples': 199936, 'counters/updates': 6248}
train stats after 199968 examples: {'rewards_train/chosen': '0.10951', 'rewards_train/rejected': '0.020335', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08918', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-135.93', 'loss/train': '0.66479', 'examples_per_second': '30.542', 'grad_norm': '57.75', 'counters/examples': 199968, 'counters/updates': 6249}
train stats after 200000 examples: {'rewards_train/chosen': '0.028894', 'rewards_train/rejected': '-0.020454', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049348', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-152.19', 'loss/train': '0.67857', 'examples_per_second': '29.79', 'grad_norm': '72.5', 'counters/examples': 200000, 'counters/updates': 6250}
train stats after 200032 examples: {'rewards_train/chosen': '0.070218', 'rewards_train/rejected': '0.043254', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026964', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-139.27', 'loss/train': '0.69359', 'examples_per_second': '31.609', 'grad_norm': '136', 'counters/examples': 200032, 'counters/updates': 6251}
train stats after 200064 examples: {'rewards_train/chosen': '0.16331', 'rewards_train/rejected': '0.12505', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038255', 'logps_train/rejected': '-157.01', 'logps_train/chosen': '-148.56', 'loss/train': '0.68448', 'examples_per_second': '33.121', 'grad_norm': '140', 'counters/examples': 200064, 'counters/updates': 6252}
train stats after 200096 examples: {'rewards_train/chosen': '0.12745', 'rewards_train/rejected': '0.0092009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11824', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-157.5', 'loss/train': '0.65733', 'examples_per_second': '31.443', 'grad_norm': '65', 'counters/examples': 200096, 'counters/updates': 6253}
train stats after 200128 examples: {'rewards_train/chosen': '0.028812', 'rewards_train/rejected': '0.054618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.025806', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-150.68', 'loss/train': '0.72473', 'examples_per_second': '31.465', 'grad_norm': '88', 'counters/examples': 200128, 'counters/updates': 6254}
train stats after 200160 examples: {'rewards_train/chosen': '0.057272', 'rewards_train/rejected': '0.0081222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04915', 'logps_train/rejected': '-160.07', 'logps_train/chosen': '-197.46', 'loss/train': '0.69519', 'examples_per_second': '31.472', 'grad_norm': '93', 'counters/examples': 200160, 'counters/updates': 6255}
train stats after 200192 examples: {'rewards_train/chosen': '0.12038', 'rewards_train/rejected': '0.15142', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.031046', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-178.11', 'loss/train': '0.71856', 'examples_per_second': '30.043', 'grad_norm': '71.5', 'counters/examples': 200192, 'counters/updates': 6256}
train stats after 200224 examples: {'rewards_train/chosen': '0.11603', 'rewards_train/rejected': '0.054437', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061597', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-149.16', 'loss/train': '0.67989', 'examples_per_second': '31.303', 'grad_norm': '166', 'counters/examples': 200224, 'counters/updates': 6257}
train stats after 200256 examples: {'rewards_train/chosen': '0.062223', 'rewards_train/rejected': '0.073007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010783', 'logps_train/rejected': '-176.48', 'logps_train/chosen': '-251.07', 'loss/train': '0.71277', 'examples_per_second': '29.742', 'grad_norm': '99', 'counters/examples': 200256, 'counters/updates': 6258}
train stats after 200288 examples: {'rewards_train/chosen': '0.20023', 'rewards_train/rejected': '0.0052958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19494', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-153.04', 'loss/train': '0.62112', 'examples_per_second': '30.049', 'grad_norm': '137', 'counters/examples': 200288, 'counters/updates': 6259}
train stats after 200320 examples: {'rewards_train/chosen': '0.11644', 'rewards_train/rejected': '0.028735', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087703', 'logps_train/rejected': '-141.67', 'logps_train/chosen': '-146.24', 'loss/train': '0.66153', 'examples_per_second': '30.965', 'grad_norm': '84.5', 'counters/examples': 200320, 'counters/updates': 6260}
skipping logging after 200352 examples to avoid logging too frequently
train stats after 200384 examples: {'rewards_train/chosen': '0.13994', 'rewards_train/rejected': '0.039145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1008', 'logps_train/rejected': '-110.37', 'logps_train/chosen': '-124.37', 'loss/train': '0.65299', 'examples_per_second': '31.509', 'grad_norm': '57', 'counters/examples': 200384, 'counters/updates': 6262}
train stats after 200416 examples: {'rewards_train/chosen': '0.068687', 'rewards_train/rejected': '0.098922', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.030235', 'logps_train/rejected': '-126.77', 'logps_train/chosen': '-143.64', 'loss/train': '0.73779', 'examples_per_second': '31.613', 'grad_norm': '160', 'counters/examples': 200416, 'counters/updates': 6263}
train stats after 200448 examples: {'rewards_train/chosen': '0.088384', 'rewards_train/rejected': '-0.076508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16489', 'logps_train/rejected': '-151.99', 'logps_train/chosen': '-138.77', 'loss/train': '0.62439', 'examples_per_second': '29.942', 'grad_norm': '59', 'counters/examples': 200448, 'counters/updates': 6264}
skipping logging after 200480 examples to avoid logging too frequently
train stats after 200512 examples: {'rewards_train/chosen': '0.096131', 'rewards_train/rejected': '-0.011944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10807', 'logps_train/rejected': '-151.72', 'logps_train/chosen': '-172.03', 'loss/train': '0.65739', 'examples_per_second': '30.071', 'grad_norm': '186', 'counters/examples': 200512, 'counters/updates': 6266}
train stats after 200544 examples: {'rewards_train/chosen': '0.28824', 'rewards_train/rejected': '0.064864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22337', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-161.37', 'loss/train': '0.61627', 'examples_per_second': '30.554', 'grad_norm': '87.5', 'counters/examples': 200544, 'counters/updates': 6267}
train stats after 200576 examples: {'rewards_train/chosen': '0.038641', 'rewards_train/rejected': '0.043799', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0051584', 'logps_train/rejected': '-143.62', 'logps_train/chosen': '-151.9', 'loss/train': '0.71141', 'examples_per_second': '32.144', 'grad_norm': '80', 'counters/examples': 200576, 'counters/updates': 6268}
skipping logging after 200608 examples to avoid logging too frequently
train stats after 200640 examples: {'rewards_train/chosen': '0.10256', 'rewards_train/rejected': '0.023852', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078712', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-123.88', 'loss/train': '0.66219', 'examples_per_second': '32.272', 'grad_norm': '65.5', 'counters/examples': 200640, 'counters/updates': 6270}
train stats after 200672 examples: {'rewards_train/chosen': '0.16203', 'rewards_train/rejected': '0.092733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0693', 'logps_train/rejected': '-160.57', 'logps_train/chosen': '-150.24', 'loss/train': '0.67049', 'examples_per_second': '31.376', 'grad_norm': '80.5', 'counters/examples': 200672, 'counters/updates': 6271}
train stats after 200704 examples: {'rewards_train/chosen': '0.066171', 'rewards_train/rejected': '0.063562', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0026088', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-142.34', 'loss/train': '0.70246', 'examples_per_second': '30.619', 'grad_norm': '57.25', 'counters/examples': 200704, 'counters/updates': 6272}
train stats after 200736 examples: {'rewards_train/chosen': '0.068467', 'rewards_train/rejected': '0.10902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.040555', 'logps_train/rejected': '-145.74', 'logps_train/chosen': '-149.83', 'loss/train': '0.72824', 'examples_per_second': '32.774', 'grad_norm': '99', 'counters/examples': 200736, 'counters/updates': 6273}
train stats after 200768 examples: {'rewards_train/chosen': '0.11197', 'rewards_train/rejected': '0.085081', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026888', 'logps_train/rejected': '-150.17', 'logps_train/chosen': '-175', 'loss/train': '0.6959', 'examples_per_second': '32.087', 'grad_norm': '87.5', 'counters/examples': 200768, 'counters/updates': 6274}
train stats after 200800 examples: {'rewards_train/chosen': '0.12507', 'rewards_train/rejected': '-0.04246', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16753', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-170.28', 'loss/train': '0.62465', 'examples_per_second': '30.351', 'grad_norm': '65.5', 'counters/examples': 200800, 'counters/updates': 6275}
train stats after 200832 examples: {'rewards_train/chosen': '0.11949', 'rewards_train/rejected': '0.070737', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048751', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-130.38', 'loss/train': '0.67942', 'examples_per_second': '32.5', 'grad_norm': '106.5', 'counters/examples': 200832, 'counters/updates': 6276}
train stats after 200864 examples: {'rewards_train/chosen': '0.21744', 'rewards_train/rejected': '0.0049837', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.21246', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-141.2', 'loss/train': '0.62209', 'examples_per_second': '30.379', 'grad_norm': '72.5', 'counters/examples': 200864, 'counters/updates': 6277}
train stats after 200896 examples: {'rewards_train/chosen': '0.14089', 'rewards_train/rejected': '0.079373', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061516', 'logps_train/rejected': '-100.81', 'logps_train/chosen': '-165.96', 'loss/train': '0.68004', 'examples_per_second': '25.055', 'grad_norm': '96.5', 'counters/examples': 200896, 'counters/updates': 6278}
train stats after 200928 examples: {'rewards_train/chosen': '0.17805', 'rewards_train/rejected': '0.072398', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10565', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-166.24', 'loss/train': '0.6559', 'examples_per_second': '31.334', 'grad_norm': '140', 'counters/examples': 200928, 'counters/updates': 6279}
train stats after 200960 examples: {'rewards_train/chosen': '0.056389', 'rewards_train/rejected': '0.057513', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011241', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-155.47', 'loss/train': '0.70826', 'examples_per_second': '31.252', 'grad_norm': '118.5', 'counters/examples': 200960, 'counters/updates': 6280}
skipping logging after 200992 examples to avoid logging too frequently
train stats after 201024 examples: {'rewards_train/chosen': '0.099952', 'rewards_train/rejected': '0.055168', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044785', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-142.1', 'loss/train': '0.68429', 'examples_per_second': '31.483', 'grad_norm': '118.5', 'counters/examples': 201024, 'counters/updates': 6282}
skipping logging after 201056 examples to avoid logging too frequently
train stats after 201088 examples: {'rewards_train/chosen': '0.072455', 'rewards_train/rejected': '0.048318', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024137', 'logps_train/rejected': '-144.42', 'logps_train/chosen': '-173.32', 'loss/train': '0.69596', 'examples_per_second': '31.641', 'grad_norm': '228', 'counters/examples': 201088, 'counters/updates': 6284}
train stats after 201120 examples: {'rewards_train/chosen': '0.049574', 'rewards_train/rejected': '0.048194', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0013802', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-136.77', 'loss/train': '0.7052', 'examples_per_second': '29.971', 'grad_norm': '117', 'counters/examples': 201120, 'counters/updates': 6285}
train stats after 201152 examples: {'rewards_train/chosen': '0.1225', 'rewards_train/rejected': '-0.030984', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15348', 'logps_train/rejected': '-95.351', 'logps_train/chosen': '-124.75', 'loss/train': '0.63007', 'examples_per_second': '32.025', 'grad_norm': '86.5', 'counters/examples': 201152, 'counters/updates': 6286}
train stats after 201184 examples: {'rewards_train/chosen': '0.073628', 'rewards_train/rejected': '0.068224', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0054042', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-149.02', 'loss/train': '0.7039', 'examples_per_second': '31.404', 'grad_norm': '108.5', 'counters/examples': 201184, 'counters/updates': 6287}
train stats after 201216 examples: {'rewards_train/chosen': '0.086437', 'rewards_train/rejected': '0.046591', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039845', 'logps_train/rejected': '-153.38', 'logps_train/chosen': '-205.69', 'loss/train': '0.68838', 'examples_per_second': '30.577', 'grad_norm': '75', 'counters/examples': 201216, 'counters/updates': 6288}
train stats after 201248 examples: {'rewards_train/chosen': '0.091635', 'rewards_train/rejected': '0.15388', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.062242', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-144.48', 'loss/train': '0.74143', 'examples_per_second': '31.86', 'grad_norm': '61.25', 'counters/examples': 201248, 'counters/updates': 6289}
train stats after 201280 examples: {'rewards_train/chosen': '0.11688', 'rewards_train/rejected': '0.056957', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05992', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-129.38', 'loss/train': '0.67176', 'examples_per_second': '31.631', 'grad_norm': '68', 'counters/examples': 201280, 'counters/updates': 6290}
train stats after 201312 examples: {'rewards_train/chosen': '0.12054', 'rewards_train/rejected': '0.057259', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063286', 'logps_train/rejected': '-152.07', 'logps_train/chosen': '-139.34', 'loss/train': '0.6793', 'examples_per_second': '30.387', 'grad_norm': '68', 'counters/examples': 201312, 'counters/updates': 6291}
train stats after 201344 examples: {'rewards_train/chosen': '0.19333', 'rewards_train/rejected': '0.17868', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014654', 'logps_train/rejected': '-144.93', 'logps_train/chosen': '-162.46', 'loss/train': '0.70134', 'examples_per_second': '33.067', 'grad_norm': '91', 'counters/examples': 201344, 'counters/updates': 6292}
train stats after 201376 examples: {'rewards_train/chosen': '0.003369', 'rewards_train/rejected': '-0.01151', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.014879', 'logps_train/rejected': '-168.72', 'logps_train/chosen': '-168.12', 'loss/train': '0.69703', 'examples_per_second': '29.85', 'grad_norm': '213', 'counters/examples': 201376, 'counters/updates': 6293}
train stats after 201408 examples: {'rewards_train/chosen': '0.099901', 'rewards_train/rejected': '0.054603', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045298', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-171.2', 'loss/train': '0.67842', 'examples_per_second': '31.377', 'grad_norm': '96.5', 'counters/examples': 201408, 'counters/updates': 6294}
train stats after 201440 examples: {'rewards_train/chosen': '0.04047', 'rewards_train/rejected': '0.092435', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.051965', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-165.05', 'loss/train': '0.72814', 'examples_per_second': '30.888', 'grad_norm': '133', 'counters/examples': 201440, 'counters/updates': 6295}
train stats after 201472 examples: {'rewards_train/chosen': '0.043206', 'rewards_train/rejected': '-0.036806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080012', 'logps_train/rejected': '-136.81', 'logps_train/chosen': '-169.8', 'loss/train': '0.67261', 'examples_per_second': '31.115', 'grad_norm': '72.5', 'counters/examples': 201472, 'counters/updates': 6296}
train stats after 201504 examples: {'rewards_train/chosen': '0.10221', 'rewards_train/rejected': '0.046042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056168', 'logps_train/rejected': '-106.17', 'logps_train/chosen': '-132.57', 'loss/train': '0.67565', 'examples_per_second': '30.21', 'grad_norm': '104.5', 'counters/examples': 201504, 'counters/updates': 6297}
train stats after 201536 examples: {'rewards_train/chosen': '0.24816', 'rewards_train/rejected': '0.043203', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20496', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-165.12', 'loss/train': '0.61062', 'examples_per_second': '30.053', 'grad_norm': '81', 'counters/examples': 201536, 'counters/updates': 6298}
train stats after 201568 examples: {'rewards_train/chosen': '0.1439', 'rewards_train/rejected': '0.0689', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075', 'logps_train/rejected': '-167.77', 'logps_train/chosen': '-166.66', 'loss/train': '0.66926', 'examples_per_second': '31.473', 'grad_norm': '98.5', 'counters/examples': 201568, 'counters/updates': 6299}
train stats after 201600 examples: {'rewards_train/chosen': '0.12667', 'rewards_train/rejected': '0.05734', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069334', 'logps_train/rejected': '-157.34', 'logps_train/chosen': '-136.71', 'loss/train': '0.66751', 'examples_per_second': '31.586', 'grad_norm': '115', 'counters/examples': 201600, 'counters/updates': 6300}
train stats after 201632 examples: {'rewards_train/chosen': '0.014622', 'rewards_train/rejected': '0.019085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0044627', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-175.06', 'loss/train': '0.7101', 'examples_per_second': '30.898', 'grad_norm': '92.5', 'counters/examples': 201632, 'counters/updates': 6301}
train stats after 201664 examples: {'rewards_train/chosen': '0.082528', 'rewards_train/rejected': '0.098466', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015937', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-147.84', 'loss/train': '0.71608', 'examples_per_second': '31.437', 'grad_norm': '107.5', 'counters/examples': 201664, 'counters/updates': 6302}
train stats after 201696 examples: {'rewards_train/chosen': '0.12001', 'rewards_train/rejected': '0.17438', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054368', 'logps_train/rejected': '-145.73', 'logps_train/chosen': '-115.5', 'loss/train': '0.73233', 'examples_per_second': '31.269', 'grad_norm': '152', 'counters/examples': 201696, 'counters/updates': 6303}
train stats after 201728 examples: {'rewards_train/chosen': '0.087482', 'rewards_train/rejected': '0.072776', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014706', 'logps_train/rejected': '-116.31', 'logps_train/chosen': '-148.63', 'loss/train': '0.69343', 'examples_per_second': '32.083', 'grad_norm': '66', 'counters/examples': 201728, 'counters/updates': 6304}
train stats after 201760 examples: {'rewards_train/chosen': '0.022247', 'rewards_train/rejected': '0.026984', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0047373', 'logps_train/rejected': '-104.81', 'logps_train/chosen': '-124.84', 'loss/train': '0.7176', 'examples_per_second': '31.52', 'grad_norm': '62.5', 'counters/examples': 201760, 'counters/updates': 6305}
skipping logging after 201792 examples to avoid logging too frequently
train stats after 201824 examples: {'rewards_train/chosen': '0.066187', 'rewards_train/rejected': '0.036221', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029966', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-163.44', 'loss/train': '0.68928', 'examples_per_second': '32.295', 'grad_norm': '104', 'counters/examples': 201824, 'counters/updates': 6307}
train stats after 201856 examples: {'rewards_train/chosen': '0.1885', 'rewards_train/rejected': '0.017265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17124', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-144.43', 'loss/train': '0.62943', 'examples_per_second': '29.896', 'grad_norm': '89.5', 'counters/examples': 201856, 'counters/updates': 6308}
train stats after 201888 examples: {'rewards_train/chosen': '0.071062', 'rewards_train/rejected': '0.034872', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03619', 'logps_train/rejected': '-102.64', 'logps_train/chosen': '-147.24', 'loss/train': '0.6861', 'examples_per_second': '30.823', 'grad_norm': '532', 'counters/examples': 201888, 'counters/updates': 6309}
train stats after 201920 examples: {'rewards_train/chosen': '0.17492', 'rewards_train/rejected': '0.059102', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11582', 'logps_train/rejected': '-150.01', 'logps_train/chosen': '-165.74', 'loss/train': '0.64781', 'examples_per_second': '32.121', 'grad_norm': '62.75', 'counters/examples': 201920, 'counters/updates': 6310}
train stats after 201952 examples: {'rewards_train/chosen': '0.20163', 'rewards_train/rejected': '0.025287', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17634', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-161.18', 'loss/train': '0.62491', 'examples_per_second': '32.428', 'grad_norm': '225', 'counters/examples': 201952, 'counters/updates': 6311}
train stats after 201984 examples: {'rewards_train/chosen': '0.1459', 'rewards_train/rejected': '0.046076', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099827', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-143.05', 'loss/train': '0.65331', 'examples_per_second': '31.782', 'grad_norm': '81.5', 'counters/examples': 201984, 'counters/updates': 6312}
skipping logging after 202016 examples to avoid logging too frequently
train stats after 202048 examples: {'rewards_train/chosen': '0.107', 'rewards_train/rejected': '-0.032804', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1398', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-111.36', 'loss/train': '0.63649', 'examples_per_second': '32.324', 'grad_norm': '68.5', 'counters/examples': 202048, 'counters/updates': 6314}
train stats after 202080 examples: {'rewards_train/chosen': '0.087764', 'rewards_train/rejected': '0.19918', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.11142', 'logps_train/rejected': '-193.34', 'logps_train/chosen': '-153.8', 'loss/train': '0.77821', 'examples_per_second': '31.45', 'grad_norm': '135', 'counters/examples': 202080, 'counters/updates': 6315}
train stats after 202112 examples: {'rewards_train/chosen': '0.028197', 'rewards_train/rejected': '-0.021619', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049816', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-142.73', 'loss/train': '0.6839', 'examples_per_second': '31.503', 'grad_norm': '181', 'counters/examples': 202112, 'counters/updates': 6316}
skipping logging after 202144 examples to avoid logging too frequently
train stats after 202176 examples: {'rewards_train/chosen': '0.09119', 'rewards_train/rejected': '0.062329', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028861', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-115.53', 'loss/train': '0.68898', 'examples_per_second': '31.641', 'grad_norm': '76', 'counters/examples': 202176, 'counters/updates': 6318}
train stats after 202208 examples: {'rewards_train/chosen': '0.071789', 'rewards_train/rejected': '0.015266', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056523', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-144.2', 'loss/train': '0.6797', 'examples_per_second': '30.1', 'grad_norm': '161', 'counters/examples': 202208, 'counters/updates': 6319}
train stats after 202240 examples: {'rewards_train/chosen': '0.058722', 'rewards_train/rejected': '0.055071', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0036513', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-137.94', 'loss/train': '0.70051', 'examples_per_second': '31.523', 'grad_norm': '81', 'counters/examples': 202240, 'counters/updates': 6320}
train stats after 202272 examples: {'rewards_train/chosen': '0.062717', 'rewards_train/rejected': '0.058748', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0039689', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-166.04', 'loss/train': '0.70664', 'examples_per_second': '32.048', 'grad_norm': '100.5', 'counters/examples': 202272, 'counters/updates': 6321}
train stats after 202304 examples: {'rewards_train/chosen': '0.088865', 'rewards_train/rejected': '0.10763', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018768', 'logps_train/rejected': '-192.78', 'logps_train/chosen': '-200.37', 'loss/train': '0.71643', 'examples_per_second': '31.484', 'grad_norm': '179', 'counters/examples': 202304, 'counters/updates': 6322}
train stats after 202336 examples: {'rewards_train/chosen': '0.066888', 'rewards_train/rejected': '0.094445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027558', 'logps_train/rejected': '-152.9', 'logps_train/chosen': '-179.58', 'loss/train': '0.7242', 'examples_per_second': '24.37', 'grad_norm': '61', 'counters/examples': 202336, 'counters/updates': 6323}
train stats after 202368 examples: {'rewards_train/chosen': '0.1092', 'rewards_train/rejected': '0.055037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054158', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-157.27', 'loss/train': '0.67574', 'examples_per_second': '32.65', 'grad_norm': '74', 'counters/examples': 202368, 'counters/updates': 6324}
skipping logging after 202400 examples to avoid logging too frequently
train stats after 202432 examples: {'rewards_train/chosen': '0.14313', 'rewards_train/rejected': '0.040785', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10234', 'logps_train/rejected': '-106.66', 'logps_train/chosen': '-151.76', 'loss/train': '0.65704', 'examples_per_second': '23.598', 'grad_norm': '56.5', 'counters/examples': 202432, 'counters/updates': 6326}
skipping logging after 202464 examples to avoid logging too frequently
train stats after 202496 examples: {'rewards_train/chosen': '0.11409', 'rewards_train/rejected': '-0.030955', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14504', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-134.74', 'loss/train': '0.64233', 'examples_per_second': '32.625', 'grad_norm': '194', 'counters/examples': 202496, 'counters/updates': 6328}
train stats after 202528 examples: {'rewards_train/chosen': '0.14645', 'rewards_train/rejected': '0.096988', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049463', 'logps_train/rejected': '-109.55', 'logps_train/chosen': '-140.58', 'loss/train': '0.68206', 'examples_per_second': '32.236', 'grad_norm': '91.5', 'counters/examples': 202528, 'counters/updates': 6329}
train stats after 202560 examples: {'rewards_train/chosen': '0.074709', 'rewards_train/rejected': '0.038582', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036127', 'logps_train/rejected': '-141.58', 'logps_train/chosen': '-147.03', 'loss/train': '0.6865', 'examples_per_second': '31.441', 'grad_norm': '205', 'counters/examples': 202560, 'counters/updates': 6330}
skipping logging after 202592 examples to avoid logging too frequently
train stats after 202624 examples: {'rewards_train/chosen': '0.13768', 'rewards_train/rejected': '0.10757', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030111', 'logps_train/rejected': '-172.28', 'logps_train/chosen': '-139.45', 'loss/train': '0.69316', 'examples_per_second': '31.979', 'grad_norm': '97.5', 'counters/examples': 202624, 'counters/updates': 6332}
train stats after 202656 examples: {'rewards_train/chosen': '0.10415', 'rewards_train/rejected': '-0.009864', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11401', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-117.45', 'loss/train': '0.65159', 'examples_per_second': '32.432', 'grad_norm': '77.5', 'counters/examples': 202656, 'counters/updates': 6333}
train stats after 202688 examples: {'rewards_train/chosen': '0.066579', 'rewards_train/rejected': '-0.00085621', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067435', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-151.2', 'loss/train': '0.67002', 'examples_per_second': '31.437', 'grad_norm': '100.5', 'counters/examples': 202688, 'counters/updates': 6334}
train stats after 202720 examples: {'rewards_train/chosen': '0.071464', 'rewards_train/rejected': '0.094942', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.023478', 'logps_train/rejected': '-106.02', 'logps_train/chosen': '-131.38', 'loss/train': '0.71463', 'examples_per_second': '31.936', 'grad_norm': '61.25', 'counters/examples': 202720, 'counters/updates': 6335}
train stats after 202752 examples: {'rewards_train/chosen': '0.11373', 'rewards_train/rejected': '0.040033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073697', 'logps_train/rejected': '-144.71', 'logps_train/chosen': '-155.14', 'loss/train': '0.69303', 'examples_per_second': '31.47', 'grad_norm': '65.5', 'counters/examples': 202752, 'counters/updates': 6336}
train stats after 202784 examples: {'rewards_train/chosen': '0.15891', 'rewards_train/rejected': '0.12633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032579', 'logps_train/rejected': '-170.35', 'logps_train/chosen': '-152.96', 'loss/train': '0.69088', 'examples_per_second': '31.468', 'grad_norm': '120.5', 'counters/examples': 202784, 'counters/updates': 6337}
train stats after 202816 examples: {'rewards_train/chosen': '0.05405', 'rewards_train/rejected': '0.019029', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035021', 'logps_train/rejected': '-132.38', 'logps_train/chosen': '-166.44', 'loss/train': '0.68884', 'examples_per_second': '32.413', 'grad_norm': '82', 'counters/examples': 202816, 'counters/updates': 6338}
skipping logging after 202848 examples to avoid logging too frequently
train stats after 202880 examples: {'rewards_train/chosen': '0.14452', 'rewards_train/rejected': '0.15808', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013558', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-139.41', 'loss/train': '0.71883', 'examples_per_second': '33.94', 'grad_norm': '101', 'counters/examples': 202880, 'counters/updates': 6340}
skipping logging after 202912 examples to avoid logging too frequently
train stats after 202944 examples: {'rewards_train/chosen': '0.017766', 'rewards_train/rejected': '0.034573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016808', 'logps_train/rejected': '-150.22', 'logps_train/chosen': '-156.95', 'loss/train': '0.71865', 'examples_per_second': '30.745', 'grad_norm': '138', 'counters/examples': 202944, 'counters/updates': 6342}
train stats after 202976 examples: {'rewards_train/chosen': '0.17008', 'rewards_train/rejected': '0.058581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1115', 'logps_train/rejected': '-120.92', 'logps_train/chosen': '-149.54', 'loss/train': '0.6616', 'examples_per_second': '31.549', 'grad_norm': '69', 'counters/examples': 202976, 'counters/updates': 6343}
train stats after 203008 examples: {'rewards_train/chosen': '0.046411', 'rewards_train/rejected': '0.057277', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010866', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-148.3', 'loss/train': '0.70973', 'examples_per_second': '32.642', 'grad_norm': '140', 'counters/examples': 203008, 'counters/updates': 6344}
train stats after 203040 examples: {'rewards_train/chosen': '0.088408', 'rewards_train/rejected': '0.052667', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035742', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-175.54', 'loss/train': '0.69229', 'examples_per_second': '30.034', 'grad_norm': '95.5', 'counters/examples': 203040, 'counters/updates': 6345}
train stats after 203072 examples: {'rewards_train/chosen': '0.085083', 'rewards_train/rejected': '0.067542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017541', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-141.51', 'loss/train': '0.68903', 'examples_per_second': '30.592', 'grad_norm': '53.5', 'counters/examples': 203072, 'counters/updates': 6346}
train stats after 203104 examples: {'rewards_train/chosen': '0.088779', 'rewards_train/rejected': '0.11818', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029396', 'logps_train/rejected': '-155.42', 'logps_train/chosen': '-127.76', 'loss/train': '0.72702', 'examples_per_second': '30.915', 'grad_norm': '171', 'counters/examples': 203104, 'counters/updates': 6347}
train stats after 203136 examples: {'rewards_train/chosen': '0.16118', 'rewards_train/rejected': '0.056793', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10438', 'logps_train/rejected': '-151.75', 'logps_train/chosen': '-178.44', 'loss/train': '0.66258', 'examples_per_second': '32.606', 'grad_norm': '67.5', 'counters/examples': 203136, 'counters/updates': 6348}
train stats after 203168 examples: {'rewards_train/chosen': '0.11829', 'rewards_train/rejected': '0.014142', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-85.049', 'logps_train/chosen': '-113', 'loss/train': '0.64979', 'examples_per_second': '31.006', 'grad_norm': '70.5', 'counters/examples': 203168, 'counters/updates': 6349}
train stats after 203200 examples: {'rewards_train/chosen': '0.053665', 'rewards_train/rejected': '-0.024537', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078202', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-141.38', 'loss/train': '0.66003', 'examples_per_second': '31.752', 'grad_norm': '184', 'counters/examples': 203200, 'counters/updates': 6350}
train stats after 203232 examples: {'rewards_train/chosen': '0.076447', 'rewards_train/rejected': '0.038158', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03829', 'logps_train/rejected': '-193.08', 'logps_train/chosen': '-178.29', 'loss/train': '0.69883', 'examples_per_second': '31.3', 'grad_norm': '163', 'counters/examples': 203232, 'counters/updates': 6351}
train stats after 203264 examples: {'rewards_train/chosen': '0.11961', 'rewards_train/rejected': '0.062138', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057469', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-161.85', 'loss/train': '0.67642', 'examples_per_second': '30.598', 'grad_norm': '97', 'counters/examples': 203264, 'counters/updates': 6352}
skipping logging after 203296 examples to avoid logging too frequently
train stats after 203328 examples: {'rewards_train/chosen': '0.078378', 'rewards_train/rejected': '0.011385', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066994', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-136.8', 'loss/train': '0.67087', 'examples_per_second': '30.807', 'grad_norm': '57.5', 'counters/examples': 203328, 'counters/updates': 6354}
train stats after 203360 examples: {'rewards_train/chosen': '0.19407', 'rewards_train/rejected': '0.11763', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076445', 'logps_train/rejected': '-127.69', 'logps_train/chosen': '-133.77', 'loss/train': '0.69201', 'examples_per_second': '31.589', 'grad_norm': '59.5', 'counters/examples': 203360, 'counters/updates': 6355}
train stats after 203392 examples: {'rewards_train/chosen': '0.13259', 'rewards_train/rejected': '0.096844', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.035748', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-127.74', 'loss/train': '0.69003', 'examples_per_second': '31.137', 'grad_norm': '70', 'counters/examples': 203392, 'counters/updates': 6356}
train stats after 203424 examples: {'rewards_train/chosen': '0.12092', 'rewards_train/rejected': '0.071962', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.04896', 'logps_train/rejected': '-156.6', 'logps_train/chosen': '-168.96', 'loss/train': '0.68166', 'examples_per_second': '31.806', 'grad_norm': '98.5', 'counters/examples': 203424, 'counters/updates': 6357}
skipping logging after 203456 examples to avoid logging too frequently
train stats after 203488 examples: {'rewards_train/chosen': '0.10752', 'rewards_train/rejected': '-0.0014536', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10897', 'logps_train/rejected': '-122.71', 'logps_train/chosen': '-179.35', 'loss/train': '0.65319', 'examples_per_second': '31.218', 'grad_norm': '54.5', 'counters/examples': 203488, 'counters/updates': 6359}
train stats after 203520 examples: {'rewards_train/chosen': '0.0016293', 'rewards_train/rejected': '0.017182', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015552', 'logps_train/rejected': '-114.5', 'logps_train/chosen': '-129.7', 'loss/train': '0.71134', 'examples_per_second': '30.055', 'grad_norm': '71.5', 'counters/examples': 203520, 'counters/updates': 6360}
train stats after 203552 examples: {'rewards_train/chosen': '0.093935', 'rewards_train/rejected': '0.055011', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038924', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-145.63', 'loss/train': '0.69476', 'examples_per_second': '30.035', 'grad_norm': '166', 'counters/examples': 203552, 'counters/updates': 6361}
train stats after 203584 examples: {'rewards_train/chosen': '0.066936', 'rewards_train/rejected': '-0.031498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098434', 'logps_train/rejected': '-160.08', 'logps_train/chosen': '-191.98', 'loss/train': '0.66466', 'examples_per_second': '30.574', 'grad_norm': '102.5', 'counters/examples': 203584, 'counters/updates': 6362}
train stats after 203616 examples: {'rewards_train/chosen': '0.031538', 'rewards_train/rejected': '0.1059', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.074361', 'logps_train/rejected': '-169.75', 'logps_train/chosen': '-163.34', 'loss/train': '0.75116', 'examples_per_second': '31.575', 'grad_norm': '97.5', 'counters/examples': 203616, 'counters/updates': 6363}
train stats after 203648 examples: {'rewards_train/chosen': '0.058072', 'rewards_train/rejected': '0.12512', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.067049', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-151.25', 'loss/train': '0.74433', 'examples_per_second': '31.533', 'grad_norm': '79.5', 'counters/examples': 203648, 'counters/updates': 6364}
train stats after 203680 examples: {'rewards_train/chosen': '0.15388', 'rewards_train/rejected': '0.13016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023722', 'logps_train/rejected': '-172.82', 'logps_train/chosen': '-137.24', 'loss/train': '0.69385', 'examples_per_second': '30.047', 'grad_norm': '119.5', 'counters/examples': 203680, 'counters/updates': 6365}
skipping logging after 203712 examples to avoid logging too frequently
train stats after 203744 examples: {'rewards_train/chosen': '0.11628', 'rewards_train/rejected': '0.035805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080475', 'logps_train/rejected': '-112.07', 'logps_train/chosen': '-135.36', 'loss/train': '0.67077', 'examples_per_second': '34.058', 'grad_norm': '133', 'counters/examples': 203744, 'counters/updates': 6367}
skipping logging after 203776 examples to avoid logging too frequently
train stats after 203808 examples: {'rewards_train/chosen': '0.052756', 'rewards_train/rejected': '0.058139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0053831', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-143.5', 'loss/train': '0.70797', 'examples_per_second': '39.303', 'grad_norm': '67.5', 'counters/examples': 203808, 'counters/updates': 6369}
train stats after 203840 examples: {'rewards_train/chosen': '0.13986', 'rewards_train/rejected': '0.066492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073369', 'logps_train/rejected': '-128.11', 'logps_train/chosen': '-141.1', 'loss/train': '0.68175', 'examples_per_second': '31.036', 'grad_norm': '94', 'counters/examples': 203840, 'counters/updates': 6370}
train stats after 203872 examples: {'rewards_train/chosen': '0.20037', 'rewards_train/rejected': '0.054746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14562', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-163.05', 'loss/train': '0.64421', 'examples_per_second': '32.458', 'grad_norm': '56', 'counters/examples': 203872, 'counters/updates': 6371}
train stats after 203904 examples: {'rewards_train/chosen': '0.039254', 'rewards_train/rejected': '0.10665', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.067395', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-168.31', 'loss/train': '0.73913', 'examples_per_second': '31.578', 'grad_norm': '91.5', 'counters/examples': 203904, 'counters/updates': 6372}
train stats after 203936 examples: {'rewards_train/chosen': '0.11209', 'rewards_train/rejected': '0.11706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0049647', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-104.97', 'loss/train': '0.71139', 'examples_per_second': '32.528', 'grad_norm': '107.5', 'counters/examples': 203936, 'counters/updates': 6373}
skipping logging after 203968 examples to avoid logging too frequently
train stats after 204000 examples: {'rewards_train/chosen': '0.07825', 'rewards_train/rejected': '0.043759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034491', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-136.77', 'loss/train': '0.68657', 'examples_per_second': '31.584', 'grad_norm': '80.5', 'counters/examples': 204000, 'counters/updates': 6375}
Running evaluation after 204000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 204000: {'rewards_eval/chosen': '0.11233', 'rewards_eval/rejected': '0.044818', 'rewards_eval/accuracies': '0.54297', 'rewards_eval/margins': '0.067513', 'logps_eval/rejected': '-127.7', 'logps_eval/chosen': '-149.88', 'loss/eval': '0.67281'}
skipping save for non epoch
train stats after 204032 examples: {'rewards_train/chosen': '0.14357', 'rewards_train/rejected': '0.095807', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047758', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-180.32', 'loss/train': '0.67974', 'examples_per_second': '34.373', 'grad_norm': '105.5', 'counters/examples': 204032, 'counters/updates': 6376}
train stats after 204064 examples: {'rewards_train/chosen': '0.074627', 'rewards_train/rejected': '0.083762', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.009135', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-133.91', 'loss/train': '0.71443', 'examples_per_second': '31.732', 'grad_norm': '61', 'counters/examples': 204064, 'counters/updates': 6377}
train stats after 204096 examples: {'rewards_train/chosen': '0.07205', 'rewards_train/rejected': '0.052833', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019216', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-144.23', 'loss/train': '0.69766', 'examples_per_second': '31.831', 'grad_norm': '124.5', 'counters/examples': 204096, 'counters/updates': 6378}
train stats after 204128 examples: {'rewards_train/chosen': '0.136', 'rewards_train/rejected': '0.17895', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04295', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-135.11', 'loss/train': '0.73224', 'examples_per_second': '33.043', 'grad_norm': '150', 'counters/examples': 204128, 'counters/updates': 6379}
skipping logging after 204160 examples to avoid logging too frequently
train stats after 204192 examples: {'rewards_train/chosen': '0.004093', 'rewards_train/rejected': '0.031799', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027706', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-123.3', 'loss/train': '0.71987', 'examples_per_second': '31.421', 'grad_norm': '61.5', 'counters/examples': 204192, 'counters/updates': 6381}
train stats after 204224 examples: {'rewards_train/chosen': '0.078249', 'rewards_train/rejected': '0.11023', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031985', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-135.08', 'loss/train': '0.71881', 'examples_per_second': '30.516', 'grad_norm': '80', 'counters/examples': 204224, 'counters/updates': 6382}
train stats after 204256 examples: {'rewards_train/chosen': '0.047597', 'rewards_train/rejected': '0.011533', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036064', 'logps_train/rejected': '-159.72', 'logps_train/chosen': '-178.1', 'loss/train': '0.68895', 'examples_per_second': '32.684', 'grad_norm': '74.5', 'counters/examples': 204256, 'counters/updates': 6383}
train stats after 204288 examples: {'rewards_train/chosen': '0.079679', 'rewards_train/rejected': '0.060434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019246', 'logps_train/rejected': '-164.35', 'logps_train/chosen': '-151.85', 'loss/train': '0.69228', 'examples_per_second': '32.127', 'grad_norm': '88.5', 'counters/examples': 204288, 'counters/updates': 6384}
train stats after 204320 examples: {'rewards_train/chosen': '0.25239', 'rewards_train/rejected': '0.087226', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16516', 'logps_train/rejected': '-155.6', 'logps_train/chosen': '-168.7', 'loss/train': '0.64049', 'examples_per_second': '31.596', 'grad_norm': '89.5', 'counters/examples': 204320, 'counters/updates': 6385}
train stats after 204352 examples: {'rewards_train/chosen': '0.13552', 'rewards_train/rejected': '0.046114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089402', 'logps_train/rejected': '-132.03', 'logps_train/chosen': '-175.25', 'loss/train': '0.6853', 'examples_per_second': '31.515', 'grad_norm': '65', 'counters/examples': 204352, 'counters/updates': 6386}
skipping logging after 204384 examples to avoid logging too frequently
train stats after 204416 examples: {'rewards_train/chosen': '0.031632', 'rewards_train/rejected': '0.038249', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0066171', 'logps_train/rejected': '-158.47', 'logps_train/chosen': '-173.17', 'loss/train': '0.71124', 'examples_per_second': '30.092', 'grad_norm': '177', 'counters/examples': 204416, 'counters/updates': 6388}
train stats after 204448 examples: {'rewards_train/chosen': '0.031112', 'rewards_train/rejected': '0.067869', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036758', 'logps_train/rejected': '-187.81', 'logps_train/chosen': '-169.8', 'loss/train': '0.72621', 'examples_per_second': '30.601', 'grad_norm': '88.5', 'counters/examples': 204448, 'counters/updates': 6389}
train stats after 204480 examples: {'rewards_train/chosen': '-0.0002368', 'rewards_train/rejected': '0.082418', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.082655', 'logps_train/rejected': '-144.44', 'logps_train/chosen': '-168.03', 'loss/train': '0.7504', 'examples_per_second': '31.957', 'grad_norm': '93', 'counters/examples': 204480, 'counters/updates': 6390}
train stats after 204512 examples: {'rewards_train/chosen': '0.059188', 'rewards_train/rejected': '0.076668', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01748', 'logps_train/rejected': '-164.07', 'logps_train/chosen': '-133.8', 'loss/train': '0.71223', 'examples_per_second': '31.108', 'grad_norm': '159', 'counters/examples': 204512, 'counters/updates': 6391}
train stats after 204544 examples: {'rewards_train/chosen': '0.18861', 'rewards_train/rejected': '0.047166', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14144', 'logps_train/rejected': '-122.87', 'logps_train/chosen': '-126.09', 'loss/train': '0.63991', 'examples_per_second': '30.689', 'grad_norm': '58', 'counters/examples': 204544, 'counters/updates': 6392}
skipping logging after 204576 examples to avoid logging too frequently
train stats after 204608 examples: {'rewards_train/chosen': '0.072281', 'rewards_train/rejected': '0.088254', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.015974', 'logps_train/rejected': '-143.91', 'logps_train/chosen': '-164.15', 'loss/train': '0.71407', 'examples_per_second': '31.64', 'grad_norm': '90', 'counters/examples': 204608, 'counters/updates': 6394}
train stats after 204640 examples: {'rewards_train/chosen': '0.10093', 'rewards_train/rejected': '0.06986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031074', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-141.69', 'loss/train': '0.69077', 'examples_per_second': '31.285', 'grad_norm': '69.5', 'counters/examples': 204640, 'counters/updates': 6395}
train stats after 204672 examples: {'rewards_train/chosen': '0.047045', 'rewards_train/rejected': '0.060861', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013816', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-116.25', 'loss/train': '0.70763', 'examples_per_second': '31.535', 'grad_norm': '77.5', 'counters/examples': 204672, 'counters/updates': 6396}
train stats after 204704 examples: {'rewards_train/chosen': '0.18049', 'rewards_train/rejected': '0.028308', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15218', 'logps_train/rejected': '-154.57', 'logps_train/chosen': '-180.57', 'loss/train': '0.65036', 'examples_per_second': '31.556', 'grad_norm': '177', 'counters/examples': 204704, 'counters/updates': 6397}
train stats after 204736 examples: {'rewards_train/chosen': '0.085464', 'rewards_train/rejected': '0.050192', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035272', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-154', 'loss/train': '0.68759', 'examples_per_second': '30.477', 'grad_norm': '102', 'counters/examples': 204736, 'counters/updates': 6398}
skipping logging after 204768 examples to avoid logging too frequently
train stats after 204800 examples: {'rewards_train/chosen': '0.083767', 'rewards_train/rejected': '-0.0037284', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087495', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-141.08', 'loss/train': '0.67205', 'examples_per_second': '31.768', 'grad_norm': '60.75', 'counters/examples': 204800, 'counters/updates': 6400}
train stats after 204832 examples: {'rewards_train/chosen': '-0.0061926', 'rewards_train/rejected': '0.015054', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021246', 'logps_train/rejected': '-174.77', 'logps_train/chosen': '-180.34', 'loss/train': '0.7126', 'examples_per_second': '31.364', 'grad_norm': '132', 'counters/examples': 204832, 'counters/updates': 6401}
train stats after 204864 examples: {'rewards_train/chosen': '0.11654', 'rewards_train/rejected': '0.10321', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013328', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-164.56', 'loss/train': '0.6956', 'examples_per_second': '31.66', 'grad_norm': '64.5', 'counters/examples': 204864, 'counters/updates': 6402}
train stats after 204896 examples: {'rewards_train/chosen': '-0.0032693', 'rewards_train/rejected': '0.068475', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.071745', 'logps_train/rejected': '-108.54', 'logps_train/chosen': '-161.27', 'loss/train': '0.74855', 'examples_per_second': '30.599', 'grad_norm': '154', 'counters/examples': 204896, 'counters/updates': 6403}
train stats after 204928 examples: {'rewards_train/chosen': '0.044029', 'rewards_train/rejected': '0.048228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.004199', 'logps_train/rejected': '-107.71', 'logps_train/chosen': '-127.45', 'loss/train': '0.70477', 'examples_per_second': '31.142', 'grad_norm': '103.5', 'counters/examples': 204928, 'counters/updates': 6404}
train stats after 204960 examples: {'rewards_train/chosen': '0.10225', 'rewards_train/rejected': '0.090458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011789', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-192.27', 'loss/train': '0.70765', 'examples_per_second': '31.658', 'grad_norm': '114.5', 'counters/examples': 204960, 'counters/updates': 6405}
train stats after 204992 examples: {'rewards_train/chosen': '0.16061', 'rewards_train/rejected': '0.024858', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13575', 'logps_train/rejected': '-141.24', 'logps_train/chosen': '-164.54', 'loss/train': '0.64873', 'examples_per_second': '31.635', 'grad_norm': '92.5', 'counters/examples': 204992, 'counters/updates': 6406}
train stats after 205024 examples: {'rewards_train/chosen': '0.089539', 'rewards_train/rejected': '0.16988', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.080344', 'logps_train/rejected': '-170.15', 'logps_train/chosen': '-150.12', 'loss/train': '0.75328', 'examples_per_second': '30.207', 'grad_norm': '112.5', 'counters/examples': 205024, 'counters/updates': 6407}
train stats after 205056 examples: {'rewards_train/chosen': '0.17033', 'rewards_train/rejected': '0.056205', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11412', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-151.14', 'loss/train': '0.6503', 'examples_per_second': '32.061', 'grad_norm': '63', 'counters/examples': 205056, 'counters/updates': 6408}
train stats after 205088 examples: {'rewards_train/chosen': '0.037954', 'rewards_train/rejected': '0.044828', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0068746', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-152.65', 'loss/train': '0.71448', 'examples_per_second': '32.753', 'grad_norm': '101', 'counters/examples': 205088, 'counters/updates': 6409}
train stats after 205120 examples: {'rewards_train/chosen': '0.11181', 'rewards_train/rejected': '0.0097091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1021', 'logps_train/rejected': '-127.49', 'logps_train/chosen': '-141.99', 'loss/train': '0.66905', 'examples_per_second': '31.226', 'grad_norm': '77.5', 'counters/examples': 205120, 'counters/updates': 6410}
skipping logging after 205152 examples to avoid logging too frequently
train stats after 205184 examples: {'rewards_train/chosen': '0.14921', 'rewards_train/rejected': '-0.011383', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16059', 'logps_train/rejected': '-140.1', 'logps_train/chosen': '-157.15', 'loss/train': '0.62373', 'examples_per_second': '31.642', 'grad_norm': '107', 'counters/examples': 205184, 'counters/updates': 6412}
train stats after 205216 examples: {'rewards_train/chosen': '0.15266', 'rewards_train/rejected': '0.073328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079331', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-138.68', 'loss/train': '0.66428', 'examples_per_second': '30.851', 'grad_norm': '95', 'counters/examples': 205216, 'counters/updates': 6413}
train stats after 205248 examples: {'rewards_train/chosen': '0.13367', 'rewards_train/rejected': '0.026923', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10674', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-163.2', 'loss/train': '0.64864', 'examples_per_second': '31.692', 'grad_norm': '93.5', 'counters/examples': 205248, 'counters/updates': 6414}
train stats after 205280 examples: {'rewards_train/chosen': '0.20324', 'rewards_train/rejected': '0.071763', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13148', 'logps_train/rejected': '-165.4', 'logps_train/chosen': '-160.01', 'loss/train': '0.63689', 'examples_per_second': '32.176', 'grad_norm': '100', 'counters/examples': 205280, 'counters/updates': 6415}
train stats after 205312 examples: {'rewards_train/chosen': '0.050638', 'rewards_train/rejected': '0.04097', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096682', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-166.35', 'loss/train': '0.70105', 'examples_per_second': '31.657', 'grad_norm': '191', 'counters/examples': 205312, 'counters/updates': 6416}
train stats after 205344 examples: {'rewards_train/chosen': '0.16119', 'rewards_train/rejected': '-0.026224', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18742', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-179.42', 'loss/train': '0.61281', 'examples_per_second': '31.787', 'grad_norm': '143', 'counters/examples': 205344, 'counters/updates': 6417}
train stats after 205376 examples: {'rewards_train/chosen': '0.21469', 'rewards_train/rejected': '-0.016912', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2316', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-165.31', 'loss/train': '0.6128', 'examples_per_second': '31.724', 'grad_norm': '62.25', 'counters/examples': 205376, 'counters/updates': 6418}
train stats after 205408 examples: {'rewards_train/chosen': '0.10901', 'rewards_train/rejected': '-0.0079006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11691', 'logps_train/rejected': '-136.09', 'logps_train/chosen': '-158.54', 'loss/train': '0.66142', 'examples_per_second': '30.318', 'grad_norm': '86.5', 'counters/examples': 205408, 'counters/updates': 6419}
train stats after 205440 examples: {'rewards_train/chosen': '0.18885', 'rewards_train/rejected': '0.055808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13304', 'logps_train/rejected': '-105.14', 'logps_train/chosen': '-152.97', 'loss/train': '0.64593', 'examples_per_second': '31.839', 'grad_norm': '76', 'counters/examples': 205440, 'counters/updates': 6420}
train stats after 205472 examples: {'rewards_train/chosen': '0.098246', 'rewards_train/rejected': '0.0097797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088466', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-122.68', 'loss/train': '0.65966', 'examples_per_second': '31.599', 'grad_norm': '65', 'counters/examples': 205472, 'counters/updates': 6421}
train stats after 205504 examples: {'rewards_train/chosen': '0.16151', 'rewards_train/rejected': '0.11431', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047198', 'logps_train/rejected': '-164.94', 'logps_train/chosen': '-181.66', 'loss/train': '0.68867', 'examples_per_second': '30.473', 'grad_norm': '90.5', 'counters/examples': 205504, 'counters/updates': 6422}
skipping logging after 205536 examples to avoid logging too frequently
train stats after 205568 examples: {'rewards_train/chosen': '0.12899', 'rewards_train/rejected': '-0.0015063', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1305', 'logps_train/rejected': '-141.04', 'logps_train/chosen': '-155.61', 'loss/train': '0.64038', 'examples_per_second': '31.572', 'grad_norm': '63.25', 'counters/examples': 205568, 'counters/updates': 6424}
train stats after 205600 examples: {'rewards_train/chosen': '0.12751', 'rewards_train/rejected': '0.073807', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053699', 'logps_train/rejected': '-146.82', 'logps_train/chosen': '-168.37', 'loss/train': '0.68448', 'examples_per_second': '30.154', 'grad_norm': '70.5', 'counters/examples': 205600, 'counters/updates': 6425}
train stats after 205632 examples: {'rewards_train/chosen': '-0.035515', 'rewards_train/rejected': '0.080985', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.1165', 'logps_train/rejected': '-158.5', 'logps_train/chosen': '-143.03', 'loss/train': '0.76765', 'examples_per_second': '30.738', 'grad_norm': '79', 'counters/examples': 205632, 'counters/updates': 6426}
train stats after 205664 examples: {'rewards_train/chosen': '0.13117', 'rewards_train/rejected': '0.061543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069631', 'logps_train/rejected': '-151.38', 'logps_train/chosen': '-163.9', 'loss/train': '0.67727', 'examples_per_second': '31.682', 'grad_norm': '65.5', 'counters/examples': 205664, 'counters/updates': 6427}
train stats after 205696 examples: {'rewards_train/chosen': '0.070694', 'rewards_train/rejected': '0.016939', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053755', 'logps_train/rejected': '-91.239', 'logps_train/chosen': '-122.87', 'loss/train': '0.67981', 'examples_per_second': '30.276', 'grad_norm': '42', 'counters/examples': 205696, 'counters/updates': 6428}
train stats after 205728 examples: {'rewards_train/chosen': '0.022337', 'rewards_train/rejected': '0.025222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028852', 'logps_train/rejected': '-143.17', 'logps_train/chosen': '-146.31', 'loss/train': '0.70559', 'examples_per_second': '30.136', 'grad_norm': '82.5', 'counters/examples': 205728, 'counters/updates': 6429}
train stats after 205760 examples: {'rewards_train/chosen': '0.045977', 'rewards_train/rejected': '0.079', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033023', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-158.01', 'loss/train': '0.72562', 'examples_per_second': '31.66', 'grad_norm': '152', 'counters/examples': 205760, 'counters/updates': 6430}
skipping logging after 205792 examples to avoid logging too frequently
train stats after 205824 examples: {'rewards_train/chosen': '0.027202', 'rewards_train/rejected': '0.10805', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.080846', 'logps_train/rejected': '-159.31', 'logps_train/chosen': '-170.8', 'loss/train': '0.75247', 'examples_per_second': '30.832', 'grad_norm': '122.5', 'counters/examples': 205824, 'counters/updates': 6432}
train stats after 205856 examples: {'rewards_train/chosen': '0.14036', 'rewards_train/rejected': '0.020789', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11958', 'logps_train/rejected': '-119.88', 'logps_train/chosen': '-141.09', 'loss/train': '0.64572', 'examples_per_second': '31.183', 'grad_norm': '65', 'counters/examples': 205856, 'counters/updates': 6433}
train stats after 205888 examples: {'rewards_train/chosen': '0.093139', 'rewards_train/rejected': '0.00096756', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092171', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-150.68', 'loss/train': '0.65989', 'examples_per_second': '31.285', 'grad_norm': '65', 'counters/examples': 205888, 'counters/updates': 6434}
train stats after 205920 examples: {'rewards_train/chosen': '0.070082', 'rewards_train/rejected': '0.038328', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031754', 'logps_train/rejected': '-146.63', 'logps_train/chosen': '-126.9', 'loss/train': '0.68653', 'examples_per_second': '32.244', 'grad_norm': '128', 'counters/examples': 205920, 'counters/updates': 6435}
train stats after 205952 examples: {'rewards_train/chosen': '0.13733', 'rewards_train/rejected': '-0.0062021', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14353', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-168.87', 'loss/train': '0.63178', 'examples_per_second': '32.91', 'grad_norm': '101.5', 'counters/examples': 205952, 'counters/updates': 6436}
train stats after 205984 examples: {'rewards_train/chosen': '0.13165', 'rewards_train/rejected': '0.050976', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080673', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-177.55', 'loss/train': '0.66896', 'examples_per_second': '31.693', 'grad_norm': '88', 'counters/examples': 205984, 'counters/updates': 6437}
skipping logging after 206016 examples to avoid logging too frequently
train stats after 206048 examples: {'rewards_train/chosen': '0.1321', 'rewards_train/rejected': '0.10269', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02941', 'logps_train/rejected': '-166.99', 'logps_train/chosen': '-150.22', 'loss/train': '0.6995', 'examples_per_second': '31.655', 'grad_norm': '76.5', 'counters/examples': 206048, 'counters/updates': 6439}
train stats after 206080 examples: {'rewards_train/chosen': '0.15139', 'rewards_train/rejected': '0.045184', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10621', 'logps_train/rejected': '-153.25', 'logps_train/chosen': '-168.08', 'loss/train': '0.66394', 'examples_per_second': '31.631', 'grad_norm': '150', 'counters/examples': 206080, 'counters/updates': 6440}
train stats after 206112 examples: {'rewards_train/chosen': '0.1238', 'rewards_train/rejected': '0.14269', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018888', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-141.52', 'loss/train': '0.71823', 'examples_per_second': '32.596', 'grad_norm': '54.75', 'counters/examples': 206112, 'counters/updates': 6441}
train stats after 206144 examples: {'rewards_train/chosen': '0.098659', 'rewards_train/rejected': '0.20156', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.1029', 'logps_train/rejected': '-134.26', 'logps_train/chosen': '-166.96', 'loss/train': '0.75801', 'examples_per_second': '30.149', 'grad_norm': '122.5', 'counters/examples': 206144, 'counters/updates': 6442}
train stats after 206176 examples: {'rewards_train/chosen': '0.056668', 'rewards_train/rejected': '-0.014151', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070819', 'logps_train/rejected': '-105.57', 'logps_train/chosen': '-175.81', 'loss/train': '0.66638', 'examples_per_second': '31.975', 'grad_norm': '81.5', 'counters/examples': 206176, 'counters/updates': 6443}
train stats after 206208 examples: {'rewards_train/chosen': '0.1598', 'rewards_train/rejected': '0.043059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11674', 'logps_train/rejected': '-125.67', 'logps_train/chosen': '-137.16', 'loss/train': '0.64783', 'examples_per_second': '30.818', 'grad_norm': '63.5', 'counters/examples': 206208, 'counters/updates': 6444}
train stats after 206240 examples: {'rewards_train/chosen': '0.1213', 'rewards_train/rejected': '0.12129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '1.3114e-05', 'logps_train/rejected': '-103.74', 'logps_train/chosen': '-105.96', 'loss/train': '0.70746', 'examples_per_second': '31.393', 'grad_norm': '90', 'counters/examples': 206240, 'counters/updates': 6445}
train stats after 206272 examples: {'rewards_train/chosen': '0.031463', 'rewards_train/rejected': '0.029311', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0021513', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-160.37', 'loss/train': '0.70025', 'examples_per_second': '30.115', 'grad_norm': '140', 'counters/examples': 206272, 'counters/updates': 6446}
skipping logging after 206304 examples to avoid logging too frequently
train stats after 206336 examples: {'rewards_train/chosen': '0.12265', 'rewards_train/rejected': '0.13443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011783', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-161.23', 'loss/train': '0.72936', 'examples_per_second': '31.67', 'grad_norm': '98', 'counters/examples': 206336, 'counters/updates': 6448}
train stats after 206368 examples: {'rewards_train/chosen': '0.24114', 'rewards_train/rejected': '-0.042868', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28401', 'logps_train/rejected': '-110.59', 'logps_train/chosen': '-156.04', 'loss/train': '0.5875', 'examples_per_second': '32.25', 'grad_norm': '90', 'counters/examples': 206368, 'counters/updates': 6449}
train stats after 206400 examples: {'rewards_train/chosen': '0.20725', 'rewards_train/rejected': '0.017304', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18994', 'logps_train/rejected': '-144.44', 'logps_train/chosen': '-152.91', 'loss/train': '0.62381', 'examples_per_second': '31.282', 'grad_norm': '55', 'counters/examples': 206400, 'counters/updates': 6450}
train stats after 206432 examples: {'rewards_train/chosen': '0.11417', 'rewards_train/rejected': '0.057703', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056465', 'logps_train/rejected': '-195.45', 'logps_train/chosen': '-168.42', 'loss/train': '0.68192', 'examples_per_second': '30.751', 'grad_norm': '120.5', 'counters/examples': 206432, 'counters/updates': 6451}
skipping logging after 206464 examples to avoid logging too frequently
train stats after 206496 examples: {'rewards_train/chosen': '0.11326', 'rewards_train/rejected': '0.064993', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048263', 'logps_train/rejected': '-147.98', 'logps_train/chosen': '-147.08', 'loss/train': '0.67764', 'examples_per_second': '25.704', 'grad_norm': '74.5', 'counters/examples': 206496, 'counters/updates': 6453}
train stats after 206528 examples: {'rewards_train/chosen': '0.12363', 'rewards_train/rejected': '0.076393', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047236', 'logps_train/rejected': '-113.95', 'logps_train/chosen': '-115.15', 'loss/train': '0.6834', 'examples_per_second': '32.448', 'grad_norm': '61', 'counters/examples': 206528, 'counters/updates': 6454}
train stats after 206560 examples: {'rewards_train/chosen': '0.095479', 'rewards_train/rejected': '0.047662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047816', 'logps_train/rejected': '-154.28', 'logps_train/chosen': '-160.54', 'loss/train': '0.68601', 'examples_per_second': '32.807', 'grad_norm': '74.5', 'counters/examples': 206560, 'counters/updates': 6455}
train stats after 206592 examples: {'rewards_train/chosen': '0.20631', 'rewards_train/rejected': '0.10273', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10359', 'logps_train/rejected': '-165.78', 'logps_train/chosen': '-185.99', 'loss/train': '0.65887', 'examples_per_second': '31.642', 'grad_norm': '137', 'counters/examples': 206592, 'counters/updates': 6456}
skipping logging after 206624 examples to avoid logging too frequently
train stats after 206656 examples: {'rewards_train/chosen': '0.099899', 'rewards_train/rejected': '-0.010617', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11052', 'logps_train/rejected': '-114.26', 'logps_train/chosen': '-125.73', 'loss/train': '0.64908', 'examples_per_second': '39.194', 'grad_norm': '127.5', 'counters/examples': 206656, 'counters/updates': 6458}
train stats after 206688 examples: {'rewards_train/chosen': '0.1264', 'rewards_train/rejected': '0.0070642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11933', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-145.82', 'loss/train': '0.65144', 'examples_per_second': '30.63', 'grad_norm': '76', 'counters/examples': 206688, 'counters/updates': 6459}
train stats after 206720 examples: {'rewards_train/chosen': '0.15675', 'rewards_train/rejected': '0.031636', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12511', 'logps_train/rejected': '-99.17', 'logps_train/chosen': '-140.42', 'loss/train': '0.64602', 'examples_per_second': '32.468', 'grad_norm': '50.5', 'counters/examples': 206720, 'counters/updates': 6460}
train stats after 206752 examples: {'rewards_train/chosen': '0.13775', 'rewards_train/rejected': '0.096424', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041323', 'logps_train/rejected': '-142.53', 'logps_train/chosen': '-177.23', 'loss/train': '0.68492', 'examples_per_second': '32.043', 'grad_norm': '79', 'counters/examples': 206752, 'counters/updates': 6461}
skipping logging after 206784 examples to avoid logging too frequently
train stats after 206816 examples: {'rewards_train/chosen': '0.06126', 'rewards_train/rejected': '0.053236', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.008024', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-115.68', 'loss/train': '0.69856', 'examples_per_second': '34.485', 'grad_norm': '105', 'counters/examples': 206816, 'counters/updates': 6463}
train stats after 206848 examples: {'rewards_train/chosen': '0.10217', 'rewards_train/rejected': '0.085557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016615', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-159', 'loss/train': '0.69468', 'examples_per_second': '31.408', 'grad_norm': '76', 'counters/examples': 206848, 'counters/updates': 6464}
train stats after 206880 examples: {'rewards_train/chosen': '0.19932', 'rewards_train/rejected': '0.059508', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13981', 'logps_train/rejected': '-142.38', 'logps_train/chosen': '-149.33', 'loss/train': '0.6405', 'examples_per_second': '31.137', 'grad_norm': '122', 'counters/examples': 206880, 'counters/updates': 6465}
train stats after 206912 examples: {'rewards_train/chosen': '0.13453', 'rewards_train/rejected': '-0.086233', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22076', 'logps_train/rejected': '-154.32', 'logps_train/chosen': '-150.1', 'loss/train': '0.60445', 'examples_per_second': '32.411', 'grad_norm': '118', 'counters/examples': 206912, 'counters/updates': 6466}
train stats after 206944 examples: {'rewards_train/chosen': '0.16684', 'rewards_train/rejected': '0.054202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11263', 'logps_train/rejected': '-133.63', 'logps_train/chosen': '-141.88', 'loss/train': '0.65248', 'examples_per_second': '32.289', 'grad_norm': '78', 'counters/examples': 206944, 'counters/updates': 6467}
train stats after 206976 examples: {'rewards_train/chosen': '0.066959', 'rewards_train/rejected': '-0.0063119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073271', 'logps_train/rejected': '-139.98', 'logps_train/chosen': '-155.61', 'loss/train': '0.67641', 'examples_per_second': '31.918', 'grad_norm': '111', 'counters/examples': 206976, 'counters/updates': 6468}
train stats after 207008 examples: {'rewards_train/chosen': '0.18701', 'rewards_train/rejected': '0.097036', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089972', 'logps_train/rejected': '-152.4', 'logps_train/chosen': '-174.78', 'loss/train': '0.68361', 'examples_per_second': '31.366', 'grad_norm': '78.5', 'counters/examples': 207008, 'counters/updates': 6469}
train stats after 207040 examples: {'rewards_train/chosen': '0.077162', 'rewards_train/rejected': '0.074057', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0031053', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-136.7', 'loss/train': '0.71027', 'examples_per_second': '31.608', 'grad_norm': '87.5', 'counters/examples': 207040, 'counters/updates': 6470}
train stats after 207072 examples: {'rewards_train/chosen': '0.13004', 'rewards_train/rejected': '0.02831', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10173', 'logps_train/rejected': '-114.54', 'logps_train/chosen': '-146.48', 'loss/train': '0.65032', 'examples_per_second': '32.35', 'grad_norm': '72', 'counters/examples': 207072, 'counters/updates': 6471}
train stats after 207104 examples: {'rewards_train/chosen': '0.21952', 'rewards_train/rejected': '0.12277', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096744', 'logps_train/rejected': '-184.26', 'logps_train/chosen': '-180.27', 'loss/train': '0.67105', 'examples_per_second': '31.626', 'grad_norm': '141', 'counters/examples': 207104, 'counters/updates': 6472}
train stats after 207136 examples: {'rewards_train/chosen': '0.039064', 'rewards_train/rejected': '-0.010701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049765', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-120.1', 'loss/train': '0.67604', 'examples_per_second': '32.637', 'grad_norm': '76', 'counters/examples': 207136, 'counters/updates': 6473}
train stats after 207168 examples: {'rewards_train/chosen': '0.076371', 'rewards_train/rejected': '0.080915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0045441', 'logps_train/rejected': '-159.68', 'logps_train/chosen': '-150.74', 'loss/train': '0.71266', 'examples_per_second': '31.328', 'grad_norm': '110.5', 'counters/examples': 207168, 'counters/updates': 6474}
train stats after 207200 examples: {'rewards_train/chosen': '0.06023', 'rewards_train/rejected': '0.023963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036266', 'logps_train/rejected': '-115.4', 'logps_train/chosen': '-162.25', 'loss/train': '0.68537', 'examples_per_second': '32.183', 'grad_norm': '71.5', 'counters/examples': 207200, 'counters/updates': 6475}
train stats after 207232 examples: {'rewards_train/chosen': '0.091699', 'rewards_train/rejected': '0.002581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089118', 'logps_train/rejected': '-130.88', 'logps_train/chosen': '-138.16', 'loss/train': '0.6634', 'examples_per_second': '32.377', 'grad_norm': '90.5', 'counters/examples': 207232, 'counters/updates': 6476}
train stats after 207264 examples: {'rewards_train/chosen': '0.11158', 'rewards_train/rejected': '0.15085', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.039265', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-152.83', 'loss/train': '0.73131', 'examples_per_second': '30.687', 'grad_norm': '168', 'counters/examples': 207264, 'counters/updates': 6477}
train stats after 207296 examples: {'rewards_train/chosen': '0.08955', 'rewards_train/rejected': '0.037145', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052405', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-140.52', 'loss/train': '0.68121', 'examples_per_second': '33.103', 'grad_norm': '86', 'counters/examples': 207296, 'counters/updates': 6478}
skipping logging after 207328 examples to avoid logging too frequently
train stats after 207360 examples: {'rewards_train/chosen': '0.14971', 'rewards_train/rejected': '0.040209', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1095', 'logps_train/rejected': '-138.74', 'logps_train/chosen': '-178.49', 'loss/train': '0.65469', 'examples_per_second': '31.576', 'grad_norm': '62', 'counters/examples': 207360, 'counters/updates': 6480}
train stats after 207392 examples: {'rewards_train/chosen': '0.23094', 'rewards_train/rejected': '0.015544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.2154', 'logps_train/rejected': '-153.09', 'logps_train/chosen': '-197.35', 'loss/train': '0.61358', 'examples_per_second': '30.622', 'grad_norm': '90', 'counters/examples': 207392, 'counters/updates': 6481}
skipping logging after 207424 examples to avoid logging too frequently
train stats after 207456 examples: {'rewards_train/chosen': '0.11359', 'rewards_train/rejected': '0.016661', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09693', 'logps_train/rejected': '-130.75', 'logps_train/chosen': '-144.61', 'loss/train': '0.65304', 'examples_per_second': '30.837', 'grad_norm': '51', 'counters/examples': 207456, 'counters/updates': 6483}
train stats after 207488 examples: {'rewards_train/chosen': '0.051893', 'rewards_train/rejected': '0.042601', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0092927', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-105.94', 'loss/train': '0.69757', 'examples_per_second': '33.078', 'grad_norm': '93.5', 'counters/examples': 207488, 'counters/updates': 6484}
train stats after 207520 examples: {'rewards_train/chosen': '0.11495', 'rewards_train/rejected': '0.090333', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024615', 'logps_train/rejected': '-125.34', 'logps_train/chosen': '-153.95', 'loss/train': '0.69681', 'examples_per_second': '31.67', 'grad_norm': '91', 'counters/examples': 207520, 'counters/updates': 6485}
train stats after 207552 examples: {'rewards_train/chosen': '0.10139', 'rewards_train/rejected': '0.1042', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0028006', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-156.1', 'loss/train': '0.70924', 'examples_per_second': '30.687', 'grad_norm': '75', 'counters/examples': 207552, 'counters/updates': 6486}
train stats after 207584 examples: {'rewards_train/chosen': '0.050836', 'rewards_train/rejected': '-0.0040217', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.054858', 'logps_train/rejected': '-114.03', 'logps_train/chosen': '-146.58', 'loss/train': '0.67585', 'examples_per_second': '32.733', 'grad_norm': '93', 'counters/examples': 207584, 'counters/updates': 6487}
train stats after 207616 examples: {'rewards_train/chosen': '0.068391', 'rewards_train/rejected': '0.0075716', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060819', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-138.73', 'loss/train': '0.67296', 'examples_per_second': '30.695', 'grad_norm': '72', 'counters/examples': 207616, 'counters/updates': 6488}
train stats after 207648 examples: {'rewards_train/chosen': '0.1539', 'rewards_train/rejected': '0.0017177', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15218', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-181.04', 'loss/train': '0.6396', 'examples_per_second': '32.727', 'grad_norm': '77.5', 'counters/examples': 207648, 'counters/updates': 6489}
train stats after 207680 examples: {'rewards_train/chosen': '0.073301', 'rewards_train/rejected': '0.040805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032496', 'logps_train/rejected': '-114.53', 'logps_train/chosen': '-120.43', 'loss/train': '0.68918', 'examples_per_second': '31.435', 'grad_norm': '46.25', 'counters/examples': 207680, 'counters/updates': 6490}
train stats after 207712 examples: {'rewards_train/chosen': '0.13073', 'rewards_train/rejected': '-0.03859', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16932', 'logps_train/rejected': '-116.74', 'logps_train/chosen': '-153.65', 'loss/train': '0.6296', 'examples_per_second': '30.576', 'grad_norm': '80.5', 'counters/examples': 207712, 'counters/updates': 6491}
train stats after 207744 examples: {'rewards_train/chosen': '0.22623', 'rewards_train/rejected': '0.0067565', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21947', 'logps_train/rejected': '-167.32', 'logps_train/chosen': '-183.39', 'loss/train': '0.60736', 'examples_per_second': '32.434', 'grad_norm': '74.5', 'counters/examples': 207744, 'counters/updates': 6492}
train stats after 207776 examples: {'rewards_train/chosen': '0.17158', 'rewards_train/rejected': '0.063684', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10789', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-174.65', 'loss/train': '0.6469', 'examples_per_second': '30.642', 'grad_norm': '71.5', 'counters/examples': 207776, 'counters/updates': 6493}
train stats after 207808 examples: {'rewards_train/chosen': '0.076785', 'rewards_train/rejected': '0.038025', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.03876', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-154.3', 'loss/train': '0.68672', 'examples_per_second': '24.375', 'grad_norm': '121.5', 'counters/examples': 207808, 'counters/updates': 6494}
skipping logging after 207840 examples to avoid logging too frequently
train stats after 207872 examples: {'rewards_train/chosen': '0.17995', 'rewards_train/rejected': '0.076477', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10348', 'logps_train/rejected': '-123.97', 'logps_train/chosen': '-159.93', 'loss/train': '0.65101', 'examples_per_second': '31.354', 'grad_norm': '78.5', 'counters/examples': 207872, 'counters/updates': 6496}
train stats after 207904 examples: {'rewards_train/chosen': '0.20746', 'rewards_train/rejected': '0.0085112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19895', 'logps_train/rejected': '-156.22', 'logps_train/chosen': '-170.38', 'loss/train': '0.61836', 'examples_per_second': '24.401', 'grad_norm': '86.5', 'counters/examples': 207904, 'counters/updates': 6497}
train stats after 207936 examples: {'rewards_train/chosen': '0.184', 'rewards_train/rejected': '0.064712', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11929', 'logps_train/rejected': '-157.38', 'logps_train/chosen': '-234.13', 'loss/train': '0.66884', 'examples_per_second': '31.676', 'grad_norm': '112.5', 'counters/examples': 207936, 'counters/updates': 6498}
train stats after 207968 examples: {'rewards_train/chosen': '0.14913', 'rewards_train/rejected': '0.036032', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1131', 'logps_train/rejected': '-127.72', 'logps_train/chosen': '-163.74', 'loss/train': '0.6583', 'examples_per_second': '31.634', 'grad_norm': '74.5', 'counters/examples': 207968, 'counters/updates': 6499}
skipping logging after 208000 examples to avoid logging too frequently
train stats after 208032 examples: {'rewards_train/chosen': '0.071259', 'rewards_train/rejected': '0.099741', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.028483', 'logps_train/rejected': '-169.96', 'logps_train/chosen': '-147.73', 'loss/train': '0.72557', 'examples_per_second': '32.07', 'grad_norm': '212', 'counters/examples': 208032, 'counters/updates': 6501}
train stats after 208064 examples: {'rewards_train/chosen': '0.084259', 'rewards_train/rejected': '0.027801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056458', 'logps_train/rejected': '-159.67', 'logps_train/chosen': '-136.94', 'loss/train': '0.67675', 'examples_per_second': '31.659', 'grad_norm': '181', 'counters/examples': 208064, 'counters/updates': 6502}
train stats after 208096 examples: {'rewards_train/chosen': '0.17496', 'rewards_train/rejected': '-0.011354', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18631', 'logps_train/rejected': '-160.5', 'logps_train/chosen': '-150.01', 'loss/train': '0.61705', 'examples_per_second': '31.625', 'grad_norm': '158', 'counters/examples': 208096, 'counters/updates': 6503}
train stats after 208128 examples: {'rewards_train/chosen': '0.15746', 'rewards_train/rejected': '0.054399', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10307', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-182.14', 'loss/train': '0.65106', 'examples_per_second': '31.305', 'grad_norm': '71', 'counters/examples': 208128, 'counters/updates': 6504}
train stats after 208160 examples: {'rewards_train/chosen': '0.069937', 'rewards_train/rejected': '0.075407', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0054697', 'logps_train/rejected': '-149.31', 'logps_train/chosen': '-151.61', 'loss/train': '0.70692', 'examples_per_second': '31.153', 'grad_norm': '103.5', 'counters/examples': 208160, 'counters/updates': 6505}
skipping logging after 208192 examples to avoid logging too frequently
train stats after 208224 examples: {'rewards_train/chosen': '0.097788', 'rewards_train/rejected': '0.091494', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0062947', 'logps_train/rejected': '-155.69', 'logps_train/chosen': '-191.49', 'loss/train': '0.72288', 'examples_per_second': '34.827', 'grad_norm': '197', 'counters/examples': 208224, 'counters/updates': 6507}
skipping logging after 208256 examples to avoid logging too frequently
train stats after 208288 examples: {'rewards_train/chosen': '0.09433', 'rewards_train/rejected': '0.052964', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041366', 'logps_train/rejected': '-131.34', 'logps_train/chosen': '-127.55', 'loss/train': '0.68482', 'examples_per_second': '30.594', 'grad_norm': '93.5', 'counters/examples': 208288, 'counters/updates': 6509}
skipping logging after 208320 examples to avoid logging too frequently
train stats after 208352 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '0.13256', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028151', 'logps_train/rejected': '-153.53', 'logps_train/chosen': '-92.141', 'loss/train': '0.71798', 'examples_per_second': '31.293', 'grad_norm': '92.5', 'counters/examples': 208352, 'counters/updates': 6511}
train stats after 208384 examples: {'rewards_train/chosen': '0.094849', 'rewards_train/rejected': '0.1154', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020551', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-122.34', 'loss/train': '0.71321', 'examples_per_second': '31.592', 'grad_norm': '107.5', 'counters/examples': 208384, 'counters/updates': 6512}
train stats after 208416 examples: {'rewards_train/chosen': '0.108', 'rewards_train/rejected': '0.026654', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.081344', 'logps_train/rejected': '-153.58', 'logps_train/chosen': '-140.84', 'loss/train': '0.67022', 'examples_per_second': '31.77', 'grad_norm': '69', 'counters/examples': 208416, 'counters/updates': 6513}
train stats after 208448 examples: {'rewards_train/chosen': '0.098597', 'rewards_train/rejected': '0.084736', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01386', 'logps_train/rejected': '-104.1', 'logps_train/chosen': '-182.38', 'loss/train': '0.69644', 'examples_per_second': '31.155', 'grad_norm': '68.5', 'counters/examples': 208448, 'counters/updates': 6514}
train stats after 208480 examples: {'rewards_train/chosen': '0.11629', 'rewards_train/rejected': '0.074171', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042117', 'logps_train/rejected': '-138.97', 'logps_train/chosen': '-156.98', 'loss/train': '0.6845', 'examples_per_second': '30.203', 'grad_norm': '79.5', 'counters/examples': 208480, 'counters/updates': 6515}
train stats after 208512 examples: {'rewards_train/chosen': '0.16618', 'rewards_train/rejected': '0.023288', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1429', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-152.19', 'loss/train': '0.63617', 'examples_per_second': '30.749', 'grad_norm': '294', 'counters/examples': 208512, 'counters/updates': 6516}
skipping logging after 208544 examples to avoid logging too frequently
train stats after 208576 examples: {'rewards_train/chosen': '0.096145', 'rewards_train/rejected': '0.010176', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085969', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-141.56', 'loss/train': '0.66229', 'examples_per_second': '31.2', 'grad_norm': '69', 'counters/examples': 208576, 'counters/updates': 6518}
train stats after 208608 examples: {'rewards_train/chosen': '0.13807', 'rewards_train/rejected': '0.044716', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093353', 'logps_train/rejected': '-175.75', 'logps_train/chosen': '-174.73', 'loss/train': '0.65719', 'examples_per_second': '32.565', 'grad_norm': '132', 'counters/examples': 208608, 'counters/updates': 6519}
skipping logging after 208640 examples to avoid logging too frequently
train stats after 208672 examples: {'rewards_train/chosen': '0.2212', 'rewards_train/rejected': '0.22436', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.00316', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-174.24', 'loss/train': '0.71935', 'examples_per_second': '32.588', 'grad_norm': '696', 'counters/examples': 208672, 'counters/updates': 6521}
skipping logging after 208704 examples to avoid logging too frequently
train stats after 208736 examples: {'rewards_train/chosen': '0.11624', 'rewards_train/rejected': '-0.0043539', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12059', 'logps_train/rejected': '-147.28', 'logps_train/chosen': '-145.5', 'loss/train': '0.64959', 'examples_per_second': '34.559', 'grad_norm': '74', 'counters/examples': 208736, 'counters/updates': 6523}
train stats after 208768 examples: {'rewards_train/chosen': '0.098769', 'rewards_train/rejected': '0.076163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022607', 'logps_train/rejected': '-122.39', 'logps_train/chosen': '-124.45', 'loss/train': '0.69348', 'examples_per_second': '30.73', 'grad_norm': '92.5', 'counters/examples': 208768, 'counters/updates': 6524}
skipping logging after 208800 examples to avoid logging too frequently
train stats after 208832 examples: {'rewards_train/chosen': '0.080341', 'rewards_train/rejected': '-0.028145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10849', 'logps_train/rejected': '-108.53', 'logps_train/chosen': '-127.5', 'loss/train': '0.65523', 'examples_per_second': '36.237', 'grad_norm': '60.75', 'counters/examples': 208832, 'counters/updates': 6526}
train stats after 208864 examples: {'rewards_train/chosen': '0.12067', 'rewards_train/rejected': '-0.025073', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14574', 'logps_train/rejected': '-144.68', 'logps_train/chosen': '-155.98', 'loss/train': '0.63476', 'examples_per_second': '31.304', 'grad_norm': '64.5', 'counters/examples': 208864, 'counters/updates': 6527}
train stats after 208896 examples: {'rewards_train/chosen': '0.076154', 'rewards_train/rejected': '0.057072', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019081', 'logps_train/rejected': '-123.18', 'logps_train/chosen': '-134.02', 'loss/train': '0.69443', 'examples_per_second': '31.498', 'grad_norm': '88', 'counters/examples': 208896, 'counters/updates': 6528}
skipping logging after 208928 examples to avoid logging too frequently
train stats after 208960 examples: {'rewards_train/chosen': '0.031268', 'rewards_train/rejected': '0.076532', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.045265', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-139.19', 'loss/train': '0.72281', 'examples_per_second': '31.594', 'grad_norm': '54.25', 'counters/examples': 208960, 'counters/updates': 6530}
train stats after 208992 examples: {'rewards_train/chosen': '0.11742', 'rewards_train/rejected': '0.058629', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058786', 'logps_train/rejected': '-148.11', 'logps_train/chosen': '-180.87', 'loss/train': '0.67702', 'examples_per_second': '30.137', 'grad_norm': '105.5', 'counters/examples': 208992, 'counters/updates': 6531}
train stats after 209024 examples: {'rewards_train/chosen': '0.085957', 'rewards_train/rejected': '0.031417', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.054541', 'logps_train/rejected': '-120.71', 'logps_train/chosen': '-135.72', 'loss/train': '0.68136', 'examples_per_second': '31.521', 'grad_norm': '96', 'counters/examples': 209024, 'counters/updates': 6532}
train stats after 209056 examples: {'rewards_train/chosen': '0.10435', 'rewards_train/rejected': '0.058353', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045994', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-184.85', 'loss/train': '0.69317', 'examples_per_second': '32.64', 'grad_norm': '103', 'counters/examples': 209056, 'counters/updates': 6533}
skipping logging after 209088 examples to avoid logging too frequently
train stats after 209120 examples: {'rewards_train/chosen': '0.11726', 'rewards_train/rejected': '0.13026', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013005', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-149.57', 'loss/train': '0.71734', 'examples_per_second': '32.822', 'grad_norm': '113.5', 'counters/examples': 209120, 'counters/updates': 6535}
train stats after 209152 examples: {'rewards_train/chosen': '0.14431', 'rewards_train/rejected': '0.022491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12182', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-176.77', 'loss/train': '0.64579', 'examples_per_second': '31.62', 'grad_norm': '93', 'counters/examples': 209152, 'counters/updates': 6536}
train stats after 209184 examples: {'rewards_train/chosen': '0.044283', 'rewards_train/rejected': '0.083509', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.039226', 'logps_train/rejected': '-152.71', 'logps_train/chosen': '-150.52', 'loss/train': '0.72836', 'examples_per_second': '30.981', 'grad_norm': '73', 'counters/examples': 209184, 'counters/updates': 6537}
train stats after 209216 examples: {'rewards_train/chosen': '0.086805', 'rewards_train/rejected': '0.093852', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0070471', 'logps_train/rejected': '-155.76', 'logps_train/chosen': '-128.55', 'loss/train': '0.70888', 'examples_per_second': '31.633', 'grad_norm': '86.5', 'counters/examples': 209216, 'counters/updates': 6538}
train stats after 209248 examples: {'rewards_train/chosen': '0.051263', 'rewards_train/rejected': '0.0063104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044952', 'logps_train/rejected': '-162.31', 'logps_train/chosen': '-163.84', 'loss/train': '0.6817', 'examples_per_second': '31.552', 'grad_norm': '113', 'counters/examples': 209248, 'counters/updates': 6539}
train stats after 209280 examples: {'rewards_train/chosen': '0.15542', 'rewards_train/rejected': '0.037044', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11838', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-172.93', 'loss/train': '0.6506', 'examples_per_second': '31.496', 'grad_norm': '68', 'counters/examples': 209280, 'counters/updates': 6540}
skipping logging after 209312 examples to avoid logging too frequently
train stats after 209344 examples: {'rewards_train/chosen': '0.094597', 'rewards_train/rejected': '-0.0047339', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09933', 'logps_train/rejected': '-105.45', 'logps_train/chosen': '-177.37', 'loss/train': '0.65438', 'examples_per_second': '33.458', 'grad_norm': '105', 'counters/examples': 209344, 'counters/updates': 6542}
skipping logging after 209376 examples to avoid logging too frequently
train stats after 209408 examples: {'rewards_train/chosen': '0.097825', 'rewards_train/rejected': '0.010772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087052', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-126.3', 'loss/train': '0.6578', 'examples_per_second': '31.786', 'grad_norm': '121.5', 'counters/examples': 209408, 'counters/updates': 6544}
train stats after 209440 examples: {'rewards_train/chosen': '0.12467', 'rewards_train/rejected': '0.016399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10827', 'logps_train/rejected': '-99.553', 'logps_train/chosen': '-121.88', 'loss/train': '0.64821', 'examples_per_second': '32.113', 'grad_norm': '47.25', 'counters/examples': 209440, 'counters/updates': 6545}
train stats after 209472 examples: {'rewards_train/chosen': '0.077579', 'rewards_train/rejected': '0.093308', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.01573', 'logps_train/rejected': '-113.89', 'logps_train/chosen': '-102.9', 'loss/train': '0.71806', 'examples_per_second': '29.881', 'grad_norm': '72', 'counters/examples': 209472, 'counters/updates': 6546}
skipping logging after 209504 examples to avoid logging too frequently
train stats after 209536 examples: {'rewards_train/chosen': '0.12959', 'rewards_train/rejected': '0.073466', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056121', 'logps_train/rejected': '-114.85', 'logps_train/chosen': '-153.32', 'loss/train': '0.68069', 'examples_per_second': '34.383', 'grad_norm': '54.75', 'counters/examples': 209536, 'counters/updates': 6548}
train stats after 209568 examples: {'rewards_train/chosen': '0.080447', 'rewards_train/rejected': '0.0069149', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.073532', 'logps_train/rejected': '-165.99', 'logps_train/chosen': '-201.09', 'loss/train': '0.68959', 'examples_per_second': '31.558', 'grad_norm': '138', 'counters/examples': 209568, 'counters/updates': 6549}
train stats after 209600 examples: {'rewards_train/chosen': '0.07662', 'rewards_train/rejected': '0.11817', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041549', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-144.9', 'loss/train': '0.72572', 'examples_per_second': '31.401', 'grad_norm': '91.5', 'counters/examples': 209600, 'counters/updates': 6550}
train stats after 209632 examples: {'rewards_train/chosen': '0.16017', 'rewards_train/rejected': '0.057625', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10254', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-127.55', 'loss/train': '0.65162', 'examples_per_second': '29.882', 'grad_norm': '69.5', 'counters/examples': 209632, 'counters/updates': 6551}
skipping logging after 209664 examples to avoid logging too frequently
train stats after 209696 examples: {'rewards_train/chosen': '0.13027', 'rewards_train/rejected': '0.055168', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075097', 'logps_train/rejected': '-147.49', 'logps_train/chosen': '-172.19', 'loss/train': '0.67445', 'examples_per_second': '30.098', 'grad_norm': '74', 'counters/examples': 209696, 'counters/updates': 6553}
train stats after 209728 examples: {'rewards_train/chosen': '0.10858', 'rewards_train/rejected': '-0.070664', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17924', 'logps_train/rejected': '-153.28', 'logps_train/chosen': '-125.01', 'loss/train': '0.61821', 'examples_per_second': '31.626', 'grad_norm': '60.25', 'counters/examples': 209728, 'counters/updates': 6554}
skipping logging after 209760 examples to avoid logging too frequently
train stats after 209792 examples: {'rewards_train/chosen': '0.10345', 'rewards_train/rejected': '0.048496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054957', 'logps_train/rejected': '-148.71', 'logps_train/chosen': '-148.5', 'loss/train': '0.67924', 'examples_per_second': '31.662', 'grad_norm': '51.5', 'counters/examples': 209792, 'counters/updates': 6556}
train stats after 209824 examples: {'rewards_train/chosen': '0.129', 'rewards_train/rejected': '0.072637', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056367', 'logps_train/rejected': '-117.4', 'logps_train/chosen': '-152.2', 'loss/train': '0.67996', 'examples_per_second': '30.844', 'grad_norm': '68.5', 'counters/examples': 209824, 'counters/updates': 6557}
train stats after 209856 examples: {'rewards_train/chosen': '0.10946', 'rewards_train/rejected': '0.08057', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028895', 'logps_train/rejected': '-134.38', 'logps_train/chosen': '-149.58', 'loss/train': '0.69361', 'examples_per_second': '31.788', 'grad_norm': '80', 'counters/examples': 209856, 'counters/updates': 6558}
train stats after 209888 examples: {'rewards_train/chosen': '0.14296', 'rewards_train/rejected': '-0.02903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17199', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-186.98', 'loss/train': '0.62068', 'examples_per_second': '31.601', 'grad_norm': '103', 'counters/examples': 209888, 'counters/updates': 6559}
train stats after 209920 examples: {'rewards_train/chosen': '0.077148', 'rewards_train/rejected': '0.022483', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054665', 'logps_train/rejected': '-162.21', 'logps_train/chosen': '-129.18', 'loss/train': '0.6878', 'examples_per_second': '32.413', 'grad_norm': '85', 'counters/examples': 209920, 'counters/updates': 6560}
train stats after 209952 examples: {'rewards_train/chosen': '0.125', 'rewards_train/rejected': '0.12418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00082312', 'logps_train/rejected': '-156.44', 'logps_train/chosen': '-141.92', 'loss/train': '0.70672', 'examples_per_second': '32.328', 'grad_norm': '165', 'counters/examples': 209952, 'counters/updates': 6561}
train stats after 209984 examples: {'rewards_train/chosen': '0.16296', 'rewards_train/rejected': '0.082891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080066', 'logps_train/rejected': '-164.66', 'logps_train/chosen': '-160.37', 'loss/train': '0.66417', 'examples_per_second': '32.353', 'grad_norm': '66.5', 'counters/examples': 209984, 'counters/updates': 6562}
skipping logging after 210016 examples to avoid logging too frequently
train stats after 210048 examples: {'rewards_train/chosen': '0.18083', 'rewards_train/rejected': '0.016769', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16406', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-189.9', 'loss/train': '0.63251', 'examples_per_second': '31.641', 'grad_norm': '96', 'counters/examples': 210048, 'counters/updates': 6564}
train stats after 210080 examples: {'rewards_train/chosen': '0.018432', 'rewards_train/rejected': '-0.054507', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072939', 'logps_train/rejected': '-122.18', 'logps_train/chosen': '-127', 'loss/train': '0.6655', 'examples_per_second': '32.394', 'grad_norm': '76.5', 'counters/examples': 210080, 'counters/updates': 6565}
train stats after 210112 examples: {'rewards_train/chosen': '0.11963', 'rewards_train/rejected': '0.13718', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.017544', 'logps_train/rejected': '-164.23', 'logps_train/chosen': '-183.1', 'loss/train': '0.72503', 'examples_per_second': '31.631', 'grad_norm': '135', 'counters/examples': 210112, 'counters/updates': 6566}
train stats after 210144 examples: {'rewards_train/chosen': '0.13557', 'rewards_train/rejected': '0.12287', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.012697', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-139.9', 'loss/train': '0.70088', 'examples_per_second': '31.97', 'grad_norm': '90', 'counters/examples': 210144, 'counters/updates': 6567}
train stats after 210176 examples: {'rewards_train/chosen': '0.1493', 'rewards_train/rejected': '-0.040793', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.19009', 'logps_train/rejected': '-153.92', 'logps_train/chosen': '-163.94', 'loss/train': '0.6294', 'examples_per_second': '30.671', 'grad_norm': '90', 'counters/examples': 210176, 'counters/updates': 6568}
train stats after 210208 examples: {'rewards_train/chosen': '0.12083', 'rewards_train/rejected': '0.084739', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036093', 'logps_train/rejected': '-152.71', 'logps_train/chosen': '-190.37', 'loss/train': '0.688', 'examples_per_second': '30.123', 'grad_norm': '125.5', 'counters/examples': 210208, 'counters/updates': 6569}
train stats after 210240 examples: {'rewards_train/chosen': '0.1849', 'rewards_train/rejected': '0.10193', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082972', 'logps_train/rejected': '-107.66', 'logps_train/chosen': '-154.62', 'loss/train': '0.68288', 'examples_per_second': '30.588', 'grad_norm': '69.5', 'counters/examples': 210240, 'counters/updates': 6570}
train stats after 210272 examples: {'rewards_train/chosen': '0.085287', 'rewards_train/rejected': '-0.024175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-108.63', 'logps_train/chosen': '-130.11', 'loss/train': '0.65284', 'examples_per_second': '32.192', 'grad_norm': '123', 'counters/examples': 210272, 'counters/updates': 6571}
train stats after 210304 examples: {'rewards_train/chosen': '0.0045597', 'rewards_train/rejected': '0.050924', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.046364', 'logps_train/rejected': '-97.35', 'logps_train/chosen': '-113.82', 'loss/train': '0.72628', 'examples_per_second': '30.212', 'grad_norm': '84.5', 'counters/examples': 210304, 'counters/updates': 6572}
train stats after 210336 examples: {'rewards_train/chosen': '0.093972', 'rewards_train/rejected': '-0.0082366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10221', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-134.28', 'loss/train': '0.65526', 'examples_per_second': '32.404', 'grad_norm': '85.5', 'counters/examples': 210336, 'counters/updates': 6573}
train stats after 210368 examples: {'rewards_train/chosen': '0.1253', 'rewards_train/rejected': '0.037728', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087568', 'logps_train/rejected': '-159.84', 'logps_train/chosen': '-191.43', 'loss/train': '0.66567', 'examples_per_second': '30.099', 'grad_norm': '91', 'counters/examples': 210368, 'counters/updates': 6574}
train stats after 210400 examples: {'rewards_train/chosen': '0.083598', 'rewards_train/rejected': '0.017694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065904', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-126.91', 'loss/train': '0.68542', 'examples_per_second': '31.592', 'grad_norm': '63.25', 'counters/examples': 210400, 'counters/updates': 6575}
train stats after 210432 examples: {'rewards_train/chosen': '-0.00053357', 'rewards_train/rejected': '0.034374', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.034907', 'logps_train/rejected': '-115.95', 'logps_train/chosen': '-139.49', 'loss/train': '0.72203', 'examples_per_second': '31.029', 'grad_norm': '62.25', 'counters/examples': 210432, 'counters/updates': 6576}
skipping logging after 210464 examples to avoid logging too frequently
train stats after 210496 examples: {'rewards_train/chosen': '0.091918', 'rewards_train/rejected': '-0.0033464', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095265', 'logps_train/rejected': '-104.74', 'logps_train/chosen': '-174.81', 'loss/train': '0.66196', 'examples_per_second': '35.669', 'grad_norm': '149', 'counters/examples': 210496, 'counters/updates': 6578}
train stats after 210528 examples: {'rewards_train/chosen': '0.083918', 'rewards_train/rejected': '0.070636', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013282', 'logps_train/rejected': '-149.36', 'logps_train/chosen': '-147.78', 'loss/train': '0.70105', 'examples_per_second': '31.618', 'grad_norm': '105', 'counters/examples': 210528, 'counters/updates': 6579}
train stats after 210560 examples: {'rewards_train/chosen': '0.20314', 'rewards_train/rejected': '0.10584', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097293', 'logps_train/rejected': '-174.38', 'logps_train/chosen': '-147.67', 'loss/train': '0.6674', 'examples_per_second': '31.797', 'grad_norm': '203', 'counters/examples': 210560, 'counters/updates': 6580}
train stats after 210592 examples: {'rewards_train/chosen': '0.22726', 'rewards_train/rejected': '0.0917', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13556', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-156.59', 'loss/train': '0.64715', 'examples_per_second': '30.727', 'grad_norm': '85', 'counters/examples': 210592, 'counters/updates': 6581}
train stats after 210624 examples: {'rewards_train/chosen': '0.022995', 'rewards_train/rejected': '0.021475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0015194', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-146.48', 'loss/train': '0.70735', 'examples_per_second': '31.628', 'grad_norm': '91', 'counters/examples': 210624, 'counters/updates': 6582}
train stats after 210656 examples: {'rewards_train/chosen': '0.085831', 'rewards_train/rejected': '0.0064168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079414', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-131.95', 'loss/train': '0.66019', 'examples_per_second': '32.391', 'grad_norm': '74', 'counters/examples': 210656, 'counters/updates': 6583}
train stats after 210688 examples: {'rewards_train/chosen': '0.18452', 'rewards_train/rejected': '0.043869', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14065', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-158.43', 'loss/train': '0.6458', 'examples_per_second': '31.106', 'grad_norm': '80.5', 'counters/examples': 210688, 'counters/updates': 6584}
train stats after 210720 examples: {'rewards_train/chosen': '0.099833', 'rewards_train/rejected': '-0.0020771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10191', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-153.8', 'loss/train': '0.65804', 'examples_per_second': '31.564', 'grad_norm': '135', 'counters/examples': 210720, 'counters/updates': 6585}
train stats after 210752 examples: {'rewards_train/chosen': '0.19772', 'rewards_train/rejected': '0.031163', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16655', 'logps_train/rejected': '-122.52', 'logps_train/chosen': '-165.48', 'loss/train': '0.62888', 'examples_per_second': '31.979', 'grad_norm': '51.5', 'counters/examples': 210752, 'counters/updates': 6586}
skipping logging after 210784 examples to avoid logging too frequently
train stats after 210816 examples: {'rewards_train/chosen': '0.12152', 'rewards_train/rejected': '0.13204', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01052', 'logps_train/rejected': '-168.59', 'logps_train/chosen': '-127.86', 'loss/train': '0.71934', 'examples_per_second': '31.624', 'grad_norm': '108', 'counters/examples': 210816, 'counters/updates': 6588}
skipping logging after 210848 examples to avoid logging too frequently
train stats after 210880 examples: {'rewards_train/chosen': '0.033811', 'rewards_train/rejected': '-0.0089444', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042756', 'logps_train/rejected': '-170', 'logps_train/chosen': '-155.32', 'loss/train': '0.69217', 'examples_per_second': '32.91', 'grad_norm': '76.5', 'counters/examples': 210880, 'counters/updates': 6590}
train stats after 210912 examples: {'rewards_train/chosen': '0.042639', 'rewards_train/rejected': '0.16438', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12174', 'logps_train/rejected': '-156.21', 'logps_train/chosen': '-138.51', 'loss/train': '0.76955', 'examples_per_second': '31.118', 'grad_norm': '122', 'counters/examples': 210912, 'counters/updates': 6591}
train stats after 210944 examples: {'rewards_train/chosen': '0.084627', 'rewards_train/rejected': '0.01951', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065117', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-146.03', 'loss/train': '0.67469', 'examples_per_second': '31.458', 'grad_norm': '113.5', 'counters/examples': 210944, 'counters/updates': 6592}
train stats after 210976 examples: {'rewards_train/chosen': '0.074694', 'rewards_train/rejected': '0.045943', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028751', 'logps_train/rejected': '-168.97', 'logps_train/chosen': '-163.47', 'loss/train': '0.70433', 'examples_per_second': '30.942', 'grad_norm': '81', 'counters/examples': 210976, 'counters/updates': 6593}
train stats after 211008 examples: {'rewards_train/chosen': '0.080691', 'rewards_train/rejected': '0.037866', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042825', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-160.36', 'loss/train': '0.68451', 'examples_per_second': '32.362', 'grad_norm': '117.5', 'counters/examples': 211008, 'counters/updates': 6594}
train stats after 211040 examples: {'rewards_train/chosen': '0.13507', 'rewards_train/rejected': '-0.029671', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16474', 'logps_train/rejected': '-153.09', 'logps_train/chosen': '-162.63', 'loss/train': '0.63116', 'examples_per_second': '31.338', 'grad_norm': '73', 'counters/examples': 211040, 'counters/updates': 6595}
train stats after 211072 examples: {'rewards_train/chosen': '0.037948', 'rewards_train/rejected': '0.054407', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016459', 'logps_train/rejected': '-159.99', 'logps_train/chosen': '-144.16', 'loss/train': '0.71169', 'examples_per_second': '30.64', 'grad_norm': '109.5', 'counters/examples': 211072, 'counters/updates': 6596}
train stats after 211104 examples: {'rewards_train/chosen': '0.19458', 'rewards_train/rejected': '0.1076', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086975', 'logps_train/rejected': '-168.11', 'logps_train/chosen': '-142.45', 'loss/train': '0.67181', 'examples_per_second': '31.59', 'grad_norm': '122', 'counters/examples': 211104, 'counters/updates': 6597}
train stats after 211136 examples: {'rewards_train/chosen': '0.0010045', 'rewards_train/rejected': '0.012617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011613', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-141.65', 'loss/train': '0.71099', 'examples_per_second': '30.917', 'grad_norm': '149', 'counters/examples': 211136, 'counters/updates': 6598}
train stats after 211168 examples: {'rewards_train/chosen': '0.15714', 'rewards_train/rejected': '0.054425', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10271', 'logps_train/rejected': '-108.68', 'logps_train/chosen': '-154.03', 'loss/train': '0.66408', 'examples_per_second': '30.462', 'grad_norm': '71.5', 'counters/examples': 211168, 'counters/updates': 6599}
skipping logging after 211200 examples to avoid logging too frequently
train stats after 211232 examples: {'rewards_train/chosen': '0.096314', 'rewards_train/rejected': '0.13877', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042459', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-138.31', 'loss/train': '0.72776', 'examples_per_second': '31.572', 'grad_norm': '103.5', 'counters/examples': 211232, 'counters/updates': 6601}
skipping logging after 211264 examples to avoid logging too frequently
train stats after 211296 examples: {'rewards_train/chosen': '0.13953', 'rewards_train/rejected': '0.10084', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03869', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-115.5', 'loss/train': '0.69196', 'examples_per_second': '31.589', 'grad_norm': '119.5', 'counters/examples': 211296, 'counters/updates': 6603}
skipping logging after 211328 examples to avoid logging too frequently
train stats after 211360 examples: {'rewards_train/chosen': '0.12164', 'rewards_train/rejected': '0.11571', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0059266', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-170.39', 'loss/train': '0.70388', 'examples_per_second': '31.926', 'grad_norm': '104', 'counters/examples': 211360, 'counters/updates': 6605}
train stats after 211392 examples: {'rewards_train/chosen': '0.20717', 'rewards_train/rejected': '0.048094', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15908', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-167.48', 'loss/train': '0.63146', 'examples_per_second': '31.577', 'grad_norm': '89', 'counters/examples': 211392, 'counters/updates': 6606}
train stats after 211424 examples: {'rewards_train/chosen': '0.24265', 'rewards_train/rejected': '0.082733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15992', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-163.25', 'loss/train': '0.63808', 'examples_per_second': '30.166', 'grad_norm': '65', 'counters/examples': 211424, 'counters/updates': 6607}
train stats after 211456 examples: {'rewards_train/chosen': '0.073364', 'rewards_train/rejected': '0.083439', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010075', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-164.61', 'loss/train': '0.71079', 'examples_per_second': '31.786', 'grad_norm': '87.5', 'counters/examples': 211456, 'counters/updates': 6608}
train stats after 211488 examples: {'rewards_train/chosen': '0.084675', 'rewards_train/rejected': '0.013937', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070738', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-147.45', 'loss/train': '0.66872', 'examples_per_second': '31.695', 'grad_norm': '77', 'counters/examples': 211488, 'counters/updates': 6609}
train stats after 211520 examples: {'rewards_train/chosen': '0.2074', 'rewards_train/rejected': '0.12179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085603', 'logps_train/rejected': '-160.9', 'logps_train/chosen': '-196.96', 'loss/train': '0.66691', 'examples_per_second': '30.398', 'grad_norm': '101', 'counters/examples': 211520, 'counters/updates': 6610}
train stats after 211552 examples: {'rewards_train/chosen': '0.12019', 'rewards_train/rejected': '0.075278', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044915', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-168.59', 'loss/train': '0.67779', 'examples_per_second': '30.039', 'grad_norm': '82.5', 'counters/examples': 211552, 'counters/updates': 6611}
train stats after 211584 examples: {'rewards_train/chosen': '0.16906', 'rewards_train/rejected': '0.047819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12124', 'logps_train/rejected': '-132.72', 'logps_train/chosen': '-164.98', 'loss/train': '0.68266', 'examples_per_second': '30.52', 'grad_norm': '84', 'counters/examples': 211584, 'counters/updates': 6612}
train stats after 211616 examples: {'rewards_train/chosen': '0.12244', 'rewards_train/rejected': '0.088781', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033664', 'logps_train/rejected': '-147.67', 'logps_train/chosen': '-149.91', 'loss/train': '0.69604', 'examples_per_second': '31.463', 'grad_norm': '68.5', 'counters/examples': 211616, 'counters/updates': 6613}
train stats after 211648 examples: {'rewards_train/chosen': '0.10048', 'rewards_train/rejected': '0.016157', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08432', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-161.57', 'loss/train': '0.67629', 'examples_per_second': '31.432', 'grad_norm': '76', 'counters/examples': 211648, 'counters/updates': 6614}
train stats after 211680 examples: {'rewards_train/chosen': '0.079123', 'rewards_train/rejected': '0.032408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046715', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-152.67', 'loss/train': '0.68279', 'examples_per_second': '30.657', 'grad_norm': '93.5', 'counters/examples': 211680, 'counters/updates': 6615}
train stats after 211712 examples: {'rewards_train/chosen': '0.11747', 'rewards_train/rejected': '0.041521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075948', 'logps_train/rejected': '-141.61', 'logps_train/chosen': '-212.62', 'loss/train': '0.67758', 'examples_per_second': '30.136', 'grad_norm': '65', 'counters/examples': 211712, 'counters/updates': 6616}
train stats after 211744 examples: {'rewards_train/chosen': '0.10624', 'rewards_train/rejected': '0.059663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046576', 'logps_train/rejected': '-161.18', 'logps_train/chosen': '-134.08', 'loss/train': '0.69152', 'examples_per_second': '31.008', 'grad_norm': '95', 'counters/examples': 211744, 'counters/updates': 6617}
train stats after 211776 examples: {'rewards_train/chosen': '0.21118', 'rewards_train/rejected': '0.1589', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052279', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-160.58', 'loss/train': '0.68533', 'examples_per_second': '31.347', 'grad_norm': '59.25', 'counters/examples': 211776, 'counters/updates': 6618}
train stats after 211808 examples: {'rewards_train/chosen': '0.17331', 'rewards_train/rejected': '0.018256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15506', 'logps_train/rejected': '-99.206', 'logps_train/chosen': '-137.6', 'loss/train': '0.62986', 'examples_per_second': '31.545', 'grad_norm': '57.25', 'counters/examples': 211808, 'counters/updates': 6619}
skipping logging after 211840 examples to avoid logging too frequently
train stats after 211872 examples: {'rewards_train/chosen': '0.13424', 'rewards_train/rejected': '0.16115', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026916', 'logps_train/rejected': '-155.43', 'logps_train/chosen': '-148.56', 'loss/train': '0.71897', 'examples_per_second': '33.888', 'grad_norm': '73.5', 'counters/examples': 211872, 'counters/updates': 6621}
train stats after 211904 examples: {'rewards_train/chosen': '0.057256', 'rewards_train/rejected': '0.059744', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0024878', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-110.62', 'loss/train': '0.70239', 'examples_per_second': '31.437', 'grad_norm': '132', 'counters/examples': 211904, 'counters/updates': 6622}
train stats after 211936 examples: {'rewards_train/chosen': '0.16322', 'rewards_train/rejected': '-0.011857', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17508', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-156.41', 'loss/train': '0.62328', 'examples_per_second': '30.143', 'grad_norm': '79.5', 'counters/examples': 211936, 'counters/updates': 6623}
train stats after 211968 examples: {'rewards_train/chosen': '0.042381', 'rewards_train/rejected': '0.048039', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0056585', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-144.16', 'loss/train': '0.70667', 'examples_per_second': '32.844', 'grad_norm': '66', 'counters/examples': 211968, 'counters/updates': 6624}
train stats after 212000 examples: {'rewards_train/chosen': '0.12613', 'rewards_train/rejected': '0.037985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088144', 'logps_train/rejected': '-142.21', 'logps_train/chosen': '-187.14', 'loss/train': '0.66681', 'examples_per_second': '30.258', 'grad_norm': '96', 'counters/examples': 212000, 'counters/updates': 6625}
train stats after 212032 examples: {'rewards_train/chosen': '0.11269', 'rewards_train/rejected': '0.062423', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050271', 'logps_train/rejected': '-129.76', 'logps_train/chosen': '-179.26', 'loss/train': '0.68558', 'examples_per_second': '31.581', 'grad_norm': '92', 'counters/examples': 212032, 'counters/updates': 6626}
train stats after 212064 examples: {'rewards_train/chosen': '0.074936', 'rewards_train/rejected': '-0.021306', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096241', 'logps_train/rejected': '-94.121', 'logps_train/chosen': '-128.81', 'loss/train': '0.65486', 'examples_per_second': '27.621', 'grad_norm': '75', 'counters/examples': 212064, 'counters/updates': 6627}
skipping logging after 212096 examples to avoid logging too frequently
train stats after 212128 examples: {'rewards_train/chosen': '0.10509', 'rewards_train/rejected': '-0.022221', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12731', 'logps_train/rejected': '-95.286', 'logps_train/chosen': '-127.44', 'loss/train': '0.64034', 'examples_per_second': '36.129', 'grad_norm': '55.25', 'counters/examples': 212128, 'counters/updates': 6629}
train stats after 212160 examples: {'rewards_train/chosen': '0.11156', 'rewards_train/rejected': '0.034818', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076737', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-191.48', 'loss/train': '0.67445', 'examples_per_second': '31.502', 'grad_norm': '137', 'counters/examples': 212160, 'counters/updates': 6630}
train stats after 212192 examples: {'rewards_train/chosen': '0.20849', 'rewards_train/rejected': '0.049447', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15904', 'logps_train/rejected': '-159.76', 'logps_train/chosen': '-194.37', 'loss/train': '0.63491', 'examples_per_second': '31.548', 'grad_norm': '84', 'counters/examples': 212192, 'counters/updates': 6631}
train stats after 212224 examples: {'rewards_train/chosen': '0.14297', 'rewards_train/rejected': '0.018515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12445', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-143.7', 'loss/train': '0.65347', 'examples_per_second': '30.249', 'grad_norm': '60', 'counters/examples': 212224, 'counters/updates': 6632}
train stats after 212256 examples: {'rewards_train/chosen': '0.10061', 'rewards_train/rejected': '-0.031133', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13174', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-150', 'loss/train': '0.64021', 'examples_per_second': '30.965', 'grad_norm': '57', 'counters/examples': 212256, 'counters/updates': 6633}
train stats after 212288 examples: {'rewards_train/chosen': '0.054405', 'rewards_train/rejected': '0.079156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.024751', 'logps_train/rejected': '-147', 'logps_train/chosen': '-193.39', 'loss/train': '0.72364', 'examples_per_second': '32.263', 'grad_norm': '118.5', 'counters/examples': 212288, 'counters/updates': 6634}
train stats after 212320 examples: {'rewards_train/chosen': '0.18958', 'rewards_train/rejected': '0.10417', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085409', 'logps_train/rejected': '-139.75', 'logps_train/chosen': '-123.23', 'loss/train': '0.66433', 'examples_per_second': '30.936', 'grad_norm': '71.5', 'counters/examples': 212320, 'counters/updates': 6635}
train stats after 212352 examples: {'rewards_train/chosen': '0.080687', 'rewards_train/rejected': '0.076477', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0042099', 'logps_train/rejected': '-111.97', 'logps_train/chosen': '-134.49', 'loss/train': '0.70175', 'examples_per_second': '31.612', 'grad_norm': '52.5', 'counters/examples': 212352, 'counters/updates': 6636}
train stats after 212384 examples: {'rewards_train/chosen': '0.044822', 'rewards_train/rejected': '-0.00012953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044952', 'logps_train/rejected': '-129.39', 'logps_train/chosen': '-138.84', 'loss/train': '0.68348', 'examples_per_second': '31.104', 'grad_norm': '55.25', 'counters/examples': 212384, 'counters/updates': 6637}
train stats after 212416 examples: {'rewards_train/chosen': '0.061355', 'rewards_train/rejected': '-0.061946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1233', 'logps_train/rejected': '-128.87', 'logps_train/chosen': '-163.27', 'loss/train': '0.64021', 'examples_per_second': '30.766', 'grad_norm': '85', 'counters/examples': 212416, 'counters/updates': 6638}
train stats after 212448 examples: {'rewards_train/chosen': '0.11993', 'rewards_train/rejected': '0.067318', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052614', 'logps_train/rejected': '-146.71', 'logps_train/chosen': '-168.36', 'loss/train': '0.67853', 'examples_per_second': '31.514', 'grad_norm': '85', 'counters/examples': 212448, 'counters/updates': 6639}
train stats after 212480 examples: {'rewards_train/chosen': '0.16136', 'rewards_train/rejected': '0.027594', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13376', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-135.92', 'loss/train': '0.64351', 'examples_per_second': '31.955', 'grad_norm': '70', 'counters/examples': 212480, 'counters/updates': 6640}
train stats after 212512 examples: {'rewards_train/chosen': '0.12745', 'rewards_train/rejected': '-0.0075431', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.135', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-159.91', 'loss/train': '0.64276', 'examples_per_second': '31.376', 'grad_norm': '106', 'counters/examples': 212512, 'counters/updates': 6641}
train stats after 212544 examples: {'rewards_train/chosen': '0.10841', 'rewards_train/rejected': '-0.020016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12843', 'logps_train/rejected': '-120.4', 'logps_train/chosen': '-141.95', 'loss/train': '0.64421', 'examples_per_second': '29.785', 'grad_norm': '64.5', 'counters/examples': 212544, 'counters/updates': 6642}
train stats after 212576 examples: {'rewards_train/chosen': '0.17852', 'rewards_train/rejected': '0.15472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023804', 'logps_train/rejected': '-141.22', 'logps_train/chosen': '-135.23', 'loss/train': '0.6991', 'examples_per_second': '30.482', 'grad_norm': '81.5', 'counters/examples': 212576, 'counters/updates': 6643}
train stats after 212608 examples: {'rewards_train/chosen': '0.030806', 'rewards_train/rejected': '0.078225', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047419', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-129.05', 'loss/train': '0.73894', 'examples_per_second': '31.225', 'grad_norm': '83', 'counters/examples': 212608, 'counters/updates': 6644}
train stats after 212640 examples: {'rewards_train/chosen': '0.13939', 'rewards_train/rejected': '0.062871', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076514', 'logps_train/rejected': '-151.85', 'logps_train/chosen': '-138.95', 'loss/train': '0.66543', 'examples_per_second': '30.796', 'grad_norm': '89.5', 'counters/examples': 212640, 'counters/updates': 6645}
train stats after 212672 examples: {'rewards_train/chosen': '0.07674', 'rewards_train/rejected': '0.14729', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.070548', 'logps_train/rejected': '-134.55', 'logps_train/chosen': '-161.07', 'loss/train': '0.74897', 'examples_per_second': '32.661', 'grad_norm': '71', 'counters/examples': 212672, 'counters/updates': 6646}
train stats after 212704 examples: {'rewards_train/chosen': '0.051572', 'rewards_train/rejected': '0.045753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0058185', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-175.49', 'loss/train': '0.69737', 'examples_per_second': '32.611', 'grad_norm': '132', 'counters/examples': 212704, 'counters/updates': 6647}
train stats after 212736 examples: {'rewards_train/chosen': '0.17556', 'rewards_train/rejected': '-0.0049498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18051', 'logps_train/rejected': '-137.28', 'logps_train/chosen': '-159.69', 'loss/train': '0.6348', 'examples_per_second': '32.047', 'grad_norm': '73.5', 'counters/examples': 212736, 'counters/updates': 6648}
skipping logging after 212768 examples to avoid logging too frequently
train stats after 212800 examples: {'rewards_train/chosen': '0.10757', 'rewards_train/rejected': '0.0625', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045068', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-172.84', 'loss/train': '0.69059', 'examples_per_second': '31.3', 'grad_norm': '135', 'counters/examples': 212800, 'counters/updates': 6650}
skipping logging after 212832 examples to avoid logging too frequently
train stats after 212864 examples: {'rewards_train/chosen': '0.14678', 'rewards_train/rejected': '0.02107', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12571', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-145.24', 'loss/train': '0.64373', 'examples_per_second': '30.827', 'grad_norm': '204', 'counters/examples': 212864, 'counters/updates': 6652}
skipping logging after 212896 examples to avoid logging too frequently
train stats after 212928 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '0.032816', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097621', 'logps_train/rejected': '-131.31', 'logps_train/chosen': '-137.22', 'loss/train': '0.6583', 'examples_per_second': '31.227', 'grad_norm': '70', 'counters/examples': 212928, 'counters/updates': 6654}
train stats after 212960 examples: {'rewards_train/chosen': '0.11012', 'rewards_train/rejected': '0.066338', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043779', 'logps_train/rejected': '-130.98', 'logps_train/chosen': '-144.04', 'loss/train': '0.69381', 'examples_per_second': '30.945', 'grad_norm': '65.5', 'counters/examples': 212960, 'counters/updates': 6655}
train stats after 212992 examples: {'rewards_train/chosen': '0.1084', 'rewards_train/rejected': '-0.074944', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18334', 'logps_train/rejected': '-82.336', 'logps_train/chosen': '-168.73', 'loss/train': '0.62209', 'examples_per_second': '31.273', 'grad_norm': '118.5', 'counters/examples': 212992, 'counters/updates': 6656}
skipping logging after 213024 examples to avoid logging too frequently
train stats after 213056 examples: {'rewards_train/chosen': '-0.026602', 'rewards_train/rejected': '-0.0012967', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025305', 'logps_train/rejected': '-154.98', 'logps_train/chosen': '-132.19', 'loss/train': '0.72041', 'examples_per_second': '31.542', 'grad_norm': '103.5', 'counters/examples': 213056, 'counters/updates': 6658}
train stats after 213088 examples: {'rewards_train/chosen': '0.12144', 'rewards_train/rejected': '0.043811', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077628', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-148.22', 'loss/train': '0.66501', 'examples_per_second': '31.406', 'grad_norm': '107.5', 'counters/examples': 213088, 'counters/updates': 6659}
train stats after 213120 examples: {'rewards_train/chosen': '0.079381', 'rewards_train/rejected': '0.016375', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063006', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-153.42', 'loss/train': '0.67083', 'examples_per_second': '30.208', 'grad_norm': '78.5', 'counters/examples': 213120, 'counters/updates': 6660}
train stats after 213152 examples: {'rewards_train/chosen': '0.13673', 'rewards_train/rejected': '0.04403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0927', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-146.7', 'loss/train': '0.66536', 'examples_per_second': '31.207', 'grad_norm': '62.25', 'counters/examples': 213152, 'counters/updates': 6661}
train stats after 213184 examples: {'rewards_train/chosen': '0.073266', 'rewards_train/rejected': '0.048474', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024793', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-157.72', 'loss/train': '0.69358', 'examples_per_second': '30.405', 'grad_norm': '137', 'counters/examples': 213184, 'counters/updates': 6662}
train stats after 213216 examples: {'rewards_train/chosen': '0.093051', 'rewards_train/rejected': '0.16274', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06969', 'logps_train/rejected': '-134.84', 'logps_train/chosen': '-165.22', 'loss/train': '0.76421', 'examples_per_second': '32.402', 'grad_norm': '123.5', 'counters/examples': 213216, 'counters/updates': 6663}
train stats after 213248 examples: {'rewards_train/chosen': '0.18666', 'rewards_train/rejected': '0.097806', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088858', 'logps_train/rejected': '-143.75', 'logps_train/chosen': '-161.32', 'loss/train': '0.68022', 'examples_per_second': '31.647', 'grad_norm': '111.5', 'counters/examples': 213248, 'counters/updates': 6664}
train stats after 213280 examples: {'rewards_train/chosen': '0.12344', 'rewards_train/rejected': '-0.0017063', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12515', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-134.9', 'loss/train': '0.65275', 'examples_per_second': '25.552', 'grad_norm': '64.5', 'counters/examples': 213280, 'counters/updates': 6665}
train stats after 213312 examples: {'rewards_train/chosen': '0.078969', 'rewards_train/rejected': '0.07477', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0041991', 'logps_train/rejected': '-120.41', 'logps_train/chosen': '-145.53', 'loss/train': '0.70325', 'examples_per_second': '30.605', 'grad_norm': '86', 'counters/examples': 213312, 'counters/updates': 6666}
train stats after 213344 examples: {'rewards_train/chosen': '0.13311', 'rewards_train/rejected': '0.025435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10768', 'logps_train/rejected': '-142.45', 'logps_train/chosen': '-136.29', 'loss/train': '0.65238', 'examples_per_second': '31.009', 'grad_norm': '51', 'counters/examples': 213344, 'counters/updates': 6667}
train stats after 213376 examples: {'rewards_train/chosen': '0.13965', 'rewards_train/rejected': '0.034565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10509', 'logps_train/rejected': '-130.75', 'logps_train/chosen': '-177.54', 'loss/train': '0.65326', 'examples_per_second': '24.687', 'grad_norm': '114', 'counters/examples': 213376, 'counters/updates': 6668}
skipping logging after 213408 examples to avoid logging too frequently
train stats after 213440 examples: {'rewards_train/chosen': '0.19646', 'rewards_train/rejected': '0.12972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066741', 'logps_train/rejected': '-151.34', 'logps_train/chosen': '-160.55', 'loss/train': '0.67672', 'examples_per_second': '30.655', 'grad_norm': '75', 'counters/examples': 213440, 'counters/updates': 6670}
skipping logging after 213472 examples to avoid logging too frequently
train stats after 213504 examples: {'rewards_train/chosen': '0.14967', 'rewards_train/rejected': '0.07177', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077903', 'logps_train/rejected': '-121.56', 'logps_train/chosen': '-158.78', 'loss/train': '0.67124', 'examples_per_second': '34.225', 'grad_norm': '77', 'counters/examples': 213504, 'counters/updates': 6672}
skipping logging after 213536 examples to avoid logging too frequently
train stats after 213568 examples: {'rewards_train/chosen': '0.11051', 'rewards_train/rejected': '0.052615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057892', 'logps_train/rejected': '-138.05', 'logps_train/chosen': '-139.56', 'loss/train': '0.68538', 'examples_per_second': '33.961', 'grad_norm': '82.5', 'counters/examples': 213568, 'counters/updates': 6674}
skipping logging after 213600 examples to avoid logging too frequently
train stats after 213632 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.12603', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026029', 'logps_train/rejected': '-147.3', 'logps_train/chosen': '-166.39', 'loss/train': '0.71982', 'examples_per_second': '30.964', 'grad_norm': '80', 'counters/examples': 213632, 'counters/updates': 6676}
train stats after 213664 examples: {'rewards_train/chosen': '0.10749', 'rewards_train/rejected': '0.053719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053774', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-196.15', 'loss/train': '0.67559', 'examples_per_second': '33.099', 'grad_norm': '102.5', 'counters/examples': 213664, 'counters/updates': 6677}
train stats after 213696 examples: {'rewards_train/chosen': '0.14359', 'rewards_train/rejected': '0.058719', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084869', 'logps_train/rejected': '-166.69', 'logps_train/chosen': '-158.8', 'loss/train': '0.66925', 'examples_per_second': '30.253', 'grad_norm': '208', 'counters/examples': 213696, 'counters/updates': 6678}
train stats after 213728 examples: {'rewards_train/chosen': '0.17957', 'rewards_train/rejected': '0.021809', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15776', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-152.55', 'loss/train': '0.63394', 'examples_per_second': '31.65', 'grad_norm': '63.25', 'counters/examples': 213728, 'counters/updates': 6679}
train stats after 213760 examples: {'rewards_train/chosen': '0.074889', 'rewards_train/rejected': '0.032884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042006', 'logps_train/rejected': '-166.99', 'logps_train/chosen': '-189.66', 'loss/train': '0.68699', 'examples_per_second': '30.172', 'grad_norm': '124', 'counters/examples': 213760, 'counters/updates': 6680}
train stats after 213792 examples: {'rewards_train/chosen': '0.02555', 'rewards_train/rejected': '0.014246', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011304', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-131.25', 'loss/train': '0.70021', 'examples_per_second': '30.141', 'grad_norm': '87', 'counters/examples': 213792, 'counters/updates': 6681}
train stats after 213824 examples: {'rewards_train/chosen': '0.13374', 'rewards_train/rejected': '0.013236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1205', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-183.77', 'loss/train': '0.64998', 'examples_per_second': '31.041', 'grad_norm': '81.5', 'counters/examples': 213824, 'counters/updates': 6682}
train stats after 213856 examples: {'rewards_train/chosen': '0.16186', 'rewards_train/rejected': '-0.054863', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21673', 'logps_train/rejected': '-115.74', 'logps_train/chosen': '-133.61', 'loss/train': '0.60327', 'examples_per_second': '32.548', 'grad_norm': '77', 'counters/examples': 213856, 'counters/updates': 6683}
train stats after 213888 examples: {'rewards_train/chosen': '0.042871', 'rewards_train/rejected': '-0.0032152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046086', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-159.85', 'loss/train': '0.68052', 'examples_per_second': '31.554', 'grad_norm': '95', 'counters/examples': 213888, 'counters/updates': 6684}
train stats after 213920 examples: {'rewards_train/chosen': '0.25243', 'rewards_train/rejected': '0.084271', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16815', 'logps_train/rejected': '-119.43', 'logps_train/chosen': '-178.93', 'loss/train': '0.63701', 'examples_per_second': '31.831', 'grad_norm': '73', 'counters/examples': 213920, 'counters/updates': 6685}
train stats after 213952 examples: {'rewards_train/chosen': '0.11772', 'rewards_train/rejected': '0.030762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086957', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-147.07', 'loss/train': '0.66415', 'examples_per_second': '31.596', 'grad_norm': '202', 'counters/examples': 213952, 'counters/updates': 6686}
train stats after 213984 examples: {'rewards_train/chosen': '0.0040189', 'rewards_train/rejected': '0.042073', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038054', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-149.23', 'loss/train': '0.72627', 'examples_per_second': '31.671', 'grad_norm': '94.5', 'counters/examples': 213984, 'counters/updates': 6687}
train stats after 214016 examples: {'rewards_train/chosen': '0.15016', 'rewards_train/rejected': '0.030851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11931', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-141.76', 'loss/train': '0.64468', 'examples_per_second': '31.606', 'grad_norm': '118', 'counters/examples': 214016, 'counters/updates': 6688}
train stats after 214048 examples: {'rewards_train/chosen': '0.11639', 'rewards_train/rejected': '0.014191', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1022', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-151.97', 'loss/train': '0.65566', 'examples_per_second': '31.189', 'grad_norm': '82.5', 'counters/examples': 214048, 'counters/updates': 6689}
train stats after 214080 examples: {'rewards_train/chosen': '0.16043', 'rewards_train/rejected': '0.12424', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036188', 'logps_train/rejected': '-160.2', 'logps_train/chosen': '-152.29', 'loss/train': '0.69947', 'examples_per_second': '31.375', 'grad_norm': '82', 'counters/examples': 214080, 'counters/updates': 6690}
train stats after 214112 examples: {'rewards_train/chosen': '0.091087', 'rewards_train/rejected': '0.0010742', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090013', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-134.8', 'loss/train': '0.66683', 'examples_per_second': '32.193', 'grad_norm': '82', 'counters/examples': 214112, 'counters/updates': 6691}
train stats after 214144 examples: {'rewards_train/chosen': '-1.1873e-05', 'rewards_train/rejected': '-0.038869', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.038857', 'logps_train/rejected': '-94.769', 'logps_train/chosen': '-98.498', 'loss/train': '0.68444', 'examples_per_second': '31.683', 'grad_norm': '64', 'counters/examples': 214144, 'counters/updates': 6692}
train stats after 214176 examples: {'rewards_train/chosen': '0.11646', 'rewards_train/rejected': '0.021175', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095286', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-150.7', 'loss/train': '0.66272', 'examples_per_second': '31.105', 'grad_norm': '144', 'counters/examples': 214176, 'counters/updates': 6693}
train stats after 214208 examples: {'rewards_train/chosen': '0.18686', 'rewards_train/rejected': '0.011719', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17514', 'logps_train/rejected': '-93.186', 'logps_train/chosen': '-135.06', 'loss/train': '0.61822', 'examples_per_second': '32.465', 'grad_norm': '80', 'counters/examples': 214208, 'counters/updates': 6694}
train stats after 214240 examples: {'rewards_train/chosen': '0.051201', 'rewards_train/rejected': '0.010277', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040924', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-146.97', 'loss/train': '0.68589', 'examples_per_second': '31.632', 'grad_norm': '67', 'counters/examples': 214240, 'counters/updates': 6695}
train stats after 214272 examples: {'rewards_train/chosen': '0.14356', 'rewards_train/rejected': '0.022092', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12147', 'logps_train/rejected': '-141.49', 'logps_train/chosen': '-189.26', 'loss/train': '0.65231', 'examples_per_second': '30.247', 'grad_norm': '101.5', 'counters/examples': 214272, 'counters/updates': 6696}
train stats after 214304 examples: {'rewards_train/chosen': '0.18749', 'rewards_train/rejected': '0.022886', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1646', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-172.77', 'loss/train': '0.63385', 'examples_per_second': '31.929', 'grad_norm': '81.5', 'counters/examples': 214304, 'counters/updates': 6697}
train stats after 214336 examples: {'rewards_train/chosen': '0.11416', 'rewards_train/rejected': '0.05912', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055036', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-134.94', 'loss/train': '0.68595', 'examples_per_second': '32.592', 'grad_norm': '96.5', 'counters/examples': 214336, 'counters/updates': 6698}
train stats after 214368 examples: {'rewards_train/chosen': '0.034563', 'rewards_train/rejected': '-0.022604', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057167', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-145.97', 'loss/train': '0.67947', 'examples_per_second': '31.181', 'grad_norm': '91.5', 'counters/examples': 214368, 'counters/updates': 6699}
train stats after 214400 examples: {'rewards_train/chosen': '0.10379', 'rewards_train/rejected': '0.034203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069582', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-154.01', 'loss/train': '0.6719', 'examples_per_second': '32.017', 'grad_norm': '101.5', 'counters/examples': 214400, 'counters/updates': 6700}
train stats after 214432 examples: {'rewards_train/chosen': '0.20894', 'rewards_train/rejected': '0.023956', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18499', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-163.51', 'loss/train': '0.62336', 'examples_per_second': '30.644', 'grad_norm': '66', 'counters/examples': 214432, 'counters/updates': 6701}
train stats after 214464 examples: {'rewards_train/chosen': '0.1093', 'rewards_train/rejected': '0.079253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030048', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-154.83', 'loss/train': '0.69586', 'examples_per_second': '31.683', 'grad_norm': '98', 'counters/examples': 214464, 'counters/updates': 6702}
skipping logging after 214496 examples to avoid logging too frequently
train stats after 214528 examples: {'rewards_train/chosen': '0.14824', 'rewards_train/rejected': '0.054174', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094071', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-161.25', 'loss/train': '0.66844', 'examples_per_second': '31.592', 'grad_norm': '92', 'counters/examples': 214528, 'counters/updates': 6704}
train stats after 214560 examples: {'rewards_train/chosen': '0.13382', 'rewards_train/rejected': '0.020175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11364', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-129.88', 'loss/train': '0.64774', 'examples_per_second': '31.799', 'grad_norm': '83', 'counters/examples': 214560, 'counters/updates': 6705}
train stats after 214592 examples: {'rewards_train/chosen': '0.096587', 'rewards_train/rejected': '0.0071538', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089433', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-190.61', 'loss/train': '0.66066', 'examples_per_second': '31.641', 'grad_norm': '78', 'counters/examples': 214592, 'counters/updates': 6706}
skipping logging after 214624 examples to avoid logging too frequently
train stats after 214656 examples: {'rewards_train/chosen': '0.055459', 'rewards_train/rejected': '0.053376', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0020834', 'logps_train/rejected': '-110.24', 'logps_train/chosen': '-121.07', 'loss/train': '0.70133', 'examples_per_second': '36.088', 'grad_norm': '59.25', 'counters/examples': 214656, 'counters/updates': 6708}
train stats after 214688 examples: {'rewards_train/chosen': '0.059603', 'rewards_train/rejected': '-0.024471', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084074', 'logps_train/rejected': '-88.309', 'logps_train/chosen': '-140.82', 'loss/train': '0.66134', 'examples_per_second': '32.673', 'grad_norm': '87', 'counters/examples': 214688, 'counters/updates': 6709}
train stats after 214720 examples: {'rewards_train/chosen': '0.19226', 'rewards_train/rejected': '0.078861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1134', 'logps_train/rejected': '-139.82', 'logps_train/chosen': '-162.63', 'loss/train': '0.65731', 'examples_per_second': '30.182', 'grad_norm': '62', 'counters/examples': 214720, 'counters/updates': 6710}
train stats after 214752 examples: {'rewards_train/chosen': '0.14012', 'rewards_train/rejected': '0.019868', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12025', 'logps_train/rejected': '-143.27', 'logps_train/chosen': '-208.12', 'loss/train': '0.64753', 'examples_per_second': '31.626', 'grad_norm': '135', 'counters/examples': 214752, 'counters/updates': 6711}
skipping logging after 214784 examples to avoid logging too frequently
train stats after 214816 examples: {'rewards_train/chosen': '0.078089', 'rewards_train/rejected': '0.059485', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018604', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-103.56', 'loss/train': '0.70191', 'examples_per_second': '31.447', 'grad_norm': '72.5', 'counters/examples': 214816, 'counters/updates': 6713}
train stats after 214848 examples: {'rewards_train/chosen': '0.048878', 'rewards_train/rejected': '0.03904', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0098385', 'logps_train/rejected': '-127.76', 'logps_train/chosen': '-137.36', 'loss/train': '0.69922', 'examples_per_second': '31.245', 'grad_norm': '98.5', 'counters/examples': 214848, 'counters/updates': 6714}
train stats after 214880 examples: {'rewards_train/chosen': '0.11661', 'rewards_train/rejected': '0.038749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077864', 'logps_train/rejected': '-129.07', 'logps_train/chosen': '-150.14', 'loss/train': '0.66242', 'examples_per_second': '33.138', 'grad_norm': '61.25', 'counters/examples': 214880, 'counters/updates': 6715}
train stats after 214912 examples: {'rewards_train/chosen': '0.076537', 'rewards_train/rejected': '0.1389', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.062361', 'logps_train/rejected': '-140.24', 'logps_train/chosen': '-144.91', 'loss/train': '0.73173', 'examples_per_second': '33.041', 'grad_norm': '76.5', 'counters/examples': 214912, 'counters/updates': 6716}
train stats after 214944 examples: {'rewards_train/chosen': '0.11416', 'rewards_train/rejected': '0.066495', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047668', 'logps_train/rejected': '-116.95', 'logps_train/chosen': '-151.89', 'loss/train': '0.6844', 'examples_per_second': '30.521', 'grad_norm': '81.5', 'counters/examples': 214944, 'counters/updates': 6717}
train stats after 214976 examples: {'rewards_train/chosen': '0.095239', 'rewards_train/rejected': '0.025974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069266', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-147.94', 'loss/train': '0.67392', 'examples_per_second': '31.684', 'grad_norm': '65', 'counters/examples': 214976, 'counters/updates': 6718}
skipping logging after 215008 examples to avoid logging too frequently
train stats after 215040 examples: {'rewards_train/chosen': '0.12264', 'rewards_train/rejected': '0.10386', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018783', 'logps_train/rejected': '-147.76', 'logps_train/chosen': '-160.8', 'loss/train': '0.70712', 'examples_per_second': '30.315', 'grad_norm': '274', 'counters/examples': 215040, 'counters/updates': 6720}
skipping logging after 215072 examples to avoid logging too frequently
train stats after 215104 examples: {'rewards_train/chosen': '0.040688', 'rewards_train/rejected': '0.0752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034513', 'logps_train/rejected': '-113.74', 'logps_train/chosen': '-195.38', 'loss/train': '0.72807', 'examples_per_second': '33.419', 'grad_norm': '61.5', 'counters/examples': 215104, 'counters/updates': 6722}
train stats after 215136 examples: {'rewards_train/chosen': '0.044181', 'rewards_train/rejected': '0.030401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013779', 'logps_train/rejected': '-130.05', 'logps_train/chosen': '-185.1', 'loss/train': '0.69821', 'examples_per_second': '31.395', 'grad_norm': '97.5', 'counters/examples': 215136, 'counters/updates': 6723}
train stats after 215168 examples: {'rewards_train/chosen': '0.098051', 'rewards_train/rejected': '0.040741', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057311', 'logps_train/rejected': '-160.95', 'logps_train/chosen': '-126.14', 'loss/train': '0.67888', 'examples_per_second': '30.222', 'grad_norm': '154', 'counters/examples': 215168, 'counters/updates': 6724}
train stats after 215200 examples: {'rewards_train/chosen': '0.039118', 'rewards_train/rejected': '-0.0027869', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041905', 'logps_train/rejected': '-125.71', 'logps_train/chosen': '-129.8', 'loss/train': '0.68919', 'examples_per_second': '31.709', 'grad_norm': '123', 'counters/examples': 215200, 'counters/updates': 6725}
train stats after 215232 examples: {'rewards_train/chosen': '0.067306', 'rewards_train/rejected': '0.040331', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026974', 'logps_train/rejected': '-115.49', 'logps_train/chosen': '-137.19', 'loss/train': '0.69685', 'examples_per_second': '31.687', 'grad_norm': '58', 'counters/examples': 215232, 'counters/updates': 6726}
train stats after 215264 examples: {'rewards_train/chosen': '0.14489', 'rewards_train/rejected': '0.091576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053311', 'logps_train/rejected': '-202.78', 'logps_train/chosen': '-177.59', 'loss/train': '0.68934', 'examples_per_second': '30.662', 'grad_norm': '220', 'counters/examples': 215264, 'counters/updates': 6727}
train stats after 215296 examples: {'rewards_train/chosen': '0.15324', 'rewards_train/rejected': '0.016132', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13711', 'logps_train/rejected': '-113.7', 'logps_train/chosen': '-161.95', 'loss/train': '0.63866', 'examples_per_second': '30.723', 'grad_norm': '56', 'counters/examples': 215296, 'counters/updates': 6728}
train stats after 215328 examples: {'rewards_train/chosen': '0.060069', 'rewards_train/rejected': '0.015504', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044565', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-155.89', 'loss/train': '0.69122', 'examples_per_second': '30.946', 'grad_norm': '98.5', 'counters/examples': 215328, 'counters/updates': 6729}
skipping logging after 215360 examples to avoid logging too frequently
train stats after 215392 examples: {'rewards_train/chosen': '0.036075', 'rewards_train/rejected': '-0.00010704', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036182', 'logps_train/rejected': '-127.23', 'logps_train/chosen': '-110.71', 'loss/train': '0.6825', 'examples_per_second': '33.716', 'grad_norm': '63.25', 'counters/examples': 215392, 'counters/updates': 6731}
train stats after 215424 examples: {'rewards_train/chosen': '0.077657', 'rewards_train/rejected': '0.093209', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015552', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-163.26', 'loss/train': '0.71767', 'examples_per_second': '32.4', 'grad_norm': '76.5', 'counters/examples': 215424, 'counters/updates': 6732}
train stats after 215456 examples: {'rewards_train/chosen': '0.055058', 'rewards_train/rejected': '0.02495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030108', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-140.25', 'loss/train': '0.68557', 'examples_per_second': '31.197', 'grad_norm': '101.5', 'counters/examples': 215456, 'counters/updates': 6733}
train stats after 215488 examples: {'rewards_train/chosen': '0.10783', 'rewards_train/rejected': '0.043508', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064318', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-142.12', 'loss/train': '0.66957', 'examples_per_second': '31.712', 'grad_norm': '101.5', 'counters/examples': 215488, 'counters/updates': 6734}
skipping logging after 215520 examples to avoid logging too frequently
train stats after 215552 examples: {'rewards_train/chosen': '0.085966', 'rewards_train/rejected': '0.020296', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065671', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-128.41', 'loss/train': '0.67145', 'examples_per_second': '33.26', 'grad_norm': '74.5', 'counters/examples': 215552, 'counters/updates': 6736}
train stats after 215584 examples: {'rewards_train/chosen': '0.085277', 'rewards_train/rejected': '0.025946', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059331', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-155.32', 'loss/train': '0.67239', 'examples_per_second': '31.655', 'grad_norm': '87.5', 'counters/examples': 215584, 'counters/updates': 6737}
train stats after 215616 examples: {'rewards_train/chosen': '0.099457', 'rewards_train/rejected': '0.084356', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015101', 'logps_train/rejected': '-157.01', 'logps_train/chosen': '-145.7', 'loss/train': '0.69594', 'examples_per_second': '30.653', 'grad_norm': '98.5', 'counters/examples': 215616, 'counters/updates': 6738}
train stats after 215648 examples: {'rewards_train/chosen': '0.12236', 'rewards_train/rejected': '-0.0090695', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13143', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-160.52', 'loss/train': '0.6369', 'examples_per_second': '30.702', 'grad_norm': '69.5', 'counters/examples': 215648, 'counters/updates': 6739}
skipping logging after 215680 examples to avoid logging too frequently
train stats after 215712 examples: {'rewards_train/chosen': '0.047508', 'rewards_train/rejected': '-0.014845', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062353', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-179.85', 'loss/train': '0.68351', 'examples_per_second': '33.33', 'grad_norm': '91', 'counters/examples': 215712, 'counters/updates': 6741}
train stats after 215744 examples: {'rewards_train/chosen': '0.12421', 'rewards_train/rejected': '0.10127', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.022942', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-153.47', 'loss/train': '0.69733', 'examples_per_second': '30.422', 'grad_norm': '89', 'counters/examples': 215744, 'counters/updates': 6742}
skipping logging after 215776 examples to avoid logging too frequently
train stats after 215808 examples: {'rewards_train/chosen': '0.037696', 'rewards_train/rejected': '0.085958', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048262', 'logps_train/rejected': '-151.69', 'logps_train/chosen': '-166.7', 'loss/train': '0.73278', 'examples_per_second': '31.44', 'grad_norm': '73', 'counters/examples': 215808, 'counters/updates': 6744}
train stats after 215840 examples: {'rewards_train/chosen': '0.10683', 'rewards_train/rejected': '0.0041285', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1027', 'logps_train/rejected': '-148.5', 'logps_train/chosen': '-152.53', 'loss/train': '0.66481', 'examples_per_second': '29.555', 'grad_norm': '67.5', 'counters/examples': 215840, 'counters/updates': 6745}
train stats after 215872 examples: {'rewards_train/chosen': '0.1587', 'rewards_train/rejected': '0.0039152', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15478', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-153.78', 'loss/train': '0.62674', 'examples_per_second': '31.623', 'grad_norm': '55', 'counters/examples': 215872, 'counters/updates': 6746}
train stats after 215904 examples: {'rewards_train/chosen': '0.1217', 'rewards_train/rejected': '0.062827', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058872', 'logps_train/rejected': '-165.94', 'logps_train/chosen': '-154.28', 'loss/train': '0.67836', 'examples_per_second': '31.704', 'grad_norm': '70', 'counters/examples': 215904, 'counters/updates': 6747}
skipping logging after 215936 examples to avoid logging too frequently
train stats after 215968 examples: {'rewards_train/chosen': '0.17379', 'rewards_train/rejected': '0.013897', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1599', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-169.82', 'loss/train': '0.67105', 'examples_per_second': '36.181', 'grad_norm': '45.25', 'counters/examples': 215968, 'counters/updates': 6749}
train stats after 216000 examples: {'rewards_train/chosen': '0.050668', 'rewards_train/rejected': '0.028723', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021945', 'logps_train/rejected': '-163.29', 'logps_train/chosen': '-157.31', 'loss/train': '0.6913', 'examples_per_second': '30.217', 'grad_norm': '112.5', 'counters/examples': 216000, 'counters/updates': 6750}
Running evaluation after 216000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.10it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.01it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.86it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.90it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.92it/s]
eval after 216000: {'rewards_eval/chosen': '0.12645', 'rewards_eval/rejected': '0.05158', 'rewards_eval/accuracies': '0.57031', 'rewards_eval/margins': '0.074868', 'logps_eval/rejected': '-127.63', 'logps_eval/chosen': '-149.74', 'loss/eval': '0.66975'}
skipping save for non epoch
train stats after 216032 examples: {'rewards_train/chosen': '0.1017', 'rewards_train/rejected': '0.010881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090821', 'logps_train/rejected': '-137.68', 'logps_train/chosen': '-156.46', 'loss/train': '0.66091', 'examples_per_second': '34.431', 'grad_norm': '52.5', 'counters/examples': 216032, 'counters/updates': 6751}
train stats after 216064 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.10088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032553', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-148.53', 'loss/train': '0.71577', 'examples_per_second': '32.527', 'grad_norm': '106.5', 'counters/examples': 216064, 'counters/updates': 6752}
skipping logging after 216096 examples to avoid logging too frequently
train stats after 216128 examples: {'rewards_train/chosen': '0.19826', 'rewards_train/rejected': '0.13718', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061072', 'logps_train/rejected': '-150.67', 'logps_train/chosen': '-217.06', 'loss/train': '0.68353', 'examples_per_second': '30.28', 'grad_norm': '79.5', 'counters/examples': 216128, 'counters/updates': 6754}
train stats after 216160 examples: {'rewards_train/chosen': '0.066997', 'rewards_train/rejected': '0.10849', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04149', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-141.7', 'loss/train': '0.73645', 'examples_per_second': '32.241', 'grad_norm': '94.5', 'counters/examples': 216160, 'counters/updates': 6755}
train stats after 216192 examples: {'rewards_train/chosen': '0.14709', 'rewards_train/rejected': '0.085359', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.061727', 'logps_train/rejected': '-183.3', 'logps_train/chosen': '-165.51', 'loss/train': '0.67481', 'examples_per_second': '31.305', 'grad_norm': '76.5', 'counters/examples': 216192, 'counters/updates': 6756}
train stats after 216224 examples: {'rewards_train/chosen': '0.11318', 'rewards_train/rejected': '0.022823', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090353', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-128.89', 'loss/train': '0.67642', 'examples_per_second': '30.749', 'grad_norm': '85', 'counters/examples': 216224, 'counters/updates': 6757}
train stats after 216256 examples: {'rewards_train/chosen': '0.078149', 'rewards_train/rejected': '0.049239', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02891', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-188.95', 'loss/train': '0.69991', 'examples_per_second': '32.442', 'grad_norm': '81', 'counters/examples': 216256, 'counters/updates': 6758}
train stats after 216288 examples: {'rewards_train/chosen': '0.05993', 'rewards_train/rejected': '0.016438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043492', 'logps_train/rejected': '-170.63', 'logps_train/chosen': '-133.68', 'loss/train': '0.69118', 'examples_per_second': '31.33', 'grad_norm': '472', 'counters/examples': 216288, 'counters/updates': 6759}
train stats after 216320 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '0.10006', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030373', 'logps_train/rejected': '-157.7', 'logps_train/chosen': '-200.47', 'loss/train': '0.70492', 'examples_per_second': '31.64', 'grad_norm': '100', 'counters/examples': 216320, 'counters/updates': 6760}
train stats after 216352 examples: {'rewards_train/chosen': '0.13901', 'rewards_train/rejected': '0.067963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071043', 'logps_train/rejected': '-149.44', 'logps_train/chosen': '-174.87', 'loss/train': '0.67944', 'examples_per_second': '33.052', 'grad_norm': '85.5', 'counters/examples': 216352, 'counters/updates': 6761}
train stats after 216384 examples: {'rewards_train/chosen': '0.16379', 'rewards_train/rejected': '-0.0029365', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16672', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-169.4', 'loss/train': '0.64054', 'examples_per_second': '31.66', 'grad_norm': '84', 'counters/examples': 216384, 'counters/updates': 6762}
train stats after 216416 examples: {'rewards_train/chosen': '0.13317', 'rewards_train/rejected': '0.057705', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075465', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-142.1', 'loss/train': '0.67111', 'examples_per_second': '30.132', 'grad_norm': '88.5', 'counters/examples': 216416, 'counters/updates': 6763}
skipping logging after 216448 examples to avoid logging too frequently
train stats after 216480 examples: {'rewards_train/chosen': '0.015011', 'rewards_train/rejected': '0.022924', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0079125', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-155.51', 'loss/train': '0.71195', 'examples_per_second': '33.824', 'grad_norm': '74.5', 'counters/examples': 216480, 'counters/updates': 6765}
train stats after 216512 examples: {'rewards_train/chosen': '0.14174', 'rewards_train/rejected': '0.066287', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07545', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-147.04', 'loss/train': '0.6746', 'examples_per_second': '31.903', 'grad_norm': '106.5', 'counters/examples': 216512, 'counters/updates': 6766}
train stats after 216544 examples: {'rewards_train/chosen': '0.095528', 'rewards_train/rejected': '0.054947', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040581', 'logps_train/rejected': '-172.87', 'logps_train/chosen': '-185.42', 'loss/train': '0.69408', 'examples_per_second': '31.658', 'grad_norm': '126.5', 'counters/examples': 216544, 'counters/updates': 6767}
train stats after 216576 examples: {'rewards_train/chosen': '0.042619', 'rewards_train/rejected': '0.11679', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.074168', 'logps_train/rejected': '-159.81', 'logps_train/chosen': '-151.33', 'loss/train': '0.75073', 'examples_per_second': '30.675', 'grad_norm': '78.5', 'counters/examples': 216576, 'counters/updates': 6768}
train stats after 216608 examples: {'rewards_train/chosen': '0.11563', 'rewards_train/rejected': '-0.0040631', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11969', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-125.46', 'loss/train': '0.64081', 'examples_per_second': '31.889', 'grad_norm': '74.5', 'counters/examples': 216608, 'counters/updates': 6769}
train stats after 216640 examples: {'rewards_train/chosen': '0.1009', 'rewards_train/rejected': '0.093625', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0072748', 'logps_train/rejected': '-157.04', 'logps_train/chosen': '-134.75', 'loss/train': '0.70787', 'examples_per_second': '30.304', 'grad_norm': '126', 'counters/examples': 216640, 'counters/updates': 6770}
skipping logging after 216672 examples to avoid logging too frequently
train stats after 216704 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '-0.03591', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18627', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-174.5', 'loss/train': '0.61884', 'examples_per_second': '32.309', 'grad_norm': '83.5', 'counters/examples': 216704, 'counters/updates': 6772}
train stats after 216736 examples: {'rewards_train/chosen': '0.084098', 'rewards_train/rejected': '0.039708', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044391', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-165.29', 'loss/train': '0.6837', 'examples_per_second': '30.824', 'grad_norm': '81', 'counters/examples': 216736, 'counters/updates': 6773}
train stats after 216768 examples: {'rewards_train/chosen': '0.15464', 'rewards_train/rejected': '0.020332', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13431', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-126.26', 'loss/train': '0.64461', 'examples_per_second': '31.686', 'grad_norm': '86.5', 'counters/examples': 216768, 'counters/updates': 6774}
skipping logging after 216800 examples to avoid logging too frequently
train stats after 216832 examples: {'rewards_train/chosen': '0.19354', 'rewards_train/rejected': '0.063573', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12997', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-144.67', 'loss/train': '0.64757', 'examples_per_second': '30.299', 'grad_norm': '159', 'counters/examples': 216832, 'counters/updates': 6776}
skipping logging after 216864 examples to avoid logging too frequently
train stats after 216896 examples: {'rewards_train/chosen': '0.054989', 'rewards_train/rejected': '0.046636', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083531', 'logps_train/rejected': '-97.242', 'logps_train/chosen': '-133.73', 'loss/train': '0.69822', 'examples_per_second': '35.753', 'grad_norm': '75', 'counters/examples': 216896, 'counters/updates': 6778}
train stats after 216928 examples: {'rewards_train/chosen': '0.1886', 'rewards_train/rejected': '0.17082', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017779', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-140.96', 'loss/train': '0.70563', 'examples_per_second': '33.041', 'grad_norm': '208', 'counters/examples': 216928, 'counters/updates': 6779}
train stats after 216960 examples: {'rewards_train/chosen': '0.13939', 'rewards_train/rejected': '0.12229', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017109', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-153.2', 'loss/train': '0.69583', 'examples_per_second': '31.536', 'grad_norm': '88.5', 'counters/examples': 216960, 'counters/updates': 6780}
train stats after 216992 examples: {'rewards_train/chosen': '-0.0074077', 'rewards_train/rejected': '0.015054', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.022461', 'logps_train/rejected': '-147.15', 'logps_train/chosen': '-119.53', 'loss/train': '0.72978', 'examples_per_second': '32.627', 'grad_norm': '129', 'counters/examples': 216992, 'counters/updates': 6781}
train stats after 217024 examples: {'rewards_train/chosen': '0.18112', 'rewards_train/rejected': '0.096013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08511', 'logps_train/rejected': '-119.78', 'logps_train/chosen': '-156.31', 'loss/train': '0.67236', 'examples_per_second': '30.994', 'grad_norm': '78.5', 'counters/examples': 217024, 'counters/updates': 6782}
train stats after 217056 examples: {'rewards_train/chosen': '0.09414', 'rewards_train/rejected': '0.069909', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024232', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-172.58', 'loss/train': '0.68843', 'examples_per_second': '31.802', 'grad_norm': '72', 'counters/examples': 217056, 'counters/updates': 6783}
skipping logging after 217088 examples to avoid logging too frequently
train stats after 217120 examples: {'rewards_train/chosen': '0.09058', 'rewards_train/rejected': '0.057704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032876', 'logps_train/rejected': '-131', 'logps_train/chosen': '-112.97', 'loss/train': '0.68512', 'examples_per_second': '34.395', 'grad_norm': '101.5', 'counters/examples': 217120, 'counters/updates': 6785}
train stats after 217152 examples: {'rewards_train/chosen': '0.10958', 'rewards_train/rejected': '0.12518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015601', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-161.07', 'loss/train': '0.71431', 'examples_per_second': '31.125', 'grad_norm': '110.5', 'counters/examples': 217152, 'counters/updates': 6786}
train stats after 217184 examples: {'rewards_train/chosen': '0.15181', 'rewards_train/rejected': '0.11652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035298', 'logps_train/rejected': '-157', 'logps_train/chosen': '-163.79', 'loss/train': '0.69008', 'examples_per_second': '31.668', 'grad_norm': '147', 'counters/examples': 217184, 'counters/updates': 6787}
train stats after 217216 examples: {'rewards_train/chosen': '0.10225', 'rewards_train/rejected': '0.027631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074616', 'logps_train/rejected': '-142.24', 'logps_train/chosen': '-153.14', 'loss/train': '0.67775', 'examples_per_second': '31.619', 'grad_norm': '112.5', 'counters/examples': 217216, 'counters/updates': 6788}
train stats after 217248 examples: {'rewards_train/chosen': '0.19982', 'rewards_train/rejected': '0.048031', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15179', 'logps_train/rejected': '-186.58', 'logps_train/chosen': '-179.74', 'loss/train': '0.65527', 'examples_per_second': '32.459', 'grad_norm': '139', 'counters/examples': 217248, 'counters/updates': 6789}
train stats after 217280 examples: {'rewards_train/chosen': '0.10519', 'rewards_train/rejected': '0.04622', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058967', 'logps_train/rejected': '-186.2', 'logps_train/chosen': '-188.32', 'loss/train': '0.68149', 'examples_per_second': '31.683', 'grad_norm': '171', 'counters/examples': 217280, 'counters/updates': 6790}
train stats after 217312 examples: {'rewards_train/chosen': '0.051134', 'rewards_train/rejected': '-0.0074711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058605', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-132.48', 'loss/train': '0.67761', 'examples_per_second': '31.693', 'grad_norm': '195', 'counters/examples': 217312, 'counters/updates': 6791}
train stats after 217344 examples: {'rewards_train/chosen': '0.12805', 'rewards_train/rejected': '0.035825', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092225', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-169.65', 'loss/train': '0.66409', 'examples_per_second': '31.328', 'grad_norm': '70', 'counters/examples': 217344, 'counters/updates': 6792}
train stats after 217376 examples: {'rewards_train/chosen': '-0.0094112', 'rewards_train/rejected': '0.0034212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012832', 'logps_train/rejected': '-130.01', 'logps_train/chosen': '-133.29', 'loss/train': '0.71071', 'examples_per_second': '30.593', 'grad_norm': '72.5', 'counters/examples': 217376, 'counters/updates': 6793}
train stats after 217408 examples: {'rewards_train/chosen': '0.10609', 'rewards_train/rejected': '0.0095515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096538', 'logps_train/rejected': '-128.3', 'logps_train/chosen': '-165.33', 'loss/train': '0.66282', 'examples_per_second': '31.296', 'grad_norm': '71', 'counters/examples': 217408, 'counters/updates': 6794}
train stats after 217440 examples: {'rewards_train/chosen': '0.063221', 'rewards_train/rejected': '0.060128', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0030928', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-144.89', 'loss/train': '0.70468', 'examples_per_second': '30.608', 'grad_norm': '79.5', 'counters/examples': 217440, 'counters/updates': 6795}
skipping logging after 217472 examples to avoid logging too frequently
train stats after 217504 examples: {'rewards_train/chosen': '0.1221', 'rewards_train/rejected': '0.024656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097443', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-179.17', 'loss/train': '0.66649', 'examples_per_second': '32.863', 'grad_norm': '64', 'counters/examples': 217504, 'counters/updates': 6797}
train stats after 217536 examples: {'rewards_train/chosen': '0.02558', 'rewards_train/rejected': '-0.0044301', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03001', 'logps_train/rejected': '-124.78', 'logps_train/chosen': '-115.91', 'loss/train': '0.69865', 'examples_per_second': '31.077', 'grad_norm': '71.5', 'counters/examples': 217536, 'counters/updates': 6798}
train stats after 217568 examples: {'rewards_train/chosen': '0.11507', 'rewards_train/rejected': '0.065611', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049455', 'logps_train/rejected': '-148.36', 'logps_train/chosen': '-122', 'loss/train': '0.68467', 'examples_per_second': '32.544', 'grad_norm': '103.5', 'counters/examples': 217568, 'counters/updates': 6799}
skipping logging after 217600 examples to avoid logging too frequently
train stats after 217632 examples: {'rewards_train/chosen': '0.09594', 'rewards_train/rejected': '0.057023', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038917', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-126.49', 'loss/train': '0.69009', 'examples_per_second': '26.363', 'grad_norm': '91', 'counters/examples': 217632, 'counters/updates': 6801}
train stats after 217664 examples: {'rewards_train/chosen': '0.070852', 'rewards_train/rejected': '-0.013435', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084287', 'logps_train/rejected': '-132.34', 'logps_train/chosen': '-195.98', 'loss/train': '0.6737', 'examples_per_second': '30.032', 'grad_norm': '134', 'counters/examples': 217664, 'counters/updates': 6802}
train stats after 217696 examples: {'rewards_train/chosen': '0.079351', 'rewards_train/rejected': '0.094961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01561', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-186.05', 'loss/train': '0.72326', 'examples_per_second': '31.546', 'grad_norm': '113', 'counters/examples': 217696, 'counters/updates': 6803}
train stats after 217728 examples: {'rewards_train/chosen': '0.10765', 'rewards_train/rejected': '0.061434', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046216', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-158.77', 'loss/train': '0.69649', 'examples_per_second': '30.209', 'grad_norm': '131', 'counters/examples': 217728, 'counters/updates': 6804}
skipping logging after 217760 examples to avoid logging too frequently
train stats after 217792 examples: {'rewards_train/chosen': '0.11787', 'rewards_train/rejected': '0.05968', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05819', 'logps_train/rejected': '-161.03', 'logps_train/chosen': '-170.11', 'loss/train': '0.68552', 'examples_per_second': '32.32', 'grad_norm': '70.5', 'counters/examples': 217792, 'counters/updates': 6806}
train stats after 217824 examples: {'rewards_train/chosen': '0.096195', 'rewards_train/rejected': '-0.035126', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13132', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-151.96', 'loss/train': '0.65562', 'examples_per_second': '30.823', 'grad_norm': '65.5', 'counters/examples': 217824, 'counters/updates': 6807}
train stats after 217856 examples: {'rewards_train/chosen': '0.12905', 'rewards_train/rejected': '-0.015527', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14458', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-173.13', 'loss/train': '0.63949', 'examples_per_second': '30.655', 'grad_norm': '146', 'counters/examples': 217856, 'counters/updates': 6808}
train stats after 217888 examples: {'rewards_train/chosen': '0.060462', 'rewards_train/rejected': '-0.0021075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06257', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-135.53', 'loss/train': '0.6762', 'examples_per_second': '31.405', 'grad_norm': '68', 'counters/examples': 217888, 'counters/updates': 6809}
skipping logging after 217920 examples to avoid logging too frequently
train stats after 217952 examples: {'rewards_train/chosen': '0.1331', 'rewards_train/rejected': '-0.0044365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13753', 'logps_train/rejected': '-120.69', 'logps_train/chosen': '-136.79', 'loss/train': '0.63747', 'examples_per_second': '31.958', 'grad_norm': '59.75', 'counters/examples': 217952, 'counters/updates': 6811}
train stats after 217984 examples: {'rewards_train/chosen': '0.061817', 'rewards_train/rejected': '0.044518', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017299', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-133.29', 'loss/train': '0.69606', 'examples_per_second': '30.595', 'grad_norm': '62', 'counters/examples': 217984, 'counters/updates': 6812}
train stats after 218016 examples: {'rewards_train/chosen': '0.12959', 'rewards_train/rejected': '0.028731', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10086', 'logps_train/rejected': '-138.15', 'logps_train/chosen': '-171.42', 'loss/train': '0.68064', 'examples_per_second': '31.575', 'grad_norm': '76.5', 'counters/examples': 218016, 'counters/updates': 6813}
train stats after 218048 examples: {'rewards_train/chosen': '0.095292', 'rewards_train/rejected': '-0.0027968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098089', 'logps_train/rejected': '-140.93', 'logps_train/chosen': '-151.88', 'loss/train': '0.65961', 'examples_per_second': '31.288', 'grad_norm': '137', 'counters/examples': 218048, 'counters/updates': 6814}
train stats after 218080 examples: {'rewards_train/chosen': '0.11544', 'rewards_train/rejected': '0.0043835', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11105', 'logps_train/rejected': '-169.75', 'logps_train/chosen': '-227.17', 'loss/train': '0.65136', 'examples_per_second': '31.627', 'grad_norm': '131', 'counters/examples': 218080, 'counters/updates': 6815}
train stats after 218112 examples: {'rewards_train/chosen': '0.10414', 'rewards_train/rejected': '0.058096', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046039', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-136.65', 'loss/train': '0.68671', 'examples_per_second': '32.003', 'grad_norm': '50.5', 'counters/examples': 218112, 'counters/updates': 6816}
train stats after 218144 examples: {'rewards_train/chosen': '0.078212', 'rewards_train/rejected': '0.060032', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01818', 'logps_train/rejected': '-138.92', 'logps_train/chosen': '-139.66', 'loss/train': '0.69091', 'examples_per_second': '31.546', 'grad_norm': '113', 'counters/examples': 218144, 'counters/updates': 6817}
train stats after 218176 examples: {'rewards_train/chosen': '0.08881', 'rewards_train/rejected': '0.084045', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0047655', 'logps_train/rejected': '-150.78', 'logps_train/chosen': '-136.69', 'loss/train': '0.70297', 'examples_per_second': '30.146', 'grad_norm': '108.5', 'counters/examples': 218176, 'counters/updates': 6818}
train stats after 218208 examples: {'rewards_train/chosen': '0.1332', 'rewards_train/rejected': '-0.036923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17012', 'logps_train/rejected': '-155.33', 'logps_train/chosen': '-167.6', 'loss/train': '0.62505', 'examples_per_second': '32.298', 'grad_norm': '67.5', 'counters/examples': 218208, 'counters/updates': 6819}
train stats after 218240 examples: {'rewards_train/chosen': '0.023896', 'rewards_train/rejected': '-0.0090939', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03299', 'logps_train/rejected': '-169.36', 'logps_train/chosen': '-157.09', 'loss/train': '0.68472', 'examples_per_second': '32.358', 'grad_norm': '89.5', 'counters/examples': 218240, 'counters/updates': 6820}
train stats after 218272 examples: {'rewards_train/chosen': '0.067755', 'rewards_train/rejected': '-0.006671', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074426', 'logps_train/rejected': '-101.98', 'logps_train/chosen': '-115.15', 'loss/train': '0.66292', 'examples_per_second': '31.735', 'grad_norm': '43', 'counters/examples': 218272, 'counters/updates': 6821}
train stats after 218304 examples: {'rewards_train/chosen': '0.082518', 'rewards_train/rejected': '0.027751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054766', 'logps_train/rejected': '-100.12', 'logps_train/chosen': '-155.49', 'loss/train': '0.67782', 'examples_per_second': '31.66', 'grad_norm': '105.5', 'counters/examples': 218304, 'counters/updates': 6822}
train stats after 218336 examples: {'rewards_train/chosen': '0.098642', 'rewards_train/rejected': '0.030333', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068309', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-142.87', 'loss/train': '0.66816', 'examples_per_second': '30.897', 'grad_norm': '53.25', 'counters/examples': 218336, 'counters/updates': 6823}
train stats after 218368 examples: {'rewards_train/chosen': '0.070316', 'rewards_train/rejected': '0.0022513', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068065', 'logps_train/rejected': '-123.17', 'logps_train/chosen': '-126.12', 'loss/train': '0.67272', 'examples_per_second': '31.651', 'grad_norm': '102.5', 'counters/examples': 218368, 'counters/updates': 6824}
train stats after 218400 examples: {'rewards_train/chosen': '0.060917', 'rewards_train/rejected': '0.092247', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03133', 'logps_train/rejected': '-149.16', 'logps_train/chosen': '-156.99', 'loss/train': '0.72956', 'examples_per_second': '31.657', 'grad_norm': '79.5', 'counters/examples': 218400, 'counters/updates': 6825}
skipping logging after 218432 examples to avoid logging too frequently
train stats after 218464 examples: {'rewards_train/chosen': '0.079957', 'rewards_train/rejected': '0.057059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.022898', 'logps_train/rejected': '-137.18', 'logps_train/chosen': '-158.98', 'loss/train': '0.69226', 'examples_per_second': '31.405', 'grad_norm': '92.5', 'counters/examples': 218464, 'counters/updates': 6827}
train stats after 218496 examples: {'rewards_train/chosen': '-0.013321', 'rewards_train/rejected': '0.092912', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10623', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-166.1', 'loss/train': '0.77143', 'examples_per_second': '31.317', 'grad_norm': '151', 'counters/examples': 218496, 'counters/updates': 6828}
train stats after 218528 examples: {'rewards_train/chosen': '0.040275', 'rewards_train/rejected': '-0.020186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060461', 'logps_train/rejected': '-100.65', 'logps_train/chosen': '-166.02', 'loss/train': '0.67285', 'examples_per_second': '31.923', 'grad_norm': '73.5', 'counters/examples': 218528, 'counters/updates': 6829}
skipping logging after 218560 examples to avoid logging too frequently
train stats after 218592 examples: {'rewards_train/chosen': '0.14104', 'rewards_train/rejected': '0.12033', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020706', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-147.51', 'loss/train': '0.7157', 'examples_per_second': '30.277', 'grad_norm': '94.5', 'counters/examples': 218592, 'counters/updates': 6831}
skipping logging after 218624 examples to avoid logging too frequently
train stats after 218656 examples: {'rewards_train/chosen': '0.058856', 'rewards_train/rejected': '-0.020637', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079493', 'logps_train/rejected': '-117.19', 'logps_train/chosen': '-157.94', 'loss/train': '0.66801', 'examples_per_second': '34.097', 'grad_norm': '85', 'counters/examples': 218656, 'counters/updates': 6833}
train stats after 218688 examples: {'rewards_train/chosen': '0.12509', 'rewards_train/rejected': '0.002786', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1223', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-174.3', 'loss/train': '0.64535', 'examples_per_second': '31.669', 'grad_norm': '178', 'counters/examples': 218688, 'counters/updates': 6834}
train stats after 218720 examples: {'rewards_train/chosen': '0.20591', 'rewards_train/rejected': '0.16614', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039767', 'logps_train/rejected': '-155.96', 'logps_train/chosen': '-174.94', 'loss/train': '0.68998', 'examples_per_second': '31.667', 'grad_norm': '284', 'counters/examples': 218720, 'counters/updates': 6835}
train stats after 218752 examples: {'rewards_train/chosen': '-0.0019561', 'rewards_train/rejected': '-0.0065436', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0045875', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-167.43', 'loss/train': '0.70594', 'examples_per_second': '24.426', 'grad_norm': '177', 'counters/examples': 218752, 'counters/updates': 6836}
train stats after 218784 examples: {'rewards_train/chosen': '0.045087', 'rewards_train/rejected': '0.023679', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021407', 'logps_train/rejected': '-110.9', 'logps_train/chosen': '-138.14', 'loss/train': '0.69855', 'examples_per_second': '32.419', 'grad_norm': '106', 'counters/examples': 218784, 'counters/updates': 6837}
skipping logging after 218816 examples to avoid logging too frequently
train stats after 218848 examples: {'rewards_train/chosen': '0.15104', 'rewards_train/rejected': '0.038846', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11219', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-150.73', 'loss/train': '0.65145', 'examples_per_second': '24.368', 'grad_norm': '83', 'counters/examples': 218848, 'counters/updates': 6839}
skipping logging after 218880 examples to avoid logging too frequently
train stats after 218912 examples: {'rewards_train/chosen': '0.064252', 'rewards_train/rejected': '0.004932', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05932', 'logps_train/rejected': '-100.4', 'logps_train/chosen': '-125.95', 'loss/train': '0.67557', 'examples_per_second': '32.979', 'grad_norm': '52', 'counters/examples': 218912, 'counters/updates': 6841}
train stats after 218944 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.069371', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040866', 'logps_train/rejected': '-122.71', 'logps_train/chosen': '-153.33', 'loss/train': '0.68363', 'examples_per_second': '31.639', 'grad_norm': '86', 'counters/examples': 218944, 'counters/updates': 6842}
train stats after 218976 examples: {'rewards_train/chosen': '0.017554', 'rewards_train/rejected': '0.12303', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10548', 'logps_train/rejected': '-142.12', 'logps_train/chosen': '-120.04', 'loss/train': '0.77126', 'examples_per_second': '30.205', 'grad_norm': '115', 'counters/examples': 218976, 'counters/updates': 6843}
train stats after 219008 examples: {'rewards_train/chosen': '0.0080934', 'rewards_train/rejected': '0.036514', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.028421', 'logps_train/rejected': '-163.35', 'logps_train/chosen': '-164.59', 'loss/train': '0.7137', 'examples_per_second': '31.639', 'grad_norm': '66.5', 'counters/examples': 219008, 'counters/updates': 6844}
train stats after 219040 examples: {'rewards_train/chosen': '0.038937', 'rewards_train/rejected': '0.0017216', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037216', 'logps_train/rejected': '-159.25', 'logps_train/chosen': '-195.82', 'loss/train': '0.68928', 'examples_per_second': '32.924', 'grad_norm': '66', 'counters/examples': 219040, 'counters/updates': 6845}
train stats after 219072 examples: {'rewards_train/chosen': '0.16157', 'rewards_train/rejected': '0.022645', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13892', 'logps_train/rejected': '-113.7', 'logps_train/chosen': '-151.46', 'loss/train': '0.63981', 'examples_per_second': '31.637', 'grad_norm': '56.75', 'counters/examples': 219072, 'counters/updates': 6846}
skipping logging after 219104 examples to avoid logging too frequently
train stats after 219136 examples: {'rewards_train/chosen': '0.067146', 'rewards_train/rejected': '0.044564', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022582', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-116.95', 'loss/train': '0.69635', 'examples_per_second': '31.667', 'grad_norm': '90', 'counters/examples': 219136, 'counters/updates': 6848}
skipping logging after 219168 examples to avoid logging too frequently
train stats after 219200 examples: {'rewards_train/chosen': '0.089131', 'rewards_train/rejected': '0.077715', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011415', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-171.31', 'loss/train': '0.69829', 'examples_per_second': '31.08', 'grad_norm': '95', 'counters/examples': 219200, 'counters/updates': 6850}
train stats after 219232 examples: {'rewards_train/chosen': '0.075406', 'rewards_train/rejected': '-0.0071955', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082602', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-117.16', 'loss/train': '0.66475', 'examples_per_second': '30.544', 'grad_norm': '77', 'counters/examples': 219232, 'counters/updates': 6851}
skipping logging after 219264 examples to avoid logging too frequently
train stats after 219296 examples: {'rewards_train/chosen': '0.21133', 'rewards_train/rejected': '0.088846', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12248', 'logps_train/rejected': '-130.02', 'logps_train/chosen': '-149.32', 'loss/train': '0.65992', 'examples_per_second': '32.116', 'grad_norm': '76.5', 'counters/examples': 219296, 'counters/updates': 6853}
train stats after 219328 examples: {'rewards_train/chosen': '0.11274', 'rewards_train/rejected': '0.08535', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027391', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-157.57', 'loss/train': '0.68587', 'examples_per_second': '31.626', 'grad_norm': '68.5', 'counters/examples': 219328, 'counters/updates': 6854}
train stats after 219360 examples: {'rewards_train/chosen': '0.035804', 'rewards_train/rejected': '-0.033664', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069469', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-133.19', 'loss/train': '0.67572', 'examples_per_second': '32.473', 'grad_norm': '105.5', 'counters/examples': 219360, 'counters/updates': 6855}
skipping logging after 219392 examples to avoid logging too frequently
train stats after 219424 examples: {'rewards_train/chosen': '0.1706', 'rewards_train/rejected': '0.12316', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047441', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-169.47', 'loss/train': '0.70297', 'examples_per_second': '32.547', 'grad_norm': '78', 'counters/examples': 219424, 'counters/updates': 6857}
train stats after 219456 examples: {'rewards_train/chosen': '0.13455', 'rewards_train/rejected': '0.072014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062533', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-158.69', 'loss/train': '0.67309', 'examples_per_second': '30.876', 'grad_norm': '90', 'counters/examples': 219456, 'counters/updates': 6858}
train stats after 219488 examples: {'rewards_train/chosen': '0.089226', 'rewards_train/rejected': '0.1139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.024671', 'logps_train/rejected': '-162.05', 'logps_train/chosen': '-169.83', 'loss/train': '0.71755', 'examples_per_second': '31.667', 'grad_norm': '74.5', 'counters/examples': 219488, 'counters/updates': 6859}
skipping logging after 219520 examples to avoid logging too frequently
train stats after 219552 examples: {'rewards_train/chosen': '0.16664', 'rewards_train/rejected': '-0.0096167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17626', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-142.22', 'loss/train': '0.6253', 'examples_per_second': '32.354', 'grad_norm': '116.5', 'counters/examples': 219552, 'counters/updates': 6861}
train stats after 219584 examples: {'rewards_train/chosen': '0.18423', 'rewards_train/rejected': '0.043027', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1412', 'logps_train/rejected': '-143.58', 'logps_train/chosen': '-182.3', 'loss/train': '0.64953', 'examples_per_second': '30.629', 'grad_norm': '105.5', 'counters/examples': 219584, 'counters/updates': 6862}
train stats after 219616 examples: {'rewards_train/chosen': '0.0061422', 'rewards_train/rejected': '0.096477', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.090335', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-178.76', 'loss/train': '0.7525', 'examples_per_second': '30.943', 'grad_norm': '75', 'counters/examples': 219616, 'counters/updates': 6863}
train stats after 219648 examples: {'rewards_train/chosen': '0.068102', 'rewards_train/rejected': '0.012999', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055103', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-141.88', 'loss/train': '0.67593', 'examples_per_second': '32.275', 'grad_norm': '54.5', 'counters/examples': 219648, 'counters/updates': 6864}
train stats after 219680 examples: {'rewards_train/chosen': '0.0023457', 'rewards_train/rejected': '-0.005062', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074076', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-156.25', 'loss/train': '0.69788', 'examples_per_second': '31.669', 'grad_norm': '102', 'counters/examples': 219680, 'counters/updates': 6865}
train stats after 219712 examples: {'rewards_train/chosen': '0.17451', 'rewards_train/rejected': '0.029085', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.14542', 'logps_train/rejected': '-142.93', 'logps_train/chosen': '-171.94', 'loss/train': '0.64531', 'examples_per_second': '30.862', 'grad_norm': '117.5', 'counters/examples': 219712, 'counters/updates': 6866}
train stats after 219744 examples: {'rewards_train/chosen': '0.10683', 'rewards_train/rejected': '0.076161', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03067', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-142.31', 'loss/train': '0.68909', 'examples_per_second': '31.281', 'grad_norm': '76.5', 'counters/examples': 219744, 'counters/updates': 6867}
train stats after 219776 examples: {'rewards_train/chosen': '0.19113', 'rewards_train/rejected': '0.11907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072056', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-153.62', 'loss/train': '0.68412', 'examples_per_second': '31.78', 'grad_norm': '80.5', 'counters/examples': 219776, 'counters/updates': 6868}
train stats after 219808 examples: {'rewards_train/chosen': '0.05845', 'rewards_train/rejected': '0.022884', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035566', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-154.8', 'loss/train': '0.69146', 'examples_per_second': '32.1', 'grad_norm': '85', 'counters/examples': 219808, 'counters/updates': 6869}
train stats after 219840 examples: {'rewards_train/chosen': '0.093014', 'rewards_train/rejected': '0.039985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053029', 'logps_train/rejected': '-119.76', 'logps_train/chosen': '-132.05', 'loss/train': '0.67615', 'examples_per_second': '31.673', 'grad_norm': '73', 'counters/examples': 219840, 'counters/updates': 6870}
train stats after 219872 examples: {'rewards_train/chosen': '0.15037', 'rewards_train/rejected': '0.041049', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10932', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-157.61', 'loss/train': '0.65609', 'examples_per_second': '31.639', 'grad_norm': '72', 'counters/examples': 219872, 'counters/updates': 6871}
skipping logging after 219904 examples to avoid logging too frequently
train stats after 219936 examples: {'rewards_train/chosen': '0.072604', 'rewards_train/rejected': '0.043234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.029369', 'logps_train/rejected': '-165.58', 'logps_train/chosen': '-164.2', 'loss/train': '0.69453', 'examples_per_second': '30.108', 'grad_norm': '61.75', 'counters/examples': 219936, 'counters/updates': 6873}
skipping logging after 219968 examples to avoid logging too frequently
train stats after 220000 examples: {'rewards_train/chosen': '0.21522', 'rewards_train/rejected': '-0.029506', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24473', 'logps_train/rejected': '-157.69', 'logps_train/chosen': '-179.73', 'loss/train': '0.61942', 'examples_per_second': '30.39', 'grad_norm': '110', 'counters/examples': 220000, 'counters/updates': 6875}
train stats after 220032 examples: {'rewards_train/chosen': '0.14085', 'rewards_train/rejected': '0.034524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10632', 'logps_train/rejected': '-149.22', 'logps_train/chosen': '-158.24', 'loss/train': '0.65475', 'examples_per_second': '32.734', 'grad_norm': '108', 'counters/examples': 220032, 'counters/updates': 6876}
train stats after 220064 examples: {'rewards_train/chosen': '0.082652', 'rewards_train/rejected': '0.029424', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053229', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-103.52', 'loss/train': '0.68113', 'examples_per_second': '31.682', 'grad_norm': '75.5', 'counters/examples': 220064, 'counters/updates': 6877}
train stats after 220096 examples: {'rewards_train/chosen': '0.12804', 'rewards_train/rejected': '0.039485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088556', 'logps_train/rejected': '-102.82', 'logps_train/chosen': '-114.02', 'loss/train': '0.66173', 'examples_per_second': '31.094', 'grad_norm': '188', 'counters/examples': 220096, 'counters/updates': 6878}
train stats after 220128 examples: {'rewards_train/chosen': '0.07', 'rewards_train/rejected': '0.06724', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0027601', 'logps_train/rejected': '-146.67', 'logps_train/chosen': '-146.22', 'loss/train': '0.70569', 'examples_per_second': '31.661', 'grad_norm': '102.5', 'counters/examples': 220128, 'counters/updates': 6879}
train stats after 220160 examples: {'rewards_train/chosen': '0.095205', 'rewards_train/rejected': '0.042026', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053179', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-141.42', 'loss/train': '0.67675', 'examples_per_second': '30.888', 'grad_norm': '95', 'counters/examples': 220160, 'counters/updates': 6880}
skipping logging after 220192 examples to avoid logging too frequently
train stats after 220224 examples: {'rewards_train/chosen': '0.10968', 'rewards_train/rejected': '0.018401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091276', 'logps_train/rejected': '-145.01', 'logps_train/chosen': '-145.06', 'loss/train': '0.66835', 'examples_per_second': '34.162', 'grad_norm': '716', 'counters/examples': 220224, 'counters/updates': 6882}
skipping logging after 220256 examples to avoid logging too frequently
train stats after 220288 examples: {'rewards_train/chosen': '0.026515', 'rewards_train/rejected': '0.046481', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019966', 'logps_train/rejected': '-148.4', 'logps_train/chosen': '-153.16', 'loss/train': '0.714', 'examples_per_second': '31.159', 'grad_norm': '77.5', 'counters/examples': 220288, 'counters/updates': 6884}
skipping logging after 220320 examples to avoid logging too frequently
train stats after 220352 examples: {'rewards_train/chosen': '0.062995', 'rewards_train/rejected': '0.046567', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016429', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-140.12', 'loss/train': '0.69703', 'examples_per_second': '31.629', 'grad_norm': '71', 'counters/examples': 220352, 'counters/updates': 6886}
skipping logging after 220384 examples to avoid logging too frequently
train stats after 220416 examples: {'rewards_train/chosen': '0.15376', 'rewards_train/rejected': '0.016492', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13727', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-159.77', 'loss/train': '0.63991', 'examples_per_second': '30.076', 'grad_norm': '165', 'counters/examples': 220416, 'counters/updates': 6888}
train stats after 220448 examples: {'rewards_train/chosen': '0.12931', 'rewards_train/rejected': '0.052224', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077083', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-147.76', 'loss/train': '0.6756', 'examples_per_second': '31.613', 'grad_norm': '70.5', 'counters/examples': 220448, 'counters/updates': 6889}
train stats after 220480 examples: {'rewards_train/chosen': '0.18806', 'rewards_train/rejected': '0.083476', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10458', 'logps_train/rejected': '-145.8', 'logps_train/chosen': '-163.58', 'loss/train': '0.65575', 'examples_per_second': '31.641', 'grad_norm': '74.5', 'counters/examples': 220480, 'counters/updates': 6890}
train stats after 220512 examples: {'rewards_train/chosen': '0.010738', 'rewards_train/rejected': '-0.0026255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013364', 'logps_train/rejected': '-120.65', 'logps_train/chosen': '-112.67', 'loss/train': '0.71126', 'examples_per_second': '30.708', 'grad_norm': '71', 'counters/examples': 220512, 'counters/updates': 6891}
train stats after 220544 examples: {'rewards_train/chosen': '0.1589', 'rewards_train/rejected': '0.017823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14107', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-164.11', 'loss/train': '0.63981', 'examples_per_second': '32.041', 'grad_norm': '115', 'counters/examples': 220544, 'counters/updates': 6892}
train stats after 220576 examples: {'rewards_train/chosen': '0.070815', 'rewards_train/rejected': '0.027037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043778', 'logps_train/rejected': '-175.49', 'logps_train/chosen': '-182.9', 'loss/train': '0.68898', 'examples_per_second': '31.132', 'grad_norm': '72', 'counters/examples': 220576, 'counters/updates': 6893}
train stats after 220608 examples: {'rewards_train/chosen': '0.17399', 'rewards_train/rejected': '0.025615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14837', 'logps_train/rejected': '-112.22', 'logps_train/chosen': '-134.06', 'loss/train': '0.63845', 'examples_per_second': '31.667', 'grad_norm': '77.5', 'counters/examples': 220608, 'counters/updates': 6894}
train stats after 220640 examples: {'rewards_train/chosen': '0.012247', 'rewards_train/rejected': '0.056314', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044066', 'logps_train/rejected': '-151.27', 'logps_train/chosen': '-138.81', 'loss/train': '0.72531', 'examples_per_second': '32.139', 'grad_norm': '93.5', 'counters/examples': 220640, 'counters/updates': 6895}
train stats after 220672 examples: {'rewards_train/chosen': '0.17714', 'rewards_train/rejected': '0.030998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14614', 'logps_train/rejected': '-198.92', 'logps_train/chosen': '-207.58', 'loss/train': '0.64034', 'examples_per_second': '32.483', 'grad_norm': '89.5', 'counters/examples': 220672, 'counters/updates': 6896}
train stats after 220704 examples: {'rewards_train/chosen': '0.081277', 'rewards_train/rejected': '0.0059501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075327', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-143.01', 'loss/train': '0.67153', 'examples_per_second': '30.809', 'grad_norm': '65', 'counters/examples': 220704, 'counters/updates': 6897}
train stats after 220736 examples: {'rewards_train/chosen': '0.091457', 'rewards_train/rejected': '0.09087', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00058649', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-163.31', 'loss/train': '0.70439', 'examples_per_second': '30.174', 'grad_norm': '89', 'counters/examples': 220736, 'counters/updates': 6898}
skipping logging after 220768 examples to avoid logging too frequently
train stats after 220800 examples: {'rewards_train/chosen': '0.10127', 'rewards_train/rejected': '0.0083896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092879', 'logps_train/rejected': '-104.14', 'logps_train/chosen': '-157.65', 'loss/train': '0.66092', 'examples_per_second': '34.499', 'grad_norm': '73.5', 'counters/examples': 220800, 'counters/updates': 6900}
train stats after 220832 examples: {'rewards_train/chosen': '0.080094', 'rewards_train/rejected': '0.057006', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023088', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-139.19', 'loss/train': '0.69598', 'examples_per_second': '32.634', 'grad_norm': '72.5', 'counters/examples': 220832, 'counters/updates': 6901}
train stats after 220864 examples: {'rewards_train/chosen': '0.077783', 'rewards_train/rejected': '0.01397', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063812', 'logps_train/rejected': '-119.2', 'logps_train/chosen': '-134.28', 'loss/train': '0.67394', 'examples_per_second': '31.467', 'grad_norm': '55', 'counters/examples': 220864, 'counters/updates': 6902}
skipping logging after 220896 examples to avoid logging too frequently
train stats after 220928 examples: {'rewards_train/chosen': '0.084953', 'rewards_train/rejected': '0.10819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023236', 'logps_train/rejected': '-149.28', 'logps_train/chosen': '-105.29', 'loss/train': '0.71372', 'examples_per_second': '31.294', 'grad_norm': '248', 'counters/examples': 220928, 'counters/updates': 6904}
train stats after 220960 examples: {'rewards_train/chosen': '0.078383', 'rewards_train/rejected': '0.010507', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067876', 'logps_train/rejected': '-115.69', 'logps_train/chosen': '-140.81', 'loss/train': '0.67141', 'examples_per_second': '32.881', 'grad_norm': '83.5', 'counters/examples': 220960, 'counters/updates': 6905}
train stats after 220992 examples: {'rewards_train/chosen': '0.095581', 'rewards_train/rejected': '0.1208', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025217', 'logps_train/rejected': '-163.91', 'logps_train/chosen': '-157.08', 'loss/train': '0.71362', 'examples_per_second': '31.547', 'grad_norm': '85', 'counters/examples': 220992, 'counters/updates': 6906}
train stats after 221024 examples: {'rewards_train/chosen': '0.026553', 'rewards_train/rejected': '-0.024213', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050767', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-158.22', 'loss/train': '0.6777', 'examples_per_second': '31.642', 'grad_norm': '86.5', 'counters/examples': 221024, 'counters/updates': 6907}
train stats after 221056 examples: {'rewards_train/chosen': '0.024833', 'rewards_train/rejected': '-0.024563', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049396', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-146.31', 'loss/train': '0.6926', 'examples_per_second': '31.679', 'grad_norm': '105.5', 'counters/examples': 221056, 'counters/updates': 6908}
train stats after 221088 examples: {'rewards_train/chosen': '0.1232', 'rewards_train/rejected': '-0.046938', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17014', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-151.01', 'loss/train': '0.62664', 'examples_per_second': '32.454', 'grad_norm': '69.5', 'counters/examples': 221088, 'counters/updates': 6909}
train stats after 221120 examples: {'rewards_train/chosen': '0.14742', 'rewards_train/rejected': '0.066473', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080944', 'logps_train/rejected': '-139.55', 'logps_train/chosen': '-145.77', 'loss/train': '0.66504', 'examples_per_second': '31.681', 'grad_norm': '83.5', 'counters/examples': 221120, 'counters/updates': 6910}
train stats after 221152 examples: {'rewards_train/chosen': '0.16568', 'rewards_train/rejected': '0.06563', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10005', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-100.41', 'loss/train': '0.65536', 'examples_per_second': '30.837', 'grad_norm': '168', 'counters/examples': 221152, 'counters/updates': 6911}
train stats after 221184 examples: {'rewards_train/chosen': '-0.014592', 'rewards_train/rejected': '0.015457', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030048', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-158.13', 'loss/train': '0.7217', 'examples_per_second': '30.488', 'grad_norm': '65.5', 'counters/examples': 221184, 'counters/updates': 6912}
train stats after 221216 examples: {'rewards_train/chosen': '0.14119', 'rewards_train/rejected': '0.044307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096881', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-143.47', 'loss/train': '0.65632', 'examples_per_second': '32.299', 'grad_norm': '75', 'counters/examples': 221216, 'counters/updates': 6913}
train stats after 221248 examples: {'rewards_train/chosen': '0.11175', 'rewards_train/rejected': '0.099446', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012299', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-142.93', 'loss/train': '0.69929', 'examples_per_second': '31.635', 'grad_norm': '106.5', 'counters/examples': 221248, 'counters/updates': 6914}
skipping logging after 221280 examples to avoid logging too frequently
train stats after 221312 examples: {'rewards_train/chosen': '0.076691', 'rewards_train/rejected': '-0.05634', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13303', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-144', 'loss/train': '0.63652', 'examples_per_second': '35.482', 'grad_norm': '116.5', 'counters/examples': 221312, 'counters/updates': 6916}
train stats after 221344 examples: {'rewards_train/chosen': '0.16328', 'rewards_train/rejected': '0.14457', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018709', 'logps_train/rejected': '-141.12', 'logps_train/chosen': '-123.52', 'loss/train': '0.70078', 'examples_per_second': '31.709', 'grad_norm': '126.5', 'counters/examples': 221344, 'counters/updates': 6917}
skipping logging after 221376 examples to avoid logging too frequently
train stats after 221408 examples: {'rewards_train/chosen': '0.10371', 'rewards_train/rejected': '0.010206', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093508', 'logps_train/rejected': '-103.45', 'logps_train/chosen': '-146.35', 'loss/train': '0.65709', 'examples_per_second': '33.6', 'grad_norm': '68', 'counters/examples': 221408, 'counters/updates': 6919}
skipping logging after 221440 examples to avoid logging too frequently
train stats after 221472 examples: {'rewards_train/chosen': '0.12817', 'rewards_train/rejected': '0.04649', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081683', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-140.78', 'loss/train': '0.66107', 'examples_per_second': '30.91', 'grad_norm': '92', 'counters/examples': 221472, 'counters/updates': 6921}
train stats after 221504 examples: {'rewards_train/chosen': '0.10539', 'rewards_train/rejected': '0.027249', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078137', 'logps_train/rejected': '-109.67', 'logps_train/chosen': '-161.83', 'loss/train': '0.6729', 'examples_per_second': '32.786', 'grad_norm': '91', 'counters/examples': 221504, 'counters/updates': 6922}
train stats after 221536 examples: {'rewards_train/chosen': '0.07086', 'rewards_train/rejected': '0.14921', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.07835', 'logps_train/rejected': '-169.27', 'logps_train/chosen': '-166.61', 'loss/train': '0.75336', 'examples_per_second': '29.779', 'grad_norm': '133', 'counters/examples': 221536, 'counters/updates': 6923}
train stats after 221568 examples: {'rewards_train/chosen': '0.16883', 'rewards_train/rejected': '0.023989', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14484', 'logps_train/rejected': '-142.59', 'logps_train/chosen': '-166.85', 'loss/train': '0.65046', 'examples_per_second': '33.29', 'grad_norm': '116', 'counters/examples': 221568, 'counters/updates': 6924}
skipping logging after 221600 examples to avoid logging too frequently
train stats after 221632 examples: {'rewards_train/chosen': '0.15932', 'rewards_train/rejected': '0.093476', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.065845', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-187.96', 'loss/train': '0.67584', 'examples_per_second': '30.674', 'grad_norm': '117', 'counters/examples': 221632, 'counters/updates': 6926}
train stats after 221664 examples: {'rewards_train/chosen': '0.1452', 'rewards_train/rejected': '0.10133', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043873', 'logps_train/rejected': '-136.31', 'logps_train/chosen': '-152.23', 'loss/train': '0.68598', 'examples_per_second': '33.151', 'grad_norm': '108', 'counters/examples': 221664, 'counters/updates': 6927}
train stats after 221696 examples: {'rewards_train/chosen': '0.16362', 'rewards_train/rejected': '0.13718', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026436', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-119.31', 'loss/train': '0.69219', 'examples_per_second': '31.449', 'grad_norm': '63.75', 'counters/examples': 221696, 'counters/updates': 6928}
train stats after 221728 examples: {'rewards_train/chosen': '0.1848', 'rewards_train/rejected': '0.022335', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16246', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-150.09', 'loss/train': '0.62822', 'examples_per_second': '32.512', 'grad_norm': '52.5', 'counters/examples': 221728, 'counters/updates': 6929}
train stats after 221760 examples: {'rewards_train/chosen': '0.092233', 'rewards_train/rejected': '-0.014317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10655', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-127.54', 'loss/train': '0.65437', 'examples_per_second': '31.626', 'grad_norm': '64.5', 'counters/examples': 221760, 'counters/updates': 6930}
train stats after 221792 examples: {'rewards_train/chosen': '0.12383', 'rewards_train/rejected': '0.036522', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087304', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-140.12', 'loss/train': '0.66333', 'examples_per_second': '29.82', 'grad_norm': '55.5', 'counters/examples': 221792, 'counters/updates': 6931}
train stats after 221824 examples: {'rewards_train/chosen': '0.19912', 'rewards_train/rejected': '0.037814', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1613', 'logps_train/rejected': '-165.43', 'logps_train/chosen': '-167.32', 'loss/train': '0.63081', 'examples_per_second': '31.671', 'grad_norm': '77', 'counters/examples': 221824, 'counters/updates': 6932}
skipping logging after 221856 examples to avoid logging too frequently
train stats after 221888 examples: {'rewards_train/chosen': '0.10207', 'rewards_train/rejected': '0.017072', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085', 'logps_train/rejected': '-132.54', 'logps_train/chosen': '-147.38', 'loss/train': '0.66572', 'examples_per_second': '31.178', 'grad_norm': '195', 'counters/examples': 221888, 'counters/updates': 6934}
skipping logging after 221920 examples to avoid logging too frequently
train stats after 221952 examples: {'rewards_train/chosen': '0.090983', 'rewards_train/rejected': '0.1035', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01252', 'logps_train/rejected': '-150.31', 'logps_train/chosen': '-182.46', 'loss/train': '0.7326', 'examples_per_second': '30.693', 'grad_norm': '135', 'counters/examples': 221952, 'counters/updates': 6936}
train stats after 221984 examples: {'rewards_train/chosen': '0.050172', 'rewards_train/rejected': '0.029833', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020339', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-124.15', 'loss/train': '0.69484', 'examples_per_second': '31.923', 'grad_norm': '70', 'counters/examples': 221984, 'counters/updates': 6937}
train stats after 222016 examples: {'rewards_train/chosen': '0.077843', 'rewards_train/rejected': '0.063793', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.01405', 'logps_train/rejected': '-128.42', 'logps_train/chosen': '-130.22', 'loss/train': '0.69805', 'examples_per_second': '30.999', 'grad_norm': '76', 'counters/examples': 222016, 'counters/updates': 6938}
train stats after 222048 examples: {'rewards_train/chosen': '0.088864', 'rewards_train/rejected': '0.060623', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.028241', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-125.74', 'loss/train': '0.69061', 'examples_per_second': '32.995', 'grad_norm': '73', 'counters/examples': 222048, 'counters/updates': 6939}
train stats after 222080 examples: {'rewards_train/chosen': '0.24446', 'rewards_train/rejected': '-0.024458', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26892', 'logps_train/rejected': '-134.97', 'logps_train/chosen': '-199.73', 'loss/train': '0.59664', 'examples_per_second': '32.422', 'grad_norm': '108', 'counters/examples': 222080, 'counters/updates': 6940}
train stats after 222112 examples: {'rewards_train/chosen': '0.13128', 'rewards_train/rejected': '0.10987', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.021405', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-150.21', 'loss/train': '0.70496', 'examples_per_second': '31.568', 'grad_norm': '63', 'counters/examples': 222112, 'counters/updates': 6941}
train stats after 222144 examples: {'rewards_train/chosen': '0.027859', 'rewards_train/rejected': '0.026724', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0011346', 'logps_train/rejected': '-150.31', 'logps_train/chosen': '-209.58', 'loss/train': '0.71508', 'examples_per_second': '31.621', 'grad_norm': '88', 'counters/examples': 222144, 'counters/updates': 6942}
skipping logging after 222176 examples to avoid logging too frequently
train stats after 222208 examples: {'rewards_train/chosen': '0.043276', 'rewards_train/rejected': '0.02946', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013816', 'logps_train/rejected': '-142.64', 'logps_train/chosen': '-163.2', 'loss/train': '0.70273', 'examples_per_second': '30.651', 'grad_norm': '43', 'counters/examples': 222208, 'counters/updates': 6944}
train stats after 222240 examples: {'rewards_train/chosen': '0.13153', 'rewards_train/rejected': '0.10185', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029686', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-141.96', 'loss/train': '0.69165', 'examples_per_second': '32.555', 'grad_norm': '70', 'counters/examples': 222240, 'counters/updates': 6945}
train stats after 222272 examples: {'rewards_train/chosen': '0.11987', 'rewards_train/rejected': '0.11909', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00077549', 'logps_train/rejected': '-123.97', 'logps_train/chosen': '-136.99', 'loss/train': '0.71327', 'examples_per_second': '30.813', 'grad_norm': '71.5', 'counters/examples': 222272, 'counters/updates': 6946}
train stats after 222304 examples: {'rewards_train/chosen': '0.12141', 'rewards_train/rejected': '-0.0030573', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12447', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-127.12', 'loss/train': '0.6472', 'examples_per_second': '30.161', 'grad_norm': '127.5', 'counters/examples': 222304, 'counters/updates': 6947}
train stats after 222336 examples: {'rewards_train/chosen': '0.12471', 'rewards_train/rejected': '0.066175', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05854', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-151.68', 'loss/train': '0.67392', 'examples_per_second': '31.41', 'grad_norm': '71.5', 'counters/examples': 222336, 'counters/updates': 6948}
train stats after 222368 examples: {'rewards_train/chosen': '0.17159', 'rewards_train/rejected': '0.028031', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14356', 'logps_train/rejected': '-112.68', 'logps_train/chosen': '-128.93', 'loss/train': '0.63249', 'examples_per_second': '31.671', 'grad_norm': '49', 'counters/examples': 222368, 'counters/updates': 6949}
train stats after 222400 examples: {'rewards_train/chosen': '0.21403', 'rewards_train/rejected': '0.07228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14175', 'logps_train/rejected': '-92.905', 'logps_train/chosen': '-174.93', 'loss/train': '0.64836', 'examples_per_second': '31.684', 'grad_norm': '73', 'counters/examples': 222400, 'counters/updates': 6950}
train stats after 222432 examples: {'rewards_train/chosen': '0.10688', 'rewards_train/rejected': '0.12017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013293', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-131.33', 'loss/train': '0.71863', 'examples_per_second': '30.352', 'grad_norm': '145', 'counters/examples': 222432, 'counters/updates': 6951}
train stats after 222464 examples: {'rewards_train/chosen': '0.26742', 'rewards_train/rejected': '0.15859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10883', 'logps_train/rejected': '-155.06', 'logps_train/chosen': '-177.2', 'loss/train': '0.67524', 'examples_per_second': '31.643', 'grad_norm': '93', 'counters/examples': 222464, 'counters/updates': 6952}
train stats after 222496 examples: {'rewards_train/chosen': '0.096361', 'rewards_train/rejected': '0.019964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076397', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-136.98', 'loss/train': '0.67843', 'examples_per_second': '31.572', 'grad_norm': '91.5', 'counters/examples': 222496, 'counters/updates': 6953}
train stats after 222528 examples: {'rewards_train/chosen': '0.12754', 'rewards_train/rejected': '0.015264', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11228', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-180.5', 'loss/train': '0.66245', 'examples_per_second': '32.727', 'grad_norm': '112.5', 'counters/examples': 222528, 'counters/updates': 6954}
train stats after 222560 examples: {'rewards_train/chosen': '0.098529', 'rewards_train/rejected': '0.027189', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07134', 'logps_train/rejected': '-115.08', 'logps_train/chosen': '-145.94', 'loss/train': '0.67804', 'examples_per_second': '32.496', 'grad_norm': '73', 'counters/examples': 222560, 'counters/updates': 6955}
train stats after 222592 examples: {'rewards_train/chosen': '0.084314', 'rewards_train/rejected': '0.029772', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054542', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-155.59', 'loss/train': '0.67604', 'examples_per_second': '30.178', 'grad_norm': '91.5', 'counters/examples': 222592, 'counters/updates': 6956}
train stats after 222624 examples: {'rewards_train/chosen': '0.095592', 'rewards_train/rejected': '0.10725', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011654', 'logps_train/rejected': '-189.65', 'logps_train/chosen': '-163.96', 'loss/train': '0.72617', 'examples_per_second': '31.436', 'grad_norm': '119', 'counters/examples': 222624, 'counters/updates': 6957}
skipping logging after 222656 examples to avoid logging too frequently
train stats after 222688 examples: {'rewards_train/chosen': '0.087018', 'rewards_train/rejected': '0.067742', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019276', 'logps_train/rejected': '-111.13', 'logps_train/chosen': '-171.32', 'loss/train': '0.69847', 'examples_per_second': '30.969', 'grad_norm': '92', 'counters/examples': 222688, 'counters/updates': 6959}
skipping logging after 222720 examples to avoid logging too frequently
train stats after 222752 examples: {'rewards_train/chosen': '0.084804', 'rewards_train/rejected': '0.061656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023148', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-142.45', 'loss/train': '0.69313', 'examples_per_second': '31.357', 'grad_norm': '80', 'counters/examples': 222752, 'counters/updates': 6961}
train stats after 222784 examples: {'rewards_train/chosen': '0.12398', 'rewards_train/rejected': '0.051777', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072204', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-123.72', 'loss/train': '0.6738', 'examples_per_second': '32.062', 'grad_norm': '59.75', 'counters/examples': 222784, 'counters/updates': 6962}
train stats after 222816 examples: {'rewards_train/chosen': '0.13806', 'rewards_train/rejected': '0.0328', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10526', 'logps_train/rejected': '-118.68', 'logps_train/chosen': '-164.9', 'loss/train': '0.65381', 'examples_per_second': '32.027', 'grad_norm': '59', 'counters/examples': 222816, 'counters/updates': 6963}
train stats after 222848 examples: {'rewards_train/chosen': '0.091962', 'rewards_train/rejected': '0.070335', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021627', 'logps_train/rejected': '-148.28', 'logps_train/chosen': '-153.45', 'loss/train': '0.69748', 'examples_per_second': '31.6', 'grad_norm': '86', 'counters/examples': 222848, 'counters/updates': 6964}
train stats after 222880 examples: {'rewards_train/chosen': '0.1131', 'rewards_train/rejected': '0.053913', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059189', 'logps_train/rejected': '-131.54', 'logps_train/chosen': '-203.89', 'loss/train': '0.69279', 'examples_per_second': '32.199', 'grad_norm': '132', 'counters/examples': 222880, 'counters/updates': 6965}
train stats after 222912 examples: {'rewards_train/chosen': '0.21417', 'rewards_train/rejected': '0.073807', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14036', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-176.09', 'loss/train': '0.6485', 'examples_per_second': '32.517', 'grad_norm': '70.5', 'counters/examples': 222912, 'counters/updates': 6966}
train stats after 222944 examples: {'rewards_train/chosen': '0.092055', 'rewards_train/rejected': '0.047386', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044669', 'logps_train/rejected': '-142.79', 'logps_train/chosen': '-155.33', 'loss/train': '0.68481', 'examples_per_second': '31.191', 'grad_norm': '88', 'counters/examples': 222944, 'counters/updates': 6967}
train stats after 222976 examples: {'rewards_train/chosen': '0.13892', 'rewards_train/rejected': '0.13885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '7.616e-05', 'logps_train/rejected': '-155.65', 'logps_train/chosen': '-147.25', 'loss/train': '0.70766', 'examples_per_second': '30.64', 'grad_norm': '139', 'counters/examples': 222976, 'counters/updates': 6968}
train stats after 223008 examples: {'rewards_train/chosen': '0.059018', 'rewards_train/rejected': '-0.015571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074589', 'logps_train/rejected': '-165.55', 'logps_train/chosen': '-134.12', 'loss/train': '0.66839', 'examples_per_second': '33.129', 'grad_norm': '81.5', 'counters/examples': 223008, 'counters/updates': 6969}
train stats after 223040 examples: {'rewards_train/chosen': '0.0767', 'rewards_train/rejected': '0.075257', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0014433', 'logps_train/rejected': '-154.62', 'logps_train/chosen': '-137', 'loss/train': '0.71285', 'examples_per_second': '31.402', 'grad_norm': '99.5', 'counters/examples': 223040, 'counters/updates': 6970}
train stats after 223072 examples: {'rewards_train/chosen': '0.16784', 'rewards_train/rejected': '0.090292', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077548', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-161.58', 'loss/train': '0.66628', 'examples_per_second': '32.412', 'grad_norm': '78.5', 'counters/examples': 223072, 'counters/updates': 6971}
skipping logging after 223104 examples to avoid logging too frequently
train stats after 223136 examples: {'rewards_train/chosen': '0.093704', 'rewards_train/rejected': '0.06003', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033674', 'logps_train/rejected': '-132', 'logps_train/chosen': '-150.84', 'loss/train': '0.70201', 'examples_per_second': '31.608', 'grad_norm': '213', 'counters/examples': 223136, 'counters/updates': 6973}
train stats after 223168 examples: {'rewards_train/chosen': '0.1084', 'rewards_train/rejected': '0.06887', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039532', 'logps_train/rejected': '-110.62', 'logps_train/chosen': '-158.04', 'loss/train': '0.68513', 'examples_per_second': '31.652', 'grad_norm': '111.5', 'counters/examples': 223168, 'counters/updates': 6974}
train stats after 223200 examples: {'rewards_train/chosen': '0.16933', 'rewards_train/rejected': '0.070141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099186', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-150.5', 'loss/train': '0.66105', 'examples_per_second': '22.336', 'grad_norm': '144', 'counters/examples': 223200, 'counters/updates': 6975}
train stats after 223232 examples: {'rewards_train/chosen': '0.084022', 'rewards_train/rejected': '-0.013194', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097217', 'logps_train/rejected': '-90.613', 'logps_train/chosen': '-129.04', 'loss/train': '0.65529', 'examples_per_second': '31.236', 'grad_norm': '90.5', 'counters/examples': 223232, 'counters/updates': 6976}
train stats after 223264 examples: {'rewards_train/chosen': '0.14035', 'rewards_train/rejected': '-0.0080196', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14837', 'logps_train/rejected': '-139.18', 'logps_train/chosen': '-125.8', 'loss/train': '0.62677', 'examples_per_second': '32.114', 'grad_norm': '97.5', 'counters/examples': 223264, 'counters/updates': 6977}
skipping logging after 223296 examples to avoid logging too frequently
train stats after 223328 examples: {'rewards_train/chosen': '0.1146', 'rewards_train/rejected': '0.04317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071426', 'logps_train/rejected': '-142.23', 'logps_train/chosen': '-157.88', 'loss/train': '0.68237', 'examples_per_second': '30.194', 'grad_norm': '239', 'counters/examples': 223328, 'counters/updates': 6979}
train stats after 223360 examples: {'rewards_train/chosen': '0.11629', 'rewards_train/rejected': '-0.0096635', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12595', 'logps_train/rejected': '-150.34', 'logps_train/chosen': '-167.39', 'loss/train': '0.65232', 'examples_per_second': '32.066', 'grad_norm': '71', 'counters/examples': 223360, 'counters/updates': 6980}
skipping logging after 223392 examples to avoid logging too frequently
train stats after 223424 examples: {'rewards_train/chosen': '0.19556', 'rewards_train/rejected': '0.018501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17706', 'logps_train/rejected': '-155.96', 'logps_train/chosen': '-166.21', 'loss/train': '0.63024', 'examples_per_second': '29.907', 'grad_norm': '77.5', 'counters/examples': 223424, 'counters/updates': 6982}
train stats after 223456 examples: {'rewards_train/chosen': '0.17638', 'rewards_train/rejected': '0.048862', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12752', 'logps_train/rejected': '-151', 'logps_train/chosen': '-152.29', 'loss/train': '0.65468', 'examples_per_second': '30.413', 'grad_norm': '260', 'counters/examples': 223456, 'counters/updates': 6983}
train stats after 223488 examples: {'rewards_train/chosen': '0.087676', 'rewards_train/rejected': '0.038475', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049201', 'logps_train/rejected': '-115.55', 'logps_train/chosen': '-140.43', 'loss/train': '0.6849', 'examples_per_second': '31.167', 'grad_norm': '76', 'counters/examples': 223488, 'counters/updates': 6984}
train stats after 223520 examples: {'rewards_train/chosen': '0.097476', 'rewards_train/rejected': '0.017002', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080474', 'logps_train/rejected': '-98.636', 'logps_train/chosen': '-157.38', 'loss/train': '0.66544', 'examples_per_second': '30.207', 'grad_norm': '79', 'counters/examples': 223520, 'counters/updates': 6985}
train stats after 223552 examples: {'rewards_train/chosen': '0.20248', 'rewards_train/rejected': '0.015712', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18676', 'logps_train/rejected': '-127.28', 'logps_train/chosen': '-166.52', 'loss/train': '0.62673', 'examples_per_second': '30.808', 'grad_norm': '85', 'counters/examples': 223552, 'counters/updates': 6986}
train stats after 223584 examples: {'rewards_train/chosen': '0.15192', 'rewards_train/rejected': '0.023936', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12798', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-198.02', 'loss/train': '0.64733', 'examples_per_second': '31.556', 'grad_norm': '88.5', 'counters/examples': 223584, 'counters/updates': 6987}
train stats after 223616 examples: {'rewards_train/chosen': '0.21251', 'rewards_train/rejected': '0.11677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095734', 'logps_train/rejected': '-153.91', 'logps_train/chosen': '-181.73', 'loss/train': '0.66304', 'examples_per_second': '31.713', 'grad_norm': '87', 'counters/examples': 223616, 'counters/updates': 6988}
skipping logging after 223648 examples to avoid logging too frequently
train stats after 223680 examples: {'rewards_train/chosen': '0.089254', 'rewards_train/rejected': '0.03747', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051784', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-167.03', 'loss/train': '0.67758', 'examples_per_second': '32.555', 'grad_norm': '57.75', 'counters/examples': 223680, 'counters/updates': 6990}
train stats after 223712 examples: {'rewards_train/chosen': '0.052349', 'rewards_train/rejected': '0.022416', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029933', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-146.62', 'loss/train': '0.68782', 'examples_per_second': '31.94', 'grad_norm': '137', 'counters/examples': 223712, 'counters/updates': 6991}
skipping logging after 223744 examples to avoid logging too frequently
train stats after 223776 examples: {'rewards_train/chosen': '0.13644', 'rewards_train/rejected': '-0.0022222', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13867', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-156.5', 'loss/train': '0.64103', 'examples_per_second': '31', 'grad_norm': '70.5', 'counters/examples': 223776, 'counters/updates': 6993}
skipping logging after 223808 examples to avoid logging too frequently
train stats after 223840 examples: {'rewards_train/chosen': '0.13784', 'rewards_train/rejected': '0.017291', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12055', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-145.08', 'loss/train': '0.64859', 'examples_per_second': '30.823', 'grad_norm': '97', 'counters/examples': 223840, 'counters/updates': 6995}
skipping logging after 223872 examples to avoid logging too frequently
train stats after 223904 examples: {'rewards_train/chosen': '0.11826', 'rewards_train/rejected': '-0.0047524', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12301', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-131.51', 'loss/train': '0.65584', 'examples_per_second': '34.114', 'grad_norm': '78.5', 'counters/examples': 223904, 'counters/updates': 6997}
train stats after 223936 examples: {'rewards_train/chosen': '0.092431', 'rewards_train/rejected': '0.024997', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067433', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-136.45', 'loss/train': '0.6713', 'examples_per_second': '32.324', 'grad_norm': '160', 'counters/examples': 223936, 'counters/updates': 6998}
train stats after 223968 examples: {'rewards_train/chosen': '0.10656', 'rewards_train/rejected': '0.14447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.037912', 'logps_train/rejected': '-155.92', 'logps_train/chosen': '-176.2', 'loss/train': '0.72895', 'examples_per_second': '31.645', 'grad_norm': '74', 'counters/examples': 223968, 'counters/updates': 6999}
train stats after 224000 examples: {'rewards_train/chosen': '0.15527', 'rewards_train/rejected': '0.0011484', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15413', 'logps_train/rejected': '-96.659', 'logps_train/chosen': '-144.7', 'loss/train': '0.62994', 'examples_per_second': '31.685', 'grad_norm': '62.75', 'counters/examples': 224000, 'counters/updates': 7000}
skipping logging after 224032 examples to avoid logging too frequently
train stats after 224064 examples: {'rewards_train/chosen': '0.0014488', 'rewards_train/rejected': '0.021666', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020217', 'logps_train/rejected': '-139.03', 'logps_train/chosen': '-192.82', 'loss/train': '0.72417', 'examples_per_second': '30.495', 'grad_norm': '105.5', 'counters/examples': 224064, 'counters/updates': 7002}
train stats after 224096 examples: {'rewards_train/chosen': '0.080483', 'rewards_train/rejected': '0.016726', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063757', 'logps_train/rejected': '-137.63', 'logps_train/chosen': '-157.67', 'loss/train': '0.67506', 'examples_per_second': '32.155', 'grad_norm': '157', 'counters/examples': 224096, 'counters/updates': 7003}
train stats after 224128 examples: {'rewards_train/chosen': '0.27886', 'rewards_train/rejected': '0.15908', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11978', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-158.91', 'loss/train': '0.6611', 'examples_per_second': '30.558', 'grad_norm': '186', 'counters/examples': 224128, 'counters/updates': 7004}
train stats after 224160 examples: {'rewards_train/chosen': '0.22034', 'rewards_train/rejected': '0.043647', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17669', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-155.15', 'loss/train': '0.63294', 'examples_per_second': '31.984', 'grad_norm': '112.5', 'counters/examples': 224160, 'counters/updates': 7005}
train stats after 224192 examples: {'rewards_train/chosen': '0.16378', 'rewards_train/rejected': '0.11974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044036', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-136.78', 'loss/train': '0.68809', 'examples_per_second': '32.479', 'grad_norm': '61.75', 'counters/examples': 224192, 'counters/updates': 7006}
train stats after 224224 examples: {'rewards_train/chosen': '0.11322', 'rewards_train/rejected': '-0.02511', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13833', 'logps_train/rejected': '-120.97', 'logps_train/chosen': '-155.31', 'loss/train': '0.6379', 'examples_per_second': '24.488', 'grad_norm': '68.5', 'counters/examples': 224224, 'counters/updates': 7007}
train stats after 224256 examples: {'rewards_train/chosen': '0.11704', 'rewards_train/rejected': '0.11364', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0033981', 'logps_train/rejected': '-150.73', 'logps_train/chosen': '-164.4', 'loss/train': '0.72199', 'examples_per_second': '32.005', 'grad_norm': '97.5', 'counters/examples': 224256, 'counters/updates': 7008}
train stats after 224288 examples: {'rewards_train/chosen': '0.10645', 'rewards_train/rejected': '0.025368', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081081', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-159.89', 'loss/train': '0.66169', 'examples_per_second': '31.655', 'grad_norm': '57.75', 'counters/examples': 224288, 'counters/updates': 7009}
train stats after 224320 examples: {'rewards_train/chosen': '0.10383', 'rewards_train/rejected': '0.10923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0054015', 'logps_train/rejected': '-138.58', 'logps_train/chosen': '-149.21', 'loss/train': '0.70924', 'examples_per_second': '24.096', 'grad_norm': '93', 'counters/examples': 224320, 'counters/updates': 7010}
train stats after 224352 examples: {'rewards_train/chosen': '0.15411', 'rewards_train/rejected': '0.073736', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080374', 'logps_train/rejected': '-143.4', 'logps_train/chosen': '-155.36', 'loss/train': '0.6661', 'examples_per_second': '31.37', 'grad_norm': '101', 'counters/examples': 224352, 'counters/updates': 7011}
train stats after 224384 examples: {'rewards_train/chosen': '0.10504', 'rewards_train/rejected': '0.053799', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051244', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-135.21', 'loss/train': '0.67834', 'examples_per_second': '31.982', 'grad_norm': '96', 'counters/examples': 224384, 'counters/updates': 7012}
train stats after 224416 examples: {'rewards_train/chosen': '0.11072', 'rewards_train/rejected': '0.072365', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038358', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-145.62', 'loss/train': '0.68128', 'examples_per_second': '30.54', 'grad_norm': '93', 'counters/examples': 224416, 'counters/updates': 7013}
train stats after 224448 examples: {'rewards_train/chosen': '0.11638', 'rewards_train/rejected': '0.094319', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022057', 'logps_train/rejected': '-144.47', 'logps_train/chosen': '-149.82', 'loss/train': '0.69788', 'examples_per_second': '32.296', 'grad_norm': '85', 'counters/examples': 224448, 'counters/updates': 7014}
train stats after 224480 examples: {'rewards_train/chosen': '0.12533', 'rewards_train/rejected': '0.015983', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10935', 'logps_train/rejected': '-156.52', 'logps_train/chosen': '-151.92', 'loss/train': '0.65664', 'examples_per_second': '32.517', 'grad_norm': '50.25', 'counters/examples': 224480, 'counters/updates': 7015}
train stats after 224512 examples: {'rewards_train/chosen': '0.16306', 'rewards_train/rejected': '-0.0059576', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16901', 'logps_train/rejected': '-163.57', 'logps_train/chosen': '-149.73', 'loss/train': '0.63669', 'examples_per_second': '31.569', 'grad_norm': '68.5', 'counters/examples': 224512, 'counters/updates': 7016}
train stats after 224544 examples: {'rewards_train/chosen': '0.12952', 'rewards_train/rejected': '0.054019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075506', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-165.83', 'loss/train': '0.66838', 'examples_per_second': '30.768', 'grad_norm': '91', 'counters/examples': 224544, 'counters/updates': 7017}
train stats after 224576 examples: {'rewards_train/chosen': '0.11103', 'rewards_train/rejected': '0.037836', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073198', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-189.1', 'loss/train': '0.66764', 'examples_per_second': '31.285', 'grad_norm': '104', 'counters/examples': 224576, 'counters/updates': 7018}
train stats after 224608 examples: {'rewards_train/chosen': '0.16743', 'rewards_train/rejected': '0.053696', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11373', 'logps_train/rejected': '-153.79', 'logps_train/chosen': '-152.05', 'loss/train': '0.65756', 'examples_per_second': '33.261', 'grad_norm': '51.5', 'counters/examples': 224608, 'counters/updates': 7019}
train stats after 224640 examples: {'rewards_train/chosen': '0.1394', 'rewards_train/rejected': '0.026256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11314', 'logps_train/rejected': '-148.62', 'logps_train/chosen': '-164.87', 'loss/train': '0.65757', 'examples_per_second': '31.398', 'grad_norm': '63', 'counters/examples': 224640, 'counters/updates': 7020}
skipping logging after 224672 examples to avoid logging too frequently
train stats after 224704 examples: {'rewards_train/chosen': '0.10144', 'rewards_train/rejected': '0.09459', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0068527', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-155.22', 'loss/train': '0.7046', 'examples_per_second': '31.661', 'grad_norm': '116.5', 'counters/examples': 224704, 'counters/updates': 7022}
train stats after 224736 examples: {'rewards_train/chosen': '0.1947', 'rewards_train/rejected': '0.087373', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10732', 'logps_train/rejected': '-169.08', 'logps_train/chosen': '-150.72', 'loss/train': '0.67237', 'examples_per_second': '31.108', 'grad_norm': '100.5', 'counters/examples': 224736, 'counters/updates': 7023}
skipping logging after 224768 examples to avoid logging too frequently
train stats after 224800 examples: {'rewards_train/chosen': '0.13934', 'rewards_train/rejected': '0.0090779', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13026', 'logps_train/rejected': '-104.68', 'logps_train/chosen': '-111.77', 'loss/train': '0.63902', 'examples_per_second': '32.752', 'grad_norm': '95.5', 'counters/examples': 224800, 'counters/updates': 7025}
train stats after 224832 examples: {'rewards_train/chosen': '0.089791', 'rewards_train/rejected': '0.03708', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052711', 'logps_train/rejected': '-144.58', 'logps_train/chosen': '-162.64', 'loss/train': '0.67552', 'examples_per_second': '31.174', 'grad_norm': '119', 'counters/examples': 224832, 'counters/updates': 7026}
train stats after 224864 examples: {'rewards_train/chosen': '0.14189', 'rewards_train/rejected': '-0.02494', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16683', 'logps_train/rejected': '-94.304', 'logps_train/chosen': '-127.4', 'loss/train': '0.62197', 'examples_per_second': '33.035', 'grad_norm': '57', 'counters/examples': 224864, 'counters/updates': 7027}
skipping logging after 224896 examples to avoid logging too frequently
train stats after 224928 examples: {'rewards_train/chosen': '0.17474', 'rewards_train/rejected': '0.06593', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10881', 'logps_train/rejected': '-107.69', 'logps_train/chosen': '-126.6', 'loss/train': '0.66099', 'examples_per_second': '31.634', 'grad_norm': '109.5', 'counters/examples': 224928, 'counters/updates': 7029}
skipping logging after 224960 examples to avoid logging too frequently
train stats after 224992 examples: {'rewards_train/chosen': '0.12145', 'rewards_train/rejected': '0.026076', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.095371', 'logps_train/rejected': '-98.375', 'logps_train/chosen': '-128.49', 'loss/train': '0.65578', 'examples_per_second': '34.234', 'grad_norm': '53.75', 'counters/examples': 224992, 'counters/updates': 7031}
skipping logging after 225024 examples to avoid logging too frequently
train stats after 225056 examples: {'rewards_train/chosen': '0.13113', 'rewards_train/rejected': '0.025212', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10592', 'logps_train/rejected': '-99.035', 'logps_train/chosen': '-148.46', 'loss/train': '0.65399', 'examples_per_second': '32.732', 'grad_norm': '111.5', 'counters/examples': 225056, 'counters/updates': 7033}
train stats after 225088 examples: {'rewards_train/chosen': '0.1468', 'rewards_train/rejected': '-0.071716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21851', 'logps_train/rejected': '-85.431', 'logps_train/chosen': '-171.32', 'loss/train': '0.60842', 'examples_per_second': '31.218', 'grad_norm': '374', 'counters/examples': 225088, 'counters/updates': 7034}
train stats after 225120 examples: {'rewards_train/chosen': '0.14913', 'rewards_train/rejected': '0.084412', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064722', 'logps_train/rejected': '-146.35', 'logps_train/chosen': '-132.72', 'loss/train': '0.66903', 'examples_per_second': '30.897', 'grad_norm': '69', 'counters/examples': 225120, 'counters/updates': 7035}
skipping logging after 225152 examples to avoid logging too frequently
train stats after 225184 examples: {'rewards_train/chosen': '0.21275', 'rewards_train/rejected': '-0.014515', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22727', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-129.38', 'loss/train': '0.60472', 'examples_per_second': '30.291', 'grad_norm': '62', 'counters/examples': 225184, 'counters/updates': 7037}
train stats after 225216 examples: {'rewards_train/chosen': '0.15476', 'rewards_train/rejected': '0.028916', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12584', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-162.25', 'loss/train': '0.64379', 'examples_per_second': '32.683', 'grad_norm': '99.5', 'counters/examples': 225216, 'counters/updates': 7038}
train stats after 225248 examples: {'rewards_train/chosen': '0.10715', 'rewards_train/rejected': '0.039846', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067304', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-155.76', 'loss/train': '0.67206', 'examples_per_second': '31.656', 'grad_norm': '60', 'counters/examples': 225248, 'counters/updates': 7039}
train stats after 225280 examples: {'rewards_train/chosen': '0.10807', 'rewards_train/rejected': '0.045106', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062961', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-127.62', 'loss/train': '0.6708', 'examples_per_second': '31.651', 'grad_norm': '63.5', 'counters/examples': 225280, 'counters/updates': 7040}
skipping logging after 225312 examples to avoid logging too frequently
train stats after 225344 examples: {'rewards_train/chosen': '-0.028482', 'rewards_train/rejected': '0.052373', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.080855', 'logps_train/rejected': '-109.92', 'logps_train/chosen': '-105.33', 'loss/train': '0.75203', 'examples_per_second': '30.372', 'grad_norm': '158', 'counters/examples': 225344, 'counters/updates': 7042}
train stats after 225376 examples: {'rewards_train/chosen': '0.056066', 'rewards_train/rejected': '0.083137', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027071', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-151.13', 'loss/train': '0.71502', 'examples_per_second': '31.23', 'grad_norm': '82', 'counters/examples': 225376, 'counters/updates': 7043}
train stats after 225408 examples: {'rewards_train/chosen': '0.1026', 'rewards_train/rejected': '0.1664', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.063795', 'logps_train/rejected': '-163.17', 'logps_train/chosen': '-155.65', 'loss/train': '0.7358', 'examples_per_second': '30.166', 'grad_norm': '73.5', 'counters/examples': 225408, 'counters/updates': 7044}
train stats after 225440 examples: {'rewards_train/chosen': '0.137', 'rewards_train/rejected': '-0.0089541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14595', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-165.08', 'loss/train': '0.65085', 'examples_per_second': '30.046', 'grad_norm': '131', 'counters/examples': 225440, 'counters/updates': 7045}
train stats after 225472 examples: {'rewards_train/chosen': '0.0078881', 'rewards_train/rejected': '0.048336', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040448', 'logps_train/rejected': '-101.91', 'logps_train/chosen': '-118.2', 'loss/train': '0.72848', 'examples_per_second': '31.401', 'grad_norm': '76.5', 'counters/examples': 225472, 'counters/updates': 7046}
train stats after 225504 examples: {'rewards_train/chosen': '0.12866', 'rewards_train/rejected': '0.13888', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010214', 'logps_train/rejected': '-181.28', 'logps_train/chosen': '-178.85', 'loss/train': '0.71532', 'examples_per_second': '30.959', 'grad_norm': '84', 'counters/examples': 225504, 'counters/updates': 7047}
skipping logging after 225536 examples to avoid logging too frequently
train stats after 225568 examples: {'rewards_train/chosen': '0.032304', 'rewards_train/rejected': '-0.0031775', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035482', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-121.26', 'loss/train': '0.68726', 'examples_per_second': '30.979', 'grad_norm': '58.25', 'counters/examples': 225568, 'counters/updates': 7049}
train stats after 225600 examples: {'rewards_train/chosen': '0.13496', 'rewards_train/rejected': '0.1603', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025338', 'logps_train/rejected': '-160.72', 'logps_train/chosen': '-157.33', 'loss/train': '0.72297', 'examples_per_second': '31.483', 'grad_norm': '80.5', 'counters/examples': 225600, 'counters/updates': 7050}
train stats after 225632 examples: {'rewards_train/chosen': '0.074604', 'rewards_train/rejected': '-0.01889', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093494', 'logps_train/rejected': '-111.31', 'logps_train/chosen': '-132.34', 'loss/train': '0.66185', 'examples_per_second': '31.296', 'grad_norm': '100.5', 'counters/examples': 225632, 'counters/updates': 7051}
train stats after 225664 examples: {'rewards_train/chosen': '0.034428', 'rewards_train/rejected': '0.0061428', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028285', 'logps_train/rejected': '-124.47', 'logps_train/chosen': '-140.05', 'loss/train': '0.68844', 'examples_per_second': '31.319', 'grad_norm': '54', 'counters/examples': 225664, 'counters/updates': 7052}
train stats after 225696 examples: {'rewards_train/chosen': '0.14274', 'rewards_train/rejected': '0.095095', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047648', 'logps_train/rejected': '-163.24', 'logps_train/chosen': '-127.75', 'loss/train': '0.67948', 'examples_per_second': '30.941', 'grad_norm': '51.75', 'counters/examples': 225696, 'counters/updates': 7053}
train stats after 225728 examples: {'rewards_train/chosen': '0.13093', 'rewards_train/rejected': '-0.084249', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21518', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-185.07', 'loss/train': '0.60184', 'examples_per_second': '31.534', 'grad_norm': '78', 'counters/examples': 225728, 'counters/updates': 7054}
train stats after 225760 examples: {'rewards_train/chosen': '0.049358', 'rewards_train/rejected': '0.052992', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0036344', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-105.74', 'loss/train': '0.70141', 'examples_per_second': '31.995', 'grad_norm': '96', 'counters/examples': 225760, 'counters/updates': 7055}
train stats after 225792 examples: {'rewards_train/chosen': '0.083926', 'rewards_train/rejected': '0.029902', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054024', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-151.82', 'loss/train': '0.67775', 'examples_per_second': '30.508', 'grad_norm': '60.75', 'counters/examples': 225792, 'counters/updates': 7056}
train stats after 225824 examples: {'rewards_train/chosen': '0.072065', 'rewards_train/rejected': '0.066009', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0060564', 'logps_train/rejected': '-145.31', 'logps_train/chosen': '-163.2', 'loss/train': '0.70481', 'examples_per_second': '32.452', 'grad_norm': '66', 'counters/examples': 225824, 'counters/updates': 7057}
skipping logging after 225856 examples to avoid logging too frequently
train stats after 225888 examples: {'rewards_train/chosen': '0.10769', 'rewards_train/rejected': '0.063011', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.044683', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-166.68', 'loss/train': '0.68667', 'examples_per_second': '30.194', 'grad_norm': '77', 'counters/examples': 225888, 'counters/updates': 7059}
train stats after 225920 examples: {'rewards_train/chosen': '0.12869', 'rewards_train/rejected': '0.041488', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087206', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-151.6', 'loss/train': '0.66232', 'examples_per_second': '29.876', 'grad_norm': '60.5', 'counters/examples': 225920, 'counters/updates': 7060}
train stats after 225952 examples: {'rewards_train/chosen': '0.13432', 'rewards_train/rejected': '0.0146', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11972', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-107.61', 'loss/train': '0.65117', 'examples_per_second': '32.406', 'grad_norm': '75', 'counters/examples': 225952, 'counters/updates': 7061}
train stats after 225984 examples: {'rewards_train/chosen': '0.075418', 'rewards_train/rejected': '0.051091', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024326', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-156.16', 'loss/train': '0.69424', 'examples_per_second': '31.481', 'grad_norm': '66', 'counters/examples': 225984, 'counters/updates': 7062}
train stats after 226016 examples: {'rewards_train/chosen': '0.14093', 'rewards_train/rejected': '0.058091', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.08284', 'logps_train/rejected': '-164.56', 'logps_train/chosen': '-177.22', 'loss/train': '0.66748', 'examples_per_second': '31.75', 'grad_norm': '96.5', 'counters/examples': 226016, 'counters/updates': 7063}
skipping logging after 226048 examples to avoid logging too frequently
train stats after 226080 examples: {'rewards_train/chosen': '0.13846', 'rewards_train/rejected': '0.046138', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092319', 'logps_train/rejected': '-104.86', 'logps_train/chosen': '-147.42', 'loss/train': '0.66068', 'examples_per_second': '31.08', 'grad_norm': '74', 'counters/examples': 226080, 'counters/updates': 7065}
skipping logging after 226112 examples to avoid logging too frequently
train stats after 226144 examples: {'rewards_train/chosen': '0.044154', 'rewards_train/rejected': '-0.00093473', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045088', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-149.02', 'loss/train': '0.67991', 'examples_per_second': '34.025', 'grad_norm': '67.5', 'counters/examples': 226144, 'counters/updates': 7067}
train stats after 226176 examples: {'rewards_train/chosen': '0.069488', 'rewards_train/rejected': '0.073697', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0042094', 'logps_train/rejected': '-163.75', 'logps_train/chosen': '-123.78', 'loss/train': '0.70774', 'examples_per_second': '31.087', 'grad_norm': '82.5', 'counters/examples': 226176, 'counters/updates': 7068}
train stats after 226208 examples: {'rewards_train/chosen': '0.038699', 'rewards_train/rejected': '0.1084', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.069701', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-119.17', 'loss/train': '0.74869', 'examples_per_second': '32.228', 'grad_norm': '93.5', 'counters/examples': 226208, 'counters/updates': 7069}
train stats after 226240 examples: {'rewards_train/chosen': '0.19099', 'rewards_train/rejected': '0.039669', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15132', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-189.41', 'loss/train': '0.63714', 'examples_per_second': '31.105', 'grad_norm': '164', 'counters/examples': 226240, 'counters/updates': 7070}
train stats after 226272 examples: {'rewards_train/chosen': '0.07579', 'rewards_train/rejected': '0.031565', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044225', 'logps_train/rejected': '-120.1', 'logps_train/chosen': '-118.03', 'loss/train': '0.6837', 'examples_per_second': '30.011', 'grad_norm': '67.5', 'counters/examples': 226272, 'counters/updates': 7071}
train stats after 226304 examples: {'rewards_train/chosen': '0.14569', 'rewards_train/rejected': '0.1074', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038286', 'logps_train/rejected': '-153.88', 'logps_train/chosen': '-128.1', 'loss/train': '0.69469', 'examples_per_second': '32.752', 'grad_norm': '87', 'counters/examples': 226304, 'counters/updates': 7072}
train stats after 226336 examples: {'rewards_train/chosen': '0.14372', 'rewards_train/rejected': '0.052314', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.091404', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-130.27', 'loss/train': '0.67549', 'examples_per_second': '33.11', 'grad_norm': '64.5', 'counters/examples': 226336, 'counters/updates': 7073}
train stats after 226368 examples: {'rewards_train/chosen': '0.19148', 'rewards_train/rejected': '-0.010638', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20212', 'logps_train/rejected': '-144.11', 'logps_train/chosen': '-165.51', 'loss/train': '0.61931', 'examples_per_second': '31.647', 'grad_norm': '77.5', 'counters/examples': 226368, 'counters/updates': 7074}
train stats after 226400 examples: {'rewards_train/chosen': '0.1051', 'rewards_train/rejected': '0.038683', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066412', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-132.88', 'loss/train': '0.66989', 'examples_per_second': '30.015', 'grad_norm': '103', 'counters/examples': 226400, 'counters/updates': 7075}
train stats after 226432 examples: {'rewards_train/chosen': '0.10583', 'rewards_train/rejected': '0.017085', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088742', 'logps_train/rejected': '-163.14', 'logps_train/chosen': '-164.57', 'loss/train': '0.67236', 'examples_per_second': '31.414', 'grad_norm': '103', 'counters/examples': 226432, 'counters/updates': 7076}
train stats after 226464 examples: {'rewards_train/chosen': '0.084984', 'rewards_train/rejected': '0.038566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046418', 'logps_train/rejected': '-104.09', 'logps_train/chosen': '-124.01', 'loss/train': '0.68238', 'examples_per_second': '31.659', 'grad_norm': '69.5', 'counters/examples': 226464, 'counters/updates': 7077}
train stats after 226496 examples: {'rewards_train/chosen': '0.14819', 'rewards_train/rejected': '0.04497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-152.12', 'loss/train': '0.65998', 'examples_per_second': '30.776', 'grad_norm': '78', 'counters/examples': 226496, 'counters/updates': 7078}
train stats after 226528 examples: {'rewards_train/chosen': '0.12675', 'rewards_train/rejected': '0.13219', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0054428', 'logps_train/rejected': '-125.08', 'logps_train/chosen': '-148.93', 'loss/train': '0.71138', 'examples_per_second': '30.539', 'grad_norm': '136', 'counters/examples': 226528, 'counters/updates': 7079}
train stats after 226560 examples: {'rewards_train/chosen': '0.017443', 'rewards_train/rejected': '0.12286', 'rewards_train/accuracies': '0.21875', 'rewards_train/margins': '-0.10541', 'logps_train/rejected': '-136.18', 'logps_train/chosen': '-148.8', 'loss/train': '0.75367', 'examples_per_second': '30.784', 'grad_norm': '118.5', 'counters/examples': 226560, 'counters/updates': 7080}
train stats after 226592 examples: {'rewards_train/chosen': '0.17091', 'rewards_train/rejected': '0.073242', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097671', 'logps_train/rejected': '-172.79', 'logps_train/chosen': '-188.63', 'loss/train': '0.66618', 'examples_per_second': '31.582', 'grad_norm': '94', 'counters/examples': 226592, 'counters/updates': 7081}
skipping logging after 226624 examples to avoid logging too frequently
train stats after 226656 examples: {'rewards_train/chosen': '0.064102', 'rewards_train/rejected': '0.10944', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.045339', 'logps_train/rejected': '-133.6', 'logps_train/chosen': '-161.21', 'loss/train': '0.73055', 'examples_per_second': '31.463', 'grad_norm': '94', 'counters/examples': 226656, 'counters/updates': 7083}
train stats after 226688 examples: {'rewards_train/chosen': '0.10718', 'rewards_train/rejected': '0.11254', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0053554', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-152.92', 'loss/train': '0.72282', 'examples_per_second': '32.172', 'grad_norm': '173', 'counters/examples': 226688, 'counters/updates': 7084}
skipping logging after 226720 examples to avoid logging too frequently
train stats after 226752 examples: {'rewards_train/chosen': '0.10141', 'rewards_train/rejected': '0.070829', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030582', 'logps_train/rejected': '-141.66', 'logps_train/chosen': '-142.79', 'loss/train': '0.68784', 'examples_per_second': '31.098', 'grad_norm': '83.5', 'counters/examples': 226752, 'counters/updates': 7086}
train stats after 226784 examples: {'rewards_train/chosen': '0.18658', 'rewards_train/rejected': '0.07519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11139', 'logps_train/rejected': '-124.68', 'logps_train/chosen': '-188.28', 'loss/train': '0.65319', 'examples_per_second': '32.791', 'grad_norm': '116', 'counters/examples': 226784, 'counters/updates': 7087}
train stats after 226816 examples: {'rewards_train/chosen': '0.063471', 'rewards_train/rejected': '0.043252', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.020219', 'logps_train/rejected': '-113.8', 'logps_train/chosen': '-150.55', 'loss/train': '0.7061', 'examples_per_second': '30.124', 'grad_norm': '128', 'counters/examples': 226816, 'counters/updates': 7088}
train stats after 226848 examples: {'rewards_train/chosen': '0.0868', 'rewards_train/rejected': '-0.016959', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10376', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-155.3', 'loss/train': '0.66625', 'examples_per_second': '32.357', 'grad_norm': '316', 'counters/examples': 226848, 'counters/updates': 7089}
train stats after 226880 examples: {'rewards_train/chosen': '0.086898', 'rewards_train/rejected': '-0.0056904', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092588', 'logps_train/rejected': '-153.32', 'logps_train/chosen': '-188.37', 'loss/train': '0.66398', 'examples_per_second': '29.946', 'grad_norm': '51.25', 'counters/examples': 226880, 'counters/updates': 7090}
train stats after 226912 examples: {'rewards_train/chosen': '0.18068', 'rewards_train/rejected': '0.031419', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14926', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-146.03', 'loss/train': '0.63125', 'examples_per_second': '32.831', 'grad_norm': '92.5', 'counters/examples': 226912, 'counters/updates': 7091}
train stats after 226944 examples: {'rewards_train/chosen': '0.091401', 'rewards_train/rejected': '-0.011785', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10319', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-173.98', 'loss/train': '0.65886', 'examples_per_second': '30.316', 'grad_norm': '69.5', 'counters/examples': 226944, 'counters/updates': 7092}
train stats after 226976 examples: {'rewards_train/chosen': '0.16731', 'rewards_train/rejected': '0.062969', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10434', 'logps_train/rejected': '-135.61', 'logps_train/chosen': '-209.11', 'loss/train': '0.66101', 'examples_per_second': '31.578', 'grad_norm': '168', 'counters/examples': 226976, 'counters/updates': 7093}
skipping logging after 227008 examples to avoid logging too frequently
train stats after 227040 examples: {'rewards_train/chosen': '0.065763', 'rewards_train/rejected': '0.04341', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022352', 'logps_train/rejected': '-147.91', 'logps_train/chosen': '-196.36', 'loss/train': '0.71679', 'examples_per_second': '31.372', 'grad_norm': '103.5', 'counters/examples': 227040, 'counters/updates': 7095}
train stats after 227072 examples: {'rewards_train/chosen': '0.11772', 'rewards_train/rejected': '-0.042034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15976', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-156.79', 'loss/train': '0.63402', 'examples_per_second': '31.689', 'grad_norm': '86', 'counters/examples': 227072, 'counters/updates': 7096}
train stats after 227104 examples: {'rewards_train/chosen': '0.18766', 'rewards_train/rejected': '0.023914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16375', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-205.88', 'loss/train': '0.62522', 'examples_per_second': '30.122', 'grad_norm': '135', 'counters/examples': 227104, 'counters/updates': 7097}
train stats after 227136 examples: {'rewards_train/chosen': '0.12895', 'rewards_train/rejected': '0.04882', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080128', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-164.17', 'loss/train': '0.67927', 'examples_per_second': '31.523', 'grad_norm': '106', 'counters/examples': 227136, 'counters/updates': 7098}
train stats after 227168 examples: {'rewards_train/chosen': '0.021119', 'rewards_train/rejected': '0.076912', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.055794', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-161.27', 'loss/train': '0.73984', 'examples_per_second': '32.234', 'grad_norm': '71', 'counters/examples': 227168, 'counters/updates': 7099}
train stats after 227200 examples: {'rewards_train/chosen': '0.079848', 'rewards_train/rejected': '0.038932', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.040916', 'logps_train/rejected': '-171.9', 'logps_train/chosen': '-174', 'loss/train': '0.7092', 'examples_per_second': '31.457', 'grad_norm': '177', 'counters/examples': 227200, 'counters/updates': 7100}
train stats after 227232 examples: {'rewards_train/chosen': '0.10232', 'rewards_train/rejected': '0.1218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019479', 'logps_train/rejected': '-150.47', 'logps_train/chosen': '-171.31', 'loss/train': '0.72085', 'examples_per_second': '31.218', 'grad_norm': '133', 'counters/examples': 227232, 'counters/updates': 7101}
skipping logging after 227264 examples to avoid logging too frequently
train stats after 227296 examples: {'rewards_train/chosen': '0.14135', 'rewards_train/rejected': '-0.009678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15103', 'logps_train/rejected': '-107.06', 'logps_train/chosen': '-178.66', 'loss/train': '0.63602', 'examples_per_second': '30.797', 'grad_norm': '78', 'counters/examples': 227296, 'counters/updates': 7103}
skipping logging after 227328 examples to avoid logging too frequently
train stats after 227360 examples: {'rewards_train/chosen': '0.01102', 'rewards_train/rejected': '-0.0044078', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015428', 'logps_train/rejected': '-149.14', 'logps_train/chosen': '-104.91', 'loss/train': '0.69589', 'examples_per_second': '30.481', 'grad_norm': '111', 'counters/examples': 227360, 'counters/updates': 7105}
skipping logging after 227392 examples to avoid logging too frequently
train stats after 227424 examples: {'rewards_train/chosen': '0.21702', 'rewards_train/rejected': '0.077068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13995', 'logps_train/rejected': '-149.33', 'logps_train/chosen': '-160.12', 'loss/train': '0.64006', 'examples_per_second': '34.26', 'grad_norm': '70', 'counters/examples': 227424, 'counters/updates': 7107}
train stats after 227456 examples: {'rewards_train/chosen': '0.15386', 'rewards_train/rejected': '0.054999', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098861', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-173.26', 'loss/train': '0.66863', 'examples_per_second': '31.423', 'grad_norm': '91.5', 'counters/examples': 227456, 'counters/updates': 7108}
train stats after 227488 examples: {'rewards_train/chosen': '0.084136', 'rewards_train/rejected': '0.078305', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0058305', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-150.56', 'loss/train': '0.71505', 'examples_per_second': '31.626', 'grad_norm': '73', 'counters/examples': 227488, 'counters/updates': 7109}
train stats after 227520 examples: {'rewards_train/chosen': '0.10866', 'rewards_train/rejected': '0.0026871', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10598', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-126.07', 'loss/train': '0.65498', 'examples_per_second': '31.422', 'grad_norm': '75', 'counters/examples': 227520, 'counters/updates': 7110}
train stats after 227552 examples: {'rewards_train/chosen': '0.042939', 'rewards_train/rejected': '0.015524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027415', 'logps_train/rejected': '-141.57', 'logps_train/chosen': '-158.34', 'loss/train': '0.70112', 'examples_per_second': '31.387', 'grad_norm': '146', 'counters/examples': 227552, 'counters/updates': 7111}
skipping logging after 227584 examples to avoid logging too frequently
train stats after 227616 examples: {'rewards_train/chosen': '0.10487', 'rewards_train/rejected': '0.061326', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043545', 'logps_train/rejected': '-136.24', 'logps_train/chosen': '-146.45', 'loss/train': '0.69027', 'examples_per_second': '35.885', 'grad_norm': '78.5', 'counters/examples': 227616, 'counters/updates': 7113}
train stats after 227648 examples: {'rewards_train/chosen': '0.13375', 'rewards_train/rejected': '0.0067922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12696', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-131.88', 'loss/train': '0.64585', 'examples_per_second': '30.747', 'grad_norm': '110.5', 'counters/examples': 227648, 'counters/updates': 7114}
train stats after 227680 examples: {'rewards_train/chosen': '0.082924', 'rewards_train/rejected': '0.014589', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068335', 'logps_train/rejected': '-123.42', 'logps_train/chosen': '-154.08', 'loss/train': '0.66757', 'examples_per_second': '32.561', 'grad_norm': '77.5', 'counters/examples': 227680, 'counters/updates': 7115}
train stats after 227712 examples: {'rewards_train/chosen': '0.055806', 'rewards_train/rejected': '0.089228', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033422', 'logps_train/rejected': '-194.15', 'logps_train/chosen': '-124.8', 'loss/train': '0.72395', 'examples_per_second': '29.873', 'grad_norm': '95', 'counters/examples': 227712, 'counters/updates': 7116}
train stats after 227744 examples: {'rewards_train/chosen': '0.04439', 'rewards_train/rejected': '0.019464', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024925', 'logps_train/rejected': '-157.91', 'logps_train/chosen': '-165.2', 'loss/train': '0.69277', 'examples_per_second': '30.073', 'grad_norm': '92', 'counters/examples': 227744, 'counters/updates': 7117}
train stats after 227776 examples: {'rewards_train/chosen': '0.042751', 'rewards_train/rejected': '-0.074647', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1174', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-152.89', 'loss/train': '0.65165', 'examples_per_second': '29.872', 'grad_norm': '73', 'counters/examples': 227776, 'counters/updates': 7118}
train stats after 227808 examples: {'rewards_train/chosen': '0.13833', 'rewards_train/rejected': '0.22475', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.086426', 'logps_train/rejected': '-171.96', 'logps_train/chosen': '-165.84', 'loss/train': '0.76128', 'examples_per_second': '31.473', 'grad_norm': '112.5', 'counters/examples': 227808, 'counters/updates': 7119}
train stats after 227840 examples: {'rewards_train/chosen': '0.092238', 'rewards_train/rejected': '0.078997', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01324', 'logps_train/rejected': '-153.58', 'logps_train/chosen': '-169.37', 'loss/train': '0.69985', 'examples_per_second': '31.583', 'grad_norm': '107.5', 'counters/examples': 227840, 'counters/updates': 7120}
skipping logging after 227872 examples to avoid logging too frequently
train stats after 227904 examples: {'rewards_train/chosen': '0.087045', 'rewards_train/rejected': '0.016696', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070349', 'logps_train/rejected': '-139.89', 'logps_train/chosen': '-173.09', 'loss/train': '0.67529', 'examples_per_second': '30.395', 'grad_norm': '77.5', 'counters/examples': 227904, 'counters/updates': 7122}
skipping logging after 227936 examples to avoid logging too frequently
train stats after 227968 examples: {'rewards_train/chosen': '0.12866', 'rewards_train/rejected': '0.028844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099821', 'logps_train/rejected': '-145.96', 'logps_train/chosen': '-163.06', 'loss/train': '0.66114', 'examples_per_second': '31.613', 'grad_norm': '70', 'counters/examples': 227968, 'counters/updates': 7124}
train stats after 228000 examples: {'rewards_train/chosen': '0.10918', 'rewards_train/rejected': '0.081736', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027446', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-158.31', 'loss/train': '0.69472', 'examples_per_second': '31.447', 'grad_norm': '106.5', 'counters/examples': 228000, 'counters/updates': 7125}
Running evaluation after 228000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.96it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.78it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.89it/s]
eval after 228000: {'rewards_eval/chosen': '0.13988', 'rewards_eval/rejected': '0.052135', 'rewards_eval/accuracies': '0.56641', 'rewards_eval/margins': '0.087748', 'logps_eval/rejected': '-127.62', 'logps_eval/chosen': '-149.61', 'loss/eval': '0.6693'}
skipping save for non epoch
train stats after 228032 examples: {'rewards_train/chosen': '0.092575', 'rewards_train/rejected': '0.089897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0026776', 'logps_train/rejected': '-129.02', 'logps_train/chosen': '-135.91', 'loss/train': '0.70481', 'examples_per_second': '32.864', 'grad_norm': '60.25', 'counters/examples': 228032, 'counters/updates': 7126}
train stats after 228064 examples: {'rewards_train/chosen': '0.30548', 'rewards_train/rejected': '0.0098965', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.29558', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-196.92', 'loss/train': '0.57959', 'examples_per_second': '31.506', 'grad_norm': '62.75', 'counters/examples': 228064, 'counters/updates': 7127}
train stats after 228096 examples: {'rewards_train/chosen': '0.075423', 'rewards_train/rejected': '0.045761', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029662', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-167.26', 'loss/train': '0.69447', 'examples_per_second': '29.747', 'grad_norm': '190', 'counters/examples': 228096, 'counters/updates': 7128}
train stats after 228128 examples: {'rewards_train/chosen': '0.139', 'rewards_train/rejected': '0.0080051', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.131', 'logps_train/rejected': '-197.29', 'logps_train/chosen': '-173.69', 'loss/train': '0.65555', 'examples_per_second': '29.899', 'grad_norm': '126', 'counters/examples': 228128, 'counters/updates': 7129}
train stats after 228160 examples: {'rewards_train/chosen': '0.10646', 'rewards_train/rejected': '0.048367', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058089', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-103.61', 'loss/train': '0.67535', 'examples_per_second': '32.715', 'grad_norm': '53', 'counters/examples': 228160, 'counters/updates': 7130}
skipping logging after 228192 examples to avoid logging too frequently
train stats after 228224 examples: {'rewards_train/chosen': '0.087381', 'rewards_train/rejected': '0.16216', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.074778', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-153.34', 'loss/train': '0.74602', 'examples_per_second': '34.224', 'grad_norm': '234', 'counters/examples': 228224, 'counters/updates': 7132}
train stats after 228256 examples: {'rewards_train/chosen': '0.13323', 'rewards_train/rejected': '0.017219', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11601', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-145.05', 'loss/train': '0.65059', 'examples_per_second': '30.875', 'grad_norm': '80', 'counters/examples': 228256, 'counters/updates': 7133}
train stats after 228288 examples: {'rewards_train/chosen': '0.10549', 'rewards_train/rejected': '0.034743', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070746', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-170.89', 'loss/train': '0.6825', 'examples_per_second': '32.623', 'grad_norm': '93.5', 'counters/examples': 228288, 'counters/updates': 7134}
train stats after 228320 examples: {'rewards_train/chosen': '0.13151', 'rewards_train/rejected': '0.075887', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055622', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-150.23', 'loss/train': '0.68331', 'examples_per_second': '32.862', 'grad_norm': '75.5', 'counters/examples': 228320, 'counters/updates': 7135}
train stats after 228352 examples: {'rewards_train/chosen': '0.16309', 'rewards_train/rejected': '0.0061868', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1569', 'logps_train/rejected': '-143.93', 'logps_train/chosen': '-176.85', 'loss/train': '0.63325', 'examples_per_second': '30.084', 'grad_norm': '117', 'counters/examples': 228352, 'counters/updates': 7136}
train stats after 228384 examples: {'rewards_train/chosen': '0.045945', 'rewards_train/rejected': '0.095012', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049067', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-147.02', 'loss/train': '0.73752', 'examples_per_second': '31.56', 'grad_norm': '94.5', 'counters/examples': 228384, 'counters/updates': 7137}
train stats after 228416 examples: {'rewards_train/chosen': '0.13148', 'rewards_train/rejected': '0.06338', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068102', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-210.77', 'loss/train': '0.67219', 'examples_per_second': '31.632', 'grad_norm': '93.5', 'counters/examples': 228416, 'counters/updates': 7138}
train stats after 228448 examples: {'rewards_train/chosen': '0.23329', 'rewards_train/rejected': '0.095585', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13771', 'logps_train/rejected': '-154.56', 'logps_train/chosen': '-175.25', 'loss/train': '0.65761', 'examples_per_second': '31.408', 'grad_norm': '112', 'counters/examples': 228448, 'counters/updates': 7139}
skipping logging after 228480 examples to avoid logging too frequently
train stats after 228512 examples: {'rewards_train/chosen': '0.1533', 'rewards_train/rejected': '0.063106', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090191', 'logps_train/rejected': '-153.12', 'logps_train/chosen': '-191.15', 'loss/train': '0.67622', 'examples_per_second': '31.551', 'grad_norm': '268', 'counters/examples': 228512, 'counters/updates': 7141}
train stats after 228544 examples: {'rewards_train/chosen': '0.15004', 'rewards_train/rejected': '-0.022303', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17234', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-142.59', 'loss/train': '0.62535', 'examples_per_second': '30.099', 'grad_norm': '90.5', 'counters/examples': 228544, 'counters/updates': 7142}
skipping logging after 228576 examples to avoid logging too frequently
train stats after 228608 examples: {'rewards_train/chosen': '0.084919', 'rewards_train/rejected': '0.010764', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074155', 'logps_train/rejected': '-109.09', 'logps_train/chosen': '-137', 'loss/train': '0.67454', 'examples_per_second': '32.077', 'grad_norm': '59.75', 'counters/examples': 228608, 'counters/updates': 7144}
train stats after 228640 examples: {'rewards_train/chosen': '0.062604', 'rewards_train/rejected': '-0.035411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098015', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-149.14', 'loss/train': '0.65471', 'examples_per_second': '31.523', 'grad_norm': '65', 'counters/examples': 228640, 'counters/updates': 7145}
skipping logging after 228672 examples to avoid logging too frequently
train stats after 228704 examples: {'rewards_train/chosen': '0.10692', 'rewards_train/rejected': '-0.0072555', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11417', 'logps_train/rejected': '-100.49', 'logps_train/chosen': '-148.09', 'loss/train': '0.64972', 'examples_per_second': '31.887', 'grad_norm': '82', 'counters/examples': 228704, 'counters/updates': 7147}
train stats after 228736 examples: {'rewards_train/chosen': '0.031968', 'rewards_train/rejected': '0.051694', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019726', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-140.39', 'loss/train': '0.727', 'examples_per_second': '31.728', 'grad_norm': '93.5', 'counters/examples': 228736, 'counters/updates': 7148}
train stats after 228768 examples: {'rewards_train/chosen': '0.09757', 'rewards_train/rejected': '0.024457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073113', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-168.87', 'loss/train': '0.68337', 'examples_per_second': '24.562', 'grad_norm': '77', 'counters/examples': 228768, 'counters/updates': 7149}
train stats after 228800 examples: {'rewards_train/chosen': '0.086543', 'rewards_train/rejected': '0.010464', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076079', 'logps_train/rejected': '-151.42', 'logps_train/chosen': '-149.77', 'loss/train': '0.6645', 'examples_per_second': '30.81', 'grad_norm': '109.5', 'counters/examples': 228800, 'counters/updates': 7150}
skipping logging after 228832 examples to avoid logging too frequently
train stats after 228864 examples: {'rewards_train/chosen': '0.077525', 'rewards_train/rejected': '0.088826', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011301', 'logps_train/rejected': '-142.95', 'logps_train/chosen': '-162.75', 'loss/train': '0.71684', 'examples_per_second': '33.316', 'grad_norm': '183', 'counters/examples': 228864, 'counters/updates': 7152}
skipping logging after 228896 examples to avoid logging too frequently
train stats after 228928 examples: {'rewards_train/chosen': '0.1731', 'rewards_train/rejected': '0.07899', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094114', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-130.43', 'loss/train': '0.65933', 'examples_per_second': '32.383', 'grad_norm': '81.5', 'counters/examples': 228928, 'counters/updates': 7154}
skipping logging after 228960 examples to avoid logging too frequently
train stats after 228992 examples: {'rewards_train/chosen': '0.23841', 'rewards_train/rejected': '0.0033902', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23502', 'logps_train/rejected': '-130.54', 'logps_train/chosen': '-151.22', 'loss/train': '0.59583', 'examples_per_second': '31.544', 'grad_norm': '46', 'counters/examples': 228992, 'counters/updates': 7156}
train stats after 229024 examples: {'rewards_train/chosen': '0.14706', 'rewards_train/rejected': '0.073229', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073833', 'logps_train/rejected': '-110.88', 'logps_train/chosen': '-160.97', 'loss/train': '0.66817', 'examples_per_second': '31.872', 'grad_norm': '51.25', 'counters/examples': 229024, 'counters/updates': 7157}
train stats after 229056 examples: {'rewards_train/chosen': '0.089731', 'rewards_train/rejected': '0.079602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010129', 'logps_train/rejected': '-132.02', 'logps_train/chosen': '-131.87', 'loss/train': '0.70664', 'examples_per_second': '31.532', 'grad_norm': '87', 'counters/examples': 229056, 'counters/updates': 7158}
train stats after 229088 examples: {'rewards_train/chosen': '0.12998', 'rewards_train/rejected': '0.073776', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056206', 'logps_train/rejected': '-147.18', 'logps_train/chosen': '-145.36', 'loss/train': '0.67988', 'examples_per_second': '31.313', 'grad_norm': '106', 'counters/examples': 229088, 'counters/updates': 7159}
train stats after 229120 examples: {'rewards_train/chosen': '0.1563', 'rewards_train/rejected': '0.0063717', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14993', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-185.11', 'loss/train': '0.64241', 'examples_per_second': '30.175', 'grad_norm': '85.5', 'counters/examples': 229120, 'counters/updates': 7160}
train stats after 229152 examples: {'rewards_train/chosen': '0.12578', 'rewards_train/rejected': '0.061977', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063808', 'logps_train/rejected': '-124.54', 'logps_train/chosen': '-146.45', 'loss/train': '0.67543', 'examples_per_second': '30.146', 'grad_norm': '69', 'counters/examples': 229152, 'counters/updates': 7161}
train stats after 229184 examples: {'rewards_train/chosen': '0.08821', 'rewards_train/rejected': '0.088696', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '-0.00048608', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-129.47', 'loss/train': '0.70921', 'examples_per_second': '31.144', 'grad_norm': '115', 'counters/examples': 229184, 'counters/updates': 7162}
train stats after 229216 examples: {'rewards_train/chosen': '0.10772', 'rewards_train/rejected': '0.032127', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075595', 'logps_train/rejected': '-148.95', 'logps_train/chosen': '-168.84', 'loss/train': '0.66896', 'examples_per_second': '31.446', 'grad_norm': '80.5', 'counters/examples': 229216, 'counters/updates': 7163}
train stats after 229248 examples: {'rewards_train/chosen': '0.16511', 'rewards_train/rejected': '0.11497', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050145', 'logps_train/rejected': '-192.44', 'logps_train/chosen': '-180.5', 'loss/train': '0.69363', 'examples_per_second': '30.278', 'grad_norm': '74', 'counters/examples': 229248, 'counters/updates': 7164}
train stats after 229280 examples: {'rewards_train/chosen': '0.18174', 'rewards_train/rejected': '-0.025321', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20706', 'logps_train/rejected': '-174.85', 'logps_train/chosen': '-166.93', 'loss/train': '0.61276', 'examples_per_second': '33.064', 'grad_norm': '184', 'counters/examples': 229280, 'counters/updates': 7165}
train stats after 229312 examples: {'rewards_train/chosen': '0.14103', 'rewards_train/rejected': '0.071333', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0697', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-146.72', 'loss/train': '0.68109', 'examples_per_second': '31.122', 'grad_norm': '90.5', 'counters/examples': 229312, 'counters/updates': 7166}
skipping logging after 229344 examples to avoid logging too frequently
train stats after 229376 examples: {'rewards_train/chosen': '0.16344', 'rewards_train/rejected': '0.0094605', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15398', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-170.4', 'loss/train': '0.62644', 'examples_per_second': '30.414', 'grad_norm': '76', 'counters/examples': 229376, 'counters/updates': 7168}
train stats after 229408 examples: {'rewards_train/chosen': '0.11726', 'rewards_train/rejected': '0.091014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026247', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-176.35', 'loss/train': '0.69991', 'examples_per_second': '32.832', 'grad_norm': '93', 'counters/examples': 229408, 'counters/updates': 7169}
train stats after 229440 examples: {'rewards_train/chosen': '0.11776', 'rewards_train/rejected': '0.04145', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.076307', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-156.62', 'loss/train': '0.6653', 'examples_per_second': '30.143', 'grad_norm': '82', 'counters/examples': 229440, 'counters/updates': 7170}
train stats after 229472 examples: {'rewards_train/chosen': '0.1043', 'rewards_train/rejected': '0.083509', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020795', 'logps_train/rejected': '-116.31', 'logps_train/chosen': '-127.47', 'loss/train': '0.69607', 'examples_per_second': '31.726', 'grad_norm': '79', 'counters/examples': 229472, 'counters/updates': 7171}
train stats after 229504 examples: {'rewards_train/chosen': '0.1461', 'rewards_train/rejected': '0.029402', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1167', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-181.6', 'loss/train': '0.66101', 'examples_per_second': '31.938', 'grad_norm': '79', 'counters/examples': 229504, 'counters/updates': 7172}
train stats after 229536 examples: {'rewards_train/chosen': '0.099808', 'rewards_train/rejected': '0.060916', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038892', 'logps_train/rejected': '-137.78', 'logps_train/chosen': '-135.45', 'loss/train': '0.69114', 'examples_per_second': '32.269', 'grad_norm': '123.5', 'counters/examples': 229536, 'counters/updates': 7173}
train stats after 229568 examples: {'rewards_train/chosen': '0.18154', 'rewards_train/rejected': '0.05627', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12527', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-164.05', 'loss/train': '0.64117', 'examples_per_second': '31.295', 'grad_norm': '87', 'counters/examples': 229568, 'counters/updates': 7174}
train stats after 229600 examples: {'rewards_train/chosen': '0.07996', 'rewards_train/rejected': '0.053021', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026939', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-124.93', 'loss/train': '0.69074', 'examples_per_second': '32.557', 'grad_norm': '64', 'counters/examples': 229600, 'counters/updates': 7175}
train stats after 229632 examples: {'rewards_train/chosen': '0.1257', 'rewards_train/rejected': '0.025672', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10003', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-152.88', 'loss/train': '0.65633', 'examples_per_second': '32.362', 'grad_norm': '67', 'counters/examples': 229632, 'counters/updates': 7176}
train stats after 229664 examples: {'rewards_train/chosen': '0.07632', 'rewards_train/rejected': '0.082294', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0059736', 'logps_train/rejected': '-133.92', 'logps_train/chosen': '-181.24', 'loss/train': '0.70727', 'examples_per_second': '30.441', 'grad_norm': '132', 'counters/examples': 229664, 'counters/updates': 7177}
train stats after 229696 examples: {'rewards_train/chosen': '0.15804', 'rewards_train/rejected': '0.047046', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11099', 'logps_train/rejected': '-105.54', 'logps_train/chosen': '-181.66', 'loss/train': '0.65334', 'examples_per_second': '24.542', 'grad_norm': '92.5', 'counters/examples': 229696, 'counters/updates': 7178}
train stats after 229728 examples: {'rewards_train/chosen': '0.16632', 'rewards_train/rejected': '0.063234', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10308', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-172.47', 'loss/train': '0.65091', 'examples_per_second': '31.295', 'grad_norm': '78', 'counters/examples': 229728, 'counters/updates': 7179}
train stats after 229760 examples: {'rewards_train/chosen': '0.11444', 'rewards_train/rejected': '0.039798', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074646', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-175.86', 'loss/train': '0.6701', 'examples_per_second': '31.424', 'grad_norm': '216', 'counters/examples': 229760, 'counters/updates': 7180}
train stats after 229792 examples: {'rewards_train/chosen': '0.075305', 'rewards_train/rejected': '0.076218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0009132', 'logps_train/rejected': '-132.59', 'logps_train/chosen': '-143.46', 'loss/train': '0.7046', 'examples_per_second': '24.373', 'grad_norm': '100', 'counters/examples': 229792, 'counters/updates': 7181}
train stats after 229824 examples: {'rewards_train/chosen': '0.11669', 'rewards_train/rejected': '-0.029126', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14581', 'logps_train/rejected': '-98.26', 'logps_train/chosen': '-140.37', 'loss/train': '0.6359', 'examples_per_second': '32.582', 'grad_norm': '80', 'counters/examples': 229824, 'counters/updates': 7182}
skipping logging after 229856 examples to avoid logging too frequently
train stats after 229888 examples: {'rewards_train/chosen': '0.23837', 'rewards_train/rejected': '0.10556', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13281', 'logps_train/rejected': '-143.62', 'logps_train/chosen': '-180.24', 'loss/train': '0.66318', 'examples_per_second': '29.865', 'grad_norm': '159', 'counters/examples': 229888, 'counters/updates': 7184}
train stats after 229920 examples: {'rewards_train/chosen': '0.035126', 'rewards_train/rejected': '0.029483', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0056433', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-158.05', 'loss/train': '0.70419', 'examples_per_second': '30.343', 'grad_norm': '75', 'counters/examples': 229920, 'counters/updates': 7185}
train stats after 229952 examples: {'rewards_train/chosen': '0.11921', 'rewards_train/rejected': '-0.00087085', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12008', 'logps_train/rejected': '-103.29', 'logps_train/chosen': '-158.75', 'loss/train': '0.64565', 'examples_per_second': '31.784', 'grad_norm': '67', 'counters/examples': 229952, 'counters/updates': 7186}
skipping logging after 229984 examples to avoid logging too frequently
train stats after 230016 examples: {'rewards_train/chosen': '0.10897', 'rewards_train/rejected': '-0.0049358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11391', 'logps_train/rejected': '-102.78', 'logps_train/chosen': '-113.47', 'loss/train': '0.64403', 'examples_per_second': '31.529', 'grad_norm': '47.75', 'counters/examples': 230016, 'counters/updates': 7188}
train stats after 230048 examples: {'rewards_train/chosen': '0.076576', 'rewards_train/rejected': '0.055521', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021055', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-171.68', 'loss/train': '0.70345', 'examples_per_second': '31.75', 'grad_norm': '78.5', 'counters/examples': 230048, 'counters/updates': 7189}
train stats after 230080 examples: {'rewards_train/chosen': '0.086794', 'rewards_train/rejected': '-0.021157', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10795', 'logps_train/rejected': '-107.01', 'logps_train/chosen': '-126.74', 'loss/train': '0.64988', 'examples_per_second': '31.018', 'grad_norm': '74', 'counters/examples': 230080, 'counters/updates': 7190}
skipping logging after 230112 examples to avoid logging too frequently
train stats after 230144 examples: {'rewards_train/chosen': '0.031379', 'rewards_train/rejected': '0.12122', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.089841', 'logps_train/rejected': '-163.53', 'logps_train/chosen': '-175.56', 'loss/train': '0.75273', 'examples_per_second': '33.864', 'grad_norm': '134', 'counters/examples': 230144, 'counters/updates': 7192}
train stats after 230176 examples: {'rewards_train/chosen': '0.069265', 'rewards_train/rejected': '0.10307', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033806', 'logps_train/rejected': '-155.39', 'logps_train/chosen': '-165.56', 'loss/train': '0.72829', 'examples_per_second': '32.367', 'grad_norm': '62.75', 'counters/examples': 230176, 'counters/updates': 7193}
train stats after 230208 examples: {'rewards_train/chosen': '0.20715', 'rewards_train/rejected': '0.026496', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18065', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-154.17', 'loss/train': '0.63932', 'examples_per_second': '30.815', 'grad_norm': '105', 'counters/examples': 230208, 'counters/updates': 7194}
train stats after 230240 examples: {'rewards_train/chosen': '0.14493', 'rewards_train/rejected': '0.099991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044936', 'logps_train/rejected': '-176.04', 'logps_train/chosen': '-144.31', 'loss/train': '0.68336', 'examples_per_second': '31.673', 'grad_norm': '217', 'counters/examples': 230240, 'counters/updates': 7195}
train stats after 230272 examples: {'rewards_train/chosen': '0.066003', 'rewards_train/rejected': '0.022698', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043305', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-131.85', 'loss/train': '0.67964', 'examples_per_second': '30.992', 'grad_norm': '120.5', 'counters/examples': 230272, 'counters/updates': 7196}
train stats after 230304 examples: {'rewards_train/chosen': '0.1531', 'rewards_train/rejected': '0.083613', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069487', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-120.57', 'loss/train': '0.66989', 'examples_per_second': '32.488', 'grad_norm': '96.5', 'counters/examples': 230304, 'counters/updates': 7197}
train stats after 230336 examples: {'rewards_train/chosen': '0.21412', 'rewards_train/rejected': '0.052352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16177', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-164.76', 'loss/train': '0.63706', 'examples_per_second': '31.159', 'grad_norm': '81.5', 'counters/examples': 230336, 'counters/updates': 7198}
train stats after 230368 examples: {'rewards_train/chosen': '0.15042', 'rewards_train/rejected': '0.020952', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12947', 'logps_train/rejected': '-153.39', 'logps_train/chosen': '-135.83', 'loss/train': '0.64818', 'examples_per_second': '30.897', 'grad_norm': '65', 'counters/examples': 230368, 'counters/updates': 7199}
train stats after 230400 examples: {'rewards_train/chosen': '0.12034', 'rewards_train/rejected': '0.25453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.13419', 'logps_train/rejected': '-166', 'logps_train/chosen': '-132.68', 'loss/train': '0.80065', 'examples_per_second': '30.954', 'grad_norm': '80', 'counters/examples': 230400, 'counters/updates': 7200}
train stats after 230432 examples: {'rewards_train/chosen': '0.18251', 'rewards_train/rejected': '0.068342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11416', 'logps_train/rejected': '-145.92', 'logps_train/chosen': '-165.93', 'loss/train': '0.65659', 'examples_per_second': '30.963', 'grad_norm': '70', 'counters/examples': 230432, 'counters/updates': 7201}
train stats after 230464 examples: {'rewards_train/chosen': '0.12775', 'rewards_train/rejected': '0.13422', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.006476', 'logps_train/rejected': '-184.43', 'logps_train/chosen': '-153.63', 'loss/train': '0.71292', 'examples_per_second': '31.611', 'grad_norm': '95', 'counters/examples': 230464, 'counters/updates': 7202}
train stats after 230496 examples: {'rewards_train/chosen': '0.072783', 'rewards_train/rejected': '0.062321', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010461', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-126.52', 'loss/train': '0.69886', 'examples_per_second': '31.216', 'grad_norm': '56', 'counters/examples': 230496, 'counters/updates': 7203}
skipping logging after 230528 examples to avoid logging too frequently
train stats after 230560 examples: {'rewards_train/chosen': '0.10873', 'rewards_train/rejected': '0.058163', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050566', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-123.39', 'loss/train': '0.68169', 'examples_per_second': '31.635', 'grad_norm': '90', 'counters/examples': 230560, 'counters/updates': 7205}
train stats after 230592 examples: {'rewards_train/chosen': '0.10241', 'rewards_train/rejected': '0.030771', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071641', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-141.17', 'loss/train': '0.66835', 'examples_per_second': '30.898', 'grad_norm': '76', 'counters/examples': 230592, 'counters/updates': 7206}
train stats after 230624 examples: {'rewards_train/chosen': '0.1353', 'rewards_train/rejected': '0.073018', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062285', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-122.13', 'loss/train': '0.67399', 'examples_per_second': '31.745', 'grad_norm': '81', 'counters/examples': 230624, 'counters/updates': 7207}
skipping logging after 230656 examples to avoid logging too frequently
train stats after 230688 examples: {'rewards_train/chosen': '0.10138', 'rewards_train/rejected': '0.077569', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023812', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-161.89', 'loss/train': '0.69235', 'examples_per_second': '31.649', 'grad_norm': '69.5', 'counters/examples': 230688, 'counters/updates': 7209}
train stats after 230720 examples: {'rewards_train/chosen': '0.14396', 'rewards_train/rejected': '0.08375', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060206', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-144.36', 'loss/train': '0.67813', 'examples_per_second': '30.809', 'grad_norm': '62', 'counters/examples': 230720, 'counters/updates': 7210}
train stats after 230752 examples: {'rewards_train/chosen': '0.10959', 'rewards_train/rejected': '-0.022801', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13239', 'logps_train/rejected': '-160.92', 'logps_train/chosen': '-147.98', 'loss/train': '0.64434', 'examples_per_second': '31.346', 'grad_norm': '76.5', 'counters/examples': 230752, 'counters/updates': 7211}
skipping logging after 230784 examples to avoid logging too frequently
train stats after 230816 examples: {'rewards_train/chosen': '0.058653', 'rewards_train/rejected': '0.078515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019862', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-168.96', 'loss/train': '0.71524', 'examples_per_second': '32.644', 'grad_norm': '103.5', 'counters/examples': 230816, 'counters/updates': 7213}
skipping logging after 230848 examples to avoid logging too frequently
train stats after 230880 examples: {'rewards_train/chosen': '0.08533', 'rewards_train/rejected': '0.062718', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022612', 'logps_train/rejected': '-115.05', 'logps_train/chosen': '-152.6', 'loss/train': '0.69474', 'examples_per_second': '29.921', 'grad_norm': '78', 'counters/examples': 230880, 'counters/updates': 7215}
train stats after 230912 examples: {'rewards_train/chosen': '0.15555', 'rewards_train/rejected': '0.15044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0051097', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-142.32', 'loss/train': '0.70405', 'examples_per_second': '32.379', 'grad_norm': '209', 'counters/examples': 230912, 'counters/updates': 7216}
train stats after 230944 examples: {'rewards_train/chosen': '0.13424', 'rewards_train/rejected': '0.04254', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091701', 'logps_train/rejected': '-180.2', 'logps_train/chosen': '-170.06', 'loss/train': '0.6612', 'examples_per_second': '32.087', 'grad_norm': '109', 'counters/examples': 230944, 'counters/updates': 7217}
train stats after 230976 examples: {'rewards_train/chosen': '0.093433', 'rewards_train/rejected': '0.11966', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026225', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-171.48', 'loss/train': '0.71824', 'examples_per_second': '31.58', 'grad_norm': '78.5', 'counters/examples': 230976, 'counters/updates': 7218}
train stats after 231008 examples: {'rewards_train/chosen': '0.12048', 'rewards_train/rejected': '0.057751', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.062729', 'logps_train/rejected': '-137.77', 'logps_train/chosen': '-146.06', 'loss/train': '0.67752', 'examples_per_second': '31.41', 'grad_norm': '85.5', 'counters/examples': 231008, 'counters/updates': 7219}
train stats after 231040 examples: {'rewards_train/chosen': '0.075609', 'rewards_train/rejected': '0.041851', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033759', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-135.97', 'loss/train': '0.68934', 'examples_per_second': '31.41', 'grad_norm': '72', 'counters/examples': 231040, 'counters/updates': 7220}
skipping logging after 231072 examples to avoid logging too frequently
train stats after 231104 examples: {'rewards_train/chosen': '0.099079', 'rewards_train/rejected': '0.12555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.026475', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-135.75', 'loss/train': '0.73496', 'examples_per_second': '31.75', 'grad_norm': '91', 'counters/examples': 231104, 'counters/updates': 7222}
skipping logging after 231136 examples to avoid logging too frequently
train stats after 231168 examples: {'rewards_train/chosen': '0.13141', 'rewards_train/rejected': '0.06439', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06702', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-161.92', 'loss/train': '0.67796', 'examples_per_second': '30.129', 'grad_norm': '58', 'counters/examples': 231168, 'counters/updates': 7224}
skipping logging after 231200 examples to avoid logging too frequently
train stats after 231232 examples: {'rewards_train/chosen': '0.15542', 'rewards_train/rejected': '0.03001', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12541', 'logps_train/rejected': '-115.44', 'logps_train/chosen': '-149.11', 'loss/train': '0.65851', 'examples_per_second': '30.064', 'grad_norm': '71.5', 'counters/examples': 231232, 'counters/updates': 7226}
train stats after 231264 examples: {'rewards_train/chosen': '0.16232', 'rewards_train/rejected': '0.011457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15086', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-182.18', 'loss/train': '0.63344', 'examples_per_second': '31.4', 'grad_norm': '73', 'counters/examples': 231264, 'counters/updates': 7227}
train stats after 231296 examples: {'rewards_train/chosen': '0.075258', 'rewards_train/rejected': '0.068351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0069075', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-163.73', 'loss/train': '0.70683', 'examples_per_second': '31.373', 'grad_norm': '86', 'counters/examples': 231296, 'counters/updates': 7228}
skipping logging after 231328 examples to avoid logging too frequently
train stats after 231360 examples: {'rewards_train/chosen': '0.17105', 'rewards_train/rejected': '0.074621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096429', 'logps_train/rejected': '-119.15', 'logps_train/chosen': '-153.21', 'loss/train': '0.66259', 'examples_per_second': '31.281', 'grad_norm': '117.5', 'counters/examples': 231360, 'counters/updates': 7230}
train stats after 231392 examples: {'rewards_train/chosen': '0.050365', 'rewards_train/rejected': '0.029421', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.020944', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-135.32', 'loss/train': '0.7008', 'examples_per_second': '31.442', 'grad_norm': '66', 'counters/examples': 231392, 'counters/updates': 7231}
train stats after 231424 examples: {'rewards_train/chosen': '0.15064', 'rewards_train/rejected': '0.011733', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1389', 'logps_train/rejected': '-152.91', 'logps_train/chosen': '-127.25', 'loss/train': '0.64147', 'examples_per_second': '30.55', 'grad_norm': '114', 'counters/examples': 231424, 'counters/updates': 7232}
skipping logging after 231456 examples to avoid logging too frequently
train stats after 231488 examples: {'rewards_train/chosen': '0.14514', 'rewards_train/rejected': '0.042835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1023', 'logps_train/rejected': '-124.81', 'logps_train/chosen': '-164.88', 'loss/train': '0.6524', 'examples_per_second': '33.336', 'grad_norm': '68', 'counters/examples': 231488, 'counters/updates': 7234}
train stats after 231520 examples: {'rewards_train/chosen': '0.16796', 'rewards_train/rejected': '0.06529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10267', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-150.57', 'loss/train': '0.65204', 'examples_per_second': '31.5', 'grad_norm': '69.5', 'counters/examples': 231520, 'counters/updates': 7235}
skipping logging after 231552 examples to avoid logging too frequently
train stats after 231584 examples: {'rewards_train/chosen': '0.13607', 'rewards_train/rejected': '0.09065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045423', 'logps_train/rejected': '-163.37', 'logps_train/chosen': '-162.84', 'loss/train': '0.69413', 'examples_per_second': '30.708', 'grad_norm': '213', 'counters/examples': 231584, 'counters/updates': 7237}
train stats after 231616 examples: {'rewards_train/chosen': '0.14389', 'rewards_train/rejected': '0.032986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1109', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-143.14', 'loss/train': '0.65016', 'examples_per_second': '32.428', 'grad_norm': '78.5', 'counters/examples': 231616, 'counters/updates': 7238}
train stats after 231648 examples: {'rewards_train/chosen': '0.13427', 'rewards_train/rejected': '0.069918', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064349', 'logps_train/rejected': '-125.32', 'logps_train/chosen': '-172.44', 'loss/train': '0.67304', 'examples_per_second': '32.683', 'grad_norm': '95', 'counters/examples': 231648, 'counters/updates': 7239}
train stats after 231680 examples: {'rewards_train/chosen': '0.16889', 'rewards_train/rejected': '0.043827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12506', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-163.61', 'loss/train': '0.64602', 'examples_per_second': '30.112', 'grad_norm': '90', 'counters/examples': 231680, 'counters/updates': 7240}
skipping logging after 231712 examples to avoid logging too frequently
train stats after 231744 examples: {'rewards_train/chosen': '0.11811', 'rewards_train/rejected': '0.099544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018569', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-161.87', 'loss/train': '0.699', 'examples_per_second': '31.562', 'grad_norm': '145', 'counters/examples': 231744, 'counters/updates': 7242}
train stats after 231776 examples: {'rewards_train/chosen': '0.012077', 'rewards_train/rejected': '0.095585', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.083508', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-137.77', 'loss/train': '0.75468', 'examples_per_second': '31.429', 'grad_norm': '696', 'counters/examples': 231776, 'counters/updates': 7243}
train stats after 231808 examples: {'rewards_train/chosen': '0.092275', 'rewards_train/rejected': '-0.069074', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16135', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-133.05', 'loss/train': '0.62049', 'examples_per_second': '30.075', 'grad_norm': '82', 'counters/examples': 231808, 'counters/updates': 7244}
train stats after 231840 examples: {'rewards_train/chosen': '0.031138', 'rewards_train/rejected': '-0.016412', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04755', 'logps_train/rejected': '-168.8', 'logps_train/chosen': '-161.51', 'loss/train': '0.68716', 'examples_per_second': '31.889', 'grad_norm': '102.5', 'counters/examples': 231840, 'counters/updates': 7245}
train stats after 231872 examples: {'rewards_train/chosen': '0.11595', 'rewards_train/rejected': '0.08556', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030388', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-167.8', 'loss/train': '0.69815', 'examples_per_second': '30.406', 'grad_norm': '166', 'counters/examples': 231872, 'counters/updates': 7246}
skipping logging after 231904 examples to avoid logging too frequently
train stats after 231936 examples: {'rewards_train/chosen': '0.10575', 'rewards_train/rejected': '0.008696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097052', 'logps_train/rejected': '-92.833', 'logps_train/chosen': '-150.62', 'loss/train': '0.65905', 'examples_per_second': '34.451', 'grad_norm': '47', 'counters/examples': 231936, 'counters/updates': 7248}
train stats after 231968 examples: {'rewards_train/chosen': '0.21091', 'rewards_train/rejected': '0.067396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14351', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-153.34', 'loss/train': '0.63808', 'examples_per_second': '30.644', 'grad_norm': '68.5', 'counters/examples': 231968, 'counters/updates': 7249}
train stats after 232000 examples: {'rewards_train/chosen': '0.12325', 'rewards_train/rejected': '0.059615', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063632', 'logps_train/rejected': '-169.6', 'logps_train/chosen': '-131.86', 'loss/train': '0.67842', 'examples_per_second': '31.576', 'grad_norm': '83', 'counters/examples': 232000, 'counters/updates': 7250}
skipping logging after 232032 examples to avoid logging too frequently
train stats after 232064 examples: {'rewards_train/chosen': '0.11153', 'rewards_train/rejected': '-0.02876', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14029', 'logps_train/rejected': '-110.9', 'logps_train/chosen': '-140.48', 'loss/train': '0.63678', 'examples_per_second': '32.172', 'grad_norm': '42.75', 'counters/examples': 232064, 'counters/updates': 7252}
train stats after 232096 examples: {'rewards_train/chosen': '0.1592', 'rewards_train/rejected': '0.12234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036856', 'logps_train/rejected': '-119.73', 'logps_train/chosen': '-166.97', 'loss/train': '0.69528', 'examples_per_second': '32.628', 'grad_norm': '118', 'counters/examples': 232096, 'counters/updates': 7253}
train stats after 232128 examples: {'rewards_train/chosen': '0.10989', 'rewards_train/rejected': '0.022073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087819', 'logps_train/rejected': '-101.4', 'logps_train/chosen': '-142.69', 'loss/train': '0.67181', 'examples_per_second': '31.776', 'grad_norm': '56.25', 'counters/examples': 232128, 'counters/updates': 7254}
train stats after 232160 examples: {'rewards_train/chosen': '0.16041', 'rewards_train/rejected': '0.05439', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10602', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-122.76', 'loss/train': '0.6548', 'examples_per_second': '31.328', 'grad_norm': '55.75', 'counters/examples': 232160, 'counters/updates': 7255}
train stats after 232192 examples: {'rewards_train/chosen': '0.17439', 'rewards_train/rejected': '0.018664', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15573', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-128.94', 'loss/train': '0.63666', 'examples_per_second': '30.583', 'grad_norm': '73', 'counters/examples': 232192, 'counters/updates': 7256}
skipping logging after 232224 examples to avoid logging too frequently
train stats after 232256 examples: {'rewards_train/chosen': '0.14089', 'rewards_train/rejected': '-0.0012575', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14215', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-143.34', 'loss/train': '0.64266', 'examples_per_second': '31.989', 'grad_norm': '48.5', 'counters/examples': 232256, 'counters/updates': 7258}
train stats after 232288 examples: {'rewards_train/chosen': '0.02579', 'rewards_train/rejected': '0.14665', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.12086', 'logps_train/rejected': '-146.06', 'logps_train/chosen': '-131.04', 'loss/train': '0.77696', 'examples_per_second': '31.583', 'grad_norm': '115', 'counters/examples': 232288, 'counters/updates': 7259}
train stats after 232320 examples: {'rewards_train/chosen': '0.082289', 'rewards_train/rejected': '-0.050846', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13314', 'logps_train/rejected': '-169.78', 'logps_train/chosen': '-175.7', 'loss/train': '0.65454', 'examples_per_second': '30.62', 'grad_norm': '222', 'counters/examples': 232320, 'counters/updates': 7260}
skipping logging after 232352 examples to avoid logging too frequently
train stats after 232384 examples: {'rewards_train/chosen': '0.13538', 'rewards_train/rejected': '0.12029', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015089', 'logps_train/rejected': '-110.1', 'logps_train/chosen': '-171.64', 'loss/train': '0.69858', 'examples_per_second': '30.544', 'grad_norm': '80', 'counters/examples': 232384, 'counters/updates': 7262}
train stats after 232416 examples: {'rewards_train/chosen': '0.083167', 'rewards_train/rejected': '0.033884', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049283', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-150.05', 'loss/train': '0.67598', 'examples_per_second': '31.558', 'grad_norm': '94.5', 'counters/examples': 232416, 'counters/updates': 7263}
train stats after 232448 examples: {'rewards_train/chosen': '0.030516', 'rewards_train/rejected': '0.05201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021494', 'logps_train/rejected': '-139.8', 'logps_train/chosen': '-160.88', 'loss/train': '0.71328', 'examples_per_second': '32.11', 'grad_norm': '130', 'counters/examples': 232448, 'counters/updates': 7264}
train stats after 232480 examples: {'rewards_train/chosen': '0.17175', 'rewards_train/rejected': '0.083609', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088137', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-156.18', 'loss/train': '0.66676', 'examples_per_second': '32.132', 'grad_norm': '111', 'counters/examples': 232480, 'counters/updates': 7265}
train stats after 232512 examples: {'rewards_train/chosen': '0.22191', 'rewards_train/rejected': '0.18297', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038945', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-177.57', 'loss/train': '0.70524', 'examples_per_second': '30.44', 'grad_norm': '123', 'counters/examples': 232512, 'counters/updates': 7266}
train stats after 232544 examples: {'rewards_train/chosen': '0.048318', 'rewards_train/rejected': '-0.011374', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059692', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-146.91', 'loss/train': '0.67334', 'examples_per_second': '30.079', 'grad_norm': '90', 'counters/examples': 232544, 'counters/updates': 7267}
train stats after 232576 examples: {'rewards_train/chosen': '0.071638', 'rewards_train/rejected': '-0.027866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099504', 'logps_train/rejected': '-103.31', 'logps_train/chosen': '-139.52', 'loss/train': '0.65337', 'examples_per_second': '32.348', 'grad_norm': '57.75', 'counters/examples': 232576, 'counters/updates': 7268}
train stats after 232608 examples: {'rewards_train/chosen': '0.1376', 'rewards_train/rejected': '-0.019953', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15755', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-146.57', 'loss/train': '0.63036', 'examples_per_second': '31.571', 'grad_norm': '67.5', 'counters/examples': 232608, 'counters/updates': 7269}
train stats after 232640 examples: {'rewards_train/chosen': '0.16813', 'rewards_train/rejected': '0.087597', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08053', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-143.48', 'loss/train': '0.68495', 'examples_per_second': '33.001', 'grad_norm': '65', 'counters/examples': 232640, 'counters/updates': 7270}
train stats after 232672 examples: {'rewards_train/chosen': '0.061157', 'rewards_train/rejected': '0.11488', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053722', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-170.26', 'loss/train': '0.73548', 'examples_per_second': '30.539', 'grad_norm': '89', 'counters/examples': 232672, 'counters/updates': 7271}
train stats after 232704 examples: {'rewards_train/chosen': '0.062985', 'rewards_train/rejected': '-0.0057497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068735', 'logps_train/rejected': '-137.11', 'logps_train/chosen': '-171.71', 'loss/train': '0.66988', 'examples_per_second': '33.035', 'grad_norm': '67.5', 'counters/examples': 232704, 'counters/updates': 7272}
train stats after 232736 examples: {'rewards_train/chosen': '0.058463', 'rewards_train/rejected': '0.16873', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.11027', 'logps_train/rejected': '-204.45', 'logps_train/chosen': '-177.34', 'loss/train': '0.7741', 'examples_per_second': '31.595', 'grad_norm': '166', 'counters/examples': 232736, 'counters/updates': 7273}
train stats after 232768 examples: {'rewards_train/chosen': '0.069211', 'rewards_train/rejected': '0.073812', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0046011', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-136.38', 'loss/train': '0.70279', 'examples_per_second': '32.718', 'grad_norm': '66.5', 'counters/examples': 232768, 'counters/updates': 7274}
train stats after 232800 examples: {'rewards_train/chosen': '0.13365', 'rewards_train/rejected': '-0.0049312', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13858', 'logps_train/rejected': '-99.749', 'logps_train/chosen': '-125.08', 'loss/train': '0.64589', 'examples_per_second': '31.586', 'grad_norm': '102', 'counters/examples': 232800, 'counters/updates': 7275}
train stats after 232832 examples: {'rewards_train/chosen': '0.11025', 'rewards_train/rejected': '0.068348', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041901', 'logps_train/rejected': '-151.62', 'logps_train/chosen': '-192.99', 'loss/train': '0.68827', 'examples_per_second': '30.271', 'grad_norm': '64.5', 'counters/examples': 232832, 'counters/updates': 7276}
skipping logging after 232864 examples to avoid logging too frequently
train stats after 232896 examples: {'rewards_train/chosen': '0.19188', 'rewards_train/rejected': '-0.029999', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22187', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-149.53', 'loss/train': '0.60193', 'examples_per_second': '31.568', 'grad_norm': '85', 'counters/examples': 232896, 'counters/updates': 7278}
train stats after 232928 examples: {'rewards_train/chosen': '0.052893', 'rewards_train/rejected': '0.068222', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01533', 'logps_train/rejected': '-129.76', 'logps_train/chosen': '-151.81', 'loss/train': '0.71375', 'examples_per_second': '31.582', 'grad_norm': '86', 'counters/examples': 232928, 'counters/updates': 7279}
train stats after 232960 examples: {'rewards_train/chosen': '0.061643', 'rewards_train/rejected': '0.020989', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040654', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-139.13', 'loss/train': '0.68256', 'examples_per_second': '32.558', 'grad_norm': '56', 'counters/examples': 232960, 'counters/updates': 7280}
train stats after 232992 examples: {'rewards_train/chosen': '0.050918', 'rewards_train/rejected': '0.048658', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0022604', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-148.97', 'loss/train': '0.6988', 'examples_per_second': '31.251', 'grad_norm': '402', 'counters/examples': 232992, 'counters/updates': 7281}
train stats after 233024 examples: {'rewards_train/chosen': '0.17531', 'rewards_train/rejected': '0.043008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1323', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-149.97', 'loss/train': '0.64422', 'examples_per_second': '31.555', 'grad_norm': '72.5', 'counters/examples': 233024, 'counters/updates': 7282}
train stats after 233056 examples: {'rewards_train/chosen': '0.11548', 'rewards_train/rejected': '0.021127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094352', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-159.8', 'loss/train': '0.66041', 'examples_per_second': '31.913', 'grad_norm': '63.75', 'counters/examples': 233056, 'counters/updates': 7283}
train stats after 233088 examples: {'rewards_train/chosen': '0.066903', 'rewards_train/rejected': '0.0053058', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061597', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-152.56', 'loss/train': '0.67893', 'examples_per_second': '31.907', 'grad_norm': '84', 'counters/examples': 233088, 'counters/updates': 7284}
train stats after 233120 examples: {'rewards_train/chosen': '0.057969', 'rewards_train/rejected': '0.0031124', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054857', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-112.57', 'loss/train': '0.67057', 'examples_per_second': '31.912', 'grad_norm': '64', 'counters/examples': 233120, 'counters/updates': 7285}
train stats after 233152 examples: {'rewards_train/chosen': '0.14666', 'rewards_train/rejected': '-0.048874', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19554', 'logps_train/rejected': '-109.87', 'logps_train/chosen': '-151.84', 'loss/train': '0.60943', 'examples_per_second': '30.159', 'grad_norm': '57', 'counters/examples': 233152, 'counters/updates': 7286}
train stats after 233184 examples: {'rewards_train/chosen': '0.079079', 'rewards_train/rejected': '0.0018105', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077269', 'logps_train/rejected': '-146.53', 'logps_train/chosen': '-144.81', 'loss/train': '0.66581', 'examples_per_second': '31.979', 'grad_norm': '53', 'counters/examples': 233184, 'counters/updates': 7287}
train stats after 233216 examples: {'rewards_train/chosen': '0.19424', 'rewards_train/rejected': '0.085358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10888', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-186.03', 'loss/train': '0.65354', 'examples_per_second': '32.035', 'grad_norm': '115', 'counters/examples': 233216, 'counters/updates': 7288}
train stats after 233248 examples: {'rewards_train/chosen': '0.07762', 'rewards_train/rejected': '0.037157', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040463', 'logps_train/rejected': '-108.88', 'logps_train/chosen': '-124.65', 'loss/train': '0.68669', 'examples_per_second': '31.567', 'grad_norm': '73.5', 'counters/examples': 233248, 'counters/updates': 7289}
train stats after 233280 examples: {'rewards_train/chosen': '0.074267', 'rewards_train/rejected': '0.10653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032264', 'logps_train/rejected': '-122.56', 'logps_train/chosen': '-165.02', 'loss/train': '0.72449', 'examples_per_second': '31.657', 'grad_norm': '113', 'counters/examples': 233280, 'counters/updates': 7290}
train stats after 233312 examples: {'rewards_train/chosen': '0.08898', 'rewards_train/rejected': '0.043892', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045088', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-129.46', 'loss/train': '0.67827', 'examples_per_second': '31.222', 'grad_norm': '83.5', 'counters/examples': 233312, 'counters/updates': 7291}
train stats after 233344 examples: {'rewards_train/chosen': '0.19642', 'rewards_train/rejected': '0.15587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040547', 'logps_train/rejected': '-168.31', 'logps_train/chosen': '-145.22', 'loss/train': '0.68811', 'examples_per_second': '31.599', 'grad_norm': '125.5', 'counters/examples': 233344, 'counters/updates': 7292}
train stats after 233376 examples: {'rewards_train/chosen': '0.0039327', 'rewards_train/rejected': '0.13617', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.13224', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-164.41', 'loss/train': '0.80263', 'examples_per_second': '31.606', 'grad_norm': '101.5', 'counters/examples': 233376, 'counters/updates': 7293}
train stats after 233408 examples: {'rewards_train/chosen': '0.08727', 'rewards_train/rejected': '0.065265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022005', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-160.76', 'loss/train': '0.69753', 'examples_per_second': '31.583', 'grad_norm': '92.5', 'counters/examples': 233408, 'counters/updates': 7294}
train stats after 233440 examples: {'rewards_train/chosen': '0.20854', 'rewards_train/rejected': '0.094984', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11356', 'logps_train/rejected': '-151.98', 'logps_train/chosen': '-167.4', 'loss/train': '0.65287', 'examples_per_second': '30.759', 'grad_norm': '90', 'counters/examples': 233440, 'counters/updates': 7295}
train stats after 233472 examples: {'rewards_train/chosen': '0.10118', 'rewards_train/rejected': '0.090796', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010388', 'logps_train/rejected': '-89.782', 'logps_train/chosen': '-139.54', 'loss/train': '0.69896', 'examples_per_second': '32.844', 'grad_norm': '113.5', 'counters/examples': 233472, 'counters/updates': 7296}
train stats after 233504 examples: {'rewards_train/chosen': '0.065442', 'rewards_train/rejected': '0.10896', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043518', 'logps_train/rejected': '-165.4', 'logps_train/chosen': '-131.79', 'loss/train': '0.73728', 'examples_per_second': '31.595', 'grad_norm': '153', 'counters/examples': 233504, 'counters/updates': 7297}
skipping logging after 233536 examples to avoid logging too frequently
train stats after 233568 examples: {'rewards_train/chosen': '0.063485', 'rewards_train/rejected': '0.097737', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.034252', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-127.91', 'loss/train': '0.71794', 'examples_per_second': '33.999', 'grad_norm': '105.5', 'counters/examples': 233568, 'counters/updates': 7299}
train stats after 233600 examples: {'rewards_train/chosen': '0.11313', 'rewards_train/rejected': '-0.045175', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15831', 'logps_train/rejected': '-158.04', 'logps_train/chosen': '-150.3', 'loss/train': '0.63804', 'examples_per_second': '32.329', 'grad_norm': '166', 'counters/examples': 233600, 'counters/updates': 7300}
train stats after 233632 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '0.046781', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076225', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-135.4', 'loss/train': '0.67666', 'examples_per_second': '30.193', 'grad_norm': '203', 'counters/examples': 233632, 'counters/updates': 7301}
train stats after 233664 examples: {'rewards_train/chosen': '0.095935', 'rewards_train/rejected': '-0.027636', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12357', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-148.74', 'loss/train': '0.64298', 'examples_per_second': '31.811', 'grad_norm': '77', 'counters/examples': 233664, 'counters/updates': 7302}
train stats after 233696 examples: {'rewards_train/chosen': '0.14735', 'rewards_train/rejected': '0.026898', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12045', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-147.26', 'loss/train': '0.65854', 'examples_per_second': '30.095', 'grad_norm': '80.5', 'counters/examples': 233696, 'counters/updates': 7303}
train stats after 233728 examples: {'rewards_train/chosen': '0.014146', 'rewards_train/rejected': '0.069583', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.055437', 'logps_train/rejected': '-176.05', 'logps_train/chosen': '-177.77', 'loss/train': '0.73419', 'examples_per_second': '31.639', 'grad_norm': '76.5', 'counters/examples': 233728, 'counters/updates': 7304}
skipping logging after 233760 examples to avoid logging too frequently
train stats after 233792 examples: {'rewards_train/chosen': '0.076284', 'rewards_train/rejected': '0.070062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0062226', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-164.45', 'loss/train': '0.69869', 'examples_per_second': '31.121', 'grad_norm': '68', 'counters/examples': 233792, 'counters/updates': 7306}
train stats after 233824 examples: {'rewards_train/chosen': '0.027786', 'rewards_train/rejected': '0.121', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.093216', 'logps_train/rejected': '-160.13', 'logps_train/chosen': '-169.28', 'loss/train': '0.75582', 'examples_per_second': '30.872', 'grad_norm': '308', 'counters/examples': 233824, 'counters/updates': 7307}
train stats after 233856 examples: {'rewards_train/chosen': '0.11458', 'rewards_train/rejected': '0.083604', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030977', 'logps_train/rejected': '-118.18', 'logps_train/chosen': '-153.27', 'loss/train': '0.69454', 'examples_per_second': '31.013', 'grad_norm': '197', 'counters/examples': 233856, 'counters/updates': 7308}
train stats after 233888 examples: {'rewards_train/chosen': '0.16674', 'rewards_train/rejected': '0.00058173', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16615', 'logps_train/rejected': '-117.66', 'logps_train/chosen': '-149.59', 'loss/train': '0.62636', 'examples_per_second': '32.8', 'grad_norm': '81', 'counters/examples': 233888, 'counters/updates': 7309}
train stats after 233920 examples: {'rewards_train/chosen': '0.15578', 'rewards_train/rejected': '0.091846', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063929', 'logps_train/rejected': '-176.08', 'logps_train/chosen': '-198.92', 'loss/train': '0.69117', 'examples_per_second': '33.103', 'grad_norm': '122.5', 'counters/examples': 233920, 'counters/updates': 7310}
train stats after 233952 examples: {'rewards_train/chosen': '0.25079', 'rewards_train/rejected': '0.099257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15153', 'logps_train/rejected': '-158.11', 'logps_train/chosen': '-162.99', 'loss/train': '0.65002', 'examples_per_second': '31.205', 'grad_norm': '119.5', 'counters/examples': 233952, 'counters/updates': 7311}
train stats after 233984 examples: {'rewards_train/chosen': '0.11424', 'rewards_train/rejected': '0.020156', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094084', 'logps_train/rejected': '-135.04', 'logps_train/chosen': '-160.47', 'loss/train': '0.6632', 'examples_per_second': '32.391', 'grad_norm': '95', 'counters/examples': 233984, 'counters/updates': 7312}
train stats after 234016 examples: {'rewards_train/chosen': '0.24129', 'rewards_train/rejected': '0.057177', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18411', 'logps_train/rejected': '-149.29', 'logps_train/chosen': '-189.42', 'loss/train': '0.62524', 'examples_per_second': '31.17', 'grad_norm': '87', 'counters/examples': 234016, 'counters/updates': 7313}
train stats after 234048 examples: {'rewards_train/chosen': '0.1983', 'rewards_train/rejected': '0.11554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082753', 'logps_train/rejected': '-172.19', 'logps_train/chosen': '-163.64', 'loss/train': '0.68295', 'examples_per_second': '31.786', 'grad_norm': '69', 'counters/examples': 234048, 'counters/updates': 7314}
train stats after 234080 examples: {'rewards_train/chosen': '0.25162', 'rewards_train/rejected': '0.035006', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21662', 'logps_train/rejected': '-158.89', 'logps_train/chosen': '-191.1', 'loss/train': '0.61441', 'examples_per_second': '30.118', 'grad_norm': '90', 'counters/examples': 234080, 'counters/updates': 7315}
train stats after 234112 examples: {'rewards_train/chosen': '0.23425', 'rewards_train/rejected': '0.036848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1974', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-180.86', 'loss/train': '0.62554', 'examples_per_second': '30.587', 'grad_norm': '52.75', 'counters/examples': 234112, 'counters/updates': 7316}
train stats after 234144 examples: {'rewards_train/chosen': '0.1845', 'rewards_train/rejected': '0.016635', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16787', 'logps_train/rejected': '-108.54', 'logps_train/chosen': '-162.49', 'loss/train': '0.62325', 'examples_per_second': '31.77', 'grad_norm': '188', 'counters/examples': 234144, 'counters/updates': 7317}
train stats after 234176 examples: {'rewards_train/chosen': '-0.019289', 'rewards_train/rejected': '0.111', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.13029', 'logps_train/rejected': '-144.02', 'logps_train/chosen': '-162.17', 'loss/train': '0.77984', 'examples_per_second': '32.049', 'grad_norm': '153', 'counters/examples': 234176, 'counters/updates': 7318}
train stats after 234208 examples: {'rewards_train/chosen': '0.077743', 'rewards_train/rejected': '0.082315', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.004572', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-130.41', 'loss/train': '0.70736', 'examples_per_second': '30.857', 'grad_norm': '91.5', 'counters/examples': 234208, 'counters/updates': 7319}
skipping logging after 234240 examples to avoid logging too frequently
train stats after 234272 examples: {'rewards_train/chosen': '0.074728', 'rewards_train/rejected': '0.094433', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019705', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-120.37', 'loss/train': '0.72355', 'examples_per_second': '31.651', 'grad_norm': '330', 'counters/examples': 234272, 'counters/updates': 7321}
skipping logging after 234304 examples to avoid logging too frequently
train stats after 234336 examples: {'rewards_train/chosen': '0.10548', 'rewards_train/rejected': '0.051376', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054105', 'logps_train/rejected': '-108.1', 'logps_train/chosen': '-165.57', 'loss/train': '0.68342', 'examples_per_second': '25.435', 'grad_norm': '54', 'counters/examples': 234336, 'counters/updates': 7323}
train stats after 234368 examples: {'rewards_train/chosen': '0.0060394', 'rewards_train/rejected': '0.1367', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.13066', 'logps_train/rejected': '-122.67', 'logps_train/chosen': '-169.22', 'loss/train': '0.79136', 'examples_per_second': '30.73', 'grad_norm': '88.5', 'counters/examples': 234368, 'counters/updates': 7324}
skipping logging after 234400 examples to avoid logging too frequently
train stats after 234432 examples: {'rewards_train/chosen': '0.11344', 'rewards_train/rejected': '0.058156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055283', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-150.36', 'loss/train': '0.67864', 'examples_per_second': '36.164', 'grad_norm': '127', 'counters/examples': 234432, 'counters/updates': 7326}
train stats after 234464 examples: {'rewards_train/chosen': '0.098013', 'rewards_train/rejected': '0.039735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058278', 'logps_train/rejected': '-153.74', 'logps_train/chosen': '-141.33', 'loss/train': '0.67622', 'examples_per_second': '31.701', 'grad_norm': '66', 'counters/examples': 234464, 'counters/updates': 7327}
train stats after 234496 examples: {'rewards_train/chosen': '0.14804', 'rewards_train/rejected': '0.046881', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10116', 'logps_train/rejected': '-149.89', 'logps_train/chosen': '-180.4', 'loss/train': '0.66799', 'examples_per_second': '31.583', 'grad_norm': '73.5', 'counters/examples': 234496, 'counters/updates': 7328}
train stats after 234528 examples: {'rewards_train/chosen': '0.092795', 'rewards_train/rejected': '0.02683', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065965', 'logps_train/rejected': '-177.04', 'logps_train/chosen': '-174.78', 'loss/train': '0.679', 'examples_per_second': '30.101', 'grad_norm': '115.5', 'counters/examples': 234528, 'counters/updates': 7329}
train stats after 234560 examples: {'rewards_train/chosen': '0.072086', 'rewards_train/rejected': '0.04742', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024666', 'logps_train/rejected': '-167.37', 'logps_train/chosen': '-150.66', 'loss/train': '0.71835', 'examples_per_second': '31.645', 'grad_norm': '99.5', 'counters/examples': 234560, 'counters/updates': 7330}
train stats after 234592 examples: {'rewards_train/chosen': '0.11259', 'rewards_train/rejected': '0.090349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02224', 'logps_train/rejected': '-136.9', 'logps_train/chosen': '-159.35', 'loss/train': '0.69975', 'examples_per_second': '30.599', 'grad_norm': '101.5', 'counters/examples': 234592, 'counters/updates': 7331}
train stats after 234624 examples: {'rewards_train/chosen': '0.090742', 'rewards_train/rejected': '-0.0128', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10354', 'logps_train/rejected': '-148.43', 'logps_train/chosen': '-143.29', 'loss/train': '0.65972', 'examples_per_second': '31.649', 'grad_norm': '90', 'counters/examples': 234624, 'counters/updates': 7332}
train stats after 234656 examples: {'rewards_train/chosen': '0.085329', 'rewards_train/rejected': '0.021183', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064147', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-166.81', 'loss/train': '0.6792', 'examples_per_second': '30.937', 'grad_norm': '66.5', 'counters/examples': 234656, 'counters/updates': 7333}
train stats after 234688 examples: {'rewards_train/chosen': '0.11164', 'rewards_train/rejected': '0.004993', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10665', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-190.17', 'loss/train': '0.68136', 'examples_per_second': '31.342', 'grad_norm': '77.5', 'counters/examples': 234688, 'counters/updates': 7334}
train stats after 234720 examples: {'rewards_train/chosen': '0.073037', 'rewards_train/rejected': '0.064369', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0086687', 'logps_train/rejected': '-119.97', 'logps_train/chosen': '-146.59', 'loss/train': '0.70769', 'examples_per_second': '30.578', 'grad_norm': '74.5', 'counters/examples': 234720, 'counters/updates': 7335}
skipping logging after 234752 examples to avoid logging too frequently
train stats after 234784 examples: {'rewards_train/chosen': '0.18011', 'rewards_train/rejected': '0.030149', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14996', 'logps_train/rejected': '-116.45', 'logps_train/chosen': '-175.62', 'loss/train': '0.63311', 'examples_per_second': '31.096', 'grad_norm': '60.75', 'counters/examples': 234784, 'counters/updates': 7337}
train stats after 234816 examples: {'rewards_train/chosen': '0.17279', 'rewards_train/rejected': '0.12066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052125', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-153.04', 'loss/train': '0.67657', 'examples_per_second': '30.278', 'grad_norm': '59.5', 'counters/examples': 234816, 'counters/updates': 7338}
train stats after 234848 examples: {'rewards_train/chosen': '0.011749', 'rewards_train/rejected': '0.072008', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.06026', 'logps_train/rejected': '-107.51', 'logps_train/chosen': '-127.53', 'loss/train': '0.73668', 'examples_per_second': '31.67', 'grad_norm': '76', 'counters/examples': 234848, 'counters/updates': 7339}
train stats after 234880 examples: {'rewards_train/chosen': '0.21514', 'rewards_train/rejected': '0.058574', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15657', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-166.18', 'loss/train': '0.64414', 'examples_per_second': '30.697', 'grad_norm': '74.5', 'counters/examples': 234880, 'counters/updates': 7340}
train stats after 234912 examples: {'rewards_train/chosen': '0.13334', 'rewards_train/rejected': '-0.031536', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16487', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-160.47', 'loss/train': '0.62457', 'examples_per_second': '31.645', 'grad_norm': '80', 'counters/examples': 234912, 'counters/updates': 7341}
train stats after 234944 examples: {'rewards_train/chosen': '0.19178', 'rewards_train/rejected': '0.00194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18984', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-165.69', 'loss/train': '0.62148', 'examples_per_second': '31.636', 'grad_norm': '74', 'counters/examples': 234944, 'counters/updates': 7342}
train stats after 234976 examples: {'rewards_train/chosen': '0.043389', 'rewards_train/rejected': '0.0036779', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.039711', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-138.82', 'loss/train': '0.69347', 'examples_per_second': '31.617', 'grad_norm': '98.5', 'counters/examples': 234976, 'counters/updates': 7343}
train stats after 235008 examples: {'rewards_train/chosen': '0.098138', 'rewards_train/rejected': '0.032369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065769', 'logps_train/rejected': '-129.82', 'logps_train/chosen': '-147.61', 'loss/train': '0.66964', 'examples_per_second': '32.568', 'grad_norm': '55.5', 'counters/examples': 235008, 'counters/updates': 7344}
train stats after 235040 examples: {'rewards_train/chosen': '0.073389', 'rewards_train/rejected': '0.032853', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040536', 'logps_train/rejected': '-129.32', 'logps_train/chosen': '-153.64', 'loss/train': '0.68215', 'examples_per_second': '32.45', 'grad_norm': '88', 'counters/examples': 235040, 'counters/updates': 7345}
skipping logging after 235072 examples to avoid logging too frequently
train stats after 235104 examples: {'rewards_train/chosen': '0.095403', 'rewards_train/rejected': '-0.047952', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14336', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-144.17', 'loss/train': '0.6393', 'examples_per_second': '31.301', 'grad_norm': '69.5', 'counters/examples': 235104, 'counters/updates': 7347}
skipping logging after 235136 examples to avoid logging too frequently
train stats after 235168 examples: {'rewards_train/chosen': '0.2485', 'rewards_train/rejected': '0.10363', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14487', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-181.54', 'loss/train': '0.65711', 'examples_per_second': '23.691', 'grad_norm': '61', 'counters/examples': 235168, 'counters/updates': 7349}
train stats after 235200 examples: {'rewards_train/chosen': '0.16902', 'rewards_train/rejected': '0.064369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10466', 'logps_train/rejected': '-134.46', 'logps_train/chosen': '-146.65', 'loss/train': '0.66421', 'examples_per_second': '31.604', 'grad_norm': '232', 'counters/examples': 235200, 'counters/updates': 7350}
train stats after 235232 examples: {'rewards_train/chosen': '0.17544', 'rewards_train/rejected': '-0.014848', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19028', 'logps_train/rejected': '-159.57', 'logps_train/chosen': '-188.85', 'loss/train': '0.62491', 'examples_per_second': '32.011', 'grad_norm': '93.5', 'counters/examples': 235232, 'counters/updates': 7351}
train stats after 235264 examples: {'rewards_train/chosen': '0.11035', 'rewards_train/rejected': '0.025649', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084698', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-131.27', 'loss/train': '0.66429', 'examples_per_second': '23.698', 'grad_norm': '84', 'counters/examples': 235264, 'counters/updates': 7352}
skipping logging after 235296 examples to avoid logging too frequently
train stats after 235328 examples: {'rewards_train/chosen': '0.041631', 'rewards_train/rejected': '-0.011619', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053249', 'logps_train/rejected': '-123.03', 'logps_train/chosen': '-174.33', 'loss/train': '0.67548', 'examples_per_second': '30.71', 'grad_norm': '56.5', 'counters/examples': 235328, 'counters/updates': 7354}
train stats after 235360 examples: {'rewards_train/chosen': '0.091599', 'rewards_train/rejected': '0.05369', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037909', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-114.71', 'loss/train': '0.68129', 'examples_per_second': '32.939', 'grad_norm': '56.25', 'counters/examples': 235360, 'counters/updates': 7355}
train stats after 235392 examples: {'rewards_train/chosen': '0.095272', 'rewards_train/rejected': '0.026871', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068402', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-113.38', 'loss/train': '0.66865', 'examples_per_second': '31.683', 'grad_norm': '55.75', 'counters/examples': 235392, 'counters/updates': 7356}
train stats after 235424 examples: {'rewards_train/chosen': '0.10843', 'rewards_train/rejected': '0.030407', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078028', 'logps_train/rejected': '-119.37', 'logps_train/chosen': '-154.61', 'loss/train': '0.66963', 'examples_per_second': '32.411', 'grad_norm': '137', 'counters/examples': 235424, 'counters/updates': 7357}
train stats after 235456 examples: {'rewards_train/chosen': '0.060912', 'rewards_train/rejected': '0.045952', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014959', 'logps_train/rejected': '-135.37', 'logps_train/chosen': '-158.63', 'loss/train': '0.69462', 'examples_per_second': '31.622', 'grad_norm': '120.5', 'counters/examples': 235456, 'counters/updates': 7358}
train stats after 235488 examples: {'rewards_train/chosen': '0.11056', 'rewards_train/rejected': '-0.044077', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15464', 'logps_train/rejected': '-138.36', 'logps_train/chosen': '-163.49', 'loss/train': '0.62762', 'examples_per_second': '31.371', 'grad_norm': '102', 'counters/examples': 235488, 'counters/updates': 7359}
train stats after 235520 examples: {'rewards_train/chosen': '0.062076', 'rewards_train/rejected': '0.095007', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032931', 'logps_train/rejected': '-147.32', 'logps_train/chosen': '-151.06', 'loss/train': '0.72427', 'examples_per_second': '31.672', 'grad_norm': '75', 'counters/examples': 235520, 'counters/updates': 7360}
train stats after 235552 examples: {'rewards_train/chosen': '0.068815', 'rewards_train/rejected': '0.012731', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056084', 'logps_train/rejected': '-99.345', 'logps_train/chosen': '-164.5', 'loss/train': '0.67677', 'examples_per_second': '30.626', 'grad_norm': '66.5', 'counters/examples': 235552, 'counters/updates': 7361}
skipping logging after 235584 examples to avoid logging too frequently
train stats after 235616 examples: {'rewards_train/chosen': '0.104', 'rewards_train/rejected': '0.0095113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094491', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-144.65', 'loss/train': '0.66054', 'examples_per_second': '31.078', 'grad_norm': '96', 'counters/examples': 235616, 'counters/updates': 7363}
train stats after 235648 examples: {'rewards_train/chosen': '0.060781', 'rewards_train/rejected': '-0.032918', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093699', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-123.94', 'loss/train': '0.65936', 'examples_per_second': '31.682', 'grad_norm': '79.5', 'counters/examples': 235648, 'counters/updates': 7364}
skipping logging after 235680 examples to avoid logging too frequently
train stats after 235712 examples: {'rewards_train/chosen': '0.083707', 'rewards_train/rejected': '0.03057', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053136', 'logps_train/rejected': '-165.48', 'logps_train/chosen': '-180.79', 'loss/train': '0.68465', 'examples_per_second': '30.695', 'grad_norm': '84.5', 'counters/examples': 235712, 'counters/updates': 7366}
train stats after 235744 examples: {'rewards_train/chosen': '0.081211', 'rewards_train/rejected': '0.12664', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045428', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-134.95', 'loss/train': '0.73068', 'examples_per_second': '32.514', 'grad_norm': '94.5', 'counters/examples': 235744, 'counters/updates': 7367}
skipping logging after 235776 examples to avoid logging too frequently
train stats after 235808 examples: {'rewards_train/chosen': '0.10343', 'rewards_train/rejected': '-0.048182', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15161', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-143.41', 'loss/train': '0.62972', 'examples_per_second': '31.166', 'grad_norm': '56.5', 'counters/examples': 235808, 'counters/updates': 7369}
train stats after 235840 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '0.1286', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013683', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-173.28', 'loss/train': '0.71628', 'examples_per_second': '32.49', 'grad_norm': '130', 'counters/examples': 235840, 'counters/updates': 7370}
train stats after 235872 examples: {'rewards_train/chosen': '0.10223', 'rewards_train/rejected': '0.046275', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055954', 'logps_train/rejected': '-154.72', 'logps_train/chosen': '-167.24', 'loss/train': '0.68336', 'examples_per_second': '32.229', 'grad_norm': '81.5', 'counters/examples': 235872, 'counters/updates': 7371}
skipping logging after 235904 examples to avoid logging too frequently
train stats after 235936 examples: {'rewards_train/chosen': '0.2273', 'rewards_train/rejected': '0.075266', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15204', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-174.68', 'loss/train': '0.63151', 'examples_per_second': '32.544', 'grad_norm': '107', 'counters/examples': 235936, 'counters/updates': 7373}
skipping logging after 235968 examples to avoid logging too frequently
train stats after 236000 examples: {'rewards_train/chosen': '0.05212', 'rewards_train/rejected': '0.023912', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028209', 'logps_train/rejected': '-96.769', 'logps_train/chosen': '-119.06', 'loss/train': '0.68587', 'examples_per_second': '30.553', 'grad_norm': '85', 'counters/examples': 236000, 'counters/updates': 7375}
skipping logging after 236032 examples to avoid logging too frequently
train stats after 236064 examples: {'rewards_train/chosen': '0.099452', 'rewards_train/rejected': '0.10178', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0023314', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-159.08', 'loss/train': '0.71276', 'examples_per_second': '30.564', 'grad_norm': '252', 'counters/examples': 236064, 'counters/updates': 7377}
train stats after 236096 examples: {'rewards_train/chosen': '0.15139', 'rewards_train/rejected': '0.02517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12622', 'logps_train/rejected': '-119.92', 'logps_train/chosen': '-142.54', 'loss/train': '0.65355', 'examples_per_second': '32.554', 'grad_norm': '96', 'counters/examples': 236096, 'counters/updates': 7378}
train stats after 236128 examples: {'rewards_train/chosen': '0.18406', 'rewards_train/rejected': '-0.053263', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23733', 'logps_train/rejected': '-121', 'logps_train/chosen': '-206.28', 'loss/train': '0.59586', 'examples_per_second': '31.242', 'grad_norm': '87', 'counters/examples': 236128, 'counters/updates': 7379}
skipping logging after 236160 examples to avoid logging too frequently
train stats after 236192 examples: {'rewards_train/chosen': '0.15387', 'rewards_train/rejected': '0.09429', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.059577', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-171.68', 'loss/train': '0.68149', 'examples_per_second': '31.299', 'grad_norm': '84', 'counters/examples': 236192, 'counters/updates': 7381}
skipping logging after 236224 examples to avoid logging too frequently
train stats after 236256 examples: {'rewards_train/chosen': '0.15878', 'rewards_train/rejected': '0.18556', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026782', 'logps_train/rejected': '-157.62', 'logps_train/chosen': '-136.94', 'loss/train': '0.7197', 'examples_per_second': '30.22', 'grad_norm': '85', 'counters/examples': 236256, 'counters/updates': 7383}
train stats after 236288 examples: {'rewards_train/chosen': '0.16263', 'rewards_train/rejected': '-0.01718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17981', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-137.72', 'loss/train': '0.62289', 'examples_per_second': '30.639', 'grad_norm': '94', 'counters/examples': 236288, 'counters/updates': 7384}
train stats after 236320 examples: {'rewards_train/chosen': '0.076818', 'rewards_train/rejected': '0.12021', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043393', 'logps_train/rejected': '-117.58', 'logps_train/chosen': '-154.63', 'loss/train': '0.72996', 'examples_per_second': '30.677', 'grad_norm': '78', 'counters/examples': 236320, 'counters/updates': 7385}
train stats after 236352 examples: {'rewards_train/chosen': '0.012304', 'rewards_train/rejected': '0.059667', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047364', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-130.83', 'loss/train': '0.73443', 'examples_per_second': '32.791', 'grad_norm': '97', 'counters/examples': 236352, 'counters/updates': 7386}
train stats after 236384 examples: {'rewards_train/chosen': '0.097896', 'rewards_train/rejected': '0.056312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041584', 'logps_train/rejected': '-161.05', 'logps_train/chosen': '-155.64', 'loss/train': '0.69389', 'examples_per_second': '31.574', 'grad_norm': '93.5', 'counters/examples': 236384, 'counters/updates': 7387}
train stats after 236416 examples: {'rewards_train/chosen': '0.14627', 'rewards_train/rejected': '0.13617', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.010091', 'logps_train/rejected': '-148.44', 'logps_train/chosen': '-134.67', 'loss/train': '0.69698', 'examples_per_second': '32.147', 'grad_norm': '66.5', 'counters/examples': 236416, 'counters/updates': 7388}
train stats after 236448 examples: {'rewards_train/chosen': '0.043512', 'rewards_train/rejected': '0.074801', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031289', 'logps_train/rejected': '-152.46', 'logps_train/chosen': '-173.1', 'loss/train': '0.72553', 'examples_per_second': '31.624', 'grad_norm': '103', 'counters/examples': 236448, 'counters/updates': 7389}
train stats after 236480 examples: {'rewards_train/chosen': '0.19842', 'rewards_train/rejected': '0.10134', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09708', 'logps_train/rejected': '-158.45', 'logps_train/chosen': '-167.67', 'loss/train': '0.66148', 'examples_per_second': '31.657', 'grad_norm': '132', 'counters/examples': 236480, 'counters/updates': 7390}
train stats after 236512 examples: {'rewards_train/chosen': '0.11516', 'rewards_train/rejected': '0.049031', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066132', 'logps_train/rejected': '-104.75', 'logps_train/chosen': '-163.01', 'loss/train': '0.67686', 'examples_per_second': '30.169', 'grad_norm': '66', 'counters/examples': 236512, 'counters/updates': 7391}
train stats after 236544 examples: {'rewards_train/chosen': '0.15143', 'rewards_train/rejected': '0.090151', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061283', 'logps_train/rejected': '-158.86', 'logps_train/chosen': '-175.08', 'loss/train': '0.67718', 'examples_per_second': '31.343', 'grad_norm': '102', 'counters/examples': 236544, 'counters/updates': 7392}
train stats after 236576 examples: {'rewards_train/chosen': '0.1773', 'rewards_train/rejected': '0.091725', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085579', 'logps_train/rejected': '-117.91', 'logps_train/chosen': '-170.95', 'loss/train': '0.67232', 'examples_per_second': '30.541', 'grad_norm': '113.5', 'counters/examples': 236576, 'counters/updates': 7393}
train stats after 236608 examples: {'rewards_train/chosen': '0.13745', 'rewards_train/rejected': '0.12796', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0094872', 'logps_train/rejected': '-161.63', 'logps_train/chosen': '-163.91', 'loss/train': '0.71214', 'examples_per_second': '31.676', 'grad_norm': '162', 'counters/examples': 236608, 'counters/updates': 7394}
train stats after 236640 examples: {'rewards_train/chosen': '0.079304', 'rewards_train/rejected': '0.0058226', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073482', 'logps_train/rejected': '-139.93', 'logps_train/chosen': '-142.99', 'loss/train': '0.67689', 'examples_per_second': '32.474', 'grad_norm': '55.5', 'counters/examples': 236640, 'counters/updates': 7395}
skipping logging after 236672 examples to avoid logging too frequently
train stats after 236704 examples: {'rewards_train/chosen': '0.06271', 'rewards_train/rejected': '0.0049778', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057732', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-132.03', 'loss/train': '0.67908', 'examples_per_second': '31.618', 'grad_norm': '62.5', 'counters/examples': 236704, 'counters/updates': 7397}
skipping logging after 236736 examples to avoid logging too frequently
train stats after 236768 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '0.07392', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041001', 'logps_train/rejected': '-114.01', 'logps_train/chosen': '-157.97', 'loss/train': '0.68496', 'examples_per_second': '31.625', 'grad_norm': '103.5', 'counters/examples': 236768, 'counters/updates': 7399}
skipping logging after 236800 examples to avoid logging too frequently
train stats after 236832 examples: {'rewards_train/chosen': '0.081865', 'rewards_train/rejected': '0.10006', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018198', 'logps_train/rejected': '-127.95', 'logps_train/chosen': '-149.17', 'loss/train': '0.71636', 'examples_per_second': '31.602', 'grad_norm': '106.5', 'counters/examples': 236832, 'counters/updates': 7401}
train stats after 236864 examples: {'rewards_train/chosen': '0.094643', 'rewards_train/rejected': '0.053003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04164', 'logps_train/rejected': '-93.38', 'logps_train/chosen': '-158.21', 'loss/train': '0.68142', 'examples_per_second': '32.594', 'grad_norm': '72.5', 'counters/examples': 236864, 'counters/updates': 7402}
skipping logging after 236896 examples to avoid logging too frequently
train stats after 236928 examples: {'rewards_train/chosen': '0.17245', 'rewards_train/rejected': '0.091745', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080704', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-144.44', 'loss/train': '0.66494', 'examples_per_second': '30.194', 'grad_norm': '118.5', 'counters/examples': 236928, 'counters/updates': 7404}
train stats after 236960 examples: {'rewards_train/chosen': '0.07602', 'rewards_train/rejected': '0.019319', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056701', 'logps_train/rejected': '-120.37', 'logps_train/chosen': '-149.19', 'loss/train': '0.6707', 'examples_per_second': '30.615', 'grad_norm': '75', 'counters/examples': 236960, 'counters/updates': 7405}
train stats after 236992 examples: {'rewards_train/chosen': '0.060709', 'rewards_train/rejected': '0.023721', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036988', 'logps_train/rejected': '-131.43', 'logps_train/chosen': '-147.53', 'loss/train': '0.68429', 'examples_per_second': '32.934', 'grad_norm': '103.5', 'counters/examples': 236992, 'counters/updates': 7406}
train stats after 237024 examples: {'rewards_train/chosen': '0.1223', 'rewards_train/rejected': '0.023851', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098451', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-191.01', 'loss/train': '0.65306', 'examples_per_second': '32.085', 'grad_norm': '376', 'counters/examples': 237024, 'counters/updates': 7407}
train stats after 237056 examples: {'rewards_train/chosen': '0.073661', 'rewards_train/rejected': '0.10915', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.035491', 'logps_train/rejected': '-150.98', 'logps_train/chosen': '-158.39', 'loss/train': '0.74317', 'examples_per_second': '32.253', 'grad_norm': '104', 'counters/examples': 237056, 'counters/updates': 7408}
train stats after 237088 examples: {'rewards_train/chosen': '0.1042', 'rewards_train/rejected': '0.021471', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082732', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-155.17', 'loss/train': '0.6615', 'examples_per_second': '32.573', 'grad_norm': '66.5', 'counters/examples': 237088, 'counters/updates': 7409}
skipping logging after 237120 examples to avoid logging too frequently
train stats after 237152 examples: {'rewards_train/chosen': '0.18814', 'rewards_train/rejected': '0.0082166', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17992', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-131.19', 'loss/train': '0.62012', 'examples_per_second': '31.542', 'grad_norm': '116', 'counters/examples': 237152, 'counters/updates': 7411}
train stats after 237184 examples: {'rewards_train/chosen': '0.080761', 'rewards_train/rejected': '0.084386', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0036258', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-139.83', 'loss/train': '0.70342', 'examples_per_second': '30.617', 'grad_norm': '126.5', 'counters/examples': 237184, 'counters/updates': 7412}
train stats after 237216 examples: {'rewards_train/chosen': '0.20292', 'rewards_train/rejected': '0.058856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14406', 'logps_train/rejected': '-124.23', 'logps_train/chosen': '-132.2', 'loss/train': '0.6529', 'examples_per_second': '31.65', 'grad_norm': '92', 'counters/examples': 237216, 'counters/updates': 7413}
skipping logging after 237248 examples to avoid logging too frequently
train stats after 237280 examples: {'rewards_train/chosen': '0.06503', 'rewards_train/rejected': '0.0086154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056415', 'logps_train/rejected': '-100.59', 'logps_train/chosen': '-88.431', 'loss/train': '0.67589', 'examples_per_second': '30.659', 'grad_norm': '60.5', 'counters/examples': 237280, 'counters/updates': 7415}
train stats after 237312 examples: {'rewards_train/chosen': '0.18123', 'rewards_train/rejected': '0.072578', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10865', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-170.79', 'loss/train': '0.66062', 'examples_per_second': '31.979', 'grad_norm': '66', 'counters/examples': 237312, 'counters/updates': 7416}
train stats after 237344 examples: {'rewards_train/chosen': '0.13682', 'rewards_train/rejected': '0.082817', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053999', 'logps_train/rejected': '-103.65', 'logps_train/chosen': '-149.37', 'loss/train': '0.67896', 'examples_per_second': '31.711', 'grad_norm': '99', 'counters/examples': 237344, 'counters/updates': 7417}
skipping logging after 237376 examples to avoid logging too frequently
train stats after 237408 examples: {'rewards_train/chosen': '0.18979', 'rewards_train/rejected': '0.13505', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054746', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-145.07', 'loss/train': '0.68805', 'examples_per_second': '30.192', 'grad_norm': '80.5', 'counters/examples': 237408, 'counters/updates': 7419}
train stats after 237440 examples: {'rewards_train/chosen': '0.14219', 'rewards_train/rejected': '0.054613', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08758', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-160.24', 'loss/train': '0.67247', 'examples_per_second': '31.647', 'grad_norm': '147', 'counters/examples': 237440, 'counters/updates': 7420}
train stats after 237472 examples: {'rewards_train/chosen': '0.15796', 'rewards_train/rejected': '0.12859', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02937', 'logps_train/rejected': '-140.93', 'logps_train/chosen': '-184.51', 'loss/train': '0.70372', 'examples_per_second': '30.789', 'grad_norm': '70.5', 'counters/examples': 237472, 'counters/updates': 7421}
train stats after 237504 examples: {'rewards_train/chosen': '0.056393', 'rewards_train/rejected': '0.017929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038464', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-119.55', 'loss/train': '0.68146', 'examples_per_second': '32.741', 'grad_norm': '67', 'counters/examples': 237504, 'counters/updates': 7422}
train stats after 237536 examples: {'rewards_train/chosen': '0.13453', 'rewards_train/rejected': '0.0549', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079629', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-154.86', 'loss/train': '0.67278', 'examples_per_second': '31.323', 'grad_norm': '53', 'counters/examples': 237536, 'counters/updates': 7423}
train stats after 237568 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.009287', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13242', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-131.61', 'loss/train': '0.64208', 'examples_per_second': '30.916', 'grad_norm': '89.5', 'counters/examples': 237568, 'counters/updates': 7424}
train stats after 237600 examples: {'rewards_train/chosen': '0.15866', 'rewards_train/rejected': '0.016707', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14196', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-151.61', 'loss/train': '0.63129', 'examples_per_second': '29.931', 'grad_norm': '99.5', 'counters/examples': 237600, 'counters/updates': 7425}
train stats after 237632 examples: {'rewards_train/chosen': '0.12498', 'rewards_train/rejected': '0.074234', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05075', 'logps_train/rejected': '-121.75', 'logps_train/chosen': '-164.81', 'loss/train': '0.68582', 'examples_per_second': '30.318', 'grad_norm': '56.5', 'counters/examples': 237632, 'counters/updates': 7426}
skipping logging after 237664 examples to avoid logging too frequently
train stats after 237696 examples: {'rewards_train/chosen': '0.11196', 'rewards_train/rejected': '0.051952', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060012', 'logps_train/rejected': '-130.19', 'logps_train/chosen': '-121.53', 'loss/train': '0.6735', 'examples_per_second': '31.661', 'grad_norm': '63.5', 'counters/examples': 237696, 'counters/updates': 7428}
train stats after 237728 examples: {'rewards_train/chosen': '0.11814', 'rewards_train/rejected': '0.042109', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076026', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-112.43', 'loss/train': '0.66906', 'examples_per_second': '31.676', 'grad_norm': '72', 'counters/examples': 237728, 'counters/updates': 7429}
skipping logging after 237760 examples to avoid logging too frequently
train stats after 237792 examples: {'rewards_train/chosen': '0.13671', 'rewards_train/rejected': '0.028223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10848', 'logps_train/rejected': '-156.28', 'logps_train/chosen': '-154.56', 'loss/train': '0.65465', 'examples_per_second': '31.402', 'grad_norm': '82', 'counters/examples': 237792, 'counters/updates': 7431}
train stats after 237824 examples: {'rewards_train/chosen': '0.1776', 'rewards_train/rejected': '0.088636', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.088967', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-154.66', 'loss/train': '0.664', 'examples_per_second': '31.289', 'grad_norm': '133', 'counters/examples': 237824, 'counters/updates': 7432}
train stats after 237856 examples: {'rewards_train/chosen': '0.18551', 'rewards_train/rejected': '-0.01629', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2018', 'logps_train/rejected': '-157.04', 'logps_train/chosen': '-166.7', 'loss/train': '0.60961', 'examples_per_second': '31.665', 'grad_norm': '100', 'counters/examples': 237856, 'counters/updates': 7433}
train stats after 237888 examples: {'rewards_train/chosen': '0.081877', 'rewards_train/rejected': '0.035368', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046509', 'logps_train/rejected': '-149.34', 'logps_train/chosen': '-170.64', 'loss/train': '0.68309', 'examples_per_second': '31.622', 'grad_norm': '78', 'counters/examples': 237888, 'counters/updates': 7434}
train stats after 237920 examples: {'rewards_train/chosen': '0.11373', 'rewards_train/rejected': '0.058716', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05501', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-172.34', 'loss/train': '0.67813', 'examples_per_second': '31.848', 'grad_norm': '69.5', 'counters/examples': 237920, 'counters/updates': 7435}
skipping logging after 237952 examples to avoid logging too frequently
train stats after 237984 examples: {'rewards_train/chosen': '0.16242', 'rewards_train/rejected': '0.052349', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11007', 'logps_train/rejected': '-164.25', 'logps_train/chosen': '-134.57', 'loss/train': '0.6538', 'examples_per_second': '33.325', 'grad_norm': '93', 'counters/examples': 237984, 'counters/updates': 7437}
skipping logging after 238016 examples to avoid logging too frequently
train stats after 238048 examples: {'rewards_train/chosen': '0.15325', 'rewards_train/rejected': '0.03512', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11813', 'logps_train/rejected': '-106.09', 'logps_train/chosen': '-109.87', 'loss/train': '0.64891', 'examples_per_second': '33.995', 'grad_norm': '54.5', 'counters/examples': 238048, 'counters/updates': 7439}
train stats after 238080 examples: {'rewards_train/chosen': '0.16718', 'rewards_train/rejected': '0.027784', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1394', 'logps_train/rejected': '-105.62', 'logps_train/chosen': '-156.53', 'loss/train': '0.64045', 'examples_per_second': '33.271', 'grad_norm': '160', 'counters/examples': 238080, 'counters/updates': 7440}
train stats after 238112 examples: {'rewards_train/chosen': '0.10897', 'rewards_train/rejected': '0.11916', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010189', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-150.85', 'loss/train': '0.71615', 'examples_per_second': '32.61', 'grad_norm': '103', 'counters/examples': 238112, 'counters/updates': 7441}
train stats after 238144 examples: {'rewards_train/chosen': '0.071095', 'rewards_train/rejected': '0.024503', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046592', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-155.59', 'loss/train': '0.68429', 'examples_per_second': '30.344', 'grad_norm': '149', 'counters/examples': 238144, 'counters/updates': 7442}
train stats after 238176 examples: {'rewards_train/chosen': '0.078703', 'rewards_train/rejected': '0.050058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028645', 'logps_train/rejected': '-118.08', 'logps_train/chosen': '-164.51', 'loss/train': '0.68947', 'examples_per_second': '31.607', 'grad_norm': '102.5', 'counters/examples': 238176, 'counters/updates': 7443}
train stats after 238208 examples: {'rewards_train/chosen': '0.071907', 'rewards_train/rejected': '0.079772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.007865', 'logps_train/rejected': '-131.44', 'logps_train/chosen': '-135.06', 'loss/train': '0.73671', 'examples_per_second': '31.002', 'grad_norm': '310', 'counters/examples': 238208, 'counters/updates': 7444}
train stats after 238240 examples: {'rewards_train/chosen': '0.096993', 'rewards_train/rejected': '0.012052', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084942', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-196.08', 'loss/train': '0.67245', 'examples_per_second': '30.429', 'grad_norm': '185', 'counters/examples': 238240, 'counters/updates': 7445}
train stats after 238272 examples: {'rewards_train/chosen': '0.13165', 'rewards_train/rejected': '0.051017', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080632', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-163.87', 'loss/train': '0.66934', 'examples_per_second': '30.84', 'grad_norm': '59', 'counters/examples': 238272, 'counters/updates': 7446}
train stats after 238304 examples: {'rewards_train/chosen': '0.10225', 'rewards_train/rejected': '-0.0043895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10664', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-158.71', 'loss/train': '0.65479', 'examples_per_second': '31.579', 'grad_norm': '110', 'counters/examples': 238304, 'counters/updates': 7447}
train stats after 238336 examples: {'rewards_train/chosen': '0.12828', 'rewards_train/rejected': '0.0067423', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12153', 'logps_train/rejected': '-138.24', 'logps_train/chosen': '-182.31', 'loss/train': '0.65948', 'examples_per_second': '32.176', 'grad_norm': '131', 'counters/examples': 238336, 'counters/updates': 7448}
train stats after 238368 examples: {'rewards_train/chosen': '0.18971', 'rewards_train/rejected': '0.016967', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17274', 'logps_train/rejected': '-162.38', 'logps_train/chosen': '-172.72', 'loss/train': '0.62573', 'examples_per_second': '32.202', 'grad_norm': '72', 'counters/examples': 238368, 'counters/updates': 7449}
train stats after 238400 examples: {'rewards_train/chosen': '0.10678', 'rewards_train/rejected': '0.12377', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016997', 'logps_train/rejected': '-146.46', 'logps_train/chosen': '-112.78', 'loss/train': '0.70741', 'examples_per_second': '32.089', 'grad_norm': '56', 'counters/examples': 238400, 'counters/updates': 7450}
train stats after 238432 examples: {'rewards_train/chosen': '0.18695', 'rewards_train/rejected': '0.032258', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15469', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-169.16', 'loss/train': '0.63024', 'examples_per_second': '30.771', 'grad_norm': '107', 'counters/examples': 238432, 'counters/updates': 7451}
train stats after 238464 examples: {'rewards_train/chosen': '0.11192', 'rewards_train/rejected': '0.065534', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046389', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-130.77', 'loss/train': '0.68197', 'examples_per_second': '32.568', 'grad_norm': '90', 'counters/examples': 238464, 'counters/updates': 7452}
train stats after 238496 examples: {'rewards_train/chosen': '0.14592', 'rewards_train/rejected': '0.015679', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13024', 'logps_train/rejected': '-158.84', 'logps_train/chosen': '-132.39', 'loss/train': '0.65234', 'examples_per_second': '30.722', 'grad_norm': '112', 'counters/examples': 238496, 'counters/updates': 7453}
train stats after 238528 examples: {'rewards_train/chosen': '0.11543', 'rewards_train/rejected': '-0.021957', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13739', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-132.25', 'loss/train': '0.64948', 'examples_per_second': '30.863', 'grad_norm': '86', 'counters/examples': 238528, 'counters/updates': 7454}
train stats after 238560 examples: {'rewards_train/chosen': '0.13238', 'rewards_train/rejected': '0.038198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094177', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-160.82', 'loss/train': '0.65813', 'examples_per_second': '31.026', 'grad_norm': '77', 'counters/examples': 238560, 'counters/updates': 7455}
train stats after 238592 examples: {'rewards_train/chosen': '0.0082622', 'rewards_train/rejected': '0.044523', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03626', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-140.74', 'loss/train': '0.72608', 'examples_per_second': '32.554', 'grad_norm': '79.5', 'counters/examples': 238592, 'counters/updates': 7456}
train stats after 238624 examples: {'rewards_train/chosen': '0.16928', 'rewards_train/rejected': '0.038237', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13104', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-158.5', 'loss/train': '0.64219', 'examples_per_second': '31.912', 'grad_norm': '70.5', 'counters/examples': 238624, 'counters/updates': 7457}
train stats after 238656 examples: {'rewards_train/chosen': '0.21639', 'rewards_train/rejected': '0.021901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19449', 'logps_train/rejected': '-137.87', 'logps_train/chosen': '-166.43', 'loss/train': '0.61726', 'examples_per_second': '31.027', 'grad_norm': '101.5', 'counters/examples': 238656, 'counters/updates': 7458}
train stats after 238688 examples: {'rewards_train/chosen': '0.13538', 'rewards_train/rejected': '0.070382', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064998', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-129.24', 'loss/train': '0.67834', 'examples_per_second': '31.824', 'grad_norm': '69.5', 'counters/examples': 238688, 'counters/updates': 7459}
train stats after 238720 examples: {'rewards_train/chosen': '0.11771', 'rewards_train/rejected': '0.16824', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.050537', 'logps_train/rejected': '-168.92', 'logps_train/chosen': '-143.66', 'loss/train': '0.73489', 'examples_per_second': '31.606', 'grad_norm': '73.5', 'counters/examples': 238720, 'counters/updates': 7460}
train stats after 238752 examples: {'rewards_train/chosen': '0.091855', 'rewards_train/rejected': '0.0426', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049255', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-138.22', 'loss/train': '0.67776', 'examples_per_second': '31.581', 'grad_norm': '122', 'counters/examples': 238752, 'counters/updates': 7461}
train stats after 238784 examples: {'rewards_train/chosen': '0.084461', 'rewards_train/rejected': '0.0068589', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077602', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-161.76', 'loss/train': '0.66755', 'examples_per_second': '31.092', 'grad_norm': '86', 'counters/examples': 238784, 'counters/updates': 7462}
train stats after 238816 examples: {'rewards_train/chosen': '0.19996', 'rewards_train/rejected': '0.095101', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10486', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-157.68', 'loss/train': '0.66428', 'examples_per_second': '31.628', 'grad_norm': '153', 'counters/examples': 238816, 'counters/updates': 7463}
train stats after 238848 examples: {'rewards_train/chosen': '0.10542', 'rewards_train/rejected': '0.17602', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0706', 'logps_train/rejected': '-148.02', 'logps_train/chosen': '-169.73', 'loss/train': '0.77227', 'examples_per_second': '30.702', 'grad_norm': '278', 'counters/examples': 238848, 'counters/updates': 7464}
train stats after 238880 examples: {'rewards_train/chosen': '0.038549', 'rewards_train/rejected': '0.031735', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0068133', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-116.46', 'loss/train': '0.6995', 'examples_per_second': '31.631', 'grad_norm': '54.5', 'counters/examples': 238880, 'counters/updates': 7465}
skipping logging after 238912 examples to avoid logging too frequently
train stats after 238944 examples: {'rewards_train/chosen': '0.14278', 'rewards_train/rejected': '0.012928', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12985', 'logps_train/rejected': '-107.3', 'logps_train/chosen': '-124.28', 'loss/train': '0.64275', 'examples_per_second': '30.816', 'grad_norm': '45.5', 'counters/examples': 238944, 'counters/updates': 7467}
train stats after 238976 examples: {'rewards_train/chosen': '0.19179', 'rewards_train/rejected': '0.019305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17249', 'logps_train/rejected': '-143.63', 'logps_train/chosen': '-204.09', 'loss/train': '0.63746', 'examples_per_second': '31.539', 'grad_norm': '115.5', 'counters/examples': 238976, 'counters/updates': 7468}
train stats after 239008 examples: {'rewards_train/chosen': '0.083258', 'rewards_train/rejected': '0.10948', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.026218', 'logps_train/rejected': '-133.02', 'logps_train/chosen': '-162.81', 'loss/train': '0.7282', 'examples_per_second': '31.95', 'grad_norm': '139', 'counters/examples': 239008, 'counters/updates': 7469}
train stats after 239040 examples: {'rewards_train/chosen': '0.16106', 'rewards_train/rejected': '0.023925', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13713', 'logps_train/rejected': '-76.417', 'logps_train/chosen': '-128.52', 'loss/train': '0.63325', 'examples_per_second': '31.977', 'grad_norm': '113', 'counters/examples': 239040, 'counters/updates': 7470}
train stats after 239072 examples: {'rewards_train/chosen': '0.14037', 'rewards_train/rejected': '0.071474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068895', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-175.5', 'loss/train': '0.66948', 'examples_per_second': '31.604', 'grad_norm': '99', 'counters/examples': 239072, 'counters/updates': 7471}
train stats after 239104 examples: {'rewards_train/chosen': '0.16645', 'rewards_train/rejected': '0.017489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14896', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-155.26', 'loss/train': '0.63598', 'examples_per_second': '30.602', 'grad_norm': '56.5', 'counters/examples': 239104, 'counters/updates': 7472}
train stats after 239136 examples: {'rewards_train/chosen': '0.19049', 'rewards_train/rejected': '-0.010498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20099', 'logps_train/rejected': '-163.31', 'logps_train/chosen': '-160.65', 'loss/train': '0.61721', 'examples_per_second': '29.745', 'grad_norm': '134', 'counters/examples': 239136, 'counters/updates': 7473}
train stats after 239168 examples: {'rewards_train/chosen': '0.1273', 'rewards_train/rejected': '0.01711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11019', 'logps_train/rejected': '-112.47', 'logps_train/chosen': '-159.26', 'loss/train': '0.65722', 'examples_per_second': '32.734', 'grad_norm': '154', 'counters/examples': 239168, 'counters/updates': 7474}
train stats after 239200 examples: {'rewards_train/chosen': '0.090158', 'rewards_train/rejected': '0.031491', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058667', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-151.8', 'loss/train': '0.6811', 'examples_per_second': '30.675', 'grad_norm': '140', 'counters/examples': 239200, 'counters/updates': 7475}
train stats after 239232 examples: {'rewards_train/chosen': '0.20105', 'rewards_train/rejected': '0.11796', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083092', 'logps_train/rejected': '-187.02', 'logps_train/chosen': '-183.73', 'loss/train': '0.66613', 'examples_per_second': '31.629', 'grad_norm': '92', 'counters/examples': 239232, 'counters/updates': 7476}
train stats after 239264 examples: {'rewards_train/chosen': '0.18816', 'rewards_train/rejected': '-0.0090015', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19716', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-167.84', 'loss/train': '0.63381', 'examples_per_second': '30.067', 'grad_norm': '84.5', 'counters/examples': 239264, 'counters/updates': 7477}
train stats after 239296 examples: {'rewards_train/chosen': '0.11322', 'rewards_train/rejected': '0.08416', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029056', 'logps_train/rejected': '-140.73', 'logps_train/chosen': '-152.41', 'loss/train': '0.70231', 'examples_per_second': '33.094', 'grad_norm': '378', 'counters/examples': 239296, 'counters/updates': 7478}
train stats after 239328 examples: {'rewards_train/chosen': '0.12924', 'rewards_train/rejected': '0.089663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039581', 'logps_train/rejected': '-106.69', 'logps_train/chosen': '-146.78', 'loss/train': '0.68324', 'examples_per_second': '31.122', 'grad_norm': '74', 'counters/examples': 239328, 'counters/updates': 7479}
train stats after 239360 examples: {'rewards_train/chosen': '0.10697', 'rewards_train/rejected': '0.030627', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.076342', 'logps_train/rejected': '-166.82', 'logps_train/chosen': '-172.57', 'loss/train': '0.67308', 'examples_per_second': '31.96', 'grad_norm': '78', 'counters/examples': 239360, 'counters/updates': 7480}
train stats after 239392 examples: {'rewards_train/chosen': '0.15257', 'rewards_train/rejected': '0.097607', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054966', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-187.95', 'loss/train': '0.6742', 'examples_per_second': '31.545', 'grad_norm': '233', 'counters/examples': 239392, 'counters/updates': 7481}
skipping logging after 239424 examples to avoid logging too frequently
train stats after 239456 examples: {'rewards_train/chosen': '0.026579', 'rewards_train/rejected': '0.051813', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025234', 'logps_train/rejected': '-150.84', 'logps_train/chosen': '-148.14', 'loss/train': '0.72805', 'examples_per_second': '30.236', 'grad_norm': '106', 'counters/examples': 239456, 'counters/updates': 7483}
train stats after 239488 examples: {'rewards_train/chosen': '0.061662', 'rewards_train/rejected': '0.081961', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020298', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-145.6', 'loss/train': '0.72283', 'examples_per_second': '31.606', 'grad_norm': '94', 'counters/examples': 239488, 'counters/updates': 7484}
train stats after 239520 examples: {'rewards_train/chosen': '0.079919', 'rewards_train/rejected': '0.050871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029049', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-160.19', 'loss/train': '0.6974', 'examples_per_second': '31.693', 'grad_norm': '88', 'counters/examples': 239520, 'counters/updates': 7485}
train stats after 239552 examples: {'rewards_train/chosen': '0.13151', 'rewards_train/rejected': '0.10503', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.026479', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-142.46', 'loss/train': '0.69564', 'examples_per_second': '31.599', 'grad_norm': '99.5', 'counters/examples': 239552, 'counters/updates': 7486}
train stats after 239584 examples: {'rewards_train/chosen': '0.076187', 'rewards_train/rejected': '0.022619', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053567', 'logps_train/rejected': '-157.52', 'logps_train/chosen': '-195.78', 'loss/train': '0.68193', 'examples_per_second': '30.674', 'grad_norm': '177', 'counters/examples': 239584, 'counters/updates': 7487}
train stats after 239616 examples: {'rewards_train/chosen': '0.14857', 'rewards_train/rejected': '0.043042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10553', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-148.23', 'loss/train': '0.65749', 'examples_per_second': '31.596', 'grad_norm': '163', 'counters/examples': 239616, 'counters/updates': 7488}
train stats after 239648 examples: {'rewards_train/chosen': '0.046381', 'rewards_train/rejected': '0.02357', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022811', 'logps_train/rejected': '-115.63', 'logps_train/chosen': '-147.19', 'loss/train': '0.69397', 'examples_per_second': '30.916', 'grad_norm': '68', 'counters/examples': 239648, 'counters/updates': 7489}
skipping logging after 239680 examples to avoid logging too frequently
train stats after 239712 examples: {'rewards_train/chosen': '0.10454', 'rewards_train/rejected': '0.025421', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079119', 'logps_train/rejected': '-109.86', 'logps_train/chosen': '-171.32', 'loss/train': '0.67098', 'examples_per_second': '31.168', 'grad_norm': '97', 'counters/examples': 239712, 'counters/updates': 7491}
train stats after 239744 examples: {'rewards_train/chosen': '0.11951', 'rewards_train/rejected': '0.1043', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015206', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-134.73', 'loss/train': '0.69258', 'examples_per_second': '32.923', 'grad_norm': '80.5', 'counters/examples': 239744, 'counters/updates': 7492}
train stats after 239776 examples: {'rewards_train/chosen': '0.10671', 'rewards_train/rejected': '0.099512', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0071998', 'logps_train/rejected': '-162.39', 'logps_train/chosen': '-130', 'loss/train': '0.70214', 'examples_per_second': '31.759', 'grad_norm': '54.75', 'counters/examples': 239776, 'counters/updates': 7493}
skipping logging after 239808 examples to avoid logging too frequently
train stats after 239840 examples: {'rewards_train/chosen': '0.023161', 'rewards_train/rejected': '0.053363', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.030203', 'logps_train/rejected': '-119.03', 'logps_train/chosen': '-148.43', 'loss/train': '0.7231', 'examples_per_second': '35.79', 'grad_norm': '94.5', 'counters/examples': 239840, 'counters/updates': 7495}
train stats after 239872 examples: {'rewards_train/chosen': '0.030952', 'rewards_train/rejected': '0.054784', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023832', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-154.93', 'loss/train': '0.72289', 'examples_per_second': '31.48', 'grad_norm': '125', 'counters/examples': 239872, 'counters/updates': 7496}
train stats after 239904 examples: {'rewards_train/chosen': '0.07479', 'rewards_train/rejected': '0.13331', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.058519', 'logps_train/rejected': '-145.95', 'logps_train/chosen': '-132.21', 'loss/train': '0.73958', 'examples_per_second': '26.311', 'grad_norm': '76', 'counters/examples': 239904, 'counters/updates': 7497}
train stats after 239936 examples: {'rewards_train/chosen': '0.054862', 'rewards_train/rejected': '0.05565', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00078765', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-158.79', 'loss/train': '0.70684', 'examples_per_second': '31.553', 'grad_norm': '86.5', 'counters/examples': 239936, 'counters/updates': 7498}
train stats after 239968 examples: {'rewards_train/chosen': '0.085888', 'rewards_train/rejected': '0.023556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062332', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-120.97', 'loss/train': '0.67093', 'examples_per_second': '32.151', 'grad_norm': '66', 'counters/examples': 239968, 'counters/updates': 7499}
train stats after 240000 examples: {'rewards_train/chosen': '0.10178', 'rewards_train/rejected': '0.048931', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052847', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-131.89', 'loss/train': '0.67445', 'examples_per_second': '32.715', 'grad_norm': '71', 'counters/examples': 240000, 'counters/updates': 7500}
Running evaluation after 240000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 240000: {'rewards_eval/chosen': '0.13796', 'rewards_eval/rejected': '0.04955', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.088407', 'logps_eval/rejected': '-127.65', 'logps_eval/chosen': '-149.63', 'loss/eval': '0.66761'}
skipping save for non epoch
train stats after 240032 examples: {'rewards_train/chosen': '0.14901', 'rewards_train/rejected': '0.1341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014915', 'logps_train/rejected': '-152.02', 'logps_train/chosen': '-163.1', 'loss/train': '0.73196', 'examples_per_second': '30.667', 'grad_norm': '120', 'counters/examples': 240032, 'counters/updates': 7501}
train stats after 240064 examples: {'rewards_train/chosen': '0.013871', 'rewards_train/rejected': '0.017056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0031856', 'logps_train/rejected': '-102.92', 'logps_train/chosen': '-125.96', 'loss/train': '0.71099', 'examples_per_second': '30.771', 'grad_norm': '87', 'counters/examples': 240064, 'counters/updates': 7502}
train stats after 240096 examples: {'rewards_train/chosen': '0.079117', 'rewards_train/rejected': '0.024285', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.054831', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-129.49', 'loss/train': '0.67512', 'examples_per_second': '31.152', 'grad_norm': '132', 'counters/examples': 240096, 'counters/updates': 7503}
train stats after 240128 examples: {'rewards_train/chosen': '0.16209', 'rewards_train/rejected': '-0.010089', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17218', 'logps_train/rejected': '-132.96', 'logps_train/chosen': '-184', 'loss/train': '0.62838', 'examples_per_second': '31.26', 'grad_norm': '89', 'counters/examples': 240128, 'counters/updates': 7504}
train stats after 240160 examples: {'rewards_train/chosen': '0.050789', 'rewards_train/rejected': '-0.018492', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069281', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-164.75', 'loss/train': '0.67604', 'examples_per_second': '30.938', 'grad_norm': '118.5', 'counters/examples': 240160, 'counters/updates': 7505}
train stats after 240192 examples: {'rewards_train/chosen': '0.12173', 'rewards_train/rejected': '0.10309', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018638', 'logps_train/rejected': '-135', 'logps_train/chosen': '-146.13', 'loss/train': '0.6942', 'examples_per_second': '30.965', 'grad_norm': '68', 'counters/examples': 240192, 'counters/updates': 7506}
train stats after 240224 examples: {'rewards_train/chosen': '0.19033', 'rewards_train/rejected': '0.058018', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13231', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-147.51', 'loss/train': '0.64991', 'examples_per_second': '31.309', 'grad_norm': '97.5', 'counters/examples': 240224, 'counters/updates': 7507}
skipping logging after 240256 examples to avoid logging too frequently
train stats after 240288 examples: {'rewards_train/chosen': '0.096305', 'rewards_train/rejected': '0.034603', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.061701', 'logps_train/rejected': '-148.1', 'logps_train/chosen': '-141.39', 'loss/train': '0.67665', 'examples_per_second': '30.613', 'grad_norm': '64', 'counters/examples': 240288, 'counters/updates': 7509}
train stats after 240320 examples: {'rewards_train/chosen': '0.024249', 'rewards_train/rejected': '0.01659', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0076596', 'logps_train/rejected': '-163.72', 'logps_train/chosen': '-165.22', 'loss/train': '0.70141', 'examples_per_second': '31.135', 'grad_norm': '155', 'counters/examples': 240320, 'counters/updates': 7510}
train stats after 240352 examples: {'rewards_train/chosen': '0.067711', 'rewards_train/rejected': '0.054341', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01337', 'logps_train/rejected': '-113.1', 'logps_train/chosen': '-139.07', 'loss/train': '0.69697', 'examples_per_second': '32.968', 'grad_norm': '73', 'counters/examples': 240352, 'counters/updates': 7511}
skipping logging after 240384 examples to avoid logging too frequently
train stats after 240416 examples: {'rewards_train/chosen': '0.1543', 'rewards_train/rejected': '0.12529', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029007', 'logps_train/rejected': '-114.65', 'logps_train/chosen': '-111.35', 'loss/train': '0.69678', 'examples_per_second': '30.723', 'grad_norm': '93.5', 'counters/examples': 240416, 'counters/updates': 7513}
train stats after 240448 examples: {'rewards_train/chosen': '0.065161', 'rewards_train/rejected': '0.046223', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018937', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-160.72', 'loss/train': '0.69769', 'examples_per_second': '31.5', 'grad_norm': '178', 'counters/examples': 240448, 'counters/updates': 7514}
train stats after 240480 examples: {'rewards_train/chosen': '0.10842', 'rewards_train/rejected': '0.14524', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036816', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-141.01', 'loss/train': '0.72339', 'examples_per_second': '30.524', 'grad_norm': '82.5', 'counters/examples': 240480, 'counters/updates': 7515}
train stats after 240512 examples: {'rewards_train/chosen': '0.077725', 'rewards_train/rejected': '0.049731', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027994', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-164.38', 'loss/train': '0.69196', 'examples_per_second': '30.613', 'grad_norm': '62', 'counters/examples': 240512, 'counters/updates': 7516}
skipping logging after 240544 examples to avoid logging too frequently
train stats after 240576 examples: {'rewards_train/chosen': '0.036903', 'rewards_train/rejected': '0.10237', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.065466', 'logps_train/rejected': '-127.58', 'logps_train/chosen': '-160.56', 'loss/train': '0.73918', 'examples_per_second': '30.672', 'grad_norm': '59.75', 'counters/examples': 240576, 'counters/updates': 7518}
train stats after 240608 examples: {'rewards_train/chosen': '0.13087', 'rewards_train/rejected': '0.0067782', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12409', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-140.77', 'loss/train': '0.64536', 'examples_per_second': '33.082', 'grad_norm': '168', 'counters/examples': 240608, 'counters/updates': 7519}
train stats after 240640 examples: {'rewards_train/chosen': '0.11322', 'rewards_train/rejected': '0.0048726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10834', 'logps_train/rejected': '-149.31', 'logps_train/chosen': '-140.94', 'loss/train': '0.64966', 'examples_per_second': '24.737', 'grad_norm': '106.5', 'counters/examples': 240640, 'counters/updates': 7520}
skipping logging after 240672 examples to avoid logging too frequently
train stats after 240704 examples: {'rewards_train/chosen': '0.15304', 'rewards_train/rejected': '0.072818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080226', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-140.33', 'loss/train': '0.66862', 'examples_per_second': '35.667', 'grad_norm': '80', 'counters/examples': 240704, 'counters/updates': 7522}
train stats after 240736 examples: {'rewards_train/chosen': '0.19312', 'rewards_train/rejected': '0.16913', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023986', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-178.7', 'loss/train': '0.71461', 'examples_per_second': '24.172', 'grad_norm': '132', 'counters/examples': 240736, 'counters/updates': 7523}
train stats after 240768 examples: {'rewards_train/chosen': '0.10826', 'rewards_train/rejected': '0.13892', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030666', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-137.87', 'loss/train': '0.72494', 'examples_per_second': '30.958', 'grad_norm': '161', 'counters/examples': 240768, 'counters/updates': 7524}
train stats after 240800 examples: {'rewards_train/chosen': '0.04612', 'rewards_train/rejected': '0.044102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0020176', 'logps_train/rejected': '-136.6', 'logps_train/chosen': '-122.25', 'loss/train': '0.70568', 'examples_per_second': '33.263', 'grad_norm': '63', 'counters/examples': 240800, 'counters/updates': 7525}
train stats after 240832 examples: {'rewards_train/chosen': '0.09058', 'rewards_train/rejected': '0.032432', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.058148', 'logps_train/rejected': '-135.35', 'logps_train/chosen': '-187.84', 'loss/train': '0.68938', 'examples_per_second': '31.045', 'grad_norm': '107', 'counters/examples': 240832, 'counters/updates': 7526}
train stats after 240864 examples: {'rewards_train/chosen': '0.18701', 'rewards_train/rejected': '0.092118', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.094894', 'logps_train/rejected': '-108.03', 'logps_train/chosen': '-148.53', 'loss/train': '0.65989', 'examples_per_second': '32.402', 'grad_norm': '105.5', 'counters/examples': 240864, 'counters/updates': 7527}
train stats after 240896 examples: {'rewards_train/chosen': '0.13087', 'rewards_train/rejected': '0.01371', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11716', 'logps_train/rejected': '-129.43', 'logps_train/chosen': '-157.27', 'loss/train': '0.65496', 'examples_per_second': '30.88', 'grad_norm': '71', 'counters/examples': 240896, 'counters/updates': 7528}
train stats after 240928 examples: {'rewards_train/chosen': '0.17649', 'rewards_train/rejected': '0.070753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10573', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-129.04', 'loss/train': '0.65259', 'examples_per_second': '31.867', 'grad_norm': '77', 'counters/examples': 240928, 'counters/updates': 7529}
train stats after 240960 examples: {'rewards_train/chosen': '0.14195', 'rewards_train/rejected': '0.083666', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058283', 'logps_train/rejected': '-141', 'logps_train/chosen': '-163.86', 'loss/train': '0.68794', 'examples_per_second': '33.411', 'grad_norm': '110.5', 'counters/examples': 240960, 'counters/updates': 7530}
skipping logging after 240992 examples to avoid logging too frequently
train stats after 241024 examples: {'rewards_train/chosen': '0.12607', 'rewards_train/rejected': '0.0708', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055265', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-148.53', 'loss/train': '0.67497', 'examples_per_second': '31.089', 'grad_norm': '72', 'counters/examples': 241024, 'counters/updates': 7532}
skipping logging after 241056 examples to avoid logging too frequently
train stats after 241088 examples: {'rewards_train/chosen': '0.16768', 'rewards_train/rejected': '0.11107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056609', 'logps_train/rejected': '-146.5', 'logps_train/chosen': '-146.99', 'loss/train': '0.67751', 'examples_per_second': '29.994', 'grad_norm': '106', 'counters/examples': 241088, 'counters/updates': 7534}
train stats after 241120 examples: {'rewards_train/chosen': '0.068537', 'rewards_train/rejected': '0.12016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.051619', 'logps_train/rejected': '-159.74', 'logps_train/chosen': '-136.13', 'loss/train': '0.73681', 'examples_per_second': '31.562', 'grad_norm': '199', 'counters/examples': 241120, 'counters/updates': 7535}
train stats after 241152 examples: {'rewards_train/chosen': '0.19183', 'rewards_train/rejected': '0.058614', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13322', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-152.99', 'loss/train': '0.64965', 'examples_per_second': '30.661', 'grad_norm': '48.25', 'counters/examples': 241152, 'counters/updates': 7536}
train stats after 241184 examples: {'rewards_train/chosen': '0.23256', 'rewards_train/rejected': '0.11115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12141', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-146.63', 'loss/train': '0.65311', 'examples_per_second': '30.951', 'grad_norm': '118.5', 'counters/examples': 241184, 'counters/updates': 7537}
train stats after 241216 examples: {'rewards_train/chosen': '0.092253', 'rewards_train/rejected': '0.027299', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064954', 'logps_train/rejected': '-110.58', 'logps_train/chosen': '-121.36', 'loss/train': '0.67622', 'examples_per_second': '30.398', 'grad_norm': '84', 'counters/examples': 241216, 'counters/updates': 7538}
skipping logging after 241248 examples to avoid logging too frequently
train stats after 241280 examples: {'rewards_train/chosen': '0.17169', 'rewards_train/rejected': '0.10095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070739', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-139.89', 'loss/train': '0.67003', 'examples_per_second': '33.495', 'grad_norm': '73.5', 'counters/examples': 241280, 'counters/updates': 7540}
train stats after 241312 examples: {'rewards_train/chosen': '0.088872', 'rewards_train/rejected': '0.081196', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0076756', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-143.45', 'loss/train': '0.70936', 'examples_per_second': '31.904', 'grad_norm': '65.5', 'counters/examples': 241312, 'counters/updates': 7541}
train stats after 241344 examples: {'rewards_train/chosen': '0.11831', 'rewards_train/rejected': '0.1266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0082939', 'logps_train/rejected': '-171.75', 'logps_train/chosen': '-161.42', 'loss/train': '0.71828', 'examples_per_second': '31.572', 'grad_norm': '127', 'counters/examples': 241344, 'counters/updates': 7542}
train stats after 241376 examples: {'rewards_train/chosen': '0.023728', 'rewards_train/rejected': '0.055338', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03161', 'logps_train/rejected': '-145.87', 'logps_train/chosen': '-180.96', 'loss/train': '0.72211', 'examples_per_second': '31.494', 'grad_norm': '231', 'counters/examples': 241376, 'counters/updates': 7543}
train stats after 241408 examples: {'rewards_train/chosen': '0.09492', 'rewards_train/rejected': '0.081284', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013636', 'logps_train/rejected': '-149.13', 'logps_train/chosen': '-158.51', 'loss/train': '0.69753', 'examples_per_second': '31.076', 'grad_norm': '82.5', 'counters/examples': 241408, 'counters/updates': 7544}
train stats after 241440 examples: {'rewards_train/chosen': '0.23572', 'rewards_train/rejected': '-0.010892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24661', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-187.97', 'loss/train': '0.60689', 'examples_per_second': '31.544', 'grad_norm': '83.5', 'counters/examples': 241440, 'counters/updates': 7545}
train stats after 241472 examples: {'rewards_train/chosen': '0.18371', 'rewards_train/rejected': '0.091727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091984', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-148.9', 'loss/train': '0.66429', 'examples_per_second': '32.42', 'grad_norm': '105.5', 'counters/examples': 241472, 'counters/updates': 7546}
train stats after 241504 examples: {'rewards_train/chosen': '0.22403', 'rewards_train/rejected': '0.082779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14125', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-192.47', 'loss/train': '0.6483', 'examples_per_second': '31.502', 'grad_norm': '205', 'counters/examples': 241504, 'counters/updates': 7547}
train stats after 241536 examples: {'rewards_train/chosen': '0.077217', 'rewards_train/rejected': '0.025035', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052182', 'logps_train/rejected': '-133.73', 'logps_train/chosen': '-165.48', 'loss/train': '0.6756', 'examples_per_second': '31.56', 'grad_norm': '98.5', 'counters/examples': 241536, 'counters/updates': 7548}
train stats after 241568 examples: {'rewards_train/chosen': '0.11899', 'rewards_train/rejected': '0.074293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044694', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-178.54', 'loss/train': '0.69047', 'examples_per_second': '30.1', 'grad_norm': '123.5', 'counters/examples': 241568, 'counters/updates': 7549}
train stats after 241600 examples: {'rewards_train/chosen': '0.11108', 'rewards_train/rejected': '0.028104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082972', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-145.65', 'loss/train': '0.67143', 'examples_per_second': '31.675', 'grad_norm': '50.75', 'counters/examples': 241600, 'counters/updates': 7550}
train stats after 241632 examples: {'rewards_train/chosen': '0.13593', 'rewards_train/rejected': '-0.0078967', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14383', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-142.7', 'loss/train': '0.6423', 'examples_per_second': '32.475', 'grad_norm': '56.25', 'counters/examples': 241632, 'counters/updates': 7551}
train stats after 241664 examples: {'rewards_train/chosen': '0.083158', 'rewards_train/rejected': '0.040203', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042955', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-181.13', 'loss/train': '0.68335', 'examples_per_second': '31.914', 'grad_norm': '93', 'counters/examples': 241664, 'counters/updates': 7552}
train stats after 241696 examples: {'rewards_train/chosen': '0.10646', 'rewards_train/rejected': '0.094551', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011913', 'logps_train/rejected': '-92.442', 'logps_train/chosen': '-126.48', 'loss/train': '0.6994', 'examples_per_second': '32.322', 'grad_norm': '79.5', 'counters/examples': 241696, 'counters/updates': 7553}
train stats after 241728 examples: {'rewards_train/chosen': '0.24286', 'rewards_train/rejected': '0.12329', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11958', 'logps_train/rejected': '-162.51', 'logps_train/chosen': '-167.56', 'loss/train': '0.65135', 'examples_per_second': '31.489', 'grad_norm': '195', 'counters/examples': 241728, 'counters/updates': 7554}
train stats after 241760 examples: {'rewards_train/chosen': '0.15802', 'rewards_train/rejected': '0.048554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-177.62', 'loss/train': '0.6526', 'examples_per_second': '31.235', 'grad_norm': '80', 'counters/examples': 241760, 'counters/updates': 7555}
train stats after 241792 examples: {'rewards_train/chosen': '0.16112', 'rewards_train/rejected': '0.14889', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012226', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-151.13', 'loss/train': '0.70876', 'examples_per_second': '31.248', 'grad_norm': '149', 'counters/examples': 241792, 'counters/updates': 7556}
train stats after 241824 examples: {'rewards_train/chosen': '0.13433', 'rewards_train/rejected': '0.10103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033295', 'logps_train/rejected': '-150.47', 'logps_train/chosen': '-189.96', 'loss/train': '0.6928', 'examples_per_second': '31.52', 'grad_norm': '92', 'counters/examples': 241824, 'counters/updates': 7557}
train stats after 241856 examples: {'rewards_train/chosen': '0.078198', 'rewards_train/rejected': '0.05215', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.026049', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-132.34', 'loss/train': '0.71392', 'examples_per_second': '31.511', 'grad_norm': '121.5', 'counters/examples': 241856, 'counters/updates': 7558}
skipping logging after 241888 examples to avoid logging too frequently
train stats after 241920 examples: {'rewards_train/chosen': '0.14064', 'rewards_train/rejected': '0.16035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019708', 'logps_train/rejected': '-141.1', 'logps_train/chosen': '-150.99', 'loss/train': '0.72381', 'examples_per_second': '31.382', 'grad_norm': '120.5', 'counters/examples': 241920, 'counters/updates': 7560}
train stats after 241952 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.1263', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.011439', 'logps_train/rejected': '-176.7', 'logps_train/chosen': '-183.5', 'loss/train': '0.71463', 'examples_per_second': '31.527', 'grad_norm': '115.5', 'counters/examples': 241952, 'counters/updates': 7561}
train stats after 241984 examples: {'rewards_train/chosen': '0.043168', 'rewards_train/rejected': '0.039382', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0037866', 'logps_train/rejected': '-163.34', 'logps_train/chosen': '-167.24', 'loss/train': '0.72687', 'examples_per_second': '31.224', 'grad_norm': '192', 'counters/examples': 241984, 'counters/updates': 7562}
train stats after 242016 examples: {'rewards_train/chosen': '0.059921', 'rewards_train/rejected': '0.092817', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032896', 'logps_train/rejected': '-112.25', 'logps_train/chosen': '-132.13', 'loss/train': '0.72449', 'examples_per_second': '30.59', 'grad_norm': '108.5', 'counters/examples': 242016, 'counters/updates': 7563}
train stats after 242048 examples: {'rewards_train/chosen': '0.086913', 'rewards_train/rejected': '0.016488', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070426', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-158.69', 'loss/train': '0.67598', 'examples_per_second': '30.6', 'grad_norm': '89.5', 'counters/examples': 242048, 'counters/updates': 7564}
train stats after 242080 examples: {'rewards_train/chosen': '0.14744', 'rewards_train/rejected': '0.060688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086755', 'logps_train/rejected': '-165.78', 'logps_train/chosen': '-180.99', 'loss/train': '0.66894', 'examples_per_second': '31.277', 'grad_norm': '85', 'counters/examples': 242080, 'counters/updates': 7565}
train stats after 242112 examples: {'rewards_train/chosen': '0.083523', 'rewards_train/rejected': '-0.0018839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085406', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-132.35', 'loss/train': '0.66913', 'examples_per_second': '31.17', 'grad_norm': '77', 'counters/examples': 242112, 'counters/updates': 7566}
train stats after 242144 examples: {'rewards_train/chosen': '0.095003', 'rewards_train/rejected': '0.16321', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.068206', 'logps_train/rejected': '-167.81', 'logps_train/chosen': '-161.95', 'loss/train': '0.74901', 'examples_per_second': '31.841', 'grad_norm': '218', 'counters/examples': 242144, 'counters/updates': 7567}
skipping logging after 242176 examples to avoid logging too frequently
train stats after 242208 examples: {'rewards_train/chosen': '0.1397', 'rewards_train/rejected': '0.072545', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067159', 'logps_train/rejected': '-149.19', 'logps_train/chosen': '-156.1', 'loss/train': '0.67572', 'examples_per_second': '33.575', 'grad_norm': '171', 'counters/examples': 242208, 'counters/updates': 7569}
skipping logging after 242240 examples to avoid logging too frequently
train stats after 242272 examples: {'rewards_train/chosen': '0.11051', 'rewards_train/rejected': '0.12361', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.013107', 'logps_train/rejected': '-155.06', 'logps_train/chosen': '-186.71', 'loss/train': '0.7163', 'examples_per_second': '30.057', 'grad_norm': '104.5', 'counters/examples': 242272, 'counters/updates': 7571}
train stats after 242304 examples: {'rewards_train/chosen': '0.13929', 'rewards_train/rejected': '0.02479', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1145', 'logps_train/rejected': '-154.44', 'logps_train/chosen': '-182.71', 'loss/train': '0.64908', 'examples_per_second': '30.173', 'grad_norm': '133', 'counters/examples': 242304, 'counters/updates': 7572}
train stats after 242336 examples: {'rewards_train/chosen': '0.16194', 'rewards_train/rejected': '0.019972', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14197', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-159.93', 'loss/train': '0.63668', 'examples_per_second': '30.592', 'grad_norm': '65.5', 'counters/examples': 242336, 'counters/updates': 7573}
train stats after 242368 examples: {'rewards_train/chosen': '0.084479', 'rewards_train/rejected': '0.069255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015224', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-147.6', 'loss/train': '0.70366', 'examples_per_second': '32.273', 'grad_norm': '75', 'counters/examples': 242368, 'counters/updates': 7574}
train stats after 242400 examples: {'rewards_train/chosen': '0.15108', 'rewards_train/rejected': '0.00066423', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15042', 'logps_train/rejected': '-137.52', 'logps_train/chosen': '-163.84', 'loss/train': '0.63933', 'examples_per_second': '31.558', 'grad_norm': '94.5', 'counters/examples': 242400, 'counters/updates': 7575}
train stats after 242432 examples: {'rewards_train/chosen': '0.056068', 'rewards_train/rejected': '0.071909', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015841', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-108.9', 'loss/train': '0.71321', 'examples_per_second': '31.551', 'grad_norm': '82.5', 'counters/examples': 242432, 'counters/updates': 7576}
train stats after 242464 examples: {'rewards_train/chosen': '0.1555', 'rewards_train/rejected': '0.060203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0953', 'logps_train/rejected': '-135.64', 'logps_train/chosen': '-163.6', 'loss/train': '0.66212', 'examples_per_second': '30.108', 'grad_norm': '154', 'counters/examples': 242464, 'counters/updates': 7577}
train stats after 242496 examples: {'rewards_train/chosen': '0.060253', 'rewards_train/rejected': '0.049797', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010456', 'logps_train/rejected': '-137.11', 'logps_train/chosen': '-148.08', 'loss/train': '0.70396', 'examples_per_second': '30.698', 'grad_norm': '115', 'counters/examples': 242496, 'counters/updates': 7578}
train stats after 242528 examples: {'rewards_train/chosen': '0.099012', 'rewards_train/rejected': '-0.061577', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16059', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-150.07', 'loss/train': '0.62846', 'examples_per_second': '31.509', 'grad_norm': '132', 'counters/examples': 242528, 'counters/updates': 7579}
skipping logging after 242560 examples to avoid logging too frequently
train stats after 242592 examples: {'rewards_train/chosen': '0.11742', 'rewards_train/rejected': '0.037754', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079662', 'logps_train/rejected': '-114.4', 'logps_train/chosen': '-144.99', 'loss/train': '0.67337', 'examples_per_second': '30.285', 'grad_norm': '147', 'counters/examples': 242592, 'counters/updates': 7581}
train stats after 242624 examples: {'rewards_train/chosen': '0.21222', 'rewards_train/rejected': '0.079737', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13249', 'logps_train/rejected': '-142.21', 'logps_train/chosen': '-137.21', 'loss/train': '0.65609', 'examples_per_second': '30.958', 'grad_norm': '68', 'counters/examples': 242624, 'counters/updates': 7582}
train stats after 242656 examples: {'rewards_train/chosen': '0.15383', 'rewards_train/rejected': '0.070322', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083511', 'logps_train/rejected': '-107.79', 'logps_train/chosen': '-129.71', 'loss/train': '0.67787', 'examples_per_second': '29.648', 'grad_norm': '88.5', 'counters/examples': 242656, 'counters/updates': 7583}
train stats after 242688 examples: {'rewards_train/chosen': '0.12699', 'rewards_train/rejected': '0.018245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10874', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-133.93', 'loss/train': '0.65268', 'examples_per_second': '31.345', 'grad_norm': '99.5', 'counters/examples': 242688, 'counters/updates': 7584}
train stats after 242720 examples: {'rewards_train/chosen': '0.1441', 'rewards_train/rejected': '0.064945', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.079159', 'logps_train/rejected': '-101.21', 'logps_train/chosen': '-119.37', 'loss/train': '0.67247', 'examples_per_second': '30.742', 'grad_norm': '51', 'counters/examples': 242720, 'counters/updates': 7585}
train stats after 242752 examples: {'rewards_train/chosen': '0.1049', 'rewards_train/rejected': '0.066434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038463', 'logps_train/rejected': '-154.48', 'logps_train/chosen': '-124.6', 'loss/train': '0.68711', 'examples_per_second': '32.644', 'grad_norm': '61.5', 'counters/examples': 242752, 'counters/updates': 7586}
train stats after 242784 examples: {'rewards_train/chosen': '0.13372', 'rewards_train/rejected': '0.044816', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088907', 'logps_train/rejected': '-113.37', 'logps_train/chosen': '-144.4', 'loss/train': '0.65905', 'examples_per_second': '32.85', 'grad_norm': '81.5', 'counters/examples': 242784, 'counters/updates': 7587}
skipping logging after 242816 examples to avoid logging too frequently
train stats after 242848 examples: {'rewards_train/chosen': '0.14138', 'rewards_train/rejected': '0.13735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0040293', 'logps_train/rejected': '-154.47', 'logps_train/chosen': '-166.51', 'loss/train': '0.71164', 'examples_per_second': '31.515', 'grad_norm': '68', 'counters/examples': 242848, 'counters/updates': 7589}
train stats after 242880 examples: {'rewards_train/chosen': '0.11969', 'rewards_train/rejected': '0.077355', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042332', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-132.83', 'loss/train': '0.69163', 'examples_per_second': '31.383', 'grad_norm': '72', 'counters/examples': 242880, 'counters/updates': 7590}
train stats after 242912 examples: {'rewards_train/chosen': '0.12425', 'rewards_train/rejected': '0.06734', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056907', 'logps_train/rejected': '-137.26', 'logps_train/chosen': '-172', 'loss/train': '0.70036', 'examples_per_second': '31.433', 'grad_norm': '79.5', 'counters/examples': 242912, 'counters/updates': 7591}
train stats after 242944 examples: {'rewards_train/chosen': '0.067631', 'rewards_train/rejected': '0.0012326', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.066398', 'logps_train/rejected': '-146.99', 'logps_train/chosen': '-143.71', 'loss/train': '0.67443', 'examples_per_second': '31.497', 'grad_norm': '95.5', 'counters/examples': 242944, 'counters/updates': 7592}
train stats after 242976 examples: {'rewards_train/chosen': '0.15148', 'rewards_train/rejected': '0.11249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038994', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-155.87', 'loss/train': '0.68208', 'examples_per_second': '31.239', 'grad_norm': '70.5', 'counters/examples': 242976, 'counters/updates': 7593}
train stats after 243008 examples: {'rewards_train/chosen': '0.17805', 'rewards_train/rejected': '0.055797', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12225', 'logps_train/rejected': '-128.2', 'logps_train/chosen': '-145.21', 'loss/train': '0.64876', 'examples_per_second': '31.537', 'grad_norm': '49.75', 'counters/examples': 243008, 'counters/updates': 7594}
skipping logging after 243040 examples to avoid logging too frequently
train stats after 243072 examples: {'rewards_train/chosen': '0.076453', 'rewards_train/rejected': '0.077064', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00061071', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-160.3', 'loss/train': '0.7033', 'examples_per_second': '34.267', 'grad_norm': '77', 'counters/examples': 243072, 'counters/updates': 7596}
train stats after 243104 examples: {'rewards_train/chosen': '0.20055', 'rewards_train/rejected': '0.037897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16266', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-165.91', 'loss/train': '0.63784', 'examples_per_second': '31.517', 'grad_norm': '77', 'counters/examples': 243104, 'counters/updates': 7597}
train stats after 243136 examples: {'rewards_train/chosen': '0.077415', 'rewards_train/rejected': '0.15127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.073851', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-157.69', 'loss/train': '0.75689', 'examples_per_second': '31.469', 'grad_norm': '74', 'counters/examples': 243136, 'counters/updates': 7598}
train stats after 243168 examples: {'rewards_train/chosen': '0.068509', 'rewards_train/rejected': '-0.0092401', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077749', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-165.96', 'loss/train': '0.66958', 'examples_per_second': '32.489', 'grad_norm': '60.5', 'counters/examples': 243168, 'counters/updates': 7599}
train stats after 243200 examples: {'rewards_train/chosen': '0.088839', 'rewards_train/rejected': '0.04257', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046268', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-113.44', 'loss/train': '0.68264', 'examples_per_second': '30.19', 'grad_norm': '82.5', 'counters/examples': 243200, 'counters/updates': 7600}
train stats after 243232 examples: {'rewards_train/chosen': '0.16665', 'rewards_train/rejected': '0.06628', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10037', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-154.4', 'loss/train': '0.65933', 'examples_per_second': '31.22', 'grad_norm': '83.5', 'counters/examples': 243232, 'counters/updates': 7601}
train stats after 243264 examples: {'rewards_train/chosen': '0.15693', 'rewards_train/rejected': '-0.025888', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18282', 'logps_train/rejected': '-131.03', 'logps_train/chosen': '-134.31', 'loss/train': '0.61834', 'examples_per_second': '32.819', 'grad_norm': '65.5', 'counters/examples': 243264, 'counters/updates': 7602}
train stats after 243296 examples: {'rewards_train/chosen': '0.083617', 'rewards_train/rejected': '0.070849', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.012768', 'logps_train/rejected': '-135.02', 'logps_train/chosen': '-160.11', 'loss/train': '0.70779', 'examples_per_second': '31.375', 'grad_norm': '145', 'counters/examples': 243296, 'counters/updates': 7603}
train stats after 243328 examples: {'rewards_train/chosen': '0.11651', 'rewards_train/rejected': '0.15274', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.036228', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-147.91', 'loss/train': '0.72821', 'examples_per_second': '31.711', 'grad_norm': '450', 'counters/examples': 243328, 'counters/updates': 7604}
train stats after 243360 examples: {'rewards_train/chosen': '0.12164', 'rewards_train/rejected': '0.15949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037842', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-150.46', 'loss/train': '0.72971', 'examples_per_second': '31.467', 'grad_norm': '85', 'counters/examples': 243360, 'counters/updates': 7605}
train stats after 243392 examples: {'rewards_train/chosen': '0.21732', 'rewards_train/rejected': '0.0041685', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21315', 'logps_train/rejected': '-146.53', 'logps_train/chosen': '-197.48', 'loss/train': '0.60989', 'examples_per_second': '33.132', 'grad_norm': '97', 'counters/examples': 243392, 'counters/updates': 7606}
train stats after 243424 examples: {'rewards_train/chosen': '0.20734', 'rewards_train/rejected': '0.054252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15309', 'logps_train/rejected': '-142.02', 'logps_train/chosen': '-135.49', 'loss/train': '0.64911', 'examples_per_second': '31.12', 'grad_norm': '103', 'counters/examples': 243424, 'counters/updates': 7607}
train stats after 243456 examples: {'rewards_train/chosen': '0.14939', 'rewards_train/rejected': '0.012738', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13665', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-166.72', 'loss/train': '0.64405', 'examples_per_second': '31.543', 'grad_norm': '130', 'counters/examples': 243456, 'counters/updates': 7608}
train stats after 243488 examples: {'rewards_train/chosen': '0.016539', 'rewards_train/rejected': '0.051765', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.035226', 'logps_train/rejected': '-111.99', 'logps_train/chosen': '-123.12', 'loss/train': '0.72368', 'examples_per_second': '31.756', 'grad_norm': '84', 'counters/examples': 243488, 'counters/updates': 7609}
skipping logging after 243520 examples to avoid logging too frequently
train stats after 243552 examples: {'rewards_train/chosen': '0.10346', 'rewards_train/rejected': '0.15209', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048628', 'logps_train/rejected': '-146.86', 'logps_train/chosen': '-175.37', 'loss/train': '0.73231', 'examples_per_second': '30.97', 'grad_norm': '103', 'counters/examples': 243552, 'counters/updates': 7611}
train stats after 243584 examples: {'rewards_train/chosen': '0.058885', 'rewards_train/rejected': '-0.02644', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.085325', 'logps_train/rejected': '-110.89', 'logps_train/chosen': '-134.95', 'loss/train': '0.66006', 'examples_per_second': '31.546', 'grad_norm': '52.5', 'counters/examples': 243584, 'counters/updates': 7612}
train stats after 243616 examples: {'rewards_train/chosen': '0.13913', 'rewards_train/rejected': '0.058763', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080363', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-155.63', 'loss/train': '0.67281', 'examples_per_second': '30.267', 'grad_norm': '174', 'counters/examples': 243616, 'counters/updates': 7613}
train stats after 243648 examples: {'rewards_train/chosen': '0.14212', 'rewards_train/rejected': '0.074434', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.067684', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-161.18', 'loss/train': '0.67384', 'examples_per_second': '30.468', 'grad_norm': '71.5', 'counters/examples': 243648, 'counters/updates': 7614}
train stats after 243680 examples: {'rewards_train/chosen': '0.10531', 'rewards_train/rejected': '0.052523', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052788', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-152.22', 'loss/train': '0.67853', 'examples_per_second': '31.484', 'grad_norm': '56', 'counters/examples': 243680, 'counters/updates': 7615}
train stats after 243712 examples: {'rewards_train/chosen': '0.026713', 'rewards_train/rejected': '-0.014654', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041367', 'logps_train/rejected': '-98.047', 'logps_train/chosen': '-133.47', 'loss/train': '0.68043', 'examples_per_second': '32.221', 'grad_norm': '103', 'counters/examples': 243712, 'counters/updates': 7616}
train stats after 243744 examples: {'rewards_train/chosen': '0.11394', 'rewards_train/rejected': '0.077067', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036874', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-166.18', 'loss/train': '0.68766', 'examples_per_second': '30.108', 'grad_norm': '98', 'counters/examples': 243744, 'counters/updates': 7617}
train stats after 243776 examples: {'rewards_train/chosen': '0.085264', 'rewards_train/rejected': '-0.02691', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11217', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-108.78', 'loss/train': '0.65023', 'examples_per_second': '31.494', 'grad_norm': '87.5', 'counters/examples': 243776, 'counters/updates': 7618}
skipping logging after 243808 examples to avoid logging too frequently
train stats after 243840 examples: {'rewards_train/chosen': '0.10796', 'rewards_train/rejected': '-0.052099', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16006', 'logps_train/rejected': '-158.68', 'logps_train/chosen': '-146.17', 'loss/train': '0.63077', 'examples_per_second': '31.636', 'grad_norm': '71', 'counters/examples': 243840, 'counters/updates': 7620}
train stats after 243872 examples: {'rewards_train/chosen': '0.091561', 'rewards_train/rejected': '0.090156', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0014053', 'logps_train/rejected': '-150.6', 'logps_train/chosen': '-144.75', 'loss/train': '0.70756', 'examples_per_second': '31.54', 'grad_norm': '111', 'counters/examples': 243872, 'counters/updates': 7621}
train stats after 243904 examples: {'rewards_train/chosen': '0.13073', 'rewards_train/rejected': '-0.016455', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14718', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-152.56', 'loss/train': '0.63765', 'examples_per_second': '31.507', 'grad_norm': '72', 'counters/examples': 243904, 'counters/updates': 7622}
train stats after 243936 examples: {'rewards_train/chosen': '0.10131', 'rewards_train/rejected': '-0.0077884', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1091', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-137.63', 'loss/train': '0.65506', 'examples_per_second': '32.925', 'grad_norm': '54.25', 'counters/examples': 243936, 'counters/updates': 7623}
train stats after 243968 examples: {'rewards_train/chosen': '0.11729', 'rewards_train/rejected': '0.022336', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09495', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-186.35', 'loss/train': '0.65945', 'examples_per_second': '30.571', 'grad_norm': '195', 'counters/examples': 243968, 'counters/updates': 7624}
train stats after 244000 examples: {'rewards_train/chosen': '0.093596', 'rewards_train/rejected': '0.067234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026362', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-157.77', 'loss/train': '0.70149', 'examples_per_second': '31.531', 'grad_norm': '103', 'counters/examples': 244000, 'counters/updates': 7625}
train stats after 244032 examples: {'rewards_train/chosen': '0.19982', 'rewards_train/rejected': '-0.051818', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.25164', 'logps_train/rejected': '-147.19', 'logps_train/chosen': '-158.9', 'loss/train': '0.59269', 'examples_per_second': '30.589', 'grad_norm': '71', 'counters/examples': 244032, 'counters/updates': 7626}
skipping logging after 244064 examples to avoid logging too frequently
train stats after 244096 examples: {'rewards_train/chosen': '0.098562', 'rewards_train/rejected': '0.058231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04033', 'logps_train/rejected': '-132.03', 'logps_train/chosen': '-156.92', 'loss/train': '0.68596', 'examples_per_second': '30.881', 'grad_norm': '92', 'counters/examples': 244096, 'counters/updates': 7628}
train stats after 244128 examples: {'rewards_train/chosen': '0.076792', 'rewards_train/rejected': '0.052715', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.024077', 'logps_train/rejected': '-138.68', 'logps_train/chosen': '-145.61', 'loss/train': '0.69179', 'examples_per_second': '31.722', 'grad_norm': '89', 'counters/examples': 244128, 'counters/updates': 7629}
train stats after 244160 examples: {'rewards_train/chosen': '0.10747', 'rewards_train/rejected': '0.053514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053958', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-130.64', 'loss/train': '0.67633', 'examples_per_second': '32.18', 'grad_norm': '124', 'counters/examples': 244160, 'counters/updates': 7630}
train stats after 244192 examples: {'rewards_train/chosen': '0.15376', 'rewards_train/rejected': '0.12874', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02502', 'logps_train/rejected': '-149.43', 'logps_train/chosen': '-135.11', 'loss/train': '0.69291', 'examples_per_second': '30.773', 'grad_norm': '67', 'counters/examples': 244192, 'counters/updates': 7631}
train stats after 244224 examples: {'rewards_train/chosen': '0.17438', 'rewards_train/rejected': '0.077335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097046', 'logps_train/rejected': '-142.19', 'logps_train/chosen': '-156.9', 'loss/train': '0.6616', 'examples_per_second': '30.62', 'grad_norm': '63.25', 'counters/examples': 244224, 'counters/updates': 7632}
skipping logging after 244256 examples to avoid logging too frequently
train stats after 244288 examples: {'rewards_train/chosen': '0.16004', 'rewards_train/rejected': '0.046191', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11385', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-145.31', 'loss/train': '0.6473', 'examples_per_second': '32.723', 'grad_norm': '77.5', 'counters/examples': 244288, 'counters/updates': 7634}
skipping logging after 244320 examples to avoid logging too frequently
train stats after 244352 examples: {'rewards_train/chosen': '0.11955', 'rewards_train/rejected': '0.026514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09304', 'logps_train/rejected': '-122.74', 'logps_train/chosen': '-127', 'loss/train': '0.67329', 'examples_per_second': '33.732', 'grad_norm': '68', 'counters/examples': 244352, 'counters/updates': 7636}
skipping logging after 244384 examples to avoid logging too frequently
train stats after 244416 examples: {'rewards_train/chosen': '0.14345', 'rewards_train/rejected': '0.10146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04199', 'logps_train/rejected': '-124.75', 'logps_train/chosen': '-156.08', 'loss/train': '0.68459', 'examples_per_second': '30.581', 'grad_norm': '85.5', 'counters/examples': 244416, 'counters/updates': 7638}
train stats after 244448 examples: {'rewards_train/chosen': '0.13195', 'rewards_train/rejected': '0.056227', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075721', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-133.35', 'loss/train': '0.66629', 'examples_per_second': '30.075', 'grad_norm': '88', 'counters/examples': 244448, 'counters/updates': 7639}
train stats after 244480 examples: {'rewards_train/chosen': '0.10971', 'rewards_train/rejected': '-0.14145', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25116', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-161.17', 'loss/train': '0.59207', 'examples_per_second': '31.695', 'grad_norm': '130', 'counters/examples': 244480, 'counters/updates': 7640}
train stats after 244512 examples: {'rewards_train/chosen': '0.17754', 'rewards_train/rejected': '0.0097662', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16778', 'logps_train/rejected': '-121.68', 'logps_train/chosen': '-164.8', 'loss/train': '0.62554', 'examples_per_second': '32.111', 'grad_norm': '55.5', 'counters/examples': 244512, 'counters/updates': 7641}
train stats after 244544 examples: {'rewards_train/chosen': '0.098171', 'rewards_train/rejected': '0.012419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085751', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-164.71', 'loss/train': '0.66771', 'examples_per_second': '31.51', 'grad_norm': '108.5', 'counters/examples': 244544, 'counters/updates': 7642}
train stats after 244576 examples: {'rewards_train/chosen': '0.10759', 'rewards_train/rejected': '0.13052', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022937', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-139.91', 'loss/train': '0.72369', 'examples_per_second': '32.735', 'grad_norm': '72', 'counters/examples': 244576, 'counters/updates': 7643}
train stats after 244608 examples: {'rewards_train/chosen': '0.11963', 'rewards_train/rejected': '0.096021', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02361', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-133.48', 'loss/train': '0.69132', 'examples_per_second': '31.585', 'grad_norm': '106', 'counters/examples': 244608, 'counters/updates': 7644}
train stats after 244640 examples: {'rewards_train/chosen': '0.08878', 'rewards_train/rejected': '0.03868', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0501', 'logps_train/rejected': '-139.27', 'logps_train/chosen': '-151.54', 'loss/train': '0.68306', 'examples_per_second': '31.08', 'grad_norm': '101', 'counters/examples': 244640, 'counters/updates': 7645}
train stats after 244672 examples: {'rewards_train/chosen': '0.042926', 'rewards_train/rejected': '0.010028', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.032898', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-133.35', 'loss/train': '0.6848', 'examples_per_second': '33.19', 'grad_norm': '68.5', 'counters/examples': 244672, 'counters/updates': 7646}
skipping logging after 244704 examples to avoid logging too frequently
train stats after 244736 examples: {'rewards_train/chosen': '0.047159', 'rewards_train/rejected': '0.016489', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030669', 'logps_train/rejected': '-118.31', 'logps_train/chosen': '-130.04', 'loss/train': '0.68915', 'examples_per_second': '31.589', 'grad_norm': '74.5', 'counters/examples': 244736, 'counters/updates': 7648}
train stats after 244768 examples: {'rewards_train/chosen': '0.047314', 'rewards_train/rejected': '-0.033157', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080471', 'logps_train/rejected': '-142.64', 'logps_train/chosen': '-137.09', 'loss/train': '0.67478', 'examples_per_second': '30.882', 'grad_norm': '133', 'counters/examples': 244768, 'counters/updates': 7649}
train stats after 244800 examples: {'rewards_train/chosen': '0.037981', 'rewards_train/rejected': '0.05226', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014278', 'logps_train/rejected': '-119.07', 'logps_train/chosen': '-132.37', 'loss/train': '0.70928', 'examples_per_second': '32.277', 'grad_norm': '86.5', 'counters/examples': 244800, 'counters/updates': 7650}
train stats after 244832 examples: {'rewards_train/chosen': '0.1114', 'rewards_train/rejected': '0.063718', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047682', 'logps_train/rejected': '-162.53', 'logps_train/chosen': '-147.2', 'loss/train': '0.69331', 'examples_per_second': '31.705', 'grad_norm': '62.75', 'counters/examples': 244832, 'counters/updates': 7651}
skipping logging after 244864 examples to avoid logging too frequently
train stats after 244896 examples: {'rewards_train/chosen': '0.03559', 'rewards_train/rejected': '-0.056288', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091877', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-174.54', 'loss/train': '0.66707', 'examples_per_second': '30.559', 'grad_norm': '72.5', 'counters/examples': 244896, 'counters/updates': 7653}
train stats after 244928 examples: {'rewards_train/chosen': '0.038035', 'rewards_train/rejected': '0.066094', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.028059', 'logps_train/rejected': '-121.19', 'logps_train/chosen': '-126.47', 'loss/train': '0.71668', 'examples_per_second': '31.287', 'grad_norm': '175', 'counters/examples': 244928, 'counters/updates': 7654}
train stats after 244960 examples: {'rewards_train/chosen': '0.16902', 'rewards_train/rejected': '0.034667', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13436', 'logps_train/rejected': '-150.64', 'logps_train/chosen': '-162.14', 'loss/train': '0.63973', 'examples_per_second': '31.335', 'grad_norm': '87.5', 'counters/examples': 244960, 'counters/updates': 7655}
train stats after 244992 examples: {'rewards_train/chosen': '0.073752', 'rewards_train/rejected': '-0.014532', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088284', 'logps_train/rejected': '-98.884', 'logps_train/chosen': '-150.86', 'loss/train': '0.66008', 'examples_per_second': '31.422', 'grad_norm': '103', 'counters/examples': 244992, 'counters/updates': 7656}
train stats after 245024 examples: {'rewards_train/chosen': '0.1081', 'rewards_train/rejected': '0.052976', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05512', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-156.15', 'loss/train': '0.67893', 'examples_per_second': '32.278', 'grad_norm': '90', 'counters/examples': 245024, 'counters/updates': 7657}
train stats after 245056 examples: {'rewards_train/chosen': '0.1015', 'rewards_train/rejected': '0.010604', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090899', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-151.93', 'loss/train': '0.65419', 'examples_per_second': '33.043', 'grad_norm': '50.5', 'counters/examples': 245056, 'counters/updates': 7658}
train stats after 245088 examples: {'rewards_train/chosen': '0.16324', 'rewards_train/rejected': '0.04625', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11699', 'logps_train/rejected': '-146.92', 'logps_train/chosen': '-161.1', 'loss/train': '0.65844', 'examples_per_second': '31.575', 'grad_norm': '127', 'counters/examples': 245088, 'counters/updates': 7659}
train stats after 245120 examples: {'rewards_train/chosen': '0.14522', 'rewards_train/rejected': '0.04163', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10359', 'logps_train/rejected': '-140.78', 'logps_train/chosen': '-170.57', 'loss/train': '0.67676', 'examples_per_second': '30.709', 'grad_norm': '129', 'counters/examples': 245120, 'counters/updates': 7660}
train stats after 245152 examples: {'rewards_train/chosen': '0.21202', 'rewards_train/rejected': '0.05577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15625', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-158.27', 'loss/train': '0.64415', 'examples_per_second': '31.261', 'grad_norm': '76', 'counters/examples': 245152, 'counters/updates': 7661}
train stats after 245184 examples: {'rewards_train/chosen': '0.14242', 'rewards_train/rejected': '0.092277', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.050144', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-129.11', 'loss/train': '0.6921', 'examples_per_second': '30.949', 'grad_norm': '77.5', 'counters/examples': 245184, 'counters/updates': 7662}
skipping logging after 245216 examples to avoid logging too frequently
train stats after 245248 examples: {'rewards_train/chosen': '0.10394', 'rewards_train/rejected': '0.050144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053795', 'logps_train/rejected': '-153.87', 'logps_train/chosen': '-154.57', 'loss/train': '0.68422', 'examples_per_second': '30.583', 'grad_norm': '109.5', 'counters/examples': 245248, 'counters/updates': 7664}
train stats after 245280 examples: {'rewards_train/chosen': '0.12075', 'rewards_train/rejected': '0.10913', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011618', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-146.97', 'loss/train': '0.69951', 'examples_per_second': '31.271', 'grad_norm': '74.5', 'counters/examples': 245280, 'counters/updates': 7665}
train stats after 245312 examples: {'rewards_train/chosen': '0.074439', 'rewards_train/rejected': '0.084425', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0099854', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-170.36', 'loss/train': '0.70633', 'examples_per_second': '30.724', 'grad_norm': '64.5', 'counters/examples': 245312, 'counters/updates': 7666}
train stats after 245344 examples: {'rewards_train/chosen': '0.11731', 'rewards_train/rejected': '0.029746', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087561', 'logps_train/rejected': '-118.29', 'logps_train/chosen': '-179.31', 'loss/train': '0.66435', 'examples_per_second': '31.529', 'grad_norm': '56.75', 'counters/examples': 245344, 'counters/updates': 7667}
train stats after 245376 examples: {'rewards_train/chosen': '0.13118', 'rewards_train/rejected': '0.082471', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048713', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-143.53', 'loss/train': '0.67761', 'examples_per_second': '31.473', 'grad_norm': '71.5', 'counters/examples': 245376, 'counters/updates': 7668}
train stats after 245408 examples: {'rewards_train/chosen': '0.03046', 'rewards_train/rejected': '0.047536', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017075', 'logps_train/rejected': '-103.1', 'logps_train/chosen': '-137.18', 'loss/train': '0.71302', 'examples_per_second': '31.577', 'grad_norm': '245', 'counters/examples': 245408, 'counters/updates': 7669}
skipping logging after 245440 examples to avoid logging too frequently
train stats after 245472 examples: {'rewards_train/chosen': '0.035689', 'rewards_train/rejected': '-0.011854', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047543', 'logps_train/rejected': '-114.29', 'logps_train/chosen': '-149.66', 'loss/train': '0.68847', 'examples_per_second': '31.555', 'grad_norm': '141', 'counters/examples': 245472, 'counters/updates': 7671}
train stats after 245504 examples: {'rewards_train/chosen': '0.18142', 'rewards_train/rejected': '0.033999', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14742', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-136.28', 'loss/train': '0.63121', 'examples_per_second': '31.222', 'grad_norm': '146', 'counters/examples': 245504, 'counters/updates': 7672}
train stats after 245536 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '0.021123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15042', 'logps_train/rejected': '-154.89', 'logps_train/chosen': '-162.12', 'loss/train': '0.63607', 'examples_per_second': '31.17', 'grad_norm': '66', 'counters/examples': 245536, 'counters/updates': 7673}
train stats after 245568 examples: {'rewards_train/chosen': '0.12616', 'rewards_train/rejected': '0.17657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.050411', 'logps_train/rejected': '-166.71', 'logps_train/chosen': '-179.88', 'loss/train': '0.7353', 'examples_per_second': '31.645', 'grad_norm': '83.5', 'counters/examples': 245568, 'counters/updates': 7674}
train stats after 245600 examples: {'rewards_train/chosen': '0.14885', 'rewards_train/rejected': '-0.012224', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16107', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-131.96', 'loss/train': '0.62984', 'examples_per_second': '32.37', 'grad_norm': '201', 'counters/examples': 245600, 'counters/updates': 7675}
train stats after 245632 examples: {'rewards_train/chosen': '0.12185', 'rewards_train/rejected': '0.079245', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.042609', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-141.32', 'loss/train': '0.69643', 'examples_per_second': '31.635', 'grad_norm': '91', 'counters/examples': 245632, 'counters/updates': 7676}
train stats after 245664 examples: {'rewards_train/chosen': '0.025049', 'rewards_train/rejected': '0.1487', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.12365', 'logps_train/rejected': '-165.92', 'logps_train/chosen': '-177.43', 'loss/train': '0.77603', 'examples_per_second': '31.613', 'grad_norm': '148', 'counters/examples': 245664, 'counters/updates': 7677}
train stats after 245696 examples: {'rewards_train/chosen': '0.12244', 'rewards_train/rejected': '0.029246', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.093199', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-160.72', 'loss/train': '0.65946', 'examples_per_second': '31.566', 'grad_norm': '126.5', 'counters/examples': 245696, 'counters/updates': 7678}
train stats after 245728 examples: {'rewards_train/chosen': '0.077059', 'rewards_train/rejected': '0.057384', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019674', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-174.53', 'loss/train': '0.71342', 'examples_per_second': '31.537', 'grad_norm': '70', 'counters/examples': 245728, 'counters/updates': 7679}
train stats after 245760 examples: {'rewards_train/chosen': '0.11659', 'rewards_train/rejected': '0.058856', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057739', 'logps_train/rejected': '-115.56', 'logps_train/chosen': '-151.66', 'loss/train': '0.68445', 'examples_per_second': '32.179', 'grad_norm': '58.5', 'counters/examples': 245760, 'counters/updates': 7680}
train stats after 245792 examples: {'rewards_train/chosen': '0.060043', 'rewards_train/rejected': '0.038592', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021452', 'logps_train/rejected': '-152.39', 'logps_train/chosen': '-159.94', 'loss/train': '0.70129', 'examples_per_second': '32.553', 'grad_norm': '93.5', 'counters/examples': 245792, 'counters/updates': 7681}
train stats after 245824 examples: {'rewards_train/chosen': '0.030774', 'rewards_train/rejected': '0.050015', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019241', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-129.08', 'loss/train': '0.71587', 'examples_per_second': '32.777', 'grad_norm': '120.5', 'counters/examples': 245824, 'counters/updates': 7682}
train stats after 245856 examples: {'rewards_train/chosen': '0.083496', 'rewards_train/rejected': '0.019526', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063971', 'logps_train/rejected': '-93.103', 'logps_train/chosen': '-148.37', 'loss/train': '0.67514', 'examples_per_second': '31.486', 'grad_norm': '82.5', 'counters/examples': 245856, 'counters/updates': 7683}
skipping logging after 245888 examples to avoid logging too frequently
train stats after 245920 examples: {'rewards_train/chosen': '0.0829', 'rewards_train/rejected': '0.03069', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05221', 'logps_train/rejected': '-101.43', 'logps_train/chosen': '-123.71', 'loss/train': '0.6732', 'examples_per_second': '35.773', 'grad_norm': '48.25', 'counters/examples': 245920, 'counters/updates': 7685}
train stats after 245952 examples: {'rewards_train/chosen': '0.099919', 'rewards_train/rejected': '0.069577', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030342', 'logps_train/rejected': '-147.72', 'logps_train/chosen': '-159.18', 'loss/train': '0.69376', 'examples_per_second': '31.68', 'grad_norm': '115', 'counters/examples': 245952, 'counters/updates': 7686}
train stats after 245984 examples: {'rewards_train/chosen': '0.18021', 'rewards_train/rejected': '0.10346', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076743', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-136.51', 'loss/train': '0.66648', 'examples_per_second': '31.553', 'grad_norm': '67.5', 'counters/examples': 245984, 'counters/updates': 7687}
train stats after 246016 examples: {'rewards_train/chosen': '0.060676', 'rewards_train/rejected': '0.031321', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029354', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-137.43', 'loss/train': '0.68985', 'examples_per_second': '32.636', 'grad_norm': '92', 'counters/examples': 246016, 'counters/updates': 7688}
skipping logging after 246048 examples to avoid logging too frequently
train stats after 246080 examples: {'rewards_train/chosen': '0.11139', 'rewards_train/rejected': '0.15334', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041952', 'logps_train/rejected': '-165.78', 'logps_train/chosen': '-154.35', 'loss/train': '0.72999', 'examples_per_second': '30.576', 'grad_norm': '194', 'counters/examples': 246080, 'counters/updates': 7690}
train stats after 246112 examples: {'rewards_train/chosen': '0.22052', 'rewards_train/rejected': '0.054844', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16567', 'logps_train/rejected': '-98.088', 'logps_train/chosen': '-147.65', 'loss/train': '0.63344', 'examples_per_second': '22.966', 'grad_norm': '44', 'counters/examples': 246112, 'counters/updates': 7691}
train stats after 246144 examples: {'rewards_train/chosen': '0.14863', 'rewards_train/rejected': '0.056534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092099', 'logps_train/rejected': '-104.78', 'logps_train/chosen': '-138.15', 'loss/train': '0.66688', 'examples_per_second': '31.167', 'grad_norm': '85', 'counters/examples': 246144, 'counters/updates': 7692}
skipping logging after 246176 examples to avoid logging too frequently
train stats after 246208 examples: {'rewards_train/chosen': '0.17867', 'rewards_train/rejected': '0.05792', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12075', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-145.46', 'loss/train': '0.65535', 'examples_per_second': '24.742', 'grad_norm': '72', 'counters/examples': 246208, 'counters/updates': 7694}
train stats after 246240 examples: {'rewards_train/chosen': '0.10821', 'rewards_train/rejected': '0.014568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093643', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-114.67', 'loss/train': '0.67004', 'examples_per_second': '32.451', 'grad_norm': '89.5', 'counters/examples': 246240, 'counters/updates': 7695}
train stats after 246272 examples: {'rewards_train/chosen': '0.2035', 'rewards_train/rejected': '0.037336', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16616', 'logps_train/rejected': '-94.643', 'logps_train/chosen': '-143.71', 'loss/train': '0.63014', 'examples_per_second': '32.436', 'grad_norm': '49.25', 'counters/examples': 246272, 'counters/updates': 7696}
train stats after 246304 examples: {'rewards_train/chosen': '0.11153', 'rewards_train/rejected': '0.056009', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055523', 'logps_train/rejected': '-155.5', 'logps_train/chosen': '-193.96', 'loss/train': '0.68181', 'examples_per_second': '30.821', 'grad_norm': '84', 'counters/examples': 246304, 'counters/updates': 7697}
train stats after 246336 examples: {'rewards_train/chosen': '0.16536', 'rewards_train/rejected': '0.090862', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074497', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-163.2', 'loss/train': '0.67544', 'examples_per_second': '30.92', 'grad_norm': '52.75', 'counters/examples': 246336, 'counters/updates': 7698}
train stats after 246368 examples: {'rewards_train/chosen': '0.1219', 'rewards_train/rejected': '0.13507', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013172', 'logps_train/rejected': '-155', 'logps_train/chosen': '-116.92', 'loss/train': '0.70828', 'examples_per_second': '31.445', 'grad_norm': '112', 'counters/examples': 246368, 'counters/updates': 7699}
train stats after 246400 examples: {'rewards_train/chosen': '0.15054', 'rewards_train/rejected': '0.086862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063677', 'logps_train/rejected': '-148.94', 'logps_train/chosen': '-126.13', 'loss/train': '0.67985', 'examples_per_second': '33.12', 'grad_norm': '93', 'counters/examples': 246400, 'counters/updates': 7700}
train stats after 246432 examples: {'rewards_train/chosen': '0.11768', 'rewards_train/rejected': '0.015885', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10179', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-176.45', 'loss/train': '0.65945', 'examples_per_second': '30.7', 'grad_norm': '67.5', 'counters/examples': 246432, 'counters/updates': 7701}
skipping logging after 246464 examples to avoid logging too frequently
train stats after 246496 examples: {'rewards_train/chosen': '0.016698', 'rewards_train/rejected': '0.01342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0032785', 'logps_train/rejected': '-111.39', 'logps_train/chosen': '-127.94', 'loss/train': '0.70655', 'examples_per_second': '31.685', 'grad_norm': '59.25', 'counters/examples': 246496, 'counters/updates': 7703}
train stats after 246528 examples: {'rewards_train/chosen': '0.1553', 'rewards_train/rejected': '0.05009', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10521', 'logps_train/rejected': '-153.01', 'logps_train/chosen': '-139.11', 'loss/train': '0.65491', 'examples_per_second': '31.595', 'grad_norm': '92.5', 'counters/examples': 246528, 'counters/updates': 7704}
train stats after 246560 examples: {'rewards_train/chosen': '0.091737', 'rewards_train/rejected': '0.018586', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073151', 'logps_train/rejected': '-112.32', 'logps_train/chosen': '-107.58', 'loss/train': '0.66581', 'examples_per_second': '30.221', 'grad_norm': '81.5', 'counters/examples': 246560, 'counters/updates': 7705}
train stats after 246592 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '0.15615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.020369', 'logps_train/rejected': '-118.73', 'logps_train/chosen': '-132.54', 'loss/train': '0.72539', 'examples_per_second': '30.968', 'grad_norm': '100', 'counters/examples': 246592, 'counters/updates': 7706}
train stats after 246624 examples: {'rewards_train/chosen': '0.1944', 'rewards_train/rejected': '0.054927', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13948', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-156.43', 'loss/train': '0.64383', 'examples_per_second': '30.377', 'grad_norm': '89', 'counters/examples': 246624, 'counters/updates': 7707}
train stats after 246656 examples: {'rewards_train/chosen': '0.22499', 'rewards_train/rejected': '0.11569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1093', 'logps_train/rejected': '-151.4', 'logps_train/chosen': '-139.52', 'loss/train': '0.66101', 'examples_per_second': '30.853', 'grad_norm': '62', 'counters/examples': 246656, 'counters/updates': 7708}
train stats after 246688 examples: {'rewards_train/chosen': '0.1221', 'rewards_train/rejected': '0.10061', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021494', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-117.18', 'loss/train': '0.69135', 'examples_per_second': '31.953', 'grad_norm': '90', 'counters/examples': 246688, 'counters/updates': 7709}
skipping logging after 246720 examples to avoid logging too frequently
train stats after 246752 examples: {'rewards_train/chosen': '0.1612', 'rewards_train/rejected': '0.073183', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088018', 'logps_train/rejected': '-150.14', 'logps_train/chosen': '-174.92', 'loss/train': '0.66457', 'examples_per_second': '31.653', 'grad_norm': '102', 'counters/examples': 246752, 'counters/updates': 7711}
train stats after 246784 examples: {'rewards_train/chosen': '0.15119', 'rewards_train/rejected': '0.088207', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062985', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-139.89', 'loss/train': '0.6792', 'examples_per_second': '24.102', 'grad_norm': '48.25', 'counters/examples': 246784, 'counters/updates': 7712}
train stats after 246816 examples: {'rewards_train/chosen': '0.17362', 'rewards_train/rejected': '0.046474', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12714', 'logps_train/rejected': '-117.66', 'logps_train/chosen': '-118.86', 'loss/train': '0.64481', 'examples_per_second': '32.287', 'grad_norm': '71', 'counters/examples': 246816, 'counters/updates': 7713}
train stats after 246848 examples: {'rewards_train/chosen': '0.15503', 'rewards_train/rejected': '0.15701', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0019785', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-140.66', 'loss/train': '0.71426', 'examples_per_second': '31.663', 'grad_norm': '92.5', 'counters/examples': 246848, 'counters/updates': 7714}
train stats after 246880 examples: {'rewards_train/chosen': '0.10011', 'rewards_train/rejected': '0.062508', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037605', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-152.61', 'loss/train': '0.69183', 'examples_per_second': '31.829', 'grad_norm': '83.5', 'counters/examples': 246880, 'counters/updates': 7715}
skipping logging after 246912 examples to avoid logging too frequently
train stats after 246944 examples: {'rewards_train/chosen': '0.093078', 'rewards_train/rejected': '0.014854', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078225', 'logps_train/rejected': '-118.85', 'logps_train/chosen': '-146.97', 'loss/train': '0.66484', 'examples_per_second': '31.675', 'grad_norm': '60.25', 'counters/examples': 246944, 'counters/updates': 7717}
train stats after 246976 examples: {'rewards_train/chosen': '0.1383', 'rewards_train/rejected': '0.014233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12406', 'logps_train/rejected': '-151.71', 'logps_train/chosen': '-190.16', 'loss/train': '0.64154', 'examples_per_second': '30.231', 'grad_norm': '106.5', 'counters/examples': 246976, 'counters/updates': 7718}
train stats after 247008 examples: {'rewards_train/chosen': '0.16233', 'rewards_train/rejected': '0.02174', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14059', 'logps_train/rejected': '-161.23', 'logps_train/chosen': '-154.21', 'loss/train': '0.63734', 'examples_per_second': '31.646', 'grad_norm': '92.5', 'counters/examples': 247008, 'counters/updates': 7719}
skipping logging after 247040 examples to avoid logging too frequently
train stats after 247072 examples: {'rewards_train/chosen': '0.085258', 'rewards_train/rejected': '0.0052799', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079978', 'logps_train/rejected': '-147.81', 'logps_train/chosen': '-114.59', 'loss/train': '0.67137', 'examples_per_second': '30.601', 'grad_norm': '98.5', 'counters/examples': 247072, 'counters/updates': 7721}
train stats after 247104 examples: {'rewards_train/chosen': '0.25218', 'rewards_train/rejected': '0.044509', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20767', 'logps_train/rejected': '-164.37', 'logps_train/chosen': '-190.45', 'loss/train': '0.60961', 'examples_per_second': '30.233', 'grad_norm': '108', 'counters/examples': 247104, 'counters/updates': 7722}
train stats after 247136 examples: {'rewards_train/chosen': '0.061691', 'rewards_train/rejected': '0.03184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029852', 'logps_train/rejected': '-149.01', 'logps_train/chosen': '-122.18', 'loss/train': '0.69536', 'examples_per_second': '31.719', 'grad_norm': '70.5', 'counters/examples': 247136, 'counters/updates': 7723}
train stats after 247168 examples: {'rewards_train/chosen': '0.18349', 'rewards_train/rejected': '0.017764', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16572', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-183.37', 'loss/train': '0.63045', 'examples_per_second': '33.34', 'grad_norm': '92.5', 'counters/examples': 247168, 'counters/updates': 7724}
train stats after 247200 examples: {'rewards_train/chosen': '0.11937', 'rewards_train/rejected': '0.10363', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015743', 'logps_train/rejected': '-168.75', 'logps_train/chosen': '-166.71', 'loss/train': '0.70003', 'examples_per_second': '31.032', 'grad_norm': '71.5', 'counters/examples': 247200, 'counters/updates': 7725}
train stats after 247232 examples: {'rewards_train/chosen': '0.11725', 'rewards_train/rejected': '0.045839', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07141', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-164.69', 'loss/train': '0.66644', 'examples_per_second': '31.436', 'grad_norm': '85.5', 'counters/examples': 247232, 'counters/updates': 7726}
skipping logging after 247264 examples to avoid logging too frequently
train stats after 247296 examples: {'rewards_train/chosen': '0.12754', 'rewards_train/rejected': '0.033749', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09379', 'logps_train/rejected': '-171.48', 'logps_train/chosen': '-149.54', 'loss/train': '0.6607', 'examples_per_second': '30.557', 'grad_norm': '88.5', 'counters/examples': 247296, 'counters/updates': 7728}
train stats after 247328 examples: {'rewards_train/chosen': '0.068688', 'rewards_train/rejected': '-0.033228', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10192', 'logps_train/rejected': '-158.22', 'logps_train/chosen': '-174.77', 'loss/train': '0.65626', 'examples_per_second': '30.138', 'grad_norm': '67', 'counters/examples': 247328, 'counters/updates': 7729}
skipping logging after 247360 examples to avoid logging too frequently
train stats after 247392 examples: {'rewards_train/chosen': '0.13656', 'rewards_train/rejected': '0.10453', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032024', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-181', 'loss/train': '0.69486', 'examples_per_second': '31.664', 'grad_norm': '86.5', 'counters/examples': 247392, 'counters/updates': 7731}
train stats after 247424 examples: {'rewards_train/chosen': '0.099395', 'rewards_train/rejected': '-0.013304', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1127', 'logps_train/rejected': '-117.07', 'logps_train/chosen': '-132.03', 'loss/train': '0.65022', 'examples_per_second': '31.813', 'grad_norm': '102', 'counters/examples': 247424, 'counters/updates': 7732}
train stats after 247456 examples: {'rewards_train/chosen': '0.16876', 'rewards_train/rejected': '0.1228', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045954', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-133.34', 'loss/train': '0.68706', 'examples_per_second': '30.705', 'grad_norm': '153', 'counters/examples': 247456, 'counters/updates': 7733}
train stats after 247488 examples: {'rewards_train/chosen': '0.085924', 'rewards_train/rejected': '0.033112', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052812', 'logps_train/rejected': '-120.83', 'logps_train/chosen': '-142.8', 'loss/train': '0.67951', 'examples_per_second': '32.218', 'grad_norm': '165', 'counters/examples': 247488, 'counters/updates': 7734}
train stats after 247520 examples: {'rewards_train/chosen': '0.20817', 'rewards_train/rejected': '0.076234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13194', 'logps_train/rejected': '-140.73', 'logps_train/chosen': '-167.41', 'loss/train': '0.65856', 'examples_per_second': '33.306', 'grad_norm': '75.5', 'counters/examples': 247520, 'counters/updates': 7735}
train stats after 247552 examples: {'rewards_train/chosen': '0.20289', 'rewards_train/rejected': '0.071393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1315', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-165.66', 'loss/train': '0.64751', 'examples_per_second': '32.451', 'grad_norm': '135', 'counters/examples': 247552, 'counters/updates': 7736}
skipping logging after 247584 examples to avoid logging too frequently
train stats after 247616 examples: {'rewards_train/chosen': '0.16719', 'rewards_train/rejected': '0.067919', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099271', 'logps_train/rejected': '-135.03', 'logps_train/chosen': '-138.64', 'loss/train': '0.66643', 'examples_per_second': '31.508', 'grad_norm': '83', 'counters/examples': 247616, 'counters/updates': 7738}
skipping logging after 247648 examples to avoid logging too frequently
train stats after 247680 examples: {'rewards_train/chosen': '0.11806', 'rewards_train/rejected': '-0.013131', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13119', 'logps_train/rejected': '-105.71', 'logps_train/chosen': '-131.72', 'loss/train': '0.64064', 'examples_per_second': '32.719', 'grad_norm': '61.75', 'counters/examples': 247680, 'counters/updates': 7740}
train stats after 247712 examples: {'rewards_train/chosen': '0.15958', 'rewards_train/rejected': '0.13677', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022814', 'logps_train/rejected': '-114.88', 'logps_train/chosen': '-182.92', 'loss/train': '0.69783', 'examples_per_second': '31.654', 'grad_norm': '229', 'counters/examples': 247712, 'counters/updates': 7741}
train stats after 247744 examples: {'rewards_train/chosen': '0.11699', 'rewards_train/rejected': '0.093443', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023543', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-125.78', 'loss/train': '0.69727', 'examples_per_second': '32.84', 'grad_norm': '72.5', 'counters/examples': 247744, 'counters/updates': 7742}
skipping logging after 247776 examples to avoid logging too frequently
train stats after 247808 examples: {'rewards_train/chosen': '0.088519', 'rewards_train/rejected': '-0.013352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10187', 'logps_train/rejected': '-93.739', 'logps_train/chosen': '-123.87', 'loss/train': '0.65371', 'examples_per_second': '31.589', 'grad_norm': '84', 'counters/examples': 247808, 'counters/updates': 7744}
train stats after 247840 examples: {'rewards_train/chosen': '0.072624', 'rewards_train/rejected': '0.061342', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011282', 'logps_train/rejected': '-101.88', 'logps_train/chosen': '-122.97', 'loss/train': '0.69366', 'examples_per_second': '30.69', 'grad_norm': '56.25', 'counters/examples': 247840, 'counters/updates': 7745}
train stats after 247872 examples: {'rewards_train/chosen': '0.13982', 'rewards_train/rejected': '0.014437', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12538', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-123.98', 'loss/train': '0.64768', 'examples_per_second': '30.598', 'grad_norm': '94.5', 'counters/examples': 247872, 'counters/updates': 7746}
train stats after 247904 examples: {'rewards_train/chosen': '0.042842', 'rewards_train/rejected': '0.057953', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01511', 'logps_train/rejected': '-133.3', 'logps_train/chosen': '-146.01', 'loss/train': '0.72844', 'examples_per_second': '31.688', 'grad_norm': '129', 'counters/examples': 247904, 'counters/updates': 7747}
skipping logging after 247936 examples to avoid logging too frequently
train stats after 247968 examples: {'rewards_train/chosen': '0.20898', 'rewards_train/rejected': '0.042289', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16669', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-153.87', 'loss/train': '0.63106', 'examples_per_second': '30.586', 'grad_norm': '142', 'counters/examples': 247968, 'counters/updates': 7749}
train stats after 248000 examples: {'rewards_train/chosen': '0.14906', 'rewards_train/rejected': '0.092811', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.056251', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-123.12', 'loss/train': '0.68085', 'examples_per_second': '31.345', 'grad_norm': '75', 'counters/examples': 248000, 'counters/updates': 7750}
train stats after 248032 examples: {'rewards_train/chosen': '0.1805', 'rewards_train/rejected': '-0.006712', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18721', 'logps_train/rejected': '-133.44', 'logps_train/chosen': '-186.72', 'loss/train': '0.6217', 'examples_per_second': '31.403', 'grad_norm': '60', 'counters/examples': 248032, 'counters/updates': 7751}
train stats after 248064 examples: {'rewards_train/chosen': '0.097318', 'rewards_train/rejected': '0.10233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0050146', 'logps_train/rejected': '-122.69', 'logps_train/chosen': '-154.07', 'loss/train': '0.71358', 'examples_per_second': '31.667', 'grad_norm': '84', 'counters/examples': 248064, 'counters/updates': 7752}
train stats after 248096 examples: {'rewards_train/chosen': '0.06263', 'rewards_train/rejected': '0.019551', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.043079', 'logps_train/rejected': '-126.92', 'logps_train/chosen': '-134.52', 'loss/train': '0.68118', 'examples_per_second': '30.498', 'grad_norm': '74.5', 'counters/examples': 248096, 'counters/updates': 7753}
train stats after 248128 examples: {'rewards_train/chosen': '0.21668', 'rewards_train/rejected': '0.043035', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17364', 'logps_train/rejected': '-148.73', 'logps_train/chosen': '-187.39', 'loss/train': '0.63364', 'examples_per_second': '31.747', 'grad_norm': '79.5', 'counters/examples': 248128, 'counters/updates': 7754}
train stats after 248160 examples: {'rewards_train/chosen': '0.1065', 'rewards_train/rejected': '0.095928', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010567', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-134.11', 'loss/train': '0.7075', 'examples_per_second': '30.921', 'grad_norm': '108.5', 'counters/examples': 248160, 'counters/updates': 7755}
train stats after 248192 examples: {'rewards_train/chosen': '0.076973', 'rewards_train/rejected': '0.032773', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0442', 'logps_train/rejected': '-110.41', 'logps_train/chosen': '-134.19', 'loss/train': '0.68232', 'examples_per_second': '31.849', 'grad_norm': '62.25', 'counters/examples': 248192, 'counters/updates': 7756}
train stats after 248224 examples: {'rewards_train/chosen': '0.16983', 'rewards_train/rejected': '0.10695', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062882', 'logps_train/rejected': '-105.45', 'logps_train/chosen': '-171.23', 'loss/train': '0.6812', 'examples_per_second': '33.078', 'grad_norm': '89', 'counters/examples': 248224, 'counters/updates': 7757}
skipping logging after 248256 examples to avoid logging too frequently
train stats after 248288 examples: {'rewards_train/chosen': '0.23931', 'rewards_train/rejected': '0.086792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15252', 'logps_train/rejected': '-163.76', 'logps_train/chosen': '-142.31', 'loss/train': '0.6555', 'examples_per_second': '31.565', 'grad_norm': '82', 'counters/examples': 248288, 'counters/updates': 7759}
train stats after 248320 examples: {'rewards_train/chosen': '0.12428', 'rewards_train/rejected': '0.070406', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053874', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-133.32', 'loss/train': '0.67702', 'examples_per_second': '30.812', 'grad_norm': '85', 'counters/examples': 248320, 'counters/updates': 7760}
skipping logging after 248352 examples to avoid logging too frequently
train stats after 248384 examples: {'rewards_train/chosen': '0.13475', 'rewards_train/rejected': '0.087796', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046956', 'logps_train/rejected': '-139.78', 'logps_train/chosen': '-129.05', 'loss/train': '0.67984', 'examples_per_second': '33.828', 'grad_norm': '69', 'counters/examples': 248384, 'counters/updates': 7762}
train stats after 248416 examples: {'rewards_train/chosen': '0.087313', 'rewards_train/rejected': '0.14546', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.058145', 'logps_train/rejected': '-143.93', 'logps_train/chosen': '-140.24', 'loss/train': '0.76216', 'examples_per_second': '30.204', 'grad_norm': '101', 'counters/examples': 248416, 'counters/updates': 7763}
skipping logging after 248448 examples to avoid logging too frequently
train stats after 248480 examples: {'rewards_train/chosen': '0.085577', 'rewards_train/rejected': '0.029605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055972', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-145.93', 'loss/train': '0.68016', 'examples_per_second': '31.681', 'grad_norm': '130', 'counters/examples': 248480, 'counters/updates': 7765}
train stats after 248512 examples: {'rewards_train/chosen': '0.12722', 'rewards_train/rejected': '0.020291', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10693', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-169.34', 'loss/train': '0.65012', 'examples_per_second': '31.244', 'grad_norm': '77', 'counters/examples': 248512, 'counters/updates': 7766}
train stats after 248544 examples: {'rewards_train/chosen': '0.19671', 'rewards_train/rejected': '0.11276', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083953', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-170.13', 'loss/train': '0.67438', 'examples_per_second': '31.679', 'grad_norm': '99.5', 'counters/examples': 248544, 'counters/updates': 7767}
train stats after 248576 examples: {'rewards_train/chosen': '0.072398', 'rewards_train/rejected': '0.067505', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0048927', 'logps_train/rejected': '-168.17', 'logps_train/chosen': '-166.74', 'loss/train': '0.71296', 'examples_per_second': '30.25', 'grad_norm': '121.5', 'counters/examples': 248576, 'counters/updates': 7768}
train stats after 248608 examples: {'rewards_train/chosen': '0.13737', 'rewards_train/rejected': '0.098414', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038953', 'logps_train/rejected': '-174.72', 'logps_train/chosen': '-146.09', 'loss/train': '0.69317', 'examples_per_second': '31.67', 'grad_norm': '78.5', 'counters/examples': 248608, 'counters/updates': 7769}
train stats after 248640 examples: {'rewards_train/chosen': '0.066135', 'rewards_train/rejected': '0.066987', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00085197', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-136.59', 'loss/train': '0.71289', 'examples_per_second': '31.575', 'grad_norm': '105.5', 'counters/examples': 248640, 'counters/updates': 7770}
skipping logging after 248672 examples to avoid logging too frequently
train stats after 248704 examples: {'rewards_train/chosen': '0.087536', 'rewards_train/rejected': '0.080259', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0072773', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-132.6', 'loss/train': '0.7008', 'examples_per_second': '33.148', 'grad_norm': '94.5', 'counters/examples': 248704, 'counters/updates': 7772}
train stats after 248736 examples: {'rewards_train/chosen': '0.12899', 'rewards_train/rejected': '0.11792', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011065', 'logps_train/rejected': '-209.68', 'logps_train/chosen': '-187.25', 'loss/train': '0.70125', 'examples_per_second': '31.635', 'grad_norm': '104.5', 'counters/examples': 248736, 'counters/updates': 7773}
train stats after 248768 examples: {'rewards_train/chosen': '0.10279', 'rewards_train/rejected': '0.1092', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0064083', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-141.78', 'loss/train': '0.71092', 'examples_per_second': '31.501', 'grad_norm': '94', 'counters/examples': 248768, 'counters/updates': 7774}
skipping logging after 248800 examples to avoid logging too frequently
train stats after 248832 examples: {'rewards_train/chosen': '0.13018', 'rewards_train/rejected': '0.041313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088868', 'logps_train/rejected': '-140.44', 'logps_train/chosen': '-166.21', 'loss/train': '0.66627', 'examples_per_second': '30.185', 'grad_norm': '72', 'counters/examples': 248832, 'counters/updates': 7776}
train stats after 248864 examples: {'rewards_train/chosen': '0.15826', 'rewards_train/rejected': '0.02369', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13457', 'logps_train/rejected': '-146.57', 'logps_train/chosen': '-154.35', 'loss/train': '0.65055', 'examples_per_second': '30.139', 'grad_norm': '86.5', 'counters/examples': 248864, 'counters/updates': 7777}
train stats after 248896 examples: {'rewards_train/chosen': '0.2378', 'rewards_train/rejected': '0.090689', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14712', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-173.51', 'loss/train': '0.65184', 'examples_per_second': '32.798', 'grad_norm': '109', 'counters/examples': 248896, 'counters/updates': 7778}
skipping logging after 248928 examples to avoid logging too frequently
train stats after 248960 examples: {'rewards_train/chosen': '0.043388', 'rewards_train/rejected': '0.028395', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014993', 'logps_train/rejected': '-128.17', 'logps_train/chosen': '-154.01', 'loss/train': '0.71438', 'examples_per_second': '31.655', 'grad_norm': '94.5', 'counters/examples': 248960, 'counters/updates': 7780}
train stats after 248992 examples: {'rewards_train/chosen': '0.18321', 'rewards_train/rejected': '0.067987', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11523', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-225.96', 'loss/train': '0.67822', 'examples_per_second': '32.998', 'grad_norm': '276', 'counters/examples': 248992, 'counters/updates': 7781}
skipping logging after 249024 examples to avoid logging too frequently
train stats after 249056 examples: {'rewards_train/chosen': '0.077872', 'rewards_train/rejected': '0.090319', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012447', 'logps_train/rejected': '-101.05', 'logps_train/chosen': '-124.71', 'loss/train': '0.71602', 'examples_per_second': '30.507', 'grad_norm': '209', 'counters/examples': 249056, 'counters/updates': 7783}
train stats after 249088 examples: {'rewards_train/chosen': '0.12331', 'rewards_train/rejected': '-0.048305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17161', 'logps_train/rejected': '-133.39', 'logps_train/chosen': '-167.46', 'loss/train': '0.63596', 'examples_per_second': '31.422', 'grad_norm': '77.5', 'counters/examples': 249088, 'counters/updates': 7784}
train stats after 249120 examples: {'rewards_train/chosen': '0.1369', 'rewards_train/rejected': '0.036477', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10042', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-166.45', 'loss/train': '0.66188', 'examples_per_second': '31.66', 'grad_norm': '239', 'counters/examples': 249120, 'counters/updates': 7785}
skipping logging after 249152 examples to avoid logging too frequently
train stats after 249184 examples: {'rewards_train/chosen': '0.026836', 'rewards_train/rejected': '-0.0076214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034458', 'logps_train/rejected': '-142.78', 'logps_train/chosen': '-121.79', 'loss/train': '0.68881', 'examples_per_second': '30.809', 'grad_norm': '51.25', 'counters/examples': 249184, 'counters/updates': 7787}
train stats after 249216 examples: {'rewards_train/chosen': '0.11213', 'rewards_train/rejected': '0.00057989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11155', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-183.51', 'loss/train': '0.64933', 'examples_per_second': '30.588', 'grad_norm': '78.5', 'counters/examples': 249216, 'counters/updates': 7788}
skipping logging after 249248 examples to avoid logging too frequently
train stats after 249280 examples: {'rewards_train/chosen': '0.16771', 'rewards_train/rejected': '0.073639', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094075', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-168.19', 'loss/train': '0.67575', 'examples_per_second': '30.15', 'grad_norm': '90.5', 'counters/examples': 249280, 'counters/updates': 7790}
train stats after 249312 examples: {'rewards_train/chosen': '0.064217', 'rewards_train/rejected': '0.030029', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034188', 'logps_train/rejected': '-127.56', 'logps_train/chosen': '-99.048', 'loss/train': '0.68731', 'examples_per_second': '30.883', 'grad_norm': '79.5', 'counters/examples': 249312, 'counters/updates': 7791}
train stats after 249344 examples: {'rewards_train/chosen': '0.20428', 'rewards_train/rejected': '0.086498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11778', 'logps_train/rejected': '-109.91', 'logps_train/chosen': '-155.42', 'loss/train': '0.66153', 'examples_per_second': '30.495', 'grad_norm': '69.5', 'counters/examples': 249344, 'counters/updates': 7792}
train stats after 249376 examples: {'rewards_train/chosen': '0.19016', 'rewards_train/rejected': '0.075669', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11449', 'logps_train/rejected': '-147.02', 'logps_train/chosen': '-153.68', 'loss/train': '0.65176', 'examples_per_second': '31.316', 'grad_norm': '81', 'counters/examples': 249376, 'counters/updates': 7793}
train stats after 249408 examples: {'rewards_train/chosen': '0.22676', 'rewards_train/rejected': '0.084838', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14193', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-176.27', 'loss/train': '0.63881', 'examples_per_second': '32.133', 'grad_norm': '146', 'counters/examples': 249408, 'counters/updates': 7794}
train stats after 249440 examples: {'rewards_train/chosen': '0.054058', 'rewards_train/rejected': '0.029695', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.024363', 'logps_train/rejected': '-169.77', 'logps_train/chosen': '-189.3', 'loss/train': '0.69662', 'examples_per_second': '31.413', 'grad_norm': '90', 'counters/examples': 249440, 'counters/updates': 7795}
skipping logging after 249472 examples to avoid logging too frequently
train stats after 249504 examples: {'rewards_train/chosen': '0.15035', 'rewards_train/rejected': '0.026235', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12412', 'logps_train/rejected': '-94.549', 'logps_train/chosen': '-117.49', 'loss/train': '0.6514', 'examples_per_second': '41.333', 'grad_norm': '54.5', 'counters/examples': 249504, 'counters/updates': 7797}
train stats after 249536 examples: {'rewards_train/chosen': '0.10244', 'rewards_train/rejected': '0.0093638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093078', 'logps_train/rejected': '-109.08', 'logps_train/chosen': '-116.86', 'loss/train': '0.65639', 'examples_per_second': '31.043', 'grad_norm': '59.25', 'counters/examples': 249536, 'counters/updates': 7798}
train stats after 249568 examples: {'rewards_train/chosen': '0.1131', 'rewards_train/rejected': '-0.010431', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12354', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-135.45', 'loss/train': '0.64419', 'examples_per_second': '31.645', 'grad_norm': '68', 'counters/examples': 249568, 'counters/updates': 7799}
train stats after 249600 examples: {'rewards_train/chosen': '0.13725', 'rewards_train/rejected': '0.12817', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0090769', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-161.17', 'loss/train': '0.70045', 'examples_per_second': '32.767', 'grad_norm': '95', 'counters/examples': 249600, 'counters/updates': 7800}
train stats after 249632 examples: {'rewards_train/chosen': '0.15035', 'rewards_train/rejected': '-0.046215', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19657', 'logps_train/rejected': '-158.87', 'logps_train/chosen': '-220.06', 'loss/train': '0.61962', 'examples_per_second': '31.64', 'grad_norm': '137', 'counters/examples': 249632, 'counters/updates': 7801}
train stats after 249664 examples: {'rewards_train/chosen': '0.068083', 'rewards_train/rejected': '0.099969', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.031887', 'logps_train/rejected': '-185.12', 'logps_train/chosen': '-148.98', 'loss/train': '0.72125', 'examples_per_second': '31.594', 'grad_norm': '86', 'counters/examples': 249664, 'counters/updates': 7802}
train stats after 249696 examples: {'rewards_train/chosen': '0.17694', 'rewards_train/rejected': '0.074316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10262', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-155.15', 'loss/train': '0.65839', 'examples_per_second': '32.661', 'grad_norm': '86', 'counters/examples': 249696, 'counters/updates': 7803}
train stats after 249728 examples: {'rewards_train/chosen': '0.15567', 'rewards_train/rejected': '0.027254', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12841', 'logps_train/rejected': '-102.96', 'logps_train/chosen': '-140.39', 'loss/train': '0.63956', 'examples_per_second': '30.602', 'grad_norm': '55', 'counters/examples': 249728, 'counters/updates': 7804}
skipping logging after 249760 examples to avoid logging too frequently
train stats after 249792 examples: {'rewards_train/chosen': '0.14881', 'rewards_train/rejected': '0.053088', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095725', 'logps_train/rejected': '-113.84', 'logps_train/chosen': '-119.14', 'loss/train': '0.65986', 'examples_per_second': '30.662', 'grad_norm': '83', 'counters/examples': 249792, 'counters/updates': 7806}
train stats after 249824 examples: {'rewards_train/chosen': '0.17345', 'rewards_train/rejected': '0.078794', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094656', 'logps_train/rejected': '-160.62', 'logps_train/chosen': '-162.05', 'loss/train': '0.65744', 'examples_per_second': '31.624', 'grad_norm': '113.5', 'counters/examples': 249824, 'counters/updates': 7807}
train stats after 249856 examples: {'rewards_train/chosen': '0.050934', 'rewards_train/rejected': '0.1007', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.049763', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-135.93', 'loss/train': '0.72681', 'examples_per_second': '32.802', 'grad_norm': '134', 'counters/examples': 249856, 'counters/updates': 7808}
train stats after 249888 examples: {'rewards_train/chosen': '0.12311', 'rewards_train/rejected': '-0.0082838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1314', 'logps_train/rejected': '-99.133', 'logps_train/chosen': '-117.95', 'loss/train': '0.64186', 'examples_per_second': '31.353', 'grad_norm': '57.5', 'counters/examples': 249888, 'counters/updates': 7809}
train stats after 249920 examples: {'rewards_train/chosen': '0.11391', 'rewards_train/rejected': '-0.015422', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12933', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-152.47', 'loss/train': '0.63944', 'examples_per_second': '30.813', 'grad_norm': '81.5', 'counters/examples': 249920, 'counters/updates': 7810}
train stats after 249952 examples: {'rewards_train/chosen': '0.16668', 'rewards_train/rejected': '-0.005531', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17221', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-131.85', 'loss/train': '0.6251', 'examples_per_second': '31.645', 'grad_norm': '59.5', 'counters/examples': 249952, 'counters/updates': 7811}
train stats after 249984 examples: {'rewards_train/chosen': '0.073919', 'rewards_train/rejected': '0.054201', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019718', 'logps_train/rejected': '-157.21', 'logps_train/chosen': '-163.57', 'loss/train': '0.71926', 'examples_per_second': '30.111', 'grad_norm': '127', 'counters/examples': 249984, 'counters/updates': 7812}
skipping logging after 250016 examples to avoid logging too frequently
train stats after 250048 examples: {'rewards_train/chosen': '0.18474', 'rewards_train/rejected': '0.032438', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1523', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-156.47', 'loss/train': '0.62727', 'examples_per_second': '31.635', 'grad_norm': '108', 'counters/examples': 250048, 'counters/updates': 7814}
train stats after 250080 examples: {'rewards_train/chosen': '0.10724', 'rewards_train/rejected': '0.055824', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051417', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-122.2', 'loss/train': '0.6716', 'examples_per_second': '32.734', 'grad_norm': '87', 'counters/examples': 250080, 'counters/updates': 7815}
skipping logging after 250112 examples to avoid logging too frequently
train stats after 250144 examples: {'rewards_train/chosen': '0.12499', 'rewards_train/rejected': '0.13653', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.011543', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-179.55', 'loss/train': '0.7336', 'examples_per_second': '32.745', 'grad_norm': '152', 'counters/examples': 250144, 'counters/updates': 7817}
train stats after 250176 examples: {'rewards_train/chosen': '0.083853', 'rewards_train/rejected': '0.0053857', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078468', 'logps_train/rejected': '-137.2', 'logps_train/chosen': '-144.75', 'loss/train': '0.66765', 'examples_per_second': '30.216', 'grad_norm': '86', 'counters/examples': 250176, 'counters/updates': 7818}
skipping logging after 250208 examples to avoid logging too frequently
train stats after 250240 examples: {'rewards_train/chosen': '0.17639', 'rewards_train/rejected': '-0.019227', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19561', 'logps_train/rejected': '-103.85', 'logps_train/chosen': '-145.23', 'loss/train': '0.61747', 'examples_per_second': '33.686', 'grad_norm': '77.5', 'counters/examples': 250240, 'counters/updates': 7820}
train stats after 250272 examples: {'rewards_train/chosen': '0.1395', 'rewards_train/rejected': '0.0047607', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13473', 'logps_train/rejected': '-109.68', 'logps_train/chosen': '-163.91', 'loss/train': '0.64172', 'examples_per_second': '32.5', 'grad_norm': '70', 'counters/examples': 250272, 'counters/updates': 7821}
train stats after 250304 examples: {'rewards_train/chosen': '0.093015', 'rewards_train/rejected': '0.01859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074425', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-123.7', 'loss/train': '0.6738', 'examples_per_second': '32.605', 'grad_norm': '76.5', 'counters/examples': 250304, 'counters/updates': 7822}
train stats after 250336 examples: {'rewards_train/chosen': '0.085371', 'rewards_train/rejected': '0.11322', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027851', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-149.15', 'loss/train': '0.72119', 'examples_per_second': '31.271', 'grad_norm': '91.5', 'counters/examples': 250336, 'counters/updates': 7823}
train stats after 250368 examples: {'rewards_train/chosen': '0.052783', 'rewards_train/rejected': '-0.019519', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.072302', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-134.47', 'loss/train': '0.67293', 'examples_per_second': '31.585', 'grad_norm': '239', 'counters/examples': 250368, 'counters/updates': 7824}
skipping logging after 250400 examples to avoid logging too frequently
train stats after 250432 examples: {'rewards_train/chosen': '0.092348', 'rewards_train/rejected': '0.23876', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.14641', 'logps_train/rejected': '-195.45', 'logps_train/chosen': '-158.21', 'loss/train': '0.78645', 'examples_per_second': '31.658', 'grad_norm': '160', 'counters/examples': 250432, 'counters/updates': 7826}
train stats after 250464 examples: {'rewards_train/chosen': '0.24239', 'rewards_train/rejected': '0.15684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085547', 'logps_train/rejected': '-181.53', 'logps_train/chosen': '-162.75', 'loss/train': '0.66534', 'examples_per_second': '30.057', 'grad_norm': '95', 'counters/examples': 250464, 'counters/updates': 7827}
skipping logging after 250496 examples to avoid logging too frequently
train stats after 250528 examples: {'rewards_train/chosen': '0.13389', 'rewards_train/rejected': '0.038016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095874', 'logps_train/rejected': '-110.27', 'logps_train/chosen': '-133.14', 'loss/train': '0.65959', 'examples_per_second': '31.22', 'grad_norm': '174', 'counters/examples': 250528, 'counters/updates': 7829}
train stats after 250560 examples: {'rewards_train/chosen': '0.11185', 'rewards_train/rejected': '-0.024046', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1359', 'logps_train/rejected': '-113.72', 'logps_train/chosen': '-119.24', 'loss/train': '0.64303', 'examples_per_second': '32.742', 'grad_norm': '62', 'counters/examples': 250560, 'counters/updates': 7830}
train stats after 250592 examples: {'rewards_train/chosen': '0.069418', 'rewards_train/rejected': '0.081066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011648', 'logps_train/rejected': '-140.36', 'logps_train/chosen': '-139.57', 'loss/train': '0.71081', 'examples_per_second': '30.924', 'grad_norm': '70', 'counters/examples': 250592, 'counters/updates': 7831}
train stats after 250624 examples: {'rewards_train/chosen': '0.09384', 'rewards_train/rejected': '0.043674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050166', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-150.8', 'loss/train': '0.68631', 'examples_per_second': '31.567', 'grad_norm': '202', 'counters/examples': 250624, 'counters/updates': 7832}
train stats after 250656 examples: {'rewards_train/chosen': '0.17106', 'rewards_train/rejected': '0.040399', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13066', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-145.59', 'loss/train': '0.63611', 'examples_per_second': '32.449', 'grad_norm': '84.5', 'counters/examples': 250656, 'counters/updates': 7833}
train stats after 250688 examples: {'rewards_train/chosen': '0.17064', 'rewards_train/rejected': '0.052281', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11836', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-183.98', 'loss/train': '0.64341', 'examples_per_second': '31.865', 'grad_norm': '87', 'counters/examples': 250688, 'counters/updates': 7834}
train stats after 250720 examples: {'rewards_train/chosen': '0.13635', 'rewards_train/rejected': '0.0072144', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12914', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-171.53', 'loss/train': '0.64108', 'examples_per_second': '31.154', 'grad_norm': '164', 'counters/examples': 250720, 'counters/updates': 7835}
train stats after 250752 examples: {'rewards_train/chosen': '0.16712', 'rewards_train/rejected': '-0.0038301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17095', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-121.55', 'loss/train': '0.63139', 'examples_per_second': '32.067', 'grad_norm': '55', 'counters/examples': 250752, 'counters/updates': 7836}
train stats after 250784 examples: {'rewards_train/chosen': '0.12917', 'rewards_train/rejected': '0.097851', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031314', 'logps_train/rejected': '-119.85', 'logps_train/chosen': '-137.05', 'loss/train': '0.69157', 'examples_per_second': '31.145', 'grad_norm': '126', 'counters/examples': 250784, 'counters/updates': 7837}
train stats after 250816 examples: {'rewards_train/chosen': '0.13395', 'rewards_train/rejected': '0.095148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0388', 'logps_train/rejected': '-137.03', 'logps_train/chosen': '-153.75', 'loss/train': '0.68665', 'examples_per_second': '30.934', 'grad_norm': '55.5', 'counters/examples': 250816, 'counters/updates': 7838}
train stats after 250848 examples: {'rewards_train/chosen': '0.13309', 'rewards_train/rejected': '0.022147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11094', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-140.25', 'loss/train': '0.65232', 'examples_per_second': '30.68', 'grad_norm': '176', 'counters/examples': 250848, 'counters/updates': 7839}
skipping logging after 250880 examples to avoid logging too frequently
train stats after 250912 examples: {'rewards_train/chosen': '0.15016', 'rewards_train/rejected': '0.042092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10807', 'logps_train/rejected': '-163.29', 'logps_train/chosen': '-170.17', 'loss/train': '0.6623', 'examples_per_second': '30.148', 'grad_norm': '169', 'counters/examples': 250912, 'counters/updates': 7841}
train stats after 250944 examples: {'rewards_train/chosen': '0.12041', 'rewards_train/rejected': '0.031211', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089196', 'logps_train/rejected': '-100.5', 'logps_train/chosen': '-116.42', 'loss/train': '0.66188', 'examples_per_second': '30.57', 'grad_norm': '65.5', 'counters/examples': 250944, 'counters/updates': 7842}
train stats after 250976 examples: {'rewards_train/chosen': '0.11063', 'rewards_train/rejected': '0.1198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0091625', 'logps_train/rejected': '-114.43', 'logps_train/chosen': '-152.15', 'loss/train': '0.70823', 'examples_per_second': '32.051', 'grad_norm': '130', 'counters/examples': 250976, 'counters/updates': 7843}
train stats after 251008 examples: {'rewards_train/chosen': '0.073599', 'rewards_train/rejected': '0.074535', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00093581', 'logps_train/rejected': '-164.48', 'logps_train/chosen': '-158.86', 'loss/train': '0.70997', 'examples_per_second': '31.531', 'grad_norm': '91', 'counters/examples': 251008, 'counters/updates': 7844}
skipping logging after 251040 examples to avoid logging too frequently
train stats after 251072 examples: {'rewards_train/chosen': '0.11524', 'rewards_train/rejected': '0.0087236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10652', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-115.32', 'loss/train': '0.65032', 'examples_per_second': '35.037', 'grad_norm': '76', 'counters/examples': 251072, 'counters/updates': 7846}
skipping logging after 251104 examples to avoid logging too frequently
train stats after 251136 examples: {'rewards_train/chosen': '0.19695', 'rewards_train/rejected': '0.093732', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-136.6', 'logps_train/chosen': '-163.97', 'loss/train': '0.66797', 'examples_per_second': '31.516', 'grad_norm': '174', 'counters/examples': 251136, 'counters/updates': 7848}
train stats after 251168 examples: {'rewards_train/chosen': '0.089919', 'rewards_train/rejected': '0.1244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034479', 'logps_train/rejected': '-122.21', 'logps_train/chosen': '-121.43', 'loss/train': '0.71907', 'examples_per_second': '31.975', 'grad_norm': '85.5', 'counters/examples': 251168, 'counters/updates': 7849}
skipping logging after 251200 examples to avoid logging too frequently
train stats after 251232 examples: {'rewards_train/chosen': '0.096481', 'rewards_train/rejected': '0.11488', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018397', 'logps_train/rejected': '-124.75', 'logps_train/chosen': '-172.38', 'loss/train': '0.72692', 'examples_per_second': '33.205', 'grad_norm': '127.5', 'counters/examples': 251232, 'counters/updates': 7851}
train stats after 251264 examples: {'rewards_train/chosen': '0.12378', 'rewards_train/rejected': '0.048257', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075527', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-159.48', 'loss/train': '0.67361', 'examples_per_second': '31.171', 'grad_norm': '63', 'counters/examples': 251264, 'counters/updates': 7852}
train stats after 251296 examples: {'rewards_train/chosen': '0.15735', 'rewards_train/rejected': '-0.013083', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17043', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-135.12', 'loss/train': '0.61992', 'examples_per_second': '31.624', 'grad_norm': '110.5', 'counters/examples': 251296, 'counters/updates': 7853}
train stats after 251328 examples: {'rewards_train/chosen': '0.23021', 'rewards_train/rejected': '0.064682', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16553', 'logps_train/rejected': '-136.04', 'logps_train/chosen': '-166.47', 'loss/train': '0.63196', 'examples_per_second': '31.532', 'grad_norm': '72.5', 'counters/examples': 251328, 'counters/updates': 7854}
train stats after 251360 examples: {'rewards_train/chosen': '0.16871', 'rewards_train/rejected': '0.1023', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066408', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-132.31', 'loss/train': '0.67343', 'examples_per_second': '31.572', 'grad_norm': '78', 'counters/examples': 251360, 'counters/updates': 7855}
train stats after 251392 examples: {'rewards_train/chosen': '0.11434', 'rewards_train/rejected': '0.16879', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.054453', 'logps_train/rejected': '-123.62', 'logps_train/chosen': '-134.6', 'loss/train': '0.73405', 'examples_per_second': '31.536', 'grad_norm': '87', 'counters/examples': 251392, 'counters/updates': 7856}
skipping logging after 251424 examples to avoid logging too frequently
train stats after 251456 examples: {'rewards_train/chosen': '0.10826', 'rewards_train/rejected': '0.012933', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095327', 'logps_train/rejected': '-165.13', 'logps_train/chosen': '-152.91', 'loss/train': '0.65613', 'examples_per_second': '30.91', 'grad_norm': '77.5', 'counters/examples': 251456, 'counters/updates': 7858}
train stats after 251488 examples: {'rewards_train/chosen': '0.0925', 'rewards_train/rejected': '0.053457', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039043', 'logps_train/rejected': '-117.73', 'logps_train/chosen': '-134.51', 'loss/train': '0.68372', 'examples_per_second': '32.075', 'grad_norm': '80', 'counters/examples': 251488, 'counters/updates': 7859}
train stats after 251520 examples: {'rewards_train/chosen': '0.15499', 'rewards_train/rejected': '0.022371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13261', 'logps_train/rejected': '-139.94', 'logps_train/chosen': '-141.24', 'loss/train': '0.64829', 'examples_per_second': '30.927', 'grad_norm': '71', 'counters/examples': 251520, 'counters/updates': 7860}
train stats after 251552 examples: {'rewards_train/chosen': '0.11861', 'rewards_train/rejected': '0.17644', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.057826', 'logps_train/rejected': '-161.45', 'logps_train/chosen': '-148.61', 'loss/train': '0.74629', 'examples_per_second': '31.616', 'grad_norm': '109', 'counters/examples': 251552, 'counters/updates': 7861}
train stats after 251584 examples: {'rewards_train/chosen': '-0.0031088', 'rewards_train/rejected': '0.079434', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.082542', 'logps_train/rejected': '-156.73', 'logps_train/chosen': '-208.03', 'loss/train': '0.77093', 'examples_per_second': '24.449', 'grad_norm': '184', 'counters/examples': 251584, 'counters/updates': 7862}
train stats after 251616 examples: {'rewards_train/chosen': '0.17259', 'rewards_train/rejected': '0.03036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14223', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-178.83', 'loss/train': '0.64815', 'examples_per_second': '31.63', 'grad_norm': '86.5', 'counters/examples': 251616, 'counters/updates': 7863}
train stats after 251648 examples: {'rewards_train/chosen': '0.14526', 'rewards_train/rejected': '0.013539', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13172', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-150.77', 'loss/train': '0.64255', 'examples_per_second': '31.604', 'grad_norm': '157', 'counters/examples': 251648, 'counters/updates': 7864}
train stats after 251680 examples: {'rewards_train/chosen': '0.22367', 'rewards_train/rejected': '0.049134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17454', 'logps_train/rejected': '-108.25', 'logps_train/chosen': '-146.08', 'loss/train': '0.63191', 'examples_per_second': '24.486', 'grad_norm': '56.25', 'counters/examples': 251680, 'counters/updates': 7865}
train stats after 251712 examples: {'rewards_train/chosen': '0.17109', 'rewards_train/rejected': '0.060069', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11102', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-169.59', 'loss/train': '0.66115', 'examples_per_second': '30.13', 'grad_norm': '76', 'counters/examples': 251712, 'counters/updates': 7866}
train stats after 251744 examples: {'rewards_train/chosen': '0.082734', 'rewards_train/rejected': '0.19353', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.1108', 'logps_train/rejected': '-184.03', 'logps_train/chosen': '-172.13', 'loss/train': '0.76946', 'examples_per_second': '31.593', 'grad_norm': '127', 'counters/examples': 251744, 'counters/updates': 7867}
train stats after 251776 examples: {'rewards_train/chosen': '0.22591', 'rewards_train/rejected': '0.12464', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10127', 'logps_train/rejected': '-129.2', 'logps_train/chosen': '-181.82', 'loss/train': '0.66078', 'examples_per_second': '31.602', 'grad_norm': '65.5', 'counters/examples': 251776, 'counters/updates': 7868}
skipping logging after 251808 examples to avoid logging too frequently
train stats after 251840 examples: {'rewards_train/chosen': '0.22517', 'rewards_train/rejected': '-0.052008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27718', 'logps_train/rejected': '-108.08', 'logps_train/chosen': '-154.66', 'loss/train': '0.59004', 'examples_per_second': '31.593', 'grad_norm': '65', 'counters/examples': 251840, 'counters/updates': 7870}
skipping logging after 251872 examples to avoid logging too frequently
train stats after 251904 examples: {'rewards_train/chosen': '0.21579', 'rewards_train/rejected': '0.052253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16354', 'logps_train/rejected': '-140.13', 'logps_train/chosen': '-143.75', 'loss/train': '0.62711', 'examples_per_second': '31.065', 'grad_norm': '78', 'counters/examples': 251904, 'counters/updates': 7872}
train stats after 251936 examples: {'rewards_train/chosen': '0.14243', 'rewards_train/rejected': '0.08599', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056442', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-165.95', 'loss/train': '0.68051', 'examples_per_second': '31.557', 'grad_norm': '154', 'counters/examples': 251936, 'counters/updates': 7873}
skipping logging after 251968 examples to avoid logging too frequently
train stats after 252000 examples: {'rewards_train/chosen': '0.20122', 'rewards_train/rejected': '0.039671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16155', 'logps_train/rejected': '-168.38', 'logps_train/chosen': '-161.66', 'loss/train': '0.64011', 'examples_per_second': '30.496', 'grad_norm': '61.5', 'counters/examples': 252000, 'counters/updates': 7875}
Running evaluation after 252000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.90it/s]
eval after 252000: {'rewards_eval/chosen': '0.1168', 'rewards_eval/rejected': '0.055676', 'rewards_eval/accuracies': '0.53516', 'rewards_eval/margins': '0.06112', 'logps_eval/rejected': '-127.59', 'logps_eval/chosen': '-149.84', 'loss/eval': '0.68128'}
skipping save for non epoch
train stats after 252032 examples: {'rewards_train/chosen': '0.10241', 'rewards_train/rejected': '0.13607', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.033657', 'logps_train/rejected': '-154.58', 'logps_train/chosen': '-148.95', 'loss/train': '0.72118', 'examples_per_second': '32.302', 'grad_norm': '428', 'counters/examples': 252032, 'counters/updates': 7876}
train stats after 252064 examples: {'rewards_train/chosen': '0.17971', 'rewards_train/rejected': '0.078063', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10164', 'logps_train/rejected': '-130.48', 'logps_train/chosen': '-138', 'loss/train': '0.65527', 'examples_per_second': '31.569', 'grad_norm': '78', 'counters/examples': 252064, 'counters/updates': 7877}
train stats after 252096 examples: {'rewards_train/chosen': '0.20841', 'rewards_train/rejected': '0.13855', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069859', 'logps_train/rejected': '-141.28', 'logps_train/chosen': '-162.14', 'loss/train': '0.68219', 'examples_per_second': '30.655', 'grad_norm': '136', 'counters/examples': 252096, 'counters/updates': 7878}
train stats after 252128 examples: {'rewards_train/chosen': '0.18191', 'rewards_train/rejected': '0.088644', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.093266', 'logps_train/rejected': '-154.92', 'logps_train/chosen': '-161.66', 'loss/train': '0.6611', 'examples_per_second': '30.937', 'grad_norm': '144', 'counters/examples': 252128, 'counters/updates': 7879}
train stats after 252160 examples: {'rewards_train/chosen': '0.16349', 'rewards_train/rejected': '0.039048', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12444', 'logps_train/rejected': '-116.54', 'logps_train/chosen': '-140.52', 'loss/train': '0.64274', 'examples_per_second': '31.97', 'grad_norm': '50.25', 'counters/examples': 252160, 'counters/updates': 7880}
skipping logging after 252192 examples to avoid logging too frequently
train stats after 252224 examples: {'rewards_train/chosen': '0.14329', 'rewards_train/rejected': '0.046773', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096515', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-140.77', 'loss/train': '0.6582', 'examples_per_second': '30.744', 'grad_norm': '63', 'counters/examples': 252224, 'counters/updates': 7882}
skipping logging after 252256 examples to avoid logging too frequently
train stats after 252288 examples: {'rewards_train/chosen': '0.12523', 'rewards_train/rejected': '0.0044374', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12079', 'logps_train/rejected': '-137.85', 'logps_train/chosen': '-131.21', 'loss/train': '0.65686', 'examples_per_second': '32.137', 'grad_norm': '91', 'counters/examples': 252288, 'counters/updates': 7884}
train stats after 252320 examples: {'rewards_train/chosen': '0.084684', 'rewards_train/rejected': '0.031465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053218', 'logps_train/rejected': '-113.89', 'logps_train/chosen': '-124.25', 'loss/train': '0.67926', 'examples_per_second': '30.4', 'grad_norm': '119.5', 'counters/examples': 252320, 'counters/updates': 7885}
train stats after 252352 examples: {'rewards_train/chosen': '0.23062', 'rewards_train/rejected': '0.072741', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15788', 'logps_train/rejected': '-148.23', 'logps_train/chosen': '-164.22', 'loss/train': '0.63761', 'examples_per_second': '30.027', 'grad_norm': '85', 'counters/examples': 252352, 'counters/updates': 7886}
train stats after 252384 examples: {'rewards_train/chosen': '0.2528', 'rewards_train/rejected': '0.070756', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.18205', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-169.01', 'loss/train': '0.67063', 'examples_per_second': '25.534', 'grad_norm': '100.5', 'counters/examples': 252384, 'counters/updates': 7887}
train stats after 252416 examples: {'rewards_train/chosen': '0.1121', 'rewards_train/rejected': '0.12378', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011689', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-173.22', 'loss/train': '0.71684', 'examples_per_second': '31.868', 'grad_norm': '142', 'counters/examples': 252416, 'counters/updates': 7888}
train stats after 252448 examples: {'rewards_train/chosen': '0.1604', 'rewards_train/rejected': '-0.0034122', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16381', 'logps_train/rejected': '-105.97', 'logps_train/chosen': '-179.98', 'loss/train': '0.64516', 'examples_per_second': '30.463', 'grad_norm': '50', 'counters/examples': 252448, 'counters/updates': 7889}
skipping logging after 252480 examples to avoid logging too frequently
train stats after 252512 examples: {'rewards_train/chosen': '0.10815', 'rewards_train/rejected': '0.051439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056713', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-137.65', 'loss/train': '0.67856', 'examples_per_second': '31.23', 'grad_norm': '84', 'counters/examples': 252512, 'counters/updates': 7891}
train stats after 252544 examples: {'rewards_train/chosen': '0.18503', 'rewards_train/rejected': '0.037684', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14735', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-144.47', 'loss/train': '0.64726', 'examples_per_second': '30.602', 'grad_norm': '69', 'counters/examples': 252544, 'counters/updates': 7892}
train stats after 252576 examples: {'rewards_train/chosen': '0.15577', 'rewards_train/rejected': '0.086577', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069196', 'logps_train/rejected': '-108.54', 'logps_train/chosen': '-123.42', 'loss/train': '0.67286', 'examples_per_second': '30.105', 'grad_norm': '49.25', 'counters/examples': 252576, 'counters/updates': 7893}
train stats after 252608 examples: {'rewards_train/chosen': '0.17877', 'rewards_train/rejected': '0.047711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13106', 'logps_train/rejected': '-152.79', 'logps_train/chosen': '-181.85', 'loss/train': '0.65961', 'examples_per_second': '31.638', 'grad_norm': '138', 'counters/examples': 252608, 'counters/updates': 7894}
train stats after 252640 examples: {'rewards_train/chosen': '0.17868', 'rewards_train/rejected': '0.11908', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059599', 'logps_train/rejected': '-124.17', 'logps_train/chosen': '-174.82', 'loss/train': '0.68797', 'examples_per_second': '31.542', 'grad_norm': '117.5', 'counters/examples': 252640, 'counters/updates': 7895}
train stats after 252672 examples: {'rewards_train/chosen': '0.048667', 'rewards_train/rejected': '0.061703', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.013035', 'logps_train/rejected': '-87.889', 'logps_train/chosen': '-137.76', 'loss/train': '0.71047', 'examples_per_second': '31.572', 'grad_norm': '99', 'counters/examples': 252672, 'counters/updates': 7896}
skipping logging after 252704 examples to avoid logging too frequently
train stats after 252736 examples: {'rewards_train/chosen': '0.15729', 'rewards_train/rejected': '0.11997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037318', 'logps_train/rejected': '-147.41', 'logps_train/chosen': '-192.41', 'loss/train': '0.70138', 'examples_per_second': '33.821', 'grad_norm': '92.5', 'counters/examples': 252736, 'counters/updates': 7898}
train stats after 252768 examples: {'rewards_train/chosen': '0.11211', 'rewards_train/rejected': '0.059929', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052179', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-139.51', 'loss/train': '0.67831', 'examples_per_second': '30.747', 'grad_norm': '140', 'counters/examples': 252768, 'counters/updates': 7899}
train stats after 252800 examples: {'rewards_train/chosen': '0.098768', 'rewards_train/rejected': '0.048059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050709', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-155.13', 'loss/train': '0.69333', 'examples_per_second': '30.46', 'grad_norm': '218', 'counters/examples': 252800, 'counters/updates': 7900}
train stats after 252832 examples: {'rewards_train/chosen': '0.1523', 'rewards_train/rejected': '0.047132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-144.96', 'logps_train/chosen': '-173.25', 'loss/train': '0.66686', 'examples_per_second': '31.563', 'grad_norm': '276', 'counters/examples': 252832, 'counters/updates': 7901}
train stats after 252864 examples: {'rewards_train/chosen': '0.119', 'rewards_train/rejected': '0.025669', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093331', 'logps_train/rejected': '-102.8', 'logps_train/chosen': '-170.64', 'loss/train': '0.65878', 'examples_per_second': '32.48', 'grad_norm': '96', 'counters/examples': 252864, 'counters/updates': 7902}
train stats after 252896 examples: {'rewards_train/chosen': '0.11149', 'rewards_train/rejected': '0.10152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0099663', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-141.47', 'loss/train': '0.69938', 'examples_per_second': '30.615', 'grad_norm': '68', 'counters/examples': 252896, 'counters/updates': 7903}
skipping logging after 252928 examples to avoid logging too frequently
train stats after 252960 examples: {'rewards_train/chosen': '0.090582', 'rewards_train/rejected': '0.04178', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048801', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-174.26', 'loss/train': '0.68042', 'examples_per_second': '34.341', 'grad_norm': '110', 'counters/examples': 252960, 'counters/updates': 7905}
train stats after 252992 examples: {'rewards_train/chosen': '0.15916', 'rewards_train/rejected': '0.012166', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14699', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-175.36', 'loss/train': '0.64055', 'examples_per_second': '31.599', 'grad_norm': '66.5', 'counters/examples': 252992, 'counters/updates': 7906}
train stats after 253024 examples: {'rewards_train/chosen': '0.15159', 'rewards_train/rejected': '0.06129', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090299', 'logps_train/rejected': '-106.94', 'logps_train/chosen': '-159.23', 'loss/train': '0.66004', 'examples_per_second': '32.408', 'grad_norm': '55', 'counters/examples': 253024, 'counters/updates': 7907}
train stats after 253056 examples: {'rewards_train/chosen': '0.10498', 'rewards_train/rejected': '0.024487', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080489', 'logps_train/rejected': '-155.55', 'logps_train/chosen': '-144.73', 'loss/train': '0.666', 'examples_per_second': '30.621', 'grad_norm': '83.5', 'counters/examples': 253056, 'counters/updates': 7908}
skipping logging after 253088 examples to avoid logging too frequently
train stats after 253120 examples: {'rewards_train/chosen': '0.067317', 'rewards_train/rejected': '0.074881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0075642', 'logps_train/rejected': '-154.77', 'logps_train/chosen': '-141.68', 'loss/train': '0.70814', 'examples_per_second': '34.707', 'grad_norm': '76.5', 'counters/examples': 253120, 'counters/updates': 7910}
train stats after 253152 examples: {'rewards_train/chosen': '0.095609', 'rewards_train/rejected': '0.094813', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00079585', 'logps_train/rejected': '-130', 'logps_train/chosen': '-140.79', 'loss/train': '0.70606', 'examples_per_second': '31.429', 'grad_norm': '95', 'counters/examples': 253152, 'counters/updates': 7911}
skipping logging after 253184 examples to avoid logging too frequently
train stats after 253216 examples: {'rewards_train/chosen': '0.17889', 'rewards_train/rejected': '0.079317', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099574', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-147.74', 'loss/train': '0.65438', 'examples_per_second': '31.037', 'grad_norm': '56.25', 'counters/examples': 253216, 'counters/updates': 7913}
train stats after 253248 examples: {'rewards_train/chosen': '0.020459', 'rewards_train/rejected': '0.03926', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018801', 'logps_train/rejected': '-111.79', 'logps_train/chosen': '-114.72', 'loss/train': '0.71161', 'examples_per_second': '32.426', 'grad_norm': '74.5', 'counters/examples': 253248, 'counters/updates': 7914}
train stats after 253280 examples: {'rewards_train/chosen': '0.12162', 'rewards_train/rejected': '0.13765', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016032', 'logps_train/rejected': '-161.61', 'logps_train/chosen': '-190.14', 'loss/train': '0.72083', 'examples_per_second': '31.485', 'grad_norm': '72', 'counters/examples': 253280, 'counters/updates': 7915}
train stats after 253312 examples: {'rewards_train/chosen': '0.09228', 'rewards_train/rejected': '0.028786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063494', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-155.84', 'loss/train': '0.67041', 'examples_per_second': '30.876', 'grad_norm': '81', 'counters/examples': 253312, 'counters/updates': 7916}
skipping logging after 253344 examples to avoid logging too frequently
train stats after 253376 examples: {'rewards_train/chosen': '0.028163', 'rewards_train/rejected': '0.07564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.047477', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-124.42', 'loss/train': '0.72647', 'examples_per_second': '31.083', 'grad_norm': '69', 'counters/examples': 253376, 'counters/updates': 7918}
skipping logging after 253408 examples to avoid logging too frequently
train stats after 253440 examples: {'rewards_train/chosen': '0.14281', 'rewards_train/rejected': '0.16553', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022724', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-133.3', 'loss/train': '0.7225', 'examples_per_second': '36.244', 'grad_norm': '73.5', 'counters/examples': 253440, 'counters/updates': 7920}
train stats after 253472 examples: {'rewards_train/chosen': '0.12886', 'rewards_train/rejected': '0.017075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11178', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-176.07', 'loss/train': '0.65488', 'examples_per_second': '31.55', 'grad_norm': '70.5', 'counters/examples': 253472, 'counters/updates': 7921}
train stats after 253504 examples: {'rewards_train/chosen': '0.05543', 'rewards_train/rejected': '0.0024475', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052982', 'logps_train/rejected': '-91.648', 'logps_train/chosen': '-158.58', 'loss/train': '0.6783', 'examples_per_second': '32.414', 'grad_norm': '81', 'counters/examples': 253504, 'counters/updates': 7922}
train stats after 253536 examples: {'rewards_train/chosen': '0.1083', 'rewards_train/rejected': '0.022013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086288', 'logps_train/rejected': '-151.16', 'logps_train/chosen': '-167.38', 'loss/train': '0.67719', 'examples_per_second': '31.663', 'grad_norm': '123', 'counters/examples': 253536, 'counters/updates': 7923}
train stats after 253568 examples: {'rewards_train/chosen': '0.16309', 'rewards_train/rejected': '0.13421', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028881', 'logps_train/rejected': '-137.78', 'logps_train/chosen': '-155.46', 'loss/train': '0.68912', 'examples_per_second': '31.276', 'grad_norm': '66.5', 'counters/examples': 253568, 'counters/updates': 7924}
train stats after 253600 examples: {'rewards_train/chosen': '0.062446', 'rewards_train/rejected': '0.097804', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035358', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-123.5', 'loss/train': '0.72218', 'examples_per_second': '30.8', 'grad_norm': '196', 'counters/examples': 253600, 'counters/updates': 7925}
train stats after 253632 examples: {'rewards_train/chosen': '0.13462', 'rewards_train/rejected': '0.031451', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10317', 'logps_train/rejected': '-113.77', 'logps_train/chosen': '-132.51', 'loss/train': '0.65287', 'examples_per_second': '33.061', 'grad_norm': '58.5', 'counters/examples': 253632, 'counters/updates': 7926}
train stats after 253664 examples: {'rewards_train/chosen': '0.17181', 'rewards_train/rejected': '0.065567', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10625', 'logps_train/rejected': '-109.89', 'logps_train/chosen': '-161.59', 'loss/train': '0.66702', 'examples_per_second': '30.876', 'grad_norm': '72.5', 'counters/examples': 253664, 'counters/updates': 7927}
train stats after 253696 examples: {'rewards_train/chosen': '0.094563', 'rewards_train/rejected': '0.12329', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.028723', 'logps_train/rejected': '-119.02', 'logps_train/chosen': '-126.92', 'loss/train': '0.71934', 'examples_per_second': '30.593', 'grad_norm': '87', 'counters/examples': 253696, 'counters/updates': 7928}
train stats after 253728 examples: {'rewards_train/chosen': '0.11967', 'rewards_train/rejected': '0.061983', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.057691', 'logps_train/rejected': '-167.54', 'logps_train/chosen': '-177.63', 'loss/train': '0.68018', 'examples_per_second': '31.562', 'grad_norm': '85', 'counters/examples': 253728, 'counters/updates': 7929}
train stats after 253760 examples: {'rewards_train/chosen': '0.16307', 'rewards_train/rejected': '-0.001707', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16478', 'logps_train/rejected': '-108.03', 'logps_train/chosen': '-133.01', 'loss/train': '0.62366', 'examples_per_second': '30.071', 'grad_norm': '61.25', 'counters/examples': 253760, 'counters/updates': 7930}
train stats after 253792 examples: {'rewards_train/chosen': '0.19959', 'rewards_train/rejected': '-0.020306', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21989', 'logps_train/rejected': '-147.6', 'logps_train/chosen': '-167.64', 'loss/train': '0.60995', 'examples_per_second': '31.585', 'grad_norm': '109.5', 'counters/examples': 253792, 'counters/updates': 7931}
skipping logging after 253824 examples to avoid logging too frequently
train stats after 253856 examples: {'rewards_train/chosen': '0.18822', 'rewards_train/rejected': '0.099514', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08871', 'logps_train/rejected': '-125.63', 'logps_train/chosen': '-161.39', 'loss/train': '0.66242', 'examples_per_second': '31.487', 'grad_norm': '117', 'counters/examples': 253856, 'counters/updates': 7933}
train stats after 253888 examples: {'rewards_train/chosen': '0.23899', 'rewards_train/rejected': '0.20459', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034399', 'logps_train/rejected': '-172.37', 'logps_train/chosen': '-163.49', 'loss/train': '0.69547', 'examples_per_second': '33.167', 'grad_norm': '90.5', 'counters/examples': 253888, 'counters/updates': 7934}
train stats after 253920 examples: {'rewards_train/chosen': '0.15748', 'rewards_train/rejected': '0.062999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094478', 'logps_train/rejected': '-161.13', 'logps_train/chosen': '-195.54', 'loss/train': '0.68655', 'examples_per_second': '31.561', 'grad_norm': '101', 'counters/examples': 253920, 'counters/updates': 7935}
train stats after 253952 examples: {'rewards_train/chosen': '0.17279', 'rewards_train/rejected': '0.14961', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023181', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-186.25', 'loss/train': '0.70163', 'examples_per_second': '31.706', 'grad_norm': '97.5', 'counters/examples': 253952, 'counters/updates': 7936}
train stats after 253984 examples: {'rewards_train/chosen': '0.16196', 'rewards_train/rejected': '0.030325', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13163', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-135.84', 'loss/train': '0.64731', 'examples_per_second': '31.326', 'grad_norm': '68', 'counters/examples': 253984, 'counters/updates': 7937}
skipping logging after 254016 examples to avoid logging too frequently
train stats after 254048 examples: {'rewards_train/chosen': '0.17704', 'rewards_train/rejected': '0.032992', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14405', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-126.21', 'loss/train': '0.63705', 'examples_per_second': '36.004', 'grad_norm': '130', 'counters/examples': 254048, 'counters/updates': 7939}
train stats after 254080 examples: {'rewards_train/chosen': '0.090881', 'rewards_train/rejected': '0.039688', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051193', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-135.14', 'loss/train': '0.68077', 'examples_per_second': '31.529', 'grad_norm': '51.75', 'counters/examples': 254080, 'counters/updates': 7940}
train stats after 254112 examples: {'rewards_train/chosen': '0.15267', 'rewards_train/rejected': '0.060154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092516', 'logps_train/rejected': '-105.93', 'logps_train/chosen': '-143.08', 'loss/train': '0.66314', 'examples_per_second': '31.91', 'grad_norm': '71.5', 'counters/examples': 254112, 'counters/updates': 7941}
skipping logging after 254144 examples to avoid logging too frequently
train stats after 254176 examples: {'rewards_train/chosen': '0.18181', 'rewards_train/rejected': '0.088186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093626', 'logps_train/rejected': '-140.24', 'logps_train/chosen': '-144.98', 'loss/train': '0.66587', 'examples_per_second': '30.619', 'grad_norm': '87', 'counters/examples': 254176, 'counters/updates': 7943}
train stats after 254208 examples: {'rewards_train/chosen': '0.13051', 'rewards_train/rejected': '0.058786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071726', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-161.13', 'loss/train': '0.66729', 'examples_per_second': '31.068', 'grad_norm': '72', 'counters/examples': 254208, 'counters/updates': 7944}
train stats after 254240 examples: {'rewards_train/chosen': '0.05343', 'rewards_train/rejected': '0.09125', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.03782', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-133.45', 'loss/train': '0.72476', 'examples_per_second': '31.542', 'grad_norm': '79.5', 'counters/examples': 254240, 'counters/updates': 7945}
skipping logging after 254272 examples to avoid logging too frequently
train stats after 254304 examples: {'rewards_train/chosen': '0.16134', 'rewards_train/rejected': '0.084644', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076694', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-143.46', 'loss/train': '0.67014', 'examples_per_second': '30.499', 'grad_norm': '57.75', 'counters/examples': 254304, 'counters/updates': 7947}
train stats after 254336 examples: {'rewards_train/chosen': '0.16761', 'rewards_train/rejected': '0.047005', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1206', 'logps_train/rejected': '-174.64', 'logps_train/chosen': '-130.93', 'loss/train': '0.6547', 'examples_per_second': '32.169', 'grad_norm': '58.25', 'counters/examples': 254336, 'counters/updates': 7948}
train stats after 254368 examples: {'rewards_train/chosen': '0.13743', 'rewards_train/rejected': '0.094685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042745', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-152.96', 'loss/train': '0.67987', 'examples_per_second': '30.15', 'grad_norm': '85', 'counters/examples': 254368, 'counters/updates': 7949}
train stats after 254400 examples: {'rewards_train/chosen': '0.073085', 'rewards_train/rejected': '0.057168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015917', 'logps_train/rejected': '-101.3', 'logps_train/chosen': '-170.8', 'loss/train': '0.70144', 'examples_per_second': '32.952', 'grad_norm': '113.5', 'counters/examples': 254400, 'counters/updates': 7950}
train stats after 254432 examples: {'rewards_train/chosen': '0.031139', 'rewards_train/rejected': '0.037961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0068217', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-144.03', 'loss/train': '0.70992', 'examples_per_second': '31.565', 'grad_norm': '68.5', 'counters/examples': 254432, 'counters/updates': 7951}
train stats after 254464 examples: {'rewards_train/chosen': '0.10111', 'rewards_train/rejected': '0.091893', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0092167', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-156.98', 'loss/train': '0.70945', 'examples_per_second': '30.261', 'grad_norm': '116', 'counters/examples': 254464, 'counters/updates': 7952}
skipping logging after 254496 examples to avoid logging too frequently
train stats after 254528 examples: {'rewards_train/chosen': '0.094969', 'rewards_train/rejected': '0.10886', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013888', 'logps_train/rejected': '-157.68', 'logps_train/chosen': '-155.25', 'loss/train': '0.70878', 'examples_per_second': '31.591', 'grad_norm': '154', 'counters/examples': 254528, 'counters/updates': 7954}
train stats after 254560 examples: {'rewards_train/chosen': '0.18789', 'rewards_train/rejected': '0.066557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12133', 'logps_train/rejected': '-108.86', 'logps_train/chosen': '-149.57', 'loss/train': '0.64845', 'examples_per_second': '30.62', 'grad_norm': '111', 'counters/examples': 254560, 'counters/updates': 7955}
train stats after 254592 examples: {'rewards_train/chosen': '0.10584', 'rewards_train/rejected': '0.058409', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047427', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-132.24', 'loss/train': '0.68847', 'examples_per_second': '31.573', 'grad_norm': '116.5', 'counters/examples': 254592, 'counters/updates': 7956}
train stats after 254624 examples: {'rewards_train/chosen': '0.10638', 'rewards_train/rejected': '0.1233', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016916', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-131.58', 'loss/train': '0.71241', 'examples_per_second': '31.839', 'grad_norm': '89', 'counters/examples': 254624, 'counters/updates': 7957}
skipping logging after 254656 examples to avoid logging too frequently
train stats after 254688 examples: {'rewards_train/chosen': '0.14538', 'rewards_train/rejected': '0.065559', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079821', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-173.2', 'loss/train': '0.66911', 'examples_per_second': '31.666', 'grad_norm': '260', 'counters/examples': 254688, 'counters/updates': 7959}
skipping logging after 254720 examples to avoid logging too frequently
train stats after 254752 examples: {'rewards_train/chosen': '0.027633', 'rewards_train/rejected': '0.065925', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038292', 'logps_train/rejected': '-130.7', 'logps_train/chosen': '-153.17', 'loss/train': '0.72253', 'examples_per_second': '30.445', 'grad_norm': '115', 'counters/examples': 254752, 'counters/updates': 7961}
train stats after 254784 examples: {'rewards_train/chosen': '0.13523', 'rewards_train/rejected': '0.045195', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090033', 'logps_train/rejected': '-163.42', 'logps_train/chosen': '-146.19', 'loss/train': '0.65741', 'examples_per_second': '31.537', 'grad_norm': '77', 'counters/examples': 254784, 'counters/updates': 7962}
train stats after 254816 examples: {'rewards_train/chosen': '0.070337', 'rewards_train/rejected': '0.063672', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0066655', 'logps_train/rejected': '-170.81', 'logps_train/chosen': '-152.75', 'loss/train': '0.69632', 'examples_per_second': '30.314', 'grad_norm': '121', 'counters/examples': 254816, 'counters/updates': 7963}
skipping logging after 254848 examples to avoid logging too frequently
train stats after 254880 examples: {'rewards_train/chosen': '0.19862', 'rewards_train/rejected': '0.15266', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045964', 'logps_train/rejected': '-188.01', 'logps_train/chosen': '-171.6', 'loss/train': '0.68299', 'examples_per_second': '31.471', 'grad_norm': '94', 'counters/examples': 254880, 'counters/updates': 7965}
skipping logging after 254912 examples to avoid logging too frequently
train stats after 254944 examples: {'rewards_train/chosen': '0.14568', 'rewards_train/rejected': '0.03514', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11054', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-148.54', 'loss/train': '0.65603', 'examples_per_second': '34.606', 'grad_norm': '70', 'counters/examples': 254944, 'counters/updates': 7967}
train stats after 254976 examples: {'rewards_train/chosen': '0.21543', 'rewards_train/rejected': '0.085137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13029', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-121.76', 'loss/train': '0.64863', 'examples_per_second': '31.397', 'grad_norm': '85.5', 'counters/examples': 254976, 'counters/updates': 7968}
skipping logging after 255008 examples to avoid logging too frequently
train stats after 255040 examples: {'rewards_train/chosen': '0.14964', 'rewards_train/rejected': '0.12034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029304', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-182.4', 'loss/train': '0.7224', 'examples_per_second': '31.612', 'grad_norm': '79', 'counters/examples': 255040, 'counters/updates': 7970}
train stats after 255072 examples: {'rewards_train/chosen': '0.23641', 'rewards_train/rejected': '0.0018842', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.23453', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-174.26', 'loss/train': '0.61571', 'examples_per_second': '31.596', 'grad_norm': '79.5', 'counters/examples': 255072, 'counters/updates': 7971}
train stats after 255104 examples: {'rewards_train/chosen': '0.10964', 'rewards_train/rejected': '0.016518', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093121', 'logps_train/rejected': '-129.45', 'logps_train/chosen': '-161.67', 'loss/train': '0.66572', 'examples_per_second': '30.539', 'grad_norm': '53', 'counters/examples': 255104, 'counters/updates': 7972}
train stats after 255136 examples: {'rewards_train/chosen': '0.16266', 'rewards_train/rejected': '0.054858', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1078', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-149', 'loss/train': '0.66186', 'examples_per_second': '32.446', 'grad_norm': '50.75', 'counters/examples': 255136, 'counters/updates': 7973}
train stats after 255168 examples: {'rewards_train/chosen': '0.22011', 'rewards_train/rejected': '0.041962', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17815', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-177.71', 'loss/train': '0.63195', 'examples_per_second': '31.026', 'grad_norm': '102.5', 'counters/examples': 255168, 'counters/updates': 7974}
train stats after 255200 examples: {'rewards_train/chosen': '0.23758', 'rewards_train/rejected': '0.067506', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17007', 'logps_train/rejected': '-141.66', 'logps_train/chosen': '-144.44', 'loss/train': '0.63198', 'examples_per_second': '30.573', 'grad_norm': '64.5', 'counters/examples': 255200, 'counters/updates': 7975}
skipping logging after 255232 examples to avoid logging too frequently
train stats after 255264 examples: {'rewards_train/chosen': '0.13494', 'rewards_train/rejected': '0.079682', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055256', 'logps_train/rejected': '-138.02', 'logps_train/chosen': '-169.75', 'loss/train': '0.67336', 'examples_per_second': '33.566', 'grad_norm': '115', 'counters/examples': 255264, 'counters/updates': 7977}
train stats after 255296 examples: {'rewards_train/chosen': '0.16759', 'rewards_train/rejected': '0.032233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13536', 'logps_train/rejected': '-140.62', 'logps_train/chosen': '-129.29', 'loss/train': '0.65041', 'examples_per_second': '31.002', 'grad_norm': '178', 'counters/examples': 255296, 'counters/updates': 7978}
train stats after 255328 examples: {'rewards_train/chosen': '0.037301', 'rewards_train/rejected': '-0.037115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074416', 'logps_train/rejected': '-115.43', 'logps_train/chosen': '-119.64', 'loss/train': '0.67007', 'examples_per_second': '32.006', 'grad_norm': '49.25', 'counters/examples': 255328, 'counters/updates': 7979}
train stats after 255360 examples: {'rewards_train/chosen': '0.18571', 'rewards_train/rejected': '0.11637', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069344', 'logps_train/rejected': '-116.61', 'logps_train/chosen': '-160.92', 'loss/train': '0.68569', 'examples_per_second': '31.798', 'grad_norm': '107', 'counters/examples': 255360, 'counters/updates': 7980}
skipping logging after 255392 examples to avoid logging too frequently
train stats after 255424 examples: {'rewards_train/chosen': '0.067487', 'rewards_train/rejected': '0.060948', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0065389', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-170.06', 'loss/train': '0.72743', 'examples_per_second': '34.521', 'grad_norm': '89', 'counters/examples': 255424, 'counters/updates': 7982}
train stats after 255456 examples: {'rewards_train/chosen': '0.091302', 'rewards_train/rejected': '0.091281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '2.0984e-05', 'logps_train/rejected': '-139.4', 'logps_train/chosen': '-139.49', 'loss/train': '0.70501', 'examples_per_second': '29.933', 'grad_norm': '69', 'counters/examples': 255456, 'counters/updates': 7983}
train stats after 255488 examples: {'rewards_train/chosen': '0.11983', 'rewards_train/rejected': '-0.020271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1401', 'logps_train/rejected': '-158.08', 'logps_train/chosen': '-178.71', 'loss/train': '0.64005', 'examples_per_second': '31.682', 'grad_norm': '96.5', 'counters/examples': 255488, 'counters/updates': 7984}
train stats after 255520 examples: {'rewards_train/chosen': '0.091486', 'rewards_train/rejected': '0.072938', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.018547', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-112.48', 'loss/train': '0.69835', 'examples_per_second': '30.476', 'grad_norm': '73', 'counters/examples': 255520, 'counters/updates': 7985}
train stats after 255552 examples: {'rewards_train/chosen': '0.060047', 'rewards_train/rejected': '0.10803', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047981', 'logps_train/rejected': '-155.25', 'logps_train/chosen': '-156.41', 'loss/train': '0.73882', 'examples_per_second': '32.674', 'grad_norm': '86', 'counters/examples': 255552, 'counters/updates': 7986}
train stats after 255584 examples: {'rewards_train/chosen': '0.20693', 'rewards_train/rejected': '0.032995', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17393', 'logps_train/rejected': '-143.76', 'logps_train/chosen': '-158.95', 'loss/train': '0.6262', 'examples_per_second': '30.861', 'grad_norm': '74.5', 'counters/examples': 255584, 'counters/updates': 7987}
train stats after 255616 examples: {'rewards_train/chosen': '0.12457', 'rewards_train/rejected': '0.10458', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019996', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-183.19', 'loss/train': '0.69494', 'examples_per_second': '31.645', 'grad_norm': '77.5', 'counters/examples': 255616, 'counters/updates': 7988}
train stats after 255648 examples: {'rewards_train/chosen': '0.065716', 'rewards_train/rejected': '0.10352', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.037807', 'logps_train/rejected': '-103.67', 'logps_train/chosen': '-103.81', 'loss/train': '0.72312', 'examples_per_second': '32.578', 'grad_norm': '68.5', 'counters/examples': 255648, 'counters/updates': 7989}
train stats after 255680 examples: {'rewards_train/chosen': '0.10202', 'rewards_train/rejected': '0.05124', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050784', 'logps_train/rejected': '-199.52', 'logps_train/chosen': '-189.42', 'loss/train': '0.68372', 'examples_per_second': '31.477', 'grad_norm': '70', 'counters/examples': 255680, 'counters/updates': 7990}
train stats after 255712 examples: {'rewards_train/chosen': '0.13397', 'rewards_train/rejected': '0.091472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042499', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-140.12', 'loss/train': '0.68892', 'examples_per_second': '30.929', 'grad_norm': '78.5', 'counters/examples': 255712, 'counters/updates': 7991}
train stats after 255744 examples: {'rewards_train/chosen': '0.15431', 'rewards_train/rejected': '0.047163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10714', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-151.36', 'loss/train': '0.65334', 'examples_per_second': '31.672', 'grad_norm': '128', 'counters/examples': 255744, 'counters/updates': 7992}
skipping logging after 255776 examples to avoid logging too frequently
train stats after 255808 examples: {'rewards_train/chosen': '0.14917', 'rewards_train/rejected': '0.14637', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0028028', 'logps_train/rejected': '-152.38', 'logps_train/chosen': '-126.23', 'loss/train': '0.70848', 'examples_per_second': '33.355', 'grad_norm': '56', 'counters/examples': 255808, 'counters/updates': 7994}
train stats after 255840 examples: {'rewards_train/chosen': '0.2255', 'rewards_train/rejected': '0.10553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11997', 'logps_train/rejected': '-145.34', 'logps_train/chosen': '-130.11', 'loss/train': '0.65525', 'examples_per_second': '30.168', 'grad_norm': '70.5', 'counters/examples': 255840, 'counters/updates': 7995}
train stats after 255872 examples: {'rewards_train/chosen': '0.2119', 'rewards_train/rejected': '0.025709', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18619', 'logps_train/rejected': '-106.98', 'logps_train/chosen': '-141.24', 'loss/train': '0.62655', 'examples_per_second': '30.167', 'grad_norm': '54.5', 'counters/examples': 255872, 'counters/updates': 7996}
train stats after 255904 examples: {'rewards_train/chosen': '0.13496', 'rewards_train/rejected': '0.12447', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.010487', 'logps_train/rejected': '-158.33', 'logps_train/chosen': '-165.33', 'loss/train': '0.70521', 'examples_per_second': '31.642', 'grad_norm': '201', 'counters/examples': 255904, 'counters/updates': 7997}
train stats after 255936 examples: {'rewards_train/chosen': '0.1104', 'rewards_train/rejected': '0.025702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084702', 'logps_train/rejected': '-147.79', 'logps_train/chosen': '-168.16', 'loss/train': '0.66827', 'examples_per_second': '32.88', 'grad_norm': '115', 'counters/examples': 255936, 'counters/updates': 7998}
train stats after 255968 examples: {'rewards_train/chosen': '0.18055', 'rewards_train/rejected': '0.054024', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12653', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-168.15', 'loss/train': '0.64942', 'examples_per_second': '31.483', 'grad_norm': '85.5', 'counters/examples': 255968, 'counters/updates': 7999}
train stats after 256000 examples: {'rewards_train/chosen': '0.20081', 'rewards_train/rejected': '0.016829', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18398', 'logps_train/rejected': '-152.66', 'logps_train/chosen': '-139.23', 'loss/train': '0.62511', 'examples_per_second': '31.677', 'grad_norm': '164', 'counters/examples': 256000, 'counters/updates': 8000}
skipping logging after 256032 examples to avoid logging too frequently
train stats after 256064 examples: {'rewards_train/chosen': '0.1342', 'rewards_train/rejected': '0.012787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12141', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-165.21', 'loss/train': '0.65725', 'examples_per_second': '31.671', 'grad_norm': '66', 'counters/examples': 256064, 'counters/updates': 8002}
skipping logging after 256096 examples to avoid logging too frequently
train stats after 256128 examples: {'rewards_train/chosen': '0.035239', 'rewards_train/rejected': '0.20551', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.17027', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-130.01', 'loss/train': '0.83253', 'examples_per_second': '31.653', 'grad_norm': '112.5', 'counters/examples': 256128, 'counters/updates': 8004}
train stats after 256160 examples: {'rewards_train/chosen': '0.13654', 'rewards_train/rejected': '0.087528', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049016', 'logps_train/rejected': '-146.69', 'logps_train/chosen': '-154.64', 'loss/train': '0.69851', 'examples_per_second': '31.019', 'grad_norm': '140', 'counters/examples': 256160, 'counters/updates': 8005}
train stats after 256192 examples: {'rewards_train/chosen': '0.13566', 'rewards_train/rejected': '0.068226', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067436', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-127.91', 'loss/train': '0.67777', 'examples_per_second': '31.566', 'grad_norm': '93', 'counters/examples': 256192, 'counters/updates': 8006}
skipping logging after 256224 examples to avoid logging too frequently
train stats after 256256 examples: {'rewards_train/chosen': '0.13267', 'rewards_train/rejected': '0.09161', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041056', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-167.04', 'loss/train': '0.68073', 'examples_per_second': '31.102', 'grad_norm': '70.5', 'counters/examples': 256256, 'counters/updates': 8008}
train stats after 256288 examples: {'rewards_train/chosen': '0.14727', 'rewards_train/rejected': '0.12946', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017811', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-158.73', 'loss/train': '0.69996', 'examples_per_second': '30.539', 'grad_norm': '78', 'counters/examples': 256288, 'counters/updates': 8009}
train stats after 256320 examples: {'rewards_train/chosen': '0.14437', 'rewards_train/rejected': '0.11114', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033232', 'logps_train/rejected': '-155.45', 'logps_train/chosen': '-171.4', 'loss/train': '0.6905', 'examples_per_second': '31.658', 'grad_norm': '87.5', 'counters/examples': 256320, 'counters/updates': 8010}
train stats after 256352 examples: {'rewards_train/chosen': '0.050886', 'rewards_train/rejected': '0.024158', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026728', 'logps_train/rejected': '-113.82', 'logps_train/chosen': '-129.26', 'loss/train': '0.69374', 'examples_per_second': '30.453', 'grad_norm': '69', 'counters/examples': 256352, 'counters/updates': 8011}
skipping logging after 256384 examples to avoid logging too frequently
train stats after 256416 examples: {'rewards_train/chosen': '0.16013', 'rewards_train/rejected': '0.082638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07749', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-127.79', 'loss/train': '0.69128', 'examples_per_second': '32.475', 'grad_norm': '84', 'counters/examples': 256416, 'counters/updates': 8013}
train stats after 256448 examples: {'rewards_train/chosen': '0.13529', 'rewards_train/rejected': '0.06321', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072078', 'logps_train/rejected': '-184.66', 'logps_train/chosen': '-194.23', 'loss/train': '0.6754', 'examples_per_second': '30.057', 'grad_norm': '96.5', 'counters/examples': 256448, 'counters/updates': 8014}
skipping logging after 256480 examples to avoid logging too frequently
train stats after 256512 examples: {'rewards_train/chosen': '0.10602', 'rewards_train/rejected': '0.043891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062128', 'logps_train/rejected': '-104.23', 'logps_train/chosen': '-155.77', 'loss/train': '0.67495', 'examples_per_second': '30.322', 'grad_norm': '83', 'counters/examples': 256512, 'counters/updates': 8016}
skipping logging after 256544 examples to avoid logging too frequently
train stats after 256576 examples: {'rewards_train/chosen': '0.067366', 'rewards_train/rejected': '0.11439', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047025', 'logps_train/rejected': '-160.31', 'logps_train/chosen': '-133.4', 'loss/train': '0.73434', 'examples_per_second': '30.797', 'grad_norm': '94.5', 'counters/examples': 256576, 'counters/updates': 8018}
train stats after 256608 examples: {'rewards_train/chosen': '0.13085', 'rewards_train/rejected': '0.098591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032264', 'logps_train/rejected': '-159.36', 'logps_train/chosen': '-211.67', 'loss/train': '0.68959', 'examples_per_second': '31.432', 'grad_norm': '147', 'counters/examples': 256608, 'counters/updates': 8019}
train stats after 256640 examples: {'rewards_train/chosen': '0.13093', 'rewards_train/rejected': '0.11856', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012363', 'logps_train/rejected': '-105.87', 'logps_train/chosen': '-116.18', 'loss/train': '0.69824', 'examples_per_second': '30.36', 'grad_norm': '58.5', 'counters/examples': 256640, 'counters/updates': 8020}
train stats after 256672 examples: {'rewards_train/chosen': '0.1625', 'rewards_train/rejected': '0.043969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11853', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-170.61', 'loss/train': '0.65559', 'examples_per_second': '32.085', 'grad_norm': '65.5', 'counters/examples': 256672, 'counters/updates': 8021}
train stats after 256704 examples: {'rewards_train/chosen': '0.085889', 'rewards_train/rejected': '0.056354', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029536', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-145.31', 'loss/train': '0.69972', 'examples_per_second': '32.453', 'grad_norm': '119.5', 'counters/examples': 256704, 'counters/updates': 8022}
train stats after 256736 examples: {'rewards_train/chosen': '0.12183', 'rewards_train/rejected': '0.031024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090803', 'logps_train/rejected': '-155.02', 'logps_train/chosen': '-149.44', 'loss/train': '0.66186', 'examples_per_second': '31.208', 'grad_norm': '108.5', 'counters/examples': 256736, 'counters/updates': 8023}
train stats after 256768 examples: {'rewards_train/chosen': '0.15188', 'rewards_train/rejected': '0.10195', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049928', 'logps_train/rejected': '-148.44', 'logps_train/chosen': '-181.04', 'loss/train': '0.68804', 'examples_per_second': '31.644', 'grad_norm': '87', 'counters/examples': 256768, 'counters/updates': 8024}
train stats after 256800 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '0.021808', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080558', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-132.69', 'loss/train': '0.66452', 'examples_per_second': '32.485', 'grad_norm': '61', 'counters/examples': 256800, 'counters/updates': 8025}
train stats after 256832 examples: {'rewards_train/chosen': '0.099721', 'rewards_train/rejected': '0.084434', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015287', 'logps_train/rejected': '-117', 'logps_train/chosen': '-174.13', 'loss/train': '0.69618', 'examples_per_second': '31.535', 'grad_norm': '55.75', 'counters/examples': 256832, 'counters/updates': 8026}
skipping logging after 256864 examples to avoid logging too frequently
train stats after 256896 examples: {'rewards_train/chosen': '0.049997', 'rewards_train/rejected': '0.083781', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033784', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-109.16', 'loss/train': '0.72478', 'examples_per_second': '31.579', 'grad_norm': '105', 'counters/examples': 256896, 'counters/updates': 8028}
skipping logging after 256928 examples to avoid logging too frequently
train stats after 256960 examples: {'rewards_train/chosen': '0.16223', 'rewards_train/rejected': '0.030004', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13223', 'logps_train/rejected': '-159.38', 'logps_train/chosen': '-188.19', 'loss/train': '0.64395', 'examples_per_second': '30.111', 'grad_norm': '121.5', 'counters/examples': 256960, 'counters/updates': 8030}
skipping logging after 256992 examples to avoid logging too frequently
train stats after 257024 examples: {'rewards_train/chosen': '0.14687', 'rewards_train/rejected': '0.077265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069606', 'logps_train/rejected': '-146.52', 'logps_train/chosen': '-157', 'loss/train': '0.67249', 'examples_per_second': '30.28', 'grad_norm': '100', 'counters/examples': 257024, 'counters/updates': 8032}
train stats after 257056 examples: {'rewards_train/chosen': '0.078339', 'rewards_train/rejected': '0.023214', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055125', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-156.73', 'loss/train': '0.67859', 'examples_per_second': '24.261', 'grad_norm': '54.5', 'counters/examples': 257056, 'counters/updates': 8033}
train stats after 257088 examples: {'rewards_train/chosen': '0.17433', 'rewards_train/rejected': '-0.088656', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26299', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-158.46', 'loss/train': '0.58373', 'examples_per_second': '30.063', 'grad_norm': '79', 'counters/examples': 257088, 'counters/updates': 8034}
train stats after 257120 examples: {'rewards_train/chosen': '0.1087', 'rewards_train/rejected': '0.21046', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10175', 'logps_train/rejected': '-176.65', 'logps_train/chosen': '-151.53', 'loss/train': '0.78031', 'examples_per_second': '31.644', 'grad_norm': '121.5', 'counters/examples': 257120, 'counters/updates': 8035}
train stats after 257152 examples: {'rewards_train/chosen': '0.12629', 'rewards_train/rejected': '0.10029', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026007', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-160.3', 'loss/train': '0.70342', 'examples_per_second': '24.512', 'grad_norm': '144', 'counters/examples': 257152, 'counters/updates': 8036}
train stats after 257184 examples: {'rewards_train/chosen': '0.064384', 'rewards_train/rejected': '0.10802', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043635', 'logps_train/rejected': '-176.27', 'logps_train/chosen': '-179.46', 'loss/train': '0.75326', 'examples_per_second': '30.168', 'grad_norm': '131', 'counters/examples': 257184, 'counters/updates': 8037}
train stats after 257216 examples: {'rewards_train/chosen': '0.1506', 'rewards_train/rejected': '-0.069427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22003', 'logps_train/rejected': '-115.45', 'logps_train/chosen': '-132.41', 'loss/train': '0.60484', 'examples_per_second': '30.314', 'grad_norm': '210', 'counters/examples': 257216, 'counters/updates': 8038}
train stats after 257248 examples: {'rewards_train/chosen': '0.15559', 'rewards_train/rejected': '0.12764', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027954', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-147.85', 'loss/train': '0.71852', 'examples_per_second': '29.865', 'grad_norm': '103', 'counters/examples': 257248, 'counters/updates': 8039}
train stats after 257280 examples: {'rewards_train/chosen': '0.09617', 'rewards_train/rejected': '0.079024', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017146', 'logps_train/rejected': '-162.76', 'logps_train/chosen': '-178.46', 'loss/train': '0.70064', 'examples_per_second': '30.272', 'grad_norm': '121', 'counters/examples': 257280, 'counters/updates': 8040}
skipping logging after 257312 examples to avoid logging too frequently
train stats after 257344 examples: {'rewards_train/chosen': '0.16328', 'rewards_train/rejected': '0.059489', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10379', 'logps_train/rejected': '-113.84', 'logps_train/chosen': '-133.81', 'loss/train': '0.65511', 'examples_per_second': '31.316', 'grad_norm': '84', 'counters/examples': 257344, 'counters/updates': 8042}
train stats after 257376 examples: {'rewards_train/chosen': '0.14495', 'rewards_train/rejected': '0.096858', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048096', 'logps_train/rejected': '-157.02', 'logps_train/chosen': '-144.51', 'loss/train': '0.70598', 'examples_per_second': '32.707', 'grad_norm': '191', 'counters/examples': 257376, 'counters/updates': 8043}
train stats after 257408 examples: {'rewards_train/chosen': '0.091603', 'rewards_train/rejected': '0.061523', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03008', 'logps_train/rejected': '-126.55', 'logps_train/chosen': '-128.17', 'loss/train': '0.69665', 'examples_per_second': '31.385', 'grad_norm': '52.75', 'counters/examples': 257408, 'counters/updates': 8044}
skipping logging after 257440 examples to avoid logging too frequently
train stats after 257472 examples: {'rewards_train/chosen': '0.29482', 'rewards_train/rejected': '0.13843', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15639', 'logps_train/rejected': '-123.27', 'logps_train/chosen': '-170.55', 'loss/train': '0.64232', 'examples_per_second': '31.505', 'grad_norm': '95.5', 'counters/examples': 257472, 'counters/updates': 8046}
train stats after 257504 examples: {'rewards_train/chosen': '0.07895', 'rewards_train/rejected': '0.2', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12105', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-130.21', 'loss/train': '0.77949', 'examples_per_second': '32.996', 'grad_norm': '94.5', 'counters/examples': 257504, 'counters/updates': 8047}
train stats after 257536 examples: {'rewards_train/chosen': '0.17384', 'rewards_train/rejected': '0.018618', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15522', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-154.04', 'loss/train': '0.63045', 'examples_per_second': '31.135', 'grad_norm': '57', 'counters/examples': 257536, 'counters/updates': 8048}
train stats after 257568 examples: {'rewards_train/chosen': '0.16054', 'rewards_train/rejected': '0.039613', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12093', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-147.91', 'loss/train': '0.65411', 'examples_per_second': '32.382', 'grad_norm': '80', 'counters/examples': 257568, 'counters/updates': 8049}
train stats after 257600 examples: {'rewards_train/chosen': '0.095897', 'rewards_train/rejected': '0.023194', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072703', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-139.42', 'loss/train': '0.66801', 'examples_per_second': '32.23', 'grad_norm': '98.5', 'counters/examples': 257600, 'counters/updates': 8050}
train stats after 257632 examples: {'rewards_train/chosen': '0.13034', 'rewards_train/rejected': '0.037646', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092696', 'logps_train/rejected': '-124.42', 'logps_train/chosen': '-153.21', 'loss/train': '0.66597', 'examples_per_second': '31.668', 'grad_norm': '83', 'counters/examples': 257632, 'counters/updates': 8051}
train stats after 257664 examples: {'rewards_train/chosen': '0.14031', 'rewards_train/rejected': '0.083108', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057199', 'logps_train/rejected': '-148.1', 'logps_train/chosen': '-180.58', 'loss/train': '0.69636', 'examples_per_second': '30.71', 'grad_norm': '102', 'counters/examples': 257664, 'counters/updates': 8052}
train stats after 257696 examples: {'rewards_train/chosen': '0.10889', 'rewards_train/rejected': '0.099903', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0089839', 'logps_train/rejected': '-191.99', 'logps_train/chosen': '-147.16', 'loss/train': '0.7034', 'examples_per_second': '31.275', 'grad_norm': '133', 'counters/examples': 257696, 'counters/updates': 8053}
train stats after 257728 examples: {'rewards_train/chosen': '0.17409', 'rewards_train/rejected': '0.15708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017007', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-144.63', 'loss/train': '0.71837', 'examples_per_second': '31.66', 'grad_norm': '808', 'counters/examples': 257728, 'counters/updates': 8054}
train stats after 257760 examples: {'rewards_train/chosen': '0.082478', 'rewards_train/rejected': '0.071433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011045', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-174.62', 'loss/train': '0.70192', 'examples_per_second': '32.302', 'grad_norm': '86.5', 'counters/examples': 257760, 'counters/updates': 8055}
train stats after 257792 examples: {'rewards_train/chosen': '0.12612', 'rewards_train/rejected': '0.09499', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03113', 'logps_train/rejected': '-131.96', 'logps_train/chosen': '-144.52', 'loss/train': '0.69001', 'examples_per_second': '32.575', 'grad_norm': '120.5', 'counters/examples': 257792, 'counters/updates': 8056}
train stats after 257824 examples: {'rewards_train/chosen': '0.17826', 'rewards_train/rejected': '0.089206', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089053', 'logps_train/rejected': '-132.64', 'logps_train/chosen': '-132.07', 'loss/train': '0.66262', 'examples_per_second': '32.003', 'grad_norm': '103', 'counters/examples': 257824, 'counters/updates': 8057}
train stats after 257856 examples: {'rewards_train/chosen': '0.11736', 'rewards_train/rejected': '0.1215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0041417', 'logps_train/rejected': '-155.13', 'logps_train/chosen': '-152.95', 'loss/train': '0.72929', 'examples_per_second': '30.614', 'grad_norm': '85.5', 'counters/examples': 257856, 'counters/updates': 8058}
train stats after 257888 examples: {'rewards_train/chosen': '0.11646', 'rewards_train/rejected': '0.091655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024807', 'logps_train/rejected': '-136.21', 'logps_train/chosen': '-129.88', 'loss/train': '0.6954', 'examples_per_second': '32.223', 'grad_norm': '88', 'counters/examples': 257888, 'counters/updates': 8059}
skipping logging after 257920 examples to avoid logging too frequently
train stats after 257952 examples: {'rewards_train/chosen': '0.11314', 'rewards_train/rejected': '0.046005', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067139', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-136.75', 'loss/train': '0.67359', 'examples_per_second': '24.556', 'grad_norm': '66.5', 'counters/examples': 257952, 'counters/updates': 8061}
train stats after 257984 examples: {'rewards_train/chosen': '0.060223', 'rewards_train/rejected': '0.03375', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026473', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-156.39', 'loss/train': '0.68974', 'examples_per_second': '32.435', 'grad_norm': '102.5', 'counters/examples': 257984, 'counters/updates': 8062}
train stats after 258016 examples: {'rewards_train/chosen': '0.10887', 'rewards_train/rejected': '0.032459', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07641', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-146.22', 'loss/train': '0.66612', 'examples_per_second': '32.418', 'grad_norm': '67.5', 'counters/examples': 258016, 'counters/updates': 8063}
train stats after 258048 examples: {'rewards_train/chosen': '0.026091', 'rewards_train/rejected': '0.066682', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.040591', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-170.07', 'loss/train': '0.7253', 'examples_per_second': '31.658', 'grad_norm': '95', 'counters/examples': 258048, 'counters/updates': 8064}
train stats after 258080 examples: {'rewards_train/chosen': '0.11403', 'rewards_train/rejected': '0.113', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0010325', 'logps_train/rejected': '-151.36', 'logps_train/chosen': '-173.96', 'loss/train': '0.70454', 'examples_per_second': '30.163', 'grad_norm': '120', 'counters/examples': 258080, 'counters/updates': 8065}
train stats after 258112 examples: {'rewards_train/chosen': '0.14236', 'rewards_train/rejected': '0.06919', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073173', 'logps_train/rejected': '-188.35', 'logps_train/chosen': '-169.84', 'loss/train': '0.68257', 'examples_per_second': '30.149', 'grad_norm': '76.5', 'counters/examples': 258112, 'counters/updates': 8066}
skipping logging after 258144 examples to avoid logging too frequently
train stats after 258176 examples: {'rewards_train/chosen': '0.13653', 'rewards_train/rejected': '0.026502', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11003', 'logps_train/rejected': '-110', 'logps_train/chosen': '-177.81', 'loss/train': '0.65173', 'examples_per_second': '30.348', 'grad_norm': '87.5', 'counters/examples': 258176, 'counters/updates': 8068}
skipping logging after 258208 examples to avoid logging too frequently
train stats after 258240 examples: {'rewards_train/chosen': '0.18678', 'rewards_train/rejected': '0.08302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10376', 'logps_train/rejected': '-144.67', 'logps_train/chosen': '-172.03', 'loss/train': '0.65534', 'examples_per_second': '31.617', 'grad_norm': '77.5', 'counters/examples': 258240, 'counters/updates': 8070}
train stats after 258272 examples: {'rewards_train/chosen': '0.081746', 'rewards_train/rejected': '0.046278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035467', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-127.09', 'loss/train': '0.68357', 'examples_per_second': '30.922', 'grad_norm': '125.5', 'counters/examples': 258272, 'counters/updates': 8071}
train stats after 258304 examples: {'rewards_train/chosen': '0.19845', 'rewards_train/rejected': '0.017495', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18095', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-145.52', 'loss/train': '0.63064', 'examples_per_second': '32.749', 'grad_norm': '61.5', 'counters/examples': 258304, 'counters/updates': 8072}
train stats after 258336 examples: {'rewards_train/chosen': '0.22612', 'rewards_train/rejected': '0.065334', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16079', 'logps_train/rejected': '-144.25', 'logps_train/chosen': '-161.65', 'loss/train': '0.63376', 'examples_per_second': '31.403', 'grad_norm': '198', 'counters/examples': 258336, 'counters/updates': 8073}
train stats after 258368 examples: {'rewards_train/chosen': '0.16834', 'rewards_train/rejected': '0.090568', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.07777', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-128.42', 'loss/train': '0.66645', 'examples_per_second': '31.543', 'grad_norm': '69', 'counters/examples': 258368, 'counters/updates': 8074}
train stats after 258400 examples: {'rewards_train/chosen': '0.15655', 'rewards_train/rejected': '0.021238', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13531', 'logps_train/rejected': '-161.58', 'logps_train/chosen': '-166.88', 'loss/train': '0.6435', 'examples_per_second': '31.635', 'grad_norm': '157', 'counters/examples': 258400, 'counters/updates': 8075}
train stats after 258432 examples: {'rewards_train/chosen': '0.2553', 'rewards_train/rejected': '0.03542', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21988', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-154.94', 'loss/train': '0.61234', 'examples_per_second': '31.458', 'grad_norm': '57', 'counters/examples': 258432, 'counters/updates': 8076}
train stats after 258464 examples: {'rewards_train/chosen': '0.18355', 'rewards_train/rejected': '0.061584', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12196', 'logps_train/rejected': '-128.82', 'logps_train/chosen': '-168.87', 'loss/train': '0.64577', 'examples_per_second': '30.054', 'grad_norm': '78', 'counters/examples': 258464, 'counters/updates': 8077}
train stats after 258496 examples: {'rewards_train/chosen': '0.1494', 'rewards_train/rejected': '0.024773', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12463', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-169.05', 'loss/train': '0.65779', 'examples_per_second': '32.447', 'grad_norm': '87.5', 'counters/examples': 258496, 'counters/updates': 8078}
train stats after 258528 examples: {'rewards_train/chosen': '0.18723', 'rewards_train/rejected': '0.07786', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10937', 'logps_train/rejected': '-154.97', 'logps_train/chosen': '-174.77', 'loss/train': '0.65846', 'examples_per_second': '30.147', 'grad_norm': '84.5', 'counters/examples': 258528, 'counters/updates': 8079}
skipping logging after 258560 examples to avoid logging too frequently
train stats after 258592 examples: {'rewards_train/chosen': '0.11325', 'rewards_train/rejected': '0.090666', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022583', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-152.63', 'loss/train': '0.70684', 'examples_per_second': '32.197', 'grad_norm': '80.5', 'counters/examples': 258592, 'counters/updates': 8081}
train stats after 258624 examples: {'rewards_train/chosen': '0.14165', 'rewards_train/rejected': '0.085394', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056257', 'logps_train/rejected': '-172.02', 'logps_train/chosen': '-184.1', 'loss/train': '0.70042', 'examples_per_second': '31.522', 'grad_norm': '103.5', 'counters/examples': 258624, 'counters/updates': 8082}
train stats after 258656 examples: {'rewards_train/chosen': '0.18276', 'rewards_train/rejected': '0.08428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098482', 'logps_train/rejected': '-145.25', 'logps_train/chosen': '-185.51', 'loss/train': '0.6612', 'examples_per_second': '31.512', 'grad_norm': '126', 'counters/examples': 258656, 'counters/updates': 8083}
skipping logging after 258688 examples to avoid logging too frequently
train stats after 258720 examples: {'rewards_train/chosen': '0.17576', 'rewards_train/rejected': '0.030204', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14555', 'logps_train/rejected': '-155.75', 'logps_train/chosen': '-172.47', 'loss/train': '0.65436', 'examples_per_second': '31.659', 'grad_norm': '77.5', 'counters/examples': 258720, 'counters/updates': 8085}
skipping logging after 258752 examples to avoid logging too frequently
train stats after 258784 examples: {'rewards_train/chosen': '0.16924', 'rewards_train/rejected': '0.078925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.090313', 'logps_train/rejected': '-96.783', 'logps_train/chosen': '-155.91', 'loss/train': '0.66333', 'examples_per_second': '34.518', 'grad_norm': '66.5', 'counters/examples': 258784, 'counters/updates': 8087}
train stats after 258816 examples: {'rewards_train/chosen': '0.30807', 'rewards_train/rejected': '0.17121', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13686', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-180.88', 'loss/train': '0.66923', 'examples_per_second': '30.325', 'grad_norm': '112', 'counters/examples': 258816, 'counters/updates': 8088}
train stats after 258848 examples: {'rewards_train/chosen': '0.12167', 'rewards_train/rejected': '0.12215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00047754', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-163.17', 'loss/train': '0.70668', 'examples_per_second': '31.457', 'grad_norm': '87.5', 'counters/examples': 258848, 'counters/updates': 8089}
train stats after 258880 examples: {'rewards_train/chosen': '0.18589', 'rewards_train/rejected': '0.066343', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11955', 'logps_train/rejected': '-172.31', 'logps_train/chosen': '-161.82', 'loss/train': '0.64621', 'examples_per_second': '30.859', 'grad_norm': '201', 'counters/examples': 258880, 'counters/updates': 8090}
train stats after 258912 examples: {'rewards_train/chosen': '0.18113', 'rewards_train/rejected': '0.11747', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063659', 'logps_train/rejected': '-179.34', 'logps_train/chosen': '-149.95', 'loss/train': '0.67695', 'examples_per_second': '30.715', 'grad_norm': '104.5', 'counters/examples': 258912, 'counters/updates': 8091}
train stats after 258944 examples: {'rewards_train/chosen': '0.13077', 'rewards_train/rejected': '0.098967', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031801', 'logps_train/rejected': '-133.29', 'logps_train/chosen': '-156.8', 'loss/train': '0.69367', 'examples_per_second': '31.351', 'grad_norm': '85.5', 'counters/examples': 258944, 'counters/updates': 8092}
train stats after 258976 examples: {'rewards_train/chosen': '0.14361', 'rewards_train/rejected': '0.039491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10412', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-169.25', 'loss/train': '0.65505', 'examples_per_second': '32.553', 'grad_norm': '110', 'counters/examples': 258976, 'counters/updates': 8093}
train stats after 259008 examples: {'rewards_train/chosen': '0.21786', 'rewards_train/rejected': '0.15372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064144', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-169.14', 'loss/train': '0.67979', 'examples_per_second': '31.634', 'grad_norm': '117', 'counters/examples': 259008, 'counters/updates': 8094}
train stats after 259040 examples: {'rewards_train/chosen': '0.23426', 'rewards_train/rejected': '0.061346', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17291', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-198.4', 'loss/train': '0.62439', 'examples_per_second': '31.589', 'grad_norm': '70', 'counters/examples': 259040, 'counters/updates': 8095}
train stats after 259072 examples: {'rewards_train/chosen': '0.23602', 'rewards_train/rejected': '0.12566', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11037', 'logps_train/rejected': '-137.95', 'logps_train/chosen': '-163.06', 'loss/train': '0.65633', 'examples_per_second': '30.952', 'grad_norm': '96.5', 'counters/examples': 259072, 'counters/updates': 8096}
train stats after 259104 examples: {'rewards_train/chosen': '0.18562', 'rewards_train/rejected': '-0.0043505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18997', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-173.73', 'loss/train': '0.62633', 'examples_per_second': '31.669', 'grad_norm': '78', 'counters/examples': 259104, 'counters/updates': 8097}
train stats after 259136 examples: {'rewards_train/chosen': '0.055587', 'rewards_train/rejected': '0.029923', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025664', 'logps_train/rejected': '-101.98', 'logps_train/chosen': '-138.57', 'loss/train': '0.69371', 'examples_per_second': '32.049', 'grad_norm': '96.5', 'counters/examples': 259136, 'counters/updates': 8098}
train stats after 259168 examples: {'rewards_train/chosen': '0.079643', 'rewards_train/rejected': '0.10924', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029598', 'logps_train/rejected': '-101.04', 'logps_train/chosen': '-127.48', 'loss/train': '0.71795', 'examples_per_second': '30.61', 'grad_norm': '66.5', 'counters/examples': 259168, 'counters/updates': 8099}
train stats after 259200 examples: {'rewards_train/chosen': '0.14328', 'rewards_train/rejected': '0.0042918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13899', 'logps_train/rejected': '-164.97', 'logps_train/chosen': '-164.52', 'loss/train': '0.65782', 'examples_per_second': '31.601', 'grad_norm': '193', 'counters/examples': 259200, 'counters/updates': 8100}
train stats after 259232 examples: {'rewards_train/chosen': '0.21281', 'rewards_train/rejected': '0.10959', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-158.99', 'loss/train': '0.65844', 'examples_per_second': '30.696', 'grad_norm': '101.5', 'counters/examples': 259232, 'counters/updates': 8101}
train stats after 259264 examples: {'rewards_train/chosen': '0.17387', 'rewards_train/rejected': '0.091568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082304', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-148.95', 'loss/train': '0.67514', 'examples_per_second': '30.392', 'grad_norm': '70', 'counters/examples': 259264, 'counters/updates': 8102}
train stats after 259296 examples: {'rewards_train/chosen': '0.20221', 'rewards_train/rejected': '0.10698', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095237', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-123.58', 'loss/train': '0.6625', 'examples_per_second': '31.534', 'grad_norm': '85', 'counters/examples': 259296, 'counters/updates': 8103}
train stats after 259328 examples: {'rewards_train/chosen': '0.178', 'rewards_train/rejected': '0.065764', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11224', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-146.79', 'loss/train': '0.64907', 'examples_per_second': '32.913', 'grad_norm': '86', 'counters/examples': 259328, 'counters/updates': 8104}
skipping logging after 259360 examples to avoid logging too frequently
train stats after 259392 examples: {'rewards_train/chosen': '0.11146', 'rewards_train/rejected': '0.033314', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.078143', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-136.72', 'loss/train': '0.66976', 'examples_per_second': '32.177', 'grad_norm': '66.5', 'counters/examples': 259392, 'counters/updates': 8106}
train stats after 259424 examples: {'rewards_train/chosen': '0.126', 'rewards_train/rejected': '0.10283', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023173', 'logps_train/rejected': '-149.86', 'logps_train/chosen': '-151.54', 'loss/train': '0.70529', 'examples_per_second': '31.67', 'grad_norm': '114', 'counters/examples': 259424, 'counters/updates': 8107}
train stats after 259456 examples: {'rewards_train/chosen': '0.15942', 'rewards_train/rejected': '0.041976', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11745', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-173.74', 'loss/train': '0.65636', 'examples_per_second': '31.329', 'grad_norm': '55', 'counters/examples': 259456, 'counters/updates': 8108}
train stats after 259488 examples: {'rewards_train/chosen': '0.093968', 'rewards_train/rejected': '0.081971', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011997', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-142.57', 'loss/train': '0.70449', 'examples_per_second': '30.811', 'grad_norm': '69', 'counters/examples': 259488, 'counters/updates': 8109}
train stats after 259520 examples: {'rewards_train/chosen': '0.18991', 'rewards_train/rejected': '0.1153', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074611', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-168.15', 'loss/train': '0.67074', 'examples_per_second': '30.534', 'grad_norm': '101', 'counters/examples': 259520, 'counters/updates': 8110}
train stats after 259552 examples: {'rewards_train/chosen': '0.071241', 'rewards_train/rejected': '0.028932', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042309', 'logps_train/rejected': '-108.61', 'logps_train/chosen': '-125.69', 'loss/train': '0.68198', 'examples_per_second': '30.586', 'grad_norm': '52.5', 'counters/examples': 259552, 'counters/updates': 8111}
train stats after 259584 examples: {'rewards_train/chosen': '0.19643', 'rewards_train/rejected': '0.076954', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11948', 'logps_train/rejected': '-139.16', 'logps_train/chosen': '-148.59', 'loss/train': '0.65075', 'examples_per_second': '31.622', 'grad_norm': '69.5', 'counters/examples': 259584, 'counters/updates': 8112}
train stats after 259616 examples: {'rewards_train/chosen': '0.15567', 'rewards_train/rejected': '0.035904', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11977', 'logps_train/rejected': '-153.29', 'logps_train/chosen': '-155.6', 'loss/train': '0.64604', 'examples_per_second': '30.711', 'grad_norm': '51.5', 'counters/examples': 259616, 'counters/updates': 8113}
skipping logging after 259648 examples to avoid logging too frequently
train stats after 259680 examples: {'rewards_train/chosen': '0.15937', 'rewards_train/rejected': '0.013149', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14622', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-214.34', 'loss/train': '0.64681', 'examples_per_second': '31.608', 'grad_norm': '124.5', 'counters/examples': 259680, 'counters/updates': 8115}
train stats after 259712 examples: {'rewards_train/chosen': '0.12895', 'rewards_train/rejected': '0.15306', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024114', 'logps_train/rejected': '-150.04', 'logps_train/chosen': '-183.47', 'loss/train': '0.73579', 'examples_per_second': '31.653', 'grad_norm': '112', 'counters/examples': 259712, 'counters/updates': 8116}
train stats after 259744 examples: {'rewards_train/chosen': '0.17256', 'rewards_train/rejected': '0.038197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13436', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-158.52', 'loss/train': '0.64391', 'examples_per_second': '32.723', 'grad_norm': '82.5', 'counters/examples': 259744, 'counters/updates': 8117}
train stats after 259776 examples: {'rewards_train/chosen': '0.087167', 'rewards_train/rejected': '0.0098657', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077301', 'logps_train/rejected': '-96.016', 'logps_train/chosen': '-178.62', 'loss/train': '0.66458', 'examples_per_second': '31.48', 'grad_norm': '89.5', 'counters/examples': 259776, 'counters/updates': 8118}
train stats after 259808 examples: {'rewards_train/chosen': '0.11705', 'rewards_train/rejected': '0.19585', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.078805', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-148.15', 'loss/train': '0.75227', 'examples_per_second': '31.816', 'grad_norm': '81', 'counters/examples': 259808, 'counters/updates': 8119}
skipping logging after 259840 examples to avoid logging too frequently
train stats after 259872 examples: {'rewards_train/chosen': '0.13221', 'rewards_train/rejected': '0.053203', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079002', 'logps_train/rejected': '-143.48', 'logps_train/chosen': '-132.47', 'loss/train': '0.66975', 'examples_per_second': '34.299', 'grad_norm': '56.75', 'counters/examples': 259872, 'counters/updates': 8121}
train stats after 259904 examples: {'rewards_train/chosen': '0.17076', 'rewards_train/rejected': '0.073677', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097084', 'logps_train/rejected': '-113.34', 'logps_train/chosen': '-144.9', 'loss/train': '0.66321', 'examples_per_second': '32.212', 'grad_norm': '87', 'counters/examples': 259904, 'counters/updates': 8122}
train stats after 259936 examples: {'rewards_train/chosen': '0.24434', 'rewards_train/rejected': '0.1454', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098939', 'logps_train/rejected': '-127.98', 'logps_train/chosen': '-208.21', 'loss/train': '0.67006', 'examples_per_second': '33.225', 'grad_norm': '392', 'counters/examples': 259936, 'counters/updates': 8123}
train stats after 259968 examples: {'rewards_train/chosen': '0.099047', 'rewards_train/rejected': '0.042161', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056886', 'logps_train/rejected': '-108.35', 'logps_train/chosen': '-118.24', 'loss/train': '0.68089', 'examples_per_second': '31.897', 'grad_norm': '53.75', 'counters/examples': 259968, 'counters/updates': 8124}
train stats after 260000 examples: {'rewards_train/chosen': '0.15201', 'rewards_train/rejected': '0.067321', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084687', 'logps_train/rejected': '-160.08', 'logps_train/chosen': '-139.42', 'loss/train': '0.66715', 'examples_per_second': '31.253', 'grad_norm': '97.5', 'counters/examples': 260000, 'counters/updates': 8125}
train stats after 260032 examples: {'rewards_train/chosen': '0.040506', 'rewards_train/rejected': '0.045318', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0048126', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-125.96', 'loss/train': '0.70683', 'examples_per_second': '31.497', 'grad_norm': '79', 'counters/examples': 260032, 'counters/updates': 8126}
skipping logging after 260064 examples to avoid logging too frequently
train stats after 260096 examples: {'rewards_train/chosen': '0.26684', 'rewards_train/rejected': '0.023901', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24294', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-150.87', 'loss/train': '0.59169', 'examples_per_second': '31.66', 'grad_norm': '63.25', 'counters/examples': 260096, 'counters/updates': 8128}
train stats after 260128 examples: {'rewards_train/chosen': '0.18756', 'rewards_train/rejected': '0.091416', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.09614', 'logps_train/rejected': '-128.95', 'logps_train/chosen': '-167.04', 'loss/train': '0.6734', 'examples_per_second': '30.728', 'grad_norm': '78.5', 'counters/examples': 260128, 'counters/updates': 8129}
train stats after 260160 examples: {'rewards_train/chosen': '0.14467', 'rewards_train/rejected': '0.063787', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080881', 'logps_train/rejected': '-142.29', 'logps_train/chosen': '-145.88', 'loss/train': '0.66737', 'examples_per_second': '31.604', 'grad_norm': '106', 'counters/examples': 260160, 'counters/updates': 8130}
train stats after 260192 examples: {'rewards_train/chosen': '0.11894', 'rewards_train/rejected': '0.14164', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022704', 'logps_train/rejected': '-158.97', 'logps_train/chosen': '-140.27', 'loss/train': '0.73952', 'examples_per_second': '30.481', 'grad_norm': '211', 'counters/examples': 260192, 'counters/updates': 8131}
train stats after 260224 examples: {'rewards_train/chosen': '0.12593', 'rewards_train/rejected': '0.11084', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015093', 'logps_train/rejected': '-119.18', 'logps_train/chosen': '-158.07', 'loss/train': '0.70455', 'examples_per_second': '31.578', 'grad_norm': '165', 'counters/examples': 260224, 'counters/updates': 8132}
train stats after 260256 examples: {'rewards_train/chosen': '0.13288', 'rewards_train/rejected': '0.12544', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074343', 'logps_train/rejected': '-133.19', 'logps_train/chosen': '-136.54', 'loss/train': '0.70423', 'examples_per_second': '31.324', 'grad_norm': '74', 'counters/examples': 260256, 'counters/updates': 8133}
train stats after 260288 examples: {'rewards_train/chosen': '0.1635', 'rewards_train/rejected': '-0.10822', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.27173', 'logps_train/rejected': '-121.14', 'logps_train/chosen': '-160.17', 'loss/train': '0.59088', 'examples_per_second': '32.435', 'grad_norm': '80', 'counters/examples': 260288, 'counters/updates': 8134}
skipping logging after 260320 examples to avoid logging too frequently
train stats after 260352 examples: {'rewards_train/chosen': '0.11289', 'rewards_train/rejected': '0.043811', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069075', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-163.82', 'loss/train': '0.6698', 'examples_per_second': '33.414', 'grad_norm': '62', 'counters/examples': 260352, 'counters/updates': 8136}
train stats after 260384 examples: {'rewards_train/chosen': '0.1148', 'rewards_train/rejected': '0.17357', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.05877', 'logps_train/rejected': '-167.59', 'logps_train/chosen': '-168.19', 'loss/train': '0.74958', 'examples_per_second': '32.522', 'grad_norm': '226', 'counters/examples': 260384, 'counters/updates': 8137}
train stats after 260416 examples: {'rewards_train/chosen': '0.16128', 'rewards_train/rejected': '0.098452', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06283', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-146.51', 'loss/train': '0.67747', 'examples_per_second': '32.269', 'grad_norm': '92', 'counters/examples': 260416, 'counters/updates': 8138}
train stats after 260448 examples: {'rewards_train/chosen': '0.060631', 'rewards_train/rejected': '0.069264', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0086336', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-150.08', 'loss/train': '0.71707', 'examples_per_second': '31.698', 'grad_norm': '88', 'counters/examples': 260448, 'counters/updates': 8139}
train stats after 260480 examples: {'rewards_train/chosen': '0.18731', 'rewards_train/rejected': '0.0059615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18134', 'logps_train/rejected': '-136.83', 'logps_train/chosen': '-173.09', 'loss/train': '0.63316', 'examples_per_second': '30.205', 'grad_norm': '65', 'counters/examples': 260480, 'counters/updates': 8140}
train stats after 260512 examples: {'rewards_train/chosen': '0.1325', 'rewards_train/rejected': '0.03653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095972', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-126.37', 'loss/train': '0.66124', 'examples_per_second': '32.732', 'grad_norm': '67.5', 'counters/examples': 260512, 'counters/updates': 8141}
train stats after 260544 examples: {'rewards_train/chosen': '0.13995', 'rewards_train/rejected': '0.019896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12006', 'logps_train/rejected': '-136.36', 'logps_train/chosen': '-124.47', 'loss/train': '0.65156', 'examples_per_second': '31.246', 'grad_norm': '60.75', 'counters/examples': 260544, 'counters/updates': 8142}
train stats after 260576 examples: {'rewards_train/chosen': '0.11282', 'rewards_train/rejected': '0.10831', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0045134', 'logps_train/rejected': '-157.17', 'logps_train/chosen': '-148.37', 'loss/train': '0.71144', 'examples_per_second': '31.598', 'grad_norm': '85', 'counters/examples': 260576, 'counters/updates': 8143}
train stats after 260608 examples: {'rewards_train/chosen': '0.16003', 'rewards_train/rejected': '0.090304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069725', 'logps_train/rejected': '-130.95', 'logps_train/chosen': '-181.83', 'loss/train': '0.67017', 'examples_per_second': '31.312', 'grad_norm': '85.5', 'counters/examples': 260608, 'counters/updates': 8144}
train stats after 260640 examples: {'rewards_train/chosen': '0.14842', 'rewards_train/rejected': '0.046467', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10195', 'logps_train/rejected': '-156.21', 'logps_train/chosen': '-194.87', 'loss/train': '0.66457', 'examples_per_second': '30.948', 'grad_norm': '152', 'counters/examples': 260640, 'counters/updates': 8145}
train stats after 260672 examples: {'rewards_train/chosen': '0.095386', 'rewards_train/rejected': '0.10326', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0078772', 'logps_train/rejected': '-175.2', 'logps_train/chosen': '-154.71', 'loss/train': '0.71124', 'examples_per_second': '31.341', 'grad_norm': '165', 'counters/examples': 260672, 'counters/updates': 8146}
skipping logging after 260704 examples to avoid logging too frequently
train stats after 260736 examples: {'rewards_train/chosen': '0.071051', 'rewards_train/rejected': '0.038322', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032729', 'logps_train/rejected': '-143.58', 'logps_train/chosen': '-156.5', 'loss/train': '0.68387', 'examples_per_second': '34.961', 'grad_norm': '63.5', 'counters/examples': 260736, 'counters/updates': 8148}
train stats after 260768 examples: {'rewards_train/chosen': '0.16894', 'rewards_train/rejected': '-0.01314', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18208', 'logps_train/rejected': '-103.84', 'logps_train/chosen': '-155.79', 'loss/train': '0.61613', 'examples_per_second': '30.878', 'grad_norm': '52', 'counters/examples': 260768, 'counters/updates': 8149}
skipping logging after 260800 examples to avoid logging too frequently
train stats after 260832 examples: {'rewards_train/chosen': '0.11429', 'rewards_train/rejected': '0.083616', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030677', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-156.85', 'loss/train': '0.6915', 'examples_per_second': '31.306', 'grad_norm': '78.5', 'counters/examples': 260832, 'counters/updates': 8151}
skipping logging after 260864 examples to avoid logging too frequently
train stats after 260896 examples: {'rewards_train/chosen': '0.097135', 'rewards_train/rejected': '-0.0072507', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10439', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-127.74', 'loss/train': '0.65838', 'examples_per_second': '33.336', 'grad_norm': '274', 'counters/examples': 260896, 'counters/updates': 8153}
train stats after 260928 examples: {'rewards_train/chosen': '0.15911', 'rewards_train/rejected': '0.072639', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086474', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-157.82', 'loss/train': '0.6646', 'examples_per_second': '31.665', 'grad_norm': '89.5', 'counters/examples': 260928, 'counters/updates': 8154}
train stats after 260960 examples: {'rewards_train/chosen': '0.16808', 'rewards_train/rejected': '0.078584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089492', 'logps_train/rejected': '-102.52', 'logps_train/chosen': '-117.59', 'loss/train': '0.65801', 'examples_per_second': '31.53', 'grad_norm': '61.25', 'counters/examples': 260960, 'counters/updates': 8155}
skipping logging after 260992 examples to avoid logging too frequently
train stats after 261024 examples: {'rewards_train/chosen': '0.156', 'rewards_train/rejected': '0.077989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078012', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-127.5', 'loss/train': '0.67432', 'examples_per_second': '30.613', 'grad_norm': '87.5', 'counters/examples': 261024, 'counters/updates': 8157}
train stats after 261056 examples: {'rewards_train/chosen': '0.12271', 'rewards_train/rejected': '0.13598', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013274', 'logps_train/rejected': '-147.4', 'logps_train/chosen': '-154.74', 'loss/train': '0.71924', 'examples_per_second': '30.93', 'grad_norm': '89.5', 'counters/examples': 261056, 'counters/updates': 8158}
train stats after 261088 examples: {'rewards_train/chosen': '0.20802', 'rewards_train/rejected': '0.018113', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18991', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-169.44', 'loss/train': '0.61405', 'examples_per_second': '32.717', 'grad_norm': '98.5', 'counters/examples': 261088, 'counters/updates': 8159}
train stats after 261120 examples: {'rewards_train/chosen': '0.12395', 'rewards_train/rejected': '0.093198', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030749', 'logps_train/rejected': '-102.62', 'logps_train/chosen': '-129.61', 'loss/train': '0.68541', 'examples_per_second': '29.977', 'grad_norm': '57', 'counters/examples': 261120, 'counters/updates': 8160}
skipping logging after 261152 examples to avoid logging too frequently
train stats after 261184 examples: {'rewards_train/chosen': '0.20124', 'rewards_train/rejected': '0.090077', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11116', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-150.2', 'loss/train': '0.65662', 'examples_per_second': '31.408', 'grad_norm': '92', 'counters/examples': 261184, 'counters/updates': 8162}
train stats after 261216 examples: {'rewards_train/chosen': '0.11669', 'rewards_train/rejected': '0.043585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073101', 'logps_train/rejected': '-134.46', 'logps_train/chosen': '-171.82', 'loss/train': '0.67146', 'examples_per_second': '30.581', 'grad_norm': '108', 'counters/examples': 261216, 'counters/updates': 8163}
train stats after 261248 examples: {'rewards_train/chosen': '0.16228', 'rewards_train/rejected': '0.11386', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048424', 'logps_train/rejected': '-113.44', 'logps_train/chosen': '-140.84', 'loss/train': '0.6847', 'examples_per_second': '31.17', 'grad_norm': '94.5', 'counters/examples': 261248, 'counters/updates': 8164}
train stats after 261280 examples: {'rewards_train/chosen': '0.10743', 'rewards_train/rejected': '0.068162', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039273', 'logps_train/rejected': '-92.435', 'logps_train/chosen': '-144.67', 'loss/train': '0.69781', 'examples_per_second': '30.685', 'grad_norm': '88', 'counters/examples': 261280, 'counters/updates': 8165}
train stats after 261312 examples: {'rewards_train/chosen': '0.082243', 'rewards_train/rejected': '0.024673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05757', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-144.43', 'loss/train': '0.68308', 'examples_per_second': '31.637', 'grad_norm': '89.5', 'counters/examples': 261312, 'counters/updates': 8166}
train stats after 261344 examples: {'rewards_train/chosen': '0.23963', 'rewards_train/rejected': '0.29788', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.05825', 'logps_train/rejected': '-184.39', 'logps_train/chosen': '-189.54', 'loss/train': '0.77174', 'examples_per_second': '32.811', 'grad_norm': '116', 'counters/examples': 261344, 'counters/updates': 8167}
train stats after 261376 examples: {'rewards_train/chosen': '0.072963', 'rewards_train/rejected': '0.058194', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014769', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-120.62', 'loss/train': '0.69832', 'examples_per_second': '30.211', 'grad_norm': '82.5', 'counters/examples': 261376, 'counters/updates': 8168}
skipping logging after 261408 examples to avoid logging too frequently
train stats after 261440 examples: {'rewards_train/chosen': '0.18039', 'rewards_train/rejected': '0.054773', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12562', 'logps_train/rejected': '-147.62', 'logps_train/chosen': '-181.41', 'loss/train': '0.64584', 'examples_per_second': '30.932', 'grad_norm': '71.5', 'counters/examples': 261440, 'counters/updates': 8170}
train stats after 261472 examples: {'rewards_train/chosen': '0.12794', 'rewards_train/rejected': '0.02168', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10626', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-127.34', 'loss/train': '0.66112', 'examples_per_second': '31.661', 'grad_norm': '131', 'counters/examples': 261472, 'counters/updates': 8171}
skipping logging after 261504 examples to avoid logging too frequently
train stats after 261536 examples: {'rewards_train/chosen': '0.15403', 'rewards_train/rejected': '0.096991', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.057044', 'logps_train/rejected': '-152.83', 'logps_train/chosen': '-154.97', 'loss/train': '0.69212', 'examples_per_second': '31.649', 'grad_norm': '78', 'counters/examples': 261536, 'counters/updates': 8173}
train stats after 261568 examples: {'rewards_train/chosen': '0.12295', 'rewards_train/rejected': '0.10324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019715', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-152.62', 'loss/train': '0.69844', 'examples_per_second': '32.17', 'grad_norm': '133', 'counters/examples': 261568, 'counters/updates': 8174}
train stats after 261600 examples: {'rewards_train/chosen': '0.066006', 'rewards_train/rejected': '0.12967', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.063668', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-115.41', 'loss/train': '0.74232', 'examples_per_second': '31.368', 'grad_norm': '64.5', 'counters/examples': 261600, 'counters/updates': 8175}
train stats after 261632 examples: {'rewards_train/chosen': '0.13462', 'rewards_train/rejected': '0.013554', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12106', 'logps_train/rejected': '-152.53', 'logps_train/chosen': '-164.19', 'loss/train': '0.65687', 'examples_per_second': '30.193', 'grad_norm': '106.5', 'counters/examples': 261632, 'counters/updates': 8176}
skipping logging after 261664 examples to avoid logging too frequently
train stats after 261696 examples: {'rewards_train/chosen': '0.093348', 'rewards_train/rejected': '0.10296', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0096159', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-116.02', 'loss/train': '0.7142', 'examples_per_second': '31.828', 'grad_norm': '60', 'counters/examples': 261696, 'counters/updates': 8178}
skipping logging after 261728 examples to avoid logging too frequently
train stats after 261760 examples: {'rewards_train/chosen': '0.14344', 'rewards_train/rejected': '0.041151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10229', 'logps_train/rejected': '-98.498', 'logps_train/chosen': '-149.72', 'loss/train': '0.64934', 'examples_per_second': '33.996', 'grad_norm': '87', 'counters/examples': 261760, 'counters/updates': 8180}
train stats after 261792 examples: {'rewards_train/chosen': '0.11724', 'rewards_train/rejected': '0.05401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063228', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-140.48', 'loss/train': '0.67114', 'examples_per_second': '31.469', 'grad_norm': '79', 'counters/examples': 261792, 'counters/updates': 8181}
train stats after 261824 examples: {'rewards_train/chosen': '0.10756', 'rewards_train/rejected': '0.049385', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058178', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-147.94', 'loss/train': '0.68013', 'examples_per_second': '32.594', 'grad_norm': '117.5', 'counters/examples': 261824, 'counters/updates': 8182}
train stats after 261856 examples: {'rewards_train/chosen': '0.16037', 'rewards_train/rejected': '0.07547', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084903', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-155.64', 'loss/train': '0.66758', 'examples_per_second': '31.858', 'grad_norm': '93', 'counters/examples': 261856, 'counters/updates': 8183}
train stats after 261888 examples: {'rewards_train/chosen': '0.1975', 'rewards_train/rejected': '0.02821', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16929', 'logps_train/rejected': '-119.76', 'logps_train/chosen': '-173.66', 'loss/train': '0.62354', 'examples_per_second': '31.638', 'grad_norm': '59.75', 'counters/examples': 261888, 'counters/updates': 8184}
train stats after 261920 examples: {'rewards_train/chosen': '0.10503', 'rewards_train/rejected': '0.044906', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060126', 'logps_train/rejected': '-162.57', 'logps_train/chosen': '-160.16', 'loss/train': '0.68232', 'examples_per_second': '31.024', 'grad_norm': '80', 'counters/examples': 261920, 'counters/updates': 8185}
train stats after 261952 examples: {'rewards_train/chosen': '0.1773', 'rewards_train/rejected': '0.059923', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11738', 'logps_train/rejected': '-115.77', 'logps_train/chosen': '-136.53', 'loss/train': '0.65567', 'examples_per_second': '33.065', 'grad_norm': '81', 'counters/examples': 261952, 'counters/updates': 8186}
skipping logging after 261984 examples to avoid logging too frequently
train stats after 262016 examples: {'rewards_train/chosen': '0.20189', 'rewards_train/rejected': '0.0040715', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19782', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-154.62', 'loss/train': '0.61695', 'examples_per_second': '32.403', 'grad_norm': '61.25', 'counters/examples': 262016, 'counters/updates': 8188}
train stats after 262048 examples: {'rewards_train/chosen': '0.11504', 'rewards_train/rejected': '0.12897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013929', 'logps_train/rejected': '-175.05', 'logps_train/chosen': '-179.8', 'loss/train': '0.71785', 'examples_per_second': '30.21', 'grad_norm': '73.5', 'counters/examples': 262048, 'counters/updates': 8189}
train stats after 262080 examples: {'rewards_train/chosen': '0.17352', 'rewards_train/rejected': '0.031401', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14212', 'logps_train/rejected': '-111.09', 'logps_train/chosen': '-141.92', 'loss/train': '0.63912', 'examples_per_second': '31.964', 'grad_norm': '52.5', 'counters/examples': 262080, 'counters/updates': 8190}
train stats after 262112 examples: {'rewards_train/chosen': '0.1383', 'rewards_train/rejected': '0.1364', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0019021', 'logps_train/rejected': '-170.39', 'logps_train/chosen': '-174.28', 'loss/train': '0.70753', 'examples_per_second': '30.78', 'grad_norm': '91', 'counters/examples': 262112, 'counters/updates': 8191}
train stats after 262144 examples: {'rewards_train/chosen': '0.1985', 'rewards_train/rejected': '0.086249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11225', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-161.92', 'loss/train': '0.64751', 'examples_per_second': '31.961', 'grad_norm': '141', 'counters/examples': 262144, 'counters/updates': 8192}
train stats after 262176 examples: {'rewards_train/chosen': '0.1423', 'rewards_train/rejected': '0.063818', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078478', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-141.76', 'loss/train': '0.66752', 'examples_per_second': '30.692', 'grad_norm': '73.5', 'counters/examples': 262176, 'counters/updates': 8193}
train stats after 262208 examples: {'rewards_train/chosen': '0.18356', 'rewards_train/rejected': '0.13875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044804', 'logps_train/rejected': '-148', 'logps_train/chosen': '-141.41', 'loss/train': '0.69246', 'examples_per_second': '32.599', 'grad_norm': '141', 'counters/examples': 262208, 'counters/updates': 8194}
train stats after 262240 examples: {'rewards_train/chosen': '0.10675', 'rewards_train/rejected': '0.11909', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012335', 'logps_train/rejected': '-159.5', 'logps_train/chosen': '-166.62', 'loss/train': '0.71371', 'examples_per_second': '31.68', 'grad_norm': '226', 'counters/examples': 262240, 'counters/updates': 8195}
train stats after 262272 examples: {'rewards_train/chosen': '0.039345', 'rewards_train/rejected': '0.13773', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.098382', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-137.58', 'loss/train': '0.76454', 'examples_per_second': '31.746', 'grad_norm': '83', 'counters/examples': 262272, 'counters/updates': 8196}
train stats after 262304 examples: {'rewards_train/chosen': '0.01007', 'rewards_train/rejected': '0.026022', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015951', 'logps_train/rejected': '-104.52', 'logps_train/chosen': '-150.23', 'loss/train': '0.71768', 'examples_per_second': '32.184', 'grad_norm': '93.5', 'counters/examples': 262304, 'counters/updates': 8197}
train stats after 262336 examples: {'rewards_train/chosen': '0.23885', 'rewards_train/rejected': '0.1786', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060248', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-157.54', 'loss/train': '0.69324', 'examples_per_second': '31.625', 'grad_norm': '117.5', 'counters/examples': 262336, 'counters/updates': 8198}
train stats after 262368 examples: {'rewards_train/chosen': '0.13187', 'rewards_train/rejected': '0.15098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.019113', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-152.34', 'loss/train': '0.72232', 'examples_per_second': '31.483', 'grad_norm': '114.5', 'counters/examples': 262368, 'counters/updates': 8199}
train stats after 262400 examples: {'rewards_train/chosen': '0.12688', 'rewards_train/rejected': '0.064749', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.062132', 'logps_train/rejected': '-149.04', 'logps_train/chosen': '-109.17', 'loss/train': '0.67549', 'examples_per_second': '31.671', 'grad_norm': '59.75', 'counters/examples': 262400, 'counters/updates': 8200}
train stats after 262432 examples: {'rewards_train/chosen': '0.1212', 'rewards_train/rejected': '0.092156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029044', 'logps_train/rejected': '-134.69', 'logps_train/chosen': '-117.82', 'loss/train': '0.6934', 'examples_per_second': '30.285', 'grad_norm': '67.5', 'counters/examples': 262432, 'counters/updates': 8201}
skipping logging after 262464 examples to avoid logging too frequently
train stats after 262496 examples: {'rewards_train/chosen': '0.21172', 'rewards_train/rejected': '0.16999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041739', 'logps_train/rejected': '-147.48', 'logps_train/chosen': '-170.68', 'loss/train': '0.6901', 'examples_per_second': '31.817', 'grad_norm': '115.5', 'counters/examples': 262496, 'counters/updates': 8203}
train stats after 262528 examples: {'rewards_train/chosen': '0.22788', 'rewards_train/rejected': '0.015455', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21243', 'logps_train/rejected': '-142.46', 'logps_train/chosen': '-157.95', 'loss/train': '0.60569', 'examples_per_second': '24.613', 'grad_norm': '121.5', 'counters/examples': 262528, 'counters/updates': 8204}
train stats after 262560 examples: {'rewards_train/chosen': '0.1545', 'rewards_train/rejected': '0.00839', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14611', 'logps_train/rejected': '-152.79', 'logps_train/chosen': '-149.7', 'loss/train': '0.63029', 'examples_per_second': '31.688', 'grad_norm': '100.5', 'counters/examples': 262560, 'counters/updates': 8205}
train stats after 262592 examples: {'rewards_train/chosen': '0.14815', 'rewards_train/rejected': '0.13382', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014332', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-149.02', 'loss/train': '0.69836', 'examples_per_second': '31.27', 'grad_norm': '83', 'counters/examples': 262592, 'counters/updates': 8206}
train stats after 262624 examples: {'rewards_train/chosen': '0.14807', 'rewards_train/rejected': '-0.0002249', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1483', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-164.05', 'loss/train': '0.63794', 'examples_per_second': '27.127', 'grad_norm': '68.5', 'counters/examples': 262624, 'counters/updates': 8207}
train stats after 262656 examples: {'rewards_train/chosen': '0.17373', 'rewards_train/rejected': '0.13909', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034637', 'logps_train/rejected': '-150.02', 'logps_train/chosen': '-145.08', 'loss/train': '0.69813', 'examples_per_second': '30.636', 'grad_norm': '97.5', 'counters/examples': 262656, 'counters/updates': 8208}
skipping logging after 262688 examples to avoid logging too frequently
train stats after 262720 examples: {'rewards_train/chosen': '0.078047', 'rewards_train/rejected': '0.0223', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055746', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-114.72', 'loss/train': '0.68513', 'examples_per_second': '39.302', 'grad_norm': '55.25', 'counters/examples': 262720, 'counters/updates': 8210}
train stats after 262752 examples: {'rewards_train/chosen': '0.036043', 'rewards_train/rejected': '0.10203', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.065987', 'logps_train/rejected': '-170.17', 'logps_train/chosen': '-136.58', 'loss/train': '0.74473', 'examples_per_second': '31.851', 'grad_norm': '160', 'counters/examples': 262752, 'counters/updates': 8211}
train stats after 262784 examples: {'rewards_train/chosen': '0.14919', 'rewards_train/rejected': '0.093923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055268', 'logps_train/rejected': '-159.49', 'logps_train/chosen': '-154.49', 'loss/train': '0.67588', 'examples_per_second': '32.18', 'grad_norm': '119.5', 'counters/examples': 262784, 'counters/updates': 8212}
train stats after 262816 examples: {'rewards_train/chosen': '0.12912', 'rewards_train/rejected': '0.0092504', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11987', 'logps_train/rejected': '-140.45', 'logps_train/chosen': '-157.5', 'loss/train': '0.65453', 'examples_per_second': '31.365', 'grad_norm': '77.5', 'counters/examples': 262816, 'counters/updates': 8213}
train stats after 262848 examples: {'rewards_train/chosen': '0.15704', 'rewards_train/rejected': '0.20673', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049685', 'logps_train/rejected': '-165.74', 'logps_train/chosen': '-192.67', 'loss/train': '0.73306', 'examples_per_second': '30.298', 'grad_norm': '103.5', 'counters/examples': 262848, 'counters/updates': 8214}
skipping logging after 262880 examples to avoid logging too frequently
train stats after 262912 examples: {'rewards_train/chosen': '0.083502', 'rewards_train/rejected': '0.10224', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018736', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-170.56', 'loss/train': '0.71433', 'examples_per_second': '34.378', 'grad_norm': '78.5', 'counters/examples': 262912, 'counters/updates': 8216}
skipping logging after 262944 examples to avoid logging too frequently
train stats after 262976 examples: {'rewards_train/chosen': '0.13163', 'rewards_train/rejected': '0.075419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056214', 'logps_train/rejected': '-165.96', 'logps_train/chosen': '-161.84', 'loss/train': '0.69166', 'examples_per_second': '31.694', 'grad_norm': '110.5', 'counters/examples': 262976, 'counters/updates': 8218}
train stats after 263008 examples: {'rewards_train/chosen': '0.25347', 'rewards_train/rejected': '0.21862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034857', 'logps_train/rejected': '-145.61', 'logps_train/chosen': '-138.68', 'loss/train': '0.69992', 'examples_per_second': '30.775', 'grad_norm': '88', 'counters/examples': 263008, 'counters/updates': 8219}
skipping logging after 263040 examples to avoid logging too frequently
train stats after 263072 examples: {'rewards_train/chosen': '0.15864', 'rewards_train/rejected': '0.018628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14001', 'logps_train/rejected': '-151.94', 'logps_train/chosen': '-156.5', 'loss/train': '0.64405', 'examples_per_second': '30.91', 'grad_norm': '159', 'counters/examples': 263072, 'counters/updates': 8221}
skipping logging after 263104 examples to avoid logging too frequently
train stats after 263136 examples: {'rewards_train/chosen': '0.18518', 'rewards_train/rejected': '0.11364', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071543', 'logps_train/rejected': '-165.04', 'logps_train/chosen': '-172.4', 'loss/train': '0.67489', 'examples_per_second': '31.647', 'grad_norm': '149', 'counters/examples': 263136, 'counters/updates': 8223}
train stats after 263168 examples: {'rewards_train/chosen': '0.21857', 'rewards_train/rejected': '0.11277', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1058', 'logps_train/rejected': '-148.89', 'logps_train/chosen': '-171.17', 'loss/train': '0.66092', 'examples_per_second': '32.758', 'grad_norm': '94.5', 'counters/examples': 263168, 'counters/updates': 8224}
train stats after 263200 examples: {'rewards_train/chosen': '0.19002', 'rewards_train/rejected': '0.030725', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15929', 'logps_train/rejected': '-112.57', 'logps_train/chosen': '-150.66', 'loss/train': '0.6327', 'examples_per_second': '31.189', 'grad_norm': '70.5', 'counters/examples': 263200, 'counters/updates': 8225}
train stats after 263232 examples: {'rewards_train/chosen': '0.080252', 'rewards_train/rejected': '0.02043', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059822', 'logps_train/rejected': '-120.72', 'logps_train/chosen': '-180.3', 'loss/train': '0.68092', 'examples_per_second': '32.951', 'grad_norm': '60', 'counters/examples': 263232, 'counters/updates': 8226}
train stats after 263264 examples: {'rewards_train/chosen': '0.27459', 'rewards_train/rejected': '0.069728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20486', 'logps_train/rejected': '-182.16', 'logps_train/chosen': '-201.24', 'loss/train': '0.61852', 'examples_per_second': '31.502', 'grad_norm': '82.5', 'counters/examples': 263264, 'counters/updates': 8227}
train stats after 263296 examples: {'rewards_train/chosen': '0.1539', 'rewards_train/rejected': '0.11208', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041822', 'logps_train/rejected': '-151.15', 'logps_train/chosen': '-194.74', 'loss/train': '0.68917', 'examples_per_second': '30.893', 'grad_norm': '124.5', 'counters/examples': 263296, 'counters/updates': 8228}
train stats after 263328 examples: {'rewards_train/chosen': '0.11738', 'rewards_train/rejected': '0.051629', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06575', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-170.34', 'loss/train': '0.66794', 'examples_per_second': '32.84', 'grad_norm': '155', 'counters/examples': 263328, 'counters/updates': 8229}
train stats after 263360 examples: {'rewards_train/chosen': '0.23707', 'rewards_train/rejected': '-0.007623', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2447', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-180.35', 'loss/train': '0.60784', 'examples_per_second': '31.911', 'grad_norm': '121', 'counters/examples': 263360, 'counters/updates': 8230}
train stats after 263392 examples: {'rewards_train/chosen': '0.23688', 'rewards_train/rejected': '0.10324', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13364', 'logps_train/rejected': '-110.48', 'logps_train/chosen': '-158.1', 'loss/train': '0.64892', 'examples_per_second': '31.643', 'grad_norm': '189', 'counters/examples': 263392, 'counters/updates': 8231}
train stats after 263424 examples: {'rewards_train/chosen': '0.21309', 'rewards_train/rejected': '0.062684', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1504', 'logps_train/rejected': '-103.23', 'logps_train/chosen': '-162.36', 'loss/train': '0.63801', 'examples_per_second': '31.414', 'grad_norm': '96.5', 'counters/examples': 263424, 'counters/updates': 8232}
train stats after 263456 examples: {'rewards_train/chosen': '0.060031', 'rewards_train/rejected': '0.097903', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037872', 'logps_train/rejected': '-105.81', 'logps_train/chosen': '-128.13', 'loss/train': '0.73759', 'examples_per_second': '31.705', 'grad_norm': '76', 'counters/examples': 263456, 'counters/updates': 8233}
train stats after 263488 examples: {'rewards_train/chosen': '0.054908', 'rewards_train/rejected': '0.056684', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0017756', 'logps_train/rejected': '-170.36', 'logps_train/chosen': '-150.62', 'loss/train': '0.71409', 'examples_per_second': '31.56', 'grad_norm': '85.5', 'counters/examples': 263488, 'counters/updates': 8234}
train stats after 263520 examples: {'rewards_train/chosen': '0.20437', 'rewards_train/rejected': '0.075914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12846', 'logps_train/rejected': '-158.78', 'logps_train/chosen': '-181.62', 'loss/train': '0.64025', 'examples_per_second': '24.183', 'grad_norm': '102.5', 'counters/examples': 263520, 'counters/updates': 8235}
train stats after 263552 examples: {'rewards_train/chosen': '0.050171', 'rewards_train/rejected': '0.13757', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.087397', 'logps_train/rejected': '-149.47', 'logps_train/chosen': '-154.79', 'loss/train': '0.7645', 'examples_per_second': '30.227', 'grad_norm': '160', 'counters/examples': 263552, 'counters/updates': 8236}
train stats after 263584 examples: {'rewards_train/chosen': '0.094526', 'rewards_train/rejected': '0.072537', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021989', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-131.08', 'loss/train': '0.69156', 'examples_per_second': '31.028', 'grad_norm': '72.5', 'counters/examples': 263584, 'counters/updates': 8237}
skipping logging after 263616 examples to avoid logging too frequently
train stats after 263648 examples: {'rewards_train/chosen': '0.045919', 'rewards_train/rejected': '0.041605', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.004314', 'logps_train/rejected': '-142.71', 'logps_train/chosen': '-130.7', 'loss/train': '0.70149', 'examples_per_second': '36.662', 'grad_norm': '111.5', 'counters/examples': 263648, 'counters/updates': 8239}
train stats after 263680 examples: {'rewards_train/chosen': '0.093157', 'rewards_train/rejected': '0.046277', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046879', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-124.64', 'loss/train': '0.67773', 'examples_per_second': '32.291', 'grad_norm': '93', 'counters/examples': 263680, 'counters/updates': 8240}
train stats after 263712 examples: {'rewards_train/chosen': '0.23155', 'rewards_train/rejected': '0.071312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16024', 'logps_train/rejected': '-161.07', 'logps_train/chosen': '-161.16', 'loss/train': '0.63301', 'examples_per_second': '32.116', 'grad_norm': '77', 'counters/examples': 263712, 'counters/updates': 8241}
train stats after 263744 examples: {'rewards_train/chosen': '0.12534', 'rewards_train/rejected': '0.082039', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043301', 'logps_train/rejected': '-139.08', 'logps_train/chosen': '-123.94', 'loss/train': '0.6827', 'examples_per_second': '30.56', 'grad_norm': '172', 'counters/examples': 263744, 'counters/updates': 8242}
train stats after 263776 examples: {'rewards_train/chosen': '0.1477', 'rewards_train/rejected': '-0.047244', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19494', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-116.12', 'loss/train': '0.62263', 'examples_per_second': '31.596', 'grad_norm': '112', 'counters/examples': 263776, 'counters/updates': 8243}
train stats after 263808 examples: {'rewards_train/chosen': '0.18293', 'rewards_train/rejected': '0.10975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07318', 'logps_train/rejected': '-162.34', 'logps_train/chosen': '-200.91', 'loss/train': '0.66936', 'examples_per_second': '31.787', 'grad_norm': '118', 'counters/examples': 263808, 'counters/updates': 8244}
train stats after 263840 examples: {'rewards_train/chosen': '0.21033', 'rewards_train/rejected': '0.10425', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10608', 'logps_train/rejected': '-167.31', 'logps_train/chosen': '-176.78', 'loss/train': '0.6579', 'examples_per_second': '30.172', 'grad_norm': '70', 'counters/examples': 263840, 'counters/updates': 8245}
skipping logging after 263872 examples to avoid logging too frequently
train stats after 263904 examples: {'rewards_train/chosen': '0.083545', 'rewards_train/rejected': '0.0080277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075517', 'logps_train/rejected': '-99.225', 'logps_train/chosen': '-147.66', 'loss/train': '0.66427', 'examples_per_second': '31.659', 'grad_norm': '58', 'counters/examples': 263904, 'counters/updates': 8247}
train stats after 263936 examples: {'rewards_train/chosen': '0.26342', 'rewards_train/rejected': '0.0098375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25359', 'logps_train/rejected': '-120.24', 'logps_train/chosen': '-185.93', 'loss/train': '0.59765', 'examples_per_second': '30.216', 'grad_norm': '78.5', 'counters/examples': 263936, 'counters/updates': 8248}
train stats after 263968 examples: {'rewards_train/chosen': '0.10148', 'rewards_train/rejected': '-0.0018824', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10336', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-153.58', 'loss/train': '0.65685', 'examples_per_second': '31.078', 'grad_norm': '91', 'counters/examples': 263968, 'counters/updates': 8249}
train stats after 264000 examples: {'rewards_train/chosen': '0.19576', 'rewards_train/rejected': '0.037781', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15797', 'logps_train/rejected': '-136.62', 'logps_train/chosen': '-185.1', 'loss/train': '0.63935', 'examples_per_second': '30.997', 'grad_norm': '91.5', 'counters/examples': 264000, 'counters/updates': 8250}
Running evaluation after 264000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.26it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]
eval after 264000: {'rewards_eval/chosen': '0.15834', 'rewards_eval/rejected': '0.060893', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.097451', 'logps_eval/rejected': '-127.53', 'logps_eval/chosen': '-149.42', 'loss/eval': '0.66541'}
skipping save for non epoch
train stats after 264032 examples: {'rewards_train/chosen': '0.082948', 'rewards_train/rejected': '0.088542', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0055936', 'logps_train/rejected': '-139.61', 'logps_train/chosen': '-155.78', 'loss/train': '0.71161', 'examples_per_second': '36.734', 'grad_norm': '73.5', 'counters/examples': 264032, 'counters/updates': 8251}
train stats after 264064 examples: {'rewards_train/chosen': '0.11282', 'rewards_train/rejected': '0.098242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014576', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-137.56', 'loss/train': '0.69314', 'examples_per_second': '32.456', 'grad_norm': '105', 'counters/examples': 264064, 'counters/updates': 8252}
skipping logging after 264096 examples to avoid logging too frequently
train stats after 264128 examples: {'rewards_train/chosen': '0.12324', 'rewards_train/rejected': '0.020395', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10284', 'logps_train/rejected': '-144.42', 'logps_train/chosen': '-177.18', 'loss/train': '0.68153', 'examples_per_second': '31.533', 'grad_norm': '104.5', 'counters/examples': 264128, 'counters/updates': 8254}
train stats after 264160 examples: {'rewards_train/chosen': '0.1972', 'rewards_train/rejected': '0.11032', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086876', 'logps_train/rejected': '-98.685', 'logps_train/chosen': '-153.48', 'loss/train': '0.67538', 'examples_per_second': '31.778', 'grad_norm': '90.5', 'counters/examples': 264160, 'counters/updates': 8255}
train stats after 264192 examples: {'rewards_train/chosen': '0.08663', 'rewards_train/rejected': '0.1323', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045666', 'logps_train/rejected': '-127.49', 'logps_train/chosen': '-155.65', 'loss/train': '0.73594', 'examples_per_second': '32.2', 'grad_norm': '98', 'counters/examples': 264192, 'counters/updates': 8256}
train stats after 264224 examples: {'rewards_train/chosen': '0.19067', 'rewards_train/rejected': '-0.0094308', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.2001', 'logps_train/rejected': '-101.39', 'logps_train/chosen': '-128.29', 'loss/train': '0.61353', 'examples_per_second': '30.936', 'grad_norm': '86', 'counters/examples': 264224, 'counters/updates': 8257}
train stats after 264256 examples: {'rewards_train/chosen': '0.090275', 'rewards_train/rejected': '0.059005', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03127', 'logps_train/rejected': '-143', 'logps_train/chosen': '-120.79', 'loss/train': '0.6923', 'examples_per_second': '30.798', 'grad_norm': '85', 'counters/examples': 264256, 'counters/updates': 8258}
skipping logging after 264288 examples to avoid logging too frequently
train stats after 264320 examples: {'rewards_train/chosen': '0.13325', 'rewards_train/rejected': '0.11417', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019074', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-166.76', 'loss/train': '0.70109', 'examples_per_second': '31.613', 'grad_norm': '90.5', 'counters/examples': 264320, 'counters/updates': 8260}
train stats after 264352 examples: {'rewards_train/chosen': '0.12661', 'rewards_train/rejected': '0.052689', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073922', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-120.77', 'loss/train': '0.66632', 'examples_per_second': '31.38', 'grad_norm': '66.5', 'counters/examples': 264352, 'counters/updates': 8261}
train stats after 264384 examples: {'rewards_train/chosen': '0.16925', 'rewards_train/rejected': '0.074351', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.094898', 'logps_train/rejected': '-152.52', 'logps_train/chosen': '-157.28', 'loss/train': '0.66875', 'examples_per_second': '31.486', 'grad_norm': '79', 'counters/examples': 264384, 'counters/updates': 8262}
train stats after 264416 examples: {'rewards_train/chosen': '0.13619', 'rewards_train/rejected': '0.0817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054489', 'logps_train/rejected': '-162.27', 'logps_train/chosen': '-119.56', 'loss/train': '0.67887', 'examples_per_second': '31.666', 'grad_norm': '54', 'counters/examples': 264416, 'counters/updates': 8263}
train stats after 264448 examples: {'rewards_train/chosen': '0.13401', 'rewards_train/rejected': '0.16883', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034824', 'logps_train/rejected': '-175.12', 'logps_train/chosen': '-176.21', 'loss/train': '0.72528', 'examples_per_second': '30.171', 'grad_norm': '97', 'counters/examples': 264448, 'counters/updates': 8264}
train stats after 264480 examples: {'rewards_train/chosen': '0.20639', 'rewards_train/rejected': '0.14445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061944', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-139.85', 'loss/train': '0.71642', 'examples_per_second': '32.726', 'grad_norm': '176', 'counters/examples': 264480, 'counters/updates': 8265}
train stats after 264512 examples: {'rewards_train/chosen': '0.12939', 'rewards_train/rejected': '-0.045246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17463', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-150.84', 'loss/train': '0.62817', 'examples_per_second': '31.582', 'grad_norm': '81', 'counters/examples': 264512, 'counters/updates': 8266}
train stats after 264544 examples: {'rewards_train/chosen': '0.1873', 'rewards_train/rejected': '-0.0067398', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19404', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-151.55', 'loss/train': '0.62572', 'examples_per_second': '30.863', 'grad_norm': '96.5', 'counters/examples': 264544, 'counters/updates': 8267}
train stats after 264576 examples: {'rewards_train/chosen': '0.17232', 'rewards_train/rejected': '0.065437', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10688', 'logps_train/rejected': '-140.67', 'logps_train/chosen': '-143.46', 'loss/train': '0.65449', 'examples_per_second': '30.238', 'grad_norm': '88.5', 'counters/examples': 264576, 'counters/updates': 8268}
train stats after 264608 examples: {'rewards_train/chosen': '0.10996', 'rewards_train/rejected': '0.061385', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048575', 'logps_train/rejected': '-114.41', 'logps_train/chosen': '-179.58', 'loss/train': '0.6892', 'examples_per_second': '30.312', 'grad_norm': '103.5', 'counters/examples': 264608, 'counters/updates': 8269}
train stats after 264640 examples: {'rewards_train/chosen': '0.085569', 'rewards_train/rejected': '0.076525', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.009044', 'logps_train/rejected': '-129.11', 'logps_train/chosen': '-157.3', 'loss/train': '0.70796', 'examples_per_second': '33.194', 'grad_norm': '82.5', 'counters/examples': 264640, 'counters/updates': 8270}
train stats after 264672 examples: {'rewards_train/chosen': '0.14393', 'rewards_train/rejected': '0.063411', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080519', 'logps_train/rejected': '-119.86', 'logps_train/chosen': '-119.88', 'loss/train': '0.67774', 'examples_per_second': '33.154', 'grad_norm': '75', 'counters/examples': 264672, 'counters/updates': 8271}
train stats after 264704 examples: {'rewards_train/chosen': '0.18094', 'rewards_train/rejected': '0.038292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14265', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-155.52', 'loss/train': '0.63502', 'examples_per_second': '32.161', 'grad_norm': '96.5', 'counters/examples': 264704, 'counters/updates': 8272}
skipping logging after 264736 examples to avoid logging too frequently
train stats after 264768 examples: {'rewards_train/chosen': '0.19436', 'rewards_train/rejected': '0.027487', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16688', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-179.68', 'loss/train': '0.62768', 'examples_per_second': '34.889', 'grad_norm': '46', 'counters/examples': 264768, 'counters/updates': 8274}
skipping logging after 264800 examples to avoid logging too frequently
train stats after 264832 examples: {'rewards_train/chosen': '0.036601', 'rewards_train/rejected': '0.052369', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015768', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-129.16', 'loss/train': '0.71561', 'examples_per_second': '31.799', 'grad_norm': '104.5', 'counters/examples': 264832, 'counters/updates': 8276}
train stats after 264864 examples: {'rewards_train/chosen': '0.14004', 'rewards_train/rejected': '0.097519', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042521', 'logps_train/rejected': '-139', 'logps_train/chosen': '-145.21', 'loss/train': '0.69647', 'examples_per_second': '30.97', 'grad_norm': '121', 'counters/examples': 264864, 'counters/updates': 8277}
train stats after 264896 examples: {'rewards_train/chosen': '0.15561', 'rewards_train/rejected': '0.15478', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00082824', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-152.75', 'loss/train': '0.7026', 'examples_per_second': '32.437', 'grad_norm': '104', 'counters/examples': 264896, 'counters/updates': 8278}
train stats after 264928 examples: {'rewards_train/chosen': '0.14962', 'rewards_train/rejected': '0.054305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095314', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-148.58', 'loss/train': '0.66105', 'examples_per_second': '30.689', 'grad_norm': '63.75', 'counters/examples': 264928, 'counters/updates': 8279}
train stats after 264960 examples: {'rewards_train/chosen': '0.20219', 'rewards_train/rejected': '0.060108', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14208', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-163', 'loss/train': '0.63842', 'examples_per_second': '30.458', 'grad_norm': '95.5', 'counters/examples': 264960, 'counters/updates': 8280}
train stats after 264992 examples: {'rewards_train/chosen': '0.21422', 'rewards_train/rejected': '0.13973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074485', 'logps_train/rejected': '-153.98', 'logps_train/chosen': '-171.56', 'loss/train': '0.67449', 'examples_per_second': '32.538', 'grad_norm': '101.5', 'counters/examples': 264992, 'counters/updates': 8281}
train stats after 265024 examples: {'rewards_train/chosen': '0.20193', 'rewards_train/rejected': '-0.052695', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.25462', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-141.01', 'loss/train': '0.58153', 'examples_per_second': '31.736', 'grad_norm': '55', 'counters/examples': 265024, 'counters/updates': 8282}
train stats after 265056 examples: {'rewards_train/chosen': '0.28461', 'rewards_train/rejected': '0.12369', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16091', 'logps_train/rejected': '-159.05', 'logps_train/chosen': '-173.21', 'loss/train': '0.64878', 'examples_per_second': '30.159', 'grad_norm': '95', 'counters/examples': 265056, 'counters/updates': 8283}
train stats after 265088 examples: {'rewards_train/chosen': '0.23789', 'rewards_train/rejected': '0.13049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10741', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-138', 'loss/train': '0.65767', 'examples_per_second': '32.632', 'grad_norm': '66.5', 'counters/examples': 265088, 'counters/updates': 8284}
train stats after 265120 examples: {'rewards_train/chosen': '0.088454', 'rewards_train/rejected': '0.17822', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.089768', 'logps_train/rejected': '-148.45', 'logps_train/chosen': '-156.85', 'loss/train': '0.75121', 'examples_per_second': '31.603', 'grad_norm': '167', 'counters/examples': 265120, 'counters/updates': 8285}
train stats after 265152 examples: {'rewards_train/chosen': '0.11964', 'rewards_train/rejected': '0.10676', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01288', 'logps_train/rejected': '-128.85', 'logps_train/chosen': '-137.51', 'loss/train': '0.70721', 'examples_per_second': '30.962', 'grad_norm': '126.5', 'counters/examples': 265152, 'counters/updates': 8286}
train stats after 265184 examples: {'rewards_train/chosen': '0.0462', 'rewards_train/rejected': '0.0025977', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043603', 'logps_train/rejected': '-147.81', 'logps_train/chosen': '-125.61', 'loss/train': '0.6897', 'examples_per_second': '32.446', 'grad_norm': '131', 'counters/examples': 265184, 'counters/updates': 8287}
skipping logging after 265216 examples to avoid logging too frequently
train stats after 265248 examples: {'rewards_train/chosen': '0.12537', 'rewards_train/rejected': '0.093867', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031506', 'logps_train/rejected': '-102.06', 'logps_train/chosen': '-174.73', 'loss/train': '0.68904', 'examples_per_second': '30.319', 'grad_norm': '91', 'counters/examples': 265248, 'counters/updates': 8289}
train stats after 265280 examples: {'rewards_train/chosen': '0.14673', 'rewards_train/rejected': '0.02226', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12447', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-168.2', 'loss/train': '0.64307', 'examples_per_second': '32.571', 'grad_norm': '62.25', 'counters/examples': 265280, 'counters/updates': 8290}
train stats after 265312 examples: {'rewards_train/chosen': '0.14253', 'rewards_train/rejected': '0.1701', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027564', 'logps_train/rejected': '-148.9', 'logps_train/chosen': '-175.64', 'loss/train': '0.73658', 'examples_per_second': '31.569', 'grad_norm': '128', 'counters/examples': 265312, 'counters/updates': 8291}
skipping logging after 265344 examples to avoid logging too frequently
train stats after 265376 examples: {'rewards_train/chosen': '-0.030335', 'rewards_train/rejected': '0.10124', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.13158', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-144.05', 'loss/train': '0.78908', 'examples_per_second': '31.647', 'grad_norm': '124', 'counters/examples': 265376, 'counters/updates': 8293}
train stats after 265408 examples: {'rewards_train/chosen': '0.025198', 'rewards_train/rejected': '0.14631', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12112', 'logps_train/rejected': '-153.55', 'logps_train/chosen': '-147.07', 'loss/train': '0.80353', 'examples_per_second': '31.293', 'grad_norm': '172', 'counters/examples': 265408, 'counters/updates': 8294}
skipping logging after 265440 examples to avoid logging too frequently
train stats after 265472 examples: {'rewards_train/chosen': '0.10257', 'rewards_train/rejected': '-0.00068525', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10325', 'logps_train/rejected': '-108.54', 'logps_train/chosen': '-111.7', 'loss/train': '0.65278', 'examples_per_second': '34.458', 'grad_norm': '83.5', 'counters/examples': 265472, 'counters/updates': 8296}
train stats after 265504 examples: {'rewards_train/chosen': '0.28327', 'rewards_train/rejected': '0.11922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16405', 'logps_train/rejected': '-134.48', 'logps_train/chosen': '-182.51', 'loss/train': '0.64189', 'examples_per_second': '30.193', 'grad_norm': '76', 'counters/examples': 265504, 'counters/updates': 8297}
train stats after 265536 examples: {'rewards_train/chosen': '0.1252', 'rewards_train/rejected': '0.10481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020383', 'logps_train/rejected': '-110.14', 'logps_train/chosen': '-151.9', 'loss/train': '0.69939', 'examples_per_second': '32.04', 'grad_norm': '79', 'counters/examples': 265536, 'counters/updates': 8298}
skipping logging after 265568 examples to avoid logging too frequently
train stats after 265600 examples: {'rewards_train/chosen': '0.097988', 'rewards_train/rejected': '-0.024302', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12229', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-170.07', 'loss/train': '0.6514', 'examples_per_second': '33.947', 'grad_norm': '89', 'counters/examples': 265600, 'counters/updates': 8300}
train stats after 265632 examples: {'rewards_train/chosen': '0.01339', 'rewards_train/rejected': '-0.0013895', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01478', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-129.03', 'loss/train': '0.69179', 'examples_per_second': '32.671', 'grad_norm': '88', 'counters/examples': 265632, 'counters/updates': 8301}
train stats after 265664 examples: {'rewards_train/chosen': '0.1851', 'rewards_train/rejected': '0.098421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086677', 'logps_train/rejected': '-164.09', 'logps_train/chosen': '-155.96', 'loss/train': '0.66178', 'examples_per_second': '30.669', 'grad_norm': '108', 'counters/examples': 265664, 'counters/updates': 8302}
train stats after 265696 examples: {'rewards_train/chosen': '0.23657', 'rewards_train/rejected': '0.14693', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089648', 'logps_train/rejected': '-161.53', 'logps_train/chosen': '-190.6', 'loss/train': '0.67486', 'examples_per_second': '30.141', 'grad_norm': '78', 'counters/examples': 265696, 'counters/updates': 8303}
train stats after 265728 examples: {'rewards_train/chosen': '0.21517', 'rewards_train/rejected': '0.0025894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21258', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-142.2', 'loss/train': '0.60923', 'examples_per_second': '32.385', 'grad_norm': '66', 'counters/examples': 265728, 'counters/updates': 8304}
train stats after 265760 examples: {'rewards_train/chosen': '0.083752', 'rewards_train/rejected': '0.096649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012898', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-140.02', 'loss/train': '0.71499', 'examples_per_second': '31.633', 'grad_norm': '117.5', 'counters/examples': 265760, 'counters/updates': 8305}
train stats after 265792 examples: {'rewards_train/chosen': '0.22835', 'rewards_train/rejected': '0.013238', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21511', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-195.28', 'loss/train': '0.61107', 'examples_per_second': '31.732', 'grad_norm': '124', 'counters/examples': 265792, 'counters/updates': 8306}
train stats after 265824 examples: {'rewards_train/chosen': '0.30945', 'rewards_train/rejected': '-0.019428', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.32888', 'logps_train/rejected': '-111.86', 'logps_train/chosen': '-186.15', 'loss/train': '0.60073', 'examples_per_second': '32.956', 'grad_norm': '59', 'counters/examples': 265824, 'counters/updates': 8307}
train stats after 265856 examples: {'rewards_train/chosen': '0.18104', 'rewards_train/rejected': '0.086967', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094074', 'logps_train/rejected': '-120.79', 'logps_train/chosen': '-145.41', 'loss/train': '0.67375', 'examples_per_second': '31.584', 'grad_norm': '100', 'counters/examples': 265856, 'counters/updates': 8308}
train stats after 265888 examples: {'rewards_train/chosen': '0.13606', 'rewards_train/rejected': '0.20243', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.066378', 'logps_train/rejected': '-147.63', 'logps_train/chosen': '-139.23', 'loss/train': '0.73817', 'examples_per_second': '30.685', 'grad_norm': '92', 'counters/examples': 265888, 'counters/updates': 8309}
train stats after 265920 examples: {'rewards_train/chosen': '0.11655', 'rewards_train/rejected': '0.036875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07968', 'logps_train/rejected': '-161.21', 'logps_train/chosen': '-172.12', 'loss/train': '0.67613', 'examples_per_second': '31.966', 'grad_norm': '97.5', 'counters/examples': 265920, 'counters/updates': 8310}
train stats after 265952 examples: {'rewards_train/chosen': '0.14593', 'rewards_train/rejected': '0.048748', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097178', 'logps_train/rejected': '-147.88', 'logps_train/chosen': '-179.23', 'loss/train': '0.65724', 'examples_per_second': '32.201', 'grad_norm': '81', 'counters/examples': 265952, 'counters/updates': 8311}
train stats after 265984 examples: {'rewards_train/chosen': '0.17446', 'rewards_train/rejected': '0.036011', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13845', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-152.51', 'loss/train': '0.64619', 'examples_per_second': '31.645', 'grad_norm': '79.5', 'counters/examples': 265984, 'counters/updates': 8312}
train stats after 266016 examples: {'rewards_train/chosen': '0.14579', 'rewards_train/rejected': '0.059658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086129', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-152.95', 'loss/train': '0.66149', 'examples_per_second': '30.614', 'grad_norm': '49.75', 'counters/examples': 266016, 'counters/updates': 8313}
train stats after 266048 examples: {'rewards_train/chosen': '0.043113', 'rewards_train/rejected': '-0.0051831', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048296', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-201.36', 'loss/train': '0.68186', 'examples_per_second': '30.715', 'grad_norm': '72.5', 'counters/examples': 266048, 'counters/updates': 8314}
train stats after 266080 examples: {'rewards_train/chosen': '0.096538', 'rewards_train/rejected': '0.12015', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02361', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-172.49', 'loss/train': '0.72259', 'examples_per_second': '31.598', 'grad_norm': '103.5', 'counters/examples': 266080, 'counters/updates': 8315}
train stats after 266112 examples: {'rewards_train/chosen': '0.2112', 'rewards_train/rejected': '0.08635', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12485', 'logps_train/rejected': '-137.6', 'logps_train/chosen': '-134.26', 'loss/train': '0.64764', 'examples_per_second': '31.963', 'grad_norm': '65.5', 'counters/examples': 266112, 'counters/updates': 8316}
train stats after 266144 examples: {'rewards_train/chosen': '0.16421', 'rewards_train/rejected': '0.15147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012739', 'logps_train/rejected': '-148.66', 'logps_train/chosen': '-169.32', 'loss/train': '0.70702', 'examples_per_second': '30.288', 'grad_norm': '67', 'counters/examples': 266144, 'counters/updates': 8317}
train stats after 266176 examples: {'rewards_train/chosen': '0.091786', 'rewards_train/rejected': '0.036725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055062', 'logps_train/rejected': '-107.34', 'logps_train/chosen': '-132.2', 'loss/train': '0.67622', 'examples_per_second': '30.815', 'grad_norm': '111', 'counters/examples': 266176, 'counters/updates': 8318}
train stats after 266208 examples: {'rewards_train/chosen': '0.13144', 'rewards_train/rejected': '0.09008', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.041356', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-136.54', 'loss/train': '0.68928', 'examples_per_second': '31.636', 'grad_norm': '66', 'counters/examples': 266208, 'counters/updates': 8319}
train stats after 266240 examples: {'rewards_train/chosen': '0.080283', 'rewards_train/rejected': '0.010751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069532', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-150.57', 'loss/train': '0.66751', 'examples_per_second': '31.432', 'grad_norm': '53', 'counters/examples': 266240, 'counters/updates': 8320}
train stats after 266272 examples: {'rewards_train/chosen': '0.20941', 'rewards_train/rejected': '0.23082', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021404', 'logps_train/rejected': '-161.14', 'logps_train/chosen': '-182.26', 'loss/train': '0.73525', 'examples_per_second': '31.533', 'grad_norm': '238', 'counters/examples': 266272, 'counters/updates': 8321}
skipping logging after 266304 examples to avoid logging too frequently
train stats after 266336 examples: {'rewards_train/chosen': '0.11371', 'rewards_train/rejected': '0.11456', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00085034', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-165.58', 'loss/train': '0.70537', 'examples_per_second': '36.342', 'grad_norm': '96.5', 'counters/examples': 266336, 'counters/updates': 8323}
train stats after 266368 examples: {'rewards_train/chosen': '0.18778', 'rewards_train/rejected': '0.12642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061358', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-177.68', 'loss/train': '0.6737', 'examples_per_second': '31.636', 'grad_norm': '63.25', 'counters/examples': 266368, 'counters/updates': 8324}
skipping logging after 266400 examples to avoid logging too frequently
train stats after 266432 examples: {'rewards_train/chosen': '0.085548', 'rewards_train/rejected': '0.025625', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059923', 'logps_train/rejected': '-115.82', 'logps_train/chosen': '-108.72', 'loss/train': '0.6752', 'examples_per_second': '33.766', 'grad_norm': '68', 'counters/examples': 266432, 'counters/updates': 8326}
skipping logging after 266464 examples to avoid logging too frequently
train stats after 266496 examples: {'rewards_train/chosen': '0.22729', 'rewards_train/rejected': '0.066238', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16105', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-168.07', 'loss/train': '0.63191', 'examples_per_second': '31.638', 'grad_norm': '78', 'counters/examples': 266496, 'counters/updates': 8328}
train stats after 266528 examples: {'rewards_train/chosen': '0.096196', 'rewards_train/rejected': '0.025082', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071114', 'logps_train/rejected': '-107.18', 'logps_train/chosen': '-147.66', 'loss/train': '0.67565', 'examples_per_second': '30.154', 'grad_norm': '88', 'counters/examples': 266528, 'counters/updates': 8329}
skipping logging after 266560 examples to avoid logging too frequently
train stats after 266592 examples: {'rewards_train/chosen': '0.14185', 'rewards_train/rejected': '0.062413', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079441', 'logps_train/rejected': '-171.07', 'logps_train/chosen': '-200.56', 'loss/train': '0.67338', 'examples_per_second': '31.601', 'grad_norm': '117', 'counters/examples': 266592, 'counters/updates': 8331}
train stats after 266624 examples: {'rewards_train/chosen': '0.10716', 'rewards_train/rejected': '0.077204', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029955', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-156.68', 'loss/train': '0.70422', 'examples_per_second': '31.598', 'grad_norm': '89', 'counters/examples': 266624, 'counters/updates': 8332}
train stats after 266656 examples: {'rewards_train/chosen': '0.12382', 'rewards_train/rejected': '0.094779', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029041', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-144.01', 'loss/train': '0.69841', 'examples_per_second': '31.068', 'grad_norm': '130', 'counters/examples': 266656, 'counters/updates': 8333}
train stats after 266688 examples: {'rewards_train/chosen': '0.13334', 'rewards_train/rejected': '0.036711', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096627', 'logps_train/rejected': '-134.99', 'logps_train/chosen': '-134.4', 'loss/train': '0.65733', 'examples_per_second': '30.265', 'grad_norm': '338', 'counters/examples': 266688, 'counters/updates': 8334}
train stats after 266720 examples: {'rewards_train/chosen': '0.14188', 'rewards_train/rejected': '-0.011958', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15384', 'logps_train/rejected': '-162.13', 'logps_train/chosen': '-142.15', 'loss/train': '0.63378', 'examples_per_second': '30.687', 'grad_norm': '61.75', 'counters/examples': 266720, 'counters/updates': 8335}
train stats after 266752 examples: {'rewards_train/chosen': '0.19865', 'rewards_train/rejected': '0.056058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1426', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-179.27', 'loss/train': '0.64077', 'examples_per_second': '31.871', 'grad_norm': '49', 'counters/examples': 266752, 'counters/updates': 8336}
train stats after 266784 examples: {'rewards_train/chosen': '0.14556', 'rewards_train/rejected': '0.10403', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041531', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-168.81', 'loss/train': '0.68991', 'examples_per_second': '32.592', 'grad_norm': '70.5', 'counters/examples': 266784, 'counters/updates': 8337}
train stats after 266816 examples: {'rewards_train/chosen': '0.17538', 'rewards_train/rejected': '0.004017', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17136', 'logps_train/rejected': '-109.28', 'logps_train/chosen': '-146.31', 'loss/train': '0.62046', 'examples_per_second': '29.931', 'grad_norm': '110', 'counters/examples': 266816, 'counters/updates': 8338}
train stats after 266848 examples: {'rewards_train/chosen': '0.051779', 'rewards_train/rejected': '0.004455', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047324', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-150.89', 'loss/train': '0.68206', 'examples_per_second': '31.59', 'grad_norm': '83', 'counters/examples': 266848, 'counters/updates': 8339}
train stats after 266880 examples: {'rewards_train/chosen': '0.12895', 'rewards_train/rejected': '0.060834', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068116', 'logps_train/rejected': '-148.15', 'logps_train/chosen': '-173.03', 'loss/train': '0.68272', 'examples_per_second': '30.185', 'grad_norm': '81.5', 'counters/examples': 266880, 'counters/updates': 8340}
train stats after 266912 examples: {'rewards_train/chosen': '0.15064', 'rewards_train/rejected': '0.060789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089854', 'logps_train/rejected': '-147.52', 'logps_train/chosen': '-187.45', 'loss/train': '0.66709', 'examples_per_second': '30.143', 'grad_norm': '60.75', 'counters/examples': 266912, 'counters/updates': 8341}
train stats after 266944 examples: {'rewards_train/chosen': '0.094856', 'rewards_train/rejected': '0.064803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030053', 'logps_train/rejected': '-188.27', 'logps_train/chosen': '-158.74', 'loss/train': '0.69369', 'examples_per_second': '30.9', 'grad_norm': '93.5', 'counters/examples': 266944, 'counters/updates': 8342}
train stats after 266976 examples: {'rewards_train/chosen': '0.13157', 'rewards_train/rejected': '-0.014328', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1459', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-128', 'loss/train': '0.63549', 'examples_per_second': '31.123', 'grad_norm': '251', 'counters/examples': 266976, 'counters/updates': 8343}
train stats after 267008 examples: {'rewards_train/chosen': '0.2005', 'rewards_train/rejected': '0.10806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092438', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-150.47', 'loss/train': '0.66018', 'examples_per_second': '30.028', 'grad_norm': '209', 'counters/examples': 267008, 'counters/updates': 8344}
train stats after 267040 examples: {'rewards_train/chosen': '0.06931', 'rewards_train/rejected': '-0.0062947', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075605', 'logps_train/rejected': '-92.376', 'logps_train/chosen': '-117.72', 'loss/train': '0.66317', 'examples_per_second': '31.303', 'grad_norm': '54.5', 'counters/examples': 267040, 'counters/updates': 8345}
train stats after 267072 examples: {'rewards_train/chosen': '0.12492', 'rewards_train/rejected': '0.022249', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10268', 'logps_train/rejected': '-146.61', 'logps_train/chosen': '-167.74', 'loss/train': '0.65102', 'examples_per_second': '31.64', 'grad_norm': '177', 'counters/examples': 267072, 'counters/updates': 8346}
train stats after 267104 examples: {'rewards_train/chosen': '0.17418', 'rewards_train/rejected': '0.098862', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075317', 'logps_train/rejected': '-147.63', 'logps_train/chosen': '-144.72', 'loss/train': '0.67834', 'examples_per_second': '30.535', 'grad_norm': '75', 'counters/examples': 267104, 'counters/updates': 8347}
train stats after 267136 examples: {'rewards_train/chosen': '0.13213', 'rewards_train/rejected': '0.048613', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08352', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-198.54', 'loss/train': '0.66074', 'examples_per_second': '30.069', 'grad_norm': '136', 'counters/examples': 267136, 'counters/updates': 8348}
train stats after 267168 examples: {'rewards_train/chosen': '0.10537', 'rewards_train/rejected': '-0.0079047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11327', 'logps_train/rejected': '-102.14', 'logps_train/chosen': '-134.62', 'loss/train': '0.64658', 'examples_per_second': '32.054', 'grad_norm': '81', 'counters/examples': 267168, 'counters/updates': 8349}
train stats after 267200 examples: {'rewards_train/chosen': '0.15745', 'rewards_train/rejected': '0.13294', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024512', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-166.84', 'loss/train': '0.70467', 'examples_per_second': '31.608', 'grad_norm': '113', 'counters/examples': 267200, 'counters/updates': 8350}
skipping logging after 267232 examples to avoid logging too frequently
train stats after 267264 examples: {'rewards_train/chosen': '0.1811', 'rewards_train/rejected': '0.0053542', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17574', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-146.76', 'loss/train': '0.62298', 'examples_per_second': '31.088', 'grad_norm': '62.5', 'counters/examples': 267264, 'counters/updates': 8352}
skipping logging after 267296 examples to avoid logging too frequently
train stats after 267328 examples: {'rewards_train/chosen': '0.095408', 'rewards_train/rejected': '-0.04149', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1369', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-130.73', 'loss/train': '0.64257', 'examples_per_second': '31.608', 'grad_norm': '113', 'counters/examples': 267328, 'counters/updates': 8354}
train stats after 267360 examples: {'rewards_train/chosen': '0.22094', 'rewards_train/rejected': '0.054755', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16619', 'logps_train/rejected': '-100.04', 'logps_train/chosen': '-160.54', 'loss/train': '0.63132', 'examples_per_second': '30.643', 'grad_norm': '79', 'counters/examples': 267360, 'counters/updates': 8355}
train stats after 267392 examples: {'rewards_train/chosen': '0.049984', 'rewards_train/rejected': '0.10262', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052633', 'logps_train/rejected': '-116.26', 'logps_train/chosen': '-159.56', 'loss/train': '0.73506', 'examples_per_second': '31.559', 'grad_norm': '79', 'counters/examples': 267392, 'counters/updates': 8356}
train stats after 267424 examples: {'rewards_train/chosen': '0.10106', 'rewards_train/rejected': '-0.012053', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11311', 'logps_train/rejected': '-138.78', 'logps_train/chosen': '-136.18', 'loss/train': '0.64405', 'examples_per_second': '32.748', 'grad_norm': '73.5', 'counters/examples': 267424, 'counters/updates': 8357}
train stats after 267456 examples: {'rewards_train/chosen': '0.20537', 'rewards_train/rejected': '0.082183', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12319', 'logps_train/rejected': '-151.36', 'logps_train/chosen': '-206.35', 'loss/train': '0.65244', 'examples_per_second': '31.577', 'grad_norm': '196', 'counters/examples': 267456, 'counters/updates': 8358}
train stats after 267488 examples: {'rewards_train/chosen': '0.10406', 'rewards_train/rejected': '0.056752', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.04731', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-148.14', 'loss/train': '0.68845', 'examples_per_second': '31.299', 'grad_norm': '47.5', 'counters/examples': 267488, 'counters/updates': 8359}
train stats after 267520 examples: {'rewards_train/chosen': '0.22204', 'rewards_train/rejected': '0.11328', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10876', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-148.59', 'loss/train': '0.65147', 'examples_per_second': '31.72', 'grad_norm': '78.5', 'counters/examples': 267520, 'counters/updates': 8360}
train stats after 267552 examples: {'rewards_train/chosen': '0.12348', 'rewards_train/rejected': '-0.02946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15294', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-158.18', 'loss/train': '0.63298', 'examples_per_second': '31.574', 'grad_norm': '86.5', 'counters/examples': 267552, 'counters/updates': 8361}
train stats after 267584 examples: {'rewards_train/chosen': '0.02389', 'rewards_train/rejected': '0.092706', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068816', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-160.97', 'loss/train': '0.74162', 'examples_per_second': '33.489', 'grad_norm': '95.5', 'counters/examples': 267584, 'counters/updates': 8362}
skipping logging after 267616 examples to avoid logging too frequently
train stats after 267648 examples: {'rewards_train/chosen': '0.12588', 'rewards_train/rejected': '0.093129', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032748', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-132.53', 'loss/train': '0.68951', 'examples_per_second': '30.861', 'grad_norm': '56.5', 'counters/examples': 267648, 'counters/updates': 8364}
train stats after 267680 examples: {'rewards_train/chosen': '0.32785', 'rewards_train/rejected': '-0.016971', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.34483', 'logps_train/rejected': '-149.68', 'logps_train/chosen': '-209.85', 'loss/train': '0.58119', 'examples_per_second': '30.139', 'grad_norm': '88', 'counters/examples': 267680, 'counters/updates': 8365}
train stats after 267712 examples: {'rewards_train/chosen': '0.074826', 'rewards_train/rejected': '0.15233', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.077504', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-105.02', 'loss/train': '0.74912', 'examples_per_second': '31.385', 'grad_norm': '72.5', 'counters/examples': 267712, 'counters/updates': 8366}
skipping logging after 267744 examples to avoid logging too frequently
train stats after 267776 examples: {'rewards_train/chosen': '0.16967', 'rewards_train/rejected': '0.074438', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09523', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-155.86', 'loss/train': '0.65672', 'examples_per_second': '31.503', 'grad_norm': '106', 'counters/examples': 267776, 'counters/updates': 8368}
train stats after 267808 examples: {'rewards_train/chosen': '0.1958', 'rewards_train/rejected': '0.024263', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17154', 'logps_train/rejected': '-100.12', 'logps_train/chosen': '-147.21', 'loss/train': '0.62678', 'examples_per_second': '30.741', 'grad_norm': '68', 'counters/examples': 267808, 'counters/updates': 8369}
train stats after 267840 examples: {'rewards_train/chosen': '0.13603', 'rewards_train/rejected': '0.069242', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066788', 'logps_train/rejected': '-99.94', 'logps_train/chosen': '-134.67', 'loss/train': '0.67406', 'examples_per_second': '31.035', 'grad_norm': '119', 'counters/examples': 267840, 'counters/updates': 8370}
skipping logging after 267872 examples to avoid logging too frequently
train stats after 267904 examples: {'rewards_train/chosen': '0.12051', 'rewards_train/rejected': '0.044807', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075705', 'logps_train/rejected': '-107.22', 'logps_train/chosen': '-144.01', 'loss/train': '0.66443', 'examples_per_second': '32.738', 'grad_norm': '102', 'counters/examples': 267904, 'counters/updates': 8372}
train stats after 267936 examples: {'rewards_train/chosen': '0.16726', 'rewards_train/rejected': '0.14099', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026265', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-140.17', 'loss/train': '0.69772', 'examples_per_second': '30.561', 'grad_norm': '59', 'counters/examples': 267936, 'counters/updates': 8373}
train stats after 267968 examples: {'rewards_train/chosen': '0.071864', 'rewards_train/rejected': '0.08739', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015526', 'logps_train/rejected': '-142.51', 'logps_train/chosen': '-169.07', 'loss/train': '0.7183', 'examples_per_second': '30.45', 'grad_norm': '134', 'counters/examples': 267968, 'counters/updates': 8374}
train stats after 268000 examples: {'rewards_train/chosen': '0.13542', 'rewards_train/rejected': '0.046357', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089067', 'logps_train/rejected': '-147.98', 'logps_train/chosen': '-172.24', 'loss/train': '0.67106', 'examples_per_second': '22.66', 'grad_norm': '70.5', 'counters/examples': 268000, 'counters/updates': 8375}
train stats after 268032 examples: {'rewards_train/chosen': '0.08144', 'rewards_train/rejected': '0.074512', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0069278', 'logps_train/rejected': '-151.9', 'logps_train/chosen': '-146.59', 'loss/train': '0.70158', 'examples_per_second': '30.96', 'grad_norm': '97.5', 'counters/examples': 268032, 'counters/updates': 8376}
train stats after 268064 examples: {'rewards_train/chosen': '0.098678', 'rewards_train/rejected': '0.028936', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069741', 'logps_train/rejected': '-109.95', 'logps_train/chosen': '-133.25', 'loss/train': '0.67453', 'examples_per_second': '30.582', 'grad_norm': '122.5', 'counters/examples': 268064, 'counters/updates': 8377}
train stats after 268096 examples: {'rewards_train/chosen': '0.16778', 'rewards_train/rejected': '0.12436', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043417', 'logps_train/rejected': '-135.68', 'logps_train/chosen': '-197.76', 'loss/train': '0.68897', 'examples_per_second': '24.24', 'grad_norm': '66.5', 'counters/examples': 268096, 'counters/updates': 8378}
train stats after 268128 examples: {'rewards_train/chosen': '0.13754', 'rewards_train/rejected': '0.05212', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085424', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-140.07', 'loss/train': '0.66622', 'examples_per_second': '31.646', 'grad_norm': '60.75', 'counters/examples': 268128, 'counters/updates': 8379}
skipping logging after 268160 examples to avoid logging too frequently
train stats after 268192 examples: {'rewards_train/chosen': '0.038872', 'rewards_train/rejected': '0.11755', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.078675', 'logps_train/rejected': '-115.89', 'logps_train/chosen': '-162.79', 'loss/train': '0.76878', 'examples_per_second': '32.204', 'grad_norm': '86', 'counters/examples': 268192, 'counters/updates': 8381}
train stats after 268224 examples: {'rewards_train/chosen': '0.14886', 'rewards_train/rejected': '0.044612', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10425', 'logps_train/rejected': '-107.97', 'logps_train/chosen': '-105.68', 'loss/train': '0.65143', 'examples_per_second': '31.627', 'grad_norm': '85.5', 'counters/examples': 268224, 'counters/updates': 8382}
train stats after 268256 examples: {'rewards_train/chosen': '0.18868', 'rewards_train/rejected': '0.17381', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014879', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-153.62', 'loss/train': '0.70377', 'examples_per_second': '30.69', 'grad_norm': '62.75', 'counters/examples': 268256, 'counters/updates': 8383}
train stats after 268288 examples: {'rewards_train/chosen': '0.2239', 'rewards_train/rejected': '0.10597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11793', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-174.05', 'loss/train': '0.65223', 'examples_per_second': '30.922', 'grad_norm': '119.5', 'counters/examples': 268288, 'counters/updates': 8384}
train stats after 268320 examples: {'rewards_train/chosen': '0.35651', 'rewards_train/rejected': '0.098247', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25826', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-194.86', 'loss/train': '0.59093', 'examples_per_second': '30.609', 'grad_norm': '68', 'counters/examples': 268320, 'counters/updates': 8385}
train stats after 268352 examples: {'rewards_train/chosen': '0.10926', 'rewards_train/rejected': '-0.044126', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15339', 'logps_train/rejected': '-197.35', 'logps_train/chosen': '-130.73', 'loss/train': '0.63512', 'examples_per_second': '31.574', 'grad_norm': '131', 'counters/examples': 268352, 'counters/updates': 8386}
train stats after 268384 examples: {'rewards_train/chosen': '0.044506', 'rewards_train/rejected': '0.0052734', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.039232', 'logps_train/rejected': '-174.28', 'logps_train/chosen': '-166.67', 'loss/train': '0.69655', 'examples_per_second': '31.62', 'grad_norm': '102.5', 'counters/examples': 268384, 'counters/updates': 8387}
train stats after 268416 examples: {'rewards_train/chosen': '0.094484', 'rewards_train/rejected': '0.09543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00094613', 'logps_train/rejected': '-171.53', 'logps_train/chosen': '-201.59', 'loss/train': '0.71215', 'examples_per_second': '31.623', 'grad_norm': '113.5', 'counters/examples': 268416, 'counters/updates': 8388}
skipping logging after 268448 examples to avoid logging too frequently
train stats after 268480 examples: {'rewards_train/chosen': '0.085861', 'rewards_train/rejected': '0.057819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028042', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-141.68', 'loss/train': '0.68886', 'examples_per_second': '31.652', 'grad_norm': '85', 'counters/examples': 268480, 'counters/updates': 8390}
skipping logging after 268512 examples to avoid logging too frequently
train stats after 268544 examples: {'rewards_train/chosen': '0.14172', 'rewards_train/rejected': '0.076483', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06524', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-132.56', 'loss/train': '0.67949', 'examples_per_second': '33.911', 'grad_norm': '66', 'counters/examples': 268544, 'counters/updates': 8392}
train stats after 268576 examples: {'rewards_train/chosen': '0.12429', 'rewards_train/rejected': '0.048835', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075451', 'logps_train/rejected': '-139.32', 'logps_train/chosen': '-139.55', 'loss/train': '0.66895', 'examples_per_second': '31.645', 'grad_norm': '61', 'counters/examples': 268576, 'counters/updates': 8393}
skipping logging after 268608 examples to avoid logging too frequently
train stats after 268640 examples: {'rewards_train/chosen': '0.13932', 'rewards_train/rejected': '0.086619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052698', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-183.61', 'loss/train': '0.68404', 'examples_per_second': '30.189', 'grad_norm': '68.5', 'counters/examples': 268640, 'counters/updates': 8395}
train stats after 268672 examples: {'rewards_train/chosen': '0.23481', 'rewards_train/rejected': '0.11393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12088', 'logps_train/rejected': '-144.05', 'logps_train/chosen': '-176.97', 'loss/train': '0.65257', 'examples_per_second': '31.641', 'grad_norm': '118', 'counters/examples': 268672, 'counters/updates': 8396}
train stats after 268704 examples: {'rewards_train/chosen': '0.15131', 'rewards_train/rejected': '0.044565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10675', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-131.48', 'loss/train': '0.65664', 'examples_per_second': '31.154', 'grad_norm': '65.5', 'counters/examples': 268704, 'counters/updates': 8397}
train stats after 268736 examples: {'rewards_train/chosen': '0.17351', 'rewards_train/rejected': '0.026479', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14703', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-150.74', 'loss/train': '0.63631', 'examples_per_second': '31.665', 'grad_norm': '61.75', 'counters/examples': 268736, 'counters/updates': 8398}
train stats after 268768 examples: {'rewards_train/chosen': '0.11802', 'rewards_train/rejected': '0.061195', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056828', 'logps_train/rejected': '-154.11', 'logps_train/chosen': '-155.95', 'loss/train': '0.67799', 'examples_per_second': '31.657', 'grad_norm': '108.5', 'counters/examples': 268768, 'counters/updates': 8399}
skipping logging after 268800 examples to avoid logging too frequently
train stats after 268832 examples: {'rewards_train/chosen': '0.14678', 'rewards_train/rejected': '0.09069', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056093', 'logps_train/rejected': '-148.73', 'logps_train/chosen': '-148.3', 'loss/train': '0.68057', 'examples_per_second': '31.586', 'grad_norm': '77', 'counters/examples': 268832, 'counters/updates': 8401}
train stats after 268864 examples: {'rewards_train/chosen': '0.18229', 'rewards_train/rejected': '0.082329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09996', 'logps_train/rejected': '-158.73', 'logps_train/chosen': '-183.18', 'loss/train': '0.6709', 'examples_per_second': '30.999', 'grad_norm': '78', 'counters/examples': 268864, 'counters/updates': 8402}
skipping logging after 268896 examples to avoid logging too frequently
train stats after 268928 examples: {'rewards_train/chosen': '0.14638', 'rewards_train/rejected': '0.077904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068471', 'logps_train/rejected': '-153.43', 'logps_train/chosen': '-117.81', 'loss/train': '0.68052', 'examples_per_second': '31.608', 'grad_norm': '70', 'counters/examples': 268928, 'counters/updates': 8404}
skipping logging after 268960 examples to avoid logging too frequently
train stats after 268992 examples: {'rewards_train/chosen': '0.18516', 'rewards_train/rejected': '0.066327', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11883', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-116.21', 'loss/train': '0.64384', 'examples_per_second': '34.652', 'grad_norm': '58.5', 'counters/examples': 268992, 'counters/updates': 8406}
train stats after 269024 examples: {'rewards_train/chosen': '0.14581', 'rewards_train/rejected': '0.24383', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.098024', 'logps_train/rejected': '-161.96', 'logps_train/chosen': '-137.07', 'loss/train': '0.77794', 'examples_per_second': '30.688', 'grad_norm': '188', 'counters/examples': 269024, 'counters/updates': 8407}
skipping logging after 269056 examples to avoid logging too frequently
train stats after 269088 examples: {'rewards_train/chosen': '0.090295', 'rewards_train/rejected': '-0.038403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1287', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-182.29', 'loss/train': '0.64424', 'examples_per_second': '23.737', 'grad_norm': '107.5', 'counters/examples': 269088, 'counters/updates': 8409}
train stats after 269120 examples: {'rewards_train/chosen': '0.20432', 'rewards_train/rejected': '0.033356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17096', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-132.36', 'loss/train': '0.62244', 'examples_per_second': '31.579', 'grad_norm': '73', 'counters/examples': 269120, 'counters/updates': 8410}
train stats after 269152 examples: {'rewards_train/chosen': '0.086946', 'rewards_train/rejected': '0.11286', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025917', 'logps_train/rejected': '-149.21', 'logps_train/chosen': '-116.68', 'loss/train': '0.71781', 'examples_per_second': '32.733', 'grad_norm': '110.5', 'counters/examples': 269152, 'counters/updates': 8411}
skipping logging after 269184 examples to avoid logging too frequently
train stats after 269216 examples: {'rewards_train/chosen': '0.1337', 'rewards_train/rejected': '0.0033737', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13033', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-170.51', 'loss/train': '0.64011', 'examples_per_second': '32.21', 'grad_norm': '86.5', 'counters/examples': 269216, 'counters/updates': 8413}
train stats after 269248 examples: {'rewards_train/chosen': '0.17366', 'rewards_train/rejected': '0.067192', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10647', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-162.5', 'loss/train': '0.67182', 'examples_per_second': '31.841', 'grad_norm': '91.5', 'counters/examples': 269248, 'counters/updates': 8414}
train stats after 269280 examples: {'rewards_train/chosen': '0.14224', 'rewards_train/rejected': '0.16747', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025228', 'logps_train/rejected': '-172.94', 'logps_train/chosen': '-175.57', 'loss/train': '0.72307', 'examples_per_second': '31.618', 'grad_norm': '84', 'counters/examples': 269280, 'counters/updates': 8415}
train stats after 269312 examples: {'rewards_train/chosen': '0.17846', 'rewards_train/rejected': '0.029606', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14886', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-169.41', 'loss/train': '0.64813', 'examples_per_second': '32.217', 'grad_norm': '52', 'counters/examples': 269312, 'counters/updates': 8416}
train stats after 269344 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '0.14505', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026496', 'logps_train/rejected': '-137.43', 'logps_train/chosen': '-151.01', 'loss/train': '0.69494', 'examples_per_second': '31.31', 'grad_norm': '69', 'counters/examples': 269344, 'counters/updates': 8417}
train stats after 269376 examples: {'rewards_train/chosen': '0.10276', 'rewards_train/rejected': '0.04979', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052969', 'logps_train/rejected': '-112.14', 'logps_train/chosen': '-119.09', 'loss/train': '0.67728', 'examples_per_second': '32.617', 'grad_norm': '63.25', 'counters/examples': 269376, 'counters/updates': 8418}
skipping logging after 269408 examples to avoid logging too frequently
train stats after 269440 examples: {'rewards_train/chosen': '0.12872', 'rewards_train/rejected': '0.12703', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0016912', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-137.93', 'loss/train': '0.7095', 'examples_per_second': '30.053', 'grad_norm': '130', 'counters/examples': 269440, 'counters/updates': 8420}
train stats after 269472 examples: {'rewards_train/chosen': '0.14708', 'rewards_train/rejected': '0.16981', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.022737', 'logps_train/rejected': '-169.71', 'logps_train/chosen': '-135.79', 'loss/train': '0.72036', 'examples_per_second': '30.713', 'grad_norm': '149', 'counters/examples': 269472, 'counters/updates': 8421}
train stats after 269504 examples: {'rewards_train/chosen': '0.17733', 'rewards_train/rejected': '0.15441', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022926', 'logps_train/rejected': '-165.24', 'logps_train/chosen': '-154.89', 'loss/train': '0.69948', 'examples_per_second': '30.2', 'grad_norm': '92.5', 'counters/examples': 269504, 'counters/updates': 8422}
train stats after 269536 examples: {'rewards_train/chosen': '0.094242', 'rewards_train/rejected': '0.065889', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028353', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-202.03', 'loss/train': '0.70465', 'examples_per_second': '32.398', 'grad_norm': '188', 'counters/examples': 269536, 'counters/updates': 8423}
train stats after 269568 examples: {'rewards_train/chosen': '0.15234', 'rewards_train/rejected': '0.093732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05861', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-167.04', 'loss/train': '0.70917', 'examples_per_second': '30.131', 'grad_norm': '86', 'counters/examples': 269568, 'counters/updates': 8424}
train stats after 269600 examples: {'rewards_train/chosen': '0.085968', 'rewards_train/rejected': '0.052853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033115', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-148.31', 'loss/train': '0.68807', 'examples_per_second': '31.27', 'grad_norm': '155', 'counters/examples': 269600, 'counters/updates': 8425}
skipping logging after 269632 examples to avoid logging too frequently
train stats after 269664 examples: {'rewards_train/chosen': '0.13493', 'rewards_train/rejected': '0.077944', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056982', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-154.98', 'loss/train': '0.67798', 'examples_per_second': '31.613', 'grad_norm': '95', 'counters/examples': 269664, 'counters/updates': 8427}
train stats after 269696 examples: {'rewards_train/chosen': '0.13393', 'rewards_train/rejected': '0.075079', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.058852', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-149.86', 'loss/train': '0.68887', 'examples_per_second': '31.635', 'grad_norm': '113', 'counters/examples': 269696, 'counters/updates': 8428}
train stats after 269728 examples: {'rewards_train/chosen': '0.082111', 'rewards_train/rejected': '0.14144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.059333', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-121.53', 'loss/train': '0.73908', 'examples_per_second': '32.052', 'grad_norm': '396', 'counters/examples': 269728, 'counters/updates': 8429}
train stats after 269760 examples: {'rewards_train/chosen': '0.22471', 'rewards_train/rejected': '0.062253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16245', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-155.01', 'loss/train': '0.63716', 'examples_per_second': '30.601', 'grad_norm': '73.5', 'counters/examples': 269760, 'counters/updates': 8430}
train stats after 269792 examples: {'rewards_train/chosen': '0.10057', 'rewards_train/rejected': '0.086686', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013887', 'logps_train/rejected': '-135.18', 'logps_train/chosen': '-139.15', 'loss/train': '0.69805', 'examples_per_second': '32.745', 'grad_norm': '55.25', 'counters/examples': 269792, 'counters/updates': 8431}
skipping logging after 269824 examples to avoid logging too frequently
train stats after 269856 examples: {'rewards_train/chosen': '0.075528', 'rewards_train/rejected': '-0.03205', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10758', 'logps_train/rejected': '-138.71', 'logps_train/chosen': '-155.17', 'loss/train': '0.65318', 'examples_per_second': '31.632', 'grad_norm': '75', 'counters/examples': 269856, 'counters/updates': 8433}
train stats after 269888 examples: {'rewards_train/chosen': '0.15473', 'rewards_train/rejected': '0.15461', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00012443', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-151.22', 'loss/train': '0.70901', 'examples_per_second': '30.851', 'grad_norm': '159', 'counters/examples': 269888, 'counters/updates': 8434}
train stats after 269920 examples: {'rewards_train/chosen': '0.15668', 'rewards_train/rejected': '0.10499', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051692', 'logps_train/rejected': '-143.19', 'logps_train/chosen': '-121.53', 'loss/train': '0.6822', 'examples_per_second': '31.009', 'grad_norm': '86.5', 'counters/examples': 269920, 'counters/updates': 8435}
train stats after 269952 examples: {'rewards_train/chosen': '0.30995', 'rewards_train/rejected': '0.01179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29816', 'logps_train/rejected': '-159.52', 'logps_train/chosen': '-157.45', 'loss/train': '0.62488', 'examples_per_second': '30.692', 'grad_norm': '62', 'counters/examples': 269952, 'counters/updates': 8436}
train stats after 269984 examples: {'rewards_train/chosen': '0.17886', 'rewards_train/rejected': '-0.028815', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20767', 'logps_train/rejected': '-114.03', 'logps_train/chosen': '-181.73', 'loss/train': '0.61367', 'examples_per_second': '32.709', 'grad_norm': '85', 'counters/examples': 269984, 'counters/updates': 8437}
train stats after 270016 examples: {'rewards_train/chosen': '0.10738', 'rewards_train/rejected': '0.12001', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012624', 'logps_train/rejected': '-139.04', 'logps_train/chosen': '-181.01', 'loss/train': '0.72901', 'examples_per_second': '32.641', 'grad_norm': '98', 'counters/examples': 270016, 'counters/updates': 8438}
skipping logging after 270048 examples to avoid logging too frequently
train stats after 270080 examples: {'rewards_train/chosen': '0.12518', 'rewards_train/rejected': '0.0071934', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11799', 'logps_train/rejected': '-105.99', 'logps_train/chosen': '-174.99', 'loss/train': '0.65752', 'examples_per_second': '31.324', 'grad_norm': '63.75', 'counters/examples': 270080, 'counters/updates': 8440}
train stats after 270112 examples: {'rewards_train/chosen': '0.15732', 'rewards_train/rejected': '0.015454', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14186', 'logps_train/rejected': '-130.12', 'logps_train/chosen': '-167.23', 'loss/train': '0.63899', 'examples_per_second': '30.995', 'grad_norm': '80', 'counters/examples': 270112, 'counters/updates': 8441}
train stats after 270144 examples: {'rewards_train/chosen': '0.13727', 'rewards_train/rejected': '0.032324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10495', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-123.61', 'loss/train': '0.66151', 'examples_per_second': '30.609', 'grad_norm': '50', 'counters/examples': 270144, 'counters/updates': 8442}
train stats after 270176 examples: {'rewards_train/chosen': '0.1902', 'rewards_train/rejected': '0.11154', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078659', 'logps_train/rejected': '-128.62', 'logps_train/chosen': '-183.27', 'loss/train': '0.66908', 'examples_per_second': '31.426', 'grad_norm': '290', 'counters/examples': 270176, 'counters/updates': 8443}
train stats after 270208 examples: {'rewards_train/chosen': '0.11147', 'rewards_train/rejected': '0.02593', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085536', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-150.63', 'loss/train': '0.66611', 'examples_per_second': '32.154', 'grad_norm': '81', 'counters/examples': 270208, 'counters/updates': 8444}
train stats after 270240 examples: {'rewards_train/chosen': '0.21189', 'rewards_train/rejected': '0.085607', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12628', 'logps_train/rejected': '-131.01', 'logps_train/chosen': '-179.56', 'loss/train': '0.65125', 'examples_per_second': '31.784', 'grad_norm': '85', 'counters/examples': 270240, 'counters/updates': 8445}
train stats after 270272 examples: {'rewards_train/chosen': '0.20833', 'rewards_train/rejected': '-0.027164', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2355', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-158.77', 'loss/train': '0.61275', 'examples_per_second': '32.558', 'grad_norm': '47.75', 'counters/examples': 270272, 'counters/updates': 8446}
train stats after 270304 examples: {'rewards_train/chosen': '0.25528', 'rewards_train/rejected': '0.056217', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19906', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-183.75', 'loss/train': '0.6197', 'examples_per_second': '31.676', 'grad_norm': '62.25', 'counters/examples': 270304, 'counters/updates': 8447}
train stats after 270336 examples: {'rewards_train/chosen': '0.1638', 'rewards_train/rejected': '0.091655', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072145', 'logps_train/rejected': '-127.75', 'logps_train/chosen': '-144.38', 'loss/train': '0.68139', 'examples_per_second': '32.552', 'grad_norm': '70.5', 'counters/examples': 270336, 'counters/updates': 8448}
train stats after 270368 examples: {'rewards_train/chosen': '0.096992', 'rewards_train/rejected': '0.082769', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014223', 'logps_train/rejected': '-102.6', 'logps_train/chosen': '-143.82', 'loss/train': '0.70521', 'examples_per_second': '32.409', 'grad_norm': '94', 'counters/examples': 270368, 'counters/updates': 8449}
skipping logging after 270400 examples to avoid logging too frequently
train stats after 270432 examples: {'rewards_train/chosen': '0.10065', 'rewards_train/rejected': '0.038492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062158', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-146.29', 'loss/train': '0.67461', 'examples_per_second': '32.491', 'grad_norm': '81', 'counters/examples': 270432, 'counters/updates': 8451}
train stats after 270464 examples: {'rewards_train/chosen': '0.19817', 'rewards_train/rejected': '0.092253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10592', 'logps_train/rejected': '-160.25', 'logps_train/chosen': '-169.97', 'loss/train': '0.65365', 'examples_per_second': '30.706', 'grad_norm': '117', 'counters/examples': 270464, 'counters/updates': 8452}
skipping logging after 270496 examples to avoid logging too frequently
train stats after 270528 examples: {'rewards_train/chosen': '0.16168', 'rewards_train/rejected': '0.082518', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.079161', 'logps_train/rejected': '-166.17', 'logps_train/chosen': '-156.83', 'loss/train': '0.6812', 'examples_per_second': '30.151', 'grad_norm': '66', 'counters/examples': 270528, 'counters/updates': 8454}
skipping logging after 270560 examples to avoid logging too frequently
train stats after 270592 examples: {'rewards_train/chosen': '0.1353', 'rewards_train/rejected': '0.028107', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10719', 'logps_train/rejected': '-155.88', 'logps_train/chosen': '-174.67', 'loss/train': '0.66012', 'examples_per_second': '31.655', 'grad_norm': '167', 'counters/examples': 270592, 'counters/updates': 8456}
train stats after 270624 examples: {'rewards_train/chosen': '0.11781', 'rewards_train/rejected': '0.09554', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022275', 'logps_train/rejected': '-163.73', 'logps_train/chosen': '-136.88', 'loss/train': '0.6911', 'examples_per_second': '32.164', 'grad_norm': '72.5', 'counters/examples': 270624, 'counters/updates': 8457}
train stats after 270656 examples: {'rewards_train/chosen': '0.18036', 'rewards_train/rejected': '0.2368', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.056448', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-173.63', 'loss/train': '0.73773', 'examples_per_second': '30.831', 'grad_norm': '102', 'counters/examples': 270656, 'counters/updates': 8458}
train stats after 270688 examples: {'rewards_train/chosen': '0.14538', 'rewards_train/rejected': '0.1055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039883', 'logps_train/rejected': '-157.47', 'logps_train/chosen': '-155.45', 'loss/train': '0.692', 'examples_per_second': '31.64', 'grad_norm': '103.5', 'counters/examples': 270688, 'counters/updates': 8459}
train stats after 270720 examples: {'rewards_train/chosen': '0.097814', 'rewards_train/rejected': '0.00091406', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0969', 'logps_train/rejected': '-105.72', 'logps_train/chosen': '-127.5', 'loss/train': '0.67088', 'examples_per_second': '31.423', 'grad_norm': '80.5', 'counters/examples': 270720, 'counters/updates': 8460}
train stats after 270752 examples: {'rewards_train/chosen': '0.084023', 'rewards_train/rejected': '0.06608', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017943', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-123.2', 'loss/train': '0.69618', 'examples_per_second': '30.444', 'grad_norm': '169', 'counters/examples': 270752, 'counters/updates': 8461}
train stats after 270784 examples: {'rewards_train/chosen': '0.10981', 'rewards_train/rejected': '0.076995', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03282', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-125.2', 'loss/train': '0.69167', 'examples_per_second': '31.048', 'grad_norm': '59.5', 'counters/examples': 270784, 'counters/updates': 8462}
skipping logging after 270816 examples to avoid logging too frequently
train stats after 270848 examples: {'rewards_train/chosen': '0.13666', 'rewards_train/rejected': '0.062071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074594', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-120.5', 'loss/train': '0.67188', 'examples_per_second': '36.303', 'grad_norm': '49', 'counters/examples': 270848, 'counters/updates': 8464}
train stats after 270880 examples: {'rewards_train/chosen': '0.10547', 'rewards_train/rejected': '0.033469', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-141.77', 'loss/train': '0.67087', 'examples_per_second': '32.705', 'grad_norm': '87.5', 'counters/examples': 270880, 'counters/updates': 8465}
skipping logging after 270912 examples to avoid logging too frequently
train stats after 270944 examples: {'rewards_train/chosen': '0.16944', 'rewards_train/rejected': '0.10112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068324', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-128.97', 'loss/train': '0.67467', 'examples_per_second': '31.658', 'grad_norm': '82', 'counters/examples': 270944, 'counters/updates': 8467}
skipping logging after 270976 examples to avoid logging too frequently
train stats after 271008 examples: {'rewards_train/chosen': '0.05905', 'rewards_train/rejected': '0.026357', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032694', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-116.67', 'loss/train': '0.69028', 'examples_per_second': '30.659', 'grad_norm': '78', 'counters/examples': 271008, 'counters/updates': 8469}
skipping logging after 271040 examples to avoid logging too frequently
train stats after 271072 examples: {'rewards_train/chosen': '0.12258', 'rewards_train/rejected': '0.082247', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04033', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-155.21', 'loss/train': '0.69354', 'examples_per_second': '36.259', 'grad_norm': '360', 'counters/examples': 271072, 'counters/updates': 8471}
train stats after 271104 examples: {'rewards_train/chosen': '0.082609', 'rewards_train/rejected': '0.085458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0028489', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-152.29', 'loss/train': '0.7088', 'examples_per_second': '31.678', 'grad_norm': '122.5', 'counters/examples': 271104, 'counters/updates': 8472}
train stats after 271136 examples: {'rewards_train/chosen': '0.12709', 'rewards_train/rejected': '0.033313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093775', 'logps_train/rejected': '-107.62', 'logps_train/chosen': '-109.55', 'loss/train': '0.66031', 'examples_per_second': '30.341', 'grad_norm': '83', 'counters/examples': 271136, 'counters/updates': 8473}
train stats after 271168 examples: {'rewards_train/chosen': '0.11588', 'rewards_train/rejected': '0.077837', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.038041', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-129.71', 'loss/train': '0.69183', 'examples_per_second': '31.527', 'grad_norm': '110', 'counters/examples': 271168, 'counters/updates': 8474}
train stats after 271200 examples: {'rewards_train/chosen': '0.18189', 'rewards_train/rejected': '0.0013514', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.18054', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-140.34', 'loss/train': '0.61633', 'examples_per_second': '30.685', 'grad_norm': '75', 'counters/examples': 271200, 'counters/updates': 8475}
train stats after 271232 examples: {'rewards_train/chosen': '0.13441', 'rewards_train/rejected': '0.013152', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12126', 'logps_train/rejected': '-159.83', 'logps_train/chosen': '-149.71', 'loss/train': '0.6599', 'examples_per_second': '31.423', 'grad_norm': '130', 'counters/examples': 271232, 'counters/updates': 8476}
train stats after 271264 examples: {'rewards_train/chosen': '0.14926', 'rewards_train/rejected': '0.041977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10728', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-169.43', 'loss/train': '0.65602', 'examples_per_second': '30.788', 'grad_norm': '83.5', 'counters/examples': 271264, 'counters/updates': 8477}
train stats after 271296 examples: {'rewards_train/chosen': '0.10047', 'rewards_train/rejected': '0.066647', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033824', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-151.47', 'loss/train': '0.70633', 'examples_per_second': '31.808', 'grad_norm': '84', 'counters/examples': 271296, 'counters/updates': 8478}
train stats after 271328 examples: {'rewards_train/chosen': '0.18102', 'rewards_train/rejected': '0.096541', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084484', 'logps_train/rejected': '-145.57', 'logps_train/chosen': '-158.87', 'loss/train': '0.67485', 'examples_per_second': '31.636', 'grad_norm': '174', 'counters/examples': 271328, 'counters/updates': 8479}
train stats after 271360 examples: {'rewards_train/chosen': '0.23002', 'rewards_train/rejected': '0.073326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15669', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-164.38', 'loss/train': '0.65084', 'examples_per_second': '31.77', 'grad_norm': '90', 'counters/examples': 271360, 'counters/updates': 8480}
train stats after 271392 examples: {'rewards_train/chosen': '0.19571', 'rewards_train/rejected': '0.042562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15315', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-154.59', 'loss/train': '0.63027', 'examples_per_second': '30.412', 'grad_norm': '137', 'counters/examples': 271392, 'counters/updates': 8481}
train stats after 271424 examples: {'rewards_train/chosen': '0.076541', 'rewards_train/rejected': '0.012042', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064499', 'logps_train/rejected': '-130.81', 'logps_train/chosen': '-117.1', 'loss/train': '0.67733', 'examples_per_second': '30.982', 'grad_norm': '63.25', 'counters/examples': 271424, 'counters/updates': 8482}
train stats after 271456 examples: {'rewards_train/chosen': '0.20052', 'rewards_train/rejected': '0.13978', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060744', 'logps_train/rejected': '-137', 'logps_train/chosen': '-196.2', 'loss/train': '0.69809', 'examples_per_second': '31.322', 'grad_norm': '170', 'counters/examples': 271456, 'counters/updates': 8483}
skipping logging after 271488 examples to avoid logging too frequently
train stats after 271520 examples: {'rewards_train/chosen': '0.10189', 'rewards_train/rejected': '0.085968', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015919', 'logps_train/rejected': '-166.18', 'logps_train/chosen': '-126.87', 'loss/train': '0.70005', 'examples_per_second': '30.61', 'grad_norm': '70', 'counters/examples': 271520, 'counters/updates': 8485}
skipping logging after 271552 examples to avoid logging too frequently
train stats after 271584 examples: {'rewards_train/chosen': '0.11107', 'rewards_train/rejected': '0.12287', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011807', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-179.73', 'loss/train': '0.71318', 'examples_per_second': '30.609', 'grad_norm': '102.5', 'counters/examples': 271584, 'counters/updates': 8487}
skipping logging after 271616 examples to avoid logging too frequently
train stats after 271648 examples: {'rewards_train/chosen': '0.21619', 'rewards_train/rejected': '0.12757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088619', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-160.03', 'loss/train': '0.66125', 'examples_per_second': '31.301', 'grad_norm': '150', 'counters/examples': 271648, 'counters/updates': 8489}
train stats after 271680 examples: {'rewards_train/chosen': '0.16908', 'rewards_train/rejected': '0.099926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06915', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-130.38', 'loss/train': '0.67499', 'examples_per_second': '30.459', 'grad_norm': '65', 'counters/examples': 271680, 'counters/updates': 8490}
train stats after 271712 examples: {'rewards_train/chosen': '0.068036', 'rewards_train/rejected': '-0.0204', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088436', 'logps_train/rejected': '-99.278', 'logps_train/chosen': '-128.52', 'loss/train': '0.6621', 'examples_per_second': '32.541', 'grad_norm': '49', 'counters/examples': 271712, 'counters/updates': 8491}
train stats after 271744 examples: {'rewards_train/chosen': '0.12506', 'rewards_train/rejected': '0.059967', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065094', 'logps_train/rejected': '-151.43', 'logps_train/chosen': '-157.7', 'loss/train': '0.67141', 'examples_per_second': '31.653', 'grad_norm': '98', 'counters/examples': 271744, 'counters/updates': 8492}
train stats after 271776 examples: {'rewards_train/chosen': '0.18996', 'rewards_train/rejected': '0.15214', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037826', 'logps_train/rejected': '-188.97', 'logps_train/chosen': '-206.18', 'loss/train': '0.71172', 'examples_per_second': '30.269', 'grad_norm': '151', 'counters/examples': 271776, 'counters/updates': 8493}
train stats after 271808 examples: {'rewards_train/chosen': '0.18596', 'rewards_train/rejected': '0.10466', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.081302', 'logps_train/rejected': '-125.88', 'logps_train/chosen': '-145.12', 'loss/train': '0.67162', 'examples_per_second': '31.666', 'grad_norm': '80', 'counters/examples': 271808, 'counters/updates': 8494}
train stats after 271840 examples: {'rewards_train/chosen': '0.076121', 'rewards_train/rejected': '0.09452', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018399', 'logps_train/rejected': '-128.65', 'logps_train/chosen': '-144.6', 'loss/train': '0.7103', 'examples_per_second': '31.65', 'grad_norm': '116', 'counters/examples': 271840, 'counters/updates': 8495}
train stats after 271872 examples: {'rewards_train/chosen': '0.25227', 'rewards_train/rejected': '0.060808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19147', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-170.5', 'loss/train': '0.61939', 'examples_per_second': '31.608', 'grad_norm': '79.5', 'counters/examples': 271872, 'counters/updates': 8496}
train stats after 271904 examples: {'rewards_train/chosen': '0.068866', 'rewards_train/rejected': '0.078832', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0099659', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-118.8', 'loss/train': '0.70973', 'examples_per_second': '31.949', 'grad_norm': '54', 'counters/examples': 271904, 'counters/updates': 8497}
skipping logging after 271936 examples to avoid logging too frequently
train stats after 271968 examples: {'rewards_train/chosen': '0.18157', 'rewards_train/rejected': '0.015223', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16634', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-148.11', 'loss/train': '0.64506', 'examples_per_second': '33.168', 'grad_norm': '149', 'counters/examples': 271968, 'counters/updates': 8499}
train stats after 272000 examples: {'rewards_train/chosen': '0.019226', 'rewards_train/rejected': '-0.020657', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.039883', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-115.55', 'loss/train': '0.68428', 'examples_per_second': '31.637', 'grad_norm': '88.5', 'counters/examples': 272000, 'counters/updates': 8500}
train stats after 272032 examples: {'rewards_train/chosen': '0.15096', 'rewards_train/rejected': '0.028979', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12198', 'logps_train/rejected': '-134.68', 'logps_train/chosen': '-150.71', 'loss/train': '0.65491', 'examples_per_second': '32.549', 'grad_norm': '80', 'counters/examples': 272032, 'counters/updates': 8501}
train stats after 272064 examples: {'rewards_train/chosen': '0.087565', 'rewards_train/rejected': '-0.050569', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13813', 'logps_train/rejected': '-105.16', 'logps_train/chosen': '-122.41', 'loss/train': '0.63596', 'examples_per_second': '32.919', 'grad_norm': '59.5', 'counters/examples': 272064, 'counters/updates': 8502}
train stats after 272096 examples: {'rewards_train/chosen': '0.14801', 'rewards_train/rejected': '0.0072207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14079', 'logps_train/rejected': '-165.84', 'logps_train/chosen': '-198.57', 'loss/train': '0.65409', 'examples_per_second': '29.785', 'grad_norm': '140', 'counters/examples': 272096, 'counters/updates': 8503}
skipping logging after 272128 examples to avoid logging too frequently
train stats after 272160 examples: {'rewards_train/chosen': '0.090217', 'rewards_train/rejected': '0.11765', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027433', 'logps_train/rejected': '-161.89', 'logps_train/chosen': '-166.43', 'loss/train': '0.719', 'examples_per_second': '30.795', 'grad_norm': '102', 'counters/examples': 272160, 'counters/updates': 8505}
train stats after 272192 examples: {'rewards_train/chosen': '0.18106', 'rewards_train/rejected': '-0.026577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20763', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-146.41', 'loss/train': '0.61118', 'examples_per_second': '31.631', 'grad_norm': '85.5', 'counters/examples': 272192, 'counters/updates': 8506}
skipping logging after 272224 examples to avoid logging too frequently
train stats after 272256 examples: {'rewards_train/chosen': '0.17034', 'rewards_train/rejected': '0.098496', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071846', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-170.75', 'loss/train': '0.6877', 'examples_per_second': '31.79', 'grad_norm': '81.5', 'counters/examples': 272256, 'counters/updates': 8508}
train stats after 272288 examples: {'rewards_train/chosen': '0.062254', 'rewards_train/rejected': '-0.016697', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078951', 'logps_train/rejected': '-87.544', 'logps_train/chosen': '-111.46', 'loss/train': '0.66068', 'examples_per_second': '30.656', 'grad_norm': '75', 'counters/examples': 272288, 'counters/updates': 8509}
train stats after 272320 examples: {'rewards_train/chosen': '0.088979', 'rewards_train/rejected': '0.028309', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06067', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-140.12', 'loss/train': '0.6931', 'examples_per_second': '31.401', 'grad_norm': '80.5', 'counters/examples': 272320, 'counters/updates': 8510}
train stats after 272352 examples: {'rewards_train/chosen': '0.16205', 'rewards_train/rejected': '0.047433', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11462', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-158.68', 'loss/train': '0.66471', 'examples_per_second': '31.663', 'grad_norm': '69', 'counters/examples': 272352, 'counters/updates': 8511}
train stats after 272384 examples: {'rewards_train/chosen': '0.19827', 'rewards_train/rejected': '0.069645', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12863', 'logps_train/rejected': '-159.04', 'logps_train/chosen': '-183.09', 'loss/train': '0.65159', 'examples_per_second': '31.524', 'grad_norm': '106.5', 'counters/examples': 272384, 'counters/updates': 8512}
train stats after 272416 examples: {'rewards_train/chosen': '0.18313', 'rewards_train/rejected': '0.11243', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070705', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-158.97', 'loss/train': '0.67221', 'examples_per_second': '31.62', 'grad_norm': '63.5', 'counters/examples': 272416, 'counters/updates': 8513}
train stats after 272448 examples: {'rewards_train/chosen': '0.1692', 'rewards_train/rejected': '0.03345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13575', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-211.05', 'loss/train': '0.6522', 'examples_per_second': '30.132', 'grad_norm': '101', 'counters/examples': 272448, 'counters/updates': 8514}
train stats after 272480 examples: {'rewards_train/chosen': '0.13088', 'rewards_train/rejected': '0.058306', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072576', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-152.07', 'loss/train': '0.67508', 'examples_per_second': '32.176', 'grad_norm': '70', 'counters/examples': 272480, 'counters/updates': 8515}
train stats after 272512 examples: {'rewards_train/chosen': '-0.00043647', 'rewards_train/rejected': '-8.0998e-06', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00042837', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-120.38', 'loss/train': '0.7043', 'examples_per_second': '30.207', 'grad_norm': '105', 'counters/examples': 272512, 'counters/updates': 8516}
train stats after 272544 examples: {'rewards_train/chosen': '0.12685', 'rewards_train/rejected': '0.023961', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10288', 'logps_train/rejected': '-152.46', 'logps_train/chosen': '-168.44', 'loss/train': '0.66378', 'examples_per_second': '31.548', 'grad_norm': '112', 'counters/examples': 272544, 'counters/updates': 8517}
skipping logging after 272576 examples to avoid logging too frequently
train stats after 272608 examples: {'rewards_train/chosen': '0.06383', 'rewards_train/rejected': '0.048101', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015729', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-123.45', 'loss/train': '0.69278', 'examples_per_second': '33.32', 'grad_norm': '130', 'counters/examples': 272608, 'counters/updates': 8519}
skipping logging after 272640 examples to avoid logging too frequently
train stats after 272672 examples: {'rewards_train/chosen': '0.09273', 'rewards_train/rejected': '0.10368', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010952', 'logps_train/rejected': '-119.65', 'logps_train/chosen': '-162.34', 'loss/train': '0.71521', 'examples_per_second': '37.359', 'grad_norm': '106', 'counters/examples': 272672, 'counters/updates': 8521}
train stats after 272704 examples: {'rewards_train/chosen': '0.059776', 'rewards_train/rejected': '0.050911', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088656', 'logps_train/rejected': '-136.1', 'logps_train/chosen': '-143', 'loss/train': '0.6987', 'examples_per_second': '32.326', 'grad_norm': '70', 'counters/examples': 272704, 'counters/updates': 8522}
skipping logging after 272736 examples to avoid logging too frequently
train stats after 272768 examples: {'rewards_train/chosen': '0.10195', 'rewards_train/rejected': '-0.033645', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1356', 'logps_train/rejected': '-88.767', 'logps_train/chosen': '-152.29', 'loss/train': '0.63868', 'examples_per_second': '31.339', 'grad_norm': '162', 'counters/examples': 272768, 'counters/updates': 8524}
train stats after 272800 examples: {'rewards_train/chosen': '0.2328', 'rewards_train/rejected': '0.071431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16137', 'logps_train/rejected': '-116.36', 'logps_train/chosen': '-147.3', 'loss/train': '0.62959', 'examples_per_second': '31.663', 'grad_norm': '101.5', 'counters/examples': 272800, 'counters/updates': 8525}
train stats after 272832 examples: {'rewards_train/chosen': '0.19715', 'rewards_train/rejected': '0.21204', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.01489', 'logps_train/rejected': '-148.47', 'logps_train/chosen': '-139.58', 'loss/train': '0.72919', 'examples_per_second': '31.597', 'grad_norm': '122', 'counters/examples': 272832, 'counters/updates': 8526}
train stats after 272864 examples: {'rewards_train/chosen': '0.12578', 'rewards_train/rejected': '0.078643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047142', 'logps_train/rejected': '-113.86', 'logps_train/chosen': '-153.46', 'loss/train': '0.69142', 'examples_per_second': '30.255', 'grad_norm': '91.5', 'counters/examples': 272864, 'counters/updates': 8527}
train stats after 272896 examples: {'rewards_train/chosen': '0.13858', 'rewards_train/rejected': '0.046497', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092079', 'logps_train/rejected': '-102.19', 'logps_train/chosen': '-116.07', 'loss/train': '0.66036', 'examples_per_second': '33.054', 'grad_norm': '88.5', 'counters/examples': 272896, 'counters/updates': 8528}
train stats after 272928 examples: {'rewards_train/chosen': '0.073507', 'rewards_train/rejected': '0.019834', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053674', 'logps_train/rejected': '-128.24', 'logps_train/chosen': '-152.38', 'loss/train': '0.67992', 'examples_per_second': '32.039', 'grad_norm': '60', 'counters/examples': 272928, 'counters/updates': 8529}
skipping logging after 272960 examples to avoid logging too frequently
train stats after 272992 examples: {'rewards_train/chosen': '0.081128', 'rewards_train/rejected': '0.10991', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.028784', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-108.52', 'loss/train': '0.7196', 'examples_per_second': '31.776', 'grad_norm': '71.5', 'counters/examples': 272992, 'counters/updates': 8531}
skipping logging after 273024 examples to avoid logging too frequently
train stats after 273056 examples: {'rewards_train/chosen': '0.13217', 'rewards_train/rejected': '0.12255', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0096187', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-145.8', 'loss/train': '0.70317', 'examples_per_second': '30.355', 'grad_norm': '86.5', 'counters/examples': 273056, 'counters/updates': 8533}
train stats after 273088 examples: {'rewards_train/chosen': '0.17491', 'rewards_train/rejected': '-0.044372', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21928', 'logps_train/rejected': '-120.28', 'logps_train/chosen': '-189.14', 'loss/train': '0.61157', 'examples_per_second': '32.706', 'grad_norm': '54.75', 'counters/examples': 273088, 'counters/updates': 8534}
train stats after 273120 examples: {'rewards_train/chosen': '0.11407', 'rewards_train/rejected': '0.028345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085726', 'logps_train/rejected': '-126.9', 'logps_train/chosen': '-144.71', 'loss/train': '0.66398', 'examples_per_second': '32.745', 'grad_norm': '255', 'counters/examples': 273120, 'counters/updates': 8535}
train stats after 273152 examples: {'rewards_train/chosen': '0.16096', 'rewards_train/rejected': '0.1074', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053559', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-160.49', 'loss/train': '0.6852', 'examples_per_second': '32.079', 'grad_norm': '90.5', 'counters/examples': 273152, 'counters/updates': 8536}
skipping logging after 273184 examples to avoid logging too frequently
train stats after 273216 examples: {'rewards_train/chosen': '0.2103', 'rewards_train/rejected': '0.069077', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14123', 'logps_train/rejected': '-141.54', 'logps_train/chosen': '-153.85', 'loss/train': '0.65232', 'examples_per_second': '32.461', 'grad_norm': '63.5', 'counters/examples': 273216, 'counters/updates': 8538}
train stats after 273248 examples: {'rewards_train/chosen': '0.095374', 'rewards_train/rejected': '0.073144', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02223', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-105.03', 'loss/train': '0.69108', 'examples_per_second': '30.784', 'grad_norm': '68.5', 'counters/examples': 273248, 'counters/updates': 8539}
train stats after 273280 examples: {'rewards_train/chosen': '0.10813', 'rewards_train/rejected': '0.035858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072273', 'logps_train/rejected': '-119.7', 'logps_train/chosen': '-153.7', 'loss/train': '0.67187', 'examples_per_second': '32.205', 'grad_norm': '65.5', 'counters/examples': 273280, 'counters/updates': 8540}
train stats after 273312 examples: {'rewards_train/chosen': '0.075962', 'rewards_train/rejected': '0.10355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027589', 'logps_train/rejected': '-130.75', 'logps_train/chosen': '-165.6', 'loss/train': '0.72493', 'examples_per_second': '29.942', 'grad_norm': '116.5', 'counters/examples': 273312, 'counters/updates': 8541}
train stats after 273344 examples: {'rewards_train/chosen': '0.2341', 'rewards_train/rejected': '0.14806', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086039', 'logps_train/rejected': '-159.96', 'logps_train/chosen': '-165.52', 'loss/train': '0.68031', 'examples_per_second': '31.642', 'grad_norm': '205', 'counters/examples': 273344, 'counters/updates': 8542}
train stats after 273376 examples: {'rewards_train/chosen': '0.098373', 'rewards_train/rejected': '0.028331', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070041', 'logps_train/rejected': '-95.321', 'logps_train/chosen': '-99.322', 'loss/train': '0.66761', 'examples_per_second': '31.559', 'grad_norm': '48.5', 'counters/examples': 273376, 'counters/updates': 8543}
train stats after 273408 examples: {'rewards_train/chosen': '0.17574', 'rewards_train/rejected': '0.035252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14049', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-141.07', 'loss/train': '0.64542', 'examples_per_second': '30.66', 'grad_norm': '73', 'counters/examples': 273408, 'counters/updates': 8544}
skipping logging after 273440 examples to avoid logging too frequently
train stats after 273472 examples: {'rewards_train/chosen': '0.16708', 'rewards_train/rejected': '0.065376', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1017', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-170.62', 'loss/train': '0.67689', 'examples_per_second': '27.149', 'grad_norm': '71.5', 'counters/examples': 273472, 'counters/updates': 8546}
train stats after 273504 examples: {'rewards_train/chosen': '0.13757', 'rewards_train/rejected': '0.038345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099226', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-135.82', 'loss/train': '0.65886', 'examples_per_second': '31.003', 'grad_norm': '75', 'counters/examples': 273504, 'counters/updates': 8547}
train stats after 273536 examples: {'rewards_train/chosen': '0.20217', 'rewards_train/rejected': '0.12067', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081498', 'logps_train/rejected': '-165.31', 'logps_train/chosen': '-145.94', 'loss/train': '0.66409', 'examples_per_second': '32.601', 'grad_norm': '159', 'counters/examples': 273536, 'counters/updates': 8548}
train stats after 273568 examples: {'rewards_train/chosen': '0.20858', 'rewards_train/rejected': '0.14109', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.067482', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-189.25', 'loss/train': '0.67034', 'examples_per_second': '25.947', 'grad_norm': '76', 'counters/examples': 273568, 'counters/updates': 8549}
train stats after 273600 examples: {'rewards_train/chosen': '0.101', 'rewards_train/rejected': '0.090004', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010995', 'logps_train/rejected': '-115.74', 'logps_train/chosen': '-135.02', 'loss/train': '0.70532', 'examples_per_second': '31.534', 'grad_norm': '65.5', 'counters/examples': 273600, 'counters/updates': 8550}
train stats after 273632 examples: {'rewards_train/chosen': '0.11533', 'rewards_train/rejected': '0.14151', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026179', 'logps_train/rejected': '-161.49', 'logps_train/chosen': '-130.67', 'loss/train': '0.7198', 'examples_per_second': '31.516', 'grad_norm': '73.5', 'counters/examples': 273632, 'counters/updates': 8551}
train stats after 273664 examples: {'rewards_train/chosen': '0.15008', 'rewards_train/rejected': '0.037644', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11244', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-131.62', 'loss/train': '0.65158', 'examples_per_second': '31.99', 'grad_norm': '74', 'counters/examples': 273664, 'counters/updates': 8552}
train stats after 273696 examples: {'rewards_train/chosen': '0.11093', 'rewards_train/rejected': '0.046305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064624', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-141.77', 'loss/train': '0.67203', 'examples_per_second': '31.042', 'grad_norm': '231', 'counters/examples': 273696, 'counters/updates': 8553}
skipping logging after 273728 examples to avoid logging too frequently
train stats after 273760 examples: {'rewards_train/chosen': '0.14253', 'rewards_train/rejected': '0.0076142', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13492', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-132.66', 'loss/train': '0.64533', 'examples_per_second': '32.577', 'grad_norm': '53.75', 'counters/examples': 273760, 'counters/updates': 8555}
train stats after 273792 examples: {'rewards_train/chosen': '0.12214', 'rewards_train/rejected': '0.030828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091313', 'logps_train/rejected': '-159.99', 'logps_train/chosen': '-154.65', 'loss/train': '0.66517', 'examples_per_second': '30.751', 'grad_norm': '84', 'counters/examples': 273792, 'counters/updates': 8556}
skipping logging after 273824 examples to avoid logging too frequently
train stats after 273856 examples: {'rewards_train/chosen': '0.2679', 'rewards_train/rejected': '0.048327', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21958', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-180', 'loss/train': '0.61225', 'examples_per_second': '39.439', 'grad_norm': '191', 'counters/examples': 273856, 'counters/updates': 8558}
train stats after 273888 examples: {'rewards_train/chosen': '0.25045', 'rewards_train/rejected': '0.16278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087665', 'logps_train/rejected': '-138.31', 'logps_train/chosen': '-141.01', 'loss/train': '0.69022', 'examples_per_second': '30.205', 'grad_norm': '132', 'counters/examples': 273888, 'counters/updates': 8559}
train stats after 273920 examples: {'rewards_train/chosen': '-0.013396', 'rewards_train/rejected': '-0.025273', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011878', 'logps_train/rejected': '-157.37', 'logps_train/chosen': '-152.9', 'loss/train': '0.69807', 'examples_per_second': '31.658', 'grad_norm': '74', 'counters/examples': 273920, 'counters/updates': 8560}
train stats after 273952 examples: {'rewards_train/chosen': '0.061017', 'rewards_train/rejected': '0.10738', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.046367', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-146.39', 'loss/train': '0.72557', 'examples_per_second': '31.6', 'grad_norm': '90', 'counters/examples': 273952, 'counters/updates': 8561}
train stats after 273984 examples: {'rewards_train/chosen': '0.1402', 'rewards_train/rejected': '0.097614', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042583', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-126.04', 'loss/train': '0.69076', 'examples_per_second': '30.774', 'grad_norm': '95', 'counters/examples': 273984, 'counters/updates': 8562}
train stats after 274016 examples: {'rewards_train/chosen': '0.1421', 'rewards_train/rejected': '0.026661', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11544', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-136.88', 'loss/train': '0.65628', 'examples_per_second': '30.303', 'grad_norm': '167', 'counters/examples': 274016, 'counters/updates': 8563}
skipping logging after 274048 examples to avoid logging too frequently
train stats after 274080 examples: {'rewards_train/chosen': '0.10353', 'rewards_train/rejected': '0.011095', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092432', 'logps_train/rejected': '-117.07', 'logps_train/chosen': '-152.54', 'loss/train': '0.65217', 'examples_per_second': '32.983', 'grad_norm': '56.75', 'counters/examples': 274080, 'counters/updates': 8565}
train stats after 274112 examples: {'rewards_train/chosen': '0.086418', 'rewards_train/rejected': '0.067199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01922', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-169.85', 'loss/train': '0.70806', 'examples_per_second': '30.24', 'grad_norm': '115', 'counters/examples': 274112, 'counters/updates': 8566}
train stats after 274144 examples: {'rewards_train/chosen': '0.15667', 'rewards_train/rejected': '0.14807', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0086027', 'logps_train/rejected': '-178.79', 'logps_train/chosen': '-172.76', 'loss/train': '0.69676', 'examples_per_second': '31.648', 'grad_norm': '52.25', 'counters/examples': 274144, 'counters/updates': 8567}
train stats after 274176 examples: {'rewards_train/chosen': '0.13796', 'rewards_train/rejected': '0.053754', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084207', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-155.15', 'loss/train': '0.67904', 'examples_per_second': '33.132', 'grad_norm': '49.25', 'counters/examples': 274176, 'counters/updates': 8568}
skipping logging after 274208 examples to avoid logging too frequently
train stats after 274240 examples: {'rewards_train/chosen': '0.11073', 'rewards_train/rejected': '0.07193', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038802', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-136.47', 'loss/train': '0.6901', 'examples_per_second': '31.597', 'grad_norm': '147', 'counters/examples': 274240, 'counters/updates': 8570}
train stats after 274272 examples: {'rewards_train/chosen': '0.068141', 'rewards_train/rejected': '0.055105', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013036', 'logps_train/rejected': '-139.03', 'logps_train/chosen': '-161.61', 'loss/train': '0.69576', 'examples_per_second': '31.707', 'grad_norm': '73', 'counters/examples': 274272, 'counters/updates': 8571}
skipping logging after 274304 examples to avoid logging too frequently
train stats after 274336 examples: {'rewards_train/chosen': '0.083744', 'rewards_train/rejected': '0.023779', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059966', 'logps_train/rejected': '-116.15', 'logps_train/chosen': '-145.2', 'loss/train': '0.67193', 'examples_per_second': '31.98', 'grad_norm': '65.5', 'counters/examples': 274336, 'counters/updates': 8573}
train stats after 274368 examples: {'rewards_train/chosen': '0.23927', 'rewards_train/rejected': '0.04878', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19049', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-141.71', 'loss/train': '0.63133', 'examples_per_second': '31.853', 'grad_norm': '52.5', 'counters/examples': 274368, 'counters/updates': 8574}
skipping logging after 274400 examples to avoid logging too frequently
train stats after 274432 examples: {'rewards_train/chosen': '0.10676', 'rewards_train/rejected': '0.14261', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.03585', 'logps_train/rejected': '-110.98', 'logps_train/chosen': '-131.2', 'loss/train': '0.72634', 'examples_per_second': '31.679', 'grad_norm': '90', 'counters/examples': 274432, 'counters/updates': 8576}
train stats after 274464 examples: {'rewards_train/chosen': '0.1533', 'rewards_train/rejected': '0.07955', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073753', 'logps_train/rejected': '-137.91', 'logps_train/chosen': '-153.53', 'loss/train': '0.69195', 'examples_per_second': '31.443', 'grad_norm': '148', 'counters/examples': 274464, 'counters/updates': 8577}
train stats after 274496 examples: {'rewards_train/chosen': '0.14776', 'rewards_train/rejected': '0.015354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1324', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-131.08', 'loss/train': '0.64259', 'examples_per_second': '31.822', 'grad_norm': '52.75', 'counters/examples': 274496, 'counters/updates': 8578}
train stats after 274528 examples: {'rewards_train/chosen': '0.14738', 'rewards_train/rejected': '-0.028414', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17579', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-151.12', 'loss/train': '0.62006', 'examples_per_second': '30.771', 'grad_norm': '57.25', 'counters/examples': 274528, 'counters/updates': 8579}
train stats after 274560 examples: {'rewards_train/chosen': '0.16096', 'rewards_train/rejected': '0.10925', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051708', 'logps_train/rejected': '-169.54', 'logps_train/chosen': '-166.69', 'loss/train': '0.69466', 'examples_per_second': '31.392', 'grad_norm': '80.5', 'counters/examples': 274560, 'counters/updates': 8580}
skipping logging after 274592 examples to avoid logging too frequently
train stats after 274624 examples: {'rewards_train/chosen': '0.069106', 'rewards_train/rejected': '-0.010753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079859', 'logps_train/rejected': '-84.577', 'logps_train/chosen': '-112.09', 'loss/train': '0.66158', 'examples_per_second': '34.373', 'grad_norm': '61', 'counters/examples': 274624, 'counters/updates': 8582}
train stats after 274656 examples: {'rewards_train/chosen': '0.046846', 'rewards_train/rejected': '0.087247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.040401', 'logps_train/rejected': '-165.75', 'logps_train/chosen': '-180.68', 'loss/train': '0.72927', 'examples_per_second': '25.945', 'grad_norm': '160', 'counters/examples': 274656, 'counters/updates': 8583}
train stats after 274688 examples: {'rewards_train/chosen': '0.12813', 'rewards_train/rejected': '0.16829', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040152', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-148.09', 'loss/train': '0.73221', 'examples_per_second': '31.426', 'grad_norm': '108', 'counters/examples': 274688, 'counters/updates': 8584}
train stats after 274720 examples: {'rewards_train/chosen': '0.086747', 'rewards_train/rejected': '0.083396', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0033512', 'logps_train/rejected': '-172.48', 'logps_train/chosen': '-142.66', 'loss/train': '0.70502', 'examples_per_second': '31.015', 'grad_norm': '93', 'counters/examples': 274720, 'counters/updates': 8585}
train stats after 274752 examples: {'rewards_train/chosen': '0.041736', 'rewards_train/rejected': '5.5213e-05', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04168', 'logps_train/rejected': '-104.34', 'logps_train/chosen': '-168.22', 'loss/train': '0.68454', 'examples_per_second': '31.633', 'grad_norm': '112.5', 'counters/examples': 274752, 'counters/updates': 8586}
train stats after 274784 examples: {'rewards_train/chosen': '0.16116', 'rewards_train/rejected': '0.047004', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11415', 'logps_train/rejected': '-126.37', 'logps_train/chosen': '-158.28', 'loss/train': '0.65492', 'examples_per_second': '30.874', 'grad_norm': '93', 'counters/examples': 274784, 'counters/updates': 8587}
train stats after 274816 examples: {'rewards_train/chosen': '0.23993', 'rewards_train/rejected': '0.13808', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10185', 'logps_train/rejected': '-150.41', 'logps_train/chosen': '-153.01', 'loss/train': '0.66013', 'examples_per_second': '30.961', 'grad_norm': '122.5', 'counters/examples': 274816, 'counters/updates': 8588}
skipping logging after 274848 examples to avoid logging too frequently
train stats after 274880 examples: {'rewards_train/chosen': '0.16108', 'rewards_train/rejected': '0.11218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048892', 'logps_train/rejected': '-154.55', 'logps_train/chosen': '-149.4', 'loss/train': '0.68871', 'examples_per_second': '31.485', 'grad_norm': '68.5', 'counters/examples': 274880, 'counters/updates': 8590}
train stats after 274912 examples: {'rewards_train/chosen': '0.16132', 'rewards_train/rejected': '0.093624', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.067698', 'logps_train/rejected': '-144.3', 'logps_train/chosen': '-167.57', 'loss/train': '0.6721', 'examples_per_second': '31.592', 'grad_norm': '99', 'counters/examples': 274912, 'counters/updates': 8591}
train stats after 274944 examples: {'rewards_train/chosen': '0.19412', 'rewards_train/rejected': '0.0225', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17162', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-114.32', 'loss/train': '0.6228', 'examples_per_second': '29.871', 'grad_norm': '79', 'counters/examples': 274944, 'counters/updates': 8592}
skipping logging after 274976 examples to avoid logging too frequently
train stats after 275008 examples: {'rewards_train/chosen': '0.18132', 'rewards_train/rejected': '0.026411', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1549', 'logps_train/rejected': '-122.27', 'logps_train/chosen': '-142.41', 'loss/train': '0.6418', 'examples_per_second': '30.321', 'grad_norm': '61', 'counters/examples': 275008, 'counters/updates': 8594}
skipping logging after 275040 examples to avoid logging too frequently
train stats after 275072 examples: {'rewards_train/chosen': '0.21743', 'rewards_train/rejected': '0.033834', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18359', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-144.59', 'loss/train': '0.62252', 'examples_per_second': '30.57', 'grad_norm': '76', 'counters/examples': 275072, 'counters/updates': 8596}
train stats after 275104 examples: {'rewards_train/chosen': '0.0030602', 'rewards_train/rejected': '0.062123', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059062', 'logps_train/rejected': '-157.51', 'logps_train/chosen': '-193.43', 'loss/train': '0.7582', 'examples_per_second': '31.543', 'grad_norm': '332', 'counters/examples': 275104, 'counters/updates': 8597}
train stats after 275136 examples: {'rewards_train/chosen': '0.22727', 'rewards_train/rejected': '0.16892', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058352', 'logps_train/rejected': '-112', 'logps_train/chosen': '-147.52', 'loss/train': '0.68746', 'examples_per_second': '31.361', 'grad_norm': '106', 'counters/examples': 275136, 'counters/updates': 8598}
train stats after 275168 examples: {'rewards_train/chosen': '0.14986', 'rewards_train/rejected': '0.082019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067845', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-122.15', 'loss/train': '0.67448', 'examples_per_second': '32.743', 'grad_norm': '52.25', 'counters/examples': 275168, 'counters/updates': 8599}
skipping logging after 275200 examples to avoid logging too frequently
train stats after 275232 examples: {'rewards_train/chosen': '0.22532', 'rewards_train/rejected': '0.026297', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19903', 'logps_train/rejected': '-120.32', 'logps_train/chosen': '-145.05', 'loss/train': '0.61155', 'examples_per_second': '31.47', 'grad_norm': '77.5', 'counters/examples': 275232, 'counters/updates': 8601}
train stats after 275264 examples: {'rewards_train/chosen': '0.1358', 'rewards_train/rejected': '0.14417', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0083602', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-135.91', 'loss/train': '0.71728', 'examples_per_second': '31.114', 'grad_norm': '72', 'counters/examples': 275264, 'counters/updates': 8602}
train stats after 275296 examples: {'rewards_train/chosen': '0.16528', 'rewards_train/rejected': '0.086505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078779', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-149.27', 'loss/train': '0.67528', 'examples_per_second': '31.344', 'grad_norm': '95', 'counters/examples': 275296, 'counters/updates': 8603}
train stats after 275328 examples: {'rewards_train/chosen': '0.044829', 'rewards_train/rejected': '0.056054', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.011225', 'logps_train/rejected': '-136.14', 'logps_train/chosen': '-106.26', 'loss/train': '0.71734', 'examples_per_second': '30.94', 'grad_norm': '94', 'counters/examples': 275328, 'counters/updates': 8604}
train stats after 275360 examples: {'rewards_train/chosen': '0.12825', 'rewards_train/rejected': '0.10006', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028193', 'logps_train/rejected': '-147.32', 'logps_train/chosen': '-175.37', 'loss/train': '0.69635', 'examples_per_second': '31.505', 'grad_norm': '84', 'counters/examples': 275360, 'counters/updates': 8605}
train stats after 275392 examples: {'rewards_train/chosen': '0.17024', 'rewards_train/rejected': '0.025295', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14494', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-143.84', 'loss/train': '0.6399', 'examples_per_second': '32.345', 'grad_norm': '55.75', 'counters/examples': 275392, 'counters/updates': 8606}
train stats after 275424 examples: {'rewards_train/chosen': '0.10915', 'rewards_train/rejected': '0.10071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0084369', 'logps_train/rejected': '-121.89', 'logps_train/chosen': '-145.38', 'loss/train': '0.69874', 'examples_per_second': '32.223', 'grad_norm': '127', 'counters/examples': 275424, 'counters/updates': 8607}
train stats after 275456 examples: {'rewards_train/chosen': '0.21006', 'rewards_train/rejected': '0.17241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037648', 'logps_train/rejected': '-150.37', 'logps_train/chosen': '-147.79', 'loss/train': '0.69143', 'examples_per_second': '31.632', 'grad_norm': '176', 'counters/examples': 275456, 'counters/updates': 8608}
train stats after 275488 examples: {'rewards_train/chosen': '0.14163', 'rewards_train/rejected': '0.094246', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047385', 'logps_train/rejected': '-120.39', 'logps_train/chosen': '-138.67', 'loss/train': '0.68364', 'examples_per_second': '30.172', 'grad_norm': '148', 'counters/examples': 275488, 'counters/updates': 8609}
train stats after 275520 examples: {'rewards_train/chosen': '0.10268', 'rewards_train/rejected': '0.083255', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.019421', 'logps_train/rejected': '-120.39', 'logps_train/chosen': '-172.79', 'loss/train': '0.70673', 'examples_per_second': '30.992', 'grad_norm': '73', 'counters/examples': 275520, 'counters/updates': 8610}
skipping logging after 275552 examples to avoid logging too frequently
train stats after 275584 examples: {'rewards_train/chosen': '0.17901', 'rewards_train/rejected': '0.07874', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10027', 'logps_train/rejected': '-149.82', 'logps_train/chosen': '-170.78', 'loss/train': '0.66026', 'examples_per_second': '33.018', 'grad_norm': '72', 'counters/examples': 275584, 'counters/updates': 8612}
train stats after 275616 examples: {'rewards_train/chosen': '0.13618', 'rewards_train/rejected': '0.054126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082057', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-159.8', 'loss/train': '0.66918', 'examples_per_second': '31.381', 'grad_norm': '54.5', 'counters/examples': 275616, 'counters/updates': 8613}
train stats after 275648 examples: {'rewards_train/chosen': '0.07427', 'rewards_train/rejected': '0.020251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054019', 'logps_train/rejected': '-133.39', 'logps_train/chosen': '-107.48', 'loss/train': '0.67737', 'examples_per_second': '31.833', 'grad_norm': '52.25', 'counters/examples': 275648, 'counters/updates': 8614}
skipping logging after 275680 examples to avoid logging too frequently
train stats after 275712 examples: {'rewards_train/chosen': '0.10428', 'rewards_train/rejected': '-0.0028783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10716', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-132.6', 'loss/train': '0.65699', 'examples_per_second': '31.391', 'grad_norm': '352', 'counters/examples': 275712, 'counters/updates': 8616}
train stats after 275744 examples: {'rewards_train/chosen': '0.11033', 'rewards_train/rejected': '0.14241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.032073', 'logps_train/rejected': '-156.78', 'logps_train/chosen': '-189.74', 'loss/train': '0.72721', 'examples_per_second': '33.116', 'grad_norm': '99', 'counters/examples': 275744, 'counters/updates': 8617}
train stats after 275776 examples: {'rewards_train/chosen': '0.16784', 'rewards_train/rejected': '-0.049131', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21697', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-160.31', 'loss/train': '0.60278', 'examples_per_second': '30.703', 'grad_norm': '87.5', 'counters/examples': 275776, 'counters/updates': 8618}
train stats after 275808 examples: {'rewards_train/chosen': '0.10761', 'rewards_train/rejected': '0.075082', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032528', 'logps_train/rejected': '-138.41', 'logps_train/chosen': '-157.51', 'loss/train': '0.68767', 'examples_per_second': '31.482', 'grad_norm': '124', 'counters/examples': 275808, 'counters/updates': 8619}
train stats after 275840 examples: {'rewards_train/chosen': '0.069738', 'rewards_train/rejected': '0.0015859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068152', 'logps_train/rejected': '-162.41', 'logps_train/chosen': '-135.69', 'loss/train': '0.68648', 'examples_per_second': '31.244', 'grad_norm': '64.5', 'counters/examples': 275840, 'counters/updates': 8620}
train stats after 275872 examples: {'rewards_train/chosen': '0.25735', 'rewards_train/rejected': '0.1544', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10295', 'logps_train/rejected': '-151.15', 'logps_train/chosen': '-179.95', 'loss/train': '0.65673', 'examples_per_second': '31.168', 'grad_norm': '99', 'counters/examples': 275872, 'counters/updates': 8621}
train stats after 275904 examples: {'rewards_train/chosen': '0.10175', 'rewards_train/rejected': '0.069841', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03191', 'logps_train/rejected': '-104.62', 'logps_train/chosen': '-91.052', 'loss/train': '0.68957', 'examples_per_second': '31.979', 'grad_norm': '110.5', 'counters/examples': 275904, 'counters/updates': 8622}
skipping logging after 275936 examples to avoid logging too frequently
train stats after 275968 examples: {'rewards_train/chosen': '0.25484', 'rewards_train/rejected': '0.057256', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19758', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-162.75', 'loss/train': '0.64869', 'examples_per_second': '31.652', 'grad_norm': '157', 'counters/examples': 275968, 'counters/updates': 8624}
train stats after 276000 examples: {'rewards_train/chosen': '0.20424', 'rewards_train/rejected': '0.09956', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10468', 'logps_train/rejected': '-141.4', 'logps_train/chosen': '-124.02', 'loss/train': '0.6691', 'examples_per_second': '32.323', 'grad_norm': '107', 'counters/examples': 276000, 'counters/updates': 8625}
Running evaluation after 276000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.96it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.93it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.93it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.94it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]
eval after 276000: {'rewards_eval/chosen': '0.17071', 'rewards_eval/rejected': '0.057402', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.1133', 'logps_eval/rejected': '-127.57', 'logps_eval/chosen': '-149.3', 'loss/eval': '0.657'}
skipping save for non epoch
train stats after 276032 examples: {'rewards_train/chosen': '0.18796', 'rewards_train/rejected': '0.087156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10081', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-150.22', 'loss/train': '0.66462', 'examples_per_second': '33.383', 'grad_norm': '67', 'counters/examples': 276032, 'counters/updates': 8626}
skipping logging after 276064 examples to avoid logging too frequently
train stats after 276096 examples: {'rewards_train/chosen': '0.22546', 'rewards_train/rejected': '0.28638', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06092', 'logps_train/rejected': '-158.37', 'logps_train/chosen': '-154.35', 'loss/train': '0.74135', 'examples_per_second': '34.725', 'grad_norm': '116', 'counters/examples': 276096, 'counters/updates': 8628}
train stats after 276128 examples: {'rewards_train/chosen': '0.074361', 'rewards_train/rejected': '0.077113', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027512', 'logps_train/rejected': '-118.06', 'logps_train/chosen': '-140.56', 'loss/train': '0.70758', 'examples_per_second': '30.252', 'grad_norm': '77', 'counters/examples': 276128, 'counters/updates': 8629}
train stats after 276160 examples: {'rewards_train/chosen': '0.12515', 'rewards_train/rejected': '0.031332', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093821', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-138.07', 'loss/train': '0.65977', 'examples_per_second': '31.644', 'grad_norm': '56', 'counters/examples': 276160, 'counters/updates': 8630}
skipping logging after 276192 examples to avoid logging too frequently
train stats after 276224 examples: {'rewards_train/chosen': '0.093657', 'rewards_train/rejected': '0.10289', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.009234', 'logps_train/rejected': '-154.16', 'logps_train/chosen': '-136.28', 'loss/train': '0.71372', 'examples_per_second': '31.308', 'grad_norm': '490', 'counters/examples': 276224, 'counters/updates': 8632}
train stats after 276256 examples: {'rewards_train/chosen': '0.12103', 'rewards_train/rejected': '0.059213', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061817', 'logps_train/rejected': '-148.25', 'logps_train/chosen': '-200.2', 'loss/train': '0.6833', 'examples_per_second': '31.369', 'grad_norm': '186', 'counters/examples': 276256, 'counters/updates': 8633}
train stats after 276288 examples: {'rewards_train/chosen': '0.14752', 'rewards_train/rejected': '0.079025', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068497', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-160.97', 'loss/train': '0.68303', 'examples_per_second': '32.171', 'grad_norm': '165', 'counters/examples': 276288, 'counters/updates': 8634}
train stats after 276320 examples: {'rewards_train/chosen': '0.097531', 'rewards_train/rejected': '0.13801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040483', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-167.62', 'loss/train': '0.7285', 'examples_per_second': '30.884', 'grad_norm': '89.5', 'counters/examples': 276320, 'counters/updates': 8635}
train stats after 276352 examples: {'rewards_train/chosen': '0.16685', 'rewards_train/rejected': '0.088367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078487', 'logps_train/rejected': '-165.62', 'logps_train/chosen': '-164.87', 'loss/train': '0.66508', 'examples_per_second': '31.385', 'grad_norm': '109.5', 'counters/examples': 276352, 'counters/updates': 8636}
train stats after 276384 examples: {'rewards_train/chosen': '0.079094', 'rewards_train/rejected': '0.09359', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014496', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-126.32', 'loss/train': '0.70896', 'examples_per_second': '30.845', 'grad_norm': '68', 'counters/examples': 276384, 'counters/updates': 8637}
skipping logging after 276416 examples to avoid logging too frequently
train stats after 276448 examples: {'rewards_train/chosen': '0.10697', 'rewards_train/rejected': '0.13801', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.031044', 'logps_train/rejected': '-166.09', 'logps_train/chosen': '-176.54', 'loss/train': '0.72843', 'examples_per_second': '31.652', 'grad_norm': '145', 'counters/examples': 276448, 'counters/updates': 8639}
skipping logging after 276480 examples to avoid logging too frequently
train stats after 276512 examples: {'rewards_train/chosen': '0.14675', 'rewards_train/rejected': '0.0012994', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14545', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-136.01', 'loss/train': '0.63446', 'examples_per_second': '34.044', 'grad_norm': '60.75', 'counters/examples': 276512, 'counters/updates': 8641}
train stats after 276544 examples: {'rewards_train/chosen': '0.052627', 'rewards_train/rejected': '0.0054937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047133', 'logps_train/rejected': '-116.6', 'logps_train/chosen': '-124.01', 'loss/train': '0.68214', 'examples_per_second': '32.586', 'grad_norm': '81.5', 'counters/examples': 276544, 'counters/updates': 8642}
train stats after 276576 examples: {'rewards_train/chosen': '0.12889', 'rewards_train/rejected': '-0.03125', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16014', 'logps_train/rejected': '-118.16', 'logps_train/chosen': '-175.52', 'loss/train': '0.63694', 'examples_per_second': '31.667', 'grad_norm': '70', 'counters/examples': 276576, 'counters/updates': 8643}
train stats after 276608 examples: {'rewards_train/chosen': '0.2876', 'rewards_train/rejected': '0.17679', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11081', 'logps_train/rejected': '-115.79', 'logps_train/chosen': '-162.7', 'loss/train': '0.66443', 'examples_per_second': '31.814', 'grad_norm': '78.5', 'counters/examples': 276608, 'counters/updates': 8644}
train stats after 276640 examples: {'rewards_train/chosen': '0.12868', 'rewards_train/rejected': '0.1215', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0071776', 'logps_train/rejected': '-112.46', 'logps_train/chosen': '-150.11', 'loss/train': '0.70504', 'examples_per_second': '30.251', 'grad_norm': '244', 'counters/examples': 276640, 'counters/updates': 8645}
train stats after 276672 examples: {'rewards_train/chosen': '0.11169', 'rewards_train/rejected': '0.060145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051549', 'logps_train/rejected': '-171.29', 'logps_train/chosen': '-143.86', 'loss/train': '0.67892', 'examples_per_second': '31.67', 'grad_norm': '60.5', 'counters/examples': 276672, 'counters/updates': 8646}
train stats after 276704 examples: {'rewards_train/chosen': '0.033633', 'rewards_train/rejected': '0.088063', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.05443', 'logps_train/rejected': '-118.5', 'logps_train/chosen': '-151.18', 'loss/train': '0.7322', 'examples_per_second': '30.191', 'grad_norm': '138', 'counters/examples': 276704, 'counters/updates': 8647}
skipping logging after 276736 examples to avoid logging too frequently
train stats after 276768 examples: {'rewards_train/chosen': '0.23264', 'rewards_train/rejected': '0.15196', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08068', 'logps_train/rejected': '-116.69', 'logps_train/chosen': '-136.55', 'loss/train': '0.66776', 'examples_per_second': '33.339', 'grad_norm': '94.5', 'counters/examples': 276768, 'counters/updates': 8649}
train stats after 276800 examples: {'rewards_train/chosen': '0.076553', 'rewards_train/rejected': '0.0071386', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069414', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-175.08', 'loss/train': '0.67097', 'examples_per_second': '30.016', 'grad_norm': '77', 'counters/examples': 276800, 'counters/updates': 8650}
train stats after 276832 examples: {'rewards_train/chosen': '0.17448', 'rewards_train/rejected': '0.079501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094982', 'logps_train/rejected': '-115.62', 'logps_train/chosen': '-102.29', 'loss/train': '0.67146', 'examples_per_second': '31.827', 'grad_norm': '69.5', 'counters/examples': 276832, 'counters/updates': 8651}
train stats after 276864 examples: {'rewards_train/chosen': '0.035261', 'rewards_train/rejected': '0.012572', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022689', 'logps_train/rejected': '-105.34', 'logps_train/chosen': '-139.6', 'loss/train': '0.69242', 'examples_per_second': '31.553', 'grad_norm': '121.5', 'counters/examples': 276864, 'counters/updates': 8652}
train stats after 276896 examples: {'rewards_train/chosen': '0.12518', 'rewards_train/rejected': '0.083032', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042146', 'logps_train/rejected': '-168.51', 'logps_train/chosen': '-127.46', 'loss/train': '0.68282', 'examples_per_second': '31.569', 'grad_norm': '77.5', 'counters/examples': 276896, 'counters/updates': 8653}
train stats after 276928 examples: {'rewards_train/chosen': '0.090024', 'rewards_train/rejected': '0.030022', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.060002', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-168.6', 'loss/train': '0.70456', 'examples_per_second': '32.644', 'grad_norm': '79.5', 'counters/examples': 276928, 'counters/updates': 8654}
train stats after 276960 examples: {'rewards_train/chosen': '0.10639', 'rewards_train/rejected': '0.087853', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018538', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-116.89', 'loss/train': '0.70088', 'examples_per_second': '31.825', 'grad_norm': '48.5', 'counters/examples': 276960, 'counters/updates': 8655}
train stats after 276992 examples: {'rewards_train/chosen': '0.19062', 'rewards_train/rejected': '-0.004919', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19554', 'logps_train/rejected': '-103.67', 'logps_train/chosen': '-149.67', 'loss/train': '0.60867', 'examples_per_second': '31.662', 'grad_norm': '55', 'counters/examples': 276992, 'counters/updates': 8656}
skipping logging after 277024 examples to avoid logging too frequently
train stats after 277056 examples: {'rewards_train/chosen': '0.18803', 'rewards_train/rejected': '0.13505', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.052975', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-160.28', 'loss/train': '0.69852', 'examples_per_second': '32.749', 'grad_norm': '284', 'counters/examples': 277056, 'counters/updates': 8658}
train stats after 277088 examples: {'rewards_train/chosen': '0.093775', 'rewards_train/rejected': '0.058144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035631', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-131.8', 'loss/train': '0.68945', 'examples_per_second': '32.436', 'grad_norm': '64.5', 'counters/examples': 277088, 'counters/updates': 8659}
train stats after 277120 examples: {'rewards_train/chosen': '0.19869', 'rewards_train/rejected': '0.042959', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15573', 'logps_train/rejected': '-191.65', 'logps_train/chosen': '-156.71', 'loss/train': '0.63341', 'examples_per_second': '31.322', 'grad_norm': '79', 'counters/examples': 277120, 'counters/updates': 8660}
train stats after 277152 examples: {'rewards_train/chosen': '0.10664', 'rewards_train/rejected': '0.030181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076456', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-140.35', 'loss/train': '0.68433', 'examples_per_second': '33.128', 'grad_norm': '78.5', 'counters/examples': 277152, 'counters/updates': 8661}
skipping logging after 277184 examples to avoid logging too frequently
train stats after 277216 examples: {'rewards_train/chosen': '0.068356', 'rewards_train/rejected': '0.092639', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024282', 'logps_train/rejected': '-131.38', 'logps_train/chosen': '-131.85', 'loss/train': '0.72068', 'examples_per_second': '34.37', 'grad_norm': '69.5', 'counters/examples': 277216, 'counters/updates': 8663}
skipping logging after 277248 examples to avoid logging too frequently
train stats after 277280 examples: {'rewards_train/chosen': '0.086129', 'rewards_train/rejected': '0.0069519', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079177', 'logps_train/rejected': '-158.93', 'logps_train/chosen': '-181.69', 'loss/train': '0.67767', 'examples_per_second': '31.597', 'grad_norm': '125', 'counters/examples': 277280, 'counters/updates': 8665}
train stats after 277312 examples: {'rewards_train/chosen': '0.076344', 'rewards_train/rejected': '0.10506', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.028717', 'logps_train/rejected': '-153.28', 'logps_train/chosen': '-134.58', 'loss/train': '0.71867', 'examples_per_second': '30.822', 'grad_norm': '63.75', 'counters/examples': 277312, 'counters/updates': 8666}
train stats after 277344 examples: {'rewards_train/chosen': '0.16521', 'rewards_train/rejected': '0.11534', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049864', 'logps_train/rejected': '-147.76', 'logps_train/chosen': '-150.4', 'loss/train': '0.6874', 'examples_per_second': '31.044', 'grad_norm': '89.5', 'counters/examples': 277344, 'counters/updates': 8667}
train stats after 277376 examples: {'rewards_train/chosen': '0.17555', 'rewards_train/rejected': '0.036388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13916', 'logps_train/rejected': '-121.85', 'logps_train/chosen': '-145.11', 'loss/train': '0.63882', 'examples_per_second': '31.85', 'grad_norm': '100', 'counters/examples': 277376, 'counters/updates': 8668}
train stats after 277408 examples: {'rewards_train/chosen': '0.11357', 'rewards_train/rejected': '-0.017913', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13148', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-176.66', 'loss/train': '0.64438', 'examples_per_second': '31.976', 'grad_norm': '57.5', 'counters/examples': 277408, 'counters/updates': 8669}
train stats after 277440 examples: {'rewards_train/chosen': '0.15818', 'rewards_train/rejected': '0.10628', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051899', 'logps_train/rejected': '-156.29', 'logps_train/chosen': '-152.76', 'loss/train': '0.67753', 'examples_per_second': '32.352', 'grad_norm': '63.75', 'counters/examples': 277440, 'counters/updates': 8670}
train stats after 277472 examples: {'rewards_train/chosen': '0.095833', 'rewards_train/rejected': '0.089614', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '0.0062182', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-134.5', 'loss/train': '0.69985', 'examples_per_second': '30.872', 'grad_norm': '83', 'counters/examples': 277472, 'counters/updates': 8671}
train stats after 277504 examples: {'rewards_train/chosen': '0.17119', 'rewards_train/rejected': '0.11126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059923', 'logps_train/rejected': '-166.31', 'logps_train/chosen': '-156.26', 'loss/train': '0.68952', 'examples_per_second': '30.409', 'grad_norm': '107', 'counters/examples': 277504, 'counters/updates': 8672}
train stats after 277536 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.042915', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057097', 'logps_train/rejected': '-129.04', 'logps_train/chosen': '-109.31', 'loss/train': '0.67216', 'examples_per_second': '32.334', 'grad_norm': '51', 'counters/examples': 277536, 'counters/updates': 8673}
train stats after 277568 examples: {'rewards_train/chosen': '0.15797', 'rewards_train/rejected': '0.030728', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12724', 'logps_train/rejected': '-156.29', 'logps_train/chosen': '-138.44', 'loss/train': '0.64141', 'examples_per_second': '30.131', 'grad_norm': '103', 'counters/examples': 277568, 'counters/updates': 8674}
train stats after 277600 examples: {'rewards_train/chosen': '0.19412', 'rewards_train/rejected': '0.017485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17664', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-141.55', 'loss/train': '0.62293', 'examples_per_second': '31.657', 'grad_norm': '90.5', 'counters/examples': 277600, 'counters/updates': 8675}
skipping logging after 277632 examples to avoid logging too frequently
train stats after 277664 examples: {'rewards_train/chosen': '0.084103', 'rewards_train/rejected': '-0.037477', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12158', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-118.56', 'loss/train': '0.64974', 'examples_per_second': '30.771', 'grad_norm': '65.5', 'counters/examples': 277664, 'counters/updates': 8677}
train stats after 277696 examples: {'rewards_train/chosen': '0.17779', 'rewards_train/rejected': '0.11726', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060532', 'logps_train/rejected': '-122.68', 'logps_train/chosen': '-141.28', 'loss/train': '0.67401', 'examples_per_second': '32.968', 'grad_norm': '83.5', 'counters/examples': 277696, 'counters/updates': 8678}
skipping logging after 277728 examples to avoid logging too frequently
train stats after 277760 examples: {'rewards_train/chosen': '0.12291', 'rewards_train/rejected': '0.044519', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078387', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-178.05', 'loss/train': '0.66598', 'examples_per_second': '33.911', 'grad_norm': '57.25', 'counters/examples': 277760, 'counters/updates': 8680}
train stats after 277792 examples: {'rewards_train/chosen': '0.10468', 'rewards_train/rejected': '0.11135', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0066639', 'logps_train/rejected': '-173.86', 'logps_train/chosen': '-174.9', 'loss/train': '0.70858', 'examples_per_second': '31.643', 'grad_norm': '156', 'counters/examples': 277792, 'counters/updates': 8681}
train stats after 277824 examples: {'rewards_train/chosen': '0.13028', 'rewards_train/rejected': '-0.0014326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13171', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-147.95', 'loss/train': '0.64986', 'examples_per_second': '31.308', 'grad_norm': '60.75', 'counters/examples': 277824, 'counters/updates': 8682}
train stats after 277856 examples: {'rewards_train/chosen': '0.17315', 'rewards_train/rejected': '0.0049688', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16818', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-168.31', 'loss/train': '0.63028', 'examples_per_second': '31.611', 'grad_norm': '67', 'counters/examples': 277856, 'counters/updates': 8683}
train stats after 277888 examples: {'rewards_train/chosen': '0.13985', 'rewards_train/rejected': '0.1128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027052', 'logps_train/rejected': '-139.96', 'logps_train/chosen': '-150.67', 'loss/train': '0.69264', 'examples_per_second': '31.037', 'grad_norm': '141', 'counters/examples': 277888, 'counters/updates': 8684}
train stats after 277920 examples: {'rewards_train/chosen': '0.22308', 'rewards_train/rejected': '0.0964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12668', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-168.86', 'loss/train': '0.65679', 'examples_per_second': '31.533', 'grad_norm': '104.5', 'counters/examples': 277920, 'counters/updates': 8685}
train stats after 277952 examples: {'rewards_train/chosen': '0.1383', 'rewards_train/rejected': '0.044364', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093936', 'logps_train/rejected': '-93.05', 'logps_train/chosen': '-122.99', 'loss/train': '0.66137', 'examples_per_second': '31.568', 'grad_norm': '322', 'counters/examples': 277952, 'counters/updates': 8686}
train stats after 277984 examples: {'rewards_train/chosen': '0.064247', 'rewards_train/rejected': '0.035926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028321', 'logps_train/rejected': '-101.98', 'logps_train/chosen': '-141.94', 'loss/train': '0.68825', 'examples_per_second': '30.581', 'grad_norm': '67', 'counters/examples': 277984, 'counters/updates': 8687}
train stats after 278016 examples: {'rewards_train/chosen': '0.16819', 'rewards_train/rejected': '0.038581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12961', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-165.01', 'loss/train': '0.66543', 'examples_per_second': '31.674', 'grad_norm': '79.5', 'counters/examples': 278016, 'counters/updates': 8688}
train stats after 278048 examples: {'rewards_train/chosen': '0.036497', 'rewards_train/rejected': '0.031717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0047798', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-138.72', 'loss/train': '0.70224', 'examples_per_second': '31.668', 'grad_norm': '135', 'counters/examples': 278048, 'counters/updates': 8689}
skipping logging after 278080 examples to avoid logging too frequently
train stats after 278112 examples: {'rewards_train/chosen': '0.22742', 'rewards_train/rejected': '0.0099898', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21743', 'logps_train/rejected': '-121.59', 'logps_train/chosen': '-174.56', 'loss/train': '0.605', 'examples_per_second': '31.974', 'grad_norm': '68', 'counters/examples': 278112, 'counters/updates': 8691}
train stats after 278144 examples: {'rewards_train/chosen': '0.14385', 'rewards_train/rejected': '0.046521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097334', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-146.41', 'loss/train': '0.66951', 'examples_per_second': '31.65', 'grad_norm': '64', 'counters/examples': 278144, 'counters/updates': 8692}
train stats after 278176 examples: {'rewards_train/chosen': '0.1037', 'rewards_train/rejected': '0.058328', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045375', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-137.73', 'loss/train': '0.67944', 'examples_per_second': '32.685', 'grad_norm': '103', 'counters/examples': 278176, 'counters/updates': 8693}
train stats after 278208 examples: {'rewards_train/chosen': '0.1308', 'rewards_train/rejected': '0.071531', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059269', 'logps_train/rejected': '-158.25', 'logps_train/chosen': '-182.39', 'loss/train': '0.69166', 'examples_per_second': '30.257', 'grad_norm': '96.5', 'counters/examples': 278208, 'counters/updates': 8694}
train stats after 278240 examples: {'rewards_train/chosen': '0.18117', 'rewards_train/rejected': '0.13491', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046258', 'logps_train/rejected': '-143.36', 'logps_train/chosen': '-151.76', 'loss/train': '0.67963', 'examples_per_second': '31.146', 'grad_norm': '69', 'counters/examples': 278240, 'counters/updates': 8695}
train stats after 278272 examples: {'rewards_train/chosen': '0.060897', 'rewards_train/rejected': '0.049904', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010993', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-138.09', 'loss/train': '0.7086', 'examples_per_second': '31.622', 'grad_norm': '93.5', 'counters/examples': 278272, 'counters/updates': 8696}
train stats after 278304 examples: {'rewards_train/chosen': '0.15102', 'rewards_train/rejected': '0.092714', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.058306', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-166.88', 'loss/train': '0.6771', 'examples_per_second': '31.023', 'grad_norm': '80', 'counters/examples': 278304, 'counters/updates': 8697}
train stats after 278336 examples: {'rewards_train/chosen': '0.14927', 'rewards_train/rejected': '-0.014216', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16348', 'logps_train/rejected': '-111.77', 'logps_train/chosen': '-152.83', 'loss/train': '0.62735', 'examples_per_second': '32.13', 'grad_norm': '77', 'counters/examples': 278336, 'counters/updates': 8698}
train stats after 278368 examples: {'rewards_train/chosen': '0.20365', 'rewards_train/rejected': '0.14938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054272', 'logps_train/rejected': '-140.32', 'logps_train/chosen': '-158.85', 'loss/train': '0.70299', 'examples_per_second': '31.638', 'grad_norm': '70', 'counters/examples': 278368, 'counters/updates': 8699}
train stats after 278400 examples: {'rewards_train/chosen': '0.20234', 'rewards_train/rejected': '0.055345', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.147', 'logps_train/rejected': '-152.36', 'logps_train/chosen': '-162.34', 'loss/train': '0.64144', 'examples_per_second': '31.659', 'grad_norm': '74', 'counters/examples': 278400, 'counters/updates': 8700}
train stats after 278432 examples: {'rewards_train/chosen': '0.19889', 'rewards_train/rejected': '0.041071', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15782', 'logps_train/rejected': '-115.68', 'logps_train/chosen': '-153.6', 'loss/train': '0.63425', 'examples_per_second': '31.786', 'grad_norm': '66.5', 'counters/examples': 278432, 'counters/updates': 8701}
train stats after 278464 examples: {'rewards_train/chosen': '0.11149', 'rewards_train/rejected': '0.087407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024082', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-170.96', 'loss/train': '0.70137', 'examples_per_second': '30.318', 'grad_norm': '66.5', 'counters/examples': 278464, 'counters/updates': 8702}
train stats after 278496 examples: {'rewards_train/chosen': '0.15749', 'rewards_train/rejected': '0.059433', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098058', 'logps_train/rejected': '-126.36', 'logps_train/chosen': '-158.8', 'loss/train': '0.66389', 'examples_per_second': '32.469', 'grad_norm': '68', 'counters/examples': 278496, 'counters/updates': 8703}
train stats after 278528 examples: {'rewards_train/chosen': '0.23233', 'rewards_train/rejected': '0.07661', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15573', 'logps_train/rejected': '-142.4', 'logps_train/chosen': '-158.95', 'loss/train': '0.65428', 'examples_per_second': '30.581', 'grad_norm': '43.5', 'counters/examples': 278528, 'counters/updates': 8704}
train stats after 278560 examples: {'rewards_train/chosen': '0.071112', 'rewards_train/rejected': '0.0092726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061839', 'logps_train/rejected': '-115.95', 'logps_train/chosen': '-129.75', 'loss/train': '0.67157', 'examples_per_second': '30.857', 'grad_norm': '93', 'counters/examples': 278560, 'counters/updates': 8705}
train stats after 278592 examples: {'rewards_train/chosen': '0.16222', 'rewards_train/rejected': '0.034002', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12822', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-139.66', 'loss/train': '0.64571', 'examples_per_second': '30.985', 'grad_norm': '84', 'counters/examples': 278592, 'counters/updates': 8706}
train stats after 278624 examples: {'rewards_train/chosen': '0.15165', 'rewards_train/rejected': '0.079936', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071716', 'logps_train/rejected': '-132.57', 'logps_train/chosen': '-139.54', 'loss/train': '0.68775', 'examples_per_second': '32.982', 'grad_norm': '136', 'counters/examples': 278624, 'counters/updates': 8707}
train stats after 278656 examples: {'rewards_train/chosen': '0.22442', 'rewards_train/rejected': '0.043626', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18079', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-224.98', 'loss/train': '0.62284', 'examples_per_second': '31.777', 'grad_norm': '96.5', 'counters/examples': 278656, 'counters/updates': 8708}
train stats after 278688 examples: {'rewards_train/chosen': '0.13821', 'rewards_train/rejected': '0.10351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0347', 'logps_train/rejected': '-147.98', 'logps_train/chosen': '-196.28', 'loss/train': '0.69694', 'examples_per_second': '30.202', 'grad_norm': '208', 'counters/examples': 278688, 'counters/updates': 8709}
skipping logging after 278720 examples to avoid logging too frequently
train stats after 278752 examples: {'rewards_train/chosen': '0.061668', 'rewards_train/rejected': '-0.027442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08911', 'logps_train/rejected': '-138.48', 'logps_train/chosen': '-143.12', 'loss/train': '0.66541', 'examples_per_second': '31.648', 'grad_norm': '91', 'counters/examples': 278752, 'counters/updates': 8711}
train stats after 278784 examples: {'rewards_train/chosen': '0.20509', 'rewards_train/rejected': '0.14849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056592', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-147.59', 'loss/train': '0.69172', 'examples_per_second': '30.584', 'grad_norm': '68.5', 'counters/examples': 278784, 'counters/updates': 8712}
train stats after 278816 examples: {'rewards_train/chosen': '0.13897', 'rewards_train/rejected': '0.037488', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10148', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-123.99', 'loss/train': '0.65927', 'examples_per_second': '31.927', 'grad_norm': '77.5', 'counters/examples': 278816, 'counters/updates': 8713}
train stats after 278848 examples: {'rewards_train/chosen': '0.084615', 'rewards_train/rejected': '0.011143', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073471', 'logps_train/rejected': '-153.4', 'logps_train/chosen': '-155.47', 'loss/train': '0.67443', 'examples_per_second': '31.627', 'grad_norm': '76.5', 'counters/examples': 278848, 'counters/updates': 8714}
skipping logging after 278880 examples to avoid logging too frequently
train stats after 278912 examples: {'rewards_train/chosen': '0.12406', 'rewards_train/rejected': '-0.0089963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13306', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-142.73', 'loss/train': '0.64561', 'examples_per_second': '32.507', 'grad_norm': '70', 'counters/examples': 278912, 'counters/updates': 8716}
train stats after 278944 examples: {'rewards_train/chosen': '0.18813', 'rewards_train/rejected': '0.11997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068155', 'logps_train/rejected': '-167.01', 'logps_train/chosen': '-170.51', 'loss/train': '0.67003', 'examples_per_second': '23.782', 'grad_norm': '98', 'counters/examples': 278944, 'counters/updates': 8717}
skipping logging after 278976 examples to avoid logging too frequently
train stats after 279008 examples: {'rewards_train/chosen': '0.14898', 'rewards_train/rejected': '0.13041', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018564', 'logps_train/rejected': '-136.97', 'logps_train/chosen': '-141.06', 'loss/train': '0.70287', 'examples_per_second': '31.647', 'grad_norm': '113', 'counters/examples': 279008, 'counters/updates': 8719}
train stats after 279040 examples: {'rewards_train/chosen': '0.13416', 'rewards_train/rejected': '0.055837', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078326', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-144.88', 'loss/train': '0.67082', 'examples_per_second': '24.559', 'grad_norm': '69.5', 'counters/examples': 279040, 'counters/updates': 8720}
skipping logging after 279072 examples to avoid logging too frequently
train stats after 279104 examples: {'rewards_train/chosen': '0.10481', 'rewards_train/rejected': '0.038619', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066187', 'logps_train/rejected': '-166.25', 'logps_train/chosen': '-164.9', 'loss/train': '0.67728', 'examples_per_second': '32.111', 'grad_norm': '231', 'counters/examples': 279104, 'counters/updates': 8722}
train stats after 279136 examples: {'rewards_train/chosen': '0.046465', 'rewards_train/rejected': '0.05251', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0060455', 'logps_train/rejected': '-174.49', 'logps_train/chosen': '-176.96', 'loss/train': '0.71198', 'examples_per_second': '31.571', 'grad_norm': '76.5', 'counters/examples': 279136, 'counters/updates': 8723}
train stats after 279168 examples: {'rewards_train/chosen': '0.10829', 'rewards_train/rejected': '0.089611', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.018678', 'logps_train/rejected': '-134.65', 'logps_train/chosen': '-166.19', 'loss/train': '0.69212', 'examples_per_second': '32.115', 'grad_norm': '54.5', 'counters/examples': 279168, 'counters/updates': 8724}
train stats after 279200 examples: {'rewards_train/chosen': '0.072697', 'rewards_train/rejected': '0.044336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02836', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-157.13', 'loss/train': '0.69237', 'examples_per_second': '30.141', 'grad_norm': '101', 'counters/examples': 279200, 'counters/updates': 8725}
train stats after 279232 examples: {'rewards_train/chosen': '0.10996', 'rewards_train/rejected': '0.016339', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093625', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-168.83', 'loss/train': '0.6677', 'examples_per_second': '31.568', 'grad_norm': '314', 'counters/examples': 279232, 'counters/updates': 8726}
train stats after 279264 examples: {'rewards_train/chosen': '0.18748', 'rewards_train/rejected': '0.053678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1338', 'logps_train/rejected': '-129.42', 'logps_train/chosen': '-128.39', 'loss/train': '0.63749', 'examples_per_second': '30.775', 'grad_norm': '95', 'counters/examples': 279264, 'counters/updates': 8727}
train stats after 279296 examples: {'rewards_train/chosen': '0.10066', 'rewards_train/rejected': '0.089206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.011455', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-144.2', 'loss/train': '0.69598', 'examples_per_second': '30.246', 'grad_norm': '53', 'counters/examples': 279296, 'counters/updates': 8728}
train stats after 279328 examples: {'rewards_train/chosen': '0.14941', 'rewards_train/rejected': '0.018137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13127', 'logps_train/rejected': '-101.33', 'logps_train/chosen': '-122.16', 'loss/train': '0.64673', 'examples_per_second': '33.098', 'grad_norm': '68.5', 'counters/examples': 279328, 'counters/updates': 8729}
train stats after 279360 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '0.097814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059353', 'logps_train/rejected': '-182.36', 'logps_train/chosen': '-180.52', 'loss/train': '0.69175', 'examples_per_second': '31.467', 'grad_norm': '74', 'counters/examples': 279360, 'counters/updates': 8730}
train stats after 279392 examples: {'rewards_train/chosen': '0.18256', 'rewards_train/rejected': '0.0285', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15406', 'logps_train/rejected': '-150.87', 'logps_train/chosen': '-166.36', 'loss/train': '0.64843', 'examples_per_second': '31.352', 'grad_norm': '78.5', 'counters/examples': 279392, 'counters/updates': 8731}
skipping logging after 279424 examples to avoid logging too frequently
train stats after 279456 examples: {'rewards_train/chosen': '0.17399', 'rewards_train/rejected': '0.021527', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15247', 'logps_train/rejected': '-119.78', 'logps_train/chosen': '-142.37', 'loss/train': '0.62944', 'examples_per_second': '30.249', 'grad_norm': '148', 'counters/examples': 279456, 'counters/updates': 8733}
train stats after 279488 examples: {'rewards_train/chosen': '0.094919', 'rewards_train/rejected': '0.072625', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022295', 'logps_train/rejected': '-150.62', 'logps_train/chosen': '-120.6', 'loss/train': '0.70092', 'examples_per_second': '31.6', 'grad_norm': '53.5', 'counters/examples': 279488, 'counters/updates': 8734}
train stats after 279520 examples: {'rewards_train/chosen': '0.20531', 'rewards_train/rejected': '0.16555', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039763', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-127.39', 'loss/train': '0.6915', 'examples_per_second': '31.963', 'grad_norm': '81.5', 'counters/examples': 279520, 'counters/updates': 8735}
train stats after 279552 examples: {'rewards_train/chosen': '0.11023', 'rewards_train/rejected': '-0.012761', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12299', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-151.15', 'loss/train': '0.64222', 'examples_per_second': '32.107', 'grad_norm': '67', 'counters/examples': 279552, 'counters/updates': 8736}
train stats after 279584 examples: {'rewards_train/chosen': '0.14941', 'rewards_train/rejected': '0.17441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.024996', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-178.71', 'loss/train': '0.72818', 'examples_per_second': '31.622', 'grad_norm': '93', 'counters/examples': 279584, 'counters/updates': 8737}
train stats after 279616 examples: {'rewards_train/chosen': '0.15794', 'rewards_train/rejected': '0.060054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097885', 'logps_train/rejected': '-154.25', 'logps_train/chosen': '-198.51', 'loss/train': '0.6678', 'examples_per_second': '31.852', 'grad_norm': '71', 'counters/examples': 279616, 'counters/updates': 8738}
train stats after 279648 examples: {'rewards_train/chosen': '0.1088', 'rewards_train/rejected': '0.030485', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078316', 'logps_train/rejected': '-156.26', 'logps_train/chosen': '-168.24', 'loss/train': '0.66498', 'examples_per_second': '31.096', 'grad_norm': '112.5', 'counters/examples': 279648, 'counters/updates': 8739}
train stats after 279680 examples: {'rewards_train/chosen': '0.13464', 'rewards_train/rejected': '0.11993', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014711', 'logps_train/rejected': '-154.49', 'logps_train/chosen': '-139.63', 'loss/train': '0.70932', 'examples_per_second': '31.347', 'grad_norm': '61.25', 'counters/examples': 279680, 'counters/updates': 8740}
train stats after 279712 examples: {'rewards_train/chosen': '0.11418', 'rewards_train/rejected': '0.028222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085957', 'logps_train/rejected': '-113.34', 'logps_train/chosen': '-145.65', 'loss/train': '0.67364', 'examples_per_second': '31.374', 'grad_norm': '75.5', 'counters/examples': 279712, 'counters/updates': 8741}
train stats after 279744 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.040825', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099339', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-204.9', 'loss/train': '0.6607', 'examples_per_second': '31.634', 'grad_norm': '95', 'counters/examples': 279744, 'counters/updates': 8742}
skipping logging after 279776 examples to avoid logging too frequently
train stats after 279808 examples: {'rewards_train/chosen': '0.053594', 'rewards_train/rejected': '0.01542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038174', 'logps_train/rejected': '-130.23', 'logps_train/chosen': '-128.34', 'loss/train': '0.68719', 'examples_per_second': '30.135', 'grad_norm': '80.5', 'counters/examples': 279808, 'counters/updates': 8744}
skipping logging after 279840 examples to avoid logging too frequently
train stats after 279872 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.059748', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080414', 'logps_train/rejected': '-98.631', 'logps_train/chosen': '-109.69', 'loss/train': '0.67661', 'examples_per_second': '31.334', 'grad_norm': '186', 'counters/examples': 279872, 'counters/updates': 8746}
skipping logging after 279904 examples to avoid logging too frequently
train stats after 279936 examples: {'rewards_train/chosen': '0.10114', 'rewards_train/rejected': '0.17595', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.074815', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-140.34', 'loss/train': '0.74461', 'examples_per_second': '31.582', 'grad_norm': '92.5', 'counters/examples': 279936, 'counters/updates': 8748}
train stats after 279968 examples: {'rewards_train/chosen': '0.11396', 'rewards_train/rejected': '0.084525', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029436', 'logps_train/rejected': '-151.61', 'logps_train/chosen': '-177.59', 'loss/train': '0.69517', 'examples_per_second': '31.647', 'grad_norm': '119.5', 'counters/examples': 279968, 'counters/updates': 8749}
train stats after 280000 examples: {'rewards_train/chosen': '0.17593', 'rewards_train/rejected': '0.093969', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081962', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-144.27', 'loss/train': '0.66977', 'examples_per_second': '30.633', 'grad_norm': '60', 'counters/examples': 280000, 'counters/updates': 8750}
train stats after 280032 examples: {'rewards_train/chosen': '0.10809', 'rewards_train/rejected': '0.12304', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.014949', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-118.62', 'loss/train': '0.71321', 'examples_per_second': '31.211', 'grad_norm': '72', 'counters/examples': 280032, 'counters/updates': 8751}
train stats after 280064 examples: {'rewards_train/chosen': '0.069398', 'rewards_train/rejected': '0.10376', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034366', 'logps_train/rejected': '-145.75', 'logps_train/chosen': '-144.67', 'loss/train': '0.73057', 'examples_per_second': '32.213', 'grad_norm': '59.75', 'counters/examples': 280064, 'counters/updates': 8752}
train stats after 280096 examples: {'rewards_train/chosen': '0.13071', 'rewards_train/rejected': '0.066545', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064163', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-129.37', 'loss/train': '0.67988', 'examples_per_second': '31.634', 'grad_norm': '90', 'counters/examples': 280096, 'counters/updates': 8753}
train stats after 280128 examples: {'rewards_train/chosen': '0.080313', 'rewards_train/rejected': '0.10966', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.029342', 'logps_train/rejected': '-164.19', 'logps_train/chosen': '-120.08', 'loss/train': '0.72501', 'examples_per_second': '31.575', 'grad_norm': '176', 'counters/examples': 280128, 'counters/updates': 8754}
train stats after 280160 examples: {'rewards_train/chosen': '0.067476', 'rewards_train/rejected': '0.07089', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0034147', 'logps_train/rejected': '-97.717', 'logps_train/chosen': '-134.51', 'loss/train': '0.70693', 'examples_per_second': '32.293', 'grad_norm': '81', 'counters/examples': 280160, 'counters/updates': 8755}
train stats after 280192 examples: {'rewards_train/chosen': '0.17102', 'rewards_train/rejected': '0.068901', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10212', 'logps_train/rejected': '-155.17', 'logps_train/chosen': '-172.57', 'loss/train': '0.65428', 'examples_per_second': '30.215', 'grad_norm': '86', 'counters/examples': 280192, 'counters/updates': 8756}
train stats after 280224 examples: {'rewards_train/chosen': '0.11233', 'rewards_train/rejected': '0.097749', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014576', 'logps_train/rejected': '-124.09', 'logps_train/chosen': '-132.87', 'loss/train': '0.69981', 'examples_per_second': '24.736', 'grad_norm': '436', 'counters/examples': 280224, 'counters/updates': 8757}
train stats after 280256 examples: {'rewards_train/chosen': '0.11821', 'rewards_train/rejected': '0.079275', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038932', 'logps_train/rejected': '-140.44', 'logps_train/chosen': '-172.09', 'loss/train': '0.6889', 'examples_per_second': '32.37', 'grad_norm': '125.5', 'counters/examples': 280256, 'counters/updates': 8758}
train stats after 280288 examples: {'rewards_train/chosen': '0.080973', 'rewards_train/rejected': '0.11812', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.037147', 'logps_train/rejected': '-154.24', 'logps_train/chosen': '-207.02', 'loss/train': '0.73106', 'examples_per_second': '30.845', 'grad_norm': '75.5', 'counters/examples': 280288, 'counters/updates': 8759}
train stats after 280320 examples: {'rewards_train/chosen': '0.21617', 'rewards_train/rejected': '0.04859', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16758', 'logps_train/rejected': '-177.27', 'logps_train/chosen': '-188.12', 'loss/train': '0.6277', 'examples_per_second': '31.017', 'grad_norm': '83', 'counters/examples': 280320, 'counters/updates': 8760}
train stats after 280352 examples: {'rewards_train/chosen': '0.10847', 'rewards_train/rejected': '0.15719', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.048719', 'logps_train/rejected': '-158.7', 'logps_train/chosen': '-157.81', 'loss/train': '0.72857', 'examples_per_second': '30.835', 'grad_norm': '123.5', 'counters/examples': 280352, 'counters/updates': 8761}
train stats after 280384 examples: {'rewards_train/chosen': '0.045245', 'rewards_train/rejected': '0.072612', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027368', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-139.13', 'loss/train': '0.72224', 'examples_per_second': '30.671', 'grad_norm': '288', 'counters/examples': 280384, 'counters/updates': 8762}
train stats after 280416 examples: {'rewards_train/chosen': '0.12956', 'rewards_train/rejected': '0.13393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0043676', 'logps_train/rejected': '-175.24', 'logps_train/chosen': '-167.76', 'loss/train': '0.7152', 'examples_per_second': '32.738', 'grad_norm': '126.5', 'counters/examples': 280416, 'counters/updates': 8763}
skipping logging after 280448 examples to avoid logging too frequently
train stats after 280480 examples: {'rewards_train/chosen': '0.13198', 'rewards_train/rejected': '0.054862', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077113', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-122.34', 'loss/train': '0.66358', 'examples_per_second': '36.236', 'grad_norm': '62.75', 'counters/examples': 280480, 'counters/updates': 8765}
skipping logging after 280512 examples to avoid logging too frequently
train stats after 280544 examples: {'rewards_train/chosen': '0.30003', 'rewards_train/rejected': '0.13071', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16933', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-183.52', 'loss/train': '0.66786', 'examples_per_second': '32.777', 'grad_norm': '80.5', 'counters/examples': 280544, 'counters/updates': 8767}
train stats after 280576 examples: {'rewards_train/chosen': '0.21077', 'rewards_train/rejected': '0.15553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055242', 'logps_train/rejected': '-158.13', 'logps_train/chosen': '-160.76', 'loss/train': '0.69839', 'examples_per_second': '30.158', 'grad_norm': '201', 'counters/examples': 280576, 'counters/updates': 8768}
train stats after 280608 examples: {'rewards_train/chosen': '0.14884', 'rewards_train/rejected': '0.008615', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14023', 'logps_train/rejected': '-153.48', 'logps_train/chosen': '-166.64', 'loss/train': '0.65397', 'examples_per_second': '31.658', 'grad_norm': '141', 'counters/examples': 280608, 'counters/updates': 8769}
train stats after 280640 examples: {'rewards_train/chosen': '0.16416', 'rewards_train/rejected': '0.042118', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12204', 'logps_train/rejected': '-155.29', 'logps_train/chosen': '-191', 'loss/train': '0.64675', 'examples_per_second': '30.817', 'grad_norm': '80', 'counters/examples': 280640, 'counters/updates': 8770}
train stats after 280672 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '0.071453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088324', 'logps_train/rejected': '-166.26', 'logps_train/chosen': '-186.72', 'loss/train': '0.672', 'examples_per_second': '30.13', 'grad_norm': '138', 'counters/examples': 280672, 'counters/updates': 8771}
train stats after 280704 examples: {'rewards_train/chosen': '0.16269', 'rewards_train/rejected': '-0.0071154', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1698', 'logps_train/rejected': '-159.97', 'logps_train/chosen': '-152.94', 'loss/train': '0.63191', 'examples_per_second': '32.873', 'grad_norm': '94', 'counters/examples': 280704, 'counters/updates': 8772}
train stats after 280736 examples: {'rewards_train/chosen': '0.28589', 'rewards_train/rejected': '0.016328', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26956', 'logps_train/rejected': '-152.93', 'logps_train/chosen': '-162.13', 'loss/train': '0.58241', 'examples_per_second': '32.395', 'grad_norm': '51.75', 'counters/examples': 280736, 'counters/updates': 8773}
skipping logging after 280768 examples to avoid logging too frequently
train stats after 280800 examples: {'rewards_train/chosen': '0.11175', 'rewards_train/rejected': '0.080007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031744', 'logps_train/rejected': '-145.57', 'logps_train/chosen': '-124.54', 'loss/train': '0.68682', 'examples_per_second': '36.907', 'grad_norm': '58.25', 'counters/examples': 280800, 'counters/updates': 8775}
train stats after 280832 examples: {'rewards_train/chosen': '0.075055', 'rewards_train/rejected': '0.034821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040233', 'logps_train/rejected': '-102.66', 'logps_train/chosen': '-126.51', 'loss/train': '0.68564', 'examples_per_second': '31.129', 'grad_norm': '95.5', 'counters/examples': 280832, 'counters/updates': 8776}
train stats after 280864 examples: {'rewards_train/chosen': '0.1609', 'rewards_train/rejected': '0.041866', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11904', 'logps_train/rejected': '-141.82', 'logps_train/chosen': '-105.3', 'loss/train': '0.65059', 'examples_per_second': '32.211', 'grad_norm': '94.5', 'counters/examples': 280864, 'counters/updates': 8777}
train stats after 280896 examples: {'rewards_train/chosen': '0.080194', 'rewards_train/rejected': '-0.034246', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11444', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-175.18', 'loss/train': '0.65344', 'examples_per_second': '31.685', 'grad_norm': '78', 'counters/examples': 280896, 'counters/updates': 8778}
train stats after 280928 examples: {'rewards_train/chosen': '0.11747', 'rewards_train/rejected': '0.036568', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080902', 'logps_train/rejected': '-111.39', 'logps_train/chosen': '-117.38', 'loss/train': '0.66083', 'examples_per_second': '30.852', 'grad_norm': '59.25', 'counters/examples': 280928, 'counters/updates': 8779}
skipping logging after 280960 examples to avoid logging too frequently
train stats after 280992 examples: {'rewards_train/chosen': '0.12136', 'rewards_train/rejected': '-0.027245', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14861', 'logps_train/rejected': '-129.13', 'logps_train/chosen': '-145.73', 'loss/train': '0.64057', 'examples_per_second': '30.343', 'grad_norm': '106', 'counters/examples': 280992, 'counters/updates': 8781}
train stats after 281024 examples: {'rewards_train/chosen': '0.112', 'rewards_train/rejected': '0.035793', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07621', 'logps_train/rejected': '-129.59', 'logps_train/chosen': '-135.36', 'loss/train': '0.66579', 'examples_per_second': '30.806', 'grad_norm': '135', 'counters/examples': 281024, 'counters/updates': 8782}
train stats after 281056 examples: {'rewards_train/chosen': '0.27704', 'rewards_train/rejected': '0.12681', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15023', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-164', 'loss/train': '0.63639', 'examples_per_second': '32.932', 'grad_norm': '54.75', 'counters/examples': 281056, 'counters/updates': 8783}
train stats after 281088 examples: {'rewards_train/chosen': '0.14888', 'rewards_train/rejected': '0.016389', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13249', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-196.31', 'loss/train': '0.64301', 'examples_per_second': '31.573', 'grad_norm': '66', 'counters/examples': 281088, 'counters/updates': 8784}
train stats after 281120 examples: {'rewards_train/chosen': '0.11222', 'rewards_train/rejected': '0.091402', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020815', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-159.66', 'loss/train': '0.69854', 'examples_per_second': '31.212', 'grad_norm': '109', 'counters/examples': 281120, 'counters/updates': 8785}
train stats after 281152 examples: {'rewards_train/chosen': '0.10543', 'rewards_train/rejected': '0.024447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080979', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-135.44', 'loss/train': '0.66637', 'examples_per_second': '30.985', 'grad_norm': '112', 'counters/examples': 281152, 'counters/updates': 8786}
train stats after 281184 examples: {'rewards_train/chosen': '0.060927', 'rewards_train/rejected': '0.011771', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049156', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-170.49', 'loss/train': '0.68188', 'examples_per_second': '30.761', 'grad_norm': '87.5', 'counters/examples': 281184, 'counters/updates': 8787}
train stats after 281216 examples: {'rewards_train/chosen': '0.07577', 'rewards_train/rejected': '0.027632', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048139', 'logps_train/rejected': '-143.66', 'logps_train/chosen': '-150.23', 'loss/train': '0.68821', 'examples_per_second': '30.137', 'grad_norm': '132', 'counters/examples': 281216, 'counters/updates': 8788}
skipping logging after 281248 examples to avoid logging too frequently
train stats after 281280 examples: {'rewards_train/chosen': '0.12401', 'rewards_train/rejected': '-0.028981', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15299', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-160.43', 'loss/train': '0.63942', 'examples_per_second': '34.005', 'grad_norm': '96', 'counters/examples': 281280, 'counters/updates': 8790}
train stats after 281312 examples: {'rewards_train/chosen': '0.17173', 'rewards_train/rejected': '0.065122', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10661', 'logps_train/rejected': '-168.19', 'logps_train/chosen': '-173.58', 'loss/train': '0.65161', 'examples_per_second': '32.937', 'grad_norm': '178', 'counters/examples': 281312, 'counters/updates': 8791}
train stats after 281344 examples: {'rewards_train/chosen': '0.15532', 'rewards_train/rejected': '0.10147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053854', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-165.98', 'loss/train': '0.68429', 'examples_per_second': '30.183', 'grad_norm': '88', 'counters/examples': 281344, 'counters/updates': 8792}
train stats after 281376 examples: {'rewards_train/chosen': '0.20197', 'rewards_train/rejected': '0.0040314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19794', 'logps_train/rejected': '-147.77', 'logps_train/chosen': '-198.02', 'loss/train': '0.62406', 'examples_per_second': '30.168', 'grad_norm': '168', 'counters/examples': 281376, 'counters/updates': 8793}
skipping logging after 281408 examples to avoid logging too frequently
train stats after 281440 examples: {'rewards_train/chosen': '0.078651', 'rewards_train/rejected': '0.010828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067823', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-142.82', 'loss/train': '0.67126', 'examples_per_second': '36.052', 'grad_norm': '67.5', 'counters/examples': 281440, 'counters/updates': 8795}
train stats after 281472 examples: {'rewards_train/chosen': '0.17045', 'rewards_train/rejected': '0.060418', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11003', 'logps_train/rejected': '-183.62', 'logps_train/chosen': '-163.19', 'loss/train': '0.66208', 'examples_per_second': '30.637', 'grad_norm': '99', 'counters/examples': 281472, 'counters/updates': 8796}
train stats after 281504 examples: {'rewards_train/chosen': '0.04286', 'rewards_train/rejected': '-0.0088667', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051727', 'logps_train/rejected': '-123.42', 'logps_train/chosen': '-151.36', 'loss/train': '0.67748', 'examples_per_second': '31.675', 'grad_norm': '58.5', 'counters/examples': 281504, 'counters/updates': 8797}
skipping logging after 281536 examples to avoid logging too frequently
train stats after 281568 examples: {'rewards_train/chosen': '0.087533', 'rewards_train/rejected': '0.10142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.013887', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-169.09', 'loss/train': '0.71189', 'examples_per_second': '32.558', 'grad_norm': '71', 'counters/examples': 281568, 'counters/updates': 8799}
train stats after 281600 examples: {'rewards_train/chosen': '0.13175', 'rewards_train/rejected': '0.09467', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.037084', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-147.58', 'loss/train': '0.68733', 'examples_per_second': '32.264', 'grad_norm': '84.5', 'counters/examples': 281600, 'counters/updates': 8800}
train stats after 281632 examples: {'rewards_train/chosen': '0.1579', 'rewards_train/rejected': '0.059659', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.098243', 'logps_train/rejected': '-174.62', 'logps_train/chosen': '-218.29', 'loss/train': '0.66086', 'examples_per_second': '32.345', 'grad_norm': '60.75', 'counters/examples': 281632, 'counters/updates': 8801}
skipping logging after 281664 examples to avoid logging too frequently
train stats after 281696 examples: {'rewards_train/chosen': '0.2423', 'rewards_train/rejected': '0.097882', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14442', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-157.79', 'loss/train': '0.63423', 'examples_per_second': '31.502', 'grad_norm': '50', 'counters/examples': 281696, 'counters/updates': 8803}
train stats after 281728 examples: {'rewards_train/chosen': '0.20648', 'rewards_train/rejected': '0.075958', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13053', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-159.7', 'loss/train': '0.64347', 'examples_per_second': '30.167', 'grad_norm': '66', 'counters/examples': 281728, 'counters/updates': 8804}
train stats after 281760 examples: {'rewards_train/chosen': '0.1122', 'rewards_train/rejected': '0.060553', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051651', 'logps_train/rejected': '-131.47', 'logps_train/chosen': '-119.72', 'loss/train': '0.67971', 'examples_per_second': '30.693', 'grad_norm': '101.5', 'counters/examples': 281760, 'counters/updates': 8805}
train stats after 281792 examples: {'rewards_train/chosen': '0.028888', 'rewards_train/rejected': '0.056176', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027288', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-162.1', 'loss/train': '0.73463', 'examples_per_second': '31.646', 'grad_norm': '120', 'counters/examples': 281792, 'counters/updates': 8806}
train stats after 281824 examples: {'rewards_train/chosen': '0.17282', 'rewards_train/rejected': '0.1115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061323', 'logps_train/rejected': '-124.66', 'logps_train/chosen': '-160.04', 'loss/train': '0.66785', 'examples_per_second': '32.101', 'grad_norm': '88', 'counters/examples': 281824, 'counters/updates': 8807}
train stats after 281856 examples: {'rewards_train/chosen': '0.1232', 'rewards_train/rejected': '0.035214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087989', 'logps_train/rejected': '-153.77', 'logps_train/chosen': '-173.8', 'loss/train': '0.66567', 'examples_per_second': '31.648', 'grad_norm': '62.5', 'counters/examples': 281856, 'counters/updates': 8808}
train stats after 281888 examples: {'rewards_train/chosen': '0.19854', 'rewards_train/rejected': '0.035584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16295', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-153.54', 'loss/train': '0.63519', 'examples_per_second': '32.195', 'grad_norm': '88', 'counters/examples': 281888, 'counters/updates': 8809}
train stats after 281920 examples: {'rewards_train/chosen': '0.17057', 'rewards_train/rejected': '0.11573', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054839', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-169.77', 'loss/train': '0.6851', 'examples_per_second': '31.697', 'grad_norm': '95', 'counters/examples': 281920, 'counters/updates': 8810}
train stats after 281952 examples: {'rewards_train/chosen': '0.086031', 'rewards_train/rejected': '0.029727', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056304', 'logps_train/rejected': '-133.8', 'logps_train/chosen': '-119.05', 'loss/train': '0.67532', 'examples_per_second': '32.623', 'grad_norm': '85.5', 'counters/examples': 281952, 'counters/updates': 8811}
train stats after 281984 examples: {'rewards_train/chosen': '0.055582', 'rewards_train/rejected': '0.088227', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032645', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-145.77', 'loss/train': '0.72482', 'examples_per_second': '30.756', 'grad_norm': '78.5', 'counters/examples': 281984, 'counters/updates': 8812}
skipping logging after 282016 examples to avoid logging too frequently
train stats after 282048 examples: {'rewards_train/chosen': '0.15082', 'rewards_train/rejected': '0.082472', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068345', 'logps_train/rejected': '-115.91', 'logps_train/chosen': '-155.77', 'loss/train': '0.67307', 'examples_per_second': '30.373', 'grad_norm': '57.5', 'counters/examples': 282048, 'counters/updates': 8814}
train stats after 282080 examples: {'rewards_train/chosen': '0.13914', 'rewards_train/rejected': '0.088915', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050228', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-143.71', 'loss/train': '0.68295', 'examples_per_second': '31.674', 'grad_norm': '74.5', 'counters/examples': 282080, 'counters/updates': 8815}
train stats after 282112 examples: {'rewards_train/chosen': '0.14398', 'rewards_train/rejected': '0.020916', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12306', 'logps_train/rejected': '-115.24', 'logps_train/chosen': '-162.68', 'loss/train': '0.65456', 'examples_per_second': '30.658', 'grad_norm': '63.5', 'counters/examples': 282112, 'counters/updates': 8816}
train stats after 282144 examples: {'rewards_train/chosen': '0.17005', 'rewards_train/rejected': '0.063803', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10625', 'logps_train/rejected': '-121.9', 'logps_train/chosen': '-176.68', 'loss/train': '0.66052', 'examples_per_second': '31.591', 'grad_norm': '84.5', 'counters/examples': 282144, 'counters/updates': 8817}
train stats after 282176 examples: {'rewards_train/chosen': '0.2752', 'rewards_train/rejected': '0.074372', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20083', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-160.12', 'loss/train': '0.62983', 'examples_per_second': '31.67', 'grad_norm': '180', 'counters/examples': 282176, 'counters/updates': 8818}
train stats after 282208 examples: {'rewards_train/chosen': '0.13138', 'rewards_train/rejected': '0.03252', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09886', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-109.81', 'loss/train': '0.6564', 'examples_per_second': '31.5', 'grad_norm': '62.25', 'counters/examples': 282208, 'counters/updates': 8819}
skipping logging after 282240 examples to avoid logging too frequently
train stats after 282272 examples: {'rewards_train/chosen': '0.078832', 'rewards_train/rejected': '0.028755', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050076', 'logps_train/rejected': '-148.26', 'logps_train/chosen': '-147.93', 'loss/train': '0.68738', 'examples_per_second': '31.577', 'grad_norm': '62.5', 'counters/examples': 282272, 'counters/updates': 8821}
skipping logging after 282304 examples to avoid logging too frequently
train stats after 282336 examples: {'rewards_train/chosen': '0.19143', 'rewards_train/rejected': '0.059167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13226', 'logps_train/rejected': '-129.73', 'logps_train/chosen': '-157.65', 'loss/train': '0.64342', 'examples_per_second': '31.619', 'grad_norm': '230', 'counters/examples': 282336, 'counters/updates': 8823}
skipping logging after 282368 examples to avoid logging too frequently
train stats after 282400 examples: {'rewards_train/chosen': '0.17935', 'rewards_train/rejected': '0.047042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13231', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-126.73', 'loss/train': '0.65571', 'examples_per_second': '30.651', 'grad_norm': '62.5', 'counters/examples': 282400, 'counters/updates': 8825}
train stats after 282432 examples: {'rewards_train/chosen': '0.081093', 'rewards_train/rejected': '0.02887', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052222', 'logps_train/rejected': '-136.97', 'logps_train/chosen': '-164.8', 'loss/train': '0.70047', 'examples_per_second': '32.45', 'grad_norm': '108', 'counters/examples': 282432, 'counters/updates': 8826}
train stats after 282464 examples: {'rewards_train/chosen': '0.11161', 'rewards_train/rejected': '0.028993', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082615', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-154.95', 'loss/train': '0.67351', 'examples_per_second': '31.037', 'grad_norm': '149', 'counters/examples': 282464, 'counters/updates': 8827}
train stats after 282496 examples: {'rewards_train/chosen': '0.14359', 'rewards_train/rejected': '0.059602', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.083991', 'logps_train/rejected': '-144.66', 'logps_train/chosen': '-179.94', 'loss/train': '0.66753', 'examples_per_second': '30.689', 'grad_norm': '118', 'counters/examples': 282496, 'counters/updates': 8828}
skipping logging after 282528 examples to avoid logging too frequently
train stats after 282560 examples: {'rewards_train/chosen': '0.11657', 'rewards_train/rejected': '-0.025277', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14185', 'logps_train/rejected': '-149.58', 'logps_train/chosen': '-184.44', 'loss/train': '0.64027', 'examples_per_second': '31.108', 'grad_norm': '44.5', 'counters/examples': 282560, 'counters/updates': 8830}
train stats after 282592 examples: {'rewards_train/chosen': '0.11448', 'rewards_train/rejected': '0.075882', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038595', 'logps_train/rejected': '-152.95', 'logps_train/chosen': '-187.19', 'loss/train': '0.69209', 'examples_per_second': '30.494', 'grad_norm': '89.5', 'counters/examples': 282592, 'counters/updates': 8831}
train stats after 282624 examples: {'rewards_train/chosen': '0.19735', 'rewards_train/rejected': '0.23296', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035613', 'logps_train/rejected': '-185.36', 'logps_train/chosen': '-163.03', 'loss/train': '0.746', 'examples_per_second': '30.688', 'grad_norm': '142', 'counters/examples': 282624, 'counters/updates': 8832}
train stats after 282656 examples: {'rewards_train/chosen': '0.15091', 'rewards_train/rejected': '0.051156', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099754', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-174.17', 'loss/train': '0.65894', 'examples_per_second': '31.683', 'grad_norm': '156', 'counters/examples': 282656, 'counters/updates': 8833}
train stats after 282688 examples: {'rewards_train/chosen': '0.13658', 'rewards_train/rejected': '0.025991', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11058', 'logps_train/rejected': '-115.23', 'logps_train/chosen': '-135.44', 'loss/train': '0.6618', 'examples_per_second': '30.245', 'grad_norm': '72', 'counters/examples': 282688, 'counters/updates': 8834}
skipping logging after 282720 examples to avoid logging too frequently
train stats after 282752 examples: {'rewards_train/chosen': '0.13612', 'rewards_train/rejected': '0.10518', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030938', 'logps_train/rejected': '-178.12', 'logps_train/chosen': '-175.29', 'loss/train': '0.69796', 'examples_per_second': '30.684', 'grad_norm': '106', 'counters/examples': 282752, 'counters/updates': 8836}
skipping logging after 282784 examples to avoid logging too frequently
train stats after 282816 examples: {'rewards_train/chosen': '0.19376', 'rewards_train/rejected': '0.078401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11536', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-174.34', 'loss/train': '0.65346', 'examples_per_second': '31.825', 'grad_norm': '63', 'counters/examples': 282816, 'counters/updates': 8838}
train stats after 282848 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '0.026869', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10343', 'logps_train/rejected': '-135.84', 'logps_train/chosen': '-159.98', 'loss/train': '0.65851', 'examples_per_second': '32.591', 'grad_norm': '59.75', 'counters/examples': 282848, 'counters/updates': 8839}
train stats after 282880 examples: {'rewards_train/chosen': '0.21946', 'rewards_train/rejected': '0.013892', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20556', 'logps_train/rejected': '-141.92', 'logps_train/chosen': '-197.4', 'loss/train': '0.6143', 'examples_per_second': '31.298', 'grad_norm': '159', 'counters/examples': 282880, 'counters/updates': 8840}
train stats after 282912 examples: {'rewards_train/chosen': '0.14457', 'rewards_train/rejected': '0.12833', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016238', 'logps_train/rejected': '-138.39', 'logps_train/chosen': '-171.99', 'loss/train': '0.70799', 'examples_per_second': '31.1', 'grad_norm': '107', 'counters/examples': 282912, 'counters/updates': 8841}
train stats after 282944 examples: {'rewards_train/chosen': '0.10136', 'rewards_train/rejected': '0.020594', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080764', 'logps_train/rejected': '-119.3', 'logps_train/chosen': '-134.36', 'loss/train': '0.66745', 'examples_per_second': '30.168', 'grad_norm': '115.5', 'counters/examples': 282944, 'counters/updates': 8842}
train stats after 282976 examples: {'rewards_train/chosen': '0.14252', 'rewards_train/rejected': '0.02254', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11998', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-156.29', 'loss/train': '0.64998', 'examples_per_second': '32.703', 'grad_norm': '54.75', 'counters/examples': 282976, 'counters/updates': 8843}
train stats after 283008 examples: {'rewards_train/chosen': '0.14585', 'rewards_train/rejected': '0.032752', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1131', 'logps_train/rejected': '-139.2', 'logps_train/chosen': '-147.77', 'loss/train': '0.6479', 'examples_per_second': '32.978', 'grad_norm': '66', 'counters/examples': 283008, 'counters/updates': 8844}
train stats after 283040 examples: {'rewards_train/chosen': '0.023713', 'rewards_train/rejected': '0.0043849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019329', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-138.38', 'loss/train': '0.69737', 'examples_per_second': '32.452', 'grad_norm': '99.5', 'counters/examples': 283040, 'counters/updates': 8845}
train stats after 283072 examples: {'rewards_train/chosen': '0.0081517', 'rewards_train/rejected': '-0.039105', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047257', 'logps_train/rejected': '-88.62', 'logps_train/chosen': '-106.71', 'loss/train': '0.6749', 'examples_per_second': '30.979', 'grad_norm': '60.25', 'counters/examples': 283072, 'counters/updates': 8846}
train stats after 283104 examples: {'rewards_train/chosen': '0.16237', 'rewards_train/rejected': '0.052108', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11026', 'logps_train/rejected': '-165.04', 'logps_train/chosen': '-143.86', 'loss/train': '0.65963', 'examples_per_second': '31.644', 'grad_norm': '160', 'counters/examples': 283104, 'counters/updates': 8847}
train stats after 283136 examples: {'rewards_train/chosen': '0.081738', 'rewards_train/rejected': '0.081232', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00050639', 'logps_train/rejected': '-116.98', 'logps_train/chosen': '-163.25', 'loss/train': '0.7073', 'examples_per_second': '30.709', 'grad_norm': '260', 'counters/examples': 283136, 'counters/updates': 8848}
train stats after 283168 examples: {'rewards_train/chosen': '0.15518', 'rewards_train/rejected': '0.078914', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07627', 'logps_train/rejected': '-149.39', 'logps_train/chosen': '-149.65', 'loss/train': '0.67047', 'examples_per_second': '31.591', 'grad_norm': '85.5', 'counters/examples': 283168, 'counters/updates': 8849}
train stats after 283200 examples: {'rewards_train/chosen': '0.054474', 'rewards_train/rejected': '-0.037345', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091819', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-166.39', 'loss/train': '0.66442', 'examples_per_second': '31.669', 'grad_norm': '83.5', 'counters/examples': 283200, 'counters/updates': 8850}
train stats after 283232 examples: {'rewards_train/chosen': '0.079875', 'rewards_train/rejected': '0.10891', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029034', 'logps_train/rejected': '-140.21', 'logps_train/chosen': '-165.57', 'loss/train': '0.72167', 'examples_per_second': '32.427', 'grad_norm': '117', 'counters/examples': 283232, 'counters/updates': 8851}
train stats after 283264 examples: {'rewards_train/chosen': '0.091009', 'rewards_train/rejected': '0.14136', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.050355', 'logps_train/rejected': '-159.53', 'logps_train/chosen': '-161.79', 'loss/train': '0.73831', 'examples_per_second': '31.341', 'grad_norm': '67.5', 'counters/examples': 283264, 'counters/updates': 8852}
train stats after 283296 examples: {'rewards_train/chosen': '0.1499', 'rewards_train/rejected': '0.047166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10273', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-152.61', 'loss/train': '0.65672', 'examples_per_second': '30.115', 'grad_norm': '115.5', 'counters/examples': 283296, 'counters/updates': 8853}
train stats after 283328 examples: {'rewards_train/chosen': '0.16088', 'rewards_train/rejected': '0.034926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12596', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-132.43', 'loss/train': '0.64814', 'examples_per_second': '31.059', 'grad_norm': '79.5', 'counters/examples': 283328, 'counters/updates': 8854}
train stats after 283360 examples: {'rewards_train/chosen': '0.044848', 'rewards_train/rejected': '0.10615', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.061299', 'logps_train/rejected': '-141.4', 'logps_train/chosen': '-147.65', 'loss/train': '0.73623', 'examples_per_second': '30.712', 'grad_norm': '57.5', 'counters/examples': 283360, 'counters/updates': 8855}
skipping logging after 283392 examples to avoid logging too frequently
train stats after 283424 examples: {'rewards_train/chosen': '0.25726', 'rewards_train/rejected': '0.082242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17502', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-155.2', 'loss/train': '0.63977', 'examples_per_second': '30.998', 'grad_norm': '68.5', 'counters/examples': 283424, 'counters/updates': 8857}
skipping logging after 283456 examples to avoid logging too frequently
train stats after 283488 examples: {'rewards_train/chosen': '0.11959', 'rewards_train/rejected': '-0.0090331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12863', 'logps_train/rejected': '-92.024', 'logps_train/chosen': '-132.91', 'loss/train': '0.6407', 'examples_per_second': '30.212', 'grad_norm': '63.25', 'counters/examples': 283488, 'counters/updates': 8859}
train stats after 283520 examples: {'rewards_train/chosen': '0.18819', 'rewards_train/rejected': '0.095541', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092651', 'logps_train/rejected': '-171.34', 'logps_train/chosen': '-189.97', 'loss/train': '0.67184', 'examples_per_second': '31.598', 'grad_norm': '118', 'counters/examples': 283520, 'counters/updates': 8860}
train stats after 283552 examples: {'rewards_train/chosen': '0.11433', 'rewards_train/rejected': '0.13376', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019429', 'logps_train/rejected': '-151.54', 'logps_train/chosen': '-145.77', 'loss/train': '0.7127', 'examples_per_second': '31.554', 'grad_norm': '69.5', 'counters/examples': 283552, 'counters/updates': 8861}
train stats after 283584 examples: {'rewards_train/chosen': '0.17747', 'rewards_train/rejected': '0.10747', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070001', 'logps_train/rejected': '-150.97', 'logps_train/chosen': '-164.39', 'loss/train': '0.68211', 'examples_per_second': '32.576', 'grad_norm': '109.5', 'counters/examples': 283584, 'counters/updates': 8862}
train stats after 283616 examples: {'rewards_train/chosen': '0.11013', 'rewards_train/rejected': '0.037194', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072933', 'logps_train/rejected': '-128.19', 'logps_train/chosen': '-152.83', 'loss/train': '0.67451', 'examples_per_second': '32.456', 'grad_norm': '119', 'counters/examples': 283616, 'counters/updates': 8863}
train stats after 283648 examples: {'rewards_train/chosen': '0.09849', 'rewards_train/rejected': '0.059258', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039232', 'logps_train/rejected': '-100.46', 'logps_train/chosen': '-146.67', 'loss/train': '0.69373', 'examples_per_second': '32.763', 'grad_norm': '52', 'counters/examples': 283648, 'counters/updates': 8864}
train stats after 283680 examples: {'rewards_train/chosen': '0.076306', 'rewards_train/rejected': '0.027712', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048595', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-135.85', 'loss/train': '0.68658', 'examples_per_second': '30.506', 'grad_norm': '71.5', 'counters/examples': 283680, 'counters/updates': 8865}
train stats after 283712 examples: {'rewards_train/chosen': '0.12501', 'rewards_train/rejected': '-0.0065004', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13151', 'logps_train/rejected': '-116.47', 'logps_train/chosen': '-149.06', 'loss/train': '0.64561', 'examples_per_second': '31.322', 'grad_norm': '63', 'counters/examples': 283712, 'counters/updates': 8866}
skipping logging after 283744 examples to avoid logging too frequently
train stats after 283776 examples: {'rewards_train/chosen': '0.15737', 'rewards_train/rejected': '0.071392', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085981', 'logps_train/rejected': '-113.6', 'logps_train/chosen': '-118.75', 'loss/train': '0.66142', 'examples_per_second': '31.12', 'grad_norm': '54.75', 'counters/examples': 283776, 'counters/updates': 8868}
train stats after 283808 examples: {'rewards_train/chosen': '0.18599', 'rewards_train/rejected': '0.066476', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11952', 'logps_train/rejected': '-138.69', 'logps_train/chosen': '-150.6', 'loss/train': '0.64783', 'examples_per_second': '31.368', 'grad_norm': '74', 'counters/examples': 283808, 'counters/updates': 8869}
train stats after 283840 examples: {'rewards_train/chosen': '0.19169', 'rewards_train/rejected': '0.035544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15615', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-166.63', 'loss/train': '0.63619', 'examples_per_second': '30.383', 'grad_norm': '104.5', 'counters/examples': 283840, 'counters/updates': 8870}
train stats after 283872 examples: {'rewards_train/chosen': '0.089391', 'rewards_train/rejected': '0.04607', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.043321', 'logps_train/rejected': '-86.856', 'logps_train/chosen': '-129.05', 'loss/train': '0.68113', 'examples_per_second': '31.31', 'grad_norm': '203', 'counters/examples': 283872, 'counters/updates': 8871}
train stats after 283904 examples: {'rewards_train/chosen': '0.17991', 'rewards_train/rejected': '0.095302', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084605', 'logps_train/rejected': '-140.11', 'logps_train/chosen': '-171.5', 'loss/train': '0.6651', 'examples_per_second': '31.835', 'grad_norm': '190', 'counters/examples': 283904, 'counters/updates': 8872}
skipping logging after 283936 examples to avoid logging too frequently
train stats after 283968 examples: {'rewards_train/chosen': '0.070785', 'rewards_train/rejected': '0.013084', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057701', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-164.98', 'loss/train': '0.67522', 'examples_per_second': '30.912', 'grad_norm': '143', 'counters/examples': 283968, 'counters/updates': 8874}
train stats after 284000 examples: {'rewards_train/chosen': '0.1154', 'rewards_train/rejected': '0.017057', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098341', 'logps_train/rejected': '-112.08', 'logps_train/chosen': '-144.36', 'loss/train': '0.66235', 'examples_per_second': '31.676', 'grad_norm': '101.5', 'counters/examples': 284000, 'counters/updates': 8875}
skipping logging after 284032 examples to avoid logging too frequently
train stats after 284064 examples: {'rewards_train/chosen': '0.044996', 'rewards_train/rejected': '-0.013273', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.058269', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-145.8', 'loss/train': '0.67784', 'examples_per_second': '31.667', 'grad_norm': '72', 'counters/examples': 284064, 'counters/updates': 8877}
train stats after 284096 examples: {'rewards_train/chosen': '0.13686', 'rewards_train/rejected': '-0.020139', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.157', 'logps_train/rejected': '-169.59', 'logps_train/chosen': '-155.05', 'loss/train': '0.64926', 'examples_per_second': '32.395', 'grad_norm': '105', 'counters/examples': 284096, 'counters/updates': 8878}
train stats after 284128 examples: {'rewards_train/chosen': '0.086512', 'rewards_train/rejected': '0.066687', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019825', 'logps_train/rejected': '-134.33', 'logps_train/chosen': '-134.89', 'loss/train': '0.70479', 'examples_per_second': '31.805', 'grad_norm': '99.5', 'counters/examples': 284128, 'counters/updates': 8879}
train stats after 284160 examples: {'rewards_train/chosen': '0.12638', 'rewards_train/rejected': '0.13394', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0075611', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-164.44', 'loss/train': '0.70329', 'examples_per_second': '31.914', 'grad_norm': '145', 'counters/examples': 284160, 'counters/updates': 8880}
train stats after 284192 examples: {'rewards_train/chosen': '0.085039', 'rewards_train/rejected': '0.082392', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0026469', 'logps_train/rejected': '-131.89', 'logps_train/chosen': '-141.74', 'loss/train': '0.70746', 'examples_per_second': '31.157', 'grad_norm': '118.5', 'counters/examples': 284192, 'counters/updates': 8881}
train stats after 284224 examples: {'rewards_train/chosen': '0.075388', 'rewards_train/rejected': '0.082417', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0070295', 'logps_train/rejected': '-171.79', 'logps_train/chosen': '-126.47', 'loss/train': '0.70536', 'examples_per_second': '31.635', 'grad_norm': '98.5', 'counters/examples': 284224, 'counters/updates': 8882}
train stats after 284256 examples: {'rewards_train/chosen': '0.072809', 'rewards_train/rejected': '0.0062858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066523', 'logps_train/rejected': '-91.4', 'logps_train/chosen': '-115.88', 'loss/train': '0.6748', 'examples_per_second': '30.139', 'grad_norm': '80.5', 'counters/examples': 284256, 'counters/updates': 8883}
train stats after 284288 examples: {'rewards_train/chosen': '0.051542', 'rewards_train/rejected': '0.13475', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.083209', 'logps_train/rejected': '-119.72', 'logps_train/chosen': '-179.31', 'loss/train': '0.75436', 'examples_per_second': '30.794', 'grad_norm': '148', 'counters/examples': 284288, 'counters/updates': 8884}
train stats after 284320 examples: {'rewards_train/chosen': '0.13145', 'rewards_train/rejected': '0.037366', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.09408', 'logps_train/rejected': '-117.58', 'logps_train/chosen': '-133.47', 'loss/train': '0.65672', 'examples_per_second': '31.7', 'grad_norm': '53.75', 'counters/examples': 284320, 'counters/updates': 8885}
train stats after 284352 examples: {'rewards_train/chosen': '0.19041', 'rewards_train/rejected': '0.10242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087986', 'logps_train/rejected': '-154.75', 'logps_train/chosen': '-136.14', 'loss/train': '0.66579', 'examples_per_second': '30.844', 'grad_norm': '63', 'counters/examples': 284352, 'counters/updates': 8886}
train stats after 284384 examples: {'rewards_train/chosen': '0.061619', 'rewards_train/rejected': '0.042545', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019074', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-177.93', 'loss/train': '0.70433', 'examples_per_second': '30.896', 'grad_norm': '180', 'counters/examples': 284384, 'counters/updates': 8887}
train stats after 284416 examples: {'rewards_train/chosen': '0.2281', 'rewards_train/rejected': '0.15514', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072967', 'logps_train/rejected': '-155.01', 'logps_train/chosen': '-125.87', 'loss/train': '0.69046', 'examples_per_second': '24.35', 'grad_norm': '162', 'counters/examples': 284416, 'counters/updates': 8888}
train stats after 284448 examples: {'rewards_train/chosen': '0.12107', 'rewards_train/rejected': '0.053838', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067228', 'logps_train/rejected': '-174.04', 'logps_train/chosen': '-161.89', 'loss/train': '0.67021', 'examples_per_second': '31.605', 'grad_norm': '66.5', 'counters/examples': 284448, 'counters/updates': 8889}
skipping logging after 284480 examples to avoid logging too frequently
train stats after 284512 examples: {'rewards_train/chosen': '0.078635', 'rewards_train/rejected': '0.090114', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011479', 'logps_train/rejected': '-104.28', 'logps_train/chosen': '-109.44', 'loss/train': '0.7184', 'examples_per_second': '24.065', 'grad_norm': '270', 'counters/examples': 284512, 'counters/updates': 8891}
train stats after 284544 examples: {'rewards_train/chosen': '0.19256', 'rewards_train/rejected': '0.071557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.121', 'logps_train/rejected': '-131.78', 'logps_train/chosen': '-143.65', 'loss/train': '0.66492', 'examples_per_second': '30.984', 'grad_norm': '64', 'counters/examples': 284544, 'counters/updates': 8892}
train stats after 284576 examples: {'rewards_train/chosen': '0.11692', 'rewards_train/rejected': '0.0058529', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11107', 'logps_train/rejected': '-98.851', 'logps_train/chosen': '-146.34', 'loss/train': '0.65251', 'examples_per_second': '31.37', 'grad_norm': '72.5', 'counters/examples': 284576, 'counters/updates': 8893}
train stats after 284608 examples: {'rewards_train/chosen': '0.12729', 'rewards_train/rejected': '0.062604', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064683', 'logps_train/rejected': '-156.61', 'logps_train/chosen': '-180.61', 'loss/train': '0.6712', 'examples_per_second': '31.385', 'grad_norm': '116.5', 'counters/examples': 284608, 'counters/updates': 8894}
train stats after 284640 examples: {'rewards_train/chosen': '0.12735', 'rewards_train/rejected': '0.046807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080546', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-136.19', 'loss/train': '0.66749', 'examples_per_second': '31.663', 'grad_norm': '67.5', 'counters/examples': 284640, 'counters/updates': 8895}
skipping logging after 284672 examples to avoid logging too frequently
train stats after 284704 examples: {'rewards_train/chosen': '0.2141', 'rewards_train/rejected': '0.10688', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10722', 'logps_train/rejected': '-135.89', 'logps_train/chosen': '-159.53', 'loss/train': '0.65516', 'examples_per_second': '31.6', 'grad_norm': '82', 'counters/examples': 284704, 'counters/updates': 8897}
skipping logging after 284736 examples to avoid logging too frequently
train stats after 284768 examples: {'rewards_train/chosen': '0.14143', 'rewards_train/rejected': '0.089975', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.05146', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-115.03', 'loss/train': '0.68602', 'examples_per_second': '31.645', 'grad_norm': '54.5', 'counters/examples': 284768, 'counters/updates': 8899}
train stats after 284800 examples: {'rewards_train/chosen': '0.070671', 'rewards_train/rejected': '0.074674', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0040027', 'logps_train/rejected': '-166.46', 'logps_train/chosen': '-138.91', 'loss/train': '0.70676', 'examples_per_second': '31.114', 'grad_norm': '81.5', 'counters/examples': 284800, 'counters/updates': 8900}
train stats after 284832 examples: {'rewards_train/chosen': '0.16747', 'rewards_train/rejected': '0.070233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097235', 'logps_train/rejected': '-144.05', 'logps_train/chosen': '-158.05', 'loss/train': '0.66311', 'examples_per_second': '31.956', 'grad_norm': '71', 'counters/examples': 284832, 'counters/updates': 8901}
train stats after 284864 examples: {'rewards_train/chosen': '0.067529', 'rewards_train/rejected': '0.066933', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.00059636', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-153.69', 'loss/train': '0.70274', 'examples_per_second': '31.413', 'grad_norm': '73.5', 'counters/examples': 284864, 'counters/updates': 8902}
train stats after 284896 examples: {'rewards_train/chosen': '0.12241', 'rewards_train/rejected': '0.098802', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023607', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-133.57', 'loss/train': '0.69604', 'examples_per_second': '32.985', 'grad_norm': '51.25', 'counters/examples': 284896, 'counters/updates': 8903}
train stats after 284928 examples: {'rewards_train/chosen': '0.15785', 'rewards_train/rejected': '0.07147', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.086377', 'logps_train/rejected': '-123.13', 'logps_train/chosen': '-152.82', 'loss/train': '0.66406', 'examples_per_second': '31.687', 'grad_norm': '63.25', 'counters/examples': 284928, 'counters/updates': 8904}
train stats after 284960 examples: {'rewards_train/chosen': '0.17904', 'rewards_train/rejected': '0.033257', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14579', 'logps_train/rejected': '-105.14', 'logps_train/chosen': '-123.37', 'loss/train': '0.63992', 'examples_per_second': '31.695', 'grad_norm': '91', 'counters/examples': 284960, 'counters/updates': 8905}
train stats after 284992 examples: {'rewards_train/chosen': '0.22959', 'rewards_train/rejected': '-0.023131', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25272', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-149.79', 'loss/train': '0.59698', 'examples_per_second': '32.225', 'grad_norm': '53.5', 'counters/examples': 284992, 'counters/updates': 8906}
train stats after 285024 examples: {'rewards_train/chosen': '0.057034', 'rewards_train/rejected': '-0.015172', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072205', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-133.54', 'loss/train': '0.67131', 'examples_per_second': '31.037', 'grad_norm': '145', 'counters/examples': 285024, 'counters/updates': 8907}
train stats after 285056 examples: {'rewards_train/chosen': '0.15449', 'rewards_train/rejected': '0.061627', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092859', 'logps_train/rejected': '-119.26', 'logps_train/chosen': '-178.98', 'loss/train': '0.66196', 'examples_per_second': '31.663', 'grad_norm': '64.5', 'counters/examples': 285056, 'counters/updates': 8908}
train stats after 285088 examples: {'rewards_train/chosen': '0.14792', 'rewards_train/rejected': '0.052427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095492', 'logps_train/rejected': '-139.97', 'logps_train/chosen': '-175.91', 'loss/train': '0.66548', 'examples_per_second': '31.595', 'grad_norm': '332', 'counters/examples': 285088, 'counters/updates': 8909}
train stats after 285120 examples: {'rewards_train/chosen': '0.20512', 'rewards_train/rejected': '0.012202', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19292', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-130.8', 'loss/train': '0.61386', 'examples_per_second': '29.942', 'grad_norm': '67', 'counters/examples': 285120, 'counters/updates': 8910}
train stats after 285152 examples: {'rewards_train/chosen': '0.16195', 'rewards_train/rejected': '-0.0093246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17127', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-152.01', 'loss/train': '0.63232', 'examples_per_second': '30.495', 'grad_norm': '93', 'counters/examples': 285152, 'counters/updates': 8911}
train stats after 285184 examples: {'rewards_train/chosen': '0.23323', 'rewards_train/rejected': '0.02998', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.20325', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-145.13', 'loss/train': '0.62395', 'examples_per_second': '31.378', 'grad_norm': '87', 'counters/examples': 285184, 'counters/updates': 8912}
train stats after 285216 examples: {'rewards_train/chosen': '0.14477', 'rewards_train/rejected': '0.0095279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13525', 'logps_train/rejected': '-134.29', 'logps_train/chosen': '-146.74', 'loss/train': '0.64165', 'examples_per_second': '31.723', 'grad_norm': '88.5', 'counters/examples': 285216, 'counters/updates': 8913}
train stats after 285248 examples: {'rewards_train/chosen': '0.044908', 'rewards_train/rejected': '0.054316', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0094084', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-129.64', 'loss/train': '0.7036', 'examples_per_second': '30.49', 'grad_norm': '53.75', 'counters/examples': 285248, 'counters/updates': 8914}
skipping logging after 285280 examples to avoid logging too frequently
train stats after 285312 examples: {'rewards_train/chosen': '0.16057', 'rewards_train/rejected': '0.035196', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12538', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-166.63', 'loss/train': '0.65808', 'examples_per_second': '31.65', 'grad_norm': '82', 'counters/examples': 285312, 'counters/updates': 8916}
train stats after 285344 examples: {'rewards_train/chosen': '0.20149', 'rewards_train/rejected': '0.080657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12084', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-160.11', 'loss/train': '0.66627', 'examples_per_second': '32.698', 'grad_norm': '72.5', 'counters/examples': 285344, 'counters/updates': 8917}
train stats after 285376 examples: {'rewards_train/chosen': '0.14493', 'rewards_train/rejected': '0.064445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080488', 'logps_train/rejected': '-157.55', 'logps_train/chosen': '-166.73', 'loss/train': '0.6697', 'examples_per_second': '31.659', 'grad_norm': '143', 'counters/examples': 285376, 'counters/updates': 8918}
train stats after 285408 examples: {'rewards_train/chosen': '0.19252', 'rewards_train/rejected': '0.02821', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16431', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-143.31', 'loss/train': '0.62873', 'examples_per_second': '32.654', 'grad_norm': '86.5', 'counters/examples': 285408, 'counters/updates': 8919}
train stats after 285440 examples: {'rewards_train/chosen': '0.10467', 'rewards_train/rejected': '0.16524', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.060573', 'logps_train/rejected': '-152.05', 'logps_train/chosen': '-175.29', 'loss/train': '0.75715', 'examples_per_second': '31.608', 'grad_norm': '165', 'counters/examples': 285440, 'counters/updates': 8920}
skipping logging after 285472 examples to avoid logging too frequently
train stats after 285504 examples: {'rewards_train/chosen': '0.078852', 'rewards_train/rejected': '0.11318', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034328', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-113.8', 'loss/train': '0.72201', 'examples_per_second': '33.516', 'grad_norm': '64.5', 'counters/examples': 285504, 'counters/updates': 8922}
train stats after 285536 examples: {'rewards_train/chosen': '0.16412', 'rewards_train/rejected': '0.062541', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10158', 'logps_train/rejected': '-165.34', 'logps_train/chosen': '-174.29', 'loss/train': '0.65628', 'examples_per_second': '30.451', 'grad_norm': '73.5', 'counters/examples': 285536, 'counters/updates': 8923}
train stats after 285568 examples: {'rewards_train/chosen': '0.14195', 'rewards_train/rejected': '0.045997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095953', 'logps_train/rejected': '-176.57', 'logps_train/chosen': '-163.71', 'loss/train': '0.65893', 'examples_per_second': '32.287', 'grad_norm': '66', 'counters/examples': 285568, 'counters/updates': 8924}
train stats after 285600 examples: {'rewards_train/chosen': '0.057076', 'rewards_train/rejected': '0.055277', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0017994', 'logps_train/rejected': '-103.05', 'logps_train/chosen': '-141.5', 'loss/train': '0.7028', 'examples_per_second': '32.352', 'grad_norm': '67.5', 'counters/examples': 285600, 'counters/updates': 8925}
train stats after 285632 examples: {'rewards_train/chosen': '0.17577', 'rewards_train/rejected': '-0.013108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18887', 'logps_train/rejected': '-142.49', 'logps_train/chosen': '-176.54', 'loss/train': '0.63375', 'examples_per_second': '31.715', 'grad_norm': '77', 'counters/examples': 285632, 'counters/updates': 8926}
skipping logging after 285664 examples to avoid logging too frequently
train stats after 285696 examples: {'rewards_train/chosen': '0.082807', 'rewards_train/rejected': '0.045709', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037098', 'logps_train/rejected': '-151.97', 'logps_train/chosen': '-183.01', 'loss/train': '0.69348', 'examples_per_second': '33.33', 'grad_norm': '230', 'counters/examples': 285696, 'counters/updates': 8928}
train stats after 285728 examples: {'rewards_train/chosen': '0.083432', 'rewards_train/rejected': '-0.058508', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14194', 'logps_train/rejected': '-112.04', 'logps_train/chosen': '-157.88', 'loss/train': '0.64529', 'examples_per_second': '31.61', 'grad_norm': '83.5', 'counters/examples': 285728, 'counters/updates': 8929}
train stats after 285760 examples: {'rewards_train/chosen': '0.13285', 'rewards_train/rejected': '0.080727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052121', 'logps_train/rejected': '-127.49', 'logps_train/chosen': '-160.42', 'loss/train': '0.67652', 'examples_per_second': '31.631', 'grad_norm': '84', 'counters/examples': 285760, 'counters/updates': 8930}
train stats after 285792 examples: {'rewards_train/chosen': '0.1188', 'rewards_train/rejected': '0.10486', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013938', 'logps_train/rejected': '-143.53', 'logps_train/chosen': '-135.56', 'loss/train': '0.70714', 'examples_per_second': '24.736', 'grad_norm': '184', 'counters/examples': 285792, 'counters/updates': 8931}
train stats after 285824 examples: {'rewards_train/chosen': '0.21452', 'rewards_train/rejected': '0.022207', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.19231', 'logps_train/rejected': '-101.29', 'logps_train/chosen': '-172.5', 'loss/train': '0.63067', 'examples_per_second': '31.95', 'grad_norm': '71', 'counters/examples': 285824, 'counters/updates': 8932}
train stats after 285856 examples: {'rewards_train/chosen': '0.13521', 'rewards_train/rejected': '0.077186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058025', 'logps_train/rejected': '-133.09', 'logps_train/chosen': '-157.11', 'loss/train': '0.67541', 'examples_per_second': '31.659', 'grad_norm': '74.5', 'counters/examples': 285856, 'counters/updates': 8933}
train stats after 285888 examples: {'rewards_train/chosen': '0.13287', 'rewards_train/rejected': '0.077969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054899', 'logps_train/rejected': '-154.36', 'logps_train/chosen': '-138.72', 'loss/train': '0.68102', 'examples_per_second': '30.724', 'grad_norm': '173', 'counters/examples': 285888, 'counters/updates': 8934}
train stats after 285920 examples: {'rewards_train/chosen': '0.12618', 'rewards_train/rejected': '0.020791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10538', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-156.88', 'loss/train': '0.65952', 'examples_per_second': '31.624', 'grad_norm': '87.5', 'counters/examples': 285920, 'counters/updates': 8935}
train stats after 285952 examples: {'rewards_train/chosen': '0.1914', 'rewards_train/rejected': '0.11406', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077336', 'logps_train/rejected': '-174.78', 'logps_train/chosen': '-171.86', 'loss/train': '0.67193', 'examples_per_second': '32.664', 'grad_norm': '67', 'counters/examples': 285952, 'counters/updates': 8936}
skipping logging after 285984 examples to avoid logging too frequently
train stats after 286016 examples: {'rewards_train/chosen': '0.11472', 'rewards_train/rejected': '0.079059', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035658', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-101.07', 'loss/train': '0.6807', 'examples_per_second': '30.562', 'grad_norm': '86', 'counters/examples': 286016, 'counters/updates': 8938}
train stats after 286048 examples: {'rewards_train/chosen': '0.1566', 'rewards_train/rejected': '0.057678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098922', 'logps_train/rejected': '-111.56', 'logps_train/chosen': '-150.94', 'loss/train': '0.66357', 'examples_per_second': '32.612', 'grad_norm': '76', 'counters/examples': 286048, 'counters/updates': 8939}
train stats after 286080 examples: {'rewards_train/chosen': '0.14097', 'rewards_train/rejected': '0.0058734', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1351', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-136.51', 'loss/train': '0.64053', 'examples_per_second': '31.663', 'grad_norm': '102.5', 'counters/examples': 286080, 'counters/updates': 8940}
train stats after 286112 examples: {'rewards_train/chosen': '0.20373', 'rewards_train/rejected': '0.047963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15577', 'logps_train/rejected': '-159.62', 'logps_train/chosen': '-168.02', 'loss/train': '0.64514', 'examples_per_second': '31.738', 'grad_norm': '254', 'counters/examples': 286112, 'counters/updates': 8941}
train stats after 286144 examples: {'rewards_train/chosen': '0.14626', 'rewards_train/rejected': '0.083779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062486', 'logps_train/rejected': '-113.73', 'logps_train/chosen': '-133.33', 'loss/train': '0.67325', 'examples_per_second': '30.247', 'grad_norm': '113', 'counters/examples': 286144, 'counters/updates': 8942}
train stats after 286176 examples: {'rewards_train/chosen': '0.12754', 'rewards_train/rejected': '0.060918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06662', 'logps_train/rejected': '-154.31', 'logps_train/chosen': '-125.26', 'loss/train': '0.67614', 'examples_per_second': '31.64', 'grad_norm': '69', 'counters/examples': 286176, 'counters/updates': 8943}
skipping logging after 286208 examples to avoid logging too frequently
train stats after 286240 examples: {'rewards_train/chosen': '0.081363', 'rewards_train/rejected': '0.10848', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027118', 'logps_train/rejected': '-128.67', 'logps_train/chosen': '-133.69', 'loss/train': '0.72405', 'examples_per_second': '31.103', 'grad_norm': '94', 'counters/examples': 286240, 'counters/updates': 8945}
train stats after 286272 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '0.13927', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0075609', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-151.34', 'loss/train': '0.70933', 'examples_per_second': '31.5', 'grad_norm': '117', 'counters/examples': 286272, 'counters/updates': 8946}
train stats after 286304 examples: {'rewards_train/chosen': '0.11936', 'rewards_train/rejected': '0.06522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054136', 'logps_train/rejected': '-164.85', 'logps_train/chosen': '-153.76', 'loss/train': '0.6879', 'examples_per_second': '30.968', 'grad_norm': '64', 'counters/examples': 286304, 'counters/updates': 8947}
train stats after 286336 examples: {'rewards_train/chosen': '0.19397', 'rewards_train/rejected': '0.033652', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16032', 'logps_train/rejected': '-118.01', 'logps_train/chosen': '-137.19', 'loss/train': '0.62755', 'examples_per_second': '32.472', 'grad_norm': '71', 'counters/examples': 286336, 'counters/updates': 8948}
train stats after 286368 examples: {'rewards_train/chosen': '0.20304', 'rewards_train/rejected': '0.074038', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.129', 'logps_train/rejected': '-106.39', 'logps_train/chosen': '-121.23', 'loss/train': '0.64499', 'examples_per_second': '31.829', 'grad_norm': '62.75', 'counters/examples': 286368, 'counters/updates': 8949}
train stats after 286400 examples: {'rewards_train/chosen': '0.044266', 'rewards_train/rejected': '-0.017555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061821', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-144.63', 'loss/train': '0.67367', 'examples_per_second': '31.619', 'grad_norm': '102', 'counters/examples': 286400, 'counters/updates': 8950}
train stats after 286432 examples: {'rewards_train/chosen': '0.12617', 'rewards_train/rejected': '0.11362', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012547', 'logps_train/rejected': '-171.82', 'logps_train/chosen': '-172.72', 'loss/train': '0.71358', 'examples_per_second': '31.389', 'grad_norm': '91', 'counters/examples': 286432, 'counters/updates': 8951}
skipping logging after 286464 examples to avoid logging too frequently
train stats after 286496 examples: {'rewards_train/chosen': '0.096528', 'rewards_train/rejected': '0.041916', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054612', 'logps_train/rejected': '-95.616', 'logps_train/chosen': '-133.61', 'loss/train': '0.67328', 'examples_per_second': '31.086', 'grad_norm': '62', 'counters/examples': 286496, 'counters/updates': 8953}
train stats after 286528 examples: {'rewards_train/chosen': '0.10102', 'rewards_train/rejected': '0.015405', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085616', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-132.84', 'loss/train': '0.66806', 'examples_per_second': '32.193', 'grad_norm': '73.5', 'counters/examples': 286528, 'counters/updates': 8954}
train stats after 286560 examples: {'rewards_train/chosen': '0.25403', 'rewards_train/rejected': '0.10711', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14693', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-187.94', 'loss/train': '0.66236', 'examples_per_second': '32.571', 'grad_norm': '116', 'counters/examples': 286560, 'counters/updates': 8955}
train stats after 286592 examples: {'rewards_train/chosen': '0.10215', 'rewards_train/rejected': '0.16972', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.067566', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-146.63', 'loss/train': '0.74507', 'examples_per_second': '31.603', 'grad_norm': '133', 'counters/examples': 286592, 'counters/updates': 8956}
train stats after 286624 examples: {'rewards_train/chosen': '0.14829', 'rewards_train/rejected': '0.10763', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040658', 'logps_train/rejected': '-130.85', 'logps_train/chosen': '-121.78', 'loss/train': '0.69119', 'examples_per_second': '31.619', 'grad_norm': '125', 'counters/examples': 286624, 'counters/updates': 8957}
train stats after 286656 examples: {'rewards_train/chosen': '0.13475', 'rewards_train/rejected': '0.1508', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016051', 'logps_train/rejected': '-148.58', 'logps_train/chosen': '-158.56', 'loss/train': '0.72482', 'examples_per_second': '31.621', 'grad_norm': '148', 'counters/examples': 286656, 'counters/updates': 8958}
train stats after 286688 examples: {'rewards_train/chosen': '0.14938', 'rewards_train/rejected': '0.016216', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13317', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-147.41', 'loss/train': '0.64322', 'examples_per_second': '31.751', 'grad_norm': '114', 'counters/examples': 286688, 'counters/updates': 8959}
skipping logging after 286720 examples to avoid logging too frequently
train stats after 286752 examples: {'rewards_train/chosen': '0.15223', 'rewards_train/rejected': '0.054243', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097983', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-176.76', 'loss/train': '0.66867', 'examples_per_second': '32.731', 'grad_norm': '71', 'counters/examples': 286752, 'counters/updates': 8961}
train stats after 286784 examples: {'rewards_train/chosen': '0.19892', 'rewards_train/rejected': '0.01594', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18298', 'logps_train/rejected': '-181.6', 'logps_train/chosen': '-187.98', 'loss/train': '0.63095', 'examples_per_second': '33.229', 'grad_norm': '258', 'counters/examples': 286784, 'counters/updates': 8962}
train stats after 286816 examples: {'rewards_train/chosen': '0.084921', 'rewards_train/rejected': '0.065931', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01899', 'logps_train/rejected': '-117.54', 'logps_train/chosen': '-126.22', 'loss/train': '0.69881', 'examples_per_second': '32.442', 'grad_norm': '55', 'counters/examples': 286816, 'counters/updates': 8963}
train stats after 286848 examples: {'rewards_train/chosen': '0.047732', 'rewards_train/rejected': '-0.0044925', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052224', 'logps_train/rejected': '-107.96', 'logps_train/chosen': '-126.26', 'loss/train': '0.67733', 'examples_per_second': '31.03', 'grad_norm': '108', 'counters/examples': 286848, 'counters/updates': 8964}
train stats after 286880 examples: {'rewards_train/chosen': '0.088619', 'rewards_train/rejected': '0.096793', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0081738', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-161.89', 'loss/train': '0.70313', 'examples_per_second': '31.264', 'grad_norm': '67.5', 'counters/examples': 286880, 'counters/updates': 8965}
train stats after 286912 examples: {'rewards_train/chosen': '0.055817', 'rewards_train/rejected': '0.061602', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0057847', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-191.91', 'loss/train': '0.72496', 'examples_per_second': '31.634', 'grad_norm': '77.5', 'counters/examples': 286912, 'counters/updates': 8966}
train stats after 286944 examples: {'rewards_train/chosen': '0.061511', 'rewards_train/rejected': '0.042128', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019383', 'logps_train/rejected': '-90.47', 'logps_train/chosen': '-143.65', 'loss/train': '0.69378', 'examples_per_second': '31.158', 'grad_norm': '68.5', 'counters/examples': 286944, 'counters/updates': 8967}
train stats after 286976 examples: {'rewards_train/chosen': '0.18363', 'rewards_train/rejected': '0.11387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069759', 'logps_train/rejected': '-151.2', 'logps_train/chosen': '-155.96', 'loss/train': '0.67872', 'examples_per_second': '30.636', 'grad_norm': '63.75', 'counters/examples': 286976, 'counters/updates': 8968}
train stats after 287008 examples: {'rewards_train/chosen': '0.12746', 'rewards_train/rejected': '0.086408', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041052', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-137.13', 'loss/train': '0.68426', 'examples_per_second': '32.562', 'grad_norm': '104.5', 'counters/examples': 287008, 'counters/updates': 8969}
skipping logging after 287040 examples to avoid logging too frequently
train stats after 287072 examples: {'rewards_train/chosen': '0.22178', 'rewards_train/rejected': '0.012979', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2088', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-180.75', 'loss/train': '0.60458', 'examples_per_second': '31.629', 'grad_norm': '90', 'counters/examples': 287072, 'counters/updates': 8971}
train stats after 287104 examples: {'rewards_train/chosen': '0.07837', 'rewards_train/rejected': '0.02295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055419', 'logps_train/rejected': '-125.52', 'logps_train/chosen': '-130.9', 'loss/train': '0.68139', 'examples_per_second': '31.608', 'grad_norm': '73', 'counters/examples': 287104, 'counters/updates': 8972}
skipping logging after 287136 examples to avoid logging too frequently
train stats after 287168 examples: {'rewards_train/chosen': '0.058508', 'rewards_train/rejected': '0.11814', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.059628', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-111.3', 'loss/train': '0.73305', 'examples_per_second': '31.592', 'grad_norm': '74', 'counters/examples': 287168, 'counters/updates': 8974}
train stats after 287200 examples: {'rewards_train/chosen': '0.19752', 'rewards_train/rejected': '0.12511', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072408', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-180.94', 'loss/train': '0.68203', 'examples_per_second': '31.698', 'grad_norm': '73.5', 'counters/examples': 287200, 'counters/updates': 8975}
skipping logging after 287232 examples to avoid logging too frequently
train stats after 287264 examples: {'rewards_train/chosen': '0.23443', 'rewards_train/rejected': '0.12089', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11354', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-133.39', 'loss/train': '0.65972', 'examples_per_second': '31.922', 'grad_norm': '71.5', 'counters/examples': 287264, 'counters/updates': 8977}
train stats after 287296 examples: {'rewards_train/chosen': '0.1414', 'rewards_train/rejected': '-0.0063041', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1477', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-187.89', 'loss/train': '0.63003', 'examples_per_second': '30.213', 'grad_norm': '67.5', 'counters/examples': 287296, 'counters/updates': 8978}
train stats after 287328 examples: {'rewards_train/chosen': '0.19603', 'rewards_train/rejected': '0.050327', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1457', 'logps_train/rejected': '-184.36', 'logps_train/chosen': '-167.48', 'loss/train': '0.6426', 'examples_per_second': '31.619', 'grad_norm': '90.5', 'counters/examples': 287328, 'counters/updates': 8979}
train stats after 287360 examples: {'rewards_train/chosen': '0.077096', 'rewards_train/rejected': '0.060107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016988', 'logps_train/rejected': '-157.85', 'logps_train/chosen': '-154.39', 'loss/train': '0.71069', 'examples_per_second': '31.758', 'grad_norm': '111.5', 'counters/examples': 287360, 'counters/updates': 8980}
train stats after 287392 examples: {'rewards_train/chosen': '0.14615', 'rewards_train/rejected': '0.025836', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12031', 'logps_train/rejected': '-112.01', 'logps_train/chosen': '-156.52', 'loss/train': '0.64598', 'examples_per_second': '30.24', 'grad_norm': '128', 'counters/examples': 287392, 'counters/updates': 8981}
train stats after 287424 examples: {'rewards_train/chosen': '0.1049', 'rewards_train/rejected': '0.093454', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011451', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-141.81', 'loss/train': '0.69792', 'examples_per_second': '33.076', 'grad_norm': '59.75', 'counters/examples': 287424, 'counters/updates': 8982}
skipping logging after 287456 examples to avoid logging too frequently
train stats after 287488 examples: {'rewards_train/chosen': '0.27786', 'rewards_train/rejected': '0.20693', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.070927', 'logps_train/rejected': '-169.51', 'logps_train/chosen': '-148.61', 'loss/train': '0.70069', 'examples_per_second': '31.566', 'grad_norm': '150', 'counters/examples': 287488, 'counters/updates': 8984}
train stats after 287520 examples: {'rewards_train/chosen': '0.2757', 'rewards_train/rejected': '0.12941', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14629', 'logps_train/rejected': '-164.82', 'logps_train/chosen': '-176.65', 'loss/train': '0.64675', 'examples_per_second': '32.536', 'grad_norm': '80', 'counters/examples': 287520, 'counters/updates': 8985}
train stats after 287552 examples: {'rewards_train/chosen': '0.11194', 'rewards_train/rejected': '0.19165', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.079709', 'logps_train/rejected': '-152.57', 'logps_train/chosen': '-164.49', 'loss/train': '0.75887', 'examples_per_second': '31.733', 'grad_norm': '155', 'counters/examples': 287552, 'counters/updates': 8986}
train stats after 287584 examples: {'rewards_train/chosen': '0.19355', 'rewards_train/rejected': '0.037061', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15649', 'logps_train/rejected': '-93.787', 'logps_train/chosen': '-133.67', 'loss/train': '0.62683', 'examples_per_second': '32.866', 'grad_norm': '58.5', 'counters/examples': 287584, 'counters/updates': 8987}
train stats after 287616 examples: {'rewards_train/chosen': '0.16221', 'rewards_train/rejected': '0.14363', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.018582', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-147.07', 'loss/train': '0.69473', 'examples_per_second': '31.845', 'grad_norm': '179', 'counters/examples': 287616, 'counters/updates': 8988}
train stats after 287648 examples: {'rewards_train/chosen': '0.088162', 'rewards_train/rejected': '0.099786', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.011624', 'logps_train/rejected': '-151.98', 'logps_train/chosen': '-152.46', 'loss/train': '0.70834', 'examples_per_second': '31.309', 'grad_norm': '62.25', 'counters/examples': 287648, 'counters/updates': 8989}
train stats after 287680 examples: {'rewards_train/chosen': '0.20614', 'rewards_train/rejected': '0.047061', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15908', 'logps_train/rejected': '-154.17', 'logps_train/chosen': '-157.1', 'loss/train': '0.64407', 'examples_per_second': '29.762', 'grad_norm': '74.5', 'counters/examples': 287680, 'counters/updates': 8990}
train stats after 287712 examples: {'rewards_train/chosen': '0.15957', 'rewards_train/rejected': '0.021066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13851', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-169.05', 'loss/train': '0.65201', 'examples_per_second': '31.612', 'grad_norm': '80', 'counters/examples': 287712, 'counters/updates': 8991}
skipping logging after 287744 examples to avoid logging too frequently
train stats after 287776 examples: {'rewards_train/chosen': '0.14616', 'rewards_train/rejected': '0.13122', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.014934', 'logps_train/rejected': '-137.57', 'logps_train/chosen': '-163.81', 'loss/train': '0.7088', 'examples_per_second': '31.597', 'grad_norm': '83.5', 'counters/examples': 287776, 'counters/updates': 8993}
skipping logging after 287808 examples to avoid logging too frequently
train stats after 287840 examples: {'rewards_train/chosen': '0.10271', 'rewards_train/rejected': '0.034881', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067826', 'logps_train/rejected': '-125.4', 'logps_train/chosen': '-125.06', 'loss/train': '0.66859', 'examples_per_second': '38.633', 'grad_norm': '69', 'counters/examples': 287840, 'counters/updates': 8995}
train stats after 287872 examples: {'rewards_train/chosen': '0.15564', 'rewards_train/rejected': '0.10942', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046226', 'logps_train/rejected': '-123.41', 'logps_train/chosen': '-159.33', 'loss/train': '0.68566', 'examples_per_second': '32.176', 'grad_norm': '68.5', 'counters/examples': 287872, 'counters/updates': 8996}
train stats after 287904 examples: {'rewards_train/chosen': '0.14109', 'rewards_train/rejected': '0.045518', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095576', 'logps_train/rejected': '-114.76', 'logps_train/chosen': '-175.48', 'loss/train': '0.66038', 'examples_per_second': '30.552', 'grad_norm': '91', 'counters/examples': 287904, 'counters/updates': 8997}
train stats after 287936 examples: {'rewards_train/chosen': '0.13182', 'rewards_train/rejected': '0.079331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052487', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-128.37', 'loss/train': '0.67802', 'examples_per_second': '32.903', 'grad_norm': '55.75', 'counters/examples': 287936, 'counters/updates': 8998}
train stats after 287968 examples: {'rewards_train/chosen': '0.19489', 'rewards_train/rejected': '0.051879', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14301', 'logps_train/rejected': '-165.04', 'logps_train/chosen': '-182.27', 'loss/train': '0.63472', 'examples_per_second': '31.607', 'grad_norm': '105.5', 'counters/examples': 287968, 'counters/updates': 8999}
train stats after 288000 examples: {'rewards_train/chosen': '0.20165', 'rewards_train/rejected': '0.078301', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12335', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-134.49', 'loss/train': '0.64709', 'examples_per_second': '31.894', 'grad_norm': '91', 'counters/examples': 288000, 'counters/updates': 9000}
Running evaluation after 288000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|▋         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|█▎        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|█▉        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|██▌       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|███▏      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|███▊      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|████▍     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|█████     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|█████▋    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|██████▎   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|██████▉   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|███████▌  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|████████▏ | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|████████▊ | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|█████████▍| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|██████████| 16/16 [00:04<00:00,  3.91it/s]
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
eval after 288000: {'rewards_eval/chosen': '0.16186', 'rewards_eval/rejected': '0.053974', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.10789', 'logps_eval/rejected': '-127.6', 'logps_eval/chosen': '-149.39', 'loss/eval': '0.65993'}
skipping save for non epoch
train stats after 288032 examples: {'rewards_train/chosen': '0.21702', 'rewards_train/rejected': '0.099524', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11749', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-171.74', 'loss/train': '0.6525', 'examples_per_second': '31.68', 'grad_norm': '126.5', 'counters/examples': 288032, 'counters/updates': 9001}
train stats after 288064 examples: {'rewards_train/chosen': '0.11273', 'rewards_train/rejected': '0.030986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081745', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-141.41', 'loss/train': '0.6659', 'examples_per_second': '30.02', 'grad_norm': '53', 'counters/examples': 288064, 'counters/updates': 9002}
train stats after 288096 examples: {'rewards_train/chosen': '0.16584', 'rewards_train/rejected': '-0.0087369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17457', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-157.04', 'loss/train': '0.62608', 'examples_per_second': '32.315', 'grad_norm': '63.25', 'counters/examples': 288096, 'counters/updates': 9003}
train stats after 288128 examples: {'rewards_train/chosen': '0.1188', 'rewards_train/rejected': '0.18464', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.065839', 'logps_train/rejected': '-131.18', 'logps_train/chosen': '-182.45', 'loss/train': '0.74263', 'examples_per_second': '32.49', 'grad_norm': '91', 'counters/examples': 288128, 'counters/updates': 9004}
skipping logging after 288160 examples to avoid logging too frequently
train stats after 288192 examples: {'rewards_train/chosen': '0.13257', 'rewards_train/rejected': '0.10322', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029345', 'logps_train/rejected': '-152.34', 'logps_train/chosen': '-189.41', 'loss/train': '0.70166', 'examples_per_second': '32.357', 'grad_norm': '340', 'counters/examples': 288192, 'counters/updates': 9006}
train stats after 288224 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.12767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015959', 'logps_train/rejected': '-139.68', 'logps_train/chosen': '-177.57', 'loss/train': '0.70053', 'examples_per_second': '32.342', 'grad_norm': '86.5', 'counters/examples': 288224, 'counters/updates': 9007}
skipping logging after 288256 examples to avoid logging too frequently
train stats after 288288 examples: {'rewards_train/chosen': '0.14058', 'rewards_train/rejected': '0.16941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.028834', 'logps_train/rejected': '-140.33', 'logps_train/chosen': '-165.34', 'loss/train': '0.72049', 'examples_per_second': '30.971', 'grad_norm': '85', 'counters/examples': 288288, 'counters/updates': 9009}
train stats after 288320 examples: {'rewards_train/chosen': '0.11864', 'rewards_train/rejected': '0.029113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089532', 'logps_train/rejected': '-133.38', 'logps_train/chosen': '-167.16', 'loss/train': '0.65998', 'examples_per_second': '30.742', 'grad_norm': '63.25', 'counters/examples': 288320, 'counters/updates': 9010}
train stats after 288352 examples: {'rewards_train/chosen': '0.14264', 'rewards_train/rejected': '0.050646', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091997', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-144.98', 'loss/train': '0.65607', 'examples_per_second': '30.028', 'grad_norm': '92.5', 'counters/examples': 288352, 'counters/updates': 9011}
skipping logging after 288384 examples to avoid logging too frequently
train stats after 288416 examples: {'rewards_train/chosen': '0.15167', 'rewards_train/rejected': '0.047339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10433', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-191.35', 'loss/train': '0.65879', 'examples_per_second': '29.992', 'grad_norm': '71', 'counters/examples': 288416, 'counters/updates': 9013}
train stats after 288448 examples: {'rewards_train/chosen': '0.1805', 'rewards_train/rejected': '0.12906', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051441', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-151.2', 'loss/train': '0.6986', 'examples_per_second': '32.362', 'grad_norm': '356', 'counters/examples': 288448, 'counters/updates': 9014}
train stats after 288480 examples: {'rewards_train/chosen': '0.088351', 'rewards_train/rejected': '0.00086798', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087483', 'logps_train/rejected': '-120.91', 'logps_train/chosen': '-165.12', 'loss/train': '0.6679', 'examples_per_second': '29.776', 'grad_norm': '65', 'counters/examples': 288480, 'counters/updates': 9015}
train stats after 288512 examples: {'rewards_train/chosen': '0.23771', 'rewards_train/rejected': '0.076966', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16075', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-139.67', 'loss/train': '0.63394', 'examples_per_second': '30.963', 'grad_norm': '91.5', 'counters/examples': 288512, 'counters/updates': 9016}
train stats after 288544 examples: {'rewards_train/chosen': '0.11256', 'rewards_train/rejected': '0.025104', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087453', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-159.69', 'loss/train': '0.66905', 'examples_per_second': '31.086', 'grad_norm': '114.5', 'counters/examples': 288544, 'counters/updates': 9017}
train stats after 288576 examples: {'rewards_train/chosen': '0.20006', 'rewards_train/rejected': '0.066991', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13307', 'logps_train/rejected': '-157.3', 'logps_train/chosen': '-175.21', 'loss/train': '0.64658', 'examples_per_second': '31.362', 'grad_norm': '86.5', 'counters/examples': 288576, 'counters/updates': 9018}
skipping logging after 288608 examples to avoid logging too frequently
train stats after 288640 examples: {'rewards_train/chosen': '0.20884', 'rewards_train/rejected': '0.095794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11305', 'logps_train/rejected': '-149.7', 'logps_train/chosen': '-181.11', 'loss/train': '0.67468', 'examples_per_second': '31.966', 'grad_norm': '94.5', 'counters/examples': 288640, 'counters/updates': 9020}
train stats after 288672 examples: {'rewards_train/chosen': '0.21169', 'rewards_train/rejected': '0.088058', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12363', 'logps_train/rejected': '-120.39', 'logps_train/chosen': '-159.33', 'loss/train': '0.64204', 'examples_per_second': '31.607', 'grad_norm': '74', 'counters/examples': 288672, 'counters/updates': 9021}
train stats after 288704 examples: {'rewards_train/chosen': '0.25275', 'rewards_train/rejected': '0.092842', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15991', 'logps_train/rejected': '-142.14', 'logps_train/chosen': '-159.77', 'loss/train': '0.63969', 'examples_per_second': '31.376', 'grad_norm': '66', 'counters/examples': 288704, 'counters/updates': 9022}
skipping logging after 288736 examples to avoid logging too frequently
train stats after 288768 examples: {'rewards_train/chosen': '0.14201', 'rewards_train/rejected': '0.0043129', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1377', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-167.86', 'loss/train': '0.64514', 'examples_per_second': '30.962', 'grad_norm': '109.5', 'counters/examples': 288768, 'counters/updates': 9024}
Finished generating 3 epochs on train split
writing checkpoint to .cache/laura/pythia2.8b_sfted3_dpo3_seed0_2024-03-19_00-55-46_773716/LATEST/policy.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:         counters/updates ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:      examples_per_second ▂▇▇▅▂▄▅▄▅█▃▅▄▃▃▅▄▂▃▇▅▆▅█▅▆▂▂▂▅▅▄▄▆▅▁▂▅▂▅
wandb:                grad_norm ▁▁▃█▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁
wandb:        logps_eval/chosen ▁▂▁▂▃▄▄▄▄▅▅▆▆▆▇▆▆▆▆▇▇▆███
wandb:      logps_eval/rejected ▁▄▃▄▃▅▅▅▆▆▆█▆▇█▇▇▆▇▇▇▇█▇▇
wandb:       logps_train/chosen ▄▆▅▄▅▃▅▃▅▆▆█▄▆█▁▄▃▂▆▅▄▅▆▆▆▅▄▆▆▄▄▆▅▅▅▄▆▅▇
wandb:     logps_train/rejected ▅▇▃▂▄▇▅▅█▆▆▇▆▇▆▃▄▄▃▇▅▂▅▅▅▁▅▅▄▃▄▂▆▆▁▄▂▅▄▇
wandb:                loss/eval ▆█▇█▅▅▅▄▅▄▅▅▃▄▃▄▃▃▃▃▂▄▂▁▁
wandb:               loss/train ▄▅▆▆▄▁█▃▅▅▆▃▃▅▃▅▄▅▂▆▄▄▃▃▅▆▂▄▄▅▂▇▂▄▅▅▅▃▄▂
wandb:  rewards_eval/accuracies ▂▁▁▃▅▄▂▄▂▄▄▄▆▆▇▆▄▄▆▆▆▄█▇▇
wandb:      rewards_eval/chosen ▁▂▁▂▃▄▄▄▄▅▅▆▆▆▇▆▆▆▆▇▇▆███
wandb:     rewards_eval/margins ▂▁▂▁▄▃▃▄▃▄▃▄▅▅▆▅▆▅▆▇▇▅▇██
wandb:    rewards_eval/rejected ▁▄▃▄▃▅▅▅▆▆▆█▆▇█▇▇▆▇▇▇▇█▇▇
wandb: rewards_train/accuracies ▆▄▆▃▇█▁▆▅▄▃▆█▄▆▆▅▅▆▂▆▆▅▇▂▃▇▆▅▃▇▂▇▅▅▅▄▄▆█
wandb:     rewards_train/chosen ▁▂▂▃▃▆▂▄▂▁▂▄▅▃▅▄▄▃▇▄▆▅▃▆▅▅▅▅▃▂█▆▅▅▄▇▆█▄█
wandb:    rewards_train/margins ▄▄▃▃▅█▁▆▄▄▂▅▆▃▅▄▅▃▇▃▅▅▅▅▃▃▆▅▄▄▆▂▇▅▃▅▃▆▅▇
wandb:   rewards_train/rejected ▁▂▃▄▁▁▆▂▂▁▃▂▂▄▃▄▃▄▃▆▅▃▂▄▆▆▂▃▃▂▄█▁▄▅▆▇▅▃▄
wandb: 
wandb: Run summary:
wandb:        counters/examples 288768
wandb:         counters/updates 9024
wandb:      examples_per_second 30.96238
wandb:                grad_norm 109.5
wandb:        logps_eval/chosen -149.3873
wandb:      logps_eval/rejected -127.60349
wandb:       logps_train/chosen -167.85691
wandb:     logps_train/rejected -131.41926
wandb:                loss/eval 0.65993
wandb:               loss/train 0.64514
wandb:  rewards_eval/accuracies 0.58594
wandb:      rewards_eval/chosen 0.16186
wandb:     rewards_eval/margins 0.10789
wandb:    rewards_eval/rejected 0.05397
wandb: rewards_train/accuracies 0.5625
wandb:     rewards_train/chosen 0.14201
wandb:    rewards_train/margins 0.1377
wandb:   rewards_train/rejected 0.00431
wandb: 
wandb: 🚀 View run pythia2.8b_sfted3_dpo3_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/jlqomxa2
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240319_005747-jlqomxa2/logs
