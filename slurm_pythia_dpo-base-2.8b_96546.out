no FSDP port specified; using open port for FSDP: 48791
seed: 0
exp_name: pythia2.8b_sfted2_dpo3_seed0
batch_size: 32
eval_batch_size: 16
debug: false
fsdp_port: 48791
datasets:
- hh_static
wandb:
  enabled: true
  entity: lauraomahony999
  project: pythia-dpo
local_dirs:
- /scr-ssd
- /scr
- .cache
sample_during_eval: false
n_eval_model_samples: 16
do_first_eval: true
local_run_dir: .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349
lr: 4.0e-07
gradient_accumulation_steps: 2
max_grad_norm: 10.0
max_length: 512
max_prompt_length: 256
n_epochs: 3
n_examples: null
n_eval_examples: 256
trainer: FSDPTrainer
optimizer: RMSprop
warmup_steps: 150
activation_checkpointing: false
eval_every: 12000
minimum_log_interval_secs: 1.0
revision: epoch2-12000
model:
  name_or_path: lomahony/pythia-2.8b-helpful-sft-3epochs
  tokenizer_name_or_path: null
  archive: null
  block_name: GPTNeoXLayer
  policy_dtype: bfloat16
  fsdp_policy_mp: null
  reference_dtype: float16
loss:
  name: dpo
  beta: 0.1
  label_smoothing: 0
  reference_free: false

================================================================================
Writing to ip-10-0-197-210:.cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349
================================================================================
building policy
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.67it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.33it/s]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.25.attention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
building reference model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:24<00:24, 24.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:27<00:00, 12.04s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:27<00:00, 13.98s/it]
Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at lomahony/pythia-2.8b-helpful-sft-3epochs and are newly initialized: ['gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.19.attention.bias', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.27.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'gpt_neox.layers.13.attention.masked_bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.25.attention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
starting 8 processes for FSDP training
setting RLIMIT_NOFILE soft limit to 131072 from 8192
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: lauraomahony999. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in .cache/laura/wandb/run-20240319_005749-7t93d6jx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pythia2.8b_sfted2_dpo3_seed0
wandb: â­ï¸ View project at https://wandb.ai/lauraomahony999/pythia-dpo
wandb: ðŸš€ View run at https://wandb.ai/lauraomahony999/pythia-dpo/runs/7t93d6jx
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
0 initializing distributed
Creating trainer on process 0 with world size 8
Loading tokenizer lomahony/pythia-2.8b-helpful-sft-3epochs
Loaded train data iterator
Loading HH static dataset (test split) from Huggingface...
done
Processing HH static:   0%|          | 0/5103 [00:00<?, ?it/s]Processing HH static:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2145/5103 [00:00<00:00, 21442.45it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 4381/5103 [00:00<00:00, 21976.27it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5103/5103 [00:00<00:00, 21824.06it/s]
FINISHED 256 EXAMPLES on test split
Loaded 16 eval batches of size 16
Sharding policy...
Sharding reference model...
Loaded model on rank 0
Using RMSprop optimizer
Loading HH static dataset (train split) from Huggingface...
done
Processing HH static:   0%|          | 0/96256 [00:00<?, ?it/s]Processing HH static:   1%|          | 487/96256 [00:00<00:41, 2329.16it/s]Processing HH static:   3%|â–Ž         | 2753/96256 [00:00<00:08, 10580.21it/s]Processing HH static:   5%|â–Œ         | 5055/96256 [00:00<00:06, 15144.01it/s]Processing HH static:   8%|â–Š         | 7352/96256 [00:00<00:04, 17832.91it/s]Processing HH static:  10%|â–‰         | 9605/96256 [00:00<00:04, 19379.61it/s]Processing HH static:  12%|â–ˆâ–        | 11880/96256 [00:00<00:04, 20457.35it/s]Processing HH static:  15%|â–ˆâ–        | 14100/96256 [00:00<00:03, 21003.05it/s]Processing HH static:  17%|â–ˆâ–‹        | 16326/96256 [00:00<00:03, 21389.56it/s]Processing HH static:  19%|â–ˆâ–‰        | 18591/96256 [00:01<00:03, 21773.04it/s]Processing HH static:  22%|â–ˆâ–ˆâ–       | 20841/96256 [00:01<00:03, 21992.74it/s]Processing HH static:  24%|â–ˆâ–ˆâ–       | 23099/96256 [00:01<00:03, 22170.37it/s]Processing HH static:  26%|â–ˆâ–ˆâ–‹       | 25352/96256 [00:01<00:03, 22277.91it/s]Processing HH static:  29%|â–ˆâ–ˆâ–Š       | 27607/96256 [00:01<00:03, 22358.19it/s]Processing HH static:  31%|â–ˆâ–ˆâ–ˆ       | 29853/96256 [00:01<00:02, 22388.20it/s]Processing HH static:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 32099/96256 [00:01<00:02, 22308.81it/s]Processing HH static:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34335/96256 [00:01<00:04, 14000.11it/s]Processing HH static:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 36572/96256 [00:02<00:03, 15770.68it/s]Processing HH static:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38827/96256 [00:02<00:03, 17343.82it/s]Processing HH static:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41097/96256 [00:02<00:02, 18675.80it/s]Processing HH static:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43374/96256 [00:02<00:02, 19748.30it/s]Processing HH static:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45564/96256 [00:02<00:02, 20334.40it/s]Processing HH static:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47765/96256 [00:02<00:02, 20802.74it/s]Processing HH static:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49939/96256 [00:02<00:02, 20981.43it/s]Processing HH static:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52145/96256 [00:02<00:02, 21290.45it/s]Processing HH static:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54355/96256 [00:02<00:01, 21526.11it/s]Processing HH static:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56566/96256 [00:02<00:01, 21695.61it/s]Processing HH static:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58763/96256 [00:03<00:01, 21774.37it/s]Processing HH static:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60958/96256 [00:03<00:01, 21813.95it/s]Processing HH static:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 63170/96256 [00:03<00:01, 21903.40it/s]Processing HH static:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65403/96256 [00:03<00:01, 22027.30it/s]Processing HH static:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67623/96256 [00:03<00:01, 22076.96it/s]Processing HH static:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69848/96256 [00:03<00:01, 22127.56it/s]Processing HH static:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72064/96256 [00:03<00:01, 22020.26it/s]Processing HH static:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 74269/96256 [00:03<00:01, 14222.97it/s]Processing HH static:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 76045/96256 [00:04<00:01, 12162.64it/s]Processing HH static:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77545/96256 [00:04<00:02, 6793.48it/s] Processing HH static:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78679/96256 [00:05<00:03, 5067.61it/s]Processing HH static:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79549/96256 [00:05<00:03, 4402.93it/s]Processing HH static:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 80241/96256 [00:05<00:03, 4095.76it/s]Processing HH static:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80818/96256 [00:05<00:03, 3933.77it/s]Processing HH static:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81321/96256 [00:05<00:03, 4060.59it/s]Processing HH static:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81817/96256 [00:06<00:03, 4143.62it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 82300/96256 [00:06<00:03, 3797.53it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 82725/96256 [00:06<00:03, 3398.15it/s]Processing HH static:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83096/96256 [00:06<00:04, 3198.20it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83435/96256 [00:06<00:04, 3051.45it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83751/96256 [00:06<00:04, 2917.38it/s]Processing HH static:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 84057/96256 [00:06<00:04, 2948.62it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84357/96256 [00:06<00:04, 2872.51it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84710/96256 [00:07<00:03, 3036.71it/s]Processing HH static:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 85096/96256 [00:07<00:03, 3248.95it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 85427/96256 [00:07<00:03, 3025.24it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85736/96256 [00:07<00:03, 2874.22it/s]Processing HH static:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86028/96256 [00:07<00:03, 2802.89it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86312/96256 [00:07<00:03, 2777.27it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 86592/96256 [00:07<00:03, 2746.46it/s]Processing HH static:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86871/96256 [00:07<00:03, 2748.94it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87147/96256 [00:07<00:03, 2730.54it/s]Processing HH static:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 87719/96256 [00:08<00:02, 3588.95it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88121/96256 [00:08<00:02, 3708.19it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88496/96256 [00:08<00:02, 3207.03it/s]Processing HH static:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 88831/96256 [00:08<00:02, 3013.58it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89144/96256 [00:08<00:02, 2884.98it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89462/96256 [00:08<00:02, 2958.88it/s]Processing HH static:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89912/96256 [00:08<00:01, 3370.85it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90258/96256 [00:08<00:01, 3148.84it/s]Processing HH static:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90582/96256 [00:08<00:01, 3001.74it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91085/96256 [00:09<00:01, 3535.39it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91450/96256 [00:09<00:01, 3270.42it/s]Processing HH static:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91788/96256 [00:09<00:01, 3123.05it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 92108/96256 [00:09<00:01, 2973.85it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 92411/96256 [00:09<00:01, 2838.50it/s]Processing HH static:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92699/96256 [00:09<00:01, 2758.00it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92978/96256 [00:09<00:01, 2675.65it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93247/96256 [00:09<00:01, 2633.50it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93512/96256 [00:10<00:01, 2626.38it/s]Processing HH static:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 93785/96256 [00:10<00:00, 2650.61it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94060/96256 [00:10<00:00, 2678.82it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94347/96256 [00:10<00:00, 2732.36it/s]Processing HH static:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 94727/96256 [00:10<00:00, 3043.36it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95075/96256 [00:10<00:00, 3168.51it/s]Processing HH static:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95393/96256 [00:10<00:00, 3004.42it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 95929/96256 [00:10<00:00, 3676.05it/s]Processing HH static: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96256/96256 [00:10<00:00, 8872.78it/s]
Running evaluation after 0 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:01<00:27,  1.85s/it]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:02<00:13,  1.07it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:02<00:08,  1.60it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:05,  2.07it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:02<00:04,  2.44it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:03<00:03,  2.88it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:03<00:02,  3.09it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:03<00:02,  3.25it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:03<00:02,  3.44it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:04<00:01,  3.49it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:04<00:01,  3.57it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:04<00:01,  3.65it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:05<00:00,  3.72it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:05<00:00,  3.68it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:05<00:00,  3.75it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  3.70it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.74it/s]
eval after 0: {'rewards_eval/chosen': '0.0019937', 'rewards_eval/rejected': '-0.0048612', 'rewards_eval/accuracies': '0.48047', 'rewards_eval/margins': '0.0068548', 'logps_eval/rejected': '-122.18', 'logps_eval/chosen': '-144.09', 'loss/eval': '0.69604'}
train stats after 32 examples: {'rewards_train/chosen': '-0.033441', 'rewards_train/rejected': '-0.023921', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0095202', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-134.11', 'loss/train': '0.70159', 'examples_per_second': '22.848', 'grad_norm': '33', 'counters/examples': 32, 'counters/updates': 1}
train stats after 64 examples: {'rewards_train/chosen': '-0.041947', 'rewards_train/rejected': '-0.048838', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0068908', 'logps_train/rejected': '-165.02', 'logps_train/chosen': '-154.21', 'loss/train': '0.71316', 'examples_per_second': '28.851', 'grad_norm': '64.5', 'counters/examples': 64, 'counters/updates': 2}
train stats after 96 examples: {'rewards_train/chosen': '-0.013936', 'rewards_train/rejected': '0.024495', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038431', 'logps_train/rejected': '-120.45', 'logps_train/chosen': '-118.91', 'loss/train': '0.71502', 'examples_per_second': '31.285', 'grad_norm': '31.5', 'counters/examples': 96, 'counters/updates': 3}
train stats after 128 examples: {'rewards_train/chosen': '-0.069338', 'rewards_train/rejected': '-0.055248', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.014089', 'logps_train/rejected': '-155.17', 'logps_train/chosen': '-182.45', 'loss/train': '0.70958', 'examples_per_second': '31.488', 'grad_norm': '43', 'counters/examples': 128, 'counters/updates': 4}
train stats after 160 examples: {'rewards_train/chosen': '-0.044698', 'rewards_train/rejected': '-0.012244', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032453', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-159.37', 'loss/train': '0.71735', 'examples_per_second': '32.213', 'grad_norm': '43', 'counters/examples': 160, 'counters/updates': 5}
train stats after 192 examples: {'rewards_train/chosen': '0.030444', 'rewards_train/rejected': '-0.030157', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060601', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-125.59', 'loss/train': '0.66722', 'examples_per_second': '31.47', 'grad_norm': '29.375', 'counters/examples': 192, 'counters/updates': 6}
train stats after 224 examples: {'rewards_train/chosen': '-0.012555', 'rewards_train/rejected': '-0.020445', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0078897', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-137.01', 'loss/train': '0.69492', 'examples_per_second': '31.14', 'grad_norm': '40.5', 'counters/examples': 224, 'counters/updates': 7}
train stats after 256 examples: {'rewards_train/chosen': '0.066429', 'rewards_train/rejected': '0.010067', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056361', 'logps_train/rejected': '-157.33', 'logps_train/chosen': '-195.35', 'loss/train': '0.67332', 'examples_per_second': '31.529', 'grad_norm': '61.75', 'counters/examples': 256, 'counters/updates': 8}
train stats after 288 examples: {'rewards_train/chosen': '0.037326', 'rewards_train/rejected': '0.042794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054673', 'logps_train/rejected': '-154.66', 'logps_train/chosen': '-130.53', 'loss/train': '0.70188', 'examples_per_second': '32.215', 'grad_norm': '32.75', 'counters/examples': 288, 'counters/updates': 9}
train stats after 320 examples: {'rewards_train/chosen': '-0.028782', 'rewards_train/rejected': '-0.046814', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018032', 'logps_train/rejected': '-140.66', 'logps_train/chosen': '-163.04', 'loss/train': '0.68692', 'examples_per_second': '30.042', 'grad_norm': '35', 'counters/examples': 320, 'counters/updates': 10}
train stats after 352 examples: {'rewards_train/chosen': '0.052726', 'rewards_train/rejected': '0.050328', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0023985', 'logps_train/rejected': '-144.57', 'logps_train/chosen': '-133.41', 'loss/train': '0.70882', 'examples_per_second': '31.835', 'grad_norm': '38.75', 'counters/examples': 352, 'counters/updates': 11}
skipping logging after 384 examples to avoid logging too frequently
train stats after 416 examples: {'rewards_train/chosen': '0.048351', 'rewards_train/rejected': '-0.032793', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081143', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-162.4', 'loss/train': '0.66165', 'examples_per_second': '30.977', 'grad_norm': '44.5', 'counters/examples': 416, 'counters/updates': 13}
train stats after 448 examples: {'rewards_train/chosen': '-0.019355', 'rewards_train/rejected': '-0.032108', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012754', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-155.91', 'loss/train': '0.68948', 'examples_per_second': '31.541', 'grad_norm': '28.375', 'counters/examples': 448, 'counters/updates': 14}
skipping logging after 480 examples to avoid logging too frequently
train stats after 512 examples: {'rewards_train/chosen': '-0.0015305', 'rewards_train/rejected': '0.046587', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.048118', 'logps_train/rejected': '-156.2', 'logps_train/chosen': '-152.63', 'loss/train': '0.72426', 'examples_per_second': '33.864', 'grad_norm': '39.75', 'counters/examples': 512, 'counters/updates': 16}
train stats after 544 examples: {'rewards_train/chosen': '0.017907', 'rewards_train/rejected': '-0.0099998', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027907', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-156.3', 'loss/train': '0.68358', 'examples_per_second': '31.523', 'grad_norm': '31.125', 'counters/examples': 544, 'counters/updates': 17}
train stats after 576 examples: {'rewards_train/chosen': '-0.073891', 'rewards_train/rejected': '-0.013338', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.060553', 'logps_train/rejected': '-125.97', 'logps_train/chosen': '-178.71', 'loss/train': '0.73642', 'examples_per_second': '31.518', 'grad_norm': '58.75', 'counters/examples': 576, 'counters/updates': 18}
train stats after 608 examples: {'rewards_train/chosen': '0.049912', 'rewards_train/rejected': '-0.030626', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.080538', 'logps_train/rejected': '-121.59', 'logps_train/chosen': '-139.59', 'loss/train': '0.66079', 'examples_per_second': '32.274', 'grad_norm': '32.5', 'counters/examples': 608, 'counters/updates': 19}
train stats after 640 examples: {'rewards_train/chosen': '-0.019984', 'rewards_train/rejected': '0.0086388', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.028622', 'logps_train/rejected': '-154.35', 'logps_train/chosen': '-160.04', 'loss/train': '0.71782', 'examples_per_second': '30.679', 'grad_norm': '142', 'counters/examples': 640, 'counters/updates': 20}
train stats after 672 examples: {'rewards_train/chosen': '-0.019569', 'rewards_train/rejected': '0.070891', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.09046', 'logps_train/rejected': '-136.54', 'logps_train/chosen': '-164.12', 'loss/train': '0.74952', 'examples_per_second': '32.707', 'grad_norm': '55', 'counters/examples': 672, 'counters/updates': 21}
skipping logging after 704 examples to avoid logging too frequently
train stats after 736 examples: {'rewards_train/chosen': '0.033238', 'rewards_train/rejected': '0.01178', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021458', 'logps_train/rejected': '-109.57', 'logps_train/chosen': '-123.93', 'loss/train': '0.68613', 'examples_per_second': '32.904', 'grad_norm': '28.875', 'counters/examples': 736, 'counters/updates': 23}
skipping logging after 768 examples to avoid logging too frequently
train stats after 800 examples: {'rewards_train/chosen': '-0.036979', 'rewards_train/rejected': '-0.010998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.025981', 'logps_train/rejected': '-129.73', 'logps_train/chosen': '-202.78', 'loss/train': '0.73403', 'examples_per_second': '30.325', 'grad_norm': '65', 'counters/examples': 800, 'counters/updates': 25}
train stats after 832 examples: {'rewards_train/chosen': '-0.024959', 'rewards_train/rejected': '0.029', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.053959', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-131.46', 'loss/train': '0.72268', 'examples_per_second': '31.244', 'grad_norm': '30.125', 'counters/examples': 832, 'counters/updates': 26}
train stats after 864 examples: {'rewards_train/chosen': '-0.0071865', 'rewards_train/rejected': '0.028725', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.035912', 'logps_train/rejected': '-107.53', 'logps_train/chosen': '-121.55', 'loss/train': '0.7195', 'examples_per_second': '31.058', 'grad_norm': '33.5', 'counters/examples': 864, 'counters/updates': 27}
skipping logging after 896 examples to avoid logging too frequently
train stats after 928 examples: {'rewards_train/chosen': '0.050621', 'rewards_train/rejected': '0.014941', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03568', 'logps_train/rejected': '-142.71', 'logps_train/chosen': '-178.97', 'loss/train': '0.6784', 'examples_per_second': '31.279', 'grad_norm': '33.75', 'counters/examples': 928, 'counters/updates': 29}
train stats after 960 examples: {'rewards_train/chosen': '0.012499', 'rewards_train/rejected': '-0.042535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055034', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-128.55', 'loss/train': '0.67063', 'examples_per_second': '31.419', 'grad_norm': '41.75', 'counters/examples': 960, 'counters/updates': 30}
train stats after 992 examples: {'rewards_train/chosen': '-0.013012', 'rewards_train/rejected': '0.0076806', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020693', 'logps_train/rejected': '-146.19', 'logps_train/chosen': '-162.54', 'loss/train': '0.71067', 'examples_per_second': '30.536', 'grad_norm': '37.25', 'counters/examples': 992, 'counters/updates': 31}
train stats after 1024 examples: {'rewards_train/chosen': '0.0020009', 'rewards_train/rejected': '0.085435', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.083434', 'logps_train/rejected': '-154.29', 'logps_train/chosen': '-131.19', 'loss/train': '0.75527', 'examples_per_second': '31.293', 'grad_norm': '52.5', 'counters/examples': 1024, 'counters/updates': 32}
skipping logging after 1056 examples to avoid logging too frequently
train stats after 1088 examples: {'rewards_train/chosen': '0.0057699', 'rewards_train/rejected': '0.012199', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.006429', 'logps_train/rejected': '-105.91', 'logps_train/chosen': '-173.26', 'loss/train': '0.70578', 'examples_per_second': '29.831', 'grad_norm': '40.5', 'counters/examples': 1088, 'counters/updates': 34}
train stats after 1120 examples: {'rewards_train/chosen': '0.0037048', 'rewards_train/rejected': '0.027351', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023646', 'logps_train/rejected': '-113.18', 'logps_train/chosen': '-95.431', 'loss/train': '0.70918', 'examples_per_second': '32.766', 'grad_norm': '27.125', 'counters/examples': 1120, 'counters/updates': 35}
train stats after 1152 examples: {'rewards_train/chosen': '-0.095698', 'rewards_train/rejected': '-0.013531', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.082166', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-147.01', 'loss/train': '0.7491', 'examples_per_second': '30.053', 'grad_norm': '40', 'counters/examples': 1152, 'counters/updates': 36}
train stats after 1184 examples: {'rewards_train/chosen': '-0.024576', 'rewards_train/rejected': '-0.016081', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0084949', 'logps_train/rejected': '-165.68', 'logps_train/chosen': '-163.69', 'loss/train': '0.70055', 'examples_per_second': '30.343', 'grad_norm': '38.25', 'counters/examples': 1184, 'counters/updates': 37}
train stats after 1216 examples: {'rewards_train/chosen': '-0.043331', 'rewards_train/rejected': '-0.006489', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036842', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-160.13', 'loss/train': '0.71602', 'examples_per_second': '30.468', 'grad_norm': '42.25', 'counters/examples': 1216, 'counters/updates': 38}
train stats after 1248 examples: {'rewards_train/chosen': '0.044926', 'rewards_train/rejected': '-0.025254', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070181', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-159.77', 'loss/train': '0.6662', 'examples_per_second': '31.299', 'grad_norm': '29.875', 'counters/examples': 1248, 'counters/updates': 39}
train stats after 1280 examples: {'rewards_train/chosen': '-0.00299', 'rewards_train/rejected': '-0.012844', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0098537', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-127.35', 'loss/train': '0.69444', 'examples_per_second': '30.321', 'grad_norm': '43.75', 'counters/examples': 1280, 'counters/updates': 40}
train stats after 1312 examples: {'rewards_train/chosen': '-0.11295', 'rewards_train/rejected': '-0.040049', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.072899', 'logps_train/rejected': '-141.76', 'logps_train/chosen': '-150.38', 'loss/train': '0.76261', 'examples_per_second': '30.363', 'grad_norm': '61.25', 'counters/examples': 1312, 'counters/updates': 41}
train stats after 1344 examples: {'rewards_train/chosen': '-0.034495', 'rewards_train/rejected': '0.0057077', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.040203', 'logps_train/rejected': '-122.04', 'logps_train/chosen': '-232.45', 'loss/train': '0.72289', 'examples_per_second': '30.761', 'grad_norm': '40.25', 'counters/examples': 1344, 'counters/updates': 42}
train stats after 1376 examples: {'rewards_train/chosen': '-0.027641', 'rewards_train/rejected': '-0.076842', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049201', 'logps_train/rejected': '-171.9', 'logps_train/chosen': '-181.1', 'loss/train': '0.67151', 'examples_per_second': '31.087', 'grad_norm': '37', 'counters/examples': 1376, 'counters/updates': 43}
skipping logging after 1408 examples to avoid logging too frequently
train stats after 1440 examples: {'rewards_train/chosen': '0.007413', 'rewards_train/rejected': '-0.018999', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026412', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-155.16', 'loss/train': '0.69224', 'examples_per_second': '31.188', 'grad_norm': '60', 'counters/examples': 1440, 'counters/updates': 45}
train stats after 1472 examples: {'rewards_train/chosen': '0.081006', 'rewards_train/rejected': '0.051012', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.029994', 'logps_train/rejected': '-156.83', 'logps_train/chosen': '-146.61', 'loss/train': '0.72679', 'examples_per_second': '29.945', 'grad_norm': '39.75', 'counters/examples': 1472, 'counters/updates': 46}
train stats after 1504 examples: {'rewards_train/chosen': '-0.019708', 'rewards_train/rejected': '-0.010598', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0091097', 'logps_train/rejected': '-121.25', 'logps_train/chosen': '-134.35', 'loss/train': '0.7046', 'examples_per_second': '32.105', 'grad_norm': '34', 'counters/examples': 1504, 'counters/updates': 47}
train stats after 1536 examples: {'rewards_train/chosen': '-0.01301', 'rewards_train/rejected': '-0.0081973', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0048131', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-153.7', 'loss/train': '0.70177', 'examples_per_second': '30.61', 'grad_norm': '36.25', 'counters/examples': 1536, 'counters/updates': 48}
train stats after 1568 examples: {'rewards_train/chosen': '0.015905', 'rewards_train/rejected': '-0.012739', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028644', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-123.32', 'loss/train': '0.69018', 'examples_per_second': '30.858', 'grad_norm': '39.5', 'counters/examples': 1568, 'counters/updates': 49}
train stats after 1600 examples: {'rewards_train/chosen': '0.018268', 'rewards_train/rejected': '-0.054998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073265', 'logps_train/rejected': '-174.69', 'logps_train/chosen': '-196.1', 'loss/train': '0.67311', 'examples_per_second': '29.322', 'grad_norm': '47', 'counters/examples': 1600, 'counters/updates': 50}
skipping logging after 1632 examples to avoid logging too frequently
train stats after 1664 examples: {'rewards_train/chosen': '-0.011325', 'rewards_train/rejected': '-0.038418', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027093', 'logps_train/rejected': '-148.07', 'logps_train/chosen': '-129.72', 'loss/train': '0.68507', 'examples_per_second': '29.848', 'grad_norm': '42.25', 'counters/examples': 1664, 'counters/updates': 52}
skipping logging after 1696 examples to avoid logging too frequently
train stats after 1728 examples: {'rewards_train/chosen': '-0.024119', 'rewards_train/rejected': '-0.0071144', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017005', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-159.7', 'loss/train': '0.70555', 'examples_per_second': '32.411', 'grad_norm': '33.75', 'counters/examples': 1728, 'counters/updates': 54}
train stats after 1760 examples: {'rewards_train/chosen': '0.042255', 'rewards_train/rejected': '0.0055868', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036668', 'logps_train/rejected': '-104.35', 'logps_train/chosen': '-191.13', 'loss/train': '0.73243', 'examples_per_second': '30.82', 'grad_norm': '64.5', 'counters/examples': 1760, 'counters/updates': 55}
skipping logging after 1792 examples to avoid logging too frequently
train stats after 1824 examples: {'rewards_train/chosen': '0.042687', 'rewards_train/rejected': '-0.0083466', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051034', 'logps_train/rejected': '-152.41', 'logps_train/chosen': '-162.58', 'loss/train': '0.67288', 'examples_per_second': '31.267', 'grad_norm': '35.75', 'counters/examples': 1824, 'counters/updates': 57}
train stats after 1856 examples: {'rewards_train/chosen': '0.0082375', 'rewards_train/rejected': '0.010407', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0021698', 'logps_train/rejected': '-112.58', 'logps_train/chosen': '-127.51', 'loss/train': '0.69706', 'examples_per_second': '30.335', 'grad_norm': '30.25', 'counters/examples': 1856, 'counters/updates': 58}
train stats after 1888 examples: {'rewards_train/chosen': '0.015237', 'rewards_train/rejected': '-0.024328', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039564', 'logps_train/rejected': '-101.64', 'logps_train/chosen': '-113.63', 'loss/train': '0.67611', 'examples_per_second': '30.08', 'grad_norm': '35.75', 'counters/examples': 1888, 'counters/updates': 59}
train stats after 1920 examples: {'rewards_train/chosen': '0.0047166', 'rewards_train/rejected': '0.010536', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0058192', 'logps_train/rejected': '-172.57', 'logps_train/chosen': '-127.47', 'loss/train': '0.70218', 'examples_per_second': '32.094', 'grad_norm': '40.25', 'counters/examples': 1920, 'counters/updates': 60}
skipping logging after 1952 examples to avoid logging too frequently
train stats after 1984 examples: {'rewards_train/chosen': '-0.010637', 'rewards_train/rejected': '-0.02265', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012012', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-158.79', 'loss/train': '0.69023', 'examples_per_second': '30.417', 'grad_norm': '31.125', 'counters/examples': 1984, 'counters/updates': 62}
train stats after 2016 examples: {'rewards_train/chosen': '0.0013055', 'rewards_train/rejected': '0.033711', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032406', 'logps_train/rejected': '-166.67', 'logps_train/chosen': '-118.1', 'loss/train': '0.71438', 'examples_per_second': '30.721', 'grad_norm': '37.5', 'counters/examples': 2016, 'counters/updates': 63}
train stats after 2048 examples: {'rewards_train/chosen': '-0.045372', 'rewards_train/rejected': '0.01537', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.060742', 'logps_train/rejected': '-153.5', 'logps_train/chosen': '-188.58', 'loss/train': '0.72943', 'examples_per_second': '31.373', 'grad_norm': '37.5', 'counters/examples': 2048, 'counters/updates': 64}
train stats after 2080 examples: {'rewards_train/chosen': '-0.030125', 'rewards_train/rejected': '-0.010387', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019738', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-130.84', 'loss/train': '0.71327', 'examples_per_second': '30.837', 'grad_norm': '33.5', 'counters/examples': 2080, 'counters/updates': 65}
train stats after 2112 examples: {'rewards_train/chosen': '0.044813', 'rewards_train/rejected': '0.037173', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0076398', 'logps_train/rejected': '-112.16', 'logps_train/chosen': '-107.65', 'loss/train': '0.69285', 'examples_per_second': '30.459', 'grad_norm': '41.75', 'counters/examples': 2112, 'counters/updates': 66}
train stats after 2144 examples: {'rewards_train/chosen': '0.018358', 'rewards_train/rejected': '0.023354', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.0049954', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-118.89', 'loss/train': '0.70249', 'examples_per_second': '31.769', 'grad_norm': '31.875', 'counters/examples': 2144, 'counters/updates': 67}
skipping logging after 2176 examples to avoid logging too frequently
train stats after 2208 examples: {'rewards_train/chosen': '-0.025985', 'rewards_train/rejected': '0.012363', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038348', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-153.63', 'loss/train': '0.71863', 'examples_per_second': '30.791', 'grad_norm': '49.5', 'counters/examples': 2208, 'counters/updates': 69}
train stats after 2240 examples: {'rewards_train/chosen': '0.014192', 'rewards_train/rejected': '-0.018599', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032792', 'logps_train/rejected': '-129.24', 'logps_train/chosen': '-158.21', 'loss/train': '0.67864', 'examples_per_second': '30.998', 'grad_norm': '40.25', 'counters/examples': 2240, 'counters/updates': 70}
train stats after 2272 examples: {'rewards_train/chosen': '-0.018534', 'rewards_train/rejected': '-0.030807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012273', 'logps_train/rejected': '-109.66', 'logps_train/chosen': '-134.34', 'loss/train': '0.68859', 'examples_per_second': '31.594', 'grad_norm': '26.375', 'counters/examples': 2272, 'counters/updates': 71}
train stats after 2304 examples: {'rewards_train/chosen': '-0.018538', 'rewards_train/rejected': '0.024468', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.043007', 'logps_train/rejected': '-100.7', 'logps_train/chosen': '-132.21', 'loss/train': '0.71671', 'examples_per_second': '30.59', 'grad_norm': '33', 'counters/examples': 2304, 'counters/updates': 72}
train stats after 2336 examples: {'rewards_train/chosen': '-0.0074321', 'rewards_train/rejected': '-0.026219', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018787', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-143.25', 'loss/train': '0.68786', 'examples_per_second': '31.328', 'grad_norm': '32', 'counters/examples': 2336, 'counters/updates': 73}
skipping logging after 2368 examples to avoid logging too frequently
train stats after 2400 examples: {'rewards_train/chosen': '-0.014344', 'rewards_train/rejected': '0.0061351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020479', 'logps_train/rejected': '-122.84', 'logps_train/chosen': '-165.46', 'loss/train': '0.70734', 'examples_per_second': '30.015', 'grad_norm': '69', 'counters/examples': 2400, 'counters/updates': 75}
train stats after 2432 examples: {'rewards_train/chosen': '-0.039725', 'rewards_train/rejected': '0.0041944', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.04392', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-117.06', 'loss/train': '0.7234', 'examples_per_second': '29.952', 'grad_norm': '38.75', 'counters/examples': 2432, 'counters/updates': 76}
train stats after 2464 examples: {'rewards_train/chosen': '-0.009304', 'rewards_train/rejected': '0.034585', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043889', 'logps_train/rejected': '-120.45', 'logps_train/chosen': '-131.85', 'loss/train': '0.72067', 'examples_per_second': '32.541', 'grad_norm': '30.625', 'counters/examples': 2464, 'counters/updates': 77}
train stats after 2496 examples: {'rewards_train/chosen': '-0.045789', 'rewards_train/rejected': '-0.083104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037314', 'logps_train/rejected': '-146.36', 'logps_train/chosen': '-145.02', 'loss/train': '0.68688', 'examples_per_second': '31.244', 'grad_norm': '30.125', 'counters/examples': 2496, 'counters/updates': 78}
train stats after 2528 examples: {'rewards_train/chosen': '-0.14565', 'rewards_train/rejected': '-0.017194', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.12845', 'logps_train/rejected': '-124.54', 'logps_train/chosen': '-166.42', 'loss/train': '0.79474', 'examples_per_second': '30.183', 'grad_norm': '71.5', 'counters/examples': 2528, 'counters/updates': 79}
train stats after 2560 examples: {'rewards_train/chosen': '0.010259', 'rewards_train/rejected': '0.0036654', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0065936', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-127.09', 'loss/train': '0.69629', 'examples_per_second': '31.273', 'grad_norm': '43.75', 'counters/examples': 2560, 'counters/updates': 80}
skipping logging after 2592 examples to avoid logging too frequently
train stats after 2624 examples: {'rewards_train/chosen': '0.034035', 'rewards_train/rejected': '0.017805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01623', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-174.72', 'loss/train': '0.69171', 'examples_per_second': '31.267', 'grad_norm': '37.25', 'counters/examples': 2624, 'counters/updates': 82}
train stats after 2656 examples: {'rewards_train/chosen': '-0.024757', 'rewards_train/rejected': '-0.036198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011441', 'logps_train/rejected': '-147.72', 'logps_train/chosen': '-144.54', 'loss/train': '0.69219', 'examples_per_second': '30.564', 'grad_norm': '34', 'counters/examples': 2656, 'counters/updates': 83}
train stats after 2688 examples: {'rewards_train/chosen': '0.031458', 'rewards_train/rejected': '-0.0061642', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037622', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-155.19', 'loss/train': '0.67775', 'examples_per_second': '32.89', 'grad_norm': '34.75', 'counters/examples': 2688, 'counters/updates': 84}
train stats after 2720 examples: {'rewards_train/chosen': '0.0011952', 'rewards_train/rejected': '0.012417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011222', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-164.75', 'loss/train': '0.70417', 'examples_per_second': '31.88', 'grad_norm': '34', 'counters/examples': 2720, 'counters/updates': 85}
train stats after 2752 examples: {'rewards_train/chosen': '-0.027735', 'rewards_train/rejected': '-0.036153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0084187', 'logps_train/rejected': '-123.21', 'logps_train/chosen': '-136.25', 'loss/train': '0.69606', 'examples_per_second': '31.546', 'grad_norm': '31.375', 'counters/examples': 2752, 'counters/updates': 86}
train stats after 2784 examples: {'rewards_train/chosen': '0.07605', 'rewards_train/rejected': '0.063345', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012704', 'logps_train/rejected': '-133.29', 'logps_train/chosen': '-139.77', 'loss/train': '0.69034', 'examples_per_second': '30.744', 'grad_norm': '35.75', 'counters/examples': 2784, 'counters/updates': 87}
train stats after 2816 examples: {'rewards_train/chosen': '0.0039656', 'rewards_train/rejected': '0.051918', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047952', 'logps_train/rejected': '-145.76', 'logps_train/chosen': '-142.48', 'loss/train': '0.72611', 'examples_per_second': '30.84', 'grad_norm': '42.75', 'counters/examples': 2816, 'counters/updates': 88}
train stats after 2848 examples: {'rewards_train/chosen': '-0.016925', 'rewards_train/rejected': '0.037386', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.054311', 'logps_train/rejected': '-100.48', 'logps_train/chosen': '-161.27', 'loss/train': '0.72532', 'examples_per_second': '31.065', 'grad_norm': '35.5', 'counters/examples': 2848, 'counters/updates': 89}
train stats after 2880 examples: {'rewards_train/chosen': '-0.014365', 'rewards_train/rejected': '0.0066036', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020968', 'logps_train/rejected': '-161.3', 'logps_train/chosen': '-166.44', 'loss/train': '0.714', 'examples_per_second': '30.69', 'grad_norm': '41.25', 'counters/examples': 2880, 'counters/updates': 90}
train stats after 2912 examples: {'rewards_train/chosen': '-0.016355', 'rewards_train/rejected': '0.059895', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.07625', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-149.14', 'loss/train': '0.74222', 'examples_per_second': '33.206', 'grad_norm': '38.25', 'counters/examples': 2912, 'counters/updates': 91}
train stats after 2944 examples: {'rewards_train/chosen': '-0.025353', 'rewards_train/rejected': '-0.07434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048987', 'logps_train/rejected': '-154.48', 'logps_train/chosen': '-166.26', 'loss/train': '0.67357', 'examples_per_second': '31.78', 'grad_norm': '45', 'counters/examples': 2944, 'counters/updates': 92}
train stats after 2976 examples: {'rewards_train/chosen': '0.01231', 'rewards_train/rejected': '-0.0091104', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021421', 'logps_train/rejected': '-136.98', 'logps_train/chosen': '-146.5', 'loss/train': '0.69601', 'examples_per_second': '31.511', 'grad_norm': '44.25', 'counters/examples': 2976, 'counters/updates': 93}
skipping logging after 3008 examples to avoid logging too frequently
train stats after 3040 examples: {'rewards_train/chosen': '0.029315', 'rewards_train/rejected': '-0.035594', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064909', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-123.74', 'loss/train': '0.66678', 'examples_per_second': '31.56', 'grad_norm': '25.875', 'counters/examples': 3040, 'counters/updates': 95}
train stats after 3072 examples: {'rewards_train/chosen': '0.081679', 'rewards_train/rejected': '-0.037647', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11933', 'logps_train/rejected': '-140.93', 'logps_train/chosen': '-160.4', 'loss/train': '0.66353', 'examples_per_second': '32.046', 'grad_norm': '32', 'counters/examples': 3072, 'counters/updates': 96}
train stats after 3104 examples: {'rewards_train/chosen': '-0.028067', 'rewards_train/rejected': '0.019119', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047186', 'logps_train/rejected': '-125.09', 'logps_train/chosen': '-134.24', 'loss/train': '0.72022', 'examples_per_second': '32.363', 'grad_norm': '31.875', 'counters/examples': 3104, 'counters/updates': 97}
train stats after 3136 examples: {'rewards_train/chosen': '0.031938', 'rewards_train/rejected': '-0.00076829', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032706', 'logps_train/rejected': '-101.71', 'logps_train/chosen': '-131.65', 'loss/train': '0.6811', 'examples_per_second': '32.722', 'grad_norm': '39', 'counters/examples': 3136, 'counters/updates': 98}
train stats after 3168 examples: {'rewards_train/chosen': '0.0089894', 'rewards_train/rejected': '-0.03584', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044829', 'logps_train/rejected': '-123.42', 'logps_train/chosen': '-116.45', 'loss/train': '0.67487', 'examples_per_second': '31.269', 'grad_norm': '32.75', 'counters/examples': 3168, 'counters/updates': 99}
train stats after 3200 examples: {'rewards_train/chosen': '0.039718', 'rewards_train/rejected': '0.012522', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027196', 'logps_train/rejected': '-108.88', 'logps_train/chosen': '-169.66', 'loss/train': '0.68673', 'examples_per_second': '32.893', 'grad_norm': '41', 'counters/examples': 3200, 'counters/updates': 100}
train stats after 3232 examples: {'rewards_train/chosen': '-0.024381', 'rewards_train/rejected': '-0.014673', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0097084', 'logps_train/rejected': '-145.12', 'logps_train/chosen': '-173.53', 'loss/train': '0.70749', 'examples_per_second': '30.026', 'grad_norm': '46.5', 'counters/examples': 3232, 'counters/updates': 101}
train stats after 3264 examples: {'rewards_train/chosen': '-0.017586', 'rewards_train/rejected': '-0.0029168', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01467', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-148.14', 'loss/train': '0.70329', 'examples_per_second': '31.302', 'grad_norm': '36.25', 'counters/examples': 3264, 'counters/updates': 102}
train stats after 3296 examples: {'rewards_train/chosen': '-0.010039', 'rewards_train/rejected': '0.027922', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037961', 'logps_train/rejected': '-127.75', 'logps_train/chosen': '-151.42', 'loss/train': '0.71779', 'examples_per_second': '30.766', 'grad_norm': '39', 'counters/examples': 3296, 'counters/updates': 103}
train stats after 3328 examples: {'rewards_train/chosen': '0.0068475', 'rewards_train/rejected': '0.0020941', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0047533', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-155.82', 'loss/train': '0.69595', 'examples_per_second': '30.51', 'grad_norm': '44.25', 'counters/examples': 3328, 'counters/updates': 104}
train stats after 3360 examples: {'rewards_train/chosen': '-0.031729', 'rewards_train/rejected': '0.026228', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.057957', 'logps_train/rejected': '-119.31', 'logps_train/chosen': '-131.54', 'loss/train': '0.72944', 'examples_per_second': '32.275', 'grad_norm': '38.25', 'counters/examples': 3360, 'counters/updates': 105}
skipping logging after 3392 examples to avoid logging too frequently
train stats after 3424 examples: {'rewards_train/chosen': '-0.036561', 'rewards_train/rejected': '-0.0041676', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032393', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-167.66', 'loss/train': '0.71274', 'examples_per_second': '33.238', 'grad_norm': '52.75', 'counters/examples': 3424, 'counters/updates': 107}
train stats after 3456 examples: {'rewards_train/chosen': '-0.0025506', 'rewards_train/rejected': '0.0019295', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0044801', 'logps_train/rejected': '-162.72', 'logps_train/chosen': '-168.05', 'loss/train': '0.70122', 'examples_per_second': '30.346', 'grad_norm': '32', 'counters/examples': 3456, 'counters/updates': 108}
train stats after 3488 examples: {'rewards_train/chosen': '-0.067801', 'rewards_train/rejected': '-0.0033736', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.064428', 'logps_train/rejected': '-162.72', 'logps_train/chosen': '-162.16', 'loss/train': '0.73771', 'examples_per_second': '29.877', 'grad_norm': '55.5', 'counters/examples': 3488, 'counters/updates': 109}
train stats after 3520 examples: {'rewards_train/chosen': '-0.057738', 'rewards_train/rejected': '-0.027469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03027', 'logps_train/rejected': '-123.57', 'logps_train/chosen': '-151.44', 'loss/train': '0.71357', 'examples_per_second': '29.907', 'grad_norm': '41.5', 'counters/examples': 3520, 'counters/updates': 110}
train stats after 3552 examples: {'rewards_train/chosen': '0.014172', 'rewards_train/rejected': '0.010574', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0035982', 'logps_train/rejected': '-127.71', 'logps_train/chosen': '-151.74', 'loss/train': '0.69501', 'examples_per_second': '31.27', 'grad_norm': '31.875', 'counters/examples': 3552, 'counters/updates': 111}
train stats after 3584 examples: {'rewards_train/chosen': '0.025454', 'rewards_train/rejected': '0.046345', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02089', 'logps_train/rejected': '-120.49', 'logps_train/chosen': '-198.65', 'loss/train': '0.71015', 'examples_per_second': '32.522', 'grad_norm': '39.25', 'counters/examples': 3584, 'counters/updates': 112}
train stats after 3616 examples: {'rewards_train/chosen': '0.016714', 'rewards_train/rejected': '-0.010627', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027341', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-150.73', 'loss/train': '0.6823', 'examples_per_second': '32.371', 'grad_norm': '27', 'counters/examples': 3616, 'counters/updates': 113}
train stats after 3648 examples: {'rewards_train/chosen': '0.007647', 'rewards_train/rejected': '-0.012317', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019964', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-132.34', 'loss/train': '0.68719', 'examples_per_second': '30.719', 'grad_norm': '36', 'counters/examples': 3648, 'counters/updates': 114}
train stats after 3680 examples: {'rewards_train/chosen': '0.017567', 'rewards_train/rejected': '0.0075236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010043', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-168.04', 'loss/train': '0.69177', 'examples_per_second': '31.317', 'grad_norm': '31.375', 'counters/examples': 3680, 'counters/updates': 115}
train stats after 3712 examples: {'rewards_train/chosen': '0.053049', 'rewards_train/rejected': '0.015557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037491', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-127.08', 'loss/train': '0.68435', 'examples_per_second': '31.45', 'grad_norm': '38', 'counters/examples': 3712, 'counters/updates': 116}
train stats after 3744 examples: {'rewards_train/chosen': '0.0064135', 'rewards_train/rejected': '-0.030797', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03721', 'logps_train/rejected': '-152.6', 'logps_train/chosen': '-137.82', 'loss/train': '0.67834', 'examples_per_second': '31.327', 'grad_norm': '35.25', 'counters/examples': 3744, 'counters/updates': 117}
train stats after 3776 examples: {'rewards_train/chosen': '-0.011022', 'rewards_train/rejected': '-0.01559', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0045678', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-177.12', 'loss/train': '0.69742', 'examples_per_second': '32.275', 'grad_norm': '52.5', 'counters/examples': 3776, 'counters/updates': 118}
train stats after 3808 examples: {'rewards_train/chosen': '-0.01321', 'rewards_train/rejected': '0.00871', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02192', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-129.06', 'loss/train': '0.7068', 'examples_per_second': '31.282', 'grad_norm': '26.5', 'counters/examples': 3808, 'counters/updates': 119}
train stats after 3840 examples: {'rewards_train/chosen': '-0.0029363', 'rewards_train/rejected': '-0.041414', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038477', 'logps_train/rejected': '-112.9', 'logps_train/chosen': '-140.44', 'loss/train': '0.68233', 'examples_per_second': '30.587', 'grad_norm': '32.25', 'counters/examples': 3840, 'counters/updates': 120}
train stats after 3872 examples: {'rewards_train/chosen': '-0.0026812', 'rewards_train/rejected': '0.054291', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056972', 'logps_train/rejected': '-128.94', 'logps_train/chosen': '-138.26', 'loss/train': '0.72735', 'examples_per_second': '29.862', 'grad_norm': '32.75', 'counters/examples': 3872, 'counters/updates': 121}
train stats after 3904 examples: {'rewards_train/chosen': '-0.029179', 'rewards_train/rejected': '0.023438', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.052618', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-157.3', 'loss/train': '0.72267', 'examples_per_second': '31.304', 'grad_norm': '37.25', 'counters/examples': 3904, 'counters/updates': 122}
train stats after 3936 examples: {'rewards_train/chosen': '-0.01948', 'rewards_train/rejected': '-0.018878', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00060172', 'logps_train/rejected': '-111.53', 'logps_train/chosen': '-157.63', 'loss/train': '0.6961', 'examples_per_second': '30.354', 'grad_norm': '31.25', 'counters/examples': 3936, 'counters/updates': 123}
skipping logging after 3968 examples to avoid logging too frequently
train stats after 4000 examples: {'rewards_train/chosen': '-0.020978', 'rewards_train/rejected': '0.018371', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039349', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-136.46', 'loss/train': '0.71564', 'examples_per_second': '33.878', 'grad_norm': '38.5', 'counters/examples': 4000, 'counters/updates': 125}
train stats after 4032 examples: {'rewards_train/chosen': '-0.04386', 'rewards_train/rejected': '0.031468', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.075328', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-132.49', 'loss/train': '0.73938', 'examples_per_second': '31.28', 'grad_norm': '46.75', 'counters/examples': 4032, 'counters/updates': 126}
skipping logging after 4064 examples to avoid logging too frequently
train stats after 4096 examples: {'rewards_train/chosen': '0.086802', 'rewards_train/rejected': '0.044267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042535', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-151.05', 'loss/train': '0.6857', 'examples_per_second': '31.985', 'grad_norm': '43.5', 'counters/examples': 4096, 'counters/updates': 128}
train stats after 4128 examples: {'rewards_train/chosen': '-0.018514', 'rewards_train/rejected': '-0.030104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01159', 'logps_train/rejected': '-124.54', 'logps_train/chosen': '-179.42', 'loss/train': '0.69204', 'examples_per_second': '31.321', 'grad_norm': '39.25', 'counters/examples': 4128, 'counters/updates': 129}
skipping logging after 4160 examples to avoid logging too frequently
train stats after 4192 examples: {'rewards_train/chosen': '-0.008284', 'rewards_train/rejected': '-0.0016263', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0066577', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-114.24', 'loss/train': '0.70035', 'examples_per_second': '31.339', 'grad_norm': '34.75', 'counters/examples': 4192, 'counters/updates': 131}
train stats after 4224 examples: {'rewards_train/chosen': '-0.042339', 'rewards_train/rejected': '-0.010894', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.031445', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-127.37', 'loss/train': '0.71528', 'examples_per_second': '31.053', 'grad_norm': '31.375', 'counters/examples': 4224, 'counters/updates': 132}
train stats after 4256 examples: {'rewards_train/chosen': '-0.00010604', 'rewards_train/rejected': '-0.0015535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0014474', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-136.11', 'loss/train': '0.69686', 'examples_per_second': '29.906', 'grad_norm': '34.25', 'counters/examples': 4256, 'counters/updates': 133}
train stats after 4288 examples: {'rewards_train/chosen': '0.0033614', 'rewards_train/rejected': '-0.0038845', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0072459', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-127.15', 'loss/train': '0.69226', 'examples_per_second': '30.292', 'grad_norm': '27.5', 'counters/examples': 4288, 'counters/updates': 134}
skipping logging after 4320 examples to avoid logging too frequently
train stats after 4352 examples: {'rewards_train/chosen': '0.0072843', 'rewards_train/rejected': '-0.006096', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.01338', 'logps_train/rejected': '-146.48', 'logps_train/chosen': '-141.15', 'loss/train': '0.69248', 'examples_per_second': '32.407', 'grad_norm': '60', 'counters/examples': 4352, 'counters/updates': 136}
skipping logging after 4384 examples to avoid logging too frequently
train stats after 4416 examples: {'rewards_train/chosen': '0.057861', 'rewards_train/rejected': '0.0032671', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054594', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-142.48', 'loss/train': '0.67129', 'examples_per_second': '29.87', 'grad_norm': '36.75', 'counters/examples': 4416, 'counters/updates': 138}
train stats after 4448 examples: {'rewards_train/chosen': '-0.011674', 'rewards_train/rejected': '-0.019028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0073547', 'logps_train/rejected': '-133.58', 'logps_train/chosen': '-153.89', 'loss/train': '0.69336', 'examples_per_second': '32.369', 'grad_norm': '35.75', 'counters/examples': 4448, 'counters/updates': 139}
skipping logging after 4480 examples to avoid logging too frequently
train stats after 4512 examples: {'rewards_train/chosen': '-0.015778', 'rewards_train/rejected': '-0.023377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0075986', 'logps_train/rejected': '-105.72', 'logps_train/chosen': '-162.64', 'loss/train': '0.69581', 'examples_per_second': '32.396', 'grad_norm': '32', 'counters/examples': 4512, 'counters/updates': 141}
train stats after 4544 examples: {'rewards_train/chosen': '0.042844', 'rewards_train/rejected': '0.027923', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.014921', 'logps_train/rejected': '-144.78', 'logps_train/chosen': '-117.09', 'loss/train': '0.69334', 'examples_per_second': '30.605', 'grad_norm': '30', 'counters/examples': 4544, 'counters/updates': 142}
train stats after 4576 examples: {'rewards_train/chosen': '-0.0095264', 'rewards_train/rejected': '0.0024753', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012002', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-151.14', 'loss/train': '0.70676', 'examples_per_second': '31.273', 'grad_norm': '38.75', 'counters/examples': 4576, 'counters/updates': 143}
train stats after 4608 examples: {'rewards_train/chosen': '-0.0083003', 'rewards_train/rejected': '0.0071702', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01547', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-146.44', 'loss/train': '0.70354', 'examples_per_second': '31.322', 'grad_norm': '30.375', 'counters/examples': 4608, 'counters/updates': 144}
train stats after 4640 examples: {'rewards_train/chosen': '-0.13802', 'rewards_train/rejected': '-0.13624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.001782', 'logps_train/rejected': '-161.62', 'logps_train/chosen': '-175.98', 'loss/train': '0.70229', 'examples_per_second': '30.328', 'grad_norm': '40.25', 'counters/examples': 4640, 'counters/updates': 145}
skipping logging after 4672 examples to avoid logging too frequently
train stats after 4704 examples: {'rewards_train/chosen': '0.011337', 'rewards_train/rejected': '0.0082608', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0030758', 'logps_train/rejected': '-136.76', 'logps_train/chosen': '-165.64', 'loss/train': '0.69717', 'examples_per_second': '31.231', 'grad_norm': '32.5', 'counters/examples': 4704, 'counters/updates': 147}
train stats after 4736 examples: {'rewards_train/chosen': '0.012884', 'rewards_train/rejected': '0.032651', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019766', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-161.94', 'loss/train': '0.71478', 'examples_per_second': '30.442', 'grad_norm': '36.5', 'counters/examples': 4736, 'counters/updates': 148}
train stats after 4768 examples: {'rewards_train/chosen': '0.019232', 'rewards_train/rejected': '-0.022086', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041318', 'logps_train/rejected': '-106.55', 'logps_train/chosen': '-130.39', 'loss/train': '0.67664', 'examples_per_second': '33.048', 'grad_norm': '23.5', 'counters/examples': 4768, 'counters/updates': 149}
train stats after 4800 examples: {'rewards_train/chosen': '0.054111', 'rewards_train/rejected': '-0.036056', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090167', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-114.67', 'loss/train': '0.65382', 'examples_per_second': '31.302', 'grad_norm': '30.25', 'counters/examples': 4800, 'counters/updates': 150}
train stats after 4832 examples: {'rewards_train/chosen': '-0.011746', 'rewards_train/rejected': '-0.028078', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016333', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-167.82', 'loss/train': '0.6915', 'examples_per_second': '31.897', 'grad_norm': '31.25', 'counters/examples': 4832, 'counters/updates': 151}
train stats after 4864 examples: {'rewards_train/chosen': '-0.06903', 'rewards_train/rejected': '-0.031275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.037755', 'logps_train/rejected': '-123', 'logps_train/chosen': '-156.52', 'loss/train': '0.73823', 'examples_per_second': '30.029', 'grad_norm': '66', 'counters/examples': 4864, 'counters/updates': 152}
train stats after 4896 examples: {'rewards_train/chosen': '-0.04485', 'rewards_train/rejected': '-0.055227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010377', 'logps_train/rejected': '-133.23', 'logps_train/chosen': '-144.5', 'loss/train': '0.69207', 'examples_per_second': '31.282', 'grad_norm': '34.25', 'counters/examples': 4896, 'counters/updates': 153}
skipping logging after 4928 examples to avoid logging too frequently
train stats after 4960 examples: {'rewards_train/chosen': '0.050368', 'rewards_train/rejected': '0.020155', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030214', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-164.65', 'loss/train': '0.68615', 'examples_per_second': '31.233', 'grad_norm': '32.75', 'counters/examples': 4960, 'counters/updates': 155}
train stats after 4992 examples: {'rewards_train/chosen': '-0.0033582', 'rewards_train/rejected': '0.034231', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.037589', 'logps_train/rejected': '-120.91', 'logps_train/chosen': '-157.53', 'loss/train': '0.71398', 'examples_per_second': '30.477', 'grad_norm': '29.75', 'counters/examples': 4992, 'counters/updates': 156}
train stats after 5024 examples: {'rewards_train/chosen': '0.042119', 'rewards_train/rejected': '0.019951', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022168', 'logps_train/rejected': '-153.96', 'logps_train/chosen': '-125.46', 'loss/train': '0.69004', 'examples_per_second': '31.295', 'grad_norm': '59.75', 'counters/examples': 5024, 'counters/updates': 157}
train stats after 5056 examples: {'rewards_train/chosen': '-0.0042733', 'rewards_train/rejected': '-0.024222', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019949', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-130.69', 'loss/train': '0.68733', 'examples_per_second': '31.35', 'grad_norm': '29.75', 'counters/examples': 5056, 'counters/updates': 158}
skipping logging after 5088 examples to avoid logging too frequently
train stats after 5120 examples: {'rewards_train/chosen': '0.01878', 'rewards_train/rejected': '-0.03247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05125', 'logps_train/rejected': '-136.45', 'logps_train/chosen': '-121.35', 'loss/train': '0.67216', 'examples_per_second': '31.245', 'grad_norm': '31.25', 'counters/examples': 5120, 'counters/updates': 160}
train stats after 5152 examples: {'rewards_train/chosen': '0.01374', 'rewards_train/rejected': '0.023878', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010138', 'logps_train/rejected': '-126.03', 'logps_train/chosen': '-187.24', 'loss/train': '0.7013', 'examples_per_second': '30.36', 'grad_norm': '35.5', 'counters/examples': 5152, 'counters/updates': 161}
train stats after 5184 examples: {'rewards_train/chosen': '0.062889', 'rewards_train/rejected': '0.012642', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050247', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-133.79', 'loss/train': '0.67025', 'examples_per_second': '31.106', 'grad_norm': '27.5', 'counters/examples': 5184, 'counters/updates': 162}
train stats after 5216 examples: {'rewards_train/chosen': '0.011809', 'rewards_train/rejected': '-0.0017017', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01351', 'logps_train/rejected': '-112.69', 'logps_train/chosen': '-149.85', 'loss/train': '0.68956', 'examples_per_second': '30.862', 'grad_norm': '26', 'counters/examples': 5216, 'counters/updates': 163}
train stats after 5248 examples: {'rewards_train/chosen': '-0.010622', 'rewards_train/rejected': '-0.046578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035956', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-115.45', 'loss/train': '0.67798', 'examples_per_second': '31.35', 'grad_norm': '31', 'counters/examples': 5248, 'counters/updates': 164}
train stats after 5280 examples: {'rewards_train/chosen': '-0.00060924', 'rewards_train/rejected': '0.0085891', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0091983', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-152.64', 'loss/train': '0.70298', 'examples_per_second': '31.332', 'grad_norm': '29.25', 'counters/examples': 5280, 'counters/updates': 165}
train stats after 5312 examples: {'rewards_train/chosen': '0.0092754', 'rewards_train/rejected': '0.030242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.020967', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-131.55', 'loss/train': '0.7077', 'examples_per_second': '30.255', 'grad_norm': '36', 'counters/examples': 5312, 'counters/updates': 166}
train stats after 5344 examples: {'rewards_train/chosen': '-0.07194', 'rewards_train/rejected': '0.023624', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.095565', 'logps_train/rejected': '-130.75', 'logps_train/chosen': '-194.48', 'loss/train': '0.74709', 'examples_per_second': '31.301', 'grad_norm': '36.75', 'counters/examples': 5344, 'counters/updates': 167}
train stats after 5376 examples: {'rewards_train/chosen': '-0.058977', 'rewards_train/rejected': '-0.0029536', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056024', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-144.81', 'loss/train': '0.72588', 'examples_per_second': '31.261', 'grad_norm': '37.75', 'counters/examples': 5376, 'counters/updates': 168}
train stats after 5408 examples: {'rewards_train/chosen': '0.090515', 'rewards_train/rejected': '-0.014898', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10541', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-128.46', 'loss/train': '0.66695', 'examples_per_second': '31.36', 'grad_norm': '31.875', 'counters/examples': 5408, 'counters/updates': 169}
train stats after 5440 examples: {'rewards_train/chosen': '0.046817', 'rewards_train/rejected': '0.0096473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037169', 'logps_train/rejected': '-100.6', 'logps_train/chosen': '-123.37', 'loss/train': '0.68608', 'examples_per_second': '31.586', 'grad_norm': '28.25', 'counters/examples': 5440, 'counters/updates': 170}
train stats after 5472 examples: {'rewards_train/chosen': '0.027902', 'rewards_train/rejected': '-0.026251', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.054153', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-170.92', 'loss/train': '0.67737', 'examples_per_second': '32.242', 'grad_norm': '37.75', 'counters/examples': 5472, 'counters/updates': 171}
skipping logging after 5504 examples to avoid logging too frequently
train stats after 5536 examples: {'rewards_train/chosen': '-0.01817', 'rewards_train/rejected': '-0.031534', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013364', 'logps_train/rejected': '-98.945', 'logps_train/chosen': '-94.795', 'loss/train': '0.68916', 'examples_per_second': '31.208', 'grad_norm': '23.75', 'counters/examples': 5536, 'counters/updates': 173}
train stats after 5568 examples: {'rewards_train/chosen': '0.041749', 'rewards_train/rejected': '0.034267', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0074826', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-146.84', 'loss/train': '0.69328', 'examples_per_second': '30.189', 'grad_norm': '29.875', 'counters/examples': 5568, 'counters/updates': 174}
skipping logging after 5600 examples to avoid logging too frequently
train stats after 5632 examples: {'rewards_train/chosen': '-0.036415', 'rewards_train/rejected': '0.015768', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052183', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-110.42', 'loss/train': '0.72469', 'examples_per_second': '35.524', 'grad_norm': '31.25', 'counters/examples': 5632, 'counters/updates': 176}
train stats after 5664 examples: {'rewards_train/chosen': '0.013168', 'rewards_train/rejected': '0.024902', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011734', 'logps_train/rejected': '-171.32', 'logps_train/chosen': '-132.9', 'loss/train': '0.70827', 'examples_per_second': '30.426', 'grad_norm': '39.5', 'counters/examples': 5664, 'counters/updates': 177}
skipping logging after 5696 examples to avoid logging too frequently
train stats after 5728 examples: {'rewards_train/chosen': '-0.041612', 'rewards_train/rejected': '0.020748', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.06236', 'logps_train/rejected': '-100.15', 'logps_train/chosen': '-160.83', 'loss/train': '0.73057', 'examples_per_second': '32.33', 'grad_norm': '71', 'counters/examples': 5728, 'counters/updates': 179}
skipping logging after 5760 examples to avoid logging too frequently
train stats after 5792 examples: {'rewards_train/chosen': '-0.087465', 'rewards_train/rejected': '-0.0046674', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.082797', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-183.53', 'loss/train': '0.74813', 'examples_per_second': '31.318', 'grad_norm': '45.5', 'counters/examples': 5792, 'counters/updates': 181}
skipping logging after 5824 examples to avoid logging too frequently
train stats after 5856 examples: {'rewards_train/chosen': '0.041843', 'rewards_train/rejected': '0.0086166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033227', 'logps_train/rejected': '-114.84', 'logps_train/chosen': '-139.26', 'loss/train': '0.68468', 'examples_per_second': '31.125', 'grad_norm': '32.5', 'counters/examples': 5856, 'counters/updates': 183}
train stats after 5888 examples: {'rewards_train/chosen': '-0.0099167', 'rewards_train/rejected': '0.014424', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024341', 'logps_train/rejected': '-100.84', 'logps_train/chosen': '-138.63', 'loss/train': '0.70775', 'examples_per_second': '31.3', 'grad_norm': '31.125', 'counters/examples': 5888, 'counters/updates': 184}
skipping logging after 5920 examples to avoid logging too frequently
train stats after 5952 examples: {'rewards_train/chosen': '0.0023356', 'rewards_train/rejected': '0.0036184', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0012828', 'logps_train/rejected': '-108.79', 'logps_train/chosen': '-118.42', 'loss/train': '0.69887', 'examples_per_second': '31.9', 'grad_norm': '36.25', 'counters/examples': 5952, 'counters/updates': 186}
train stats after 5984 examples: {'rewards_train/chosen': '0.0042776', 'rewards_train/rejected': '0.018526', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014249', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-109.07', 'loss/train': '0.70445', 'examples_per_second': '30.513', 'grad_norm': '71', 'counters/examples': 5984, 'counters/updates': 187}
train stats after 6016 examples: {'rewards_train/chosen': '-0.020939', 'rewards_train/rejected': '-0.03355', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012611', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-167.26', 'loss/train': '0.69874', 'examples_per_second': '31.069', 'grad_norm': '34', 'counters/examples': 6016, 'counters/updates': 188}
train stats after 6048 examples: {'rewards_train/chosen': '0.051562', 'rewards_train/rejected': '0.04026', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011302', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-163.33', 'loss/train': '0.69426', 'examples_per_second': '31.389', 'grad_norm': '33.5', 'counters/examples': 6048, 'counters/updates': 189}
train stats after 6080 examples: {'rewards_train/chosen': '0.03701', 'rewards_train/rejected': '0.0010659', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035944', 'logps_train/rejected': '-149.18', 'logps_train/chosen': '-146.35', 'loss/train': '0.67884', 'examples_per_second': '31.259', 'grad_norm': '28.625', 'counters/examples': 6080, 'counters/updates': 190}
skipping logging after 6112 examples to avoid logging too frequently
train stats after 6144 examples: {'rewards_train/chosen': '-0.032313', 'rewards_train/rejected': '0.031746', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.064059', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-123.43', 'loss/train': '0.73221', 'examples_per_second': '31.016', 'grad_norm': '37.75', 'counters/examples': 6144, 'counters/updates': 192}
train stats after 6176 examples: {'rewards_train/chosen': '0.023694', 'rewards_train/rejected': '-0.014394', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038087', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-134.87', 'loss/train': '0.67682', 'examples_per_second': '33.066', 'grad_norm': '27.25', 'counters/examples': 6176, 'counters/updates': 193}
train stats after 6208 examples: {'rewards_train/chosen': '0.044343', 'rewards_train/rejected': '-0.031843', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076186', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-166.21', 'loss/train': '0.66354', 'examples_per_second': '31.659', 'grad_norm': '30.5', 'counters/examples': 6208, 'counters/updates': 194}
skipping logging after 6240 examples to avoid logging too frequently
train stats after 6272 examples: {'rewards_train/chosen': '0.0156', 'rewards_train/rejected': '-0.013541', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02914', 'logps_train/rejected': '-86.775', 'logps_train/chosen': '-152.28', 'loss/train': '0.68449', 'examples_per_second': '31.209', 'grad_norm': '38.75', 'counters/examples': 6272, 'counters/updates': 196}
train stats after 6304 examples: {'rewards_train/chosen': '-0.019573', 'rewards_train/rejected': '-0.024977', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0054036', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-165.63', 'loss/train': '0.69793', 'examples_per_second': '31.263', 'grad_norm': '40', 'counters/examples': 6304, 'counters/updates': 197}
train stats after 6336 examples: {'rewards_train/chosen': '0.054601', 'rewards_train/rejected': '-0.030058', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084659', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-144.18', 'loss/train': '0.66072', 'examples_per_second': '31.268', 'grad_norm': '27.875', 'counters/examples': 6336, 'counters/updates': 198}
train stats after 6368 examples: {'rewards_train/chosen': '-0.19845', 'rewards_train/rejected': '0.0059642', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.20441', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-188.07', 'loss/train': '0.85023', 'examples_per_second': '30.851', 'grad_norm': '188', 'counters/examples': 6368, 'counters/updates': 199}
train stats after 6400 examples: {'rewards_train/chosen': '0.0041525', 'rewards_train/rejected': '0.014901', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010748', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-161.45', 'loss/train': '0.70327', 'examples_per_second': '32.18', 'grad_norm': '36.25', 'counters/examples': 6400, 'counters/updates': 200}
train stats after 6432 examples: {'rewards_train/chosen': '0.043484', 'rewards_train/rejected': '-0.037459', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080943', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-156.06', 'loss/train': '0.65823', 'examples_per_second': '31.032', 'grad_norm': '37.75', 'counters/examples': 6432, 'counters/updates': 201}
train stats after 6464 examples: {'rewards_train/chosen': '-0.0012095', 'rewards_train/rejected': '-0.030603', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029394', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-127.6', 'loss/train': '0.69127', 'examples_per_second': '32.736', 'grad_norm': '33.5', 'counters/examples': 6464, 'counters/updates': 202}
train stats after 6496 examples: {'rewards_train/chosen': '-0.032326', 'rewards_train/rejected': '-0.039151', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0068251', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-139.98', 'loss/train': '0.69167', 'examples_per_second': '29.809', 'grad_norm': '33.25', 'counters/examples': 6496, 'counters/updates': 203}
train stats after 6528 examples: {'rewards_train/chosen': '0.035543', 'rewards_train/rejected': '-0.0090102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044553', 'logps_train/rejected': '-109.9', 'logps_train/chosen': '-109.06', 'loss/train': '0.67901', 'examples_per_second': '31.155', 'grad_norm': '25.875', 'counters/examples': 6528, 'counters/updates': 204}
train stats after 6560 examples: {'rewards_train/chosen': '-0.017224', 'rewards_train/rejected': '-0.0068881', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010336', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-119.34', 'loss/train': '0.70346', 'examples_per_second': '21.542', 'grad_norm': '28.75', 'counters/examples': 6560, 'counters/updates': 205}
train stats after 6592 examples: {'rewards_train/chosen': '-0.041158', 'rewards_train/rejected': '-0.059368', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01821', 'logps_train/rejected': '-188.81', 'logps_train/chosen': '-163.12', 'loss/train': '0.68783', 'examples_per_second': '31.226', 'grad_norm': '38', 'counters/examples': 6592, 'counters/updates': 206}
skipping logging after 6624 examples to avoid logging too frequently
train stats after 6656 examples: {'rewards_train/chosen': '0.052102', 'rewards_train/rejected': '0.01866', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033442', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-139.13', 'loss/train': '0.67899', 'examples_per_second': '23.962', 'grad_norm': '39.5', 'counters/examples': 6656, 'counters/updates': 208}
train stats after 6688 examples: {'rewards_train/chosen': '0.012875', 'rewards_train/rejected': '0.0086142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.004261', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-161.63', 'loss/train': '0.69447', 'examples_per_second': '30.98', 'grad_norm': '30.375', 'counters/examples': 6688, 'counters/updates': 209}
train stats after 6720 examples: {'rewards_train/chosen': '0.025178', 'rewards_train/rejected': '-0.0097445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034922', 'logps_train/rejected': '-94.904', 'logps_train/chosen': '-106.99', 'loss/train': '0.6788', 'examples_per_second': '32.731', 'grad_norm': '24.625', 'counters/examples': 6720, 'counters/updates': 210}
train stats after 6752 examples: {'rewards_train/chosen': '0.14171', 'rewards_train/rejected': '0.012438', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12927', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-151.42', 'loss/train': '0.64755', 'examples_per_second': '30.466', 'grad_norm': '35.25', 'counters/examples': 6752, 'counters/updates': 211}
train stats after 6784 examples: {'rewards_train/chosen': '-0.08015', 'rewards_train/rejected': '-0.0036054', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.076544', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-156.86', 'loss/train': '0.74398', 'examples_per_second': '30.633', 'grad_norm': '46.75', 'counters/examples': 6784, 'counters/updates': 212}
train stats after 6816 examples: {'rewards_train/chosen': '0.0029672', 'rewards_train/rejected': '-0.014329', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017296', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-150.46', 'loss/train': '0.68642', 'examples_per_second': '31.285', 'grad_norm': '31.375', 'counters/examples': 6816, 'counters/updates': 213}
train stats after 6848 examples: {'rewards_train/chosen': '-0.010018', 'rewards_train/rejected': '0.081592', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.091609', 'logps_train/rejected': '-159.59', 'logps_train/chosen': '-157.64', 'loss/train': '0.75052', 'examples_per_second': '31.266', 'grad_norm': '58', 'counters/examples': 6848, 'counters/updates': 214}
train stats after 6880 examples: {'rewards_train/chosen': '0.062467', 'rewards_train/rejected': '0.0090568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05341', 'logps_train/rejected': '-152.79', 'logps_train/chosen': '-172.93', 'loss/train': '0.6733', 'examples_per_second': '29.921', 'grad_norm': '37.75', 'counters/examples': 6880, 'counters/updates': 215}
train stats after 6912 examples: {'rewards_train/chosen': '-0.014248', 'rewards_train/rejected': '0.047413', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.061661', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-140.8', 'loss/train': '0.72839', 'examples_per_second': '31.084', 'grad_norm': '39.75', 'counters/examples': 6912, 'counters/updates': 216}
skipping logging after 6944 examples to avoid logging too frequently
train stats after 6976 examples: {'rewards_train/chosen': '0.014659', 'rewards_train/rejected': '0.035484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020826', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-168.42', 'loss/train': '0.70988', 'examples_per_second': '30.209', 'grad_norm': '49.25', 'counters/examples': 6976, 'counters/updates': 218}
train stats after 7008 examples: {'rewards_train/chosen': '-0.032638', 'rewards_train/rejected': '-0.039185', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0065464', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-167.96', 'loss/train': '0.69623', 'examples_per_second': '31.335', 'grad_norm': '98.5', 'counters/examples': 7008, 'counters/updates': 219}
train stats after 7040 examples: {'rewards_train/chosen': '-0.012999', 'rewards_train/rejected': '0.0050236', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018022', 'logps_train/rejected': '-130.67', 'logps_train/chosen': '-151.84', 'loss/train': '0.70429', 'examples_per_second': '31.322', 'grad_norm': '34.25', 'counters/examples': 7040, 'counters/updates': 220}
train stats after 7072 examples: {'rewards_train/chosen': '-0.0097772', 'rewards_train/rejected': '-0.0228', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.013023', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-153.9', 'loss/train': '0.69404', 'examples_per_second': '30.399', 'grad_norm': '51.5', 'counters/examples': 7072, 'counters/updates': 221}
train stats after 7104 examples: {'rewards_train/chosen': '0.0004421', 'rewards_train/rejected': '0.024271', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023829', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-120.96', 'loss/train': '0.71427', 'examples_per_second': '30.674', 'grad_norm': '29.375', 'counters/examples': 7104, 'counters/updates': 222}
train stats after 7136 examples: {'rewards_train/chosen': '0.01614', 'rewards_train/rejected': '0.018246', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0021054', 'logps_train/rejected': '-139.25', 'logps_train/chosen': '-175.32', 'loss/train': '0.69722', 'examples_per_second': '24.465', 'grad_norm': '38.5', 'counters/examples': 7136, 'counters/updates': 223}
train stats after 7168 examples: {'rewards_train/chosen': '0.024821', 'rewards_train/rejected': '0.023034', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0017869', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-142.9', 'loss/train': '0.6956', 'examples_per_second': '29.977', 'grad_norm': '33.25', 'counters/examples': 7168, 'counters/updates': 224}
train stats after 7200 examples: {'rewards_train/chosen': '0.027197', 'rewards_train/rejected': '-0.017166', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044363', 'logps_train/rejected': '-93.205', 'logps_train/chosen': '-147.3', 'loss/train': '0.67445', 'examples_per_second': '32.321', 'grad_norm': '37', 'counters/examples': 7200, 'counters/updates': 225}
skipping logging after 7232 examples to avoid logging too frequently
train stats after 7264 examples: {'rewards_train/chosen': '0.024726', 'rewards_train/rejected': '-0.048338', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073064', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-156.61', 'loss/train': '0.66158', 'examples_per_second': '31.332', 'grad_norm': '32.5', 'counters/examples': 7264, 'counters/updates': 227}
train stats after 7296 examples: {'rewards_train/chosen': '0.020909', 'rewards_train/rejected': '0.025733', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.004824', 'logps_train/rejected': '-140.83', 'logps_train/chosen': '-158.48', 'loss/train': '0.69866', 'examples_per_second': '31.858', 'grad_norm': '31.125', 'counters/examples': 7296, 'counters/updates': 228}
skipping logging after 7328 examples to avoid logging too frequently
train stats after 7360 examples: {'rewards_train/chosen': '0.011228', 'rewards_train/rejected': '0.01019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0010386', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-113', 'loss/train': '0.69542', 'examples_per_second': '31.587', 'grad_norm': '28.75', 'counters/examples': 7360, 'counters/updates': 230}
skipping logging after 7392 examples to avoid logging too frequently
train stats after 7424 examples: {'rewards_train/chosen': '-0.0021241', 'rewards_train/rejected': '-0.01605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013925', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-137.87', 'loss/train': '0.68956', 'examples_per_second': '31.325', 'grad_norm': '54', 'counters/examples': 7424, 'counters/updates': 232}
train stats after 7456 examples: {'rewards_train/chosen': '0.011737', 'rewards_train/rejected': '0.03132', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019584', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-156.19', 'loss/train': '0.70724', 'examples_per_second': '32.746', 'grad_norm': '36', 'counters/examples': 7456, 'counters/updates': 233}
train stats after 7488 examples: {'rewards_train/chosen': '0.048827', 'rewards_train/rejected': '-0.019284', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068112', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-180.61', 'loss/train': '0.66661', 'examples_per_second': '29.921', 'grad_norm': '35', 'counters/examples': 7488, 'counters/updates': 234}
train stats after 7520 examples: {'rewards_train/chosen': '-0.018329', 'rewards_train/rejected': '-0.00048333', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.017845', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-108.04', 'loss/train': '0.70669', 'examples_per_second': '30.368', 'grad_norm': '28.625', 'counters/examples': 7520, 'counters/updates': 235}
train stats after 7552 examples: {'rewards_train/chosen': '0.025451', 'rewards_train/rejected': '0.042844', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017393', 'logps_train/rejected': '-127.33', 'logps_train/chosen': '-151.55', 'loss/train': '0.70589', 'examples_per_second': '32.158', 'grad_norm': '35.75', 'counters/examples': 7552, 'counters/updates': 236}
train stats after 7584 examples: {'rewards_train/chosen': '-0.015796', 'rewards_train/rejected': '0.0076789', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.023475', 'logps_train/rejected': '-152.22', 'logps_train/chosen': '-137.21', 'loss/train': '0.71605', 'examples_per_second': '31.332', 'grad_norm': '33.75', 'counters/examples': 7584, 'counters/updates': 237}
train stats after 7616 examples: {'rewards_train/chosen': '0.02008', 'rewards_train/rejected': '-0.0066437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026723', 'logps_train/rejected': '-96.694', 'logps_train/chosen': '-131.18', 'loss/train': '0.68337', 'examples_per_second': '32.299', 'grad_norm': '34', 'counters/examples': 7616, 'counters/updates': 238}
train stats after 7648 examples: {'rewards_train/chosen': '0.062545', 'rewards_train/rejected': '0.047756', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014789', 'logps_train/rejected': '-143.22', 'logps_train/chosen': '-151.07', 'loss/train': '0.69473', 'examples_per_second': '32.311', 'grad_norm': '53.75', 'counters/examples': 7648, 'counters/updates': 239}
train stats after 7680 examples: {'rewards_train/chosen': '-0.05799', 'rewards_train/rejected': '0.018459', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.076449', 'logps_train/rejected': '-147.7', 'logps_train/chosen': '-167.48', 'loss/train': '0.74564', 'examples_per_second': '33.066', 'grad_norm': '36.75', 'counters/examples': 7680, 'counters/updates': 240}
skipping logging after 7712 examples to avoid logging too frequently
train stats after 7744 examples: {'rewards_train/chosen': '0.0094116', 'rewards_train/rejected': '0.002363', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0070486', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-175.82', 'loss/train': '0.69323', 'examples_per_second': '30.378', 'grad_norm': '29.875', 'counters/examples': 7744, 'counters/updates': 242}
skipping logging after 7776 examples to avoid logging too frequently
train stats after 7808 examples: {'rewards_train/chosen': '0.012032', 'rewards_train/rejected': '0.011233', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00079925', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-171.26', 'loss/train': '0.70028', 'examples_per_second': '31.564', 'grad_norm': '34.25', 'counters/examples': 7808, 'counters/updates': 244}
train stats after 7840 examples: {'rewards_train/chosen': '0.015887', 'rewards_train/rejected': '-0.0035346', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019422', 'logps_train/rejected': '-158.07', 'logps_train/chosen': '-161.17', 'loss/train': '0.68841', 'examples_per_second': '31.311', 'grad_norm': '36', 'counters/examples': 7840, 'counters/updates': 245}
train stats after 7872 examples: {'rewards_train/chosen': '0.041868', 'rewards_train/rejected': '-0.018966', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.060835', 'logps_train/rejected': '-115.51', 'logps_train/chosen': '-124.07', 'loss/train': '0.66832', 'examples_per_second': '30.28', 'grad_norm': '25.375', 'counters/examples': 7872, 'counters/updates': 246}
train stats after 7904 examples: {'rewards_train/chosen': '0.0022869', 'rewards_train/rejected': '0.017475', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.015188', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-107.57', 'loss/train': '0.70538', 'examples_per_second': '29.969', 'grad_norm': '32.25', 'counters/examples': 7904, 'counters/updates': 247}
train stats after 7936 examples: {'rewards_train/chosen': '0.053724', 'rewards_train/rejected': '0.020202', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.033522', 'logps_train/rejected': '-126.25', 'logps_train/chosen': '-134.17', 'loss/train': '0.68131', 'examples_per_second': '32.872', 'grad_norm': '28.5', 'counters/examples': 7936, 'counters/updates': 248}
train stats after 7968 examples: {'rewards_train/chosen': '0.050535', 'rewards_train/rejected': '0.013012', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037523', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-143.9', 'loss/train': '0.67839', 'examples_per_second': '32.88', 'grad_norm': '35', 'counters/examples': 7968, 'counters/updates': 249}
skipping logging after 8000 examples to avoid logging too frequently
train stats after 8032 examples: {'rewards_train/chosen': '0.0035081', 'rewards_train/rejected': '-0.00078048', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0042886', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-153.43', 'loss/train': '0.69509', 'examples_per_second': '30.988', 'grad_norm': '55.5', 'counters/examples': 8032, 'counters/updates': 251}
train stats after 8064 examples: {'rewards_train/chosen': '-0.011871', 'rewards_train/rejected': '0.063166', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.075037', 'logps_train/rejected': '-141.67', 'logps_train/chosen': '-166.45', 'loss/train': '0.74993', 'examples_per_second': '30.308', 'grad_norm': '54.75', 'counters/examples': 8064, 'counters/updates': 252}
skipping logging after 8096 examples to avoid logging too frequently
train stats after 8128 examples: {'rewards_train/chosen': '-0.010033', 'rewards_train/rejected': '-0.032346', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022313', 'logps_train/rejected': '-151.82', 'logps_train/chosen': '-157.09', 'loss/train': '0.69082', 'examples_per_second': '31.211', 'grad_norm': '54', 'counters/examples': 8128, 'counters/updates': 254}
train stats after 8160 examples: {'rewards_train/chosen': '0.010617', 'rewards_train/rejected': '-0.069819', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080436', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-158.03', 'loss/train': '0.68348', 'examples_per_second': '29.918', 'grad_norm': '27.625', 'counters/examples': 8160, 'counters/updates': 255}
train stats after 8192 examples: {'rewards_train/chosen': '0.0011477', 'rewards_train/rejected': '0.0082284', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0070807', 'logps_train/rejected': '-132.68', 'logps_train/chosen': '-193.29', 'loss/train': '0.70859', 'examples_per_second': '30.076', 'grad_norm': '34', 'counters/examples': 8192, 'counters/updates': 256}
train stats after 8224 examples: {'rewards_train/chosen': '0.011572', 'rewards_train/rejected': '-0.013215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024787', 'logps_train/rejected': '-117.8', 'logps_train/chosen': '-141.16', 'loss/train': '0.68389', 'examples_per_second': '31.965', 'grad_norm': '29.875', 'counters/examples': 8224, 'counters/updates': 257}
train stats after 8256 examples: {'rewards_train/chosen': '0.0092493', 'rewards_train/rejected': '0.0093755', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00012627', 'logps_train/rejected': '-158.68', 'logps_train/chosen': '-156.19', 'loss/train': '0.69837', 'examples_per_second': '30.54', 'grad_norm': '37.25', 'counters/examples': 8256, 'counters/updates': 258}
train stats after 8288 examples: {'rewards_train/chosen': '-0.025812', 'rewards_train/rejected': '0.030873', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.056685', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-140.12', 'loss/train': '0.72866', 'examples_per_second': '31.792', 'grad_norm': '60.5', 'counters/examples': 8288, 'counters/updates': 259}
train stats after 8320 examples: {'rewards_train/chosen': '0.029821', 'rewards_train/rejected': '0.014967', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014854', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-134.52', 'loss/train': '0.68802', 'examples_per_second': '31.732', 'grad_norm': '26.25', 'counters/examples': 8320, 'counters/updates': 260}
skipping logging after 8352 examples to avoid logging too frequently
train stats after 8384 examples: {'rewards_train/chosen': '0.012779', 'rewards_train/rejected': '-0.023701', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03648', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-144.86', 'loss/train': '0.68055', 'examples_per_second': '30.689', 'grad_norm': '33.5', 'counters/examples': 8384, 'counters/updates': 262}
train stats after 8416 examples: {'rewards_train/chosen': '0.01956', 'rewards_train/rejected': '-0.0027256', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022286', 'logps_train/rejected': '-134.11', 'logps_train/chosen': '-125.42', 'loss/train': '0.68733', 'examples_per_second': '31.296', 'grad_norm': '35.75', 'counters/examples': 8416, 'counters/updates': 263}
train stats after 8448 examples: {'rewards_train/chosen': '0.018661', 'rewards_train/rejected': '-0.012665', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031327', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-180.33', 'loss/train': '0.68178', 'examples_per_second': '29.853', 'grad_norm': '35.5', 'counters/examples': 8448, 'counters/updates': 264}
train stats after 8480 examples: {'rewards_train/chosen': '0.030071', 'rewards_train/rejected': '0.088811', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05874', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-153.3', 'loss/train': '0.74106', 'examples_per_second': '31.283', 'grad_norm': '45.75', 'counters/examples': 8480, 'counters/updates': 265}
skipping logging after 8512 examples to avoid logging too frequently
train stats after 8544 examples: {'rewards_train/chosen': '0.0064303', 'rewards_train/rejected': '0.040628', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034197', 'logps_train/rejected': '-142.18', 'logps_train/chosen': '-165.18', 'loss/train': '0.71565', 'examples_per_second': '30.259', 'grad_norm': '68.5', 'counters/examples': 8544, 'counters/updates': 267}
train stats after 8576 examples: {'rewards_train/chosen': '-0.0022788', 'rewards_train/rejected': '0.018429', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020708', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-150.25', 'loss/train': '0.7063', 'examples_per_second': '31.295', 'grad_norm': '35', 'counters/examples': 8576, 'counters/updates': 268}
train stats after 8608 examples: {'rewards_train/chosen': '-0.021535', 'rewards_train/rejected': '-0.018457', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0030786', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-133.52', 'loss/train': '0.69726', 'examples_per_second': '31.702', 'grad_norm': '28.625', 'counters/examples': 8608, 'counters/updates': 269}
train stats after 8640 examples: {'rewards_train/chosen': '0.0066366', 'rewards_train/rejected': '0.049583', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042946', 'logps_train/rejected': '-96.576', 'logps_train/chosen': '-136.64', 'loss/train': '0.71971', 'examples_per_second': '31.136', 'grad_norm': '63.5', 'counters/examples': 8640, 'counters/updates': 270}
train stats after 8672 examples: {'rewards_train/chosen': '-0.00085266', 'rewards_train/rejected': '-0.030619', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029766', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-146', 'loss/train': '0.68595', 'examples_per_second': '31.301', 'grad_norm': '41.5', 'counters/examples': 8672, 'counters/updates': 271}
train stats after 8704 examples: {'rewards_train/chosen': '0.024317', 'rewards_train/rejected': '0.0061115', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.018205', 'logps_train/rejected': '-105.09', 'logps_train/chosen': '-112.29', 'loss/train': '0.68644', 'examples_per_second': '31.577', 'grad_norm': '31', 'counters/examples': 8704, 'counters/updates': 272}
skipping logging after 8736 examples to avoid logging too frequently
train stats after 8768 examples: {'rewards_train/chosen': '0.020431', 'rewards_train/rejected': '0.009366', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011065', 'logps_train/rejected': '-145.44', 'logps_train/chosen': '-136.79', 'loss/train': '0.69199', 'examples_per_second': '30.222', 'grad_norm': '36.75', 'counters/examples': 8768, 'counters/updates': 274}
train stats after 8800 examples: {'rewards_train/chosen': '-0.040713', 'rewards_train/rejected': '-0.017413', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0233', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-142.69', 'loss/train': '0.70712', 'examples_per_second': '31.327', 'grad_norm': '38.5', 'counters/examples': 8800, 'counters/updates': 275}
train stats after 8832 examples: {'rewards_train/chosen': '-0.036478', 'rewards_train/rejected': '-0.0080857', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028393', 'logps_train/rejected': '-138.27', 'logps_train/chosen': '-162.19', 'loss/train': '0.71355', 'examples_per_second': '32.453', 'grad_norm': '38', 'counters/examples': 8832, 'counters/updates': 276}
skipping logging after 8864 examples to avoid logging too frequently
train stats after 8896 examples: {'rewards_train/chosen': '0.02079', 'rewards_train/rejected': '0.014047', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0067431', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-157.95', 'loss/train': '0.69391', 'examples_per_second': '31.338', 'grad_norm': '37', 'counters/examples': 8896, 'counters/updates': 278}
train stats after 8928 examples: {'rewards_train/chosen': '0.0077893', 'rewards_train/rejected': '0.028382', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020593', 'logps_train/rejected': '-116.08', 'logps_train/chosen': '-141.44', 'loss/train': '0.7079', 'examples_per_second': '32.446', 'grad_norm': '39', 'counters/examples': 8928, 'counters/updates': 279}
train stats after 8960 examples: {'rewards_train/chosen': '0.033833', 'rewards_train/rejected': '-0.025226', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059059', 'logps_train/rejected': '-102.85', 'logps_train/chosen': '-123.17', 'loss/train': '0.66828', 'examples_per_second': '30.298', 'grad_norm': '27.5', 'counters/examples': 8960, 'counters/updates': 280}
train stats after 8992 examples: {'rewards_train/chosen': '-0.027859', 'rewards_train/rejected': '-0.06987', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042011', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-151.38', 'loss/train': '0.68443', 'examples_per_second': '29.902', 'grad_norm': '37.25', 'counters/examples': 8992, 'counters/updates': 281}
train stats after 9024 examples: {'rewards_train/chosen': '0.017718', 'rewards_train/rejected': '0.027285', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0095678', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-160.22', 'loss/train': '0.70388', 'examples_per_second': '30.63', 'grad_norm': '34', 'counters/examples': 9024, 'counters/updates': 282}
skipping logging after 9056 examples to avoid logging too frequently
train stats after 9088 examples: {'rewards_train/chosen': '0.015318', 'rewards_train/rejected': '-0.027404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042723', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-125.58', 'loss/train': '0.6764', 'examples_per_second': '31.112', 'grad_norm': '29.125', 'counters/examples': 9088, 'counters/updates': 284}
train stats after 9120 examples: {'rewards_train/chosen': '-0.00027913', 'rewards_train/rejected': '-0.016117', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015838', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-150.16', 'loss/train': '0.68791', 'examples_per_second': '30.967', 'grad_norm': '29.25', 'counters/examples': 9120, 'counters/updates': 285}
train stats after 9152 examples: {'rewards_train/chosen': '0.0066215', 'rewards_train/rejected': '-0.025382', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032003', 'logps_train/rejected': '-127.85', 'logps_train/chosen': '-130.61', 'loss/train': '0.68464', 'examples_per_second': '30.622', 'grad_norm': '36', 'counters/examples': 9152, 'counters/updates': 286}
train stats after 9184 examples: {'rewards_train/chosen': '0.057101', 'rewards_train/rejected': '0.049795', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0073062', 'logps_train/rejected': '-145.01', 'logps_train/chosen': '-154.13', 'loss/train': '0.69634', 'examples_per_second': '31.322', 'grad_norm': '35.75', 'counters/examples': 9184, 'counters/updates': 287}
train stats after 9216 examples: {'rewards_train/chosen': '0.025824', 'rewards_train/rejected': '-0.059899', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085723', 'logps_train/rejected': '-136.07', 'logps_train/chosen': '-166.61', 'loss/train': '0.65351', 'examples_per_second': '31.3', 'grad_norm': '37.75', 'counters/examples': 9216, 'counters/updates': 288}
train stats after 9248 examples: {'rewards_train/chosen': '0.079399', 'rewards_train/rejected': '0.0027803', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076618', 'logps_train/rejected': '-111.47', 'logps_train/chosen': '-185.49', 'loss/train': '0.66205', 'examples_per_second': '31.035', 'grad_norm': '32.25', 'counters/examples': 9248, 'counters/updates': 289}
skipping logging after 9280 examples to avoid logging too frequently
train stats after 9312 examples: {'rewards_train/chosen': '-0.0046224', 'rewards_train/rejected': '0.034674', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.039296', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-139.06', 'loss/train': '0.7195', 'examples_per_second': '30.845', 'grad_norm': '35.25', 'counters/examples': 9312, 'counters/updates': 291}
train stats after 9344 examples: {'rewards_train/chosen': '-0.0056027', 'rewards_train/rejected': '-0.0093365', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0037338', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-157.29', 'loss/train': '0.69509', 'examples_per_second': '31.299', 'grad_norm': '33.5', 'counters/examples': 9344, 'counters/updates': 292}
skipping logging after 9376 examples to avoid logging too frequently
train stats after 9408 examples: {'rewards_train/chosen': '0.033421', 'rewards_train/rejected': '0.035046', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0016255', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-127.5', 'loss/train': '0.70532', 'examples_per_second': '31.334', 'grad_norm': '66.5', 'counters/examples': 9408, 'counters/updates': 294}
train stats after 9440 examples: {'rewards_train/chosen': '0.018315', 'rewards_train/rejected': '0.039713', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021398', 'logps_train/rejected': '-107.35', 'logps_train/chosen': '-127.02', 'loss/train': '0.70679', 'examples_per_second': '30.589', 'grad_norm': '38.5', 'counters/examples': 9440, 'counters/updates': 295}
train stats after 9472 examples: {'rewards_train/chosen': '0.046563', 'rewards_train/rejected': '0.043036', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0035277', 'logps_train/rejected': '-119.5', 'logps_train/chosen': '-107.5', 'loss/train': '0.69627', 'examples_per_second': '31.322', 'grad_norm': '34.5', 'counters/examples': 9472, 'counters/updates': 296}
skipping logging after 9504 examples to avoid logging too frequently
train stats after 9536 examples: {'rewards_train/chosen': '0.011175', 'rewards_train/rejected': '0.023643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012468', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-135.11', 'loss/train': '0.70355', 'examples_per_second': '31.316', 'grad_norm': '32', 'counters/examples': 9536, 'counters/updates': 298}
train stats after 9568 examples: {'rewards_train/chosen': '0.037981', 'rewards_train/rejected': '0.032503', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0054785', 'logps_train/rejected': '-134.85', 'logps_train/chosen': '-111.34', 'loss/train': '0.69411', 'examples_per_second': '31.328', 'grad_norm': '26.625', 'counters/examples': 9568, 'counters/updates': 299}
train stats after 9600 examples: {'rewards_train/chosen': '0.053011', 'rewards_train/rejected': '-0.010356', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063367', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-175.54', 'loss/train': '0.66733', 'examples_per_second': '30.993', 'grad_norm': '36.5', 'counters/examples': 9600, 'counters/updates': 300}
train stats after 9632 examples: {'rewards_train/chosen': '0.055373', 'rewards_train/rejected': '0.041927', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013446', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-189.4', 'loss/train': '0.69003', 'examples_per_second': '31.331', 'grad_norm': '37', 'counters/examples': 9632, 'counters/updates': 301}
train stats after 9664 examples: {'rewards_train/chosen': '-0.024351', 'rewards_train/rejected': '-0.023782', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00056956', 'logps_train/rejected': '-133.3', 'logps_train/chosen': '-178.58', 'loss/train': '0.69771', 'examples_per_second': '30.734', 'grad_norm': '33.5', 'counters/examples': 9664, 'counters/updates': 302}
train stats after 9696 examples: {'rewards_train/chosen': '-0.0049103', 'rewards_train/rejected': '-0.02728', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02237', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-143.9', 'loss/train': '0.68411', 'examples_per_second': '29.827', 'grad_norm': '32', 'counters/examples': 9696, 'counters/updates': 303}
train stats after 9728 examples: {'rewards_train/chosen': '0.0097421', 'rewards_train/rejected': '0.016222', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0064794', 'logps_train/rejected': '-126.78', 'logps_train/chosen': '-154.24', 'loss/train': '0.70105', 'examples_per_second': '31.419', 'grad_norm': '33.5', 'counters/examples': 9728, 'counters/updates': 304}
train stats after 9760 examples: {'rewards_train/chosen': '0.079182', 'rewards_train/rejected': '-0.018432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097615', 'logps_train/rejected': '-205.71', 'logps_train/chosen': '-174.17', 'loss/train': '0.65987', 'examples_per_second': '31.122', 'grad_norm': '48', 'counters/examples': 9760, 'counters/updates': 305}
train stats after 9792 examples: {'rewards_train/chosen': '-0.031542', 'rewards_train/rejected': '-0.0391', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0075575', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-168.33', 'loss/train': '0.69897', 'examples_per_second': '30.634', 'grad_norm': '40.75', 'counters/examples': 9792, 'counters/updates': 306}
train stats after 9824 examples: {'rewards_train/chosen': '-0.048719', 'rewards_train/rejected': '-0.0048437', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043875', 'logps_train/rejected': '-90.618', 'logps_train/chosen': '-120.17', 'loss/train': '0.72173', 'examples_per_second': '32.212', 'grad_norm': '26.125', 'counters/examples': 9824, 'counters/updates': 307}
train stats after 9856 examples: {'rewards_train/chosen': '0.067552', 'rewards_train/rejected': '0.04266', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024892', 'logps_train/rejected': '-96.177', 'logps_train/chosen': '-110.52', 'loss/train': '0.68454', 'examples_per_second': '31.778', 'grad_norm': '32.25', 'counters/examples': 9856, 'counters/updates': 308}
train stats after 9888 examples: {'rewards_train/chosen': '0.050337', 'rewards_train/rejected': '0.037322', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013015', 'logps_train/rejected': '-120.99', 'logps_train/chosen': '-130.1', 'loss/train': '0.69564', 'examples_per_second': '30.778', 'grad_norm': '33', 'counters/examples': 9888, 'counters/updates': 309}
train stats after 9920 examples: {'rewards_train/chosen': '-0.022225', 'rewards_train/rejected': '-0.0075796', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014646', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-103.47', 'loss/train': '0.70308', 'examples_per_second': '31.773', 'grad_norm': '26.5', 'counters/examples': 9920, 'counters/updates': 310}
train stats after 9952 examples: {'rewards_train/chosen': '0.046852', 'rewards_train/rejected': '0.011653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035198', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-161.51', 'loss/train': '0.67993', 'examples_per_second': '31.014', 'grad_norm': '37.25', 'counters/examples': 9952, 'counters/updates': 311}
train stats after 9984 examples: {'rewards_train/chosen': '-0.0056688', 'rewards_train/rejected': '0.02237', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028038', 'logps_train/rejected': '-124.35', 'logps_train/chosen': '-116.1', 'loss/train': '0.70973', 'examples_per_second': '30.649', 'grad_norm': '33.5', 'counters/examples': 9984, 'counters/updates': 312}
train stats after 10016 examples: {'rewards_train/chosen': '0.025374', 'rewards_train/rejected': '0.042104', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.01673', 'logps_train/rejected': '-103.14', 'logps_train/chosen': '-129.1', 'loss/train': '0.70371', 'examples_per_second': '31.788', 'grad_norm': '29.5', 'counters/examples': 10016, 'counters/updates': 313}
train stats after 10048 examples: {'rewards_train/chosen': '0.070787', 'rewards_train/rejected': '0.019457', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05133', 'logps_train/rejected': '-125.62', 'logps_train/chosen': '-142.49', 'loss/train': '0.67421', 'examples_per_second': '29.97', 'grad_norm': '38.75', 'counters/examples': 10048, 'counters/updates': 314}
train stats after 10080 examples: {'rewards_train/chosen': '-0.0061892', 'rewards_train/rejected': '-0.028901', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022712', 'logps_train/rejected': '-124.75', 'logps_train/chosen': '-173.62', 'loss/train': '0.68553', 'examples_per_second': '31.297', 'grad_norm': '30.875', 'counters/examples': 10080, 'counters/updates': 315}
train stats after 10112 examples: {'rewards_train/chosen': '-0.042436', 'rewards_train/rejected': '0.046551', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.088987', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-181.78', 'loss/train': '0.74444', 'examples_per_second': '31.264', 'grad_norm': '46.25', 'counters/examples': 10112, 'counters/updates': 316}
train stats after 10144 examples: {'rewards_train/chosen': '-0.005301', 'rewards_train/rejected': '-0.037279', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031978', 'logps_train/rejected': '-114.01', 'logps_train/chosen': '-161.88', 'loss/train': '0.68087', 'examples_per_second': '30.629', 'grad_norm': '26.5', 'counters/examples': 10144, 'counters/updates': 317}
skipping logging after 10176 examples to avoid logging too frequently
train stats after 10208 examples: {'rewards_train/chosen': '0.011201', 'rewards_train/rejected': '0.016013', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0048113', 'logps_train/rejected': '-143.11', 'logps_train/chosen': '-177.44', 'loss/train': '0.69886', 'examples_per_second': '30.348', 'grad_norm': '31', 'counters/examples': 10208, 'counters/updates': 319}
train stats after 10240 examples: {'rewards_train/chosen': '0.0042818', 'rewards_train/rejected': '0.015027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010745', 'logps_train/rejected': '-111.98', 'logps_train/chosen': '-158.89', 'loss/train': '0.70775', 'examples_per_second': '31.278', 'grad_norm': '74', 'counters/examples': 10240, 'counters/updates': 320}
train stats after 10272 examples: {'rewards_train/chosen': '0.0087633', 'rewards_train/rejected': '0.05142', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042657', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-180.28', 'loss/train': '0.72161', 'examples_per_second': '29.886', 'grad_norm': '31.75', 'counters/examples': 10272, 'counters/updates': 321}
train stats after 10304 examples: {'rewards_train/chosen': '0.023924', 'rewards_train/rejected': '0.0038705', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.020053', 'logps_train/rejected': '-121.61', 'logps_train/chosen': '-125.96', 'loss/train': '0.68953', 'examples_per_second': '31.302', 'grad_norm': '38.75', 'counters/examples': 10304, 'counters/updates': 322}
train stats after 10336 examples: {'rewards_train/chosen': '-0.011041', 'rewards_train/rejected': '0.052133', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.063174', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-170.75', 'loss/train': '0.73349', 'examples_per_second': '31.196', 'grad_norm': '41.25', 'counters/examples': 10336, 'counters/updates': 323}
train stats after 10368 examples: {'rewards_train/chosen': '-0.032776', 'rewards_train/rejected': '0.019376', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.052152', 'logps_train/rejected': '-96.232', 'logps_train/chosen': '-128.79', 'loss/train': '0.72637', 'examples_per_second': '32.255', 'grad_norm': '32.25', 'counters/examples': 10368, 'counters/updates': 324}
train stats after 10400 examples: {'rewards_train/chosen': '0.056379', 'rewards_train/rejected': '0.044198', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01218', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-206.17', 'loss/train': '0.71334', 'examples_per_second': '33.121', 'grad_norm': '60.25', 'counters/examples': 10400, 'counters/updates': 325}
train stats after 10432 examples: {'rewards_train/chosen': '-0.0022157', 'rewards_train/rejected': '0.014823', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017039', 'logps_train/rejected': '-162.91', 'logps_train/chosen': '-164.85', 'loss/train': '0.71208', 'examples_per_second': '31.225', 'grad_norm': '50.75', 'counters/examples': 10432, 'counters/updates': 326}
train stats after 10464 examples: {'rewards_train/chosen': '0.046258', 'rewards_train/rejected': '0.012366', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.033891', 'logps_train/rejected': '-140.8', 'logps_train/chosen': '-115.9', 'loss/train': '0.67897', 'examples_per_second': '31.287', 'grad_norm': '31.625', 'counters/examples': 10464, 'counters/updates': 327}
skipping logging after 10496 examples to avoid logging too frequently
train stats after 10528 examples: {'rewards_train/chosen': '0.16457', 'rewards_train/rejected': '0.13866', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025912', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-155.6', 'loss/train': '0.69521', 'examples_per_second': '31.146', 'grad_norm': '32.75', 'counters/examples': 10528, 'counters/updates': 329}
skipping logging after 10560 examples to avoid logging too frequently
train stats after 10592 examples: {'rewards_train/chosen': '-0.011698', 'rewards_train/rejected': '-0.0096292', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0020687', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-143.54', 'loss/train': '0.69757', 'examples_per_second': '30.624', 'grad_norm': '53.75', 'counters/examples': 10592, 'counters/updates': 331}
train stats after 10624 examples: {'rewards_train/chosen': '0.11923', 'rewards_train/rejected': '0.069063', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050169', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-125.75', 'loss/train': '0.67704', 'examples_per_second': '31.003', 'grad_norm': '42.25', 'counters/examples': 10624, 'counters/updates': 332}
train stats after 10656 examples: {'rewards_train/chosen': '0.01709', 'rewards_train/rejected': '0.018893', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0018025', 'logps_train/rejected': '-93.958', 'logps_train/chosen': '-123.16', 'loss/train': '0.69758', 'examples_per_second': '29.852', 'grad_norm': '28.25', 'counters/examples': 10656, 'counters/updates': 333}
train stats after 10688 examples: {'rewards_train/chosen': '-0.013428', 'rewards_train/rejected': '0.046631', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.060058', 'logps_train/rejected': '-134.8', 'logps_train/chosen': '-148.18', 'loss/train': '0.72852', 'examples_per_second': '31.289', 'grad_norm': '38', 'counters/examples': 10688, 'counters/updates': 334}
train stats after 10720 examples: {'rewards_train/chosen': '0.011714', 'rewards_train/rejected': '0.026415', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014701', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-150.89', 'loss/train': '0.71157', 'examples_per_second': '29.849', 'grad_norm': '46.75', 'counters/examples': 10720, 'counters/updates': 335}
skipping logging after 10752 examples to avoid logging too frequently
train stats after 10784 examples: {'rewards_train/chosen': '0.020243', 'rewards_train/rejected': '0.057092', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.036849', 'logps_train/rejected': '-149.17', 'logps_train/chosen': '-175.06', 'loss/train': '0.71623', 'examples_per_second': '31.328', 'grad_norm': '39.25', 'counters/examples': 10784, 'counters/updates': 337}
train stats after 10816 examples: {'rewards_train/chosen': '-0.0032171', 'rewards_train/rejected': '-0.0036526', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00043543', 'logps_train/rejected': '-196.74', 'logps_train/chosen': '-155.46', 'loss/train': '0.71013', 'examples_per_second': '30.879', 'grad_norm': '34.75', 'counters/examples': 10816, 'counters/updates': 338}
train stats after 10848 examples: {'rewards_train/chosen': '0.022576', 'rewards_train/rejected': '-0.016389', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038965', 'logps_train/rejected': '-172.73', 'logps_train/chosen': '-138.38', 'loss/train': '0.67935', 'examples_per_second': '31.259', 'grad_norm': '34.25', 'counters/examples': 10848, 'counters/updates': 339}
train stats after 10880 examples: {'rewards_train/chosen': '0.056', 'rewards_train/rejected': '0.021003', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034997', 'logps_train/rejected': '-177.3', 'logps_train/chosen': '-192.8', 'loss/train': '0.68085', 'examples_per_second': '30.425', 'grad_norm': '43', 'counters/examples': 10880, 'counters/updates': 340}
train stats after 10912 examples: {'rewards_train/chosen': '0.025575', 'rewards_train/rejected': '0.031913', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0063381', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-157.64', 'loss/train': '0.69944', 'examples_per_second': '31.293', 'grad_norm': '37.25', 'counters/examples': 10912, 'counters/updates': 341}
train stats after 10944 examples: {'rewards_train/chosen': '0.017228', 'rewards_train/rejected': '0.015775', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0014528', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-136.82', 'loss/train': '0.69762', 'examples_per_second': '31.35', 'grad_norm': '29.5', 'counters/examples': 10944, 'counters/updates': 342}
train stats after 10976 examples: {'rewards_train/chosen': '0.028483', 'rewards_train/rejected': '0.04436', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015877', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-167.88', 'loss/train': '0.70678', 'examples_per_second': '30.437', 'grad_norm': '31.375', 'counters/examples': 10976, 'counters/updates': 343}
skipping logging after 11008 examples to avoid logging too frequently
train stats after 11040 examples: {'rewards_train/chosen': '-0.0013727', 'rewards_train/rejected': '0.020962', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022335', 'logps_train/rejected': '-101.8', 'logps_train/chosen': '-144.4', 'loss/train': '0.70726', 'examples_per_second': '30.825', 'grad_norm': '32.5', 'counters/examples': 11040, 'counters/updates': 345}
train stats after 11072 examples: {'rewards_train/chosen': '0.014603', 'rewards_train/rejected': '-0.0032434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017846', 'logps_train/rejected': '-161.04', 'logps_train/chosen': '-138.79', 'loss/train': '0.68738', 'examples_per_second': '31.012', 'grad_norm': '38.75', 'counters/examples': 11072, 'counters/updates': 346}
train stats after 11104 examples: {'rewards_train/chosen': '0.01905', 'rewards_train/rejected': '-0.026973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046023', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-148.65', 'loss/train': '0.67499', 'examples_per_second': '31.245', 'grad_norm': '32.5', 'counters/examples': 11104, 'counters/updates': 347}
train stats after 11136 examples: {'rewards_train/chosen': '0.057145', 'rewards_train/rejected': '-0.0096942', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066839', 'logps_train/rejected': '-161.78', 'logps_train/chosen': '-185.78', 'loss/train': '0.66501', 'examples_per_second': '30.639', 'grad_norm': '36.25', 'counters/examples': 11136, 'counters/updates': 348}
train stats after 11168 examples: {'rewards_train/chosen': '0.049984', 'rewards_train/rejected': '0.017413', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032571', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-132.5', 'loss/train': '0.67995', 'examples_per_second': '31.287', 'grad_norm': '26.25', 'counters/examples': 11168, 'counters/updates': 349}
train stats after 11200 examples: {'rewards_train/chosen': '0.024152', 'rewards_train/rejected': '0.037206', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013054', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-160.54', 'loss/train': '0.70798', 'examples_per_second': '32.294', 'grad_norm': '36.75', 'counters/examples': 11200, 'counters/updates': 350}
train stats after 11232 examples: {'rewards_train/chosen': '0.027098', 'rewards_train/rejected': '-0.0077366', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034834', 'logps_train/rejected': '-165.19', 'logps_train/chosen': '-140.46', 'loss/train': '0.67992', 'examples_per_second': '31.285', 'grad_norm': '28.5', 'counters/examples': 11232, 'counters/updates': 351}
train stats after 11264 examples: {'rewards_train/chosen': '0.045044', 'rewards_train/rejected': '0.0041349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040909', 'logps_train/rejected': '-130.41', 'logps_train/chosen': '-150.23', 'loss/train': '0.68111', 'examples_per_second': '29.755', 'grad_norm': '42', 'counters/examples': 11264, 'counters/updates': 352}
skipping logging after 11296 examples to avoid logging too frequently
train stats after 11328 examples: {'rewards_train/chosen': '0.036502', 'rewards_train/rejected': '0.012969', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023533', 'logps_train/rejected': '-128.85', 'logps_train/chosen': '-151.19', 'loss/train': '0.68436', 'examples_per_second': '30.599', 'grad_norm': '33', 'counters/examples': 11328, 'counters/updates': 354}
train stats after 11360 examples: {'rewards_train/chosen': '-0.036174', 'rewards_train/rejected': '-0.045643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0094682', 'logps_train/rejected': '-144.76', 'logps_train/chosen': '-122.2', 'loss/train': '0.69469', 'examples_per_second': '30.344', 'grad_norm': '31.375', 'counters/examples': 11360, 'counters/updates': 355}
train stats after 11392 examples: {'rewards_train/chosen': '-0.02811', 'rewards_train/rejected': '0.010846', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.038956', 'logps_train/rejected': '-119.05', 'logps_train/chosen': '-165.44', 'loss/train': '0.71912', 'examples_per_second': '31.962', 'grad_norm': '30.75', 'counters/examples': 11392, 'counters/updates': 356}
train stats after 11424 examples: {'rewards_train/chosen': '0.031764', 'rewards_train/rejected': '0.017515', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014249', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-160.78', 'loss/train': '0.69298', 'examples_per_second': '30.046', 'grad_norm': '34.25', 'counters/examples': 11424, 'counters/updates': 357}
train stats after 11456 examples: {'rewards_train/chosen': '0.021706', 'rewards_train/rejected': '0.023258', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0015526', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-130.81', 'loss/train': '0.70371', 'examples_per_second': '31.314', 'grad_norm': '51', 'counters/examples': 11456, 'counters/updates': 358}
train stats after 11488 examples: {'rewards_train/chosen': '0.051482', 'rewards_train/rejected': '0.10246', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.05098', 'logps_train/rejected': '-105.4', 'logps_train/chosen': '-146.65', 'loss/train': '0.73655', 'examples_per_second': '32.562', 'grad_norm': '55.5', 'counters/examples': 11488, 'counters/updates': 359}
train stats after 11520 examples: {'rewards_train/chosen': '-0.092572', 'rewards_train/rejected': '0.045604', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.13818', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-187.49', 'loss/train': '0.79392', 'examples_per_second': '31.336', 'grad_norm': '64.5', 'counters/examples': 11520, 'counters/updates': 360}
train stats after 11552 examples: {'rewards_train/chosen': '0.062386', 'rewards_train/rejected': '0.012826', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04956', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-134.11', 'loss/train': '0.67443', 'examples_per_second': '30.913', 'grad_norm': '28.5', 'counters/examples': 11552, 'counters/updates': 361}
train stats after 11584 examples: {'rewards_train/chosen': '0.035453', 'rewards_train/rejected': '0.061805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026353', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-141.27', 'loss/train': '0.71035', 'examples_per_second': '31.013', 'grad_norm': '41', 'counters/examples': 11584, 'counters/updates': 362}
skipping logging after 11616 examples to avoid logging too frequently
train stats after 11648 examples: {'rewards_train/chosen': '0.013597', 'rewards_train/rejected': '0.0098837', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0037134', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-124.15', 'loss/train': '0.69458', 'examples_per_second': '30.397', 'grad_norm': '36', 'counters/examples': 11648, 'counters/updates': 364}
train stats after 11680 examples: {'rewards_train/chosen': '0.016984', 'rewards_train/rejected': '-0.041076', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058059', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-142.98', 'loss/train': '0.6715', 'examples_per_second': '32.822', 'grad_norm': '26.125', 'counters/examples': 11680, 'counters/updates': 365}
train stats after 11712 examples: {'rewards_train/chosen': '-0.048013', 'rewards_train/rejected': '0.042225', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.090238', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-130.12', 'loss/train': '0.7638', 'examples_per_second': '31.315', 'grad_norm': '70.5', 'counters/examples': 11712, 'counters/updates': 366}
train stats after 11744 examples: {'rewards_train/chosen': '0.041019', 'rewards_train/rejected': '-0.026231', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06725', 'logps_train/rejected': '-141.67', 'logps_train/chosen': '-194.67', 'loss/train': '0.66385', 'examples_per_second': '30.576', 'grad_norm': '41', 'counters/examples': 11744, 'counters/updates': 367}
train stats after 11776 examples: {'rewards_train/chosen': '-0.018594', 'rewards_train/rejected': '-0.022186', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0035919', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-157.35', 'loss/train': '0.69416', 'examples_per_second': '31.254', 'grad_norm': '34.25', 'counters/examples': 11776, 'counters/updates': 368}
train stats after 11808 examples: {'rewards_train/chosen': '-0.012013', 'rewards_train/rejected': '-0.048421', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036408', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-128.44', 'loss/train': '0.68169', 'examples_per_second': '31.092', 'grad_norm': '43.25', 'counters/examples': 11808, 'counters/updates': 369}
train stats after 11840 examples: {'rewards_train/chosen': '-0.009299', 'rewards_train/rejected': '-0.0014494', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0078496', 'logps_train/rejected': '-109.64', 'logps_train/chosen': '-150.51', 'loss/train': '0.70189', 'examples_per_second': '31.051', 'grad_norm': '30.125', 'counters/examples': 11840, 'counters/updates': 370}
train stats after 11872 examples: {'rewards_train/chosen': '0.037472', 'rewards_train/rejected': '0.082665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.045193', 'logps_train/rejected': '-176.05', 'logps_train/chosen': '-154.66', 'loss/train': '0.7288', 'examples_per_second': '31.171', 'grad_norm': '197', 'counters/examples': 11872, 'counters/updates': 371}
train stats after 11904 examples: {'rewards_train/chosen': '0.016008', 'rewards_train/rejected': '0.032341', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016334', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-152.05', 'loss/train': '0.70425', 'examples_per_second': '29.873', 'grad_norm': '31.875', 'counters/examples': 11904, 'counters/updates': 372}
train stats after 11936 examples: {'rewards_train/chosen': '0.041142', 'rewards_train/rejected': '0.042063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.00092119', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-143.03', 'loss/train': '0.69754', 'examples_per_second': '31.534', 'grad_norm': '30.625', 'counters/examples': 11936, 'counters/updates': 373}
train stats after 11968 examples: {'rewards_train/chosen': '0.057382', 'rewards_train/rejected': '0.025821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031562', 'logps_train/rejected': '-148', 'logps_train/chosen': '-159.49', 'loss/train': '0.68412', 'examples_per_second': '31.285', 'grad_norm': '51', 'counters/examples': 11968, 'counters/updates': 374}
train stats after 12000 examples: {'rewards_train/chosen': '-0.019749', 'rewards_train/rejected': '-0.010044', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0097057', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-119.77', 'loss/train': '0.70136', 'examples_per_second': '31.318', 'grad_norm': '27.125', 'counters/examples': 12000, 'counters/updates': 375}
Running evaluation after 12000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.13it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.77it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.87it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.87it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.79it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.01it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.92it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.87it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.84it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.85it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.85it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.87it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.78it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.83it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.74it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.84it/s]
eval after 12000: {'rewards_eval/chosen': '0.025121', 'rewards_eval/rejected': '0.0168', 'rewards_eval/accuracies': '0.5', 'rewards_eval/margins': '0.0083214', 'logps_eval/rejected': '-121.96', 'logps_eval/chosen': '-143.85', 'loss/eval': '0.69355'}
skipping save for non epoch
train stats after 12032 examples: {'rewards_train/chosen': '0.043978', 'rewards_train/rejected': '0.020575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023403', 'logps_train/rejected': '-111.45', 'logps_train/chosen': '-175.23', 'loss/train': '0.68966', 'examples_per_second': '31.526', 'grad_norm': '35.25', 'counters/examples': 12032, 'counters/updates': 376}
skipping logging after 12064 examples to avoid logging too frequently
train stats after 12096 examples: {'rewards_train/chosen': '0.11695', 'rewards_train/rejected': '0.047531', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069416', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-123.06', 'loss/train': '0.6671', 'examples_per_second': '29.858', 'grad_norm': '40.75', 'counters/examples': 12096, 'counters/updates': 378}
skipping logging after 12128 examples to avoid logging too frequently
train stats after 12160 examples: {'rewards_train/chosen': '0.081438', 'rewards_train/rejected': '0.065281', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016157', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-165.21', 'loss/train': '0.69143', 'examples_per_second': '32.225', 'grad_norm': '32', 'counters/examples': 12160, 'counters/updates': 380}
train stats after 12192 examples: {'rewards_train/chosen': '0.059399', 'rewards_train/rejected': '0.016411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042988', 'logps_train/rejected': '-141.31', 'logps_train/chosen': '-154.63', 'loss/train': '0.67619', 'examples_per_second': '30.256', 'grad_norm': '34', 'counters/examples': 12192, 'counters/updates': 381}
train stats after 12224 examples: {'rewards_train/chosen': '-0.038759', 'rewards_train/rejected': '-0.018164', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020595', 'logps_train/rejected': '-100.26', 'logps_train/chosen': '-114.81', 'loss/train': '0.70551', 'examples_per_second': '32.028', 'grad_norm': '28.25', 'counters/examples': 12224, 'counters/updates': 382}
train stats after 12256 examples: {'rewards_train/chosen': '0.033954', 'rewards_train/rejected': '0.068968', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035015', 'logps_train/rejected': '-118.72', 'logps_train/chosen': '-136.26', 'loss/train': '0.72743', 'examples_per_second': '31.046', 'grad_norm': '61.75', 'counters/examples': 12256, 'counters/updates': 383}
train stats after 12288 examples: {'rewards_train/chosen': '0.03316', 'rewards_train/rejected': '0.0050275', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028133', 'logps_train/rejected': '-146.25', 'logps_train/chosen': '-157.88', 'loss/train': '0.68249', 'examples_per_second': '30.695', 'grad_norm': '63.5', 'counters/examples': 12288, 'counters/updates': 384}
train stats after 12320 examples: {'rewards_train/chosen': '0.062112', 'rewards_train/rejected': '0.017731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044382', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-150.79', 'loss/train': '0.67413', 'examples_per_second': '32.983', 'grad_norm': '28.25', 'counters/examples': 12320, 'counters/updates': 385}
skipping logging after 12352 examples to avoid logging too frequently
train stats after 12384 examples: {'rewards_train/chosen': '0.0080277', 'rewards_train/rejected': '0.012158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0041306', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-122.69', 'loss/train': '0.69722', 'examples_per_second': '31.388', 'grad_norm': '26.375', 'counters/examples': 12384, 'counters/updates': 387}
train stats after 12416 examples: {'rewards_train/chosen': '0.026315', 'rewards_train/rejected': '0.024103', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0022122', 'logps_train/rejected': '-135.33', 'logps_train/chosen': '-110.19', 'loss/train': '0.69681', 'examples_per_second': '31.27', 'grad_norm': '31.5', 'counters/examples': 12416, 'counters/updates': 388}
train stats after 12448 examples: {'rewards_train/chosen': '0.043751', 'rewards_train/rejected': '0.0034454', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040306', 'logps_train/rejected': '-167.22', 'logps_train/chosen': '-186.73', 'loss/train': '0.68033', 'examples_per_second': '29.855', 'grad_norm': '34.5', 'counters/examples': 12448, 'counters/updates': 389}
train stats after 12480 examples: {'rewards_train/chosen': '0.082177', 'rewards_train/rejected': '0.042629', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039548', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-180.81', 'loss/train': '0.69145', 'examples_per_second': '30.533', 'grad_norm': '39.75', 'counters/examples': 12480, 'counters/updates': 390}
train stats after 12512 examples: {'rewards_train/chosen': '-0.074253', 'rewards_train/rejected': '-0.017054', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0572', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-151.25', 'loss/train': '0.73005', 'examples_per_second': '30.683', 'grad_norm': '48.25', 'counters/examples': 12512, 'counters/updates': 391}
train stats after 12544 examples: {'rewards_train/chosen': '0.012779', 'rewards_train/rejected': '-0.005977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018756', 'logps_train/rejected': '-92.405', 'logps_train/chosen': '-162.25', 'loss/train': '0.68627', 'examples_per_second': '31.29', 'grad_norm': '30.625', 'counters/examples': 12544, 'counters/updates': 392}
train stats after 12576 examples: {'rewards_train/chosen': '0.13005', 'rewards_train/rejected': '0.0036085', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12644', 'logps_train/rejected': '-131.68', 'logps_train/chosen': '-128.65', 'loss/train': '0.64286', 'examples_per_second': '31.671', 'grad_norm': '29.25', 'counters/examples': 12576, 'counters/updates': 393}
skipping logging after 12608 examples to avoid logging too frequently
train stats after 12640 examples: {'rewards_train/chosen': '0.09646', 'rewards_train/rejected': '0.031184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065276', 'logps_train/rejected': '-134.91', 'logps_train/chosen': '-146.54', 'loss/train': '0.67276', 'examples_per_second': '30', 'grad_norm': '38', 'counters/examples': 12640, 'counters/updates': 395}
train stats after 12672 examples: {'rewards_train/chosen': '0.051009', 'rewards_train/rejected': '0.004551', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046458', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-158.08', 'loss/train': '0.67692', 'examples_per_second': '30.229', 'grad_norm': '38.5', 'counters/examples': 12672, 'counters/updates': 396}
train stats after 12704 examples: {'rewards_train/chosen': '0.02713', 'rewards_train/rejected': '-0.018535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045665', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-131.34', 'loss/train': '0.67263', 'examples_per_second': '33.026', 'grad_norm': '27.125', 'counters/examples': 12704, 'counters/updates': 397}
train stats after 12736 examples: {'rewards_train/chosen': '0.0019697', 'rewards_train/rejected': '0.010986', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0090167', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-145.06', 'loss/train': '0.70154', 'examples_per_second': '25.003', 'grad_norm': '31.875', 'counters/examples': 12736, 'counters/updates': 398}
train stats after 12768 examples: {'rewards_train/chosen': '0.067184', 'rewards_train/rejected': '0.041523', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025661', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-142.13', 'loss/train': '0.68739', 'examples_per_second': '30.686', 'grad_norm': '29.5', 'counters/examples': 12768, 'counters/updates': 399}
train stats after 12800 examples: {'rewards_train/chosen': '0.11868', 'rewards_train/rejected': '0.067548', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051133', 'logps_train/rejected': '-123.87', 'logps_train/chosen': '-138.87', 'loss/train': '0.67732', 'examples_per_second': '31.311', 'grad_norm': '41.25', 'counters/examples': 12800, 'counters/updates': 400}
skipping logging after 12832 examples to avoid logging too frequently
train stats after 12864 examples: {'rewards_train/chosen': '-0.016404', 'rewards_train/rejected': '0.040277', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.056681', 'logps_train/rejected': '-137.37', 'logps_train/chosen': '-155.46', 'loss/train': '0.72474', 'examples_per_second': '31.308', 'grad_norm': '37.25', 'counters/examples': 12864, 'counters/updates': 402}
train stats after 12896 examples: {'rewards_train/chosen': '0.030066', 'rewards_train/rejected': '0.012451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017615', 'logps_train/rejected': '-124.22', 'logps_train/chosen': '-128.05', 'loss/train': '0.6869', 'examples_per_second': '31.188', 'grad_norm': '26.25', 'counters/examples': 12896, 'counters/updates': 403}
skipping logging after 12928 examples to avoid logging too frequently
train stats after 12960 examples: {'rewards_train/chosen': '0.037912', 'rewards_train/rejected': '0.0016324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03628', 'logps_train/rejected': '-132.76', 'logps_train/chosen': '-149.53', 'loss/train': '0.68231', 'examples_per_second': '33.466', 'grad_norm': '33.5', 'counters/examples': 12960, 'counters/updates': 405}
train stats after 12992 examples: {'rewards_train/chosen': '0.036593', 'rewards_train/rejected': '-0.033639', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070232', 'logps_train/rejected': '-154.3', 'logps_train/chosen': '-132.97', 'loss/train': '0.66451', 'examples_per_second': '30.811', 'grad_norm': '60.25', 'counters/examples': 12992, 'counters/updates': 406}
train stats after 13024 examples: {'rewards_train/chosen': '0.049412', 'rewards_train/rejected': '-0.016002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065415', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-126.07', 'loss/train': '0.67062', 'examples_per_second': '30.815', 'grad_norm': '27.25', 'counters/examples': 13024, 'counters/updates': 407}
train stats after 13056 examples: {'rewards_train/chosen': '-0.021235', 'rewards_train/rejected': '-0.037832', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016598', 'logps_train/rejected': '-106.6', 'logps_train/chosen': '-113.73', 'loss/train': '0.68842', 'examples_per_second': '31.95', 'grad_norm': '32.75', 'counters/examples': 13056, 'counters/updates': 408}
train stats after 13088 examples: {'rewards_train/chosen': '0.055138', 'rewards_train/rejected': '-0.011527', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066665', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-140.21', 'loss/train': '0.66625', 'examples_per_second': '33.021', 'grad_norm': '27.75', 'counters/examples': 13088, 'counters/updates': 409}
train stats after 13120 examples: {'rewards_train/chosen': '0.11613', 'rewards_train/rejected': '0.070145', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04599', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-182.08', 'loss/train': '0.69715', 'examples_per_second': '31.31', 'grad_norm': '47', 'counters/examples': 13120, 'counters/updates': 410}
train stats after 13152 examples: {'rewards_train/chosen': '0.026427', 'rewards_train/rejected': '0.037653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011226', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-146.65', 'loss/train': '0.70418', 'examples_per_second': '30.927', 'grad_norm': '38.75', 'counters/examples': 13152, 'counters/updates': 411}
train stats after 13184 examples: {'rewards_train/chosen': '0.026451', 'rewards_train/rejected': '-0.027483', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053934', 'logps_train/rejected': '-134.33', 'logps_train/chosen': '-142.75', 'loss/train': '0.66954', 'examples_per_second': '30.417', 'grad_norm': '35.75', 'counters/examples': 13184, 'counters/updates': 412}
skipping logging after 13216 examples to avoid logging too frequently
train stats after 13248 examples: {'rewards_train/chosen': '0.018661', 'rewards_train/rejected': '0.05775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039089', 'logps_train/rejected': '-117.11', 'logps_train/chosen': '-164.11', 'loss/train': '0.71915', 'examples_per_second': '33.838', 'grad_norm': '36.25', 'counters/examples': 13248, 'counters/updates': 414}
train stats after 13280 examples: {'rewards_train/chosen': '0.051818', 'rewards_train/rejected': '0.049773', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0020445', 'logps_train/rejected': '-143.13', 'logps_train/chosen': '-153.55', 'loss/train': '0.69693', 'examples_per_second': '31.254', 'grad_norm': '38.75', 'counters/examples': 13280, 'counters/updates': 415}
skipping logging after 13312 examples to avoid logging too frequently
train stats after 13344 examples: {'rewards_train/chosen': '0.026187', 'rewards_train/rejected': '-0.078888', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10507', 'logps_train/rejected': '-140.11', 'logps_train/chosen': '-150.42', 'loss/train': '0.65326', 'examples_per_second': '31.252', 'grad_norm': '33.75', 'counters/examples': 13344, 'counters/updates': 417}
train stats after 13376 examples: {'rewards_train/chosen': '0.030221', 'rewards_train/rejected': '0.035847', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0056257', 'logps_train/rejected': '-145.58', 'logps_train/chosen': '-142.76', 'loss/train': '0.70647', 'examples_per_second': '29.905', 'grad_norm': '36.5', 'counters/examples': 13376, 'counters/updates': 418}
train stats after 13408 examples: {'rewards_train/chosen': '0.059812', 'rewards_train/rejected': '0.0011312', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058681', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-154.83', 'loss/train': '0.67241', 'examples_per_second': '31.324', 'grad_norm': '28.375', 'counters/examples': 13408, 'counters/updates': 419}
train stats after 13440 examples: {'rewards_train/chosen': '0.07207', 'rewards_train/rejected': '0.023849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048221', 'logps_train/rejected': '-134.15', 'logps_train/chosen': '-139.7', 'loss/train': '0.67531', 'examples_per_second': '31.328', 'grad_norm': '49.25', 'counters/examples': 13440, 'counters/updates': 420}
train stats after 13472 examples: {'rewards_train/chosen': '0.029469', 'rewards_train/rejected': '-0.015927', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045396', 'logps_train/rejected': '-96.331', 'logps_train/chosen': '-138.26', 'loss/train': '0.67358', 'examples_per_second': '24.887', 'grad_norm': '28.625', 'counters/examples': 13472, 'counters/updates': 421}
train stats after 13504 examples: {'rewards_train/chosen': '0.0024562', 'rewards_train/rejected': '0.038359', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.035903', 'logps_train/rejected': '-154.68', 'logps_train/chosen': '-167.38', 'loss/train': '0.71675', 'examples_per_second': '31.323', 'grad_norm': '42.25', 'counters/examples': 13504, 'counters/updates': 422}
skipping logging after 13536 examples to avoid logging too frequently
train stats after 13568 examples: {'rewards_train/chosen': '0.029097', 'rewards_train/rejected': '0.011474', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.017623', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-145.72', 'loss/train': '0.69281', 'examples_per_second': '23.178', 'grad_norm': '32.25', 'counters/examples': 13568, 'counters/updates': 424}
train stats after 13600 examples: {'rewards_train/chosen': '0.04657', 'rewards_train/rejected': '-0.0064662', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053036', 'logps_train/rejected': '-143.33', 'logps_train/chosen': '-156.98', 'loss/train': '0.67014', 'examples_per_second': '30.856', 'grad_norm': '32.75', 'counters/examples': 13600, 'counters/updates': 425}
train stats after 13632 examples: {'rewards_train/chosen': '0.039368', 'rewards_train/rejected': '0.034554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0048134', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-124', 'loss/train': '0.701', 'examples_per_second': '30.633', 'grad_norm': '28', 'counters/examples': 13632, 'counters/updates': 426}
train stats after 13664 examples: {'rewards_train/chosen': '0.13536', 'rewards_train/rejected': '0.0085865', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12678', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-137.59', 'loss/train': '0.64636', 'examples_per_second': '33.088', 'grad_norm': '29.5', 'counters/examples': 13664, 'counters/updates': 427}
train stats after 13696 examples: {'rewards_train/chosen': '-0.030425', 'rewards_train/rejected': '-0.031764', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0013386', 'logps_train/rejected': '-153.4', 'logps_train/chosen': '-157.67', 'loss/train': '0.7004', 'examples_per_second': '31.77', 'grad_norm': '34.5', 'counters/examples': 13696, 'counters/updates': 428}
train stats after 13728 examples: {'rewards_train/chosen': '-0.016599', 'rewards_train/rejected': '-0.013992', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0026074', 'logps_train/rejected': '-120.7', 'logps_train/chosen': '-170.34', 'loss/train': '0.69834', 'examples_per_second': '30.38', 'grad_norm': '39.75', 'counters/examples': 13728, 'counters/updates': 429}
train stats after 13760 examples: {'rewards_train/chosen': '0.029117', 'rewards_train/rejected': '-0.0059857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035102', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-172.76', 'loss/train': '0.6817', 'examples_per_second': '30.149', 'grad_norm': '53.25', 'counters/examples': 13760, 'counters/updates': 430}
train stats after 13792 examples: {'rewards_train/chosen': '-0.020639', 'rewards_train/rejected': '0.0081641', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028803', 'logps_train/rejected': '-144.88', 'logps_train/chosen': '-163.09', 'loss/train': '0.71271', 'examples_per_second': '30.626', 'grad_norm': '33.75', 'counters/examples': 13792, 'counters/updates': 431}
train stats after 13824 examples: {'rewards_train/chosen': '0.010311', 'rewards_train/rejected': '0.057578', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.047267', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-127.38', 'loss/train': '0.72081', 'examples_per_second': '31.175', 'grad_norm': '41', 'counters/examples': 13824, 'counters/updates': 432}
train stats after 13856 examples: {'rewards_train/chosen': '0.02069', 'rewards_train/rejected': '0.022665', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0019748', 'logps_train/rejected': '-159.96', 'logps_train/chosen': '-151.29', 'loss/train': '0.69808', 'examples_per_second': '32.644', 'grad_norm': '44.5', 'counters/examples': 13856, 'counters/updates': 433}
train stats after 13888 examples: {'rewards_train/chosen': '-0.0059626', 'rewards_train/rejected': '-0.01275', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0067872', 'logps_train/rejected': '-158.93', 'logps_train/chosen': '-130.85', 'loss/train': '0.69456', 'examples_per_second': '31.261', 'grad_norm': '36.75', 'counters/examples': 13888, 'counters/updates': 434}
skipping logging after 13920 examples to avoid logging too frequently
train stats after 13952 examples: {'rewards_train/chosen': '0.029351', 'rewards_train/rejected': '0.026357', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0029937', 'logps_train/rejected': '-101.89', 'logps_train/chosen': '-130.93', 'loss/train': '0.69759', 'examples_per_second': '32.824', 'grad_norm': '31.125', 'counters/examples': 13952, 'counters/updates': 436}
train stats after 13984 examples: {'rewards_train/chosen': '0.02801', 'rewards_train/rejected': '0.016205', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.011804', 'logps_train/rejected': '-98.768', 'logps_train/chosen': '-126.28', 'loss/train': '0.69113', 'examples_per_second': '32.275', 'grad_norm': '36.5', 'counters/examples': 13984, 'counters/updates': 437}
train stats after 14016 examples: {'rewards_train/chosen': '0.12381', 'rewards_train/rejected': '0.025821', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097989', 'logps_train/rejected': '-141.2', 'logps_train/chosen': '-158.52', 'loss/train': '0.6656', 'examples_per_second': '32.102', 'grad_norm': '33', 'counters/examples': 14016, 'counters/updates': 438}
train stats after 14048 examples: {'rewards_train/chosen': '-0.011695', 'rewards_train/rejected': '-0.02841', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016716', 'logps_train/rejected': '-132.8', 'logps_train/chosen': '-118.48', 'loss/train': '0.68933', 'examples_per_second': '31.929', 'grad_norm': '29.75', 'counters/examples': 14048, 'counters/updates': 439}
train stats after 14080 examples: {'rewards_train/chosen': '0.014821', 'rewards_train/rejected': '0.0042955', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010525', 'logps_train/rejected': '-139.83', 'logps_train/chosen': '-130.99', 'loss/train': '0.68994', 'examples_per_second': '31.25', 'grad_norm': '30', 'counters/examples': 14080, 'counters/updates': 440}
train stats after 14112 examples: {'rewards_train/chosen': '0.0034076', 'rewards_train/rejected': '0.022954', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019547', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-157.31', 'loss/train': '0.71013', 'examples_per_second': '32.556', 'grad_norm': '38.75', 'counters/examples': 14112, 'counters/updates': 441}
train stats after 14144 examples: {'rewards_train/chosen': '0.034197', 'rewards_train/rejected': '0.049536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.01534', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-128.98', 'loss/train': '0.70665', 'examples_per_second': '31.265', 'grad_norm': '40.25', 'counters/examples': 14144, 'counters/updates': 442}
train stats after 14176 examples: {'rewards_train/chosen': '0.057378', 'rewards_train/rejected': '0.059153', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0017746', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-164.5', 'loss/train': '0.70341', 'examples_per_second': '32.656', 'grad_norm': '33.5', 'counters/examples': 14176, 'counters/updates': 443}
train stats after 14208 examples: {'rewards_train/chosen': '-0.0054273', 'rewards_train/rejected': '0.094256', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.099683', 'logps_train/rejected': '-104.01', 'logps_train/chosen': '-135.16', 'loss/train': '0.75212', 'examples_per_second': '32.996', 'grad_norm': '33.75', 'counters/examples': 14208, 'counters/updates': 444}
train stats after 14240 examples: {'rewards_train/chosen': '0.037144', 'rewards_train/rejected': '0.032221', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0049229', 'logps_train/rejected': '-109.46', 'logps_train/chosen': '-142.04', 'loss/train': '0.69448', 'examples_per_second': '31.284', 'grad_norm': '34.75', 'counters/examples': 14240, 'counters/updates': 445}
train stats after 14272 examples: {'rewards_train/chosen': '0.036917', 'rewards_train/rejected': '0.043095', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0061779', 'logps_train/rejected': '-102.76', 'logps_train/chosen': '-116.05', 'loss/train': '0.69871', 'examples_per_second': '31.909', 'grad_norm': '27.25', 'counters/examples': 14272, 'counters/updates': 446}
skipping logging after 14304 examples to avoid logging too frequently
train stats after 14336 examples: {'rewards_train/chosen': '0.0535', 'rewards_train/rejected': '0.065315', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.011815', 'logps_train/rejected': '-146.49', 'logps_train/chosen': '-170.17', 'loss/train': '0.71042', 'examples_per_second': '30.272', 'grad_norm': '32.5', 'counters/examples': 14336, 'counters/updates': 448}
train stats after 14368 examples: {'rewards_train/chosen': '0.0017963', 'rewards_train/rejected': '0.0083792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0065829', 'logps_train/rejected': '-101.28', 'logps_train/chosen': '-143.4', 'loss/train': '0.69956', 'examples_per_second': '29.858', 'grad_norm': '34', 'counters/examples': 14368, 'counters/updates': 449}
skipping logging after 14400 examples to avoid logging too frequently
train stats after 14432 examples: {'rewards_train/chosen': '-0.055792', 'rewards_train/rejected': '0.068162', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.12395', 'logps_train/rejected': '-145.16', 'logps_train/chosen': '-180.81', 'loss/train': '0.77217', 'examples_per_second': '31.416', 'grad_norm': '71', 'counters/examples': 14432, 'counters/updates': 451}
train stats after 14464 examples: {'rewards_train/chosen': '0.089109', 'rewards_train/rejected': '-0.0017268', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.090836', 'logps_train/rejected': '-140.73', 'logps_train/chosen': '-137.14', 'loss/train': '0.65858', 'examples_per_second': '31.389', 'grad_norm': '32.25', 'counters/examples': 14464, 'counters/updates': 452}
train stats after 14496 examples: {'rewards_train/chosen': '0.073928', 'rewards_train/rejected': '0.016358', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05757', 'logps_train/rejected': '-152.69', 'logps_train/chosen': '-162.87', 'loss/train': '0.67435', 'examples_per_second': '30.497', 'grad_norm': '38.75', 'counters/examples': 14496, 'counters/updates': 453}
train stats after 14528 examples: {'rewards_train/chosen': '0.002361', 'rewards_train/rejected': '0.0026619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0003009', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-119.31', 'loss/train': '0.69841', 'examples_per_second': '32.132', 'grad_norm': '32.5', 'counters/examples': 14528, 'counters/updates': 454}
skipping logging after 14560 examples to avoid logging too frequently
train stats after 14592 examples: {'rewards_train/chosen': '0.048251', 'rewards_train/rejected': '0.051301', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0030504', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-138.15', 'loss/train': '0.6982', 'examples_per_second': '31.195', 'grad_norm': '36.5', 'counters/examples': 14592, 'counters/updates': 456}
train stats after 14624 examples: {'rewards_train/chosen': '0.012464', 'rewards_train/rejected': '-0.030881', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043345', 'logps_train/rejected': '-87.651', 'logps_train/chosen': '-113.02', 'loss/train': '0.67769', 'examples_per_second': '31.166', 'grad_norm': '37', 'counters/examples': 14624, 'counters/updates': 457}
train stats after 14656 examples: {'rewards_train/chosen': '0.027277', 'rewards_train/rejected': '0.029983', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027055', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-132.7', 'loss/train': '0.70075', 'examples_per_second': '31.244', 'grad_norm': '31.5', 'counters/examples': 14656, 'counters/updates': 458}
skipping logging after 14688 examples to avoid logging too frequently
train stats after 14720 examples: {'rewards_train/chosen': '0.056624', 'rewards_train/rejected': '-0.0022527', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058877', 'logps_train/rejected': '-157.86', 'logps_train/chosen': '-183.74', 'loss/train': '0.66905', 'examples_per_second': '31.11', 'grad_norm': '38', 'counters/examples': 14720, 'counters/updates': 460}
train stats after 14752 examples: {'rewards_train/chosen': '0.03943', 'rewards_train/rejected': '0.054501', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01507', 'logps_train/rejected': '-163.64', 'logps_train/chosen': '-165.03', 'loss/train': '0.70772', 'examples_per_second': '31.06', 'grad_norm': '31.125', 'counters/examples': 14752, 'counters/updates': 461}
skipping logging after 14784 examples to avoid logging too frequently
train stats after 14816 examples: {'rewards_train/chosen': '0.040351', 'rewards_train/rejected': '0.082572', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.042221', 'logps_train/rejected': '-151.17', 'logps_train/chosen': '-128.85', 'loss/train': '0.72124', 'examples_per_second': '31.273', 'grad_norm': '37.5', 'counters/examples': 14816, 'counters/updates': 463}
skipping logging after 14848 examples to avoid logging too frequently
train stats after 14880 examples: {'rewards_train/chosen': '0.013971', 'rewards_train/rejected': '0.0069675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0070037', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-142.87', 'loss/train': '0.69477', 'examples_per_second': '32.311', 'grad_norm': '32.25', 'counters/examples': 14880, 'counters/updates': 465}
skipping logging after 14912 examples to avoid logging too frequently
train stats after 14944 examples: {'rewards_train/chosen': '0.012963', 'rewards_train/rejected': '-0.10017', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11313', 'logps_train/rejected': '-135.57', 'logps_train/chosen': '-131.72', 'loss/train': '0.64155', 'examples_per_second': '31.268', 'grad_norm': '35.5', 'counters/examples': 14944, 'counters/updates': 467}
train stats after 14976 examples: {'rewards_train/chosen': '0.11944', 'rewards_train/rejected': '0.011153', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10829', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-119.64', 'loss/train': '0.66474', 'examples_per_second': '32.382', 'grad_norm': '28.375', 'counters/examples': 14976, 'counters/updates': 468}
train stats after 15008 examples: {'rewards_train/chosen': '0.044133', 'rewards_train/rejected': '0.021269', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022864', 'logps_train/rejected': '-98.007', 'logps_train/chosen': '-99.081', 'loss/train': '0.68458', 'examples_per_second': '30.748', 'grad_norm': '25.25', 'counters/examples': 15008, 'counters/updates': 469}
skipping logging after 15040 examples to avoid logging too frequently
train stats after 15072 examples: {'rewards_train/chosen': '-0.002053', 'rewards_train/rejected': '0.052659', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.054712', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-150.43', 'loss/train': '0.72734', 'examples_per_second': '30.152', 'grad_norm': '35.75', 'counters/examples': 15072, 'counters/updates': 471}
train stats after 15104 examples: {'rewards_train/chosen': '0.047126', 'rewards_train/rejected': '0.077994', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030867', 'logps_train/rejected': '-141.11', 'logps_train/chosen': '-144.31', 'loss/train': '0.75001', 'examples_per_second': '30.654', 'grad_norm': '81.5', 'counters/examples': 15104, 'counters/updates': 472}
train stats after 15136 examples: {'rewards_train/chosen': '-0.012727', 'rewards_train/rejected': '0.027556', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.040282', 'logps_train/rejected': '-147.03', 'logps_train/chosen': '-149.52', 'loss/train': '0.71889', 'examples_per_second': '32.356', 'grad_norm': '32.25', 'counters/examples': 15136, 'counters/updates': 473}
train stats after 15168 examples: {'rewards_train/chosen': '0.041534', 'rewards_train/rejected': '-0.017167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058701', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-121.97', 'loss/train': '0.66791', 'examples_per_second': '31.317', 'grad_norm': '26.75', 'counters/examples': 15168, 'counters/updates': 474}
train stats after 15200 examples: {'rewards_train/chosen': '0.063733', 'rewards_train/rejected': '-0.003613', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067346', 'logps_train/rejected': '-121.14', 'logps_train/chosen': '-170.62', 'loss/train': '0.66706', 'examples_per_second': '32.73', 'grad_norm': '28.5', 'counters/examples': 15200, 'counters/updates': 475}
train stats after 15232 examples: {'rewards_train/chosen': '0.014548', 'rewards_train/rejected': '0.033733', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019185', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-116.09', 'loss/train': '0.70595', 'examples_per_second': '31.029', 'grad_norm': '25.125', 'counters/examples': 15232, 'counters/updates': 476}
train stats after 15264 examples: {'rewards_train/chosen': '-0.0015605', 'rewards_train/rejected': '-0.0043662', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0028056', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-158.49', 'loss/train': '0.69718', 'examples_per_second': '30.527', 'grad_norm': '45', 'counters/examples': 15264, 'counters/updates': 477}
train stats after 15296 examples: {'rewards_train/chosen': '-0.021326', 'rewards_train/rejected': '0.025166', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046492', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-162.25', 'loss/train': '0.72125', 'examples_per_second': '29.654', 'grad_norm': '55.25', 'counters/examples': 15296, 'counters/updates': 478}
skipping logging after 15328 examples to avoid logging too frequently
train stats after 15360 examples: {'rewards_train/chosen': '0.091195', 'rewards_train/rejected': '0.021217', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069978', 'logps_train/rejected': '-111.86', 'logps_train/chosen': '-156.72', 'loss/train': '0.67315', 'examples_per_second': '30.041', 'grad_norm': '33.75', 'counters/examples': 15360, 'counters/updates': 480}
train stats after 15392 examples: {'rewards_train/chosen': '-0.006553', 'rewards_train/rejected': '-0.010112', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.003559', 'logps_train/rejected': '-109.81', 'logps_train/chosen': '-141.22', 'loss/train': '0.6949', 'examples_per_second': '31.584', 'grad_norm': '28.625', 'counters/examples': 15392, 'counters/updates': 481}
skipping logging after 15424 examples to avoid logging too frequently
train stats after 15456 examples: {'rewards_train/chosen': '-0.041377', 'rewards_train/rejected': '-0.007645', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033732', 'logps_train/rejected': '-101.43', 'logps_train/chosen': '-133.29', 'loss/train': '0.71373', 'examples_per_second': '31.302', 'grad_norm': '46.5', 'counters/examples': 15456, 'counters/updates': 483}
train stats after 15488 examples: {'rewards_train/chosen': '0.025566', 'rewards_train/rejected': '0.027895', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0023291', 'logps_train/rejected': '-98.139', 'logps_train/chosen': '-119.53', 'loss/train': '0.69879', 'examples_per_second': '31.341', 'grad_norm': '28.5', 'counters/examples': 15488, 'counters/updates': 484}
train stats after 15520 examples: {'rewards_train/chosen': '0.12723', 'rewards_train/rejected': '0.057834', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069398', 'logps_train/rejected': '-178.74', 'logps_train/chosen': '-182.69', 'loss/train': '0.66842', 'examples_per_second': '32.312', 'grad_norm': '33.25', 'counters/examples': 15520, 'counters/updates': 485}
train stats after 15552 examples: {'rewards_train/chosen': '0.039988', 'rewards_train/rejected': '0.023727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016262', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-137.69', 'loss/train': '0.69103', 'examples_per_second': '32.738', 'grad_norm': '30.125', 'counters/examples': 15552, 'counters/updates': 486}
train stats after 15584 examples: {'rewards_train/chosen': '0.0069203', 'rewards_train/rejected': '0.025793', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018873', 'logps_train/rejected': '-126.48', 'logps_train/chosen': '-152.22', 'loss/train': '0.70924', 'examples_per_second': '30.009', 'grad_norm': '35', 'counters/examples': 15584, 'counters/updates': 487}
train stats after 15616 examples: {'rewards_train/chosen': '0.049981', 'rewards_train/rejected': '0.027086', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022895', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-147.28', 'loss/train': '0.69011', 'examples_per_second': '31.295', 'grad_norm': '34.25', 'counters/examples': 15616, 'counters/updates': 488}
skipping logging after 15648 examples to avoid logging too frequently
train stats after 15680 examples: {'rewards_train/chosen': '0.0054622', 'rewards_train/rejected': '0.023364', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.017902', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-115.43', 'loss/train': '0.70755', 'examples_per_second': '32.047', 'grad_norm': '29', 'counters/examples': 15680, 'counters/updates': 490}
train stats after 15712 examples: {'rewards_train/chosen': '0.084711', 'rewards_train/rejected': '0.0073808', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07733', 'logps_train/rejected': '-147.05', 'logps_train/chosen': '-141.5', 'loss/train': '0.6734', 'examples_per_second': '32.223', 'grad_norm': '33.75', 'counters/examples': 15712, 'counters/updates': 491}
train stats after 15744 examples: {'rewards_train/chosen': '0.052402', 'rewards_train/rejected': '-0.013884', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066286', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-117.84', 'loss/train': '0.66312', 'examples_per_second': '31.076', 'grad_norm': '30.125', 'counters/examples': 15744, 'counters/updates': 492}
train stats after 15776 examples: {'rewards_train/chosen': '0.05468', 'rewards_train/rejected': '-0.016397', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071077', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-141.81', 'loss/train': '0.6628', 'examples_per_second': '31.361', 'grad_norm': '26.125', 'counters/examples': 15776, 'counters/updates': 493}
train stats after 15808 examples: {'rewards_train/chosen': '0.0087971', 'rewards_train/rejected': '0.00487', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0039271', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-122.95', 'loss/train': '0.69604', 'examples_per_second': '31.303', 'grad_norm': '36.5', 'counters/examples': 15808, 'counters/updates': 494}
train stats after 15840 examples: {'rewards_train/chosen': '0.059616', 'rewards_train/rejected': '0.05437', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0052464', 'logps_train/rejected': '-141.61', 'logps_train/chosen': '-120.67', 'loss/train': '0.70577', 'examples_per_second': '31.494', 'grad_norm': '36.5', 'counters/examples': 15840, 'counters/updates': 495}
train stats after 15872 examples: {'rewards_train/chosen': '0.033267', 'rewards_train/rejected': '0.021985', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011281', 'logps_train/rejected': '-141.84', 'logps_train/chosen': '-142.07', 'loss/train': '0.69121', 'examples_per_second': '31.33', 'grad_norm': '40.75', 'counters/examples': 15872, 'counters/updates': 496}
skipping logging after 15904 examples to avoid logging too frequently
train stats after 15936 examples: {'rewards_train/chosen': '-0.003153', 'rewards_train/rejected': '0.054482', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.057635', 'logps_train/rejected': '-170.91', 'logps_train/chosen': '-119.15', 'loss/train': '0.73045', 'examples_per_second': '34.982', 'grad_norm': '44', 'counters/examples': 15936, 'counters/updates': 498}
train stats after 15968 examples: {'rewards_train/chosen': '0.015076', 'rewards_train/rejected': '0.035024', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019947', 'logps_train/rejected': '-155.61', 'logps_train/chosen': '-154.08', 'loss/train': '0.70924', 'examples_per_second': '30.527', 'grad_norm': '38.5', 'counters/examples': 15968, 'counters/updates': 499}
train stats after 16000 examples: {'rewards_train/chosen': '0.054353', 'rewards_train/rejected': '0.033003', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02135', 'logps_train/rejected': '-121.34', 'logps_train/chosen': '-161.19', 'loss/train': '0.6873', 'examples_per_second': '31.282', 'grad_norm': '35.25', 'counters/examples': 16000, 'counters/updates': 500}
train stats after 16032 examples: {'rewards_train/chosen': '0.039334', 'rewards_train/rejected': '0.037112', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0022214', 'logps_train/rejected': '-107.44', 'logps_train/chosen': '-167.75', 'loss/train': '0.70042', 'examples_per_second': '31.321', 'grad_norm': '30.625', 'counters/examples': 16032, 'counters/updates': 501}
skipping logging after 16064 examples to avoid logging too frequently
train stats after 16096 examples: {'rewards_train/chosen': '-0.0089321', 'rewards_train/rejected': '0.064183', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.073115', 'logps_train/rejected': '-172.3', 'logps_train/chosen': '-164.2', 'loss/train': '0.73709', 'examples_per_second': '31.154', 'grad_norm': '40.25', 'counters/examples': 16096, 'counters/updates': 503}
train stats after 16128 examples: {'rewards_train/chosen': '0.090836', 'rewards_train/rejected': '0.050351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040485', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-156.39', 'loss/train': '0.67733', 'examples_per_second': '31.281', 'grad_norm': '30', 'counters/examples': 16128, 'counters/updates': 504}
train stats after 16160 examples: {'rewards_train/chosen': '0.030857', 'rewards_train/rejected': '0.026295', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0045621', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-126.15', 'loss/train': '0.69406', 'examples_per_second': '31.346', 'grad_norm': '31.75', 'counters/examples': 16160, 'counters/updates': 505}
skipping logging after 16192 examples to avoid logging too frequently
train stats after 16224 examples: {'rewards_train/chosen': '0.01309', 'rewards_train/rejected': '0.016791', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0037007', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-139.01', 'loss/train': '0.70052', 'examples_per_second': '31.414', 'grad_norm': '35.25', 'counters/examples': 16224, 'counters/updates': 507}
skipping logging after 16256 examples to avoid logging too frequently
train stats after 16288 examples: {'rewards_train/chosen': '0.10665', 'rewards_train/rejected': '0.065218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04143', 'logps_train/rejected': '-94.819', 'logps_train/chosen': '-125.65', 'loss/train': '0.67668', 'examples_per_second': '30.535', 'grad_norm': '25', 'counters/examples': 16288, 'counters/updates': 509}
train stats after 16320 examples: {'rewards_train/chosen': '0.047343', 'rewards_train/rejected': '0.087455', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040113', 'logps_train/rejected': '-141.86', 'logps_train/chosen': '-161.24', 'loss/train': '0.71981', 'examples_per_second': '33.279', 'grad_norm': '32.5', 'counters/examples': 16320, 'counters/updates': 510}
train stats after 16352 examples: {'rewards_train/chosen': '0.058295', 'rewards_train/rejected': '0.039171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.019124', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-136.56', 'loss/train': '0.68934', 'examples_per_second': '30.114', 'grad_norm': '31.875', 'counters/examples': 16352, 'counters/updates': 511}
train stats after 16384 examples: {'rewards_train/chosen': '0.048491', 'rewards_train/rejected': '0.056336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0078451', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-161.68', 'loss/train': '0.70274', 'examples_per_second': '31.275', 'grad_norm': '44.25', 'counters/examples': 16384, 'counters/updates': 512}
skipping logging after 16416 examples to avoid logging too frequently
train stats after 16448 examples: {'rewards_train/chosen': '0.072578', 'rewards_train/rejected': '0.052008', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02057', 'logps_train/rejected': '-141.03', 'logps_train/chosen': '-160.69', 'loss/train': '0.68996', 'examples_per_second': '31.288', 'grad_norm': '38.25', 'counters/examples': 16448, 'counters/updates': 514}
train stats after 16480 examples: {'rewards_train/chosen': '-0.0071267', 'rewards_train/rejected': '-0.0063307', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00079607', 'logps_train/rejected': '-97.03', 'logps_train/chosen': '-123.03', 'loss/train': '0.6963', 'examples_per_second': '31.052', 'grad_norm': '32', 'counters/examples': 16480, 'counters/updates': 515}
skipping logging after 16512 examples to avoid logging too frequently
train stats after 16544 examples: {'rewards_train/chosen': '0.046819', 'rewards_train/rejected': '-0.013374', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060193', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-176.88', 'loss/train': '0.66993', 'examples_per_second': '31.491', 'grad_norm': '28.25', 'counters/examples': 16544, 'counters/updates': 517}
train stats after 16576 examples: {'rewards_train/chosen': '0.022569', 'rewards_train/rejected': '0.011391', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011178', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-109.65', 'loss/train': '0.69083', 'examples_per_second': '30.372', 'grad_norm': '26.5', 'counters/examples': 16576, 'counters/updates': 518}
train stats after 16608 examples: {'rewards_train/chosen': '0.029793', 'rewards_train/rejected': '0.053473', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.02368', 'logps_train/rejected': '-107', 'logps_train/chosen': '-117.83', 'loss/train': '0.71402', 'examples_per_second': '32.373', 'grad_norm': '51.25', 'counters/examples': 16608, 'counters/updates': 519}
train stats after 16640 examples: {'rewards_train/chosen': '0.2242', 'rewards_train/rejected': '0.02693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19727', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-169.23', 'loss/train': '0.67321', 'examples_per_second': '29.864', 'grad_norm': '34.25', 'counters/examples': 16640, 'counters/updates': 520}
train stats after 16672 examples: {'rewards_train/chosen': '-0.022675', 'rewards_train/rejected': '-0.048167', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.025492', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-153.29', 'loss/train': '0.6854', 'examples_per_second': '30.404', 'grad_norm': '47', 'counters/examples': 16672, 'counters/updates': 521}
train stats after 16704 examples: {'rewards_train/chosen': '0.067298', 'rewards_train/rejected': '0.0039483', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06335', 'logps_train/rejected': '-137.64', 'logps_train/chosen': '-132.36', 'loss/train': '0.668', 'examples_per_second': '30.463', 'grad_norm': '31.625', 'counters/examples': 16704, 'counters/updates': 522}
train stats after 16736 examples: {'rewards_train/chosen': '-0.020784', 'rewards_train/rejected': '0.0037909', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024574', 'logps_train/rejected': '-134.97', 'logps_train/chosen': '-122.13', 'loss/train': '0.7122', 'examples_per_second': '31.261', 'grad_norm': '45', 'counters/examples': 16736, 'counters/updates': 523}
train stats after 16768 examples: {'rewards_train/chosen': '0.024181', 'rewards_train/rejected': '0.072245', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.048064', 'logps_train/rejected': '-120.39', 'logps_train/chosen': '-150.47', 'loss/train': '0.72558', 'examples_per_second': '30.896', 'grad_norm': '39', 'counters/examples': 16768, 'counters/updates': 524}
train stats after 16800 examples: {'rewards_train/chosen': '0.00056306', 'rewards_train/rejected': '0.016288', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015725', 'logps_train/rejected': '-180.31', 'logps_train/chosen': '-164.22', 'loss/train': '0.70449', 'examples_per_second': '31.341', 'grad_norm': '50.25', 'counters/examples': 16800, 'counters/updates': 525}
train stats after 16832 examples: {'rewards_train/chosen': '0.029408', 'rewards_train/rejected': '-0.034487', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063895', 'logps_train/rejected': '-156.34', 'logps_train/chosen': '-123.1', 'loss/train': '0.66362', 'examples_per_second': '30.132', 'grad_norm': '25.5', 'counters/examples': 16832, 'counters/updates': 526}
train stats after 16864 examples: {'rewards_train/chosen': '0.035758', 'rewards_train/rejected': '-0.034585', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.070344', 'logps_train/rejected': '-192.12', 'logps_train/chosen': '-136.81', 'loss/train': '0.66261', 'examples_per_second': '31.258', 'grad_norm': '61.75', 'counters/examples': 16864, 'counters/updates': 527}
train stats after 16896 examples: {'rewards_train/chosen': '-0.0081211', 'rewards_train/rejected': '-0.059933', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051812', 'logps_train/rejected': '-145.06', 'logps_train/chosen': '-174.68', 'loss/train': '0.67665', 'examples_per_second': '30.81', 'grad_norm': '37.25', 'counters/examples': 16896, 'counters/updates': 528}
train stats after 16928 examples: {'rewards_train/chosen': '0.055284', 'rewards_train/rejected': '0.071733', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016449', 'logps_train/rejected': '-135.73', 'logps_train/chosen': '-157.82', 'loss/train': '0.71722', 'examples_per_second': '29.69', 'grad_norm': '36.75', 'counters/examples': 16928, 'counters/updates': 529}
train stats after 16960 examples: {'rewards_train/chosen': '0.07479', 'rewards_train/rejected': '0.010968', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063822', 'logps_train/rejected': '-78.533', 'logps_train/chosen': '-119.79', 'loss/train': '0.66561', 'examples_per_second': '30.503', 'grad_norm': '30.125', 'counters/examples': 16960, 'counters/updates': 530}
train stats after 16992 examples: {'rewards_train/chosen': '0.040474', 'rewards_train/rejected': '0.082168', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.041695', 'logps_train/rejected': '-200.77', 'logps_train/chosen': '-159.54', 'loss/train': '0.72596', 'examples_per_second': '29.773', 'grad_norm': '47.25', 'counters/examples': 16992, 'counters/updates': 531}
train stats after 17024 examples: {'rewards_train/chosen': '0.011401', 'rewards_train/rejected': '0.0036011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0077998', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-121.02', 'loss/train': '0.69096', 'examples_per_second': '31.227', 'grad_norm': '39', 'counters/examples': 17024, 'counters/updates': 532}
train stats after 17056 examples: {'rewards_train/chosen': '0.075172', 'rewards_train/rejected': '0.071948', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0032243', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-192.38', 'loss/train': '0.69888', 'examples_per_second': '31.349', 'grad_norm': '36.5', 'counters/examples': 17056, 'counters/updates': 533}
train stats after 17088 examples: {'rewards_train/chosen': '0.083519', 'rewards_train/rejected': '0.041555', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041964', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-174.55', 'loss/train': '0.68716', 'examples_per_second': '29.916', 'grad_norm': '35.75', 'counters/examples': 17088, 'counters/updates': 534}
train stats after 17120 examples: {'rewards_train/chosen': '0.030412', 'rewards_train/rejected': '0.0035458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026866', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-140.29', 'loss/train': '0.69034', 'examples_per_second': '31.232', 'grad_norm': '34.25', 'counters/examples': 17120, 'counters/updates': 535}
train stats after 17152 examples: {'rewards_train/chosen': '0.10204', 'rewards_train/rejected': '0.032733', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069307', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-156.61', 'loss/train': '0.67906', 'examples_per_second': '29.959', 'grad_norm': '32.25', 'counters/examples': 17152, 'counters/updates': 536}
skipping logging after 17184 examples to avoid logging too frequently
train stats after 17216 examples: {'rewards_train/chosen': '0.018035', 'rewards_train/rejected': '0.023993', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0059576', 'logps_train/rejected': '-112.9', 'logps_train/chosen': '-131.01', 'loss/train': '0.70251', 'examples_per_second': '30.161', 'grad_norm': '29.75', 'counters/examples': 17216, 'counters/updates': 538}
train stats after 17248 examples: {'rewards_train/chosen': '0.038713', 'rewards_train/rejected': '0.029997', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087158', 'logps_train/rejected': '-125.64', 'logps_train/chosen': '-163.62', 'loss/train': '0.69473', 'examples_per_second': '32.697', 'grad_norm': '42', 'counters/examples': 17248, 'counters/updates': 539}
train stats after 17280 examples: {'rewards_train/chosen': '0.013345', 'rewards_train/rejected': '-0.014403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027748', 'logps_train/rejected': '-139.05', 'logps_train/chosen': '-119.34', 'loss/train': '0.68318', 'examples_per_second': '31.253', 'grad_norm': '34.75', 'counters/examples': 17280, 'counters/updates': 540}
train stats after 17312 examples: {'rewards_train/chosen': '0.084166', 'rewards_train/rejected': '0.034348', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049818', 'logps_train/rejected': '-76.348', 'logps_train/chosen': '-149.74', 'loss/train': '0.67084', 'examples_per_second': '32.467', 'grad_norm': '26.125', 'counters/examples': 17312, 'counters/updates': 541}
train stats after 17344 examples: {'rewards_train/chosen': '0.0012768', 'rewards_train/rejected': '0.0027502', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014734', 'logps_train/rejected': '-106.71', 'logps_train/chosen': '-129.26', 'loss/train': '0.69614', 'examples_per_second': '32.476', 'grad_norm': '28', 'counters/examples': 17344, 'counters/updates': 542}
skipping logging after 17376 examples to avoid logging too frequently
train stats after 17408 examples: {'rewards_train/chosen': '0.067234', 'rewards_train/rejected': '-0.001035', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068269', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-147.54', 'loss/train': '0.66254', 'examples_per_second': '30.108', 'grad_norm': '26.125', 'counters/examples': 17408, 'counters/updates': 544}
train stats after 17440 examples: {'rewards_train/chosen': '0.04676', 'rewards_train/rejected': '0.035104', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011656', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-138.77', 'loss/train': '0.70095', 'examples_per_second': '31.937', 'grad_norm': '58.75', 'counters/examples': 17440, 'counters/updates': 545}
train stats after 17472 examples: {'rewards_train/chosen': '-0.003359', 'rewards_train/rejected': '0.033393', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036752', 'logps_train/rejected': '-111.72', 'logps_train/chosen': '-126.84', 'loss/train': '0.71671', 'examples_per_second': '30.011', 'grad_norm': '29.75', 'counters/examples': 17472, 'counters/updates': 546}
skipping logging after 17504 examples to avoid logging too frequently
train stats after 17536 examples: {'rewards_train/chosen': '0.013826', 'rewards_train/rejected': '0.028502', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014676', 'logps_train/rejected': '-109.5', 'logps_train/chosen': '-140.96', 'loss/train': '0.70693', 'examples_per_second': '32.174', 'grad_norm': '26.625', 'counters/examples': 17536, 'counters/updates': 548}
train stats after 17568 examples: {'rewards_train/chosen': '0.055417', 'rewards_train/rejected': '-0.01585', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071267', 'logps_train/rejected': '-152.42', 'logps_train/chosen': '-150.86', 'loss/train': '0.67044', 'examples_per_second': '30.224', 'grad_norm': '32.5', 'counters/examples': 17568, 'counters/updates': 549}
skipping logging after 17600 examples to avoid logging too frequently
train stats after 17632 examples: {'rewards_train/chosen': '0.037945', 'rewards_train/rejected': '0.059901', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.021957', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-114.25', 'loss/train': '0.72042', 'examples_per_second': '33.566', 'grad_norm': '107.5', 'counters/examples': 17632, 'counters/updates': 551}
train stats after 17664 examples: {'rewards_train/chosen': '0.029762', 'rewards_train/rejected': '0.069899', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040137', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-136.67', 'loss/train': '0.72827', 'examples_per_second': '30.425', 'grad_norm': '30.5', 'counters/examples': 17664, 'counters/updates': 552}
train stats after 17696 examples: {'rewards_train/chosen': '0.025088', 'rewards_train/rejected': '-0.032484', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057571', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-174.92', 'loss/train': '0.669', 'examples_per_second': '33.195', 'grad_norm': '35.75', 'counters/examples': 17696, 'counters/updates': 553}
train stats after 17728 examples: {'rewards_train/chosen': '0.059515', 'rewards_train/rejected': '0.07458', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015065', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-144.65', 'loss/train': '0.70507', 'examples_per_second': '32.197', 'grad_norm': '32', 'counters/examples': 17728, 'counters/updates': 554}
train stats after 17760 examples: {'rewards_train/chosen': '0.0043858', 'rewards_train/rejected': '0.0036625', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0007233', 'logps_train/rejected': '-117.81', 'logps_train/chosen': '-110.31', 'loss/train': '0.6963', 'examples_per_second': '32.087', 'grad_norm': '27.25', 'counters/examples': 17760, 'counters/updates': 555}
train stats after 17792 examples: {'rewards_train/chosen': '0.016068', 'rewards_train/rejected': '0.013297', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0027714', 'logps_train/rejected': '-93.182', 'logps_train/chosen': '-104.16', 'loss/train': '0.6932', 'examples_per_second': '32.321', 'grad_norm': '23.25', 'counters/examples': 17792, 'counters/updates': 556}
train stats after 17824 examples: {'rewards_train/chosen': '0.025561', 'rewards_train/rejected': '-0.030728', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056289', 'logps_train/rejected': '-99.104', 'logps_train/chosen': '-143.61', 'loss/train': '0.67283', 'examples_per_second': '31.248', 'grad_norm': '35.25', 'counters/examples': 17824, 'counters/updates': 557}
train stats after 17856 examples: {'rewards_train/chosen': '0.0043968', 'rewards_train/rejected': '0.013669', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0092717', 'logps_train/rejected': '-142.63', 'logps_train/chosen': '-196.31', 'loss/train': '0.70375', 'examples_per_second': '31.296', 'grad_norm': '40', 'counters/examples': 17856, 'counters/updates': 558}
skipping logging after 17888 examples to avoid logging too frequently
train stats after 17920 examples: {'rewards_train/chosen': '0.017241', 'rewards_train/rejected': '-0.017102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034344', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-117.8', 'loss/train': '0.67954', 'examples_per_second': '33.784', 'grad_norm': '26', 'counters/examples': 17920, 'counters/updates': 560}
skipping logging after 17952 examples to avoid logging too frequently
train stats after 17984 examples: {'rewards_train/chosen': '0.096609', 'rewards_train/rejected': '-0.013364', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10997', 'logps_train/rejected': '-119.38', 'logps_train/chosen': '-142.71', 'loss/train': '0.65597', 'examples_per_second': '32.926', 'grad_norm': '32.5', 'counters/examples': 17984, 'counters/updates': 562}
train stats after 18016 examples: {'rewards_train/chosen': '0.059651', 'rewards_train/rejected': '0.021701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03795', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-141.29', 'loss/train': '0.67768', 'examples_per_second': '32.371', 'grad_norm': '27.875', 'counters/examples': 18016, 'counters/updates': 563}
train stats after 18048 examples: {'rewards_train/chosen': '0.028276', 'rewards_train/rejected': '0.06674', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.038464', 'logps_train/rejected': '-125.26', 'logps_train/chosen': '-146.68', 'loss/train': '0.71738', 'examples_per_second': '31.18', 'grad_norm': '33', 'counters/examples': 18048, 'counters/updates': 564}
train stats after 18080 examples: {'rewards_train/chosen': '0.10919', 'rewards_train/rejected': '0.056525', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052666', 'logps_train/rejected': '-160.29', 'logps_train/chosen': '-141.06', 'loss/train': '0.67511', 'examples_per_second': '30.758', 'grad_norm': '30.375', 'counters/examples': 18080, 'counters/updates': 565}
train stats after 18112 examples: {'rewards_train/chosen': '-0.0056021', 'rewards_train/rejected': '0.011998', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0176', 'logps_train/rejected': '-106.76', 'logps_train/chosen': '-113.43', 'loss/train': '0.70486', 'examples_per_second': '32.369', 'grad_norm': '30.125', 'counters/examples': 18112, 'counters/updates': 566}
train stats after 18144 examples: {'rewards_train/chosen': '-0.028773', 'rewards_train/rejected': '-0.033391', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0046183', 'logps_train/rejected': '-102.9', 'logps_train/chosen': '-113.34', 'loss/train': '0.69346', 'examples_per_second': '29.901', 'grad_norm': '25', 'counters/examples': 18144, 'counters/updates': 567}
train stats after 18176 examples: {'rewards_train/chosen': '0.093969', 'rewards_train/rejected': '0.091775', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0021946', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-174.47', 'loss/train': '0.69781', 'examples_per_second': '31.837', 'grad_norm': '30.625', 'counters/examples': 18176, 'counters/updates': 568}
skipping logging after 18208 examples to avoid logging too frequently
train stats after 18240 examples: {'rewards_train/chosen': '0.046766', 'rewards_train/rejected': '0.025043', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021723', 'logps_train/rejected': '-115.93', 'logps_train/chosen': '-158.2', 'loss/train': '0.68724', 'examples_per_second': '37.056', 'grad_norm': '38', 'counters/examples': 18240, 'counters/updates': 570}
train stats after 18272 examples: {'rewards_train/chosen': '0.063597', 'rewards_train/rejected': '0.057924', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0056736', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-167.51', 'loss/train': '0.6961', 'examples_per_second': '30.52', 'grad_norm': '35.25', 'counters/examples': 18272, 'counters/updates': 571}
train stats after 18304 examples: {'rewards_train/chosen': '0.035729', 'rewards_train/rejected': '0.0061818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029547', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-173.65', 'loss/train': '0.68309', 'examples_per_second': '11.606', 'grad_norm': '32.5', 'counters/examples': 18304, 'counters/updates': 572}
train stats after 18336 examples: {'rewards_train/chosen': '0.035228', 'rewards_train/rejected': '0.041931', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0067023', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-144.33', 'loss/train': '0.70007', 'examples_per_second': '31.314', 'grad_norm': '29.5', 'counters/examples': 18336, 'counters/updates': 573}
train stats after 18368 examples: {'rewards_train/chosen': '0.015508', 'rewards_train/rejected': '-0.0016411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017149', 'logps_train/rejected': '-114.74', 'logps_train/chosen': '-136.7', 'loss/train': '0.68765', 'examples_per_second': '31.354', 'grad_norm': '27.25', 'counters/examples': 18368, 'counters/updates': 574}
skipping logging after 18400 examples to avoid logging too frequently
train stats after 18432 examples: {'rewards_train/chosen': '0.030856', 'rewards_train/rejected': '0.021803', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0090523', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-148.02', 'loss/train': '0.69424', 'examples_per_second': '30.339', 'grad_norm': '38.75', 'counters/examples': 18432, 'counters/updates': 576}
skipping logging after 18464 examples to avoid logging too frequently
train stats after 18496 examples: {'rewards_train/chosen': '0.11732', 'rewards_train/rejected': '0.046296', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071026', 'logps_train/rejected': '-145.43', 'logps_train/chosen': '-128.44', 'loss/train': '0.6661', 'examples_per_second': '31.366', 'grad_norm': '35.25', 'counters/examples': 18496, 'counters/updates': 578}
train stats after 18528 examples: {'rewards_train/chosen': '-0.025792', 'rewards_train/rejected': '-0.032555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0067628', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-211.35', 'loss/train': '0.69236', 'examples_per_second': '31.233', 'grad_norm': '39', 'counters/examples': 18528, 'counters/updates': 579}
train stats after 18560 examples: {'rewards_train/chosen': '0.021717', 'rewards_train/rejected': '0.022775', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0010572', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-125.6', 'loss/train': '0.69606', 'examples_per_second': '30.492', 'grad_norm': '32.5', 'counters/examples': 18560, 'counters/updates': 580}
train stats after 18592 examples: {'rewards_train/chosen': '-0.0087128', 'rewards_train/rejected': '0.0085344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017247', 'logps_train/rejected': '-80.178', 'logps_train/chosen': '-121.66', 'loss/train': '0.70415', 'examples_per_second': '32.195', 'grad_norm': '40.25', 'counters/examples': 18592, 'counters/updates': 581}
train stats after 18624 examples: {'rewards_train/chosen': '0.094092', 'rewards_train/rejected': '0.0098043', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084288', 'logps_train/rejected': '-117.38', 'logps_train/chosen': '-174.8', 'loss/train': '0.66134', 'examples_per_second': '31.202', 'grad_norm': '52.75', 'counters/examples': 18624, 'counters/updates': 582}
train stats after 18656 examples: {'rewards_train/chosen': '0.069244', 'rewards_train/rejected': '0.050884', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018361', 'logps_train/rejected': '-129.29', 'logps_train/chosen': '-126.75', 'loss/train': '0.68857', 'examples_per_second': '33.118', 'grad_norm': '28.375', 'counters/examples': 18656, 'counters/updates': 583}
train stats after 18688 examples: {'rewards_train/chosen': '0.076679', 'rewards_train/rejected': '0.0024695', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074209', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-149.15', 'loss/train': '0.67791', 'examples_per_second': '31.305', 'grad_norm': '81', 'counters/examples': 18688, 'counters/updates': 584}
train stats after 18720 examples: {'rewards_train/chosen': '-0.0068711', 'rewards_train/rejected': '-0.021187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014316', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-122.02', 'loss/train': '0.68864', 'examples_per_second': '33.156', 'grad_norm': '26', 'counters/examples': 18720, 'counters/updates': 585}
train stats after 18752 examples: {'rewards_train/chosen': '0.063665', 'rewards_train/rejected': '0.018239', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045426', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-157.22', 'loss/train': '0.67702', 'examples_per_second': '30.332', 'grad_norm': '34.25', 'counters/examples': 18752, 'counters/updates': 586}
train stats after 18784 examples: {'rewards_train/chosen': '0.087264', 'rewards_train/rejected': '0.043218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044046', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-145.87', 'loss/train': '0.68079', 'examples_per_second': '31.282', 'grad_norm': '28.75', 'counters/examples': 18784, 'counters/updates': 587}
train stats after 18816 examples: {'rewards_train/chosen': '0.039582', 'rewards_train/rejected': '0.0068081', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032774', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-152', 'loss/train': '0.68179', 'examples_per_second': '29.745', 'grad_norm': '30.25', 'counters/examples': 18816, 'counters/updates': 588}
train stats after 18848 examples: {'rewards_train/chosen': '0.0411', 'rewards_train/rejected': '0.11234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.071237', 'logps_train/rejected': '-152.55', 'logps_train/chosen': '-169.01', 'loss/train': '0.74457', 'examples_per_second': '33.074', 'grad_norm': '42.5', 'counters/examples': 18848, 'counters/updates': 589}
train stats after 18880 examples: {'rewards_train/chosen': '0.14406', 'rewards_train/rejected': '0.017462', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1266', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-182.67', 'loss/train': '0.6405', 'examples_per_second': '32.567', 'grad_norm': '30.125', 'counters/examples': 18880, 'counters/updates': 590}
train stats after 18912 examples: {'rewards_train/chosen': '0.071353', 'rewards_train/rejected': '0.029236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042117', 'logps_train/rejected': '-98.475', 'logps_train/chosen': '-135.08', 'loss/train': '0.6775', 'examples_per_second': '29.795', 'grad_norm': '29.875', 'counters/examples': 18912, 'counters/updates': 591}
train stats after 18944 examples: {'rewards_train/chosen': '0.05226', 'rewards_train/rejected': '0.0038499', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04841', 'logps_train/rejected': '-150.51', 'logps_train/chosen': '-144.01', 'loss/train': '0.6774', 'examples_per_second': '31.253', 'grad_norm': '35', 'counters/examples': 18944, 'counters/updates': 592}
train stats after 18976 examples: {'rewards_train/chosen': '0.010723', 'rewards_train/rejected': '-0.025586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036309', 'logps_train/rejected': '-116.68', 'logps_train/chosen': '-144.38', 'loss/train': '0.67909', 'examples_per_second': '31.866', 'grad_norm': '28', 'counters/examples': 18976, 'counters/updates': 593}
train stats after 19008 examples: {'rewards_train/chosen': '0.030019', 'rewards_train/rejected': '0.0086843', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021335', 'logps_train/rejected': '-139.33', 'logps_train/chosen': '-163.94', 'loss/train': '0.68907', 'examples_per_second': '31.228', 'grad_norm': '31.25', 'counters/examples': 19008, 'counters/updates': 594}
skipping logging after 19040 examples to avoid logging too frequently
train stats after 19072 examples: {'rewards_train/chosen': '0.03847', 'rewards_train/rejected': '0.021896', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.016574', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-169.7', 'loss/train': '0.68758', 'examples_per_second': '22.814', 'grad_norm': '30.375', 'counters/examples': 19072, 'counters/updates': 596}
skipping logging after 19104 examples to avoid logging too frequently
train stats after 19136 examples: {'rewards_train/chosen': '0.022689', 'rewards_train/rejected': '0.094141', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.071451', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-138.62', 'loss/train': '0.73437', 'examples_per_second': '31.819', 'grad_norm': '32.5', 'counters/examples': 19136, 'counters/updates': 598}
train stats after 19168 examples: {'rewards_train/chosen': '0.059857', 'rewards_train/rejected': '0.03697', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022887', 'logps_train/rejected': '-152.28', 'logps_train/chosen': '-153.7', 'loss/train': '0.68883', 'examples_per_second': '20.696', 'grad_norm': '39.75', 'counters/examples': 19168, 'counters/updates': 599}
train stats after 19200 examples: {'rewards_train/chosen': '0.047233', 'rewards_train/rejected': '0.011146', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036087', 'logps_train/rejected': '-128.6', 'logps_train/chosen': '-176.28', 'loss/train': '0.68118', 'examples_per_second': '29.668', 'grad_norm': '41.5', 'counters/examples': 19200, 'counters/updates': 600}
skipping logging after 19232 examples to avoid logging too frequently
train stats after 19264 examples: {'rewards_train/chosen': '0.022609', 'rewards_train/rejected': '0.035065', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012457', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-147.48', 'loss/train': '0.70391', 'examples_per_second': '32.637', 'grad_norm': '34', 'counters/examples': 19264, 'counters/updates': 602}
train stats after 19296 examples: {'rewards_train/chosen': '0.030915', 'rewards_train/rejected': '0.048224', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01731', 'logps_train/rejected': '-140.83', 'logps_train/chosen': '-168.84', 'loss/train': '0.7071', 'examples_per_second': '31.226', 'grad_norm': '34.25', 'counters/examples': 19296, 'counters/updates': 603}
train stats after 19328 examples: {'rewards_train/chosen': '0.058191', 'rewards_train/rejected': '0.023512', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034679', 'logps_train/rejected': '-113.44', 'logps_train/chosen': '-148.44', 'loss/train': '0.67861', 'examples_per_second': '31.943', 'grad_norm': '31.875', 'counters/examples': 19328, 'counters/updates': 604}
train stats after 19360 examples: {'rewards_train/chosen': '0.026459', 'rewards_train/rejected': '0.010155', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.016304', 'logps_train/rejected': '-117.55', 'logps_train/chosen': '-133.52', 'loss/train': '0.68885', 'examples_per_second': '29.795', 'grad_norm': '28.75', 'counters/examples': 19360, 'counters/updates': 605}
train stats after 19392 examples: {'rewards_train/chosen': '0.07229', 'rewards_train/rejected': '0.08946', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.01717', 'logps_train/rejected': '-158.28', 'logps_train/chosen': '-140.19', 'loss/train': '0.71317', 'examples_per_second': '31.287', 'grad_norm': '43.75', 'counters/examples': 19392, 'counters/updates': 606}
train stats after 19424 examples: {'rewards_train/chosen': '0.030389', 'rewards_train/rejected': '0.036445', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.006056', 'logps_train/rejected': '-142.61', 'logps_train/chosen': '-142.11', 'loss/train': '0.70199', 'examples_per_second': '30.54', 'grad_norm': '42.25', 'counters/examples': 19424, 'counters/updates': 607}
skipping logging after 19456 examples to avoid logging too frequently
train stats after 19488 examples: {'rewards_train/chosen': '0.0022879', 'rewards_train/rejected': '0.021703', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019415', 'logps_train/rejected': '-165.45', 'logps_train/chosen': '-155.65', 'loss/train': '0.70564', 'examples_per_second': '32.982', 'grad_norm': '32', 'counters/examples': 19488, 'counters/updates': 609}
train stats after 19520 examples: {'rewards_train/chosen': '0.060523', 'rewards_train/rejected': '0.012202', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048322', 'logps_train/rejected': '-115.15', 'logps_train/chosen': '-134.59', 'loss/train': '0.67357', 'examples_per_second': '30.809', 'grad_norm': '30.875', 'counters/examples': 19520, 'counters/updates': 610}
train stats after 19552 examples: {'rewards_train/chosen': '0.074617', 'rewards_train/rejected': '-0.0084312', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083048', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-160.6', 'loss/train': '0.65986', 'examples_per_second': '32.142', 'grad_norm': '30.875', 'counters/examples': 19552, 'counters/updates': 611}
train stats after 19584 examples: {'rewards_train/chosen': '0.10356', 'rewards_train/rejected': '-0.020349', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12391', 'logps_train/rejected': '-166.18', 'logps_train/chosen': '-160.8', 'loss/train': '0.65008', 'examples_per_second': '31.308', 'grad_norm': '30.625', 'counters/examples': 19584, 'counters/updates': 612}
train stats after 19616 examples: {'rewards_train/chosen': '0.11425', 'rewards_train/rejected': '0.03641', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077844', 'logps_train/rejected': '-78.074', 'logps_train/chosen': '-113.36', 'loss/train': '0.66021', 'examples_per_second': '31.275', 'grad_norm': '25.375', 'counters/examples': 19616, 'counters/updates': 613}
train stats after 19648 examples: {'rewards_train/chosen': '0.040527', 'rewards_train/rejected': '-0.031154', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071682', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-151.8', 'loss/train': '0.66221', 'examples_per_second': '30.236', 'grad_norm': '27.625', 'counters/examples': 19648, 'counters/updates': 614}
train stats after 19680 examples: {'rewards_train/chosen': '0.0049956', 'rewards_train/rejected': '0.01151', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0065146', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-163.06', 'loss/train': '0.69918', 'examples_per_second': '31.074', 'grad_norm': '34', 'counters/examples': 19680, 'counters/updates': 615}
train stats after 19712 examples: {'rewards_train/chosen': '0.031408', 'rewards_train/rejected': '0.042671', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011263', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-121.44', 'loss/train': '0.70143', 'examples_per_second': '31.322', 'grad_norm': '34.5', 'counters/examples': 19712, 'counters/updates': 616}
train stats after 19744 examples: {'rewards_train/chosen': '0.056125', 'rewards_train/rejected': '0.0031241', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053001', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-126.57', 'loss/train': '0.67396', 'examples_per_second': '31.547', 'grad_norm': '33.25', 'counters/examples': 19744, 'counters/updates': 617}
train stats after 19776 examples: {'rewards_train/chosen': '0.0025172', 'rewards_train/rejected': '0.013737', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.01122', 'logps_train/rejected': '-128.46', 'logps_train/chosen': '-140.73', 'loss/train': '0.70169', 'examples_per_second': '30.58', 'grad_norm': '28.375', 'counters/examples': 19776, 'counters/updates': 618}
train stats after 19808 examples: {'rewards_train/chosen': '0.00033645', 'rewards_train/rejected': '0.0016071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0012706', 'logps_train/rejected': '-132.91', 'logps_train/chosen': '-137.36', 'loss/train': '0.69532', 'examples_per_second': '31.178', 'grad_norm': '34.5', 'counters/examples': 19808, 'counters/updates': 619}
train stats after 19840 examples: {'rewards_train/chosen': '-0.0096902', 'rewards_train/rejected': '0.016141', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025831', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-169.42', 'loss/train': '0.70905', 'examples_per_second': '32.29', 'grad_norm': '33.75', 'counters/examples': 19840, 'counters/updates': 620}
skipping logging after 19872 examples to avoid logging too frequently
train stats after 19904 examples: {'rewards_train/chosen': '0.06019', 'rewards_train/rejected': '0.042685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017505', 'logps_train/rejected': '-161.69', 'logps_train/chosen': '-169.63', 'loss/train': '0.68979', 'examples_per_second': '29.822', 'grad_norm': '32.5', 'counters/examples': 19904, 'counters/updates': 622}
train stats after 19936 examples: {'rewards_train/chosen': '0.065335', 'rewards_train/rejected': '0.038052', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027283', 'logps_train/rejected': '-98.996', 'logps_train/chosen': '-169.41', 'loss/train': '0.68363', 'examples_per_second': '31.802', 'grad_norm': '29.375', 'counters/examples': 19936, 'counters/updates': 623}
skipping logging after 19968 examples to avoid logging too frequently
train stats after 20000 examples: {'rewards_train/chosen': '0.039483', 'rewards_train/rejected': '0.02839', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011093', 'logps_train/rejected': '-134.81', 'logps_train/chosen': '-190.72', 'loss/train': '0.69959', 'examples_per_second': '30.344', 'grad_norm': '42.25', 'counters/examples': 20000, 'counters/updates': 625}
train stats after 20032 examples: {'rewards_train/chosen': '-0.0014126', 'rewards_train/rejected': '0.13363', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.13504', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-149.75', 'loss/train': '0.78595', 'examples_per_second': '29.936', 'grad_norm': '80', 'counters/examples': 20032, 'counters/updates': 626}
skipping logging after 20064 examples to avoid logging too frequently
train stats after 20096 examples: {'rewards_train/chosen': '0.018306', 'rewards_train/rejected': '0.059903', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041597', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-171.92', 'loss/train': '0.72135', 'examples_per_second': '34.238', 'grad_norm': '37.5', 'counters/examples': 20096, 'counters/updates': 628}
train stats after 20128 examples: {'rewards_train/chosen': '0.024609', 'rewards_train/rejected': '0.10822', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.083608', 'logps_train/rejected': '-159.63', 'logps_train/chosen': '-138.22', 'loss/train': '0.74902', 'examples_per_second': '31.317', 'grad_norm': '53.25', 'counters/examples': 20128, 'counters/updates': 629}
train stats after 20160 examples: {'rewards_train/chosen': '0.09075', 'rewards_train/rejected': '0.010843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079907', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-158.74', 'loss/train': '0.66393', 'examples_per_second': '31.433', 'grad_norm': '41.25', 'counters/examples': 20160, 'counters/updates': 630}
train stats after 20192 examples: {'rewards_train/chosen': '-0.0029887', 'rewards_train/rejected': '0.002069', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0050578', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-131.77', 'loss/train': '0.69937', 'examples_per_second': '30.778', 'grad_norm': '46.25', 'counters/examples': 20192, 'counters/updates': 631}
train stats after 20224 examples: {'rewards_train/chosen': '0.14081', 'rewards_train/rejected': '0.022917', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11789', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-203.23', 'loss/train': '0.64797', 'examples_per_second': '31.238', 'grad_norm': '31.125', 'counters/examples': 20224, 'counters/updates': 632}
train stats after 20256 examples: {'rewards_train/chosen': '0.058368', 'rewards_train/rejected': '0.030786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027581', 'logps_train/rejected': '-167.13', 'logps_train/chosen': '-194.34', 'loss/train': '0.69064', 'examples_per_second': '31.03', 'grad_norm': '63.5', 'counters/examples': 20256, 'counters/updates': 633}
train stats after 20288 examples: {'rewards_train/chosen': '0.058476', 'rewards_train/rejected': '-0.0047931', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063269', 'logps_train/rejected': '-107.25', 'logps_train/chosen': '-133.54', 'loss/train': '0.66526', 'examples_per_second': '29.823', 'grad_norm': '25.375', 'counters/examples': 20288, 'counters/updates': 634}
train stats after 20320 examples: {'rewards_train/chosen': '0.056918', 'rewards_train/rejected': '0.049829', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0070887', 'logps_train/rejected': '-93.835', 'logps_train/chosen': '-128.07', 'loss/train': '0.69247', 'examples_per_second': '30.583', 'grad_norm': '29.75', 'counters/examples': 20320, 'counters/updates': 635}
train stats after 20352 examples: {'rewards_train/chosen': '0.073935', 'rewards_train/rejected': '0.046933', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027003', 'logps_train/rejected': '-143.39', 'logps_train/chosen': '-162.45', 'loss/train': '0.68573', 'examples_per_second': '31.167', 'grad_norm': '36.5', 'counters/examples': 20352, 'counters/updates': 636}
train stats after 20384 examples: {'rewards_train/chosen': '0.033869', 'rewards_train/rejected': '0.026245', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0076243', 'logps_train/rejected': '-165.63', 'logps_train/chosen': '-157.54', 'loss/train': '0.69712', 'examples_per_second': '30.741', 'grad_norm': '37', 'counters/examples': 20384, 'counters/updates': 637}
train stats after 20416 examples: {'rewards_train/chosen': '0.08172', 'rewards_train/rejected': '0.072299', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0094206', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-156.63', 'loss/train': '0.69747', 'examples_per_second': '30.881', 'grad_norm': '36.25', 'counters/examples': 20416, 'counters/updates': 638}
train stats after 20448 examples: {'rewards_train/chosen': '0.013702', 'rewards_train/rejected': '0.083654', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.069952', 'logps_train/rejected': '-142.93', 'logps_train/chosen': '-138.99', 'loss/train': '0.73527', 'examples_per_second': '31.273', 'grad_norm': '35.25', 'counters/examples': 20448, 'counters/updates': 639}
train stats after 20480 examples: {'rewards_train/chosen': '0.012408', 'rewards_train/rejected': '0.0036275', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0087806', 'logps_train/rejected': '-185.64', 'logps_train/chosen': '-157.82', 'loss/train': '0.69626', 'examples_per_second': '31.257', 'grad_norm': '35.25', 'counters/examples': 20480, 'counters/updates': 640}
train stats after 20512 examples: {'rewards_train/chosen': '0.044734', 'rewards_train/rejected': '-0.01278', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057514', 'logps_train/rejected': '-103.48', 'logps_train/chosen': '-150.35', 'loss/train': '0.66979', 'examples_per_second': '31.233', 'grad_norm': '32.75', 'counters/examples': 20512, 'counters/updates': 641}
train stats after 20544 examples: {'rewards_train/chosen': '0.012849', 'rewards_train/rejected': '0.067662', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054813', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-144.23', 'loss/train': '0.73365', 'examples_per_second': '30.722', 'grad_norm': '48.25', 'counters/examples': 20544, 'counters/updates': 642}
train stats after 20576 examples: {'rewards_train/chosen': '0.10134', 'rewards_train/rejected': '0.069745', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.031595', 'logps_train/rejected': '-153.8', 'logps_train/chosen': '-191.73', 'loss/train': '0.70066', 'examples_per_second': '31.282', 'grad_norm': '46.5', 'counters/examples': 20576, 'counters/updates': 643}
train stats after 20608 examples: {'rewards_train/chosen': '0.05645', 'rewards_train/rejected': '-0.051425', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10787', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-179.38', 'loss/train': '0.64837', 'examples_per_second': '32.913', 'grad_norm': '28.375', 'counters/examples': 20608, 'counters/updates': 644}
train stats after 20640 examples: {'rewards_train/chosen': '0.016608', 'rewards_train/rejected': '0.021746', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0051376', 'logps_train/rejected': '-95.8', 'logps_train/chosen': '-152.35', 'loss/train': '0.69908', 'examples_per_second': '33.03', 'grad_norm': '49.25', 'counters/examples': 20640, 'counters/updates': 645}
train stats after 20672 examples: {'rewards_train/chosen': '0.011308', 'rewards_train/rejected': '0.033539', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022231', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-139.73', 'loss/train': '0.70833', 'examples_per_second': '30.044', 'grad_norm': '26', 'counters/examples': 20672, 'counters/updates': 646}
train stats after 20704 examples: {'rewards_train/chosen': '-0.015898', 'rewards_train/rejected': '0.014929', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030827', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-154.56', 'loss/train': '0.71261', 'examples_per_second': '31.202', 'grad_norm': '29.625', 'counters/examples': 20704, 'counters/updates': 647}
train stats after 20736 examples: {'rewards_train/chosen': '0.15084', 'rewards_train/rejected': '0.030643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1202', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-198.15', 'loss/train': '0.65076', 'examples_per_second': '29.972', 'grad_norm': '34.75', 'counters/examples': 20736, 'counters/updates': 648}
train stats after 20768 examples: {'rewards_train/chosen': '0.061861', 'rewards_train/rejected': '0.022606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039255', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-126.71', 'loss/train': '0.67942', 'examples_per_second': '30.773', 'grad_norm': '29.875', 'counters/examples': 20768, 'counters/updates': 649}
train stats after 20800 examples: {'rewards_train/chosen': '0.015704', 'rewards_train/rejected': '0.0058794', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0098241', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-116.11', 'loss/train': '0.69115', 'examples_per_second': '30.789', 'grad_norm': '42.25', 'counters/examples': 20800, 'counters/updates': 650}
train stats after 20832 examples: {'rewards_train/chosen': '0.034374', 'rewards_train/rejected': '-0.00090351', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035277', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-129.71', 'loss/train': '0.67969', 'examples_per_second': '29.862', 'grad_norm': '28.375', 'counters/examples': 20832, 'counters/updates': 651}
train stats after 20864 examples: {'rewards_train/chosen': '0.06829', 'rewards_train/rejected': '0.047267', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021023', 'logps_train/rejected': '-129.06', 'logps_train/chosen': '-162.72', 'loss/train': '0.6886', 'examples_per_second': '32.6', 'grad_norm': '55.25', 'counters/examples': 20864, 'counters/updates': 652}
train stats after 20896 examples: {'rewards_train/chosen': '0.064541', 'rewards_train/rejected': '0.062649', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0018925', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-162.09', 'loss/train': '0.70283', 'examples_per_second': '31.949', 'grad_norm': '33.5', 'counters/examples': 20896, 'counters/updates': 653}
train stats after 20928 examples: {'rewards_train/chosen': '0.14735', 'rewards_train/rejected': '-0.018164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16551', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-142.14', 'loss/train': '0.62123', 'examples_per_second': '31.02', 'grad_norm': '31.125', 'counters/examples': 20928, 'counters/updates': 654}
train stats after 20960 examples: {'rewards_train/chosen': '0.044609', 'rewards_train/rejected': '-0.0036334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048242', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-148.07', 'loss/train': '0.67271', 'examples_per_second': '30.894', 'grad_norm': '29.125', 'counters/examples': 20960, 'counters/updates': 655}
train stats after 20992 examples: {'rewards_train/chosen': '0.10943', 'rewards_train/rejected': '0.096919', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012506', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-157.87', 'loss/train': '0.69298', 'examples_per_second': '31.312', 'grad_norm': '32.5', 'counters/examples': 20992, 'counters/updates': 656}
train stats after 21024 examples: {'rewards_train/chosen': '0.082329', 'rewards_train/rejected': '0.034922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047408', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-122.91', 'loss/train': '0.6743', 'examples_per_second': '30.736', 'grad_norm': '28.5', 'counters/examples': 21024, 'counters/updates': 657}
train stats after 21056 examples: {'rewards_train/chosen': '0.30008', 'rewards_train/rejected': '0.045122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.25496', 'logps_train/rejected': '-139.21', 'logps_train/chosen': '-198.52', 'loss/train': '0.6169', 'examples_per_second': '31.254', 'grad_norm': '44.25', 'counters/examples': 21056, 'counters/updates': 658}
train stats after 21088 examples: {'rewards_train/chosen': '-0.021542', 'rewards_train/rejected': '0.02933', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.050872', 'logps_train/rejected': '-135.53', 'logps_train/chosen': '-123.44', 'loss/train': '0.724', 'examples_per_second': '31.233', 'grad_norm': '31.5', 'counters/examples': 21088, 'counters/updates': 659}
train stats after 21120 examples: {'rewards_train/chosen': '0.0052791', 'rewards_train/rejected': '0.035497', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.030218', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-146.4', 'loss/train': '0.71631', 'examples_per_second': '31.303', 'grad_norm': '32.25', 'counters/examples': 21120, 'counters/updates': 660}
train stats after 21152 examples: {'rewards_train/chosen': '0.05032', 'rewards_train/rejected': '0.044349', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0059706', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-140.12', 'loss/train': '0.69311', 'examples_per_second': '31.385', 'grad_norm': '35.5', 'counters/examples': 21152, 'counters/updates': 661}
train stats after 21184 examples: {'rewards_train/chosen': '0.043022', 'rewards_train/rejected': '0.02621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016812', 'logps_train/rejected': '-131.32', 'logps_train/chosen': '-135.89', 'loss/train': '0.68707', 'examples_per_second': '30.418', 'grad_norm': '49.25', 'counters/examples': 21184, 'counters/updates': 662}
skipping logging after 21216 examples to avoid logging too frequently
train stats after 21248 examples: {'rewards_train/chosen': '0.077159', 'rewards_train/rejected': '-0.0072108', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08437', 'logps_train/rejected': '-114.59', 'logps_train/chosen': '-116.15', 'loss/train': '0.66059', 'examples_per_second': '31.762', 'grad_norm': '22.625', 'counters/examples': 21248, 'counters/updates': 664}
train stats after 21280 examples: {'rewards_train/chosen': '0.039387', 'rewards_train/rejected': '0.04081', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0014236', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-148.01', 'loss/train': '0.69816', 'examples_per_second': '30.576', 'grad_norm': '29.125', 'counters/examples': 21280, 'counters/updates': 665}
train stats after 21312 examples: {'rewards_train/chosen': '0.061986', 'rewards_train/rejected': '0.055015', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.006971', 'logps_train/rejected': '-147.92', 'logps_train/chosen': '-142.15', 'loss/train': '0.69573', 'examples_per_second': '32.566', 'grad_norm': '28.625', 'counters/examples': 21312, 'counters/updates': 666}
train stats after 21344 examples: {'rewards_train/chosen': '0.10007', 'rewards_train/rejected': '0.083352', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016722', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-145.21', 'loss/train': '0.69327', 'examples_per_second': '30.282', 'grad_norm': '38.5', 'counters/examples': 21344, 'counters/updates': 667}
train stats after 21376 examples: {'rewards_train/chosen': '0.045541', 'rewards_train/rejected': '0.034873', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.010668', 'logps_train/rejected': '-167.79', 'logps_train/chosen': '-137.78', 'loss/train': '0.69235', 'examples_per_second': '31.396', 'grad_norm': '48.75', 'counters/examples': 21376, 'counters/updates': 668}
train stats after 21408 examples: {'rewards_train/chosen': '0.083208', 'rewards_train/rejected': '0.043776', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039432', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-147.79', 'loss/train': '0.68343', 'examples_per_second': '30.862', 'grad_norm': '31.75', 'counters/examples': 21408, 'counters/updates': 669}
train stats after 21440 examples: {'rewards_train/chosen': '0.032004', 'rewards_train/rejected': '0.0021421', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029862', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-174.68', 'loss/train': '0.69025', 'examples_per_second': '31.184', 'grad_norm': '65', 'counters/examples': 21440, 'counters/updates': 670}
train stats after 21472 examples: {'rewards_train/chosen': '0.05018', 'rewards_train/rejected': '0.060608', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.010428', 'logps_train/rejected': '-118.84', 'logps_train/chosen': '-151.95', 'loss/train': '0.70122', 'examples_per_second': '31.257', 'grad_norm': '31.5', 'counters/examples': 21472, 'counters/updates': 671}
train stats after 21504 examples: {'rewards_train/chosen': '0.077195', 'rewards_train/rejected': '0.07021', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0069844', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-172.66', 'loss/train': '0.70323', 'examples_per_second': '31.321', 'grad_norm': '42.75', 'counters/examples': 21504, 'counters/updates': 672}
train stats after 21536 examples: {'rewards_train/chosen': '0.031649', 'rewards_train/rejected': '0.054971', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023323', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-163.59', 'loss/train': '0.70872', 'examples_per_second': '31.063', 'grad_norm': '32.25', 'counters/examples': 21536, 'counters/updates': 673}
train stats after 21568 examples: {'rewards_train/chosen': '0.023069', 'rewards_train/rejected': '0.012173', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010896', 'logps_train/rejected': '-130', 'logps_train/chosen': '-145.38', 'loss/train': '0.69042', 'examples_per_second': '29.977', 'grad_norm': '31.375', 'counters/examples': 21568, 'counters/updates': 674}
train stats after 21600 examples: {'rewards_train/chosen': '0.097834', 'rewards_train/rejected': '0.046968', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050866', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-158.24', 'loss/train': '0.68591', 'examples_per_second': '31.306', 'grad_norm': '35.5', 'counters/examples': 21600, 'counters/updates': 675}
train stats after 21632 examples: {'rewards_train/chosen': '0.025911', 'rewards_train/rejected': '-0.03968', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06559', 'logps_train/rejected': '-97.632', 'logps_train/chosen': '-116.43', 'loss/train': '0.66307', 'examples_per_second': '30.975', 'grad_norm': '24.25', 'counters/examples': 21632, 'counters/updates': 676}
train stats after 21664 examples: {'rewards_train/chosen': '0.061847', 'rewards_train/rejected': '0.046825', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015022', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-191.45', 'loss/train': '0.69468', 'examples_per_second': '31.075', 'grad_norm': '32.75', 'counters/examples': 21664, 'counters/updates': 677}
train stats after 21696 examples: {'rewards_train/chosen': '0.037686', 'rewards_train/rejected': '0.035871', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0018142', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-143', 'loss/train': '0.69828', 'examples_per_second': '30.347', 'grad_norm': '33.5', 'counters/examples': 21696, 'counters/updates': 678}
train stats after 21728 examples: {'rewards_train/chosen': '0.038005', 'rewards_train/rejected': '0.021325', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01668', 'logps_train/rejected': '-160.38', 'logps_train/chosen': '-168.71', 'loss/train': '0.69287', 'examples_per_second': '31.584', 'grad_norm': '42.5', 'counters/examples': 21728, 'counters/updates': 679}
skipping logging after 21760 examples to avoid logging too frequently
train stats after 21792 examples: {'rewards_train/chosen': '0.026287', 'rewards_train/rejected': '0.027221', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0009337', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-169.07', 'loss/train': '0.69778', 'examples_per_second': '30.213', 'grad_norm': '36', 'counters/examples': 21792, 'counters/updates': 681}
train stats after 21824 examples: {'rewards_train/chosen': '0.043763', 'rewards_train/rejected': '0.0088095', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034953', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-152.23', 'loss/train': '0.68078', 'examples_per_second': '30.629', 'grad_norm': '31.125', 'counters/examples': 21824, 'counters/updates': 682}
train stats after 21856 examples: {'rewards_train/chosen': '0.053519', 'rewards_train/rejected': '0.041404', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012115', 'logps_train/rejected': '-139.14', 'logps_train/chosen': '-127.66', 'loss/train': '0.69103', 'examples_per_second': '31.205', 'grad_norm': '29.25', 'counters/examples': 21856, 'counters/updates': 683}
train stats after 21888 examples: {'rewards_train/chosen': '0.0080576', 'rewards_train/rejected': '0.048834', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.040776', 'logps_train/rejected': '-138.74', 'logps_train/chosen': '-137.18', 'loss/train': '0.71945', 'examples_per_second': '32.841', 'grad_norm': '30.125', 'counters/examples': 21888, 'counters/updates': 684}
train stats after 21920 examples: {'rewards_train/chosen': '0.013213', 'rewards_train/rejected': '-0.022696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035909', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-155.66', 'loss/train': '0.67918', 'examples_per_second': '30.698', 'grad_norm': '29.75', 'counters/examples': 21920, 'counters/updates': 685}
train stats after 21952 examples: {'rewards_train/chosen': '0.032344', 'rewards_train/rejected': '-0.0025386', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034883', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-152.64', 'loss/train': '0.67887', 'examples_per_second': '30.56', 'grad_norm': '46', 'counters/examples': 21952, 'counters/updates': 686}
train stats after 21984 examples: {'rewards_train/chosen': '0.11069', 'rewards_train/rejected': '0.064522', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046164', 'logps_train/rejected': '-157.46', 'logps_train/chosen': '-167.77', 'loss/train': '0.67537', 'examples_per_second': '31.148', 'grad_norm': '52', 'counters/examples': 21984, 'counters/updates': 687}
train stats after 22016 examples: {'rewards_train/chosen': '0.00068033', 'rewards_train/rejected': '0.066984', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.066303', 'logps_train/rejected': '-146', 'logps_train/chosen': '-108.65', 'loss/train': '0.73052', 'examples_per_second': '30.78', 'grad_norm': '29.125', 'counters/examples': 22016, 'counters/updates': 688}
train stats after 22048 examples: {'rewards_train/chosen': '0.075239', 'rewards_train/rejected': '0.053888', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021352', 'logps_train/rejected': '-99.583', 'logps_train/chosen': '-128.48', 'loss/train': '0.68527', 'examples_per_second': '30.374', 'grad_norm': '27.875', 'counters/examples': 22048, 'counters/updates': 689}
train stats after 22080 examples: {'rewards_train/chosen': '0.063193', 'rewards_train/rejected': '0.016207', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.046986', 'logps_train/rejected': '-178.23', 'logps_train/chosen': '-175.23', 'loss/train': '0.67962', 'examples_per_second': '31.124', 'grad_norm': '33.25', 'counters/examples': 22080, 'counters/updates': 690}
skipping logging after 22112 examples to avoid logging too frequently
train stats after 22144 examples: {'rewards_train/chosen': '0.026256', 'rewards_train/rejected': '0.048418', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022162', 'logps_train/rejected': '-99.069', 'logps_train/chosen': '-117.14', 'loss/train': '0.70839', 'examples_per_second': '30.163', 'grad_norm': '43', 'counters/examples': 22144, 'counters/updates': 692}
train stats after 22176 examples: {'rewards_train/chosen': '0.0081715', 'rewards_train/rejected': '0.049535', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.041364', 'logps_train/rejected': '-121.09', 'logps_train/chosen': '-104.56', 'loss/train': '0.7204', 'examples_per_second': '31.264', 'grad_norm': '31', 'counters/examples': 22176, 'counters/updates': 693}
train stats after 22208 examples: {'rewards_train/chosen': '0.10957', 'rewards_train/rejected': '0.044766', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064806', 'logps_train/rejected': '-119.97', 'logps_train/chosen': '-161.93', 'loss/train': '0.68094', 'examples_per_second': '31.52', 'grad_norm': '41.5', 'counters/examples': 22208, 'counters/updates': 694}
train stats after 22240 examples: {'rewards_train/chosen': '0.07292', 'rewards_train/rejected': '0.054526', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018393', 'logps_train/rejected': '-110.04', 'logps_train/chosen': '-157.73', 'loss/train': '0.69234', 'examples_per_second': '31.507', 'grad_norm': '43.25', 'counters/examples': 22240, 'counters/updates': 695}
train stats after 22272 examples: {'rewards_train/chosen': '-0.0082883', 'rewards_train/rejected': '0.04403', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.052318', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-110.19', 'loss/train': '0.72311', 'examples_per_second': '29.899', 'grad_norm': '57', 'counters/examples': 22272, 'counters/updates': 696}
train stats after 22304 examples: {'rewards_train/chosen': '0.086631', 'rewards_train/rejected': '0.040922', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045708', 'logps_train/rejected': '-153.16', 'logps_train/chosen': '-188.28', 'loss/train': '0.67959', 'examples_per_second': '31.291', 'grad_norm': '32', 'counters/examples': 22304, 'counters/updates': 697}
train stats after 22336 examples: {'rewards_train/chosen': '0.067573', 'rewards_train/rejected': '0.02947', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038103', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-170.22', 'loss/train': '0.6794', 'examples_per_second': '31.196', 'grad_norm': '33', 'counters/examples': 22336, 'counters/updates': 698}
train stats after 22368 examples: {'rewards_train/chosen': '0.060503', 'rewards_train/rejected': '0.036453', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02405', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-173.89', 'loss/train': '0.68578', 'examples_per_second': '32.32', 'grad_norm': '32.5', 'counters/examples': 22368, 'counters/updates': 699}
train stats after 22400 examples: {'rewards_train/chosen': '-0.047983', 'rewards_train/rejected': '-0.084934', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036952', 'logps_train/rejected': '-107.01', 'logps_train/chosen': '-141.67', 'loss/train': '0.67624', 'examples_per_second': '31.173', 'grad_norm': '39.75', 'counters/examples': 22400, 'counters/updates': 700}
train stats after 22432 examples: {'rewards_train/chosen': '0.014168', 'rewards_train/rejected': '0.074875', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.060706', 'logps_train/rejected': '-157.91', 'logps_train/chosen': '-157.27', 'loss/train': '0.72694', 'examples_per_second': '31.287', 'grad_norm': '58.5', 'counters/examples': 22432, 'counters/updates': 701}
skipping logging after 22464 examples to avoid logging too frequently
train stats after 22496 examples: {'rewards_train/chosen': '0.049607', 'rewards_train/rejected': '0.014532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035075', 'logps_train/rejected': '-141.22', 'logps_train/chosen': '-180.94', 'loss/train': '0.68171', 'examples_per_second': '31.272', 'grad_norm': '47.5', 'counters/examples': 22496, 'counters/updates': 703}
train stats after 22528 examples: {'rewards_train/chosen': '0.038616', 'rewards_train/rejected': '0.020064', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.018552', 'logps_train/rejected': '-120.6', 'logps_train/chosen': '-137.1', 'loss/train': '0.68699', 'examples_per_second': '31.013', 'grad_norm': '29.375', 'counters/examples': 22528, 'counters/updates': 704}
skipping logging after 22560 examples to avoid logging too frequently
train stats after 22592 examples: {'rewards_train/chosen': '0.028419', 'rewards_train/rejected': '-0.035761', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.06418', 'logps_train/rejected': '-156.16', 'logps_train/chosen': '-151.09', 'loss/train': '0.67956', 'examples_per_second': '30.138', 'grad_norm': '32.25', 'counters/examples': 22592, 'counters/updates': 706}
train stats after 22624 examples: {'rewards_train/chosen': '0.025258', 'rewards_train/rejected': '0.02521', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '4.8282e-05', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-169.45', 'loss/train': '0.6987', 'examples_per_second': '31.179', 'grad_norm': '33.5', 'counters/examples': 22624, 'counters/updates': 707}
train stats after 22656 examples: {'rewards_train/chosen': '0.051599', 'rewards_train/rejected': '0.017638', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033961', 'logps_train/rejected': '-110.74', 'logps_train/chosen': '-138.37', 'loss/train': '0.68405', 'examples_per_second': '31.317', 'grad_norm': '40.75', 'counters/examples': 22656, 'counters/updates': 708}
train stats after 22688 examples: {'rewards_train/chosen': '0.1007', 'rewards_train/rejected': '0.055343', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045358', 'logps_train/rejected': '-140.33', 'logps_train/chosen': '-156.8', 'loss/train': '0.67607', 'examples_per_second': '31.593', 'grad_norm': '35.75', 'counters/examples': 22688, 'counters/updates': 709}
train stats after 22720 examples: {'rewards_train/chosen': '0.013941', 'rewards_train/rejected': '-0.045371', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059312', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-123.96', 'loss/train': '0.66958', 'examples_per_second': '31.327', 'grad_norm': '32.5', 'counters/examples': 22720, 'counters/updates': 710}
skipping logging after 22752 examples to avoid logging too frequently
train stats after 22784 examples: {'rewards_train/chosen': '0.043184', 'rewards_train/rejected': '0.034599', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0085856', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-157.72', 'loss/train': '0.6974', 'examples_per_second': '32.053', 'grad_norm': '36', 'counters/examples': 22784, 'counters/updates': 712}
train stats after 22816 examples: {'rewards_train/chosen': '0.041354', 'rewards_train/rejected': '-0.028532', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069887', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-100.55', 'loss/train': '0.6629', 'examples_per_second': '30.566', 'grad_norm': '38', 'counters/examples': 22816, 'counters/updates': 713}
train stats after 22848 examples: {'rewards_train/chosen': '-0.037429', 'rewards_train/rejected': '-0.030406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0070238', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-132.77', 'loss/train': '0.7039', 'examples_per_second': '29.798', 'grad_norm': '30.5', 'counters/examples': 22848, 'counters/updates': 714}
skipping logging after 22880 examples to avoid logging too frequently
train stats after 22912 examples: {'rewards_train/chosen': '0.084576', 'rewards_train/rejected': '0.057423', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027153', 'logps_train/rejected': '-123.64', 'logps_train/chosen': '-157.88', 'loss/train': '0.69367', 'examples_per_second': '33.485', 'grad_norm': '62', 'counters/examples': 22912, 'counters/updates': 716}
skipping logging after 22944 examples to avoid logging too frequently
train stats after 22976 examples: {'rewards_train/chosen': '-0.019274', 'rewards_train/rejected': '0.035652', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054925', 'logps_train/rejected': '-116.76', 'logps_train/chosen': '-128.97', 'loss/train': '0.72363', 'examples_per_second': '32.935', 'grad_norm': '29.375', 'counters/examples': 22976, 'counters/updates': 718}
train stats after 23008 examples: {'rewards_train/chosen': '0.01663', 'rewards_train/rejected': '-0.043057', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059688', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-118.28', 'loss/train': '0.67119', 'examples_per_second': '31.025', 'grad_norm': '28.25', 'counters/examples': 23008, 'counters/updates': 719}
train stats after 23040 examples: {'rewards_train/chosen': '0.0042766', 'rewards_train/rejected': '0.0025192', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0017575', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-135.45', 'loss/train': '0.69616', 'examples_per_second': '30.356', 'grad_norm': '28.25', 'counters/examples': 23040, 'counters/updates': 720}
skipping logging after 23072 examples to avoid logging too frequently
train stats after 23104 examples: {'rewards_train/chosen': '-0.0092533', 'rewards_train/rejected': '0.015494', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024747', 'logps_train/rejected': '-142.52', 'logps_train/chosen': '-147.56', 'loss/train': '0.71569', 'examples_per_second': '33.158', 'grad_norm': '39.5', 'counters/examples': 23104, 'counters/updates': 722}
skipping logging after 23136 examples to avoid logging too frequently
train stats after 23168 examples: {'rewards_train/chosen': '0.059961', 'rewards_train/rejected': '0.036594', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023367', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-183.56', 'loss/train': '0.68835', 'examples_per_second': '31.236', 'grad_norm': '28.5', 'counters/examples': 23168, 'counters/updates': 724}
train stats after 23200 examples: {'rewards_train/chosen': '0.055524', 'rewards_train/rejected': '0.071156', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015632', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-161.31', 'loss/train': '0.70761', 'examples_per_second': '31.226', 'grad_norm': '40', 'counters/examples': 23200, 'counters/updates': 725}
train stats after 23232 examples: {'rewards_train/chosen': '0.027296', 'rewards_train/rejected': '0.10416', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.076859', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-127.73', 'loss/train': '0.74207', 'examples_per_second': '31.201', 'grad_norm': '60', 'counters/examples': 23232, 'counters/updates': 726}
train stats after 23264 examples: {'rewards_train/chosen': '-0.0083708', 'rewards_train/rejected': '0.00021951', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0085904', 'logps_train/rejected': '-160.48', 'logps_train/chosen': '-150.57', 'loss/train': '0.70265', 'examples_per_second': '31.126', 'grad_norm': '35.5', 'counters/examples': 23264, 'counters/updates': 727}
train stats after 23296 examples: {'rewards_train/chosen': '0.10324', 'rewards_train/rejected': '0.049502', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05374', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-170.47', 'loss/train': '0.67287', 'examples_per_second': '30.624', 'grad_norm': '35.5', 'counters/examples': 23296, 'counters/updates': 728}
train stats after 23328 examples: {'rewards_train/chosen': '0.043385', 'rewards_train/rejected': '-0.0037772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047162', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-175.3', 'loss/train': '0.67341', 'examples_per_second': '31.282', 'grad_norm': '37.5', 'counters/examples': 23328, 'counters/updates': 729}
train stats after 23360 examples: {'rewards_train/chosen': '0.039583', 'rewards_train/rejected': '0.032502', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0070808', 'logps_train/rejected': '-146.78', 'logps_train/chosen': '-136.09', 'loss/train': '0.695', 'examples_per_second': '31.506', 'grad_norm': '27.375', 'counters/examples': 23360, 'counters/updates': 730}
train stats after 23392 examples: {'rewards_train/chosen': '-0.036546', 'rewards_train/rejected': '0.022215', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.058761', 'logps_train/rejected': '-130.32', 'logps_train/chosen': '-135.38', 'loss/train': '0.72915', 'examples_per_second': '31.267', 'grad_norm': '41', 'counters/examples': 23392, 'counters/updates': 731}
train stats after 23424 examples: {'rewards_train/chosen': '0.012072', 'rewards_train/rejected': '0.022342', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010269', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-137.15', 'loss/train': '0.70261', 'examples_per_second': '31.899', 'grad_norm': '31.75', 'counters/examples': 23424, 'counters/updates': 732}
train stats after 23456 examples: {'rewards_train/chosen': '0.073709', 'rewards_train/rejected': '0.0038215', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069887', 'logps_train/rejected': '-157.38', 'logps_train/chosen': '-153.77', 'loss/train': '0.66582', 'examples_per_second': '30.246', 'grad_norm': '42.25', 'counters/examples': 23456, 'counters/updates': 733}
train stats after 23488 examples: {'rewards_train/chosen': '0.075647', 'rewards_train/rejected': '0.12939', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.053745', 'logps_train/rejected': '-167.46', 'logps_train/chosen': '-145.7', 'loss/train': '0.73217', 'examples_per_second': '31.292', 'grad_norm': '43.25', 'counters/examples': 23488, 'counters/updates': 734}
train stats after 23520 examples: {'rewards_train/chosen': '0.00035823', 'rewards_train/rejected': '0.014693', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014335', 'logps_train/rejected': '-168.5', 'logps_train/chosen': '-131.35', 'loss/train': '0.70558', 'examples_per_second': '31.277', 'grad_norm': '32', 'counters/examples': 23520, 'counters/updates': 735}
skipping logging after 23552 examples to avoid logging too frequently
train stats after 23584 examples: {'rewards_train/chosen': '0.038023', 'rewards_train/rejected': '0.040057', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.002034', 'logps_train/rejected': '-117.94', 'logps_train/chosen': '-177.19', 'loss/train': '0.70259', 'examples_per_second': '29.821', 'grad_norm': '32.5', 'counters/examples': 23584, 'counters/updates': 737}
train stats after 23616 examples: {'rewards_train/chosen': '0.087182', 'rewards_train/rejected': '-0.0069177', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094099', 'logps_train/rejected': '-171.85', 'logps_train/chosen': '-160.51', 'loss/train': '0.65605', 'examples_per_second': '29.798', 'grad_norm': '35.5', 'counters/examples': 23616, 'counters/updates': 738}
train stats after 23648 examples: {'rewards_train/chosen': '0.025872', 'rewards_train/rejected': '-0.0036484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02952', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-134.06', 'loss/train': '0.68432', 'examples_per_second': '31.995', 'grad_norm': '31', 'counters/examples': 23648, 'counters/updates': 739}
train stats after 23680 examples: {'rewards_train/chosen': '0.042669', 'rewards_train/rejected': '0.024793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017876', 'logps_train/rejected': '-100.21', 'logps_train/chosen': '-137.44', 'loss/train': '0.69026', 'examples_per_second': '29.847', 'grad_norm': '28.875', 'counters/examples': 23680, 'counters/updates': 740}
train stats after 23712 examples: {'rewards_train/chosen': '0.047606', 'rewards_train/rejected': '0.048008', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00040236', 'logps_train/rejected': '-155', 'logps_train/chosen': '-152.33', 'loss/train': '0.70027', 'examples_per_second': '31.051', 'grad_norm': '31.75', 'counters/examples': 23712, 'counters/updates': 741}
skipping logging after 23744 examples to avoid logging too frequently
train stats after 23776 examples: {'rewards_train/chosen': '-0.048114', 'rewards_train/rejected': '0.062486', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.1106', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-138.78', 'loss/train': '0.76085', 'examples_per_second': '30.812', 'grad_norm': '40', 'counters/examples': 23776, 'counters/updates': 743}
train stats after 23808 examples: {'rewards_train/chosen': '-0.0042489', 'rewards_train/rejected': '0.031573', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035822', 'logps_train/rejected': '-141.85', 'logps_train/chosen': '-155.49', 'loss/train': '0.71561', 'examples_per_second': '32.105', 'grad_norm': '40.75', 'counters/examples': 23808, 'counters/updates': 744}
skipping logging after 23840 examples to avoid logging too frequently
train stats after 23872 examples: {'rewards_train/chosen': '0.064881', 'rewards_train/rejected': '0.00037567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064505', 'logps_train/rejected': '-98.244', 'logps_train/chosen': '-124.69', 'loss/train': '0.66505', 'examples_per_second': '23.852', 'grad_norm': '25.875', 'counters/examples': 23872, 'counters/updates': 746}
train stats after 23904 examples: {'rewards_train/chosen': '0.065747', 'rewards_train/rejected': '0.043613', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022134', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-150.2', 'loss/train': '0.68508', 'examples_per_second': '31.057', 'grad_norm': '30', 'counters/examples': 23904, 'counters/updates': 747}
skipping logging after 23936 examples to avoid logging too frequently
train stats after 23968 examples: {'rewards_train/chosen': '0.042078', 'rewards_train/rejected': '0.036115', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0059633', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-128.7', 'loss/train': '0.69263', 'examples_per_second': '33.937', 'grad_norm': '28.375', 'counters/examples': 23968, 'counters/updates': 749}
train stats after 24000 examples: {'rewards_train/chosen': '0.057341', 'rewards_train/rejected': '0.0034512', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05389', 'logps_train/rejected': '-113.35', 'logps_train/chosen': '-148.71', 'loss/train': '0.66986', 'examples_per_second': '31.292', 'grad_norm': '25.75', 'counters/examples': 24000, 'counters/updates': 750}
Running evaluation after 24000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:04,  3.08it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:04,  3.22it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.45it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.61it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.63it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.89it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.80it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.87it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.82it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.83it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.84it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.86it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.77it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.82it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.74it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.73it/s]
eval after 24000: {'rewards_eval/chosen': '0.043786', 'rewards_eval/rejected': '0.029626', 'rewards_eval/accuracies': '0.52734', 'rewards_eval/margins': '0.01416', 'logps_eval/rejected': '-121.84', 'logps_eval/chosen': '-143.67', 'loss/eval': '0.69237'}
skipping save for non epoch
train stats after 24032 examples: {'rewards_train/chosen': '0.081427', 'rewards_train/rejected': '0.10159', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020162', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-128.87', 'loss/train': '0.71113', 'examples_per_second': '31.791', 'grad_norm': '32.75', 'counters/examples': 24032, 'counters/updates': 751}
train stats after 24064 examples: {'rewards_train/chosen': '0.051564', 'rewards_train/rejected': '0.0094781', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042086', 'logps_train/rejected': '-102.31', 'logps_train/chosen': '-132.22', 'loss/train': '0.67834', 'examples_per_second': '29.813', 'grad_norm': '27.5', 'counters/examples': 24064, 'counters/updates': 752}
train stats after 24096 examples: {'rewards_train/chosen': '0.049998', 'rewards_train/rejected': '-0.00047071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050468', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-162.66', 'loss/train': '0.6717', 'examples_per_second': '31.46', 'grad_norm': '26.75', 'counters/examples': 24096, 'counters/updates': 753}
train stats after 24128 examples: {'rewards_train/chosen': '0.095101', 'rewards_train/rejected': '0.006466', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088635', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-133.8', 'loss/train': '0.68088', 'examples_per_second': '31.241', 'grad_norm': '51.25', 'counters/examples': 24128, 'counters/updates': 754}
train stats after 24160 examples: {'rewards_train/chosen': '0.098786', 'rewards_train/rejected': '0.026926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07186', 'logps_train/rejected': '-108.69', 'logps_train/chosen': '-133.95', 'loss/train': '0.67385', 'examples_per_second': '31.932', 'grad_norm': '30.125', 'counters/examples': 24160, 'counters/updates': 755}
train stats after 24192 examples: {'rewards_train/chosen': '0.029505', 'rewards_train/rejected': '0.039676', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010171', 'logps_train/rejected': '-111.83', 'logps_train/chosen': '-137.2', 'loss/train': '0.70216', 'examples_per_second': '30.419', 'grad_norm': '30.875', 'counters/examples': 24192, 'counters/updates': 756}
train stats after 24224 examples: {'rewards_train/chosen': '0.048989', 'rewards_train/rejected': '0.0063003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042689', 'logps_train/rejected': '-106.51', 'logps_train/chosen': '-187.85', 'loss/train': '0.67586', 'examples_per_second': '30.553', 'grad_norm': '61', 'counters/examples': 24224, 'counters/updates': 757}
train stats after 24256 examples: {'rewards_train/chosen': '0.0076944', 'rewards_train/rejected': '0.017766', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010071', 'logps_train/rejected': '-133.93', 'logps_train/chosen': '-171.52', 'loss/train': '0.70327', 'examples_per_second': '31.298', 'grad_norm': '38.25', 'counters/examples': 24256, 'counters/updates': 758}
train stats after 24288 examples: {'rewards_train/chosen': '0.023364', 'rewards_train/rejected': '0.028507', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0051432', 'logps_train/rejected': '-114.06', 'logps_train/chosen': '-145.11', 'loss/train': '0.6973', 'examples_per_second': '32.314', 'grad_norm': '28.75', 'counters/examples': 24288, 'counters/updates': 759}
skipping logging after 24320 examples to avoid logging too frequently
train stats after 24352 examples: {'rewards_train/chosen': '0.023435', 'rewards_train/rejected': '0.017064', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0063701', 'logps_train/rejected': '-135.96', 'logps_train/chosen': '-142.92', 'loss/train': '0.69734', 'examples_per_second': '30.852', 'grad_norm': '40', 'counters/examples': 24352, 'counters/updates': 761}
train stats after 24384 examples: {'rewards_train/chosen': '0.082189', 'rewards_train/rejected': '0.04291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039279', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-152.21', 'loss/train': '0.69241', 'examples_per_second': '30.888', 'grad_norm': '31.75', 'counters/examples': 24384, 'counters/updates': 762}
train stats after 24416 examples: {'rewards_train/chosen': '0.069597', 'rewards_train/rejected': '0.059816', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0097808', 'logps_train/rejected': '-132.17', 'logps_train/chosen': '-130.95', 'loss/train': '0.69239', 'examples_per_second': '30.036', 'grad_norm': '47.75', 'counters/examples': 24416, 'counters/updates': 763}
train stats after 24448 examples: {'rewards_train/chosen': '0.025193', 'rewards_train/rejected': '0.017948', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.007245', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-146.34', 'loss/train': '0.69471', 'examples_per_second': '31.543', 'grad_norm': '32.25', 'counters/examples': 24448, 'counters/updates': 764}
train stats after 24480 examples: {'rewards_train/chosen': '0.048182', 'rewards_train/rejected': '0.02919', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018992', 'logps_train/rejected': '-107.36', 'logps_train/chosen': '-149.11', 'loss/train': '0.6891', 'examples_per_second': '30.078', 'grad_norm': '30', 'counters/examples': 24480, 'counters/updates': 765}
train stats after 24512 examples: {'rewards_train/chosen': '0.025282', 'rewards_train/rejected': '-0.035759', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.061041', 'logps_train/rejected': '-113.43', 'logps_train/chosen': '-142.64', 'loss/train': '0.66626', 'examples_per_second': '31.778', 'grad_norm': '35.75', 'counters/examples': 24512, 'counters/updates': 766}
train stats after 24544 examples: {'rewards_train/chosen': '0.057661', 'rewards_train/rejected': '0.091285', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033624', 'logps_train/rejected': '-153.02', 'logps_train/chosen': '-159.17', 'loss/train': '0.7225', 'examples_per_second': '24.13', 'grad_norm': '56', 'counters/examples': 24544, 'counters/updates': 767}
train stats after 24576 examples: {'rewards_train/chosen': '0.038127', 'rewards_train/rejected': '0.023271', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014856', 'logps_train/rejected': '-121.79', 'logps_train/chosen': '-138.76', 'loss/train': '0.69078', 'examples_per_second': '31.534', 'grad_norm': '28.375', 'counters/examples': 24576, 'counters/updates': 768}
train stats after 24608 examples: {'rewards_train/chosen': '0.11748', 'rewards_train/rejected': '0.081142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036336', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-136.12', 'loss/train': '0.68125', 'examples_per_second': '31.929', 'grad_norm': '44.5', 'counters/examples': 24608, 'counters/updates': 769}
train stats after 24640 examples: {'rewards_train/chosen': '0.0075481', 'rewards_train/rejected': '0.023584', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016036', 'logps_train/rejected': '-82.799', 'logps_train/chosen': '-162.72', 'loss/train': '0.70502', 'examples_per_second': '24.817', 'grad_norm': '49.5', 'counters/examples': 24640, 'counters/updates': 770}
skipping logging after 24672 examples to avoid logging too frequently
train stats after 24704 examples: {'rewards_train/chosen': '0.061554', 'rewards_train/rejected': '0.026537', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035017', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-131.02', 'loss/train': '0.68012', 'examples_per_second': '32.145', 'grad_norm': '28.5', 'counters/examples': 24704, 'counters/updates': 772}
train stats after 24736 examples: {'rewards_train/chosen': '0.019511', 'rewards_train/rejected': '-0.023587', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043098', 'logps_train/rejected': '-159.11', 'logps_train/chosen': '-135.66', 'loss/train': '0.67912', 'examples_per_second': '31.687', 'grad_norm': '45', 'counters/examples': 24736, 'counters/updates': 773}
train stats after 24768 examples: {'rewards_train/chosen': '0.034603', 'rewards_train/rejected': '0.02084', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013763', 'logps_train/rejected': '-129.46', 'logps_train/chosen': '-140.74', 'loss/train': '0.69011', 'examples_per_second': '32.781', 'grad_norm': '34.75', 'counters/examples': 24768, 'counters/updates': 774}
train stats after 24800 examples: {'rewards_train/chosen': '0.064569', 'rewards_train/rejected': '-0.026665', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091234', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-152.45', 'loss/train': '0.6527', 'examples_per_second': '31.1', 'grad_norm': '31.125', 'counters/examples': 24800, 'counters/updates': 775}
train stats after 24832 examples: {'rewards_train/chosen': '0.063474', 'rewards_train/rejected': '0.016777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046696', 'logps_train/rejected': '-99.762', 'logps_train/chosen': '-152.55', 'loss/train': '0.67911', 'examples_per_second': '32.133', 'grad_norm': '29.375', 'counters/examples': 24832, 'counters/updates': 776}
train stats after 24864 examples: {'rewards_train/chosen': '0.082731', 'rewards_train/rejected': '-0.032604', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11533', 'logps_train/rejected': '-150.43', 'logps_train/chosen': '-163.4', 'loss/train': '0.64791', 'examples_per_second': '30.672', 'grad_norm': '32.5', 'counters/examples': 24864, 'counters/updates': 777}
train stats after 24896 examples: {'rewards_train/chosen': '-0.034378', 'rewards_train/rejected': '0.013053', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047432', 'logps_train/rejected': '-186.81', 'logps_train/chosen': '-165.42', 'loss/train': '0.74172', 'examples_per_second': '31.154', 'grad_norm': '77', 'counters/examples': 24896, 'counters/updates': 778}
skipping logging after 24928 examples to avoid logging too frequently
train stats after 24960 examples: {'rewards_train/chosen': '0.098072', 'rewards_train/rejected': '0.029873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068198', 'logps_train/rejected': '-100.88', 'logps_train/chosen': '-125.12', 'loss/train': '0.67016', 'examples_per_second': '30.579', 'grad_norm': '34.25', 'counters/examples': 24960, 'counters/updates': 780}
train stats after 24992 examples: {'rewards_train/chosen': '0.079672', 'rewards_train/rejected': '-0.028356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10803', 'logps_train/rejected': '-159.53', 'logps_train/chosen': '-173.93', 'loss/train': '0.64515', 'examples_per_second': '30.837', 'grad_norm': '32.75', 'counters/examples': 24992, 'counters/updates': 781}
train stats after 25024 examples: {'rewards_train/chosen': '0.039619', 'rewards_train/rejected': '0.027892', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011727', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-136.44', 'loss/train': '0.69111', 'examples_per_second': '32.408', 'grad_norm': '28.75', 'counters/examples': 25024, 'counters/updates': 782}
train stats after 25056 examples: {'rewards_train/chosen': '0.16052', 'rewards_train/rejected': '0.12353', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036995', 'logps_train/rejected': '-156.48', 'logps_train/chosen': '-149.42', 'loss/train': '0.69431', 'examples_per_second': '31.465', 'grad_norm': '34', 'counters/examples': 25056, 'counters/updates': 783}
train stats after 25088 examples: {'rewards_train/chosen': '-0.0053778', 'rewards_train/rejected': '-0.062655', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.057277', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-155.07', 'loss/train': '0.70097', 'examples_per_second': '31.427', 'grad_norm': '44.5', 'counters/examples': 25088, 'counters/updates': 784}
train stats after 25120 examples: {'rewards_train/chosen': '0.040834', 'rewards_train/rejected': '0.011691', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029144', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-140.4', 'loss/train': '0.68425', 'examples_per_second': '30.641', 'grad_norm': '34.25', 'counters/examples': 25120, 'counters/updates': 785}
train stats after 25152 examples: {'rewards_train/chosen': '0.0086399', 'rewards_train/rejected': '-0.011849', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.020489', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-127.65', 'loss/train': '0.68991', 'examples_per_second': '31.512', 'grad_norm': '29', 'counters/examples': 25152, 'counters/updates': 786}
skipping logging after 25184 examples to avoid logging too frequently
train stats after 25216 examples: {'rewards_train/chosen': '0.053747', 'rewards_train/rejected': '0.0039725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049774', 'logps_train/rejected': '-107.45', 'logps_train/chosen': '-138.91', 'loss/train': '0.67428', 'examples_per_second': '32.812', 'grad_norm': '35.5', 'counters/examples': 25216, 'counters/updates': 788}
train stats after 25248 examples: {'rewards_train/chosen': '0.022219', 'rewards_train/rejected': '0.045319', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0231', 'logps_train/rejected': '-142.06', 'logps_train/chosen': '-125.85', 'loss/train': '0.71118', 'examples_per_second': '31.894', 'grad_norm': '30.875', 'counters/examples': 25248, 'counters/updates': 789}
train stats after 25280 examples: {'rewards_train/chosen': '0.054051', 'rewards_train/rejected': '0.02592', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028131', 'logps_train/rejected': '-110.37', 'logps_train/chosen': '-126.35', 'loss/train': '0.68524', 'examples_per_second': '31.485', 'grad_norm': '34.5', 'counters/examples': 25280, 'counters/updates': 790}
skipping logging after 25312 examples to avoid logging too frequently
train stats after 25344 examples: {'rewards_train/chosen': '0.14127', 'rewards_train/rejected': '0.07451', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066763', 'logps_train/rejected': '-133.77', 'logps_train/chosen': '-151.36', 'loss/train': '0.66497', 'examples_per_second': '30.422', 'grad_norm': '33.5', 'counters/examples': 25344, 'counters/updates': 792}
skipping logging after 25376 examples to avoid logging too frequently
train stats after 25408 examples: {'rewards_train/chosen': '0.1119', 'rewards_train/rejected': '-0.037708', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14961', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-152.06', 'loss/train': '0.64259', 'examples_per_second': '31.443', 'grad_norm': '33.75', 'counters/examples': 25408, 'counters/updates': 794}
train stats after 25440 examples: {'rewards_train/chosen': '0.046162', 'rewards_train/rejected': '0.020747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.025415', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-146.37', 'loss/train': '0.68368', 'examples_per_second': '31.457', 'grad_norm': '30.5', 'counters/examples': 25440, 'counters/updates': 795}
train stats after 25472 examples: {'rewards_train/chosen': '0.023737', 'rewards_train/rejected': '0.097695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.073958', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-105.27', 'loss/train': '0.74775', 'examples_per_second': '31.624', 'grad_norm': '38', 'counters/examples': 25472, 'counters/updates': 796}
train stats after 25504 examples: {'rewards_train/chosen': '0.045899', 'rewards_train/rejected': '0.045244', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00065516', 'logps_train/rejected': '-117.76', 'logps_train/chosen': '-153.44', 'loss/train': '0.69758', 'examples_per_second': '31.448', 'grad_norm': '54.5', 'counters/examples': 25504, 'counters/updates': 797}
train stats after 25536 examples: {'rewards_train/chosen': '0.041344', 'rewards_train/rejected': '-0.00066457', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.042008', 'logps_train/rejected': '-101.49', 'logps_train/chosen': '-126.51', 'loss/train': '0.6748', 'examples_per_second': '31.654', 'grad_norm': '27.625', 'counters/examples': 25536, 'counters/updates': 798}
train stats after 25568 examples: {'rewards_train/chosen': '0.10927', 'rewards_train/rejected': '0.012371', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096894', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-143.5', 'loss/train': '0.66455', 'examples_per_second': '30.769', 'grad_norm': '31.625', 'counters/examples': 25568, 'counters/updates': 799}
train stats after 25600 examples: {'rewards_train/chosen': '0.084874', 'rewards_train/rejected': '0.085525', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0006512', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-153.66', 'loss/train': '0.70332', 'examples_per_second': '31.523', 'grad_norm': '44', 'counters/examples': 25600, 'counters/updates': 800}
train stats after 25632 examples: {'rewards_train/chosen': '0.044499', 'rewards_train/rejected': '-0.011036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055534', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-132.46', 'loss/train': '0.6724', 'examples_per_second': '31.182', 'grad_norm': '29.75', 'counters/examples': 25632, 'counters/updates': 801}
train stats after 25664 examples: {'rewards_train/chosen': '0.049123', 'rewards_train/rejected': '0.0080581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041065', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-138.2', 'loss/train': '0.67762', 'examples_per_second': '30.424', 'grad_norm': '25.75', 'counters/examples': 25664, 'counters/updates': 802}
train stats after 25696 examples: {'rewards_train/chosen': '0.041055', 'rewards_train/rejected': '0.013049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028006', 'logps_train/rejected': '-141.02', 'logps_train/chosen': '-199.78', 'loss/train': '0.68584', 'examples_per_second': '30.354', 'grad_norm': '34.25', 'counters/examples': 25696, 'counters/updates': 803}
train stats after 25728 examples: {'rewards_train/chosen': '0.018766', 'rewards_train/rejected': '0.046053', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027286', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-167.8', 'loss/train': '0.71184', 'examples_per_second': '32.233', 'grad_norm': '37.25', 'counters/examples': 25728, 'counters/updates': 804}
train stats after 25760 examples: {'rewards_train/chosen': '0.069969', 'rewards_train/rejected': '-0.0041493', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074119', 'logps_train/rejected': '-138.04', 'logps_train/chosen': '-138.04', 'loss/train': '0.65955', 'examples_per_second': '30.505', 'grad_norm': '30', 'counters/examples': 25760, 'counters/updates': 805}
skipping logging after 25792 examples to avoid logging too frequently
train stats after 25824 examples: {'rewards_train/chosen': '0.027712', 'rewards_train/rejected': '0.037302', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0095904', 'logps_train/rejected': '-151.19', 'logps_train/chosen': '-142.79', 'loss/train': '0.70179', 'examples_per_second': '29.965', 'grad_norm': '43.5', 'counters/examples': 25824, 'counters/updates': 807}
train stats after 25856 examples: {'rewards_train/chosen': '0.035833', 'rewards_train/rejected': '0.026328', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0095045', 'logps_train/rejected': '-105.01', 'logps_train/chosen': '-162.44', 'loss/train': '0.69194', 'examples_per_second': '31.68', 'grad_norm': '31.25', 'counters/examples': 25856, 'counters/updates': 808}
train stats after 25888 examples: {'rewards_train/chosen': '0.092925', 'rewards_train/rejected': '0.056386', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036539', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-155.14', 'loss/train': '0.68187', 'examples_per_second': '31.723', 'grad_norm': '29.625', 'counters/examples': 25888, 'counters/updates': 809}
train stats after 25920 examples: {'rewards_train/chosen': '-0.0095381', 'rewards_train/rejected': '0.033815', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043353', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-167.98', 'loss/train': '0.72066', 'examples_per_second': '31.57', 'grad_norm': '52.5', 'counters/examples': 25920, 'counters/updates': 810}
train stats after 25952 examples: {'rewards_train/chosen': '0.040265', 'rewards_train/rejected': '0.0028696', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037395', 'logps_train/rejected': '-108.94', 'logps_train/chosen': '-131.7', 'loss/train': '0.67722', 'examples_per_second': '31.42', 'grad_norm': '25.625', 'counters/examples': 25952, 'counters/updates': 811}
train stats after 25984 examples: {'rewards_train/chosen': '0.052038', 'rewards_train/rejected': '0.046359', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.005679', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-129.61', 'loss/train': '0.70107', 'examples_per_second': '32.148', 'grad_norm': '32.25', 'counters/examples': 25984, 'counters/updates': 812}
train stats after 26016 examples: {'rewards_train/chosen': '0.087317', 'rewards_train/rejected': '0.061937', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02538', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-148.29', 'loss/train': '0.70492', 'examples_per_second': '32.193', 'grad_norm': '58.5', 'counters/examples': 26016, 'counters/updates': 813}
train stats after 26048 examples: {'rewards_train/chosen': '0.031112', 'rewards_train/rejected': '-0.011878', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04299', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-186.26', 'loss/train': '0.67758', 'examples_per_second': '30.495', 'grad_norm': '38.75', 'counters/examples': 26048, 'counters/updates': 814}
skipping logging after 26080 examples to avoid logging too frequently
train stats after 26112 examples: {'rewards_train/chosen': '0.098014', 'rewards_train/rejected': '0.070144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02787', 'logps_train/rejected': '-137.23', 'logps_train/chosen': '-135.53', 'loss/train': '0.6844', 'examples_per_second': '36.141', 'grad_norm': '35.25', 'counters/examples': 26112, 'counters/updates': 816}
train stats after 26144 examples: {'rewards_train/chosen': '0.058091', 'rewards_train/rejected': '-0.022586', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080677', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-141.4', 'loss/train': '0.65626', 'examples_per_second': '32.088', 'grad_norm': '27.25', 'counters/examples': 26144, 'counters/updates': 817}
train stats after 26176 examples: {'rewards_train/chosen': '0.034826', 'rewards_train/rejected': '0.028488', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0063373', 'logps_train/rejected': '-104.76', 'logps_train/chosen': '-113.27', 'loss/train': '0.6955', 'examples_per_second': '30.209', 'grad_norm': '28.875', 'counters/examples': 26176, 'counters/updates': 818}
train stats after 26208 examples: {'rewards_train/chosen': '0.052188', 'rewards_train/rejected': '0.10472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.052533', 'logps_train/rejected': '-148.24', 'logps_train/chosen': '-127.35', 'loss/train': '0.73095', 'examples_per_second': '30.677', 'grad_norm': '40.25', 'counters/examples': 26208, 'counters/updates': 819}
train stats after 26240 examples: {'rewards_train/chosen': '0.047325', 'rewards_train/rejected': '0.00053587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046789', 'logps_train/rejected': '-93.263', 'logps_train/chosen': '-127.64', 'loss/train': '0.67496', 'examples_per_second': '30.69', 'grad_norm': '31.125', 'counters/examples': 26240, 'counters/updates': 820}
train stats after 26272 examples: {'rewards_train/chosen': '-0.003898', 'rewards_train/rejected': '-0.019162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015264', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-150.45', 'loss/train': '0.6882', 'examples_per_second': '31.897', 'grad_norm': '31.625', 'counters/examples': 26272, 'counters/updates': 821}
train stats after 26304 examples: {'rewards_train/chosen': '0.014267', 'rewards_train/rejected': '0.050718', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036451', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-162.64', 'loss/train': '0.71783', 'examples_per_second': '32.1', 'grad_norm': '79', 'counters/examples': 26304, 'counters/updates': 822}
train stats after 26336 examples: {'rewards_train/chosen': '0.039926', 'rewards_train/rejected': '0.040217', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00029087', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-165.46', 'loss/train': '0.70128', 'examples_per_second': '32.546', 'grad_norm': '34.25', 'counters/examples': 26336, 'counters/updates': 823}
train stats after 26368 examples: {'rewards_train/chosen': '0.099717', 'rewards_train/rejected': '0.045125', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054592', 'logps_train/rejected': '-138.77', 'logps_train/chosen': '-149.85', 'loss/train': '0.67899', 'examples_per_second': '30.863', 'grad_norm': '37.75', 'counters/examples': 26368, 'counters/updates': 824}
train stats after 26400 examples: {'rewards_train/chosen': '0.058904', 'rewards_train/rejected': '0.043088', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015816', 'logps_train/rejected': '-176.79', 'logps_train/chosen': '-186.08', 'loss/train': '0.68949', 'examples_per_second': '30.199', 'grad_norm': '38.25', 'counters/examples': 26400, 'counters/updates': 825}
skipping logging after 26432 examples to avoid logging too frequently
train stats after 26464 examples: {'rewards_train/chosen': '0.075363', 'rewards_train/rejected': '0.037597', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037766', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-158.47', 'loss/train': '0.6823', 'examples_per_second': '30.151', 'grad_norm': '35.5', 'counters/examples': 26464, 'counters/updates': 827}
skipping logging after 26496 examples to avoid logging too frequently
train stats after 26528 examples: {'rewards_train/chosen': '0.048172', 'rewards_train/rejected': '0.0039685', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044204', 'logps_train/rejected': '-130.89', 'logps_train/chosen': '-132.28', 'loss/train': '0.67586', 'examples_per_second': '32.699', 'grad_norm': '43', 'counters/examples': 26528, 'counters/updates': 829}
train stats after 26560 examples: {'rewards_train/chosen': '0.048353', 'rewards_train/rejected': '0.03359', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014763', 'logps_train/rejected': '-113.52', 'logps_train/chosen': '-115.9', 'loss/train': '0.68754', 'examples_per_second': '31.717', 'grad_norm': '24.375', 'counters/examples': 26560, 'counters/updates': 830}
skipping logging after 26592 examples to avoid logging too frequently
train stats after 26624 examples: {'rewards_train/chosen': '0.055531', 'rewards_train/rejected': '0.051881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0036505', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-134.14', 'loss/train': '0.70712', 'examples_per_second': '31.844', 'grad_norm': '32.25', 'counters/examples': 26624, 'counters/updates': 832}
skipping logging after 26656 examples to avoid logging too frequently
train stats after 26688 examples: {'rewards_train/chosen': '0.073503', 'rewards_train/rejected': '0.0020201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071483', 'logps_train/rejected': '-164.07', 'logps_train/chosen': '-180.26', 'loss/train': '0.67081', 'examples_per_second': '31.49', 'grad_norm': '30.625', 'counters/examples': 26688, 'counters/updates': 834}
train stats after 26720 examples: {'rewards_train/chosen': '0.038952', 'rewards_train/rejected': '0.039169', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00021683', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-132.34', 'loss/train': '0.70451', 'examples_per_second': '30.315', 'grad_norm': '37.5', 'counters/examples': 26720, 'counters/updates': 835}
train stats after 26752 examples: {'rewards_train/chosen': '0.023289', 'rewards_train/rejected': '-0.01148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.034769', 'logps_train/rejected': '-101.78', 'logps_train/chosen': '-123.08', 'loss/train': '0.67848', 'examples_per_second': '30.669', 'grad_norm': '24', 'counters/examples': 26752, 'counters/updates': 836}
skipping logging after 26784 examples to avoid logging too frequently
train stats after 26816 examples: {'rewards_train/chosen': '-0.012322', 'rewards_train/rejected': '-0.024901', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01258', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-171.01', 'loss/train': '0.69058', 'examples_per_second': '31.469', 'grad_norm': '33.25', 'counters/examples': 26816, 'counters/updates': 838}
train stats after 26848 examples: {'rewards_train/chosen': '0.038476', 'rewards_train/rejected': '0.015265', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02321', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-192.4', 'loss/train': '0.68466', 'examples_per_second': '31.5', 'grad_norm': '28.625', 'counters/examples': 26848, 'counters/updates': 839}
train stats after 26880 examples: {'rewards_train/chosen': '0.044988', 'rewards_train/rejected': '0.059499', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014511', 'logps_train/rejected': '-112.07', 'logps_train/chosen': '-165.51', 'loss/train': '0.71116', 'examples_per_second': '31.51', 'grad_norm': '33.5', 'counters/examples': 26880, 'counters/updates': 840}
train stats after 26912 examples: {'rewards_train/chosen': '-0.012309', 'rewards_train/rejected': '0.0024446', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.014754', 'logps_train/rejected': '-161.53', 'logps_train/chosen': '-160.02', 'loss/train': '0.70577', 'examples_per_second': '32.335', 'grad_norm': '39', 'counters/examples': 26912, 'counters/updates': 841}
train stats after 26944 examples: {'rewards_train/chosen': '0.024093', 'rewards_train/rejected': '0.1221', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.098005', 'logps_train/rejected': '-175.85', 'logps_train/chosen': '-149.9', 'loss/train': '0.75849', 'examples_per_second': '32.999', 'grad_norm': '61', 'counters/examples': 26944, 'counters/updates': 842}
skipping logging after 26976 examples to avoid logging too frequently
train stats after 27008 examples: {'rewards_train/chosen': '0.019037', 'rewards_train/rejected': '-0.0027072', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.021745', 'logps_train/rejected': '-153.06', 'logps_train/chosen': '-139.7', 'loss/train': '0.69219', 'examples_per_second': '33.178', 'grad_norm': '27.875', 'counters/examples': 27008, 'counters/updates': 844}
train stats after 27040 examples: {'rewards_train/chosen': '0.0084724', 'rewards_train/rejected': '0.003927', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0045454', 'logps_train/rejected': '-109.43', 'logps_train/chosen': '-131.14', 'loss/train': '0.69362', 'examples_per_second': '30.977', 'grad_norm': '29.5', 'counters/examples': 27040, 'counters/updates': 845}
train stats after 27072 examples: {'rewards_train/chosen': '0.037715', 'rewards_train/rejected': '0.025804', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011911', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-154.98', 'loss/train': '0.69304', 'examples_per_second': '31.147', 'grad_norm': '43.5', 'counters/examples': 27072, 'counters/updates': 846}
train stats after 27104 examples: {'rewards_train/chosen': '0.03292', 'rewards_train/rejected': '0.058193', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025272', 'logps_train/rejected': '-145.15', 'logps_train/chosen': '-159.76', 'loss/train': '0.70966', 'examples_per_second': '31.488', 'grad_norm': '38', 'counters/examples': 27104, 'counters/updates': 847}
train stats after 27136 examples: {'rewards_train/chosen': '0.021306', 'rewards_train/rejected': '-0.014426', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.035731', 'logps_train/rejected': '-103.39', 'logps_train/chosen': '-121.87', 'loss/train': '0.6779', 'examples_per_second': '31.182', 'grad_norm': '30.875', 'counters/examples': 27136, 'counters/updates': 848}
skipping logging after 27168 examples to avoid logging too frequently
train stats after 27200 examples: {'rewards_train/chosen': '0.025755', 'rewards_train/rejected': '0.0058817', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019874', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-113.09', 'loss/train': '0.68596', 'examples_per_second': '30', 'grad_norm': '27.75', 'counters/examples': 27200, 'counters/updates': 850}
train stats after 27232 examples: {'rewards_train/chosen': '0.021503', 'rewards_train/rejected': '0.013586', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0079172', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-165.18', 'loss/train': '0.6959', 'examples_per_second': '31.519', 'grad_norm': '30.75', 'counters/examples': 27232, 'counters/updates': 851}
skipping logging after 27264 examples to avoid logging too frequently
train stats after 27296 examples: {'rewards_train/chosen': '0.04866', 'rewards_train/rejected': '0.023474', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025186', 'logps_train/rejected': '-134.3', 'logps_train/chosen': '-105.74', 'loss/train': '0.69029', 'examples_per_second': '34.394', 'grad_norm': '29.75', 'counters/examples': 27296, 'counters/updates': 853}
train stats after 27328 examples: {'rewards_train/chosen': '-0.0082744', 'rewards_train/rejected': '-0.039844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031569', 'logps_train/rejected': '-175.76', 'logps_train/chosen': '-157.86', 'loss/train': '0.68396', 'examples_per_second': '31.525', 'grad_norm': '40', 'counters/examples': 27328, 'counters/updates': 854}
train stats after 27360 examples: {'rewards_train/chosen': '0.0438', 'rewards_train/rejected': '0.016813', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026988', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-135.8', 'loss/train': '0.6857', 'examples_per_second': '32.227', 'grad_norm': '33.25', 'counters/examples': 27360, 'counters/updates': 855}
train stats after 27392 examples: {'rewards_train/chosen': '0.068145', 'rewards_train/rejected': '0.038179', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029966', 'logps_train/rejected': '-111.44', 'logps_train/chosen': '-149.01', 'loss/train': '0.6832', 'examples_per_second': '31.487', 'grad_norm': '32.25', 'counters/examples': 27392, 'counters/updates': 856}
train stats after 27424 examples: {'rewards_train/chosen': '0.069278', 'rewards_train/rejected': '0.047884', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021395', 'logps_train/rejected': '-162.29', 'logps_train/chosen': '-178.93', 'loss/train': '0.68777', 'examples_per_second': '30.73', 'grad_norm': '43', 'counters/examples': 27424, 'counters/updates': 857}
train stats after 27456 examples: {'rewards_train/chosen': '0.026034', 'rewards_train/rejected': '0.030011', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0039775', 'logps_train/rejected': '-156.44', 'logps_train/chosen': '-191.43', 'loss/train': '0.71569', 'examples_per_second': '31.395', 'grad_norm': '40', 'counters/examples': 27456, 'counters/updates': 858}
train stats after 27488 examples: {'rewards_train/chosen': '0.0631', 'rewards_train/rejected': '0.037419', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.025681', 'logps_train/rejected': '-116.54', 'logps_train/chosen': '-127.78', 'loss/train': '0.6851', 'examples_per_second': '32.266', 'grad_norm': '32.75', 'counters/examples': 27488, 'counters/updates': 859}
skipping logging after 27520 examples to avoid logging too frequently
train stats after 27552 examples: {'rewards_train/chosen': '-0.034665', 'rewards_train/rejected': '0.0015713', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036237', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-160.64', 'loss/train': '0.72549', 'examples_per_second': '30.646', 'grad_norm': '78', 'counters/examples': 27552, 'counters/updates': 861}
train stats after 27584 examples: {'rewards_train/chosen': '0.049633', 'rewards_train/rejected': '0.029666', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.019967', 'logps_train/rejected': '-127.29', 'logps_train/chosen': '-133.38', 'loss/train': '0.68693', 'examples_per_second': '30.174', 'grad_norm': '24.625', 'counters/examples': 27584, 'counters/updates': 862}
train stats after 27616 examples: {'rewards_train/chosen': '0.045507', 'rewards_train/rejected': '0.010259', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035248', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-158.18', 'loss/train': '0.68081', 'examples_per_second': '31.135', 'grad_norm': '41.25', 'counters/examples': 27616, 'counters/updates': 863}
skipping logging after 27648 examples to avoid logging too frequently
train stats after 27680 examples: {'rewards_train/chosen': '0.07073', 'rewards_train/rejected': '-0.011567', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082297', 'logps_train/rejected': '-96.734', 'logps_train/chosen': '-128.22', 'loss/train': '0.65587', 'examples_per_second': '31.433', 'grad_norm': '37.25', 'counters/examples': 27680, 'counters/updates': 865}
train stats after 27712 examples: {'rewards_train/chosen': '0.05624', 'rewards_train/rejected': '0.0085427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047697', 'logps_train/rejected': '-155.74', 'logps_train/chosen': '-151.57', 'loss/train': '0.67625', 'examples_per_second': '30.357', 'grad_norm': '30.875', 'counters/examples': 27712, 'counters/updates': 866}
skipping logging after 27744 examples to avoid logging too frequently
train stats after 27776 examples: {'rewards_train/chosen': '0.060166', 'rewards_train/rejected': '0.051601', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0085652', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-146.54', 'loss/train': '0.69236', 'examples_per_second': '36.58', 'grad_norm': '29.875', 'counters/examples': 27776, 'counters/updates': 868}
train stats after 27808 examples: {'rewards_train/chosen': '-0.0040816', 'rewards_train/rejected': '-0.0071576', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.003076', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-135.09', 'loss/train': '0.6982', 'examples_per_second': '31.47', 'grad_norm': '37.25', 'counters/examples': 27808, 'counters/updates': 869}
train stats after 27840 examples: {'rewards_train/chosen': '0.026683', 'rewards_train/rejected': '0.016036', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010647', 'logps_train/rejected': '-121.5', 'logps_train/chosen': '-142.6', 'loss/train': '0.69573', 'examples_per_second': '31.533', 'grad_norm': '35.5', 'counters/examples': 27840, 'counters/updates': 870}
train stats after 27872 examples: {'rewards_train/chosen': '0.053641', 'rewards_train/rejected': '-0.010135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063776', 'logps_train/rejected': '-83.863', 'logps_train/chosen': '-137.33', 'loss/train': '0.66979', 'examples_per_second': '31.257', 'grad_norm': '31.375', 'counters/examples': 27872, 'counters/updates': 871}
train stats after 27904 examples: {'rewards_train/chosen': '0.05528', 'rewards_train/rejected': '-0.068508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12379', 'logps_train/rejected': '-154.09', 'logps_train/chosen': '-155.06', 'loss/train': '0.64627', 'examples_per_second': '31.478', 'grad_norm': '35.5', 'counters/examples': 27904, 'counters/updates': 872}
train stats after 27936 examples: {'rewards_train/chosen': '0.019866', 'rewards_train/rejected': '0.032719', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012854', 'logps_train/rejected': '-96.783', 'logps_train/chosen': '-117.05', 'loss/train': '0.70193', 'examples_per_second': '31.537', 'grad_norm': '25.125', 'counters/examples': 27936, 'counters/updates': 873}
train stats after 27968 examples: {'rewards_train/chosen': '-0.02332', 'rewards_train/rejected': '0.0249', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.04822', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-138.46', 'loss/train': '0.7259', 'examples_per_second': '30.434', 'grad_norm': '33', 'counters/examples': 27968, 'counters/updates': 874}
train stats after 28000 examples: {'rewards_train/chosen': '0.060758', 'rewards_train/rejected': '0.023794', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036963', 'logps_train/rejected': '-184.4', 'logps_train/chosen': '-188.3', 'loss/train': '0.68875', 'examples_per_second': '31.534', 'grad_norm': '62.5', 'counters/examples': 28000, 'counters/updates': 875}
train stats after 28032 examples: {'rewards_train/chosen': '0.095678', 'rewards_train/rejected': '0.085876', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0098021', 'logps_train/rejected': '-188.85', 'logps_train/chosen': '-145.76', 'loss/train': '0.69211', 'examples_per_second': '30.718', 'grad_norm': '40.75', 'counters/examples': 28032, 'counters/updates': 876}
train stats after 28064 examples: {'rewards_train/chosen': '0.084262', 'rewards_train/rejected': '0.015441', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06882', 'logps_train/rejected': '-109.24', 'logps_train/chosen': '-186.07', 'loss/train': '0.66648', 'examples_per_second': '31.388', 'grad_norm': '39', 'counters/examples': 28064, 'counters/updates': 877}
train stats after 28096 examples: {'rewards_train/chosen': '0.053205', 'rewards_train/rejected': '-0.00039095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053595', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-150.25', 'loss/train': '0.68719', 'examples_per_second': '31.533', 'grad_norm': '49.5', 'counters/examples': 28096, 'counters/updates': 878}
skipping logging after 28128 examples to avoid logging too frequently
train stats after 28160 examples: {'rewards_train/chosen': '0.0053689', 'rewards_train/rejected': '0.077693', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.072324', 'logps_train/rejected': '-134.25', 'logps_train/chosen': '-138.1', 'loss/train': '0.73417', 'examples_per_second': '30.021', 'grad_norm': '30.875', 'counters/examples': 28160, 'counters/updates': 880}
train stats after 28192 examples: {'rewards_train/chosen': '0.03559', 'rewards_train/rejected': '-0.053065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088655', 'logps_train/rejected': '-104.88', 'logps_train/chosen': '-149.88', 'loss/train': '0.65382', 'examples_per_second': '30.024', 'grad_norm': '30.375', 'counters/examples': 28192, 'counters/updates': 881}
skipping logging after 28224 examples to avoid logging too frequently
train stats after 28256 examples: {'rewards_train/chosen': '-0.018256', 'rewards_train/rejected': '0.019602', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037858', 'logps_train/rejected': '-149.05', 'logps_train/chosen': '-137.61', 'loss/train': '0.71612', 'examples_per_second': '31.441', 'grad_norm': '28.5', 'counters/examples': 28256, 'counters/updates': 883}
train stats after 28288 examples: {'rewards_train/chosen': '0.074286', 'rewards_train/rejected': '-0.0085483', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082834', 'logps_train/rejected': '-106.12', 'logps_train/chosen': '-127.26', 'loss/train': '0.65673', 'examples_per_second': '31.45', 'grad_norm': '28.625', 'counters/examples': 28288, 'counters/updates': 884}
train stats after 28320 examples: {'rewards_train/chosen': '0.033984', 'rewards_train/rejected': '0.022354', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011629', 'logps_train/rejected': '-155.88', 'logps_train/chosen': '-144.69', 'loss/train': '0.69161', 'examples_per_second': '32.602', 'grad_norm': '45', 'counters/examples': 28320, 'counters/updates': 885}
train stats after 28352 examples: {'rewards_train/chosen': '0.034361', 'rewards_train/rejected': '0.052383', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018022', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-121.53', 'loss/train': '0.7101', 'examples_per_second': '32.758', 'grad_norm': '45.25', 'counters/examples': 28352, 'counters/updates': 886}
train stats after 28384 examples: {'rewards_train/chosen': '0.018459', 'rewards_train/rejected': '-0.0037677', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.022227', 'logps_train/rejected': '-140.31', 'logps_train/chosen': '-172.79', 'loss/train': '0.70367', 'examples_per_second': '31.249', 'grad_norm': '32.5', 'counters/examples': 28384, 'counters/updates': 887}
train stats after 28416 examples: {'rewards_train/chosen': '0.096683', 'rewards_train/rejected': '0.05463', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042054', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-151.5', 'loss/train': '0.68082', 'examples_per_second': '30.945', 'grad_norm': '38.75', 'counters/examples': 28416, 'counters/updates': 888}
train stats after 28448 examples: {'rewards_train/chosen': '0.048438', 'rewards_train/rejected': '-0.023488', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071926', 'logps_train/rejected': '-144.84', 'logps_train/chosen': '-172.48', 'loss/train': '0.66913', 'examples_per_second': '31.513', 'grad_norm': '35.5', 'counters/examples': 28448, 'counters/updates': 889}
train stats after 28480 examples: {'rewards_train/chosen': '0.061589', 'rewards_train/rejected': '0.070406', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0088173', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-122.01', 'loss/train': '0.70603', 'examples_per_second': '30.536', 'grad_norm': '31.875', 'counters/examples': 28480, 'counters/updates': 890}
train stats after 28512 examples: {'rewards_train/chosen': '0.065673', 'rewards_train/rejected': '0.0036556', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062017', 'logps_train/rejected': '-108.57', 'logps_train/chosen': '-128.24', 'loss/train': '0.67243', 'examples_per_second': '31.55', 'grad_norm': '36', 'counters/examples': 28512, 'counters/updates': 891}
train stats after 28544 examples: {'rewards_train/chosen': '0.061346', 'rewards_train/rejected': '-0.024589', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085935', 'logps_train/rejected': '-131.94', 'logps_train/chosen': '-162.31', 'loss/train': '0.65871', 'examples_per_second': '30.018', 'grad_norm': '27', 'counters/examples': 28544, 'counters/updates': 892}
skipping logging after 28576 examples to avoid logging too frequently
train stats after 28608 examples: {'rewards_train/chosen': '0.064017', 'rewards_train/rejected': '0.056059', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0079581', 'logps_train/rejected': '-128.8', 'logps_train/chosen': '-168.73', 'loss/train': '0.6959', 'examples_per_second': '32.26', 'grad_norm': '34.25', 'counters/examples': 28608, 'counters/updates': 894}
train stats after 28640 examples: {'rewards_train/chosen': '0.059615', 'rewards_train/rejected': '0.0086886', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.050926', 'logps_train/rejected': '-139.07', 'logps_train/chosen': '-146.7', 'loss/train': '0.67279', 'examples_per_second': '31.983', 'grad_norm': '34', 'counters/examples': 28640, 'counters/updates': 895}
skipping logging after 28672 examples to avoid logging too frequently
train stats after 28704 examples: {'rewards_train/chosen': '0.18363', 'rewards_train/rejected': '0.051096', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13254', 'logps_train/rejected': '-106.02', 'logps_train/chosen': '-169.07', 'loss/train': '0.65168', 'examples_per_second': '31.528', 'grad_norm': '30.25', 'counters/examples': 28704, 'counters/updates': 897}
train stats after 28736 examples: {'rewards_train/chosen': '0.022394', 'rewards_train/rejected': '0.050074', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027681', 'logps_train/rejected': '-137.01', 'logps_train/chosen': '-148.98', 'loss/train': '0.7223', 'examples_per_second': '30.49', 'grad_norm': '40.5', 'counters/examples': 28736, 'counters/updates': 898}
train stats after 28768 examples: {'rewards_train/chosen': '0.074572', 'rewards_train/rejected': '0.030748', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.043824', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-181.11', 'loss/train': '0.68377', 'examples_per_second': '30.064', 'grad_norm': '41.75', 'counters/examples': 28768, 'counters/updates': 899}
train stats after 28800 examples: {'rewards_train/chosen': '0.069636', 'rewards_train/rejected': '0.085705', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016069', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-133.69', 'loss/train': '0.70371', 'examples_per_second': '31.506', 'grad_norm': '46.25', 'counters/examples': 28800, 'counters/updates': 900}
train stats after 28832 examples: {'rewards_train/chosen': '0.089904', 'rewards_train/rejected': '0.019655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070249', 'logps_train/rejected': '-97.167', 'logps_train/chosen': '-127.57', 'loss/train': '0.66137', 'examples_per_second': '30.715', 'grad_norm': '28.5', 'counters/examples': 28832, 'counters/updates': 901}
train stats after 28864 examples: {'rewards_train/chosen': '0.067676', 'rewards_train/rejected': '0.041522', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026154', 'logps_train/rejected': '-118.06', 'logps_train/chosen': '-126.63', 'loss/train': '0.68417', 'examples_per_second': '32.571', 'grad_norm': '32', 'counters/examples': 28864, 'counters/updates': 902}
train stats after 28896 examples: {'rewards_train/chosen': '0.047489', 'rewards_train/rejected': '0.054743', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0072542', 'logps_train/rejected': '-96.821', 'logps_train/chosen': '-134.64', 'loss/train': '0.7103', 'examples_per_second': '30.644', 'grad_norm': '35.25', 'counters/examples': 28896, 'counters/updates': 903}
train stats after 28928 examples: {'rewards_train/chosen': '0.09797', 'rewards_train/rejected': '0.068654', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029316', 'logps_train/rejected': '-129.33', 'logps_train/chosen': '-135.96', 'loss/train': '0.69276', 'examples_per_second': '30.081', 'grad_norm': '74', 'counters/examples': 28928, 'counters/updates': 904}
skipping logging after 28960 examples to avoid logging too frequently
train stats after 28992 examples: {'rewards_train/chosen': '0.048843', 'rewards_train/rejected': '0.059452', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010609', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-116.95', 'loss/train': '0.70737', 'examples_per_second': '32.235', 'grad_norm': '46.5', 'counters/examples': 28992, 'counters/updates': 906}
train stats after 29024 examples: {'rewards_train/chosen': '0.023994', 'rewards_train/rejected': '-0.0020073', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026001', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-158.81', 'loss/train': '0.68711', 'examples_per_second': '30.5', 'grad_norm': '31.125', 'counters/examples': 29024, 'counters/updates': 907}
train stats after 29056 examples: {'rewards_train/chosen': '0.041119', 'rewards_train/rejected': '-0.041408', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082528', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-150.11', 'loss/train': '0.65767', 'examples_per_second': '31.18', 'grad_norm': '33.75', 'counters/examples': 29056, 'counters/updates': 908}
train stats after 29088 examples: {'rewards_train/chosen': '0.09046', 'rewards_train/rejected': '0.033522', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056938', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-154.58', 'loss/train': '0.668', 'examples_per_second': '31.234', 'grad_norm': '36.75', 'counters/examples': 29088, 'counters/updates': 909}
train stats after 29120 examples: {'rewards_train/chosen': '0.053578', 'rewards_train/rejected': '0.063544', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.009966', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-164.86', 'loss/train': '0.70597', 'examples_per_second': '30.019', 'grad_norm': '33.25', 'counters/examples': 29120, 'counters/updates': 910}
train stats after 29152 examples: {'rewards_train/chosen': '0.027887', 'rewards_train/rejected': '0.0088973', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.01899', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-151.27', 'loss/train': '0.6906', 'examples_per_second': '31.497', 'grad_norm': '36.75', 'counters/examples': 29152, 'counters/updates': 911}
skipping logging after 29184 examples to avoid logging too frequently
train stats after 29216 examples: {'rewards_train/chosen': '0.096453', 'rewards_train/rejected': '-0.0033327', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099786', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-146.74', 'loss/train': '0.65246', 'examples_per_second': '30.138', 'grad_norm': '34', 'counters/examples': 29216, 'counters/updates': 913}
train stats after 29248 examples: {'rewards_train/chosen': '0.18156', 'rewards_train/rejected': '0.064232', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11733', 'logps_train/rejected': '-122.22', 'logps_train/chosen': '-164.51', 'loss/train': '0.66529', 'examples_per_second': '31.5', 'grad_norm': '34', 'counters/examples': 29248, 'counters/updates': 914}
train stats after 29280 examples: {'rewards_train/chosen': '0.039669', 'rewards_train/rejected': '0.064293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.024624', 'logps_train/rejected': '-117.74', 'logps_train/chosen': '-145.58', 'loss/train': '0.71164', 'examples_per_second': '31.291', 'grad_norm': '30.75', 'counters/examples': 29280, 'counters/updates': 915}
train stats after 29312 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '0.075188', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.046004', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-148.42', 'loss/train': '0.68889', 'examples_per_second': '30.335', 'grad_norm': '32', 'counters/examples': 29312, 'counters/updates': 916}
train stats after 29344 examples: {'rewards_train/chosen': '0.0066323', 'rewards_train/rejected': '0.058601', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051969', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-180.38', 'loss/train': '0.72559', 'examples_per_second': '30.396', 'grad_norm': '32.5', 'counters/examples': 29344, 'counters/updates': 917}
train stats after 29376 examples: {'rewards_train/chosen': '0.0088633', 'rewards_train/rejected': '0.022973', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01411', 'logps_train/rejected': '-115.54', 'logps_train/chosen': '-140.77', 'loss/train': '0.7065', 'examples_per_second': '30.208', 'grad_norm': '29.75', 'counters/examples': 29376, 'counters/updates': 918}
train stats after 29408 examples: {'rewards_train/chosen': '0.073476', 'rewards_train/rejected': '0.056363', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017113', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-149.62', 'loss/train': '0.68959', 'examples_per_second': '32.419', 'grad_norm': '29.5', 'counters/examples': 29408, 'counters/updates': 919}
skipping logging after 29440 examples to avoid logging too frequently
train stats after 29472 examples: {'rewards_train/chosen': '0.050009', 'rewards_train/rejected': '-0.014728', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064737', 'logps_train/rejected': '-99.557', 'logps_train/chosen': '-147.3', 'loss/train': '0.66403', 'examples_per_second': '32.301', 'grad_norm': '25.625', 'counters/examples': 29472, 'counters/updates': 921}
train stats after 29504 examples: {'rewards_train/chosen': '0.0097901', 'rewards_train/rejected': '-0.006274', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016064', 'logps_train/rejected': '-90.1', 'logps_train/chosen': '-106.41', 'loss/train': '0.68894', 'examples_per_second': '31.134', 'grad_norm': '24.375', 'counters/examples': 29504, 'counters/updates': 922}
train stats after 29536 examples: {'rewards_train/chosen': '0.051203', 'rewards_train/rejected': '0.0007837', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05042', 'logps_train/rejected': '-140.77', 'logps_train/chosen': '-159.58', 'loss/train': '0.67211', 'examples_per_second': '30.535', 'grad_norm': '31.125', 'counters/examples': 29536, 'counters/updates': 923}
train stats after 29568 examples: {'rewards_train/chosen': '0.033737', 'rewards_train/rejected': '0.087195', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.053458', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-131.42', 'loss/train': '0.73865', 'examples_per_second': '31.181', 'grad_norm': '43.25', 'counters/examples': 29568, 'counters/updates': 924}
skipping logging after 29600 examples to avoid logging too frequently
train stats after 29632 examples: {'rewards_train/chosen': '0.11533', 'rewards_train/rejected': '0.0048511', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11048', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-127.16', 'loss/train': '0.65475', 'examples_per_second': '31.566', 'grad_norm': '32.5', 'counters/examples': 29632, 'counters/updates': 926}
train stats after 29664 examples: {'rewards_train/chosen': '0.075325', 'rewards_train/rejected': '0.049111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026214', 'logps_train/rejected': '-111.34', 'logps_train/chosen': '-134.16', 'loss/train': '0.6876', 'examples_per_second': '31.761', 'grad_norm': '58.25', 'counters/examples': 29664, 'counters/updates': 927}
skipping logging after 29696 examples to avoid logging too frequently
train stats after 29728 examples: {'rewards_train/chosen': '0.056089', 'rewards_train/rejected': '0.07805', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021961', 'logps_train/rejected': '-154.9', 'logps_train/chosen': '-149.47', 'loss/train': '0.71321', 'examples_per_second': '31.228', 'grad_norm': '34', 'counters/examples': 29728, 'counters/updates': 929}
train stats after 29760 examples: {'rewards_train/chosen': '0.017121', 'rewards_train/rejected': '0.028231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01111', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-135', 'loss/train': '0.70695', 'examples_per_second': '30.586', 'grad_norm': '41.25', 'counters/examples': 29760, 'counters/updates': 930}
train stats after 29792 examples: {'rewards_train/chosen': '0.073874', 'rewards_train/rejected': '-0.0011462', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07502', 'logps_train/rejected': '-138.73', 'logps_train/chosen': '-147.18', 'loss/train': '0.66013', 'examples_per_second': '30.327', 'grad_norm': '31.5', 'counters/examples': 29792, 'counters/updates': 931}
skipping logging after 29824 examples to avoid logging too frequently
train stats after 29856 examples: {'rewards_train/chosen': '0.058155', 'rewards_train/rejected': '0.057328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00082726', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-131.15', 'loss/train': '0.69937', 'examples_per_second': '34.56', 'grad_norm': '29.5', 'counters/examples': 29856, 'counters/updates': 933}
skipping logging after 29888 examples to avoid logging too frequently
train stats after 29920 examples: {'rewards_train/chosen': '0.11353', 'rewards_train/rejected': '-0.020173', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1337', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-114.51', 'loss/train': '0.66124', 'examples_per_second': '31.472', 'grad_norm': '31.25', 'counters/examples': 29920, 'counters/updates': 935}
skipping logging after 29952 examples to avoid logging too frequently
train stats after 29984 examples: {'rewards_train/chosen': '0.047086', 'rewards_train/rejected': '0.021059', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026027', 'logps_train/rejected': '-102.48', 'logps_train/chosen': '-114.78', 'loss/train': '0.6855', 'examples_per_second': '35.536', 'grad_norm': '29.5', 'counters/examples': 29984, 'counters/updates': 937}
train stats after 30016 examples: {'rewards_train/chosen': '0.035016', 'rewards_train/rejected': '0.026722', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0082941', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-139.65', 'loss/train': '0.69283', 'examples_per_second': '25.28', 'grad_norm': '34.5', 'counters/examples': 30016, 'counters/updates': 938}
skipping logging after 30048 examples to avoid logging too frequently
train stats after 30080 examples: {'rewards_train/chosen': '0.057919', 'rewards_train/rejected': '0.019737', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038182', 'logps_train/rejected': '-127.78', 'logps_train/chosen': '-148.69', 'loss/train': '0.68005', 'examples_per_second': '30.496', 'grad_norm': '35.5', 'counters/examples': 30080, 'counters/updates': 940}
train stats after 30112 examples: {'rewards_train/chosen': '0.10994', 'rewards_train/rejected': '0.039932', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.070012', 'logps_train/rejected': '-132.5', 'logps_train/chosen': '-139.45', 'loss/train': '0.66571', 'examples_per_second': '24.11', 'grad_norm': '33', 'counters/examples': 30112, 'counters/updates': 941}
train stats after 30144 examples: {'rewards_train/chosen': '-0.01954', 'rewards_train/rejected': '0.0097051', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.029245', 'logps_train/rejected': '-124.72', 'logps_train/chosen': '-135.6', 'loss/train': '0.71036', 'examples_per_second': '32.22', 'grad_norm': '37.75', 'counters/examples': 30144, 'counters/updates': 942}
train stats after 30176 examples: {'rewards_train/chosen': '0.066912', 'rewards_train/rejected': '0.030473', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03644', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-118.19', 'loss/train': '0.67795', 'examples_per_second': '32.636', 'grad_norm': '28.625', 'counters/examples': 30176, 'counters/updates': 943}
skipping logging after 30208 examples to avoid logging too frequently
train stats after 30240 examples: {'rewards_train/chosen': '0.11764', 'rewards_train/rejected': '0.038307', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.079337', 'logps_train/rejected': '-160.65', 'logps_train/chosen': '-152', 'loss/train': '0.65667', 'examples_per_second': '33.15', 'grad_norm': '30.875', 'counters/examples': 30240, 'counters/updates': 945}
train stats after 30272 examples: {'rewards_train/chosen': '0.063484', 'rewards_train/rejected': '0.034883', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0286', 'logps_train/rejected': '-120.53', 'logps_train/chosen': '-144.3', 'loss/train': '0.68275', 'examples_per_second': '31.508', 'grad_norm': '27.5', 'counters/examples': 30272, 'counters/updates': 946}
train stats after 30304 examples: {'rewards_train/chosen': '0.064245', 'rewards_train/rejected': '0.072194', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0079493', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-145.12', 'loss/train': '0.70109', 'examples_per_second': '30.92', 'grad_norm': '42.25', 'counters/examples': 30304, 'counters/updates': 947}
train stats after 30336 examples: {'rewards_train/chosen': '0.16223', 'rewards_train/rejected': '0.10758', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054646', 'logps_train/rejected': '-157.41', 'logps_train/chosen': '-159.88', 'loss/train': '0.68128', 'examples_per_second': '31.503', 'grad_norm': '50', 'counters/examples': 30336, 'counters/updates': 948}
train stats after 30368 examples: {'rewards_train/chosen': '0.022121', 'rewards_train/rejected': '0.045801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.02368', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-145.31', 'loss/train': '0.71853', 'examples_per_second': '32.143', 'grad_norm': '36.25', 'counters/examples': 30368, 'counters/updates': 949}
train stats after 30400 examples: {'rewards_train/chosen': '0.02785', 'rewards_train/rejected': '0.05414', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.02629', 'logps_train/rejected': '-154.48', 'logps_train/chosen': '-146.17', 'loss/train': '0.71042', 'examples_per_second': '31.472', 'grad_norm': '33', 'counters/examples': 30400, 'counters/updates': 950}
skipping logging after 30432 examples to avoid logging too frequently
train stats after 30464 examples: {'rewards_train/chosen': '0.032708', 'rewards_train/rejected': '-0.0051888', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037897', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-144.29', 'loss/train': '0.69014', 'examples_per_second': '30.063', 'grad_norm': '26.125', 'counters/examples': 30464, 'counters/updates': 952}
train stats after 30496 examples: {'rewards_train/chosen': '0.076666', 'rewards_train/rejected': '0.085541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0088749', 'logps_train/rejected': '-149.94', 'logps_train/chosen': '-140.1', 'loss/train': '0.72127', 'examples_per_second': '31.514', 'grad_norm': '37.75', 'counters/examples': 30496, 'counters/updates': 953}
skipping logging after 30528 examples to avoid logging too frequently
train stats after 30560 examples: {'rewards_train/chosen': '0.060537', 'rewards_train/rejected': '0.0069402', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053597', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-123.76', 'loss/train': '0.66964', 'examples_per_second': '30.757', 'grad_norm': '34.5', 'counters/examples': 30560, 'counters/updates': 955}
train stats after 30592 examples: {'rewards_train/chosen': '0.00033139', 'rewards_train/rejected': '0.074549', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.074217', 'logps_train/rejected': '-117.37', 'logps_train/chosen': '-151.6', 'loss/train': '0.73535', 'examples_per_second': '30.677', 'grad_norm': '32.25', 'counters/examples': 30592, 'counters/updates': 956}
train stats after 30624 examples: {'rewards_train/chosen': '0.043093', 'rewards_train/rejected': '-0.0050879', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048181', 'logps_train/rejected': '-155.21', 'logps_train/chosen': '-147.17', 'loss/train': '0.67498', 'examples_per_second': '32.028', 'grad_norm': '30.75', 'counters/examples': 30624, 'counters/updates': 957}
train stats after 30656 examples: {'rewards_train/chosen': '0.031521', 'rewards_train/rejected': '0.048884', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017363', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-141.3', 'loss/train': '0.70516', 'examples_per_second': '30.142', 'grad_norm': '33.5', 'counters/examples': 30656, 'counters/updates': 958}
skipping logging after 30688 examples to avoid logging too frequently
train stats after 30720 examples: {'rewards_train/chosen': '0.073095', 'rewards_train/rejected': '0.024213', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048883', 'logps_train/rejected': '-119.65', 'logps_train/chosen': '-125.32', 'loss/train': '0.67404', 'examples_per_second': '35.859', 'grad_norm': '30.625', 'counters/examples': 30720, 'counters/updates': 960}
train stats after 30752 examples: {'rewards_train/chosen': '0.11443', 'rewards_train/rejected': '0.075978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038448', 'logps_train/rejected': '-105.84', 'logps_train/chosen': '-140', 'loss/train': '0.68893', 'examples_per_second': '25.671', 'grad_norm': '32.25', 'counters/examples': 30752, 'counters/updates': 961}
train stats after 30784 examples: {'rewards_train/chosen': '0.048343', 'rewards_train/rejected': '-0.011186', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059528', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-120.16', 'loss/train': '0.66924', 'examples_per_second': '31.519', 'grad_norm': '28.875', 'counters/examples': 30784, 'counters/updates': 962}
train stats after 30816 examples: {'rewards_train/chosen': '0.091097', 'rewards_train/rejected': '0.06369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027407', 'logps_train/rejected': '-144.5', 'logps_train/chosen': '-171.3', 'loss/train': '0.68861', 'examples_per_second': '31.316', 'grad_norm': '54.5', 'counters/examples': 30816, 'counters/updates': 963}
train stats after 30848 examples: {'rewards_train/chosen': '0.018222', 'rewards_train/rejected': '0.076821', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0586', 'logps_train/rejected': '-146.14', 'logps_train/chosen': '-172.63', 'loss/train': '0.73084', 'examples_per_second': '31.526', 'grad_norm': '41.5', 'counters/examples': 30848, 'counters/updates': 964}
train stats after 30880 examples: {'rewards_train/chosen': '0.0076348', 'rewards_train/rejected': '0.028892', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021257', 'logps_train/rejected': '-100.1', 'logps_train/chosen': '-145.92', 'loss/train': '0.70742', 'examples_per_second': '31.342', 'grad_norm': '32.75', 'counters/examples': 30880, 'counters/updates': 965}
train stats after 30912 examples: {'rewards_train/chosen': '0.02887', 'rewards_train/rejected': '0.030095', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0012245', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-165.97', 'loss/train': '0.69998', 'examples_per_second': '30.153', 'grad_norm': '34.5', 'counters/examples': 30912, 'counters/updates': 966}
train stats after 30944 examples: {'rewards_train/chosen': '0.082501', 'rewards_train/rejected': '0.042038', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040462', 'logps_train/rejected': '-147.17', 'logps_train/chosen': '-132.69', 'loss/train': '0.68596', 'examples_per_second': '31.457', 'grad_norm': '46', 'counters/examples': 30944, 'counters/updates': 967}
skipping logging after 30976 examples to avoid logging too frequently
train stats after 31008 examples: {'rewards_train/chosen': '0.028288', 'rewards_train/rejected': '0.032298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0040093', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-149.77', 'loss/train': '0.69993', 'examples_per_second': '34.28', 'grad_norm': '33.25', 'counters/examples': 31008, 'counters/updates': 969}
train stats after 31040 examples: {'rewards_train/chosen': '0.048007', 'rewards_train/rejected': '0.022474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025533', 'logps_train/rejected': '-120.82', 'logps_train/chosen': '-103.7', 'loss/train': '0.68422', 'examples_per_second': '31.304', 'grad_norm': '26.625', 'counters/examples': 31040, 'counters/updates': 970}
train stats after 31072 examples: {'rewards_train/chosen': '0.0069409', 'rewards_train/rejected': '0.024892', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017951', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-143.27', 'loss/train': '0.70637', 'examples_per_second': '32.85', 'grad_norm': '39.75', 'counters/examples': 31072, 'counters/updates': 971}
train stats after 31104 examples: {'rewards_train/chosen': '0.045027', 'rewards_train/rejected': '0.11708', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.072052', 'logps_train/rejected': '-163.34', 'logps_train/chosen': '-125.13', 'loss/train': '0.73871', 'examples_per_second': '32.76', 'grad_norm': '40.75', 'counters/examples': 31104, 'counters/updates': 972}
train stats after 31136 examples: {'rewards_train/chosen': '0.03336', 'rewards_train/rejected': '0.054675', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.021314', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-150.79', 'loss/train': '0.71039', 'examples_per_second': '30.428', 'grad_norm': '34.25', 'counters/examples': 31136, 'counters/updates': 973}
train stats after 31168 examples: {'rewards_train/chosen': '0.08124', 'rewards_train/rejected': '0.06363', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017609', 'logps_train/rejected': '-173.86', 'logps_train/chosen': '-144', 'loss/train': '0.69112', 'examples_per_second': '31.425', 'grad_norm': '38.75', 'counters/examples': 31168, 'counters/updates': 974}
train stats after 31200 examples: {'rewards_train/chosen': '0.10301', 'rewards_train/rejected': '0.052038', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.050969', 'logps_train/rejected': '-122.35', 'logps_train/chosen': '-162.56', 'loss/train': '0.67095', 'examples_per_second': '32.595', 'grad_norm': '27.75', 'counters/examples': 31200, 'counters/updates': 975}
skipping logging after 31232 examples to avoid logging too frequently
train stats after 31264 examples: {'rewards_train/chosen': '0.1002', 'rewards_train/rejected': '-0.0082304', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10843', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-174.02', 'loss/train': '0.65289', 'examples_per_second': '31.304', 'grad_norm': '35.5', 'counters/examples': 31264, 'counters/updates': 977}
train stats after 31296 examples: {'rewards_train/chosen': '0.047628', 'rewards_train/rejected': '0.022186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025442', 'logps_train/rejected': '-106.2', 'logps_train/chosen': '-133.92', 'loss/train': '0.68477', 'examples_per_second': '30.559', 'grad_norm': '30.5', 'counters/examples': 31296, 'counters/updates': 978}
train stats after 31328 examples: {'rewards_train/chosen': '0.093848', 'rewards_train/rejected': '0.015409', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.078439', 'logps_train/rejected': '-96.115', 'logps_train/chosen': '-153.34', 'loss/train': '0.67473', 'examples_per_second': '30.049', 'grad_norm': '37.5', 'counters/examples': 31328, 'counters/updates': 979}
train stats after 31360 examples: {'rewards_train/chosen': '0.055985', 'rewards_train/rejected': '0.057473', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0014887', 'logps_train/rejected': '-166.2', 'logps_train/chosen': '-165.04', 'loss/train': '0.70352', 'examples_per_second': '31.561', 'grad_norm': '35', 'counters/examples': 31360, 'counters/updates': 980}
train stats after 31392 examples: {'rewards_train/chosen': '0.0713', 'rewards_train/rejected': '0.012673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058627', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-161.73', 'loss/train': '0.66901', 'examples_per_second': '30.701', 'grad_norm': '25.875', 'counters/examples': 31392, 'counters/updates': 981}
train stats after 31424 examples: {'rewards_train/chosen': '0.043069', 'rewards_train/rejected': '-0.01853', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061599', 'logps_train/rejected': '-112.2', 'logps_train/chosen': '-139', 'loss/train': '0.66668', 'examples_per_second': '30.518', 'grad_norm': '26.875', 'counters/examples': 31424, 'counters/updates': 982}
train stats after 31456 examples: {'rewards_train/chosen': '0.067421', 'rewards_train/rejected': '0.042997', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024424', 'logps_train/rejected': '-133.21', 'logps_train/chosen': '-169.24', 'loss/train': '0.69318', 'examples_per_second': '31.517', 'grad_norm': '34.25', 'counters/examples': 31456, 'counters/updates': 983}
train stats after 31488 examples: {'rewards_train/chosen': '0.046222', 'rewards_train/rejected': '0.0017182', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044504', 'logps_train/rejected': '-138.98', 'logps_train/chosen': '-157.41', 'loss/train': '0.68088', 'examples_per_second': '31.395', 'grad_norm': '41.5', 'counters/examples': 31488, 'counters/updates': 984}
train stats after 31520 examples: {'rewards_train/chosen': '0.0914', 'rewards_train/rejected': '0.10525', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01385', 'logps_train/rejected': '-152.86', 'logps_train/chosen': '-183.04', 'loss/train': '0.71934', 'examples_per_second': '33.142', 'grad_norm': '33.5', 'counters/examples': 31520, 'counters/updates': 985}
train stats after 31552 examples: {'rewards_train/chosen': '0.064268', 'rewards_train/rejected': '0.0711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0068328', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-152.56', 'loss/train': '0.70377', 'examples_per_second': '31.495', 'grad_norm': '40.5', 'counters/examples': 31552, 'counters/updates': 986}
train stats after 31584 examples: {'rewards_train/chosen': '0.11084', 'rewards_train/rejected': '0.04824', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062597', 'logps_train/rejected': '-148.56', 'logps_train/chosen': '-147.64', 'loss/train': '0.6801', 'examples_per_second': '31.503', 'grad_norm': '42.5', 'counters/examples': 31584, 'counters/updates': 987}
train stats after 31616 examples: {'rewards_train/chosen': '-0.01628', 'rewards_train/rejected': '-0.0018269', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.014453', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-161.24', 'loss/train': '0.70501', 'examples_per_second': '30.702', 'grad_norm': '31', 'counters/examples': 31616, 'counters/updates': 988}
train stats after 31648 examples: {'rewards_train/chosen': '0.089172', 'rewards_train/rejected': '-0.034395', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12357', 'logps_train/rejected': '-142.36', 'logps_train/chosen': '-145.17', 'loss/train': '0.63924', 'examples_per_second': '30.967', 'grad_norm': '32.25', 'counters/examples': 31648, 'counters/updates': 989}
train stats after 31680 examples: {'rewards_train/chosen': '0.079384', 'rewards_train/rejected': '-0.013336', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092719', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-164.87', 'loss/train': '0.65283', 'examples_per_second': '31.557', 'grad_norm': '27.375', 'counters/examples': 31680, 'counters/updates': 990}
train stats after 31712 examples: {'rewards_train/chosen': '0.096645', 'rewards_train/rejected': '0.077073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019572', 'logps_train/rejected': '-112.91', 'logps_train/chosen': '-154.65', 'loss/train': '0.70002', 'examples_per_second': '31.065', 'grad_norm': '38.5', 'counters/examples': 31712, 'counters/updates': 991}
train stats after 31744 examples: {'rewards_train/chosen': '0.064152', 'rewards_train/rejected': '0.13319', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069038', 'logps_train/rejected': '-168.08', 'logps_train/chosen': '-174.53', 'loss/train': '0.745', 'examples_per_second': '30.968', 'grad_norm': '145', 'counters/examples': 31744, 'counters/updates': 992}
skipping logging after 31776 examples to avoid logging too frequently
train stats after 31808 examples: {'rewards_train/chosen': '0.11651', 'rewards_train/rejected': '0.039683', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076827', 'logps_train/rejected': '-138.7', 'logps_train/chosen': '-181.49', 'loss/train': '0.65878', 'examples_per_second': '31.373', 'grad_norm': '33.75', 'counters/examples': 31808, 'counters/updates': 994}
train stats after 31840 examples: {'rewards_train/chosen': '0.03977', 'rewards_train/rejected': '0.0086476', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031123', 'logps_train/rejected': '-93.687', 'logps_train/chosen': '-106.31', 'loss/train': '0.68191', 'examples_per_second': '31.62', 'grad_norm': '27', 'counters/examples': 31840, 'counters/updates': 995}
train stats after 31872 examples: {'rewards_train/chosen': '0.082615', 'rewards_train/rejected': '0.018206', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064408', 'logps_train/rejected': '-116.37', 'logps_train/chosen': '-141.05', 'loss/train': '0.66684', 'examples_per_second': '32.013', 'grad_norm': '30.375', 'counters/examples': 31872, 'counters/updates': 996}
train stats after 31904 examples: {'rewards_train/chosen': '0.094047', 'rewards_train/rejected': '0.02807', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065976', 'logps_train/rejected': '-165.45', 'logps_train/chosen': '-126.75', 'loss/train': '0.66729', 'examples_per_second': '30.53', 'grad_norm': '29.625', 'counters/examples': 31904, 'counters/updates': 997}
skipping logging after 31936 examples to avoid logging too frequently
train stats after 31968 examples: {'rewards_train/chosen': '0.08927', 'rewards_train/rejected': '0.034602', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054669', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-154.43', 'loss/train': '0.67201', 'examples_per_second': '31.985', 'grad_norm': '31.25', 'counters/examples': 31968, 'counters/updates': 999}
train stats after 32000 examples: {'rewards_train/chosen': '0.035026', 'rewards_train/rejected': '0.032092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0029338', 'logps_train/rejected': '-126.03', 'logps_train/chosen': '-152.4', 'loss/train': '0.69699', 'examples_per_second': '31.149', 'grad_norm': '35.5', 'counters/examples': 32000, 'counters/updates': 1000}
train stats after 32032 examples: {'rewards_train/chosen': '0.090225', 'rewards_train/rejected': '-0.0064657', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096691', 'logps_train/rejected': '-129.82', 'logps_train/chosen': '-164.31', 'loss/train': '0.65593', 'examples_per_second': '31.465', 'grad_norm': '26.875', 'counters/examples': 32032, 'counters/updates': 1001}
train stats after 32064 examples: {'rewards_train/chosen': '0.011419', 'rewards_train/rejected': '0.048402', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036983', 'logps_train/rejected': '-135.48', 'logps_train/chosen': '-138.03', 'loss/train': '0.71441', 'examples_per_second': '31.456', 'grad_norm': '41.5', 'counters/examples': 32064, 'counters/updates': 1002}
train stats after 32096 examples: {'rewards_train/chosen': '0.02973', 'rewards_train/rejected': '0.027071', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0026591', 'logps_train/rejected': '-129.49', 'logps_train/chosen': '-104.59', 'loss/train': '0.69378', 'examples_per_second': '31.55', 'grad_norm': '31.5', 'counters/examples': 32096, 'counters/updates': 1003}
train stats after 32128 examples: {'rewards_train/chosen': '0.068983', 'rewards_train/rejected': '0.042142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026841', 'logps_train/rejected': '-105.01', 'logps_train/chosen': '-123.69', 'loss/train': '0.68983', 'examples_per_second': '31.25', 'grad_norm': '32.75', 'counters/examples': 32128, 'counters/updates': 1004}
skipping logging after 32160 examples to avoid logging too frequently
train stats after 32192 examples: {'rewards_train/chosen': '0.034308', 'rewards_train/rejected': '0.056327', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022019', 'logps_train/rejected': '-153.78', 'logps_train/chosen': '-160.13', 'loss/train': '0.70846', 'examples_per_second': '30.073', 'grad_norm': '33', 'counters/examples': 32192, 'counters/updates': 1006}
train stats after 32224 examples: {'rewards_train/chosen': '0.087837', 'rewards_train/rejected': '0.059154', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028684', 'logps_train/rejected': '-116.47', 'logps_train/chosen': '-120.61', 'loss/train': '0.6861', 'examples_per_second': '30.486', 'grad_norm': '25.875', 'counters/examples': 32224, 'counters/updates': 1007}
train stats after 32256 examples: {'rewards_train/chosen': '0.078814', 'rewards_train/rejected': '0.021027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057787', 'logps_train/rejected': '-103.94', 'logps_train/chosen': '-127.21', 'loss/train': '0.66862', 'examples_per_second': '30.532', 'grad_norm': '26.875', 'counters/examples': 32256, 'counters/updates': 1008}
skipping logging after 32288 examples to avoid logging too frequently
train stats after 32320 examples: {'rewards_train/chosen': '0.015684', 'rewards_train/rejected': '0.012326', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0033579', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-151.73', 'loss/train': '0.69548', 'examples_per_second': '32.668', 'grad_norm': '39.5', 'counters/examples': 32320, 'counters/updates': 1010}
train stats after 32352 examples: {'rewards_train/chosen': '0.060857', 'rewards_train/rejected': '0.046674', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014183', 'logps_train/rejected': '-132.44', 'logps_train/chosen': '-138.29', 'loss/train': '0.6944', 'examples_per_second': '30.973', 'grad_norm': '31.5', 'counters/examples': 32352, 'counters/updates': 1011}
train stats after 32384 examples: {'rewards_train/chosen': '0.032842', 'rewards_train/rejected': '-0.0062237', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039066', 'logps_train/rejected': '-94.651', 'logps_train/chosen': '-156.31', 'loss/train': '0.67629', 'examples_per_second': '33.02', 'grad_norm': '33.75', 'counters/examples': 32384, 'counters/updates': 1012}
train stats after 32416 examples: {'rewards_train/chosen': '0.034671', 'rewards_train/rejected': '0.12389', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.089223', 'logps_train/rejected': '-127.63', 'logps_train/chosen': '-136.25', 'loss/train': '0.74435', 'examples_per_second': '31.525', 'grad_norm': '33', 'counters/examples': 32416, 'counters/updates': 1013}
train stats after 32448 examples: {'rewards_train/chosen': '0.10772', 'rewards_train/rejected': '0.010043', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097672', 'logps_train/rejected': '-148.24', 'logps_train/chosen': '-169.03', 'loss/train': '0.65545', 'examples_per_second': '31.503', 'grad_norm': '30.625', 'counters/examples': 32448, 'counters/updates': 1014}
train stats after 32480 examples: {'rewards_train/chosen': '0.033021', 'rewards_train/rejected': '0.028497', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0045241', 'logps_train/rejected': '-112.43', 'logps_train/chosen': '-111.94', 'loss/train': '0.69648', 'examples_per_second': '31.467', 'grad_norm': '33', 'counters/examples': 32480, 'counters/updates': 1015}
skipping logging after 32512 examples to avoid logging too frequently
train stats after 32544 examples: {'rewards_train/chosen': '0.1841', 'rewards_train/rejected': '-0.0077644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19186', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-143.21', 'loss/train': '0.6469', 'examples_per_second': '33.491', 'grad_norm': '28.875', 'counters/examples': 32544, 'counters/updates': 1017}
train stats after 32576 examples: {'rewards_train/chosen': '0.0010122', 'rewards_train/rejected': '0.03314', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.032128', 'logps_train/rejected': '-117.05', 'logps_train/chosen': '-136.62', 'loss/train': '0.71206', 'examples_per_second': '31.534', 'grad_norm': '30.375', 'counters/examples': 32576, 'counters/updates': 1018}
train stats after 32608 examples: {'rewards_train/chosen': '0.094577', 'rewards_train/rejected': '0.025024', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069554', 'logps_train/rejected': '-105.84', 'logps_train/chosen': '-129.82', 'loss/train': '0.66266', 'examples_per_second': '32.536', 'grad_norm': '25.125', 'counters/examples': 32608, 'counters/updates': 1019}
skipping logging after 32640 examples to avoid logging too frequently
train stats after 32672 examples: {'rewards_train/chosen': '0.12435', 'rewards_train/rejected': '0.078044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04631', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-162.8', 'loss/train': '0.68413', 'examples_per_second': '31.465', 'grad_norm': '35', 'counters/examples': 32672, 'counters/updates': 1021}
train stats after 32704 examples: {'rewards_train/chosen': '0.051161', 'rewards_train/rejected': '0.061375', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010214', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-171.65', 'loss/train': '0.70314', 'examples_per_second': '33.141', 'grad_norm': '38', 'counters/examples': 32704, 'counters/updates': 1022}
train stats after 32736 examples: {'rewards_train/chosen': '0.077849', 'rewards_train/rejected': '-0.024886', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10273', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-166.77', 'loss/train': '0.64882', 'examples_per_second': '29.872', 'grad_norm': '32', 'counters/examples': 32736, 'counters/updates': 1023}
train stats after 32768 examples: {'rewards_train/chosen': '0.082333', 'rewards_train/rejected': '0.047155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035178', 'logps_train/rejected': '-170.77', 'logps_train/chosen': '-164.21', 'loss/train': '0.68106', 'examples_per_second': '31.546', 'grad_norm': '36.75', 'counters/examples': 32768, 'counters/updates': 1024}
skipping logging after 32800 examples to avoid logging too frequently
train stats after 32832 examples: {'rewards_train/chosen': '0.058532', 'rewards_train/rejected': '0.0043652', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054167', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-124.69', 'loss/train': '0.67865', 'examples_per_second': '31.486', 'grad_norm': '26.875', 'counters/examples': 32832, 'counters/updates': 1026}
train stats after 32864 examples: {'rewards_train/chosen': '-0.0027177', 'rewards_train/rejected': '0.024243', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.02696', 'logps_train/rejected': '-137.07', 'logps_train/chosen': '-154.36', 'loss/train': '0.71325', 'examples_per_second': '31.087', 'grad_norm': '38.25', 'counters/examples': 32864, 'counters/updates': 1027}
train stats after 32896 examples: {'rewards_train/chosen': '0.10198', 'rewards_train/rejected': '0.075146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026833', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-137.17', 'loss/train': '0.68466', 'examples_per_second': '31.527', 'grad_norm': '38.75', 'counters/examples': 32896, 'counters/updates': 1028}
skipping logging after 32928 examples to avoid logging too frequently
train stats after 32960 examples: {'rewards_train/chosen': '0.050623', 'rewards_train/rejected': '0.066147', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015524', 'logps_train/rejected': '-166.26', 'logps_train/chosen': '-148.31', 'loss/train': '0.71091', 'examples_per_second': '30.538', 'grad_norm': '55.25', 'counters/examples': 32960, 'counters/updates': 1030}
train stats after 32992 examples: {'rewards_train/chosen': '0.06433', 'rewards_train/rejected': '0.05123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013099', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-126.26', 'loss/train': '0.69412', 'examples_per_second': '31.17', 'grad_norm': '60', 'counters/examples': 32992, 'counters/updates': 1031}
train stats after 33024 examples: {'rewards_train/chosen': '0.062862', 'rewards_train/rejected': '0.023767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039095', 'logps_train/rejected': '-108.85', 'logps_train/chosen': '-112.7', 'loss/train': '0.67802', 'examples_per_second': '31.508', 'grad_norm': '31.25', 'counters/examples': 33024, 'counters/updates': 1032}
train stats after 33056 examples: {'rewards_train/chosen': '0.078462', 'rewards_train/rejected': '0.022126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056336', 'logps_train/rejected': '-131.61', 'logps_train/chosen': '-155.86', 'loss/train': '0.67629', 'examples_per_second': '31.644', 'grad_norm': '49.5', 'counters/examples': 33056, 'counters/updates': 1033}
train stats after 33088 examples: {'rewards_train/chosen': '0.029574', 'rewards_train/rejected': '0.0032336', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02634', 'logps_train/rejected': '-193.78', 'logps_train/chosen': '-172.25', 'loss/train': '0.6885', 'examples_per_second': '31.532', 'grad_norm': '39.25', 'counters/examples': 33088, 'counters/updates': 1034}
skipping logging after 33120 examples to avoid logging too frequently
train stats after 33152 examples: {'rewards_train/chosen': '0.019784', 'rewards_train/rejected': '0.0023719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017412', 'logps_train/rejected': '-130.39', 'logps_train/chosen': '-145.95', 'loss/train': '0.68831', 'examples_per_second': '30.564', 'grad_norm': '36.25', 'counters/examples': 33152, 'counters/updates': 1036}
train stats after 33184 examples: {'rewards_train/chosen': '0.025271', 'rewards_train/rejected': '-0.0089453', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.034217', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-142.38', 'loss/train': '0.69429', 'examples_per_second': '31.415', 'grad_norm': '33.5', 'counters/examples': 33184, 'counters/updates': 1037}
train stats after 33216 examples: {'rewards_train/chosen': '-0.0038199', 'rewards_train/rejected': '0.069956', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.073776', 'logps_train/rejected': '-128.53', 'logps_train/chosen': '-140.96', 'loss/train': '0.73761', 'examples_per_second': '30.506', 'grad_norm': '105.5', 'counters/examples': 33216, 'counters/updates': 1038}
train stats after 33248 examples: {'rewards_train/chosen': '0.10498', 'rewards_train/rejected': '-0.018517', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1235', 'logps_train/rejected': '-180.96', 'logps_train/chosen': '-185.06', 'loss/train': '0.64432', 'examples_per_second': '29.988', 'grad_norm': '35.25', 'counters/examples': 33248, 'counters/updates': 1039}
train stats after 33280 examples: {'rewards_train/chosen': '0.068982', 'rewards_train/rejected': '-0.013171', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.082153', 'logps_train/rejected': '-159.9', 'logps_train/chosen': '-135.23', 'loss/train': '0.6565', 'examples_per_second': '32.46', 'grad_norm': '37', 'counters/examples': 33280, 'counters/updates': 1040}
train stats after 33312 examples: {'rewards_train/chosen': '0.014664', 'rewards_train/rejected': '0.041207', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026543', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-162.63', 'loss/train': '0.71183', 'examples_per_second': '32.019', 'grad_norm': '33.25', 'counters/examples': 33312, 'counters/updates': 1041}
train stats after 33344 examples: {'rewards_train/chosen': '0.057271', 'rewards_train/rejected': '0.053354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.003917', 'logps_train/rejected': '-160.05', 'logps_train/chosen': '-154.03', 'loss/train': '0.6962', 'examples_per_second': '32.96', 'grad_norm': '32.75', 'counters/examples': 33344, 'counters/updates': 1042}
train stats after 33376 examples: {'rewards_train/chosen': '0.10101', 'rewards_train/rejected': '0.004073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096941', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-178.04', 'loss/train': '0.65162', 'examples_per_second': '32.416', 'grad_norm': '42.5', 'counters/examples': 33376, 'counters/updates': 1043}
skipping logging after 33408 examples to avoid logging too frequently
train stats after 33440 examples: {'rewards_train/chosen': '0.029053', 'rewards_train/rejected': '0.066364', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.037311', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-164.09', 'loss/train': '0.71937', 'examples_per_second': '33.03', 'grad_norm': '33.25', 'counters/examples': 33440, 'counters/updates': 1045}
train stats after 33472 examples: {'rewards_train/chosen': '0.054492', 'rewards_train/rejected': '0.026525', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027967', 'logps_train/rejected': '-141.36', 'logps_train/chosen': '-150.54', 'loss/train': '0.68861', 'examples_per_second': '31.484', 'grad_norm': '128', 'counters/examples': 33472, 'counters/updates': 1046}
skipping logging after 33504 examples to avoid logging too frequently
train stats after 33536 examples: {'rewards_train/chosen': '0.095134', 'rewards_train/rejected': '0.031977', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063156', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-158.65', 'loss/train': '0.6712', 'examples_per_second': '32.929', 'grad_norm': '35', 'counters/examples': 33536, 'counters/updates': 1048}
train stats after 33568 examples: {'rewards_train/chosen': '0.039384', 'rewards_train/rejected': '0.015458', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023926', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-133.04', 'loss/train': '0.68413', 'examples_per_second': '31.347', 'grad_norm': '27.375', 'counters/examples': 33568, 'counters/updates': 1049}
train stats after 33600 examples: {'rewards_train/chosen': '0.040346', 'rewards_train/rejected': '-0.0044036', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04475', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-195.42', 'loss/train': '0.68185', 'examples_per_second': '31.547', 'grad_norm': '36.25', 'counters/examples': 33600, 'counters/updates': 1050}
train stats after 33632 examples: {'rewards_train/chosen': '0.050672', 'rewards_train/rejected': '0.062612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01194', 'logps_train/rejected': '-123.01', 'logps_train/chosen': '-159.72', 'loss/train': '0.70486', 'examples_per_second': '31.537', 'grad_norm': '35.75', 'counters/examples': 33632, 'counters/updates': 1051}
train stats after 33664 examples: {'rewards_train/chosen': '0.038029', 'rewards_train/rejected': '0.063005', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024976', 'logps_train/rejected': '-134.14', 'logps_train/chosen': '-134.23', 'loss/train': '0.71448', 'examples_per_second': '30.015', 'grad_norm': '50.5', 'counters/examples': 33664, 'counters/updates': 1052}
train stats after 33696 examples: {'rewards_train/chosen': '0.14346', 'rewards_train/rejected': '0.027036', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11643', 'logps_train/rejected': '-115.28', 'logps_train/chosen': '-158.91', 'loss/train': '0.64014', 'examples_per_second': '30.003', 'grad_norm': '26.375', 'counters/examples': 33696, 'counters/updates': 1053}
train stats after 33728 examples: {'rewards_train/chosen': '0.086561', 'rewards_train/rejected': '0.019811', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06675', 'logps_train/rejected': '-147.93', 'logps_train/chosen': '-138.99', 'loss/train': '0.66581', 'examples_per_second': '30.342', 'grad_norm': '51.25', 'counters/examples': 33728, 'counters/updates': 1054}
train stats after 33760 examples: {'rewards_train/chosen': '0.078294', 'rewards_train/rejected': '0.04538', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032915', 'logps_train/rejected': '-108', 'logps_train/chosen': '-156.25', 'loss/train': '0.68187', 'examples_per_second': '30.5', 'grad_norm': '30.25', 'counters/examples': 33760, 'counters/updates': 1055}
train stats after 33792 examples: {'rewards_train/chosen': '0.080238', 'rewards_train/rejected': '0.021452', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058785', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-157.12', 'loss/train': '0.66939', 'examples_per_second': '31.487', 'grad_norm': '45', 'counters/examples': 33792, 'counters/updates': 1056}
train stats after 33824 examples: {'rewards_train/chosen': '0.038848', 'rewards_train/rejected': '0.14531', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.10646', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-160.89', 'loss/train': '0.77545', 'examples_per_second': '30.21', 'grad_norm': '32.5', 'counters/examples': 33824, 'counters/updates': 1057}
train stats after 33856 examples: {'rewards_train/chosen': '0.083047', 'rewards_train/rejected': '0.044427', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03862', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-127.44', 'loss/train': '0.67961', 'examples_per_second': '31.511', 'grad_norm': '27.75', 'counters/examples': 33856, 'counters/updates': 1058}
train stats after 33888 examples: {'rewards_train/chosen': '0.054246', 'rewards_train/rejected': '-0.0038285', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058075', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-156.84', 'loss/train': '0.66886', 'examples_per_second': '31.236', 'grad_norm': '29.125', 'counters/examples': 33888, 'counters/updates': 1059}
train stats after 33920 examples: {'rewards_train/chosen': '0.060481', 'rewards_train/rejected': '0.034781', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025699', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-152.09', 'loss/train': '0.68477', 'examples_per_second': '31.473', 'grad_norm': '32.5', 'counters/examples': 33920, 'counters/updates': 1060}
train stats after 33952 examples: {'rewards_train/chosen': '0.13109', 'rewards_train/rejected': '0.022881', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10821', 'logps_train/rejected': '-131.52', 'logps_train/chosen': '-139.28', 'loss/train': '0.66036', 'examples_per_second': '32.123', 'grad_norm': '34.25', 'counters/examples': 33952, 'counters/updates': 1061}
train stats after 33984 examples: {'rewards_train/chosen': '0.15764', 'rewards_train/rejected': '0.10268', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054961', 'logps_train/rejected': '-100.68', 'logps_train/chosen': '-165.93', 'loss/train': '0.67029', 'examples_per_second': '31.482', 'grad_norm': '34.75', 'counters/examples': 33984, 'counters/updates': 1062}
train stats after 34016 examples: {'rewards_train/chosen': '0.074757', 'rewards_train/rejected': '0.023189', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051568', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-141.68', 'loss/train': '0.6725', 'examples_per_second': '30.516', 'grad_norm': '30.75', 'counters/examples': 34016, 'counters/updates': 1063}
train stats after 34048 examples: {'rewards_train/chosen': '0.0316', 'rewards_train/rejected': '0.022363', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0092371', 'logps_train/rejected': '-124.32', 'logps_train/chosen': '-143.72', 'loss/train': '0.69316', 'examples_per_second': '30.576', 'grad_norm': '30.75', 'counters/examples': 34048, 'counters/updates': 1064}
train stats after 34080 examples: {'rewards_train/chosen': '-0.021547', 'rewards_train/rejected': '0.0051407', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026688', 'logps_train/rejected': '-90.291', 'logps_train/chosen': '-160.92', 'loss/train': '0.71156', 'examples_per_second': '30.112', 'grad_norm': '29.125', 'counters/examples': 34080, 'counters/updates': 1065}
skipping logging after 34112 examples to avoid logging too frequently
train stats after 34144 examples: {'rewards_train/chosen': '0.11477', 'rewards_train/rejected': '0.032723', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08205', 'logps_train/rejected': '-138.09', 'logps_train/chosen': '-130.29', 'loss/train': '0.65925', 'examples_per_second': '33.996', 'grad_norm': '43.5', 'counters/examples': 34144, 'counters/updates': 1067}
train stats after 34176 examples: {'rewards_train/chosen': '0.092431', 'rewards_train/rejected': '0.039902', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052529', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-165.28', 'loss/train': '0.67388', 'examples_per_second': '31.557', 'grad_norm': '48', 'counters/examples': 34176, 'counters/updates': 1068}
train stats after 34208 examples: {'rewards_train/chosen': '0.034518', 'rewards_train/rejected': '-0.036156', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.070674', 'logps_train/rejected': '-100.08', 'logps_train/chosen': '-130.35', 'loss/train': '0.6614', 'examples_per_second': '32.604', 'grad_norm': '26', 'counters/examples': 34208, 'counters/updates': 1069}
train stats after 34240 examples: {'rewards_train/chosen': '0.040889', 'rewards_train/rejected': '0.039727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0011622', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-152.85', 'loss/train': '0.69951', 'examples_per_second': '31.663', 'grad_norm': '37.25', 'counters/examples': 34240, 'counters/updates': 1070}
train stats after 34272 examples: {'rewards_train/chosen': '0.059616', 'rewards_train/rejected': '0.041079', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018537', 'logps_train/rejected': '-144.5', 'logps_train/chosen': '-191.19', 'loss/train': '0.68792', 'examples_per_second': '31.395', 'grad_norm': '69.5', 'counters/examples': 34272, 'counters/updates': 1071}
train stats after 34304 examples: {'rewards_train/chosen': '0.028179', 'rewards_train/rejected': '0.1132', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.085019', 'logps_train/rejected': '-143.3', 'logps_train/chosen': '-154.64', 'loss/train': '0.75401', 'examples_per_second': '31.59', 'grad_norm': '83', 'counters/examples': 34304, 'counters/updates': 1072}
train stats after 34336 examples: {'rewards_train/chosen': '0.066695', 'rewards_train/rejected': '0.01655', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050145', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-95.872', 'loss/train': '0.67244', 'examples_per_second': '31.531', 'grad_norm': '25.25', 'counters/examples': 34336, 'counters/updates': 1073}
skipping logging after 34368 examples to avoid logging too frequently
train stats after 34400 examples: {'rewards_train/chosen': '0.01706', 'rewards_train/rejected': '0.0049629', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012097', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-176.09', 'loss/train': '0.691', 'examples_per_second': '30.302', 'grad_norm': '31.125', 'counters/examples': 34400, 'counters/updates': 1075}
train stats after 34432 examples: {'rewards_train/chosen': '0.042861', 'rewards_train/rejected': '-0.0059872', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048848', 'logps_train/rejected': '-115.76', 'logps_train/chosen': '-142.2', 'loss/train': '0.67188', 'examples_per_second': '31.004', 'grad_norm': '30.25', 'counters/examples': 34432, 'counters/updates': 1076}
train stats after 34464 examples: {'rewards_train/chosen': '0.11781', 'rewards_train/rejected': '0.057843', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059971', 'logps_train/rejected': '-153.98', 'logps_train/chosen': '-133.63', 'loss/train': '0.67368', 'examples_per_second': '31.325', 'grad_norm': '34.75', 'counters/examples': 34464, 'counters/updates': 1077}
train stats after 34496 examples: {'rewards_train/chosen': '0.02782', 'rewards_train/rejected': '0.006636', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021184', 'logps_train/rejected': '-105.85', 'logps_train/chosen': '-141.14', 'loss/train': '0.68548', 'examples_per_second': '31.296', 'grad_norm': '26.625', 'counters/examples': 34496, 'counters/updates': 1078}
train stats after 34528 examples: {'rewards_train/chosen': '0.025049', 'rewards_train/rejected': '0.034949', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0099006', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-118.15', 'loss/train': '0.70648', 'examples_per_second': '32.542', 'grad_norm': '33.5', 'counters/examples': 34528, 'counters/updates': 1079}
train stats after 34560 examples: {'rewards_train/chosen': '0.096454', 'rewards_train/rejected': '0.027513', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068941', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-145.11', 'loss/train': '0.66412', 'examples_per_second': '30.543', 'grad_norm': '28.5', 'counters/examples': 34560, 'counters/updates': 1080}
train stats after 34592 examples: {'rewards_train/chosen': '0.078478', 'rewards_train/rejected': '0.17117', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.092694', 'logps_train/rejected': '-166.74', 'logps_train/chosen': '-156.19', 'loss/train': '0.76424', 'examples_per_second': '30.932', 'grad_norm': '102.5', 'counters/examples': 34592, 'counters/updates': 1081}
skipping logging after 34624 examples to avoid logging too frequently
train stats after 34656 examples: {'rewards_train/chosen': '0.16755', 'rewards_train/rejected': '0.11021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057344', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-158.01', 'loss/train': '0.68885', 'examples_per_second': '31.527', 'grad_norm': '47.75', 'counters/examples': 34656, 'counters/updates': 1083}
train stats after 34688 examples: {'rewards_train/chosen': '0.057529', 'rewards_train/rejected': '0.076494', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018965', 'logps_train/rejected': '-101', 'logps_train/chosen': '-119.87', 'loss/train': '0.71061', 'examples_per_second': '30.618', 'grad_norm': '28.625', 'counters/examples': 34688, 'counters/updates': 1084}
skipping logging after 34720 examples to avoid logging too frequently
train stats after 34752 examples: {'rewards_train/chosen': '0.081763', 'rewards_train/rejected': '0.03442', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047342', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-123.25', 'loss/train': '0.6767', 'examples_per_second': '31.645', 'grad_norm': '34.75', 'counters/examples': 34752, 'counters/updates': 1086}
train stats after 34784 examples: {'rewards_train/chosen': '0.07124', 'rewards_train/rejected': '0.082519', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011279', 'logps_train/rejected': '-131.21', 'logps_train/chosen': '-127.58', 'loss/train': '0.71091', 'examples_per_second': '30.154', 'grad_norm': '42.25', 'counters/examples': 34784, 'counters/updates': 1087}
train stats after 34816 examples: {'rewards_train/chosen': '0.0447', 'rewards_train/rejected': '0.054423', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009722', 'logps_train/rejected': '-162.11', 'logps_train/chosen': '-108.5', 'loss/train': '0.70477', 'examples_per_second': '30.861', 'grad_norm': '36', 'counters/examples': 34816, 'counters/updates': 1088}
train stats after 34848 examples: {'rewards_train/chosen': '0.079527', 'rewards_train/rejected': '-0.00039455', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079921', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-159.51', 'loss/train': '0.66363', 'examples_per_second': '31.444', 'grad_norm': '45.5', 'counters/examples': 34848, 'counters/updates': 1089}
train stats after 34880 examples: {'rewards_train/chosen': '0.057901', 'rewards_train/rejected': '0.032743', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025159', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-124.26', 'loss/train': '0.68383', 'examples_per_second': '33.187', 'grad_norm': '33.75', 'counters/examples': 34880, 'counters/updates': 1090}
train stats after 34912 examples: {'rewards_train/chosen': '0.10979', 'rewards_train/rejected': '0.046068', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063727', 'logps_train/rejected': '-145.39', 'logps_train/chosen': '-162.87', 'loss/train': '0.68974', 'examples_per_second': '30.513', 'grad_norm': '46.5', 'counters/examples': 34912, 'counters/updates': 1091}
train stats after 34944 examples: {'rewards_train/chosen': '0.05654', 'rewards_train/rejected': '0.10876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.052224', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-149.34', 'loss/train': '0.73508', 'examples_per_second': '30.585', 'grad_norm': '36.75', 'counters/examples': 34944, 'counters/updates': 1092}
train stats after 34976 examples: {'rewards_train/chosen': '0.07038', 'rewards_train/rejected': '0.048034', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022346', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-156.4', 'loss/train': '0.70662', 'examples_per_second': '31.956', 'grad_norm': '49.5', 'counters/examples': 34976, 'counters/updates': 1093}
train stats after 35008 examples: {'rewards_train/chosen': '0.019116', 'rewards_train/rejected': '0.0028929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016223', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-134.05', 'loss/train': '0.68935', 'examples_per_second': '31.643', 'grad_norm': '29.625', 'counters/examples': 35008, 'counters/updates': 1094}
train stats after 35040 examples: {'rewards_train/chosen': '0.035998', 'rewards_train/rejected': '0.052238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.01624', 'logps_train/rejected': '-147.63', 'logps_train/chosen': '-131.54', 'loss/train': '0.71384', 'examples_per_second': '31.388', 'grad_norm': '33.75', 'counters/examples': 35040, 'counters/updates': 1095}
skipping logging after 35072 examples to avoid logging too frequently
train stats after 35104 examples: {'rewards_train/chosen': '0.067359', 'rewards_train/rejected': '-0.007699', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075058', 'logps_train/rejected': '-102.55', 'logps_train/chosen': '-142.85', 'loss/train': '0.66236', 'examples_per_second': '31.479', 'grad_norm': '25.75', 'counters/examples': 35104, 'counters/updates': 1097}
train stats after 35136 examples: {'rewards_train/chosen': '0.02504', 'rewards_train/rejected': '0.061578', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036537', 'logps_train/rejected': '-130.12', 'logps_train/chosen': '-118.36', 'loss/train': '0.71838', 'examples_per_second': '32.076', 'grad_norm': '29.375', 'counters/examples': 35136, 'counters/updates': 1098}
train stats after 35168 examples: {'rewards_train/chosen': '0.070531', 'rewards_train/rejected': '0.12551', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.054979', 'logps_train/rejected': '-161.44', 'logps_train/chosen': '-142.65', 'loss/train': '0.73245', 'examples_per_second': '30.591', 'grad_norm': '50.25', 'counters/examples': 35168, 'counters/updates': 1099}
train stats after 35200 examples: {'rewards_train/chosen': '0.038395', 'rewards_train/rejected': '-0.010361', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048756', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-165.55', 'loss/train': '0.6745', 'examples_per_second': '31.282', 'grad_norm': '31.5', 'counters/examples': 35200, 'counters/updates': 1100}
train stats after 35232 examples: {'rewards_train/chosen': '0.022948', 'rewards_train/rejected': '0.057215', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034267', 'logps_train/rejected': '-122.43', 'logps_train/chosen': '-159.62', 'loss/train': '0.72118', 'examples_per_second': '31.576', 'grad_norm': '31.25', 'counters/examples': 35232, 'counters/updates': 1101}
train stats after 35264 examples: {'rewards_train/chosen': '0.079279', 'rewards_train/rejected': '0.0098911', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069388', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-110.76', 'loss/train': '0.66446', 'examples_per_second': '31.179', 'grad_norm': '25.125', 'counters/examples': 35264, 'counters/updates': 1102}
train stats after 35296 examples: {'rewards_train/chosen': '0.021575', 'rewards_train/rejected': '0.028158', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0065838', 'logps_train/rejected': '-97.142', 'logps_train/chosen': '-121.89', 'loss/train': '0.7003', 'examples_per_second': '31.598', 'grad_norm': '26', 'counters/examples': 35296, 'counters/updates': 1103}
train stats after 35328 examples: {'rewards_train/chosen': '0.13945', 'rewards_train/rejected': '0.0344', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10505', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-138.84', 'loss/train': '0.65107', 'examples_per_second': '30.328', 'grad_norm': '30.375', 'counters/examples': 35328, 'counters/updates': 1104}
train stats after 35360 examples: {'rewards_train/chosen': '0.028781', 'rewards_train/rejected': '-0.028997', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057778', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-160.66', 'loss/train': '0.66948', 'examples_per_second': '31.552', 'grad_norm': '40.5', 'counters/examples': 35360, 'counters/updates': 1105}
train stats after 35392 examples: {'rewards_train/chosen': '0.014233', 'rewards_train/rejected': '0.026758', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.012525', 'logps_train/rejected': '-122.05', 'logps_train/chosen': '-155.59', 'loss/train': '0.70348', 'examples_per_second': '29.96', 'grad_norm': '33', 'counters/examples': 35392, 'counters/updates': 1106}
train stats after 35424 examples: {'rewards_train/chosen': '0.098179', 'rewards_train/rejected': '0.096477', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0017013', 'logps_train/rejected': '-144.54', 'logps_train/chosen': '-118.95', 'loss/train': '0.70722', 'examples_per_second': '31.547', 'grad_norm': '38.75', 'counters/examples': 35424, 'counters/updates': 1107}
train stats after 35456 examples: {'rewards_train/chosen': '0.077181', 'rewards_train/rejected': '0.028248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048933', 'logps_train/rejected': '-147.14', 'logps_train/chosen': '-135.82', 'loss/train': '0.67871', 'examples_per_second': '31.055', 'grad_norm': '28', 'counters/examples': 35456, 'counters/updates': 1108}
train stats after 35488 examples: {'rewards_train/chosen': '0.10041', 'rewards_train/rejected': '0.10595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0055416', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-133.37', 'loss/train': '0.71202', 'examples_per_second': '24.067', 'grad_norm': '58.75', 'counters/examples': 35488, 'counters/updates': 1109}
train stats after 35520 examples: {'rewards_train/chosen': '0.066127', 'rewards_train/rejected': '0.0030049', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063122', 'logps_train/rejected': '-130.77', 'logps_train/chosen': '-177.5', 'loss/train': '0.67071', 'examples_per_second': '31.593', 'grad_norm': '29.75', 'counters/examples': 35520, 'counters/updates': 1110}
train stats after 35552 examples: {'rewards_train/chosen': '0.040167', 'rewards_train/rejected': '0.050094', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0099274', 'logps_train/rejected': '-146.53', 'logps_train/chosen': '-129.27', 'loss/train': '0.70176', 'examples_per_second': '31.436', 'grad_norm': '32.75', 'counters/examples': 35552, 'counters/updates': 1111}
train stats after 35584 examples: {'rewards_train/chosen': '0.010165', 'rewards_train/rejected': '-0.04509', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055255', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-126.95', 'loss/train': '0.67047', 'examples_per_second': '24.533', 'grad_norm': '33', 'counters/examples': 35584, 'counters/updates': 1112}
skipping logging after 35616 examples to avoid logging too frequently
train stats after 35648 examples: {'rewards_train/chosen': '0.14581', 'rewards_train/rejected': '0.073475', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072337', 'logps_train/rejected': '-130.72', 'logps_train/chosen': '-156.66', 'loss/train': '0.67524', 'examples_per_second': '31.466', 'grad_norm': '30.5', 'counters/examples': 35648, 'counters/updates': 1114}
train stats after 35680 examples: {'rewards_train/chosen': '0.035226', 'rewards_train/rejected': '0.071023', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035796', 'logps_train/rejected': '-135.95', 'logps_train/chosen': '-159.42', 'loss/train': '0.71542', 'examples_per_second': '31.89', 'grad_norm': '35.75', 'counters/examples': 35680, 'counters/updates': 1115}
train stats after 35712 examples: {'rewards_train/chosen': '0.09359', 'rewards_train/rejected': '0.032234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061355', 'logps_train/rejected': '-154.79', 'logps_train/chosen': '-135.21', 'loss/train': '0.67273', 'examples_per_second': '30.53', 'grad_norm': '36.25', 'counters/examples': 35712, 'counters/updates': 1116}
train stats after 35744 examples: {'rewards_train/chosen': '0.18189', 'rewards_train/rejected': '0.10032', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081564', 'logps_train/rejected': '-169.95', 'logps_train/chosen': '-157', 'loss/train': '0.66088', 'examples_per_second': '32.281', 'grad_norm': '43.5', 'counters/examples': 35744, 'counters/updates': 1117}
train stats after 35776 examples: {'rewards_train/chosen': '0.019039', 'rewards_train/rejected': '0.027256', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0082162', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-145.86', 'loss/train': '0.70126', 'examples_per_second': '30.435', 'grad_norm': '39.25', 'counters/examples': 35776, 'counters/updates': 1118}
train stats after 35808 examples: {'rewards_train/chosen': '-0.026751', 'rewards_train/rejected': '0.038738', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.065489', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-164.59', 'loss/train': '0.73975', 'examples_per_second': '31.496', 'grad_norm': '48', 'counters/examples': 35808, 'counters/updates': 1119}
train stats after 35840 examples: {'rewards_train/chosen': '0.044062', 'rewards_train/rejected': '0.04193', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0021319', 'logps_train/rejected': '-148.87', 'logps_train/chosen': '-126.73', 'loss/train': '0.70524', 'examples_per_second': '31.483', 'grad_norm': '49.75', 'counters/examples': 35840, 'counters/updates': 1120}
train stats after 35872 examples: {'rewards_train/chosen': '0.077685', 'rewards_train/rejected': '0.06568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012005', 'logps_train/rejected': '-149.15', 'logps_train/chosen': '-145.49', 'loss/train': '0.69062', 'examples_per_second': '32.931', 'grad_norm': '33.25', 'counters/examples': 35872, 'counters/updates': 1121}
train stats after 35904 examples: {'rewards_train/chosen': '0.061006', 'rewards_train/rejected': '0.049445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011562', 'logps_train/rejected': '-109.36', 'logps_train/chosen': '-149.36', 'loss/train': '0.69175', 'examples_per_second': '32.002', 'grad_norm': '28', 'counters/examples': 35904, 'counters/updates': 1122}
train stats after 35936 examples: {'rewards_train/chosen': '0.095942', 'rewards_train/rejected': '0.037598', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058344', 'logps_train/rejected': '-104.6', 'logps_train/chosen': '-132.36', 'loss/train': '0.67083', 'examples_per_second': '30.042', 'grad_norm': '25.875', 'counters/examples': 35936, 'counters/updates': 1123}
train stats after 35968 examples: {'rewards_train/chosen': '0.048119', 'rewards_train/rejected': '0.075606', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027488', 'logps_train/rejected': '-107.23', 'logps_train/chosen': '-119.29', 'loss/train': '0.71216', 'examples_per_second': '30.795', 'grad_norm': '50', 'counters/examples': 35968, 'counters/updates': 1124}
train stats after 36000 examples: {'rewards_train/chosen': '-0.0044695', 'rewards_train/rejected': '0.0239', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02837', 'logps_train/rejected': '-163.59', 'logps_train/chosen': '-152.98', 'loss/train': '0.71238', 'examples_per_second': '31.49', 'grad_norm': '32.25', 'counters/examples': 36000, 'counters/updates': 1125}
Running evaluation after 36000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.86it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 36000: {'rewards_eval/chosen': '0.069318', 'rewards_eval/rejected': '0.041144', 'rewards_eval/accuracies': '0.52734', 'rewards_eval/margins': '0.028174', 'logps_eval/rejected': '-121.72', 'logps_eval/chosen': '-143.41', 'loss/eval': '0.68627'}
skipping save for non epoch
train stats after 36032 examples: {'rewards_train/chosen': '0.077769', 'rewards_train/rejected': '0.0011733', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076596', 'logps_train/rejected': '-101.97', 'logps_train/chosen': '-165.77', 'loss/train': '0.67', 'examples_per_second': '33.241', 'grad_norm': '35.5', 'counters/examples': 36032, 'counters/updates': 1126}
train stats after 36064 examples: {'rewards_train/chosen': '0.10486', 'rewards_train/rejected': '0.074577', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030285', 'logps_train/rejected': '-168.69', 'logps_train/chosen': '-142.25', 'loss/train': '0.68162', 'examples_per_second': '31.37', 'grad_norm': '35.75', 'counters/examples': 36064, 'counters/updates': 1127}
train stats after 36096 examples: {'rewards_train/chosen': '0.044944', 'rewards_train/rejected': '0.048388', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0034437', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-121.62', 'loss/train': '0.70312', 'examples_per_second': '31.433', 'grad_norm': '31.5', 'counters/examples': 36096, 'counters/updates': 1128}
train stats after 36128 examples: {'rewards_train/chosen': '0.076307', 'rewards_train/rejected': '0.093123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016816', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-146.34', 'loss/train': '0.70875', 'examples_per_second': '30.686', 'grad_norm': '33.75', 'counters/examples': 36128, 'counters/updates': 1129}
train stats after 36160 examples: {'rewards_train/chosen': '-0.018058', 'rewards_train/rejected': '0.0098633', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.027921', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-139.34', 'loss/train': '0.71289', 'examples_per_second': '30.734', 'grad_norm': '33.25', 'counters/examples': 36160, 'counters/updates': 1130}
train stats after 36192 examples: {'rewards_train/chosen': '0.064564', 'rewards_train/rejected': '0.11346', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.048894', 'logps_train/rejected': '-145.75', 'logps_train/chosen': '-151.5', 'loss/train': '0.72753', 'examples_per_second': '31.943', 'grad_norm': '58.5', 'counters/examples': 36192, 'counters/updates': 1131}
train stats after 36224 examples: {'rewards_train/chosen': '0.031692', 'rewards_train/rejected': '0.016633', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015059', 'logps_train/rejected': '-146.59', 'logps_train/chosen': '-148.93', 'loss/train': '0.6898', 'examples_per_second': '31.011', 'grad_norm': '34.25', 'counters/examples': 36224, 'counters/updates': 1132}
train stats after 36256 examples: {'rewards_train/chosen': '0.053382', 'rewards_train/rejected': '0.104', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050621', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-168.9', 'loss/train': '0.73656', 'examples_per_second': '31.106', 'grad_norm': '49', 'counters/examples': 36256, 'counters/updates': 1133}
skipping logging after 36288 examples to avoid logging too frequently
train stats after 36320 examples: {'rewards_train/chosen': '0.066527', 'rewards_train/rejected': '0.043917', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02261', 'logps_train/rejected': '-156.89', 'logps_train/chosen': '-174.78', 'loss/train': '0.69043', 'examples_per_second': '32.327', 'grad_norm': '60.25', 'counters/examples': 36320, 'counters/updates': 1135}
train stats after 36352 examples: {'rewards_train/chosen': '0.077445', 'rewards_train/rejected': '-0.020497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097942', 'logps_train/rejected': '-95.883', 'logps_train/chosen': '-132.66', 'loss/train': '0.64906', 'examples_per_second': '24.071', 'grad_norm': '23.25', 'counters/examples': 36352, 'counters/updates': 1136}
skipping logging after 36384 examples to avoid logging too frequently
train stats after 36416 examples: {'rewards_train/chosen': '0.12339', 'rewards_train/rejected': '0.026033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097355', 'logps_train/rejected': '-105.4', 'logps_train/chosen': '-147.11', 'loss/train': '0.66116', 'examples_per_second': '29.96', 'grad_norm': '30', 'counters/examples': 36416, 'counters/updates': 1138}
train stats after 36448 examples: {'rewards_train/chosen': '0.056266', 'rewards_train/rejected': '0.037833', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018432', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-154.38', 'loss/train': '0.68754', 'examples_per_second': '32.295', 'grad_norm': '35', 'counters/examples': 36448, 'counters/updates': 1139}
train stats after 36480 examples: {'rewards_train/chosen': '0.04639', 'rewards_train/rejected': '0.040251', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0061385', 'logps_train/rejected': '-95.843', 'logps_train/chosen': '-129.12', 'loss/train': '0.69465', 'examples_per_second': '32.028', 'grad_norm': '26.75', 'counters/examples': 36480, 'counters/updates': 1140}
train stats after 36512 examples: {'rewards_train/chosen': '0.091059', 'rewards_train/rejected': '0.066366', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024693', 'logps_train/rejected': '-118.93', 'logps_train/chosen': '-123.82', 'loss/train': '0.68685', 'examples_per_second': '32.641', 'grad_norm': '34.5', 'counters/examples': 36512, 'counters/updates': 1141}
train stats after 36544 examples: {'rewards_train/chosen': '0.084115', 'rewards_train/rejected': '0.064872', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019242', 'logps_train/rejected': '-149.28', 'logps_train/chosen': '-177.05', 'loss/train': '0.69222', 'examples_per_second': '30.181', 'grad_norm': '39.25', 'counters/examples': 36544, 'counters/updates': 1142}
skipping logging after 36576 examples to avoid logging too frequently
train stats after 36608 examples: {'rewards_train/chosen': '0.059371', 'rewards_train/rejected': '0.067093', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0077216', 'logps_train/rejected': '-130.84', 'logps_train/chosen': '-121.32', 'loss/train': '0.7013', 'examples_per_second': '29.73', 'grad_norm': '29.125', 'counters/examples': 36608, 'counters/updates': 1144}
train stats after 36640 examples: {'rewards_train/chosen': '0.018238', 'rewards_train/rejected': '0.1218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10356', 'logps_train/rejected': '-144', 'logps_train/chosen': '-117.75', 'loss/train': '0.76999', 'examples_per_second': '31.416', 'grad_norm': '49.25', 'counters/examples': 36640, 'counters/updates': 1145}
train stats after 36672 examples: {'rewards_train/chosen': '0.030338', 'rewards_train/rejected': '0.054043', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023705', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-166.11', 'loss/train': '0.70783', 'examples_per_second': '31.295', 'grad_norm': '32.5', 'counters/examples': 36672, 'counters/updates': 1146}
train stats after 36704 examples: {'rewards_train/chosen': '0.055206', 'rewards_train/rejected': '0.0094732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045733', 'logps_train/rejected': '-75.337', 'logps_train/chosen': '-159.62', 'loss/train': '0.67616', 'examples_per_second': '30.359', 'grad_norm': '25.125', 'counters/examples': 36704, 'counters/updates': 1147}
train stats after 36736 examples: {'rewards_train/chosen': '0.057248', 'rewards_train/rejected': '0.11206', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.054809', 'logps_train/rejected': '-120.37', 'logps_train/chosen': '-143.23', 'loss/train': '0.7454', 'examples_per_second': '31.422', 'grad_norm': '64', 'counters/examples': 36736, 'counters/updates': 1148}
skipping logging after 36768 examples to avoid logging too frequently
train stats after 36800 examples: {'rewards_train/chosen': '0.097164', 'rewards_train/rejected': '0.011866', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085298', 'logps_train/rejected': '-92.736', 'logps_train/chosen': '-157.22', 'loss/train': '0.66325', 'examples_per_second': '30.915', 'grad_norm': '28', 'counters/examples': 36800, 'counters/updates': 1150}
skipping logging after 36832 examples to avoid logging too frequently
train stats after 36864 examples: {'rewards_train/chosen': '0.17891', 'rewards_train/rejected': '-0.017876', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19678', 'logps_train/rejected': '-146.29', 'logps_train/chosen': '-169.22', 'loss/train': '0.61856', 'examples_per_second': '30.403', 'grad_norm': '33.5', 'counters/examples': 36864, 'counters/updates': 1152}
train stats after 36896 examples: {'rewards_train/chosen': '0.048426', 'rewards_train/rejected': '0.039544', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0088817', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-133.53', 'loss/train': '0.70977', 'examples_per_second': '30.627', 'grad_norm': '78.5', 'counters/examples': 36896, 'counters/updates': 1153}
train stats after 36928 examples: {'rewards_train/chosen': '0.065251', 'rewards_train/rejected': '0.022883', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042368', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-142.49', 'loss/train': '0.67907', 'examples_per_second': '31.043', 'grad_norm': '26.75', 'counters/examples': 36928, 'counters/updates': 1154}
train stats after 36960 examples: {'rewards_train/chosen': '0.035569', 'rewards_train/rejected': '0.10809', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.072525', 'logps_train/rejected': '-151.83', 'logps_train/chosen': '-114.66', 'loss/train': '0.73422', 'examples_per_second': '31.783', 'grad_norm': '36.25', 'counters/examples': 36960, 'counters/updates': 1155}
train stats after 36992 examples: {'rewards_train/chosen': '0.031681', 'rewards_train/rejected': '0.054801', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023119', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-159.78', 'loss/train': '0.71279', 'examples_per_second': '31.528', 'grad_norm': '45.5', 'counters/examples': 36992, 'counters/updates': 1156}
train stats after 37024 examples: {'rewards_train/chosen': '0.10325', 'rewards_train/rejected': '0.045598', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057649', 'logps_train/rejected': '-157.42', 'logps_train/chosen': '-177.07', 'loss/train': '0.67472', 'examples_per_second': '31.502', 'grad_norm': '35.75', 'counters/examples': 37024, 'counters/updates': 1157}
train stats after 37056 examples: {'rewards_train/chosen': '0.073582', 'rewards_train/rejected': '0.10565', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.032067', 'logps_train/rejected': '-133.09', 'logps_train/chosen': '-144.93', 'loss/train': '0.72158', 'examples_per_second': '31.307', 'grad_norm': '61.75', 'counters/examples': 37056, 'counters/updates': 1158}
skipping logging after 37088 examples to avoid logging too frequently
train stats after 37120 examples: {'rewards_train/chosen': '0.032914', 'rewards_train/rejected': '0.041341', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0084264', 'logps_train/rejected': '-128.27', 'logps_train/chosen': '-165.42', 'loss/train': '0.70355', 'examples_per_second': '36.159', 'grad_norm': '31', 'counters/examples': 37120, 'counters/updates': 1160}
skipping logging after 37152 examples to avoid logging too frequently
train stats after 37184 examples: {'rewards_train/chosen': '0.079508', 'rewards_train/rejected': '0.03768', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041827', 'logps_train/rejected': '-102.33', 'logps_train/chosen': '-174.09', 'loss/train': '0.67922', 'examples_per_second': '31.133', 'grad_norm': '28.75', 'counters/examples': 37184, 'counters/updates': 1162}
train stats after 37216 examples: {'rewards_train/chosen': '0.014658', 'rewards_train/rejected': '-0.0088647', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023522', 'logps_train/rejected': '-87.603', 'logps_train/chosen': '-113.5', 'loss/train': '0.68542', 'examples_per_second': '31.413', 'grad_norm': '37.25', 'counters/examples': 37216, 'counters/updates': 1163}
train stats after 37248 examples: {'rewards_train/chosen': '0.086035', 'rewards_train/rejected': '0.0068466', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079189', 'logps_train/rejected': '-143.18', 'logps_train/chosen': '-157.8', 'loss/train': '0.66248', 'examples_per_second': '32.995', 'grad_norm': '29.25', 'counters/examples': 37248, 'counters/updates': 1164}
train stats after 37280 examples: {'rewards_train/chosen': '0.041397', 'rewards_train/rejected': '0.014618', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026778', 'logps_train/rejected': '-94.028', 'logps_train/chosen': '-168.07', 'loss/train': '0.68341', 'examples_per_second': '30.725', 'grad_norm': '34.25', 'counters/examples': 37280, 'counters/updates': 1165}
train stats after 37312 examples: {'rewards_train/chosen': '0.047704', 'rewards_train/rejected': '0.061837', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014133', 'logps_train/rejected': '-172.71', 'logps_train/chosen': '-171.33', 'loss/train': '0.7044', 'examples_per_second': '31.311', 'grad_norm': '36.25', 'counters/examples': 37312, 'counters/updates': 1166}
skipping logging after 37344 examples to avoid logging too frequently
train stats after 37376 examples: {'rewards_train/chosen': '0.064122', 'rewards_train/rejected': '-0.0069573', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071079', 'logps_train/rejected': '-108.26', 'logps_train/chosen': '-157.01', 'loss/train': '0.66502', 'examples_per_second': '30.861', 'grad_norm': '34.75', 'counters/examples': 37376, 'counters/updates': 1168}
train stats after 37408 examples: {'rewards_train/chosen': '0.1095', 'rewards_train/rejected': '0.085235', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024267', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-165.61', 'loss/train': '0.69372', 'examples_per_second': '29.911', 'grad_norm': '30.875', 'counters/examples': 37408, 'counters/updates': 1169}
train stats after 37440 examples: {'rewards_train/chosen': '0.11833', 'rewards_train/rejected': '0.011992', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10633', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-174.07', 'loss/train': '0.64897', 'examples_per_second': '31.137', 'grad_norm': '29.625', 'counters/examples': 37440, 'counters/updates': 1170}
train stats after 37472 examples: {'rewards_train/chosen': '0.030634', 'rewards_train/rejected': '0.039218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0085846', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-142.2', 'loss/train': '0.70095', 'examples_per_second': '30.399', 'grad_norm': '32', 'counters/examples': 37472, 'counters/updates': 1171}
skipping logging after 37504 examples to avoid logging too frequently
train stats after 37536 examples: {'rewards_train/chosen': '0.096689', 'rewards_train/rejected': '0.12875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.032059', 'logps_train/rejected': '-159.35', 'logps_train/chosen': '-146.32', 'loss/train': '0.73252', 'examples_per_second': '32.474', 'grad_norm': '104.5', 'counters/examples': 37536, 'counters/updates': 1173}
train stats after 37568 examples: {'rewards_train/chosen': '0.013934', 'rewards_train/rejected': '-0.036298', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050232', 'logps_train/rejected': '-139.5', 'logps_train/chosen': '-110.53', 'loss/train': '0.67507', 'examples_per_second': '30.503', 'grad_norm': '28.5', 'counters/examples': 37568, 'counters/updates': 1174}
train stats after 37600 examples: {'rewards_train/chosen': '0.11138', 'rewards_train/rejected': '0.11412', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0027484', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-174.56', 'loss/train': '0.70196', 'examples_per_second': '29.793', 'grad_norm': '31.5', 'counters/examples': 37600, 'counters/updates': 1175}
train stats after 37632 examples: {'rewards_train/chosen': '0.031226', 'rewards_train/rejected': '-0.0060841', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03731', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-118.91', 'loss/train': '0.67866', 'examples_per_second': '30.325', 'grad_norm': '27.125', 'counters/examples': 37632, 'counters/updates': 1176}
skipping logging after 37664 examples to avoid logging too frequently
train stats after 37696 examples: {'rewards_train/chosen': '0.08663', 'rewards_train/rejected': '0.037165', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049465', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-171.73', 'loss/train': '0.67647', 'examples_per_second': '34.233', 'grad_norm': '52', 'counters/examples': 37696, 'counters/updates': 1178}
skipping logging after 37728 examples to avoid logging too frequently
train stats after 37760 examples: {'rewards_train/chosen': '0.14918', 'rewards_train/rejected': '0.031911', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11727', 'logps_train/rejected': '-115.3', 'logps_train/chosen': '-193.77', 'loss/train': '0.64841', 'examples_per_second': '31.539', 'grad_norm': '37.5', 'counters/examples': 37760, 'counters/updates': 1180}
train stats after 37792 examples: {'rewards_train/chosen': '0.072652', 'rewards_train/rejected': '0.074907', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0022556', 'logps_train/rejected': '-159.29', 'logps_train/chosen': '-121.64', 'loss/train': '0.70452', 'examples_per_second': '31.528', 'grad_norm': '66', 'counters/examples': 37792, 'counters/updates': 1181}
train stats after 37824 examples: {'rewards_train/chosen': '0.082502', 'rewards_train/rejected': '0.051562', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03094', 'logps_train/rejected': '-164.95', 'logps_train/chosen': '-168.12', 'loss/train': '0.69003', 'examples_per_second': '31.532', 'grad_norm': '35', 'counters/examples': 37824, 'counters/updates': 1182}
train stats after 37856 examples: {'rewards_train/chosen': '0.14229', 'rewards_train/rejected': '0.026056', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11624', 'logps_train/rejected': '-134.95', 'logps_train/chosen': '-185.12', 'loss/train': '0.64756', 'examples_per_second': '31.493', 'grad_norm': '31.75', 'counters/examples': 37856, 'counters/updates': 1183}
skipping logging after 37888 examples to avoid logging too frequently
train stats after 37920 examples: {'rewards_train/chosen': '0.058528', 'rewards_train/rejected': '0.077811', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.019283', 'logps_train/rejected': '-154.45', 'logps_train/chosen': '-146.13', 'loss/train': '0.71041', 'examples_per_second': '32.122', 'grad_norm': '34.25', 'counters/examples': 37920, 'counters/updates': 1185}
skipping logging after 37952 examples to avoid logging too frequently
train stats after 37984 examples: {'rewards_train/chosen': '0.080456', 'rewards_train/rejected': '0.04952', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030936', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-150.25', 'loss/train': '0.68233', 'examples_per_second': '31.392', 'grad_norm': '38.5', 'counters/examples': 37984, 'counters/updates': 1187}
train stats after 38016 examples: {'rewards_train/chosen': '0.038626', 'rewards_train/rejected': '0.061446', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02282', 'logps_train/rejected': '-122.72', 'logps_train/chosen': '-174.32', 'loss/train': '0.71337', 'examples_per_second': '31.632', 'grad_norm': '39.5', 'counters/examples': 38016, 'counters/updates': 1188}
train stats after 38048 examples: {'rewards_train/chosen': '0.039543', 'rewards_train/rejected': '0.014847', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024695', 'logps_train/rejected': '-157.28', 'logps_train/chosen': '-131.95', 'loss/train': '0.68516', 'examples_per_second': '31.543', 'grad_norm': '32', 'counters/examples': 38048, 'counters/updates': 1189}
train stats after 38080 examples: {'rewards_train/chosen': '0.12347', 'rewards_train/rejected': '0.030856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092612', 'logps_train/rejected': '-90.857', 'logps_train/chosen': '-159.39', 'loss/train': '0.66502', 'examples_per_second': '30.12', 'grad_norm': '28.25', 'counters/examples': 38080, 'counters/updates': 1190}
train stats after 38112 examples: {'rewards_train/chosen': '0.042441', 'rewards_train/rejected': '0.060502', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018061', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-150.42', 'loss/train': '0.71239', 'examples_per_second': '31.952', 'grad_norm': '41', 'counters/examples': 38112, 'counters/updates': 1191}
train stats after 38144 examples: {'rewards_train/chosen': '0.024633', 'rewards_train/rejected': '0.020767', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0038655', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-125.13', 'loss/train': '0.69485', 'examples_per_second': '31.511', 'grad_norm': '28.875', 'counters/examples': 38144, 'counters/updates': 1192}
train stats after 38176 examples: {'rewards_train/chosen': '0.060257', 'rewards_train/rejected': '0.09689', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036633', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-125.46', 'loss/train': '0.72579', 'examples_per_second': '32.016', 'grad_norm': '46.5', 'counters/examples': 38176, 'counters/updates': 1193}
train stats after 38208 examples: {'rewards_train/chosen': '0.12808', 'rewards_train/rejected': '0.043733', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084349', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-178.89', 'loss/train': '0.65736', 'examples_per_second': '31.667', 'grad_norm': '27.875', 'counters/examples': 38208, 'counters/updates': 1194}
train stats after 38240 examples: {'rewards_train/chosen': '-0.013508', 'rewards_train/rejected': '-0.019687', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0061782', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-136.57', 'loss/train': '0.69253', 'examples_per_second': '30.021', 'grad_norm': '26.125', 'counters/examples': 38240, 'counters/updates': 1195}
train stats after 38272 examples: {'rewards_train/chosen': '0.027424', 'rewards_train/rejected': '0.075418', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.047994', 'logps_train/rejected': '-135.1', 'logps_train/chosen': '-155.39', 'loss/train': '0.72408', 'examples_per_second': '30.481', 'grad_norm': '29.25', 'counters/examples': 38272, 'counters/updates': 1196}
train stats after 38304 examples: {'rewards_train/chosen': '0.13361', 'rewards_train/rejected': '0.16698', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.033361', 'logps_train/rejected': '-165.01', 'logps_train/chosen': '-155.67', 'loss/train': '0.72743', 'examples_per_second': '31.339', 'grad_norm': '60.25', 'counters/examples': 38304, 'counters/updates': 1197}
skipping logging after 38336 examples to avoid logging too frequently
train stats after 38368 examples: {'rewards_train/chosen': '0.057501', 'rewards_train/rejected': '0.060224', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0027236', 'logps_train/rejected': '-140.34', 'logps_train/chosen': '-165.95', 'loss/train': '0.69887', 'examples_per_second': '31.431', 'grad_norm': '35.25', 'counters/examples': 38368, 'counters/updates': 1199}
train stats after 38400 examples: {'rewards_train/chosen': '0.15805', 'rewards_train/rejected': '0.26599', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.10794', 'logps_train/rejected': '-133.41', 'logps_train/chosen': '-147.95', 'loss/train': '0.77716', 'examples_per_second': '31.85', 'grad_norm': '55.5', 'counters/examples': 38400, 'counters/updates': 1200}
train stats after 38432 examples: {'rewards_train/chosen': '0.024592', 'rewards_train/rejected': '0.024007', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00058535', 'logps_train/rejected': '-145.51', 'logps_train/chosen': '-158.89', 'loss/train': '0.70068', 'examples_per_second': '31.416', 'grad_norm': '37', 'counters/examples': 38432, 'counters/updates': 1201}
train stats after 38464 examples: {'rewards_train/chosen': '0.089999', 'rewards_train/rejected': '0.0080998', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0819', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-173.28', 'loss/train': '0.66818', 'examples_per_second': '31.453', 'grad_norm': '25.875', 'counters/examples': 38464, 'counters/updates': 1202}
train stats after 38496 examples: {'rewards_train/chosen': '0.042381', 'rewards_train/rejected': '0.042624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00024268', 'logps_train/rejected': '-165.75', 'logps_train/chosen': '-167.82', 'loss/train': '0.69984', 'examples_per_second': '30.473', 'grad_norm': '36.75', 'counters/examples': 38496, 'counters/updates': 1203}
train stats after 38528 examples: {'rewards_train/chosen': '0.075884', 'rewards_train/rejected': '0.011775', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064109', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-138.34', 'loss/train': '0.66696', 'examples_per_second': '30.929', 'grad_norm': '31.125', 'counters/examples': 38528, 'counters/updates': 1204}
train stats after 38560 examples: {'rewards_train/chosen': '0.11421', 'rewards_train/rejected': '-0.020428', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13463', 'logps_train/rejected': '-123.55', 'logps_train/chosen': '-117.78', 'loss/train': '0.63697', 'examples_per_second': '30.921', 'grad_norm': '29.625', 'counters/examples': 38560, 'counters/updates': 1205}
skipping logging after 38592 examples to avoid logging too frequently
train stats after 38624 examples: {'rewards_train/chosen': '0.010044', 'rewards_train/rejected': '0.046599', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.036555', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-139.35', 'loss/train': '0.7144', 'examples_per_second': '30.126', 'grad_norm': '32.25', 'counters/examples': 38624, 'counters/updates': 1207}
train stats after 38656 examples: {'rewards_train/chosen': '0.065369', 'rewards_train/rejected': '0.1576', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.092234', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-160.09', 'loss/train': '0.76858', 'examples_per_second': '31.357', 'grad_norm': '82.5', 'counters/examples': 38656, 'counters/updates': 1208}
train stats after 38688 examples: {'rewards_train/chosen': '0.03929', 'rewards_train/rejected': '0.0069767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032313', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-100.95', 'loss/train': '0.67986', 'examples_per_second': '32.893', 'grad_norm': '39.75', 'counters/examples': 38688, 'counters/updates': 1209}
train stats after 38720 examples: {'rewards_train/chosen': '0.18114', 'rewards_train/rejected': '0.056587', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12456', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-163.41', 'loss/train': '0.65932', 'examples_per_second': '30.656', 'grad_norm': '31.5', 'counters/examples': 38720, 'counters/updates': 1210}
train stats after 38752 examples: {'rewards_train/chosen': '0.015216', 'rewards_train/rejected': '0.05043', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035214', 'logps_train/rejected': '-132.02', 'logps_train/chosen': '-155.84', 'loss/train': '0.71337', 'examples_per_second': '30.385', 'grad_norm': '32.75', 'counters/examples': 38752, 'counters/updates': 1211}
train stats after 38784 examples: {'rewards_train/chosen': '0.04513', 'rewards_train/rejected': '-0.0008066', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045937', 'logps_train/rejected': '-154.53', 'logps_train/chosen': '-153.81', 'loss/train': '0.67824', 'examples_per_second': '30.033', 'grad_norm': '38.75', 'counters/examples': 38784, 'counters/updates': 1212}
train stats after 38816 examples: {'rewards_train/chosen': '0.047587', 'rewards_train/rejected': '0.023056', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024531', 'logps_train/rejected': '-124.91', 'logps_train/chosen': '-173.96', 'loss/train': '0.68619', 'examples_per_second': '31.543', 'grad_norm': '31', 'counters/examples': 38816, 'counters/updates': 1213}
train stats after 38848 examples: {'rewards_train/chosen': '0.10391', 'rewards_train/rejected': '-0.043687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1476', 'logps_train/rejected': '-184.84', 'logps_train/chosen': '-191.03', 'loss/train': '0.63663', 'examples_per_second': '30.401', 'grad_norm': '44', 'counters/examples': 38848, 'counters/updates': 1214}
train stats after 38880 examples: {'rewards_train/chosen': '0.1032', 'rewards_train/rejected': '0.057643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045558', 'logps_train/rejected': '-125.24', 'logps_train/chosen': '-160.48', 'loss/train': '0.67904', 'examples_per_second': '31.42', 'grad_norm': '33.5', 'counters/examples': 38880, 'counters/updates': 1215}
train stats after 38912 examples: {'rewards_train/chosen': '0.027299', 'rewards_train/rejected': '-0.021658', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048957', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-126.96', 'loss/train': '0.67378', 'examples_per_second': '31.338', 'grad_norm': '28.75', 'counters/examples': 38912, 'counters/updates': 1216}
train stats after 38944 examples: {'rewards_train/chosen': '0.066639', 'rewards_train/rejected': '0.042081', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024558', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-159.46', 'loss/train': '0.69164', 'examples_per_second': '30.739', 'grad_norm': '40.75', 'counters/examples': 38944, 'counters/updates': 1217}
train stats after 38976 examples: {'rewards_train/chosen': '0.099913', 'rewards_train/rejected': '0.040501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059413', 'logps_train/rejected': '-128.07', 'logps_train/chosen': '-184.85', 'loss/train': '0.67336', 'examples_per_second': '31.472', 'grad_norm': '32.75', 'counters/examples': 38976, 'counters/updates': 1218}
train stats after 39008 examples: {'rewards_train/chosen': '0.088621', 'rewards_train/rejected': '0.0016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087021', 'logps_train/rejected': '-149.98', 'logps_train/chosen': '-137.32', 'loss/train': '0.65812', 'examples_per_second': '32.524', 'grad_norm': '30.625', 'counters/examples': 39008, 'counters/updates': 1219}
train stats after 39040 examples: {'rewards_train/chosen': '0.023403', 'rewards_train/rejected': '0.06981', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.046407', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-130.01', 'loss/train': '0.72786', 'examples_per_second': '31.53', 'grad_norm': '34', 'counters/examples': 39040, 'counters/updates': 1220}
skipping logging after 39072 examples to avoid logging too frequently
train stats after 39104 examples: {'rewards_train/chosen': '0.047146', 'rewards_train/rejected': '0.028646', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0185', 'logps_train/rejected': '-96.872', 'logps_train/chosen': '-201.74', 'loss/train': '0.69191', 'examples_per_second': '32.515', 'grad_norm': '32.25', 'counters/examples': 39104, 'counters/updates': 1222}
train stats after 39136 examples: {'rewards_train/chosen': '0.073779', 'rewards_train/rejected': '0.019806', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053973', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-157.1', 'loss/train': '0.67319', 'examples_per_second': '31.063', 'grad_norm': '32.25', 'counters/examples': 39136, 'counters/updates': 1223}
skipping logging after 39168 examples to avoid logging too frequently
train stats after 39200 examples: {'rewards_train/chosen': '0.049591', 'rewards_train/rejected': '0.028449', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021142', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-137.06', 'loss/train': '0.68533', 'examples_per_second': '35.945', 'grad_norm': '26.25', 'counters/examples': 39200, 'counters/updates': 1225}
skipping logging after 39232 examples to avoid logging too frequently
train stats after 39264 examples: {'rewards_train/chosen': '0.14673', 'rewards_train/rejected': '0.045417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10132', 'logps_train/rejected': '-119.1', 'logps_train/chosen': '-141.76', 'loss/train': '0.66007', 'examples_per_second': '31.703', 'grad_norm': '29.125', 'counters/examples': 39264, 'counters/updates': 1227}
skipping logging after 39296 examples to avoid logging too frequently
train stats after 39328 examples: {'rewards_train/chosen': '0.029165', 'rewards_train/rejected': '0.081574', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.052409', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-150.94', 'loss/train': '0.73003', 'examples_per_second': '32.109', 'grad_norm': '44.25', 'counters/examples': 39328, 'counters/updates': 1229}
train stats after 39360 examples: {'rewards_train/chosen': '0.077933', 'rewards_train/rejected': '0.017947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059987', 'logps_train/rejected': '-146.37', 'logps_train/chosen': '-128.38', 'loss/train': '0.66657', 'examples_per_second': '32.782', 'grad_norm': '28.5', 'counters/examples': 39360, 'counters/updates': 1230}
skipping logging after 39392 examples to avoid logging too frequently
train stats after 39424 examples: {'rewards_train/chosen': '0.017694', 'rewards_train/rejected': '0.029201', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.011507', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-159.68', 'loss/train': '0.70728', 'examples_per_second': '31.316', 'grad_norm': '36.5', 'counters/examples': 39424, 'counters/updates': 1232}
train stats after 39456 examples: {'rewards_train/chosen': '0.094274', 'rewards_train/rejected': '0.035263', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059011', 'logps_train/rejected': '-89.691', 'logps_train/chosen': '-141.49', 'loss/train': '0.66767', 'examples_per_second': '31.511', 'grad_norm': '28.25', 'counters/examples': 39456, 'counters/updates': 1233}
train stats after 39488 examples: {'rewards_train/chosen': '0.045677', 'rewards_train/rejected': '0.025725', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019952', 'logps_train/rejected': '-140.89', 'logps_train/chosen': '-121.46', 'loss/train': '0.68922', 'examples_per_second': '30.064', 'grad_norm': '48.5', 'counters/examples': 39488, 'counters/updates': 1234}
train stats after 39520 examples: {'rewards_train/chosen': '0.07183', 'rewards_train/rejected': '0.063269', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0085609', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-147.63', 'loss/train': '0.69099', 'examples_per_second': '29.982', 'grad_norm': '39.75', 'counters/examples': 39520, 'counters/updates': 1235}
train stats after 39552 examples: {'rewards_train/chosen': '0.046994', 'rewards_train/rejected': '0.071989', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024994', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-112.96', 'loss/train': '0.711', 'examples_per_second': '30.86', 'grad_norm': '33.75', 'counters/examples': 39552, 'counters/updates': 1236}
train stats after 39584 examples: {'rewards_train/chosen': '0.073046', 'rewards_train/rejected': '0.03032', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042726', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-136.62', 'loss/train': '0.67867', 'examples_per_second': '31.338', 'grad_norm': '36', 'counters/examples': 39584, 'counters/updates': 1237}
train stats after 39616 examples: {'rewards_train/chosen': '0.050389', 'rewards_train/rejected': '0.016846', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033543', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-142.49', 'loss/train': '0.68', 'examples_per_second': '33.085', 'grad_norm': '29.625', 'counters/examples': 39616, 'counters/updates': 1238}
train stats after 39648 examples: {'rewards_train/chosen': '0.10298', 'rewards_train/rejected': '0.091905', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011075', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-139.43', 'loss/train': '0.69996', 'examples_per_second': '31.466', 'grad_norm': '34.75', 'counters/examples': 39648, 'counters/updates': 1239}
train stats after 39680 examples: {'rewards_train/chosen': '0.07759', 'rewards_train/rejected': '0.023704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053885', 'logps_train/rejected': '-115.28', 'logps_train/chosen': '-157.32', 'loss/train': '0.67248', 'examples_per_second': '32.377', 'grad_norm': '31.75', 'counters/examples': 39680, 'counters/updates': 1240}
train stats after 39712 examples: {'rewards_train/chosen': '0.06013', 'rewards_train/rejected': '0.049865', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.010265', 'logps_train/rejected': '-142.75', 'logps_train/chosen': '-160.29', 'loss/train': '0.69213', 'examples_per_second': '32.419', 'grad_norm': '36.5', 'counters/examples': 39712, 'counters/updates': 1241}
train stats after 39744 examples: {'rewards_train/chosen': '0.040725', 'rewards_train/rejected': '0.00077072', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039955', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-170.56', 'loss/train': '0.6758', 'examples_per_second': '31.745', 'grad_norm': '26.625', 'counters/examples': 39744, 'counters/updates': 1242}
train stats after 39776 examples: {'rewards_train/chosen': '0.030562', 'rewards_train/rejected': '0.071823', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.041262', 'logps_train/rejected': '-156.99', 'logps_train/chosen': '-158.4', 'loss/train': '0.71999', 'examples_per_second': '32.05', 'grad_norm': '45.5', 'counters/examples': 39776, 'counters/updates': 1243}
train stats after 39808 examples: {'rewards_train/chosen': '0.023335', 'rewards_train/rejected': '0.050587', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027252', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-126.81', 'loss/train': '0.71507', 'examples_per_second': '32.43', 'grad_norm': '27.625', 'counters/examples': 39808, 'counters/updates': 1244}
train stats after 39840 examples: {'rewards_train/chosen': '-0.0047255', 'rewards_train/rejected': '0.04184', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.046566', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-118.7', 'loss/train': '0.72428', 'examples_per_second': '31.871', 'grad_norm': '44.75', 'counters/examples': 39840, 'counters/updates': 1245}
train stats after 39872 examples: {'rewards_train/chosen': '0.066132', 'rewards_train/rejected': '0.11002', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043886', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-115.94', 'loss/train': '0.72782', 'examples_per_second': '30.129', 'grad_norm': '26.5', 'counters/examples': 39872, 'counters/updates': 1246}
train stats after 39904 examples: {'rewards_train/chosen': '0.089455', 'rewards_train/rejected': '0.075138', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014317', 'logps_train/rejected': '-158.69', 'logps_train/chosen': '-181.53', 'loss/train': '0.70769', 'examples_per_second': '31.798', 'grad_norm': '36.5', 'counters/examples': 39904, 'counters/updates': 1247}
train stats after 39936 examples: {'rewards_train/chosen': '0.023537', 'rewards_train/rejected': '0.034441', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.010904', 'logps_train/rejected': '-96.13', 'logps_train/chosen': '-145.15', 'loss/train': '0.70091', 'examples_per_second': '30.086', 'grad_norm': '27.375', 'counters/examples': 39936, 'counters/updates': 1248}
train stats after 39968 examples: {'rewards_train/chosen': '0.041451', 'rewards_train/rejected': '0.01945', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-115.69', 'loss/train': '0.68636', 'examples_per_second': '30.477', 'grad_norm': '35', 'counters/examples': 39968, 'counters/updates': 1249}
train stats after 40000 examples: {'rewards_train/chosen': '0.058632', 'rewards_train/rejected': '0.040096', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018537', 'logps_train/rejected': '-145.92', 'logps_train/chosen': '-127.96', 'loss/train': '0.69058', 'examples_per_second': '30.308', 'grad_norm': '35', 'counters/examples': 40000, 'counters/updates': 1250}
train stats after 40032 examples: {'rewards_train/chosen': '0.065853', 'rewards_train/rejected': '-0.022876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088729', 'logps_train/rejected': '-115.66', 'logps_train/chosen': '-149.63', 'loss/train': '0.65528', 'examples_per_second': '32.465', 'grad_norm': '27.5', 'counters/examples': 40032, 'counters/updates': 1251}
train stats after 40064 examples: {'rewards_train/chosen': '0.051208', 'rewards_train/rejected': '0.060703', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0094948', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-149.37', 'loss/train': '0.70875', 'examples_per_second': '30.084', 'grad_norm': '37', 'counters/examples': 40064, 'counters/updates': 1252}
skipping logging after 40096 examples to avoid logging too frequently
train stats after 40128 examples: {'rewards_train/chosen': '0.11905', 'rewards_train/rejected': '0.1033', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01575', 'logps_train/rejected': '-140.36', 'logps_train/chosen': '-161.93', 'loss/train': '0.70112', 'examples_per_second': '31.43', 'grad_norm': '62.75', 'counters/examples': 40128, 'counters/updates': 1254}
train stats after 40160 examples: {'rewards_train/chosen': '0.029923', 'rewards_train/rejected': '0.010061', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.019862', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-136.59', 'loss/train': '0.69165', 'examples_per_second': '33.029', 'grad_norm': '37', 'counters/examples': 40160, 'counters/updates': 1255}
train stats after 40192 examples: {'rewards_train/chosen': '0.038079', 'rewards_train/rejected': '0.032506', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0055732', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-161.6', 'loss/train': '0.69515', 'examples_per_second': '32.408', 'grad_norm': '29.5', 'counters/examples': 40192, 'counters/updates': 1256}
train stats after 40224 examples: {'rewards_train/chosen': '0.020632', 'rewards_train/rejected': '0.012657', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0079755', 'logps_train/rejected': '-130.87', 'logps_train/chosen': '-161.08', 'loss/train': '0.69797', 'examples_per_second': '31.523', 'grad_norm': '34.5', 'counters/examples': 40224, 'counters/updates': 1257}
train stats after 40256 examples: {'rewards_train/chosen': '0.032578', 'rewards_train/rejected': '0.024139', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.008439', 'logps_train/rejected': '-143.22', 'logps_train/chosen': '-166.59', 'loss/train': '0.69804', 'examples_per_second': '30.679', 'grad_norm': '44.25', 'counters/examples': 40256, 'counters/updates': 1258}
train stats after 40288 examples: {'rewards_train/chosen': '0.050973', 'rewards_train/rejected': '0.034726', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016247', 'logps_train/rejected': '-146.18', 'logps_train/chosen': '-145.23', 'loss/train': '0.69121', 'examples_per_second': '31.516', 'grad_norm': '39.25', 'counters/examples': 40288, 'counters/updates': 1259}
train stats after 40320 examples: {'rewards_train/chosen': '0.063705', 'rewards_train/rejected': '0.048234', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015471', 'logps_train/rejected': '-114.34', 'logps_train/chosen': '-131.5', 'loss/train': '0.68987', 'examples_per_second': '31.904', 'grad_norm': '44.5', 'counters/examples': 40320, 'counters/updates': 1260}
skipping logging after 40352 examples to avoid logging too frequently
train stats after 40384 examples: {'rewards_train/chosen': '0.0592', 'rewards_train/rejected': '0.042496', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016704', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-130.49', 'loss/train': '0.68807', 'examples_per_second': '32.707', 'grad_norm': '30.75', 'counters/examples': 40384, 'counters/updates': 1262}
train stats after 40416 examples: {'rewards_train/chosen': '0.018936', 'rewards_train/rejected': '-0.034669', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053605', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-128.28', 'loss/train': '0.67351', 'examples_per_second': '30.493', 'grad_norm': '31.25', 'counters/examples': 40416, 'counters/updates': 1263}
train stats after 40448 examples: {'rewards_train/chosen': '0.044658', 'rewards_train/rejected': '0.013284', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031374', 'logps_train/rejected': '-165.94', 'logps_train/chosen': '-137.75', 'loss/train': '0.68242', 'examples_per_second': '32.191', 'grad_norm': '32.75', 'counters/examples': 40448, 'counters/updates': 1264}
train stats after 40480 examples: {'rewards_train/chosen': '0.10656', 'rewards_train/rejected': '0.10098', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0055798', 'logps_train/rejected': '-137.68', 'logps_train/chosen': '-141.76', 'loss/train': '0.70207', 'examples_per_second': '31.533', 'grad_norm': '59.25', 'counters/examples': 40480, 'counters/updates': 1265}
train stats after 40512 examples: {'rewards_train/chosen': '0.11232', 'rewards_train/rejected': '0.082465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029859', 'logps_train/rejected': '-136.71', 'logps_train/chosen': '-139.43', 'loss/train': '0.68334', 'examples_per_second': '32.447', 'grad_norm': '28', 'counters/examples': 40512, 'counters/updates': 1266}
train stats after 40544 examples: {'rewards_train/chosen': '0.020461', 'rewards_train/rejected': '0.067078', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046617', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-152.17', 'loss/train': '0.72201', 'examples_per_second': '31.492', 'grad_norm': '44.75', 'counters/examples': 40544, 'counters/updates': 1267}
train stats after 40576 examples: {'rewards_train/chosen': '0.020693', 'rewards_train/rejected': '0.017181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0035124', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-145.24', 'loss/train': '0.69883', 'examples_per_second': '30.146', 'grad_norm': '32.5', 'counters/examples': 40576, 'counters/updates': 1268}
train stats after 40608 examples: {'rewards_train/chosen': '0.026284', 'rewards_train/rejected': '0.023727', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0025575', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-179.05', 'loss/train': '0.69425', 'examples_per_second': '30.032', 'grad_norm': '32.25', 'counters/examples': 40608, 'counters/updates': 1269}
train stats after 40640 examples: {'rewards_train/chosen': '-0.0084191', 'rewards_train/rejected': '-0.016444', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0080247', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-111.34', 'loss/train': '0.69241', 'examples_per_second': '31.893', 'grad_norm': '25.5', 'counters/examples': 40640, 'counters/updates': 1270}
train stats after 40672 examples: {'rewards_train/chosen': '0.039135', 'rewards_train/rejected': '0.052757', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.013622', 'logps_train/rejected': '-156.48', 'logps_train/chosen': '-134.1', 'loss/train': '0.70564', 'examples_per_second': '31.499', 'grad_norm': '40.75', 'counters/examples': 40672, 'counters/updates': 1271}
train stats after 40704 examples: {'rewards_train/chosen': '-0.016521', 'rewards_train/rejected': '0.083851', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.10037', 'logps_train/rejected': '-105.49', 'logps_train/chosen': '-133.35', 'loss/train': '0.75817', 'examples_per_second': '32.764', 'grad_norm': '76', 'counters/examples': 40704, 'counters/updates': 1272}
skipping logging after 40736 examples to avoid logging too frequently
train stats after 40768 examples: {'rewards_train/chosen': '0.11479', 'rewards_train/rejected': '0.012587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1022', 'logps_train/rejected': '-124.75', 'logps_train/chosen': '-157.88', 'loss/train': '0.64994', 'examples_per_second': '33.191', 'grad_norm': '29.5', 'counters/examples': 40768, 'counters/updates': 1274}
skipping logging after 40800 examples to avoid logging too frequently
train stats after 40832 examples: {'rewards_train/chosen': '-0.0012649', 'rewards_train/rejected': '0.02881', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030075', 'logps_train/rejected': '-90.6', 'logps_train/chosen': '-121.6', 'loss/train': '0.71623', 'examples_per_second': '35.333', 'grad_norm': '26.875', 'counters/examples': 40832, 'counters/updates': 1276}
skipping logging after 40864 examples to avoid logging too frequently
train stats after 40896 examples: {'rewards_train/chosen': '0.10742', 'rewards_train/rejected': '0.08732', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0201', 'logps_train/rejected': '-143.51', 'logps_train/chosen': '-164.6', 'loss/train': '0.69343', 'examples_per_second': '31.492', 'grad_norm': '43.25', 'counters/examples': 40896, 'counters/updates': 1278}
train stats after 40928 examples: {'rewards_train/chosen': '0.070611', 'rewards_train/rejected': '0.027366', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043245', 'logps_train/rejected': '-141.97', 'logps_train/chosen': '-127.03', 'loss/train': '0.67605', 'examples_per_second': '31.82', 'grad_norm': '30.125', 'counters/examples': 40928, 'counters/updates': 1279}
train stats after 40960 examples: {'rewards_train/chosen': '-0.002659', 'rewards_train/rejected': '0.023727', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.026386', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-125.98', 'loss/train': '0.71138', 'examples_per_second': '25.844', 'grad_norm': '30.625', 'counters/examples': 40960, 'counters/updates': 1280}
train stats after 40992 examples: {'rewards_train/chosen': '0.029471', 'rewards_train/rejected': '0.010041', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01943', 'logps_train/rejected': '-101.59', 'logps_train/chosen': '-122.7', 'loss/train': '0.68663', 'examples_per_second': '31.539', 'grad_norm': '22.75', 'counters/examples': 40992, 'counters/updates': 1281}
train stats after 41024 examples: {'rewards_train/chosen': '0.086862', 'rewards_train/rejected': '0.050586', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036275', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-139.92', 'loss/train': '0.68251', 'examples_per_second': '31.515', 'grad_norm': '34.5', 'counters/examples': 41024, 'counters/updates': 1282}
train stats after 41056 examples: {'rewards_train/chosen': '0.066904', 'rewards_train/rejected': '0.021816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045087', 'logps_train/rejected': '-136.56', 'logps_train/chosen': '-176.16', 'loss/train': '0.67774', 'examples_per_second': '24.321', 'grad_norm': '31.5', 'counters/examples': 41056, 'counters/updates': 1283}
train stats after 41088 examples: {'rewards_train/chosen': '0.0047587', 'rewards_train/rejected': '0.027488', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022729', 'logps_train/rejected': '-146.39', 'logps_train/chosen': '-148.05', 'loss/train': '0.71317', 'examples_per_second': '30.332', 'grad_norm': '46.75', 'counters/examples': 41088, 'counters/updates': 1284}
skipping logging after 41120 examples to avoid logging too frequently
train stats after 41152 examples: {'rewards_train/chosen': '0.066104', 'rewards_train/rejected': '0.016656', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049448', 'logps_train/rejected': '-105.39', 'logps_train/chosen': '-145.23', 'loss/train': '0.67799', 'examples_per_second': '34.657', 'grad_norm': '27.375', 'counters/examples': 41152, 'counters/updates': 1286}
skipping logging after 41184 examples to avoid logging too frequently
train stats after 41216 examples: {'rewards_train/chosen': '0.038684', 'rewards_train/rejected': '0.037893', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00079069', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-124.11', 'loss/train': '0.69605', 'examples_per_second': '37.013', 'grad_norm': '25.875', 'counters/examples': 41216, 'counters/updates': 1288}
train stats after 41248 examples: {'rewards_train/chosen': '0.031874', 'rewards_train/rejected': '0.021462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010412', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-157.43', 'loss/train': '0.69335', 'examples_per_second': '32.961', 'grad_norm': '52', 'counters/examples': 41248, 'counters/updates': 1289}
skipping logging after 41280 examples to avoid logging too frequently
train stats after 41312 examples: {'rewards_train/chosen': '0.12733', 'rewards_train/rejected': '0.045056', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082271', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-144.56', 'loss/train': '0.66321', 'examples_per_second': '32.361', 'grad_norm': '31.625', 'counters/examples': 41312, 'counters/updates': 1291}
skipping logging after 41344 examples to avoid logging too frequently
train stats after 41376 examples: {'rewards_train/chosen': '0.064688', 'rewards_train/rejected': '0.063161', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0015269', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-111.63', 'loss/train': '0.70428', 'examples_per_second': '30.981', 'grad_norm': '41', 'counters/examples': 41376, 'counters/updates': 1293}
train stats after 41408 examples: {'rewards_train/chosen': '0.043816', 'rewards_train/rejected': '0.016732', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027084', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-128.98', 'loss/train': '0.68225', 'examples_per_second': '32.013', 'grad_norm': '27.625', 'counters/examples': 41408, 'counters/updates': 1294}
train stats after 41440 examples: {'rewards_train/chosen': '0.060541', 'rewards_train/rejected': '0.090658', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.030118', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-139.09', 'loss/train': '0.72149', 'examples_per_second': '31.486', 'grad_norm': '37.75', 'counters/examples': 41440, 'counters/updates': 1295}
train stats after 41472 examples: {'rewards_train/chosen': '0.10816', 'rewards_train/rejected': '-0.02996', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13812', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-179.22', 'loss/train': '0.63694', 'examples_per_second': '30.787', 'grad_norm': '33.5', 'counters/examples': 41472, 'counters/updates': 1296}
skipping logging after 41504 examples to avoid logging too frequently
train stats after 41536 examples: {'rewards_train/chosen': '0.041406', 'rewards_train/rejected': '0.066805', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025398', 'logps_train/rejected': '-155.12', 'logps_train/chosen': '-148.88', 'loss/train': '0.7108', 'examples_per_second': '32.72', 'grad_norm': '33.75', 'counters/examples': 41536, 'counters/updates': 1298}
train stats after 41568 examples: {'rewards_train/chosen': '0.038889', 'rewards_train/rejected': '0.009695', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029194', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-124.32', 'loss/train': '0.68428', 'examples_per_second': '30.295', 'grad_norm': '31.125', 'counters/examples': 41568, 'counters/updates': 1299}
train stats after 41600 examples: {'rewards_train/chosen': '0.056768', 'rewards_train/rejected': '0.04447', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012298', 'logps_train/rejected': '-149.68', 'logps_train/chosen': '-136.09', 'loss/train': '0.69318', 'examples_per_second': '30.967', 'grad_norm': '41.5', 'counters/examples': 41600, 'counters/updates': 1300}
train stats after 41632 examples: {'rewards_train/chosen': '0.043156', 'rewards_train/rejected': '0.038533', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0046232', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-143.98', 'loss/train': '0.69834', 'examples_per_second': '30.391', 'grad_norm': '33.25', 'counters/examples': 41632, 'counters/updates': 1301}
train stats after 41664 examples: {'rewards_train/chosen': '-0.014097', 'rewards_train/rejected': '0.035852', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.049948', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-125.81', 'loss/train': '0.72288', 'examples_per_second': '31.448', 'grad_norm': '39.25', 'counters/examples': 41664, 'counters/updates': 1302}
train stats after 41696 examples: {'rewards_train/chosen': '0.069383', 'rewards_train/rejected': '0.086517', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '-0.017134', 'logps_train/rejected': '-128.81', 'logps_train/chosen': '-157.93', 'loss/train': '0.72222', 'examples_per_second': '31.515', 'grad_norm': '61', 'counters/examples': 41696, 'counters/updates': 1303}
train stats after 41728 examples: {'rewards_train/chosen': '0.0076399', 'rewards_train/rejected': '0.01026', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0026205', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-131.65', 'loss/train': '0.6985', 'examples_per_second': '30.189', 'grad_norm': '34.75', 'counters/examples': 41728, 'counters/updates': 1304}
skipping logging after 41760 examples to avoid logging too frequently
train stats after 41792 examples: {'rewards_train/chosen': '0.083785', 'rewards_train/rejected': '0.069814', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.013971', 'logps_train/rejected': '-163.41', 'logps_train/chosen': '-142.1', 'loss/train': '0.68954', 'examples_per_second': '31.516', 'grad_norm': '34.5', 'counters/examples': 41792, 'counters/updates': 1306}
train stats after 41824 examples: {'rewards_train/chosen': '0.011785', 'rewards_train/rejected': '0.011586', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00019907', 'logps_train/rejected': '-106.84', 'logps_train/chosen': '-126.62', 'loss/train': '0.69489', 'examples_per_second': '32.645', 'grad_norm': '25.375', 'counters/examples': 41824, 'counters/updates': 1307}
train stats after 41856 examples: {'rewards_train/chosen': '0.046047', 'rewards_train/rejected': '0.061479', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.015432', 'logps_train/rejected': '-127.57', 'logps_train/chosen': '-155.08', 'loss/train': '0.70727', 'examples_per_second': '31.512', 'grad_norm': '52.5', 'counters/examples': 41856, 'counters/updates': 1308}
train stats after 41888 examples: {'rewards_train/chosen': '0.048382', 'rewards_train/rejected': '0.022461', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02592', 'logps_train/rejected': '-93.083', 'logps_train/chosen': '-126.41', 'loss/train': '0.68297', 'examples_per_second': '32.023', 'grad_norm': '27.125', 'counters/examples': 41888, 'counters/updates': 1309}
train stats after 41920 examples: {'rewards_train/chosen': '0.13879', 'rewards_train/rejected': '0.077034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061754', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-139.77', 'loss/train': '0.66825', 'examples_per_second': '24.278', 'grad_norm': '33', 'counters/examples': 41920, 'counters/updates': 1310}
train stats after 41952 examples: {'rewards_train/chosen': '0.012844', 'rewards_train/rejected': '0.032512', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.019668', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-148.78', 'loss/train': '0.70724', 'examples_per_second': '31.01', 'grad_norm': '29.625', 'counters/examples': 41952, 'counters/updates': 1311}
skipping logging after 41984 examples to avoid logging too frequently
train stats after 42016 examples: {'rewards_train/chosen': '0.10001', 'rewards_train/rejected': '0.035196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064815', 'logps_train/rejected': '-110.38', 'logps_train/chosen': '-135.27', 'loss/train': '0.66758', 'examples_per_second': '30.142', 'grad_norm': '28.375', 'counters/examples': 42016, 'counters/updates': 1313}
train stats after 42048 examples: {'rewards_train/chosen': '0.039716', 'rewards_train/rejected': '0.037642', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.002074', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-142.84', 'loss/train': '0.69891', 'examples_per_second': '32.961', 'grad_norm': '30.125', 'counters/examples': 42048, 'counters/updates': 1314}
train stats after 42080 examples: {'rewards_train/chosen': '0.019432', 'rewards_train/rejected': '0.011917', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0075156', 'logps_train/rejected': '-146.05', 'logps_train/chosen': '-116.72', 'loss/train': '0.69478', 'examples_per_second': '31.485', 'grad_norm': '60.5', 'counters/examples': 42080, 'counters/updates': 1315}
train stats after 42112 examples: {'rewards_train/chosen': '0.10404', 'rewards_train/rejected': '0.010142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093897', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-129.25', 'loss/train': '0.65543', 'examples_per_second': '31.519', 'grad_norm': '24.625', 'counters/examples': 42112, 'counters/updates': 1316}
train stats after 42144 examples: {'rewards_train/chosen': '0.069426', 'rewards_train/rejected': '0.021326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0481', 'logps_train/rejected': '-129.7', 'logps_train/chosen': '-148.76', 'loss/train': '0.67529', 'examples_per_second': '31.534', 'grad_norm': '35', 'counters/examples': 42144, 'counters/updates': 1317}
train stats after 42176 examples: {'rewards_train/chosen': '0.070959', 'rewards_train/rejected': '-0.0059179', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076876', 'logps_train/rejected': '-180.35', 'logps_train/chosen': '-173.33', 'loss/train': '0.66356', 'examples_per_second': '31.512', 'grad_norm': '40.75', 'counters/examples': 42176, 'counters/updates': 1318}
train stats after 42208 examples: {'rewards_train/chosen': '0.032747', 'rewards_train/rejected': '0.015552', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.017194', 'logps_train/rejected': '-97.655', 'logps_train/chosen': '-151.69', 'loss/train': '0.68694', 'examples_per_second': '30.144', 'grad_norm': '25.25', 'counters/examples': 42208, 'counters/updates': 1319}
train stats after 42240 examples: {'rewards_train/chosen': '0.043321', 'rewards_train/rejected': '-0.017784', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061104', 'logps_train/rejected': '-114.99', 'logps_train/chosen': '-153.05', 'loss/train': '0.66939', 'examples_per_second': '30.349', 'grad_norm': '29.75', 'counters/examples': 42240, 'counters/updates': 1320}
train stats after 42272 examples: {'rewards_train/chosen': '0.0049928', 'rewards_train/rejected': '0.038365', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.033372', 'logps_train/rejected': '-122.67', 'logps_train/chosen': '-133.12', 'loss/train': '0.71541', 'examples_per_second': '30.858', 'grad_norm': '30.5', 'counters/examples': 42272, 'counters/updates': 1321}
train stats after 42304 examples: {'rewards_train/chosen': '0.03419', 'rewards_train/rejected': '0.044035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0098446', 'logps_train/rejected': '-142.16', 'logps_train/chosen': '-126.9', 'loss/train': '0.70209', 'examples_per_second': '32.448', 'grad_norm': '44.5', 'counters/examples': 42304, 'counters/updates': 1322}
skipping logging after 42336 examples to avoid logging too frequently
train stats after 42368 examples: {'rewards_train/chosen': '0.056332', 'rewards_train/rejected': '0.077285', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.020953', 'logps_train/rejected': '-137', 'logps_train/chosen': '-130.93', 'loss/train': '0.71061', 'examples_per_second': '36.058', 'grad_norm': '32.75', 'counters/examples': 42368, 'counters/updates': 1324}
train stats after 42400 examples: {'rewards_train/chosen': '0.079587', 'rewards_train/rejected': '0.026403', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053183', 'logps_train/rejected': '-143.4', 'logps_train/chosen': '-145.86', 'loss/train': '0.68279', 'examples_per_second': '31.881', 'grad_norm': '37', 'counters/examples': 42400, 'counters/updates': 1325}
train stats after 42432 examples: {'rewards_train/chosen': '0.034227', 'rewards_train/rejected': '0.031888', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0023391', 'logps_train/rejected': '-90.311', 'logps_train/chosen': '-117.92', 'loss/train': '0.69747', 'examples_per_second': '32.305', 'grad_norm': '27.875', 'counters/examples': 42432, 'counters/updates': 1326}
train stats after 42464 examples: {'rewards_train/chosen': '0.034032', 'rewards_train/rejected': '-0.013908', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04794', 'logps_train/rejected': '-113.28', 'logps_train/chosen': '-153.93', 'loss/train': '0.6764', 'examples_per_second': '31.175', 'grad_norm': '32', 'counters/examples': 42464, 'counters/updates': 1327}
train stats after 42496 examples: {'rewards_train/chosen': '0.047998', 'rewards_train/rejected': '0.026771', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021227', 'logps_train/rejected': '-101.76', 'logps_train/chosen': '-128.79', 'loss/train': '0.68628', 'examples_per_second': '31.23', 'grad_norm': '46', 'counters/examples': 42496, 'counters/updates': 1328}
train stats after 42528 examples: {'rewards_train/chosen': '-0.053364', 'rewards_train/rejected': '-0.0040888', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049276', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-150.63', 'loss/train': '0.72248', 'examples_per_second': '31.977', 'grad_norm': '38', 'counters/examples': 42528, 'counters/updates': 1329}
train stats after 42560 examples: {'rewards_train/chosen': '0.14428', 'rewards_train/rejected': '0.049134', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.095148', 'logps_train/rejected': '-142.79', 'logps_train/chosen': '-132.04', 'loss/train': '0.65393', 'examples_per_second': '31.542', 'grad_norm': '37', 'counters/examples': 42560, 'counters/updates': 1330}
train stats after 42592 examples: {'rewards_train/chosen': '0.10707', 'rewards_train/rejected': '0.0064385', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10063', 'logps_train/rejected': '-122.99', 'logps_train/chosen': '-142.08', 'loss/train': '0.64893', 'examples_per_second': '32.979', 'grad_norm': '27.75', 'counters/examples': 42592, 'counters/updates': 1331}
train stats after 42624 examples: {'rewards_train/chosen': '0.059968', 'rewards_train/rejected': '0.032448', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02752', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-136.47', 'loss/train': '0.68253', 'examples_per_second': '30.753', 'grad_norm': '26.875', 'counters/examples': 42624, 'counters/updates': 1332}
skipping logging after 42656 examples to avoid logging too frequently
train stats after 42688 examples: {'rewards_train/chosen': '0.041997', 'rewards_train/rejected': '0.03366', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083365', 'logps_train/rejected': '-133.37', 'logps_train/chosen': '-146.89', 'loss/train': '0.69625', 'examples_per_second': '33.314', 'grad_norm': '35.5', 'counters/examples': 42688, 'counters/updates': 1334}
skipping logging after 42720 examples to avoid logging too frequently
train stats after 42752 examples: {'rewards_train/chosen': '0.033066', 'rewards_train/rejected': '0.00061442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032451', 'logps_train/rejected': '-125.35', 'logps_train/chosen': '-163.87', 'loss/train': '0.68216', 'examples_per_second': '31.45', 'grad_norm': '28.25', 'counters/examples': 42752, 'counters/updates': 1336}
train stats after 42784 examples: {'rewards_train/chosen': '0.073436', 'rewards_train/rejected': '0.028552', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044884', 'logps_train/rejected': '-130.05', 'logps_train/chosen': '-121.85', 'loss/train': '0.67717', 'examples_per_second': '30.052', 'grad_norm': '30.625', 'counters/examples': 42784, 'counters/updates': 1337}
train stats after 42816 examples: {'rewards_train/chosen': '0.057285', 'rewards_train/rejected': '0.021686', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035599', 'logps_train/rejected': '-153.75', 'logps_train/chosen': '-168.24', 'loss/train': '0.68276', 'examples_per_second': '32.347', 'grad_norm': '37.25', 'counters/examples': 42816, 'counters/updates': 1338}
train stats after 42848 examples: {'rewards_train/chosen': '0.044328', 'rewards_train/rejected': '0.015462', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028866', 'logps_train/rejected': '-116.13', 'logps_train/chosen': '-103.92', 'loss/train': '0.68104', 'examples_per_second': '32.266', 'grad_norm': '24', 'counters/examples': 42848, 'counters/updates': 1339}
skipping logging after 42880 examples to avoid logging too frequently
train stats after 42912 examples: {'rewards_train/chosen': '0.14028', 'rewards_train/rejected': '0.032263', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10802', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-136.48', 'loss/train': '0.64894', 'examples_per_second': '31.791', 'grad_norm': '32.5', 'counters/examples': 42912, 'counters/updates': 1341}
train stats after 42944 examples: {'rewards_train/chosen': '0.063084', 'rewards_train/rejected': '-0.0090797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072163', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-137.01', 'loss/train': '0.66203', 'examples_per_second': '31.217', 'grad_norm': '25.125', 'counters/examples': 42944, 'counters/updates': 1342}
train stats after 42976 examples: {'rewards_train/chosen': '0.037135', 'rewards_train/rejected': '0.039166', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0020306', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-152.77', 'loss/train': '0.70086', 'examples_per_second': '31.472', 'grad_norm': '37', 'counters/examples': 42976, 'counters/updates': 1343}
train stats after 43008 examples: {'rewards_train/chosen': '0.10383', 'rewards_train/rejected': '0.025117', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078713', 'logps_train/rejected': '-141.59', 'logps_train/chosen': '-179.08', 'loss/train': '0.66999', 'examples_per_second': '31.496', 'grad_norm': '34', 'counters/examples': 43008, 'counters/updates': 1344}
train stats after 43040 examples: {'rewards_train/chosen': '0.05074', 'rewards_train/rejected': '0.054736', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0039959', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-148.35', 'loss/train': '0.70378', 'examples_per_second': '31.441', 'grad_norm': '32.75', 'counters/examples': 43040, 'counters/updates': 1345}
train stats after 43072 examples: {'rewards_train/chosen': '0.11053', 'rewards_train/rejected': '0.039256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071279', 'logps_train/rejected': '-128.75', 'logps_train/chosen': '-160.91', 'loss/train': '0.67095', 'examples_per_second': '32.123', 'grad_norm': '39.25', 'counters/examples': 43072, 'counters/updates': 1346}
train stats after 43104 examples: {'rewards_train/chosen': '0.055839', 'rewards_train/rejected': '0.083755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027916', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-124.89', 'loss/train': '0.71271', 'examples_per_second': '31.046', 'grad_norm': '31.375', 'counters/examples': 43104, 'counters/updates': 1347}
train stats after 43136 examples: {'rewards_train/chosen': '0.15395', 'rewards_train/rejected': '0.063518', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.090437', 'logps_train/rejected': '-124.48', 'logps_train/chosen': '-163.02', 'loss/train': '0.67906', 'examples_per_second': '32.645', 'grad_norm': '86.5', 'counters/examples': 43136, 'counters/updates': 1348}
skipping logging after 43168 examples to avoid logging too frequently
train stats after 43200 examples: {'rewards_train/chosen': '0.049422', 'rewards_train/rejected': '0.016865', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.032557', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-153.54', 'loss/train': '0.68466', 'examples_per_second': '31.482', 'grad_norm': '37', 'counters/examples': 43200, 'counters/updates': 1350}
train stats after 43232 examples: {'rewards_train/chosen': '0.0065083', 'rewards_train/rejected': '0.01258', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0060719', 'logps_train/rejected': '-91.829', 'logps_train/chosen': '-139.32', 'loss/train': '0.69897', 'examples_per_second': '30.883', 'grad_norm': '29.125', 'counters/examples': 43232, 'counters/updates': 1351}
train stats after 43264 examples: {'rewards_train/chosen': '0.0031167', 'rewards_train/rejected': '0.014616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011499', 'logps_train/rejected': '-152.66', 'logps_train/chosen': '-163.16', 'loss/train': '0.70433', 'examples_per_second': '30.523', 'grad_norm': '32.25', 'counters/examples': 43264, 'counters/updates': 1352}
skipping logging after 43296 examples to avoid logging too frequently
train stats after 43328 examples: {'rewards_train/chosen': '0.1845', 'rewards_train/rejected': '0.057079', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12742', 'logps_train/rejected': '-143.74', 'logps_train/chosen': '-171.16', 'loss/train': '0.6422', 'examples_per_second': '30.434', 'grad_norm': '48.75', 'counters/examples': 43328, 'counters/updates': 1354}
skipping logging after 43360 examples to avoid logging too frequently
train stats after 43392 examples: {'rewards_train/chosen': '0.14233', 'rewards_train/rejected': '0.057294', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08504', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-159.04', 'loss/train': '0.65992', 'examples_per_second': '30.404', 'grad_norm': '43.25', 'counters/examples': 43392, 'counters/updates': 1356}
train stats after 43424 examples: {'rewards_train/chosen': '0.0409', 'rewards_train/rejected': '0.060777', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019877', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-131.06', 'loss/train': '0.71187', 'examples_per_second': '32.005', 'grad_norm': '34', 'counters/examples': 43424, 'counters/updates': 1357}
train stats after 43456 examples: {'rewards_train/chosen': '0.080772', 'rewards_train/rejected': '-0.0015657', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082338', 'logps_train/rejected': '-152.79', 'logps_train/chosen': '-129.98', 'loss/train': '0.66026', 'examples_per_second': '31.522', 'grad_norm': '32.25', 'counters/examples': 43456, 'counters/updates': 1358}
train stats after 43488 examples: {'rewards_train/chosen': '0.044371', 'rewards_train/rejected': '-0.00096197', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045333', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-116.16', 'loss/train': '0.67324', 'examples_per_second': '30.058', 'grad_norm': '24', 'counters/examples': 43488, 'counters/updates': 1359}
train stats after 43520 examples: {'rewards_train/chosen': '0.084236', 'rewards_train/rejected': '0.0098609', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.074375', 'logps_train/rejected': '-122.9', 'logps_train/chosen': '-160.26', 'loss/train': '0.66012', 'examples_per_second': '31.036', 'grad_norm': '35', 'counters/examples': 43520, 'counters/updates': 1360}
train stats after 43552 examples: {'rewards_train/chosen': '0.06789', 'rewards_train/rejected': '0.097447', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029557', 'logps_train/rejected': '-164.53', 'logps_train/chosen': '-121.68', 'loss/train': '0.71192', 'examples_per_second': '31.493', 'grad_norm': '40.75', 'counters/examples': 43552, 'counters/updates': 1361}
train stats after 43584 examples: {'rewards_train/chosen': '0.088291', 'rewards_train/rejected': '0.012091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0762', 'logps_train/rejected': '-124.98', 'logps_train/chosen': '-151.58', 'loss/train': '0.66093', 'examples_per_second': '31.354', 'grad_norm': '32', 'counters/examples': 43584, 'counters/updates': 1362}
train stats after 43616 examples: {'rewards_train/chosen': '0.033598', 'rewards_train/rejected': '0.043269', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0096714', 'logps_train/rejected': '-77.113', 'logps_train/chosen': '-102.04', 'loss/train': '0.70256', 'examples_per_second': '30.815', 'grad_norm': '44.5', 'counters/examples': 43616, 'counters/updates': 1363}
skipping logging after 43648 examples to avoid logging too frequently
train stats after 43680 examples: {'rewards_train/chosen': '0.078294', 'rewards_train/rejected': '0.035406', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042888', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-123.27', 'loss/train': '0.67791', 'examples_per_second': '35.615', 'grad_norm': '51.75', 'counters/examples': 43680, 'counters/updates': 1365}
train stats after 43712 examples: {'rewards_train/chosen': '0.05378', 'rewards_train/rejected': '0.030112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023668', 'logps_train/rejected': '-130.78', 'logps_train/chosen': '-178.14', 'loss/train': '0.68975', 'examples_per_second': '31.485', 'grad_norm': '43.75', 'counters/examples': 43712, 'counters/updates': 1366}
train stats after 43744 examples: {'rewards_train/chosen': '0.050275', 'rewards_train/rejected': '0.0096138', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040661', 'logps_train/rejected': '-93.919', 'logps_train/chosen': '-138.36', 'loss/train': '0.68121', 'examples_per_second': '31.401', 'grad_norm': '35', 'counters/examples': 43744, 'counters/updates': 1367}
train stats after 43776 examples: {'rewards_train/chosen': '0.045074', 'rewards_train/rejected': '0.019427', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.025647', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-152.9', 'loss/train': '0.68371', 'examples_per_second': '32.976', 'grad_norm': '30', 'counters/examples': 43776, 'counters/updates': 1368}
skipping logging after 43808 examples to avoid logging too frequently
train stats after 43840 examples: {'rewards_train/chosen': '0.031178', 'rewards_train/rejected': '-0.012695', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043873', 'logps_train/rejected': '-165.41', 'logps_train/chosen': '-158.01', 'loss/train': '0.67895', 'examples_per_second': '31.127', 'grad_norm': '34', 'counters/examples': 43840, 'counters/updates': 1370}
skipping logging after 43872 examples to avoid logging too frequently
train stats after 43904 examples: {'rewards_train/chosen': '0.081365', 'rewards_train/rejected': '0.031868', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049497', 'logps_train/rejected': '-93.999', 'logps_train/chosen': '-118.43', 'loss/train': '0.67765', 'examples_per_second': '34.114', 'grad_norm': '41.25', 'counters/examples': 43904, 'counters/updates': 1372}
train stats after 43936 examples: {'rewards_train/chosen': '0.014551', 'rewards_train/rejected': '0.030421', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01587', 'logps_train/rejected': '-162.32', 'logps_train/chosen': '-193.13', 'loss/train': '0.70682', 'examples_per_second': '30.832', 'grad_norm': '40.75', 'counters/examples': 43936, 'counters/updates': 1373}
train stats after 43968 examples: {'rewards_train/chosen': '0.072354', 'rewards_train/rejected': '0.066745', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0056093', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-128.78', 'loss/train': '0.69375', 'examples_per_second': '32.967', 'grad_norm': '35', 'counters/examples': 43968, 'counters/updates': 1374}
train stats after 44000 examples: {'rewards_train/chosen': '0.083105', 'rewards_train/rejected': '-0.010138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.093243', 'logps_train/rejected': '-149.93', 'logps_train/chosen': '-171.76', 'loss/train': '0.65757', 'examples_per_second': '31.392', 'grad_norm': '31.875', 'counters/examples': 44000, 'counters/updates': 1375}
train stats after 44032 examples: {'rewards_train/chosen': '0.069366', 'rewards_train/rejected': '0.042265', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027101', 'logps_train/rejected': '-129', 'logps_train/chosen': '-137.98', 'loss/train': '0.69008', 'examples_per_second': '31.527', 'grad_norm': '27.375', 'counters/examples': 44032, 'counters/updates': 1376}
skipping logging after 44064 examples to avoid logging too frequently
train stats after 44096 examples: {'rewards_train/chosen': '-0.023317', 'rewards_train/rejected': '-0.0076067', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01571', 'logps_train/rejected': '-99.035', 'logps_train/chosen': '-105.7', 'loss/train': '0.70461', 'examples_per_second': '30.797', 'grad_norm': '27', 'counters/examples': 44096, 'counters/updates': 1378}
train stats after 44128 examples: {'rewards_train/chosen': '0.21821', 'rewards_train/rejected': '0.041528', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17669', 'logps_train/rejected': '-143.4', 'logps_train/chosen': '-192.58', 'loss/train': '0.63141', 'examples_per_second': '31.453', 'grad_norm': '38.75', 'counters/examples': 44128, 'counters/updates': 1379}
train stats after 44160 examples: {'rewards_train/chosen': '0.085165', 'rewards_train/rejected': '0.0086792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.076485', 'logps_train/rejected': '-110.28', 'logps_train/chosen': '-122.56', 'loss/train': '0.66335', 'examples_per_second': '31.585', 'grad_norm': '33.5', 'counters/examples': 44160, 'counters/updates': 1380}
train stats after 44192 examples: {'rewards_train/chosen': '0.10746', 'rewards_train/rejected': '0.0063023', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10116', 'logps_train/rejected': '-88.954', 'logps_train/chosen': '-144.63', 'loss/train': '0.64856', 'examples_per_second': '31.527', 'grad_norm': '26', 'counters/examples': 44192, 'counters/updates': 1381}
train stats after 44224 examples: {'rewards_train/chosen': '0.029801', 'rewards_train/rejected': '0.013369', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016433', 'logps_train/rejected': '-103.07', 'logps_train/chosen': '-176.29', 'loss/train': '0.68863', 'examples_per_second': '31.435', 'grad_norm': '42.75', 'counters/examples': 44224, 'counters/updates': 1382}
train stats after 44256 examples: {'rewards_train/chosen': '0.080777', 'rewards_train/rejected': '0.053789', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026988', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-146.22', 'loss/train': '0.68418', 'examples_per_second': '32.609', 'grad_norm': '27.75', 'counters/examples': 44256, 'counters/updates': 1383}
train stats after 44288 examples: {'rewards_train/chosen': '0.091403', 'rewards_train/rejected': '0.0050219', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086382', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-123.52', 'loss/train': '0.66788', 'examples_per_second': '31.656', 'grad_norm': '33.75', 'counters/examples': 44288, 'counters/updates': 1384}
skipping logging after 44320 examples to avoid logging too frequently
train stats after 44352 examples: {'rewards_train/chosen': '0.063911', 'rewards_train/rejected': '0.037292', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.026619', 'logps_train/rejected': '-122.04', 'logps_train/chosen': '-128.74', 'loss/train': '0.68477', 'examples_per_second': '30.073', 'grad_norm': '30.125', 'counters/examples': 44352, 'counters/updates': 1386}
train stats after 44384 examples: {'rewards_train/chosen': '0.096212', 'rewards_train/rejected': '0.095644', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.00056817', 'logps_train/rejected': '-121.03', 'logps_train/chosen': '-129.68', 'loss/train': '0.70534', 'examples_per_second': '31.442', 'grad_norm': '48', 'counters/examples': 44384, 'counters/updates': 1387}
train stats after 44416 examples: {'rewards_train/chosen': '0.10406', 'rewards_train/rejected': '0.017117', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086939', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-152.82', 'loss/train': '0.65991', 'examples_per_second': '29.968', 'grad_norm': '26.125', 'counters/examples': 44416, 'counters/updates': 1388}
train stats after 44448 examples: {'rewards_train/chosen': '0.061758', 'rewards_train/rejected': '0.0092404', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052517', 'logps_train/rejected': '-120.29', 'logps_train/chosen': '-144.53', 'loss/train': '0.67093', 'examples_per_second': '32.191', 'grad_norm': '28.125', 'counters/examples': 44448, 'counters/updates': 1389}
train stats after 44480 examples: {'rewards_train/chosen': '0.041925', 'rewards_train/rejected': '0.013395', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028529', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-145.61', 'loss/train': '0.68792', 'examples_per_second': '32.695', 'grad_norm': '28.5', 'counters/examples': 44480, 'counters/updates': 1390}
skipping logging after 44512 examples to avoid logging too frequently
train stats after 44544 examples: {'rewards_train/chosen': '0.035997', 'rewards_train/rejected': '0.0052366', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.03076', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-121.89', 'loss/train': '0.67977', 'examples_per_second': '31.602', 'grad_norm': '27.375', 'counters/examples': 44544, 'counters/updates': 1392}
train stats after 44576 examples: {'rewards_train/chosen': '0.087244', 'rewards_train/rejected': '0.070765', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016479', 'logps_train/rejected': '-134.47', 'logps_train/chosen': '-178.03', 'loss/train': '0.69071', 'examples_per_second': '31.26', 'grad_norm': '44.25', 'counters/examples': 44576, 'counters/updates': 1393}
train stats after 44608 examples: {'rewards_train/chosen': '0.15757', 'rewards_train/rejected': '0.030468', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1271', 'logps_train/rejected': '-105.56', 'logps_train/chosen': '-175.59', 'loss/train': '0.64273', 'examples_per_second': '30.649', 'grad_norm': '34', 'counters/examples': 44608, 'counters/updates': 1394}
skipping logging after 44640 examples to avoid logging too frequently
train stats after 44672 examples: {'rewards_train/chosen': '0.055629', 'rewards_train/rejected': '0.033097', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022532', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-149.68', 'loss/train': '0.68545', 'examples_per_second': '31.521', 'grad_norm': '35.75', 'counters/examples': 44672, 'counters/updates': 1396}
skipping logging after 44704 examples to avoid logging too frequently
train stats after 44736 examples: {'rewards_train/chosen': '0.0514', 'rewards_train/rejected': '0.065279', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013879', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-143', 'loss/train': '0.70294', 'examples_per_second': '31.019', 'grad_norm': '28.875', 'counters/examples': 44736, 'counters/updates': 1398}
train stats after 44768 examples: {'rewards_train/chosen': '0.070953', 'rewards_train/rejected': '0.016765', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054188', 'logps_train/rejected': '-137.5', 'logps_train/chosen': '-136.31', 'loss/train': '0.67173', 'examples_per_second': '31.385', 'grad_norm': '47.25', 'counters/examples': 44768, 'counters/updates': 1399}
train stats after 44800 examples: {'rewards_train/chosen': '0.11798', 'rewards_train/rejected': '0.025227', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092752', 'logps_train/rejected': '-150.95', 'logps_train/chosen': '-196.69', 'loss/train': '0.65797', 'examples_per_second': '31.421', 'grad_norm': '40.5', 'counters/examples': 44800, 'counters/updates': 1400}
train stats after 44832 examples: {'rewards_train/chosen': '0.07823', 'rewards_train/rejected': '0.022404', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055826', 'logps_train/rejected': '-153.8', 'logps_train/chosen': '-145.83', 'loss/train': '0.67508', 'examples_per_second': '30.083', 'grad_norm': '36', 'counters/examples': 44832, 'counters/updates': 1401}
train stats after 44864 examples: {'rewards_train/chosen': '0.078004', 'rewards_train/rejected': '0.048498', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029506', 'logps_train/rejected': '-155.32', 'logps_train/chosen': '-151.46', 'loss/train': '0.68485', 'examples_per_second': '31.962', 'grad_norm': '39', 'counters/examples': 44864, 'counters/updates': 1402}
train stats after 44896 examples: {'rewards_train/chosen': '0.012891', 'rewards_train/rejected': '0.066657', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.053767', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-147.73', 'loss/train': '0.73254', 'examples_per_second': '31.495', 'grad_norm': '41.5', 'counters/examples': 44896, 'counters/updates': 1403}
train stats after 44928 examples: {'rewards_train/chosen': '0.04552', 'rewards_train/rejected': '0.087442', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041922', 'logps_train/rejected': '-185.87', 'logps_train/chosen': '-158.27', 'loss/train': '0.72107', 'examples_per_second': '31.27', 'grad_norm': '48.5', 'counters/examples': 44928, 'counters/updates': 1404}
train stats after 44960 examples: {'rewards_train/chosen': '0.045415', 'rewards_train/rejected': '0.019719', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025697', 'logps_train/rejected': '-146.61', 'logps_train/chosen': '-134.7', 'loss/train': '0.68517', 'examples_per_second': '31.618', 'grad_norm': '32.75', 'counters/examples': 44960, 'counters/updates': 1405}
skipping logging after 44992 examples to avoid logging too frequently
train stats after 45024 examples: {'rewards_train/chosen': '0.094759', 'rewards_train/rejected': '0.079649', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01511', 'logps_train/rejected': '-142.82', 'logps_train/chosen': '-167.61', 'loss/train': '0.69176', 'examples_per_second': '31.451', 'grad_norm': '51', 'counters/examples': 45024, 'counters/updates': 1407}
train stats after 45056 examples: {'rewards_train/chosen': '0.073024', 'rewards_train/rejected': '0.016062', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056962', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-130.01', 'loss/train': '0.67223', 'examples_per_second': '32.221', 'grad_norm': '35.75', 'counters/examples': 45056, 'counters/updates': 1408}
train stats after 45088 examples: {'rewards_train/chosen': '-0.019451', 'rewards_train/rejected': '0.051278', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.070728', 'logps_train/rejected': '-109.45', 'logps_train/chosen': '-138.2', 'loss/train': '0.74021', 'examples_per_second': '29.968', 'grad_norm': '44.75', 'counters/examples': 45088, 'counters/updates': 1409}
train stats after 45120 examples: {'rewards_train/chosen': '0.075162', 'rewards_train/rejected': '0.055046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020116', 'logps_train/rejected': '-102.67', 'logps_train/chosen': '-118.51', 'loss/train': '0.68626', 'examples_per_second': '32.809', 'grad_norm': '26', 'counters/examples': 45120, 'counters/updates': 1410}
train stats after 45152 examples: {'rewards_train/chosen': '0.043527', 'rewards_train/rejected': '0.049871', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0063441', 'logps_train/rejected': '-112.35', 'logps_train/chosen': '-118.23', 'loss/train': '0.70113', 'examples_per_second': '32.438', 'grad_norm': '34.5', 'counters/examples': 45152, 'counters/updates': 1411}
train stats after 45184 examples: {'rewards_train/chosen': '0.047319', 'rewards_train/rejected': '0.091521', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044202', 'logps_train/rejected': '-107.37', 'logps_train/chosen': '-139.89', 'loss/train': '0.72506', 'examples_per_second': '30.758', 'grad_norm': '44.75', 'counters/examples': 45184, 'counters/updates': 1412}
train stats after 45216 examples: {'rewards_train/chosen': '0.081652', 'rewards_train/rejected': '0.031664', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049988', 'logps_train/rejected': '-137.51', 'logps_train/chosen': '-136.9', 'loss/train': '0.67431', 'examples_per_second': '30.484', 'grad_norm': '27.625', 'counters/examples': 45216, 'counters/updates': 1413}
train stats after 45248 examples: {'rewards_train/chosen': '0.065828', 'rewards_train/rejected': '0.050513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.015316', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-120', 'loss/train': '0.69066', 'examples_per_second': '32.337', 'grad_norm': '39.5', 'counters/examples': 45248, 'counters/updates': 1414}
train stats after 45280 examples: {'rewards_train/chosen': '0.11782', 'rewards_train/rejected': '0.037324', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.080494', 'logps_train/rejected': '-100.89', 'logps_train/chosen': '-125.67', 'loss/train': '0.65805', 'examples_per_second': '30.938', 'grad_norm': '26.625', 'counters/examples': 45280, 'counters/updates': 1415}
train stats after 45312 examples: {'rewards_train/chosen': '0.07497', 'rewards_train/rejected': '-0.024619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099589', 'logps_train/rejected': '-147.42', 'logps_train/chosen': '-162.43', 'loss/train': '0.65985', 'examples_per_second': '31.89', 'grad_norm': '37.5', 'counters/examples': 45312, 'counters/updates': 1416}
skipping logging after 45344 examples to avoid logging too frequently
train stats after 45376 examples: {'rewards_train/chosen': '0.0070204', 'rewards_train/rejected': '0.041027', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034007', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-135.42', 'loss/train': '0.71378', 'examples_per_second': '31.461', 'grad_norm': '63.75', 'counters/examples': 45376, 'counters/updates': 1418}
train stats after 45408 examples: {'rewards_train/chosen': '0.094241', 'rewards_train/rejected': '0.024268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069973', 'logps_train/rejected': '-130.2', 'logps_train/chosen': '-135.93', 'loss/train': '0.69116', 'examples_per_second': '32.19', 'grad_norm': '50', 'counters/examples': 45408, 'counters/updates': 1419}
train stats after 45440 examples: {'rewards_train/chosen': '0.062446', 'rewards_train/rejected': '0.047568', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014878', 'logps_train/rejected': '-108.76', 'logps_train/chosen': '-117.23', 'loss/train': '0.70195', 'examples_per_second': '31.12', 'grad_norm': '28.75', 'counters/examples': 45440, 'counters/updates': 1420}
train stats after 45472 examples: {'rewards_train/chosen': '0.020131', 'rewards_train/rejected': '0.006605', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013526', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-113.57', 'loss/train': '0.69143', 'examples_per_second': '30.496', 'grad_norm': '27.125', 'counters/examples': 45472, 'counters/updates': 1421}
train stats after 45504 examples: {'rewards_train/chosen': '0.14399', 'rewards_train/rejected': '0.071891', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0721', 'logps_train/rejected': '-137.26', 'logps_train/chosen': '-191.27', 'loss/train': '0.66412', 'examples_per_second': '32.686', 'grad_norm': '41.25', 'counters/examples': 45504, 'counters/updates': 1422}
skipping logging after 45536 examples to avoid logging too frequently
train stats after 45568 examples: {'rewards_train/chosen': '0.01908', 'rewards_train/rejected': '0.0099414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0091389', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-115.3', 'loss/train': '0.69188', 'examples_per_second': '32.933', 'grad_norm': '25.5', 'counters/examples': 45568, 'counters/updates': 1424}
train stats after 45600 examples: {'rewards_train/chosen': '0.089048', 'rewards_train/rejected': '0.13396', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.044914', 'logps_train/rejected': '-104.02', 'logps_train/chosen': '-145.81', 'loss/train': '0.75756', 'examples_per_second': '31.508', 'grad_norm': '51', 'counters/examples': 45600, 'counters/updates': 1425}
skipping logging after 45632 examples to avoid logging too frequently
train stats after 45664 examples: {'rewards_train/chosen': '-0.0020238', 'rewards_train/rejected': '0.035693', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.037717', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-137.84', 'loss/train': '0.71671', 'examples_per_second': '31.47', 'grad_norm': '35.25', 'counters/examples': 45664, 'counters/updates': 1427}
train stats after 45696 examples: {'rewards_train/chosen': '0.10304', 'rewards_train/rejected': '0.094242', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088027', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-127.7', 'loss/train': '0.69905', 'examples_per_second': '32.437', 'grad_norm': '39', 'counters/examples': 45696, 'counters/updates': 1428}
train stats after 45728 examples: {'rewards_train/chosen': '0.02792', 'rewards_train/rejected': '0.063881', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.035961', 'logps_train/rejected': '-130.48', 'logps_train/chosen': '-169.13', 'loss/train': '0.7157', 'examples_per_second': '31.829', 'grad_norm': '29.625', 'counters/examples': 45728, 'counters/updates': 1429}
train stats after 45760 examples: {'rewards_train/chosen': '0.099214', 'rewards_train/rejected': '0.00098236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098232', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-159.1', 'loss/train': '0.64988', 'examples_per_second': '30.704', 'grad_norm': '26.25', 'counters/examples': 45760, 'counters/updates': 1430}
train stats after 45792 examples: {'rewards_train/chosen': '0.10541', 'rewards_train/rejected': '0.07626', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029151', 'logps_train/rejected': '-92.225', 'logps_train/chosen': '-130.11', 'loss/train': '0.70341', 'examples_per_second': '31.272', 'grad_norm': '43.25', 'counters/examples': 45792, 'counters/updates': 1431}
train stats after 45824 examples: {'rewards_train/chosen': '0.018423', 'rewards_train/rejected': '0.0039376', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014486', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-139.33', 'loss/train': '0.69193', 'examples_per_second': '31.502', 'grad_norm': '29.375', 'counters/examples': 45824, 'counters/updates': 1432}
train stats after 45856 examples: {'rewards_train/chosen': '0.047466', 'rewards_train/rejected': '0.019575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027892', 'logps_train/rejected': '-139.92', 'logps_train/chosen': '-160.67', 'loss/train': '0.68438', 'examples_per_second': '31.271', 'grad_norm': '30', 'counters/examples': 45856, 'counters/updates': 1433}
train stats after 45888 examples: {'rewards_train/chosen': '0.15189', 'rewards_train/rejected': '0.0090246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14286', 'logps_train/rejected': '-135.7', 'logps_train/chosen': '-159.69', 'loss/train': '0.64327', 'examples_per_second': '29.947', 'grad_norm': '30.625', 'counters/examples': 45888, 'counters/updates': 1434}
train stats after 45920 examples: {'rewards_train/chosen': '0.07419', 'rewards_train/rejected': '0.064501', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0096895', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-177.16', 'loss/train': '0.70295', 'examples_per_second': '31.996', 'grad_norm': '37.75', 'counters/examples': 45920, 'counters/updates': 1435}
train stats after 45952 examples: {'rewards_train/chosen': '0.1022', 'rewards_train/rejected': '0.042638', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059561', 'logps_train/rejected': '-93.709', 'logps_train/chosen': '-107.93', 'loss/train': '0.66694', 'examples_per_second': '32.595', 'grad_norm': '22.375', 'counters/examples': 45952, 'counters/updates': 1436}
train stats after 45984 examples: {'rewards_train/chosen': '0.11192', 'rewards_train/rejected': '0.025082', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.086841', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-183.14', 'loss/train': '0.67058', 'examples_per_second': '31.297', 'grad_norm': '68', 'counters/examples': 45984, 'counters/updates': 1437}
train stats after 46016 examples: {'rewards_train/chosen': '0.072968', 'rewards_train/rejected': '0.15943', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.086464', 'logps_train/rejected': '-141.47', 'logps_train/chosen': '-162.77', 'loss/train': '0.76791', 'examples_per_second': '31.601', 'grad_norm': '70.5', 'counters/examples': 46016, 'counters/updates': 1438}
train stats after 46048 examples: {'rewards_train/chosen': '0.033341', 'rewards_train/rejected': '0.015342', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017999', 'logps_train/rejected': '-145.5', 'logps_train/chosen': '-125.19', 'loss/train': '0.6871', 'examples_per_second': '30.029', 'grad_norm': '40', 'counters/examples': 46048, 'counters/updates': 1439}
train stats after 46080 examples: {'rewards_train/chosen': '0.040119', 'rewards_train/rejected': '0.019669', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020449', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-142.24', 'loss/train': '0.6916', 'examples_per_second': '32.517', 'grad_norm': '31.875', 'counters/examples': 46080, 'counters/updates': 1440}
skipping logging after 46112 examples to avoid logging too frequently
train stats after 46144 examples: {'rewards_train/chosen': '0.05264', 'rewards_train/rejected': '0.075792', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023151', 'logps_train/rejected': '-139.36', 'logps_train/chosen': '-183.09', 'loss/train': '0.71091', 'examples_per_second': '31.422', 'grad_norm': '36.5', 'counters/examples': 46144, 'counters/updates': 1442}
train stats after 46176 examples: {'rewards_train/chosen': '0.056122', 'rewards_train/rejected': '0.042958', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013164', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-153.57', 'loss/train': '0.69348', 'examples_per_second': '32.421', 'grad_norm': '32', 'counters/examples': 46176, 'counters/updates': 1443}
train stats after 46208 examples: {'rewards_train/chosen': '0.081884', 'rewards_train/rejected': '-0.022809', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10469', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-157.48', 'loss/train': '0.64544', 'examples_per_second': '31.447', 'grad_norm': '35.25', 'counters/examples': 46208, 'counters/updates': 1444}
train stats after 46240 examples: {'rewards_train/chosen': '0.028972', 'rewards_train/rejected': '0.016286', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012686', 'logps_train/rejected': '-140.83', 'logps_train/chosen': '-125.17', 'loss/train': '0.69084', 'examples_per_second': '31.437', 'grad_norm': '32.25', 'counters/examples': 46240, 'counters/updates': 1445}
train stats after 46272 examples: {'rewards_train/chosen': '0.069137', 'rewards_train/rejected': '0.027633', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041504', 'logps_train/rejected': '-147.04', 'logps_train/chosen': '-160.1', 'loss/train': '0.68088', 'examples_per_second': '31.428', 'grad_norm': '51', 'counters/examples': 46272, 'counters/updates': 1446}
skipping logging after 46304 examples to avoid logging too frequently
train stats after 46336 examples: {'rewards_train/chosen': '0.045572', 'rewards_train/rejected': '0.015094', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030477', 'logps_train/rejected': '-123.03', 'logps_train/chosen': '-150.9', 'loss/train': '0.68243', 'examples_per_second': '31.665', 'grad_norm': '27.75', 'counters/examples': 46336, 'counters/updates': 1448}
skipping logging after 46368 examples to avoid logging too frequently
train stats after 46400 examples: {'rewards_train/chosen': '0.12243', 'rewards_train/rejected': '0.049467', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07296', 'logps_train/rejected': '-193.18', 'logps_train/chosen': '-152.27', 'loss/train': '0.6701', 'examples_per_second': '32.854', 'grad_norm': '48', 'counters/examples': 46400, 'counters/updates': 1450}
train stats after 46432 examples: {'rewards_train/chosen': '0.048796', 'rewards_train/rejected': '0.063245', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014449', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-124.17', 'loss/train': '0.70927', 'examples_per_second': '23.906', 'grad_norm': '36.25', 'counters/examples': 46432, 'counters/updates': 1451}
train stats after 46464 examples: {'rewards_train/chosen': '0.059187', 'rewards_train/rejected': '0.047326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.011861', 'logps_train/rejected': '-106.05', 'logps_train/chosen': '-114.63', 'loss/train': '0.69205', 'examples_per_second': '32.137', 'grad_norm': '24.125', 'counters/examples': 46464, 'counters/updates': 1452}
train stats after 46496 examples: {'rewards_train/chosen': '-0.024955', 'rewards_train/rejected': '0.028989', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.053943', 'logps_train/rejected': '-157.6', 'logps_train/chosen': '-169.15', 'loss/train': '0.73082', 'examples_per_second': '31.487', 'grad_norm': '44.25', 'counters/examples': 46496, 'counters/updates': 1453}
train stats after 46528 examples: {'rewards_train/chosen': '0.091945', 'rewards_train/rejected': '0.042832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049112', 'logps_train/rejected': '-94.243', 'logps_train/chosen': '-147.59', 'loss/train': '0.67666', 'examples_per_second': '27.614', 'grad_norm': '33.75', 'counters/examples': 46528, 'counters/updates': 1454}
train stats after 46560 examples: {'rewards_train/chosen': '0.098532', 'rewards_train/rejected': '-0.0089311', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10746', 'logps_train/rejected': '-154.31', 'logps_train/chosen': '-127.04', 'loss/train': '0.64879', 'examples_per_second': '30.037', 'grad_norm': '35.25', 'counters/examples': 46560, 'counters/updates': 1455}
train stats after 46592 examples: {'rewards_train/chosen': '0.031722', 'rewards_train/rejected': '-0.0096963', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041418', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-151.97', 'loss/train': '0.67776', 'examples_per_second': '31.449', 'grad_norm': '50', 'counters/examples': 46592, 'counters/updates': 1456}
train stats after 46624 examples: {'rewards_train/chosen': '0.061752', 'rewards_train/rejected': '0.0395', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022252', 'logps_train/rejected': '-106.58', 'logps_train/chosen': '-154.41', 'loss/train': '0.68821', 'examples_per_second': '31.384', 'grad_norm': '73', 'counters/examples': 46624, 'counters/updates': 1457}
train stats after 46656 examples: {'rewards_train/chosen': '0.099634', 'rewards_train/rejected': '0.048161', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051473', 'logps_train/rejected': '-92.085', 'logps_train/chosen': '-119.78', 'loss/train': '0.67585', 'examples_per_second': '31.013', 'grad_norm': '29.375', 'counters/examples': 46656, 'counters/updates': 1458}
skipping logging after 46688 examples to avoid logging too frequently
train stats after 46720 examples: {'rewards_train/chosen': '0.10082', 'rewards_train/rejected': '0.039806', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061013', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-188.74', 'loss/train': '0.67416', 'examples_per_second': '30.493', 'grad_norm': '31.75', 'counters/examples': 46720, 'counters/updates': 1460}
train stats after 46752 examples: {'rewards_train/chosen': '0.081586', 'rewards_train/rejected': '0.031429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050157', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-152.99', 'loss/train': '0.67395', 'examples_per_second': '31.497', 'grad_norm': '30.625', 'counters/examples': 46752, 'counters/updates': 1461}
train stats after 46784 examples: {'rewards_train/chosen': '0.038662', 'rewards_train/rejected': '0.081668', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043006', 'logps_train/rejected': '-165.85', 'logps_train/chosen': '-157.36', 'loss/train': '0.73228', 'examples_per_second': '32.435', 'grad_norm': '48.75', 'counters/examples': 46784, 'counters/updates': 1462}
train stats after 46816 examples: {'rewards_train/chosen': '0.021495', 'rewards_train/rejected': '0.0088149', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01268', 'logps_train/rejected': '-98.894', 'logps_train/chosen': '-147.25', 'loss/train': '0.69056', 'examples_per_second': '30.125', 'grad_norm': '25.375', 'counters/examples': 46816, 'counters/updates': 1463}
skipping logging after 46848 examples to avoid logging too frequently
train stats after 46880 examples: {'rewards_train/chosen': '0.11295', 'rewards_train/rejected': '0.038558', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074395', 'logps_train/rejected': '-131.3', 'logps_train/chosen': '-151.86', 'loss/train': '0.66744', 'examples_per_second': '29.926', 'grad_norm': '45', 'counters/examples': 46880, 'counters/updates': 1465}
train stats after 46912 examples: {'rewards_train/chosen': '0.1583', 'rewards_train/rejected': '0.094199', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064106', 'logps_train/rejected': '-143.74', 'logps_train/chosen': '-189.5', 'loss/train': '0.67118', 'examples_per_second': '31.471', 'grad_norm': '36.25', 'counters/examples': 46912, 'counters/updates': 1466}
train stats after 46944 examples: {'rewards_train/chosen': '0.04453', 'rewards_train/rejected': '-0.020131', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064661', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-144.97', 'loss/train': '0.66634', 'examples_per_second': '30.406', 'grad_norm': '27', 'counters/examples': 46944, 'counters/updates': 1467}
train stats after 46976 examples: {'rewards_train/chosen': '0.07128', 'rewards_train/rejected': '0.036643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034638', 'logps_train/rejected': '-115.72', 'logps_train/chosen': '-152.91', 'loss/train': '0.68421', 'examples_per_second': '32.547', 'grad_norm': '51', 'counters/examples': 46976, 'counters/updates': 1468}
train stats after 47008 examples: {'rewards_train/chosen': '0.086215', 'rewards_train/rejected': '0.039801', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046414', 'logps_train/rejected': '-154.75', 'logps_train/chosen': '-161.42', 'loss/train': '0.67578', 'examples_per_second': '31.715', 'grad_norm': '28.125', 'counters/examples': 47008, 'counters/updates': 1469}
train stats after 47040 examples: {'rewards_train/chosen': '0.093669', 'rewards_train/rejected': '0.033208', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060461', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-136.48', 'loss/train': '0.66659', 'examples_per_second': '29.953', 'grad_norm': '28.25', 'counters/examples': 47040, 'counters/updates': 1470}
train stats after 47072 examples: {'rewards_train/chosen': '0.088555', 'rewards_train/rejected': '0.040285', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04827', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-144.68', 'loss/train': '0.6758', 'examples_per_second': '31.617', 'grad_norm': '33.5', 'counters/examples': 47072, 'counters/updates': 1471}
train stats after 47104 examples: {'rewards_train/chosen': '0.059456', 'rewards_train/rejected': '0.015818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043638', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-159.6', 'loss/train': '0.68021', 'examples_per_second': '31.472', 'grad_norm': '37.25', 'counters/examples': 47104, 'counters/updates': 1472}
train stats after 47136 examples: {'rewards_train/chosen': '0.05621', 'rewards_train/rejected': '0.022718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033492', 'logps_train/rejected': '-113.51', 'logps_train/chosen': '-145.14', 'loss/train': '0.68152', 'examples_per_second': '30.812', 'grad_norm': '30.25', 'counters/examples': 47136, 'counters/updates': 1473}
train stats after 47168 examples: {'rewards_train/chosen': '-0.01138', 'rewards_train/rejected': '0.028884', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.040264', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-131.74', 'loss/train': '0.71727', 'examples_per_second': '32.506', 'grad_norm': '31.625', 'counters/examples': 47168, 'counters/updates': 1474}
train stats after 47200 examples: {'rewards_train/chosen': '0.048765', 'rewards_train/rejected': '-0.015537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064302', 'logps_train/rejected': '-102.41', 'logps_train/chosen': '-109.44', 'loss/train': '0.66592', 'examples_per_second': '31.486', 'grad_norm': '27.375', 'counters/examples': 47200, 'counters/updates': 1475}
skipping logging after 47232 examples to avoid logging too frequently
train stats after 47264 examples: {'rewards_train/chosen': '0.062383', 'rewards_train/rejected': '0.044984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.017399', 'logps_train/rejected': '-141.05', 'logps_train/chosen': '-167.27', 'loss/train': '0.69099', 'examples_per_second': '30.531', 'grad_norm': '39', 'counters/examples': 47264, 'counters/updates': 1477}
skipping logging after 47296 examples to avoid logging too frequently
train stats after 47328 examples: {'rewards_train/chosen': '0.081668', 'rewards_train/rejected': '0.056645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025023', 'logps_train/rejected': '-132.33', 'logps_train/chosen': '-172.65', 'loss/train': '0.68634', 'examples_per_second': '31.554', 'grad_norm': '36.5', 'counters/examples': 47328, 'counters/updates': 1479}
skipping logging after 47360 examples to avoid logging too frequently
train stats after 47392 examples: {'rewards_train/chosen': '0.053542', 'rewards_train/rejected': '0.0059339', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047608', 'logps_train/rejected': '-112.11', 'logps_train/chosen': '-151.65', 'loss/train': '0.67432', 'examples_per_second': '31.03', 'grad_norm': '27.875', 'counters/examples': 47392, 'counters/updates': 1481}
skipping logging after 47424 examples to avoid logging too frequently
train stats after 47456 examples: {'rewards_train/chosen': '0.06799', 'rewards_train/rejected': '0.02684', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04115', 'logps_train/rejected': '-129.37', 'logps_train/chosen': '-146.21', 'loss/train': '0.68144', 'examples_per_second': '31.521', 'grad_norm': '29.875', 'counters/examples': 47456, 'counters/updates': 1483}
train stats after 47488 examples: {'rewards_train/chosen': '0.032121', 'rewards_train/rejected': '0.029383', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0027378', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-175.4', 'loss/train': '0.69539', 'examples_per_second': '24.723', 'grad_norm': '34.5', 'counters/examples': 47488, 'counters/updates': 1484}
train stats after 47520 examples: {'rewards_train/chosen': '0.16316', 'rewards_train/rejected': '-0.013641', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17681', 'logps_train/rejected': '-108.45', 'logps_train/chosen': '-136.56', 'loss/train': '0.63489', 'examples_per_second': '31.548', 'grad_norm': '24.625', 'counters/examples': 47520, 'counters/updates': 1485}
train stats after 47552 examples: {'rewards_train/chosen': '0.14052', 'rewards_train/rejected': '0.027109', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11342', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-148.38', 'loss/train': '0.64778', 'examples_per_second': '31.427', 'grad_norm': '30.25', 'counters/examples': 47552, 'counters/updates': 1486}
train stats after 47584 examples: {'rewards_train/chosen': '0.12056', 'rewards_train/rejected': '0.042729', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077826', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-157.63', 'loss/train': '0.66143', 'examples_per_second': '29.854', 'grad_norm': '33.5', 'counters/examples': 47584, 'counters/updates': 1487}
skipping logging after 47616 examples to avoid logging too frequently
train stats after 47648 examples: {'rewards_train/chosen': '0.057749', 'rewards_train/rejected': '0.020152', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037597', 'logps_train/rejected': '-104.62', 'logps_train/chosen': '-115.87', 'loss/train': '0.67821', 'examples_per_second': '32.687', 'grad_norm': '36.75', 'counters/examples': 47648, 'counters/updates': 1489}
skipping logging after 47680 examples to avoid logging too frequently
train stats after 47712 examples: {'rewards_train/chosen': '0.076068', 'rewards_train/rejected': '0.059834', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.016234', 'logps_train/rejected': '-113.81', 'logps_train/chosen': '-135.29', 'loss/train': '0.69669', 'examples_per_second': '37.551', 'grad_norm': '31.375', 'counters/examples': 47712, 'counters/updates': 1491}
train stats after 47744 examples: {'rewards_train/chosen': '0.080361', 'rewards_train/rejected': '0.075368', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0049933', 'logps_train/rejected': '-125.03', 'logps_train/chosen': '-125.37', 'loss/train': '0.69344', 'examples_per_second': '31.561', 'grad_norm': '38.5', 'counters/examples': 47744, 'counters/updates': 1492}
skipping logging after 47776 examples to avoid logging too frequently
train stats after 47808 examples: {'rewards_train/chosen': '0.05565', 'rewards_train/rejected': '0.052774', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0028767', 'logps_train/rejected': '-176.78', 'logps_train/chosen': '-155.17', 'loss/train': '0.70529', 'examples_per_second': '30.042', 'grad_norm': '45', 'counters/examples': 47808, 'counters/updates': 1494}
train stats after 47840 examples: {'rewards_train/chosen': '0.031526', 'rewards_train/rejected': '0.057256', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.025731', 'logps_train/rejected': '-143.89', 'logps_train/chosen': '-118.99', 'loss/train': '0.71666', 'examples_per_second': '29.901', 'grad_norm': '35.5', 'counters/examples': 47840, 'counters/updates': 1495}
skipping logging after 47872 examples to avoid logging too frequently
train stats after 47904 examples: {'rewards_train/chosen': '0.0025879', 'rewards_train/rejected': '-0.009291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011879', 'logps_train/rejected': '-148.06', 'logps_train/chosen': '-166.11', 'loss/train': '0.69314', 'examples_per_second': '30.107', 'grad_norm': '31.5', 'counters/examples': 47904, 'counters/updates': 1497}
train stats after 47936 examples: {'rewards_train/chosen': '0.075529', 'rewards_train/rejected': '0.049918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025611', 'logps_train/rejected': '-111.91', 'logps_train/chosen': '-139.15', 'loss/train': '0.68533', 'examples_per_second': '31.404', 'grad_norm': '35.75', 'counters/examples': 47936, 'counters/updates': 1498}
train stats after 47968 examples: {'rewards_train/chosen': '0.092135', 'rewards_train/rejected': '0.032507', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059628', 'logps_train/rejected': '-138.14', 'logps_train/chosen': '-161.64', 'loss/train': '0.66941', 'examples_per_second': '30.473', 'grad_norm': '41', 'counters/examples': 47968, 'counters/updates': 1499}
skipping logging after 48000 examples to avoid logging too frequently
Running evaluation after 48000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.19it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.80it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.82it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 48000: {'rewards_eval/chosen': '0.10081', 'rewards_eval/rejected': '0.038053', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.062756', 'logps_eval/rejected': '-121.75', 'logps_eval/chosen': '-143.1', 'loss/eval': '0.67042'}
skipping save for non epoch
train stats after 48032 examples: {'rewards_train/chosen': '0.07878', 'rewards_train/rejected': '0.0050477', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.073732', 'logps_train/rejected': '-134.77', 'logps_train/chosen': '-134.04', 'loss/train': '0.6627', 'examples_per_second': '37.66', 'grad_norm': '33.75', 'counters/examples': 48032, 'counters/updates': 1501}
train stats after 48064 examples: {'rewards_train/chosen': '0.097328', 'rewards_train/rejected': '0.037931', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059397', 'logps_train/rejected': '-111.08', 'logps_train/chosen': '-173.71', 'loss/train': '0.67221', 'examples_per_second': '31.974', 'grad_norm': '45', 'counters/examples': 48064, 'counters/updates': 1502}
skipping logging after 48096 examples to avoid logging too frequently
train stats after 48128 examples: {'rewards_train/chosen': '0.10242', 'rewards_train/rejected': '0.042931', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059485', 'logps_train/rejected': '-105.94', 'logps_train/chosen': '-136.31', 'loss/train': '0.67151', 'examples_per_second': '31.614', 'grad_norm': '31.75', 'counters/examples': 48128, 'counters/updates': 1504}
train stats after 48160 examples: {'rewards_train/chosen': '0.075608', 'rewards_train/rejected': '-0.0046539', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080262', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-137.05', 'loss/train': '0.66076', 'examples_per_second': '30.806', 'grad_norm': '27.25', 'counters/examples': 48160, 'counters/updates': 1505}
train stats after 48192 examples: {'rewards_train/chosen': '0.044744', 'rewards_train/rejected': '0.047701', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0029571', 'logps_train/rejected': '-102.89', 'logps_train/chosen': '-114.64', 'loss/train': '0.70138', 'examples_per_second': '31.218', 'grad_norm': '27.625', 'counters/examples': 48192, 'counters/updates': 1506}
train stats after 48224 examples: {'rewards_train/chosen': '0.047322', 'rewards_train/rejected': '0.075055', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027733', 'logps_train/rejected': '-130.6', 'logps_train/chosen': '-109.65', 'loss/train': '0.71252', 'examples_per_second': '30.849', 'grad_norm': '30.375', 'counters/examples': 48224, 'counters/updates': 1507}
skipping logging after 48256 examples to avoid logging too frequently
train stats after 48288 examples: {'rewards_train/chosen': '0.045477', 'rewards_train/rejected': '0.047699', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0022219', 'logps_train/rejected': '-130.65', 'logps_train/chosen': '-128.85', 'loss/train': '0.69903', 'examples_per_second': '30.973', 'grad_norm': '34', 'counters/examples': 48288, 'counters/updates': 1509}
train stats after 48320 examples: {'rewards_train/chosen': '0.12926', 'rewards_train/rejected': '0.059126', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070137', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-135.87', 'loss/train': '0.67325', 'examples_per_second': '30.47', 'grad_norm': '33', 'counters/examples': 48320, 'counters/updates': 1510}
train stats after 48352 examples: {'rewards_train/chosen': '0.061052', 'rewards_train/rejected': '-0.017162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078214', 'logps_train/rejected': '-76.555', 'logps_train/chosen': '-139.58', 'loss/train': '0.66129', 'examples_per_second': '31.143', 'grad_norm': '27', 'counters/examples': 48352, 'counters/updates': 1511}
skipping logging after 48384 examples to avoid logging too frequently
train stats after 48416 examples: {'rewards_train/chosen': '0.046109', 'rewards_train/rejected': '0.05925', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.013141', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-127.53', 'loss/train': '0.70427', 'examples_per_second': '32.333', 'grad_norm': '28.125', 'counters/examples': 48416, 'counters/updates': 1513}
train stats after 48448 examples: {'rewards_train/chosen': '0.20416', 'rewards_train/rejected': '0.080186', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12397', 'logps_train/rejected': '-121.96', 'logps_train/chosen': '-157.85', 'loss/train': '0.64335', 'examples_per_second': '31.439', 'grad_norm': '33', 'counters/examples': 48448, 'counters/updates': 1514}
train stats after 48480 examples: {'rewards_train/chosen': '0.049419', 'rewards_train/rejected': '0.068023', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018604', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-169.69', 'loss/train': '0.70797', 'examples_per_second': '31.502', 'grad_norm': '36.25', 'counters/examples': 48480, 'counters/updates': 1515}
train stats after 48512 examples: {'rewards_train/chosen': '0.043446', 'rewards_train/rejected': '0.089751', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.046305', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-127.31', 'loss/train': '0.72569', 'examples_per_second': '30.941', 'grad_norm': '29.75', 'counters/examples': 48512, 'counters/updates': 1516}
train stats after 48544 examples: {'rewards_train/chosen': '0.080493', 'rewards_train/rejected': '0.027965', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052528', 'logps_train/rejected': '-102.68', 'logps_train/chosen': '-137.73', 'loss/train': '0.67095', 'examples_per_second': '33.094', 'grad_norm': '28.75', 'counters/examples': 48544, 'counters/updates': 1517}
train stats after 48576 examples: {'rewards_train/chosen': '0.025788', 'rewards_train/rejected': '-0.023189', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048977', 'logps_train/rejected': '-157.14', 'logps_train/chosen': '-153', 'loss/train': '0.68384', 'examples_per_second': '31.503', 'grad_norm': '33.75', 'counters/examples': 48576, 'counters/updates': 1518}
skipping logging after 48608 examples to avoid logging too frequently
train stats after 48640 examples: {'rewards_train/chosen': '0.081052', 'rewards_train/rejected': '0.084464', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0034122', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-120.9', 'loss/train': '0.70068', 'examples_per_second': '32.002', 'grad_norm': '29', 'counters/examples': 48640, 'counters/updates': 1520}
train stats after 48672 examples: {'rewards_train/chosen': '0.10462', 'rewards_train/rejected': '0.071872', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032744', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-138.67', 'loss/train': '0.68303', 'examples_per_second': '31.437', 'grad_norm': '34.5', 'counters/examples': 48672, 'counters/updates': 1521}
train stats after 48704 examples: {'rewards_train/chosen': '0.074109', 'rewards_train/rejected': '0.049731', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.024378', 'logps_train/rejected': '-92.873', 'logps_train/chosen': '-129.99', 'loss/train': '0.68647', 'examples_per_second': '31.605', 'grad_norm': '26.125', 'counters/examples': 48704, 'counters/updates': 1522}
skipping logging after 48736 examples to avoid logging too frequently
train stats after 48768 examples: {'rewards_train/chosen': '0.10654', 'rewards_train/rejected': '0.12582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019286', 'logps_train/rejected': '-146.36', 'logps_train/chosen': '-165.34', 'loss/train': '0.71142', 'examples_per_second': '32.003', 'grad_norm': '34.5', 'counters/examples': 48768, 'counters/updates': 1524}
train stats after 48800 examples: {'rewards_train/chosen': '0.10002', 'rewards_train/rejected': '0.045783', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054233', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-173.16', 'loss/train': '0.67318', 'examples_per_second': '31.451', 'grad_norm': '35', 'counters/examples': 48800, 'counters/updates': 1525}
train stats after 48832 examples: {'rewards_train/chosen': '0.069657', 'rewards_train/rejected': '0.07223', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0025731', 'logps_train/rejected': '-158.45', 'logps_train/chosen': '-165.43', 'loss/train': '0.70523', 'examples_per_second': '31.473', 'grad_norm': '48', 'counters/examples': 48832, 'counters/updates': 1526}
train stats after 48864 examples: {'rewards_train/chosen': '0.024078', 'rewards_train/rejected': '0.00396', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020118', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-136.05', 'loss/train': '0.68875', 'examples_per_second': '31.552', 'grad_norm': '32.75', 'counters/examples': 48864, 'counters/updates': 1527}
train stats after 48896 examples: {'rewards_train/chosen': '0.063988', 'rewards_train/rejected': '-0.011923', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075911', 'logps_train/rejected': '-147.67', 'logps_train/chosen': '-157.81', 'loss/train': '0.66782', 'examples_per_second': '31.433', 'grad_norm': '39', 'counters/examples': 48896, 'counters/updates': 1528}
train stats after 48928 examples: {'rewards_train/chosen': '0.13892', 'rewards_train/rejected': '0.054489', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.084436', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-180.95', 'loss/train': '0.66228', 'examples_per_second': '31.457', 'grad_norm': '31.625', 'counters/examples': 48928, 'counters/updates': 1529}
train stats after 48960 examples: {'rewards_train/chosen': '0.0348', 'rewards_train/rejected': '0.013237', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021563', 'logps_train/rejected': '-98.281', 'logps_train/chosen': '-156.28', 'loss/train': '0.68754', 'examples_per_second': '32.286', 'grad_norm': '29.75', 'counters/examples': 48960, 'counters/updates': 1530}
train stats after 48992 examples: {'rewards_train/chosen': '0.084561', 'rewards_train/rejected': '0.043327', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041234', 'logps_train/rejected': '-147.22', 'logps_train/chosen': '-108.52', 'loss/train': '0.67744', 'examples_per_second': '31.431', 'grad_norm': '34.75', 'counters/examples': 48992, 'counters/updates': 1531}
train stats after 49024 examples: {'rewards_train/chosen': '0.042021', 'rewards_train/rejected': '0.055718', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013697', 'logps_train/rejected': '-111.74', 'logps_train/chosen': '-129.69', 'loss/train': '0.70796', 'examples_per_second': '31.256', 'grad_norm': '34.75', 'counters/examples': 49024, 'counters/updates': 1532}
skipping logging after 49056 examples to avoid logging too frequently
train stats after 49088 examples: {'rewards_train/chosen': '0.092551', 'rewards_train/rejected': '0.036783', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055768', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-130.26', 'loss/train': '0.67517', 'examples_per_second': '31.47', 'grad_norm': '39.75', 'counters/examples': 49088, 'counters/updates': 1534}
train stats after 49120 examples: {'rewards_train/chosen': '0.088677', 'rewards_train/rejected': '0.067298', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021378', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-146.42', 'loss/train': '0.69429', 'examples_per_second': '31.942', 'grad_norm': '31.125', 'counters/examples': 49120, 'counters/updates': 1535}
train stats after 49152 examples: {'rewards_train/chosen': '0.093549', 'rewards_train/rejected': '-0.011623', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10517', 'logps_train/rejected': '-113.55', 'logps_train/chosen': '-154.54', 'loss/train': '0.64922', 'examples_per_second': '31.291', 'grad_norm': '36.75', 'counters/examples': 49152, 'counters/updates': 1536}
train stats after 49184 examples: {'rewards_train/chosen': '0.069242', 'rewards_train/rejected': '0.057451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011791', 'logps_train/rejected': '-162.33', 'logps_train/chosen': '-180.55', 'loss/train': '0.70051', 'examples_per_second': '31.525', 'grad_norm': '41', 'counters/examples': 49184, 'counters/updates': 1537}
train stats after 49216 examples: {'rewards_train/chosen': '0.063714', 'rewards_train/rejected': '0.010871', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052842', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-183.4', 'loss/train': '0.67234', 'examples_per_second': '31.436', 'grad_norm': '33.75', 'counters/examples': 49216, 'counters/updates': 1538}
train stats after 49248 examples: {'rewards_train/chosen': '0.11994', 'rewards_train/rejected': '0.027303', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092641', 'logps_train/rejected': '-94.844', 'logps_train/chosen': '-164.55', 'loss/train': '0.65603', 'examples_per_second': '32.934', 'grad_norm': '30.75', 'counters/examples': 49248, 'counters/updates': 1539}
train stats after 49280 examples: {'rewards_train/chosen': '0.13928', 'rewards_train/rejected': '0.0553', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083984', 'logps_train/rejected': '-103.66', 'logps_train/chosen': '-132.78', 'loss/train': '0.65968', 'examples_per_second': '31.144', 'grad_norm': '27.25', 'counters/examples': 49280, 'counters/updates': 1540}
train stats after 49312 examples: {'rewards_train/chosen': '0.12964', 'rewards_train/rejected': '0.024551', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10509', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-148.27', 'loss/train': '0.65', 'examples_per_second': '31.524', 'grad_norm': '29.75', 'counters/examples': 49312, 'counters/updates': 1541}
train stats after 49344 examples: {'rewards_train/chosen': '0.039266', 'rewards_train/rejected': '0.037735', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0015307', 'logps_train/rejected': '-105.64', 'logps_train/chosen': '-152.77', 'loss/train': '0.69747', 'examples_per_second': '30.329', 'grad_norm': '28.375', 'counters/examples': 49344, 'counters/updates': 1542}
train stats after 49376 examples: {'rewards_train/chosen': '0.16525', 'rewards_train/rejected': '0.088507', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.076743', 'logps_train/rejected': '-158.25', 'logps_train/chosen': '-174.03', 'loss/train': '0.66215', 'examples_per_second': '32.377', 'grad_norm': '33.25', 'counters/examples': 49376, 'counters/updates': 1543}
train stats after 49408 examples: {'rewards_train/chosen': '0.075294', 'rewards_train/rejected': '0.030975', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044319', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-149.68', 'loss/train': '0.67602', 'examples_per_second': '31.373', 'grad_norm': '32.75', 'counters/examples': 49408, 'counters/updates': 1544}
train stats after 49440 examples: {'rewards_train/chosen': '0.075928', 'rewards_train/rejected': '0.090732', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014804', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-138.91', 'loss/train': '0.70378', 'examples_per_second': '32.424', 'grad_norm': '27.25', 'counters/examples': 49440, 'counters/updates': 1545}
skipping logging after 49472 examples to avoid logging too frequently
train stats after 49504 examples: {'rewards_train/chosen': '0.10022', 'rewards_train/rejected': '0.055053', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045171', 'logps_train/rejected': '-180.58', 'logps_train/chosen': '-131.33', 'loss/train': '0.6848', 'examples_per_second': '31.428', 'grad_norm': '35.75', 'counters/examples': 49504, 'counters/updates': 1547}
skipping logging after 49536 examples to avoid logging too frequently
train stats after 49568 examples: {'rewards_train/chosen': '0.077418', 'rewards_train/rejected': '0.047774', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029644', 'logps_train/rejected': '-150.04', 'logps_train/chosen': '-159.12', 'loss/train': '0.6841', 'examples_per_second': '31.49', 'grad_norm': '44', 'counters/examples': 49568, 'counters/updates': 1549}
train stats after 49600 examples: {'rewards_train/chosen': '0.076889', 'rewards_train/rejected': '0.034154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042735', 'logps_train/rejected': '-109.34', 'logps_train/chosen': '-142.95', 'loss/train': '0.67773', 'examples_per_second': '30.063', 'grad_norm': '33.75', 'counters/examples': 49600, 'counters/updates': 1550}
skipping logging after 49632 examples to avoid logging too frequently
train stats after 49664 examples: {'rewards_train/chosen': '0.085693', 'rewards_train/rejected': '-0.017376', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10307', 'logps_train/rejected': '-110.04', 'logps_train/chosen': '-128.67', 'loss/train': '0.64734', 'examples_per_second': '34.281', 'grad_norm': '29.125', 'counters/examples': 49664, 'counters/updates': 1552}
skipping logging after 49696 examples to avoid logging too frequently
train stats after 49728 examples: {'rewards_train/chosen': '0.040253', 'rewards_train/rejected': '0.10652', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.066268', 'logps_train/rejected': '-145.63', 'logps_train/chosen': '-128.53', 'loss/train': '0.74025', 'examples_per_second': '31.508', 'grad_norm': '83', 'counters/examples': 49728, 'counters/updates': 1554}
skipping logging after 49760 examples to avoid logging too frequently
train stats after 49792 examples: {'rewards_train/chosen': '0.034582', 'rewards_train/rejected': '0.018957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015626', 'logps_train/rejected': '-125.53', 'logps_train/chosen': '-168.35', 'loss/train': '0.69348', 'examples_per_second': '31.501', 'grad_norm': '30.875', 'counters/examples': 49792, 'counters/updates': 1556}
train stats after 49824 examples: {'rewards_train/chosen': '0.032589', 'rewards_train/rejected': '0.014869', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.017719', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-136.52', 'loss/train': '0.68733', 'examples_per_second': '31.092', 'grad_norm': '41.25', 'counters/examples': 49824, 'counters/updates': 1557}
skipping logging after 49856 examples to avoid logging too frequently
train stats after 49888 examples: {'rewards_train/chosen': '0.088009', 'rewards_train/rejected': '0.054246', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033763', 'logps_train/rejected': '-134.39', 'logps_train/chosen': '-137.04', 'loss/train': '0.68099', 'examples_per_second': '31.838', 'grad_norm': '27.5', 'counters/examples': 49888, 'counters/updates': 1559}
train stats after 49920 examples: {'rewards_train/chosen': '0.087836', 'rewards_train/rejected': '0.0035893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084247', 'logps_train/rejected': '-109.32', 'logps_train/chosen': '-157.1', 'loss/train': '0.65959', 'examples_per_second': '29.992', 'grad_norm': '37.5', 'counters/examples': 49920, 'counters/updates': 1560}
train stats after 49952 examples: {'rewards_train/chosen': '0.053004', 'rewards_train/rejected': '-0.054399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1074', 'logps_train/rejected': '-129.64', 'logps_train/chosen': '-132.39', 'loss/train': '0.65008', 'examples_per_second': '31.494', 'grad_norm': '33.25', 'counters/examples': 49952, 'counters/updates': 1561}
train stats after 49984 examples: {'rewards_train/chosen': '0.082409', 'rewards_train/rejected': '0.052378', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.030031', 'logps_train/rejected': '-100.15', 'logps_train/chosen': '-135.06', 'loss/train': '0.68271', 'examples_per_second': '31.493', 'grad_norm': '30', 'counters/examples': 49984, 'counters/updates': 1562}
train stats after 50016 examples: {'rewards_train/chosen': '0.069791', 'rewards_train/rejected': '0.055066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014724', 'logps_train/rejected': '-138.38', 'logps_train/chosen': '-112.99', 'loss/train': '0.69248', 'examples_per_second': '31.519', 'grad_norm': '38.5', 'counters/examples': 50016, 'counters/updates': 1563}
train stats after 50048 examples: {'rewards_train/chosen': '0.05802', 'rewards_train/rejected': '0.066915', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0088951', 'logps_train/rejected': '-140.49', 'logps_train/chosen': '-149.42', 'loss/train': '0.70476', 'examples_per_second': '31.499', 'grad_norm': '41.25', 'counters/examples': 50048, 'counters/updates': 1564}
train stats after 50080 examples: {'rewards_train/chosen': '0.086882', 'rewards_train/rejected': '0.030904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055978', 'logps_train/rejected': '-137.5', 'logps_train/chosen': '-155.39', 'loss/train': '0.66998', 'examples_per_second': '31.497', 'grad_norm': '28.875', 'counters/examples': 50080, 'counters/updates': 1565}
train stats after 50112 examples: {'rewards_train/chosen': '0.076362', 'rewards_train/rejected': '0.035653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040708', 'logps_train/rejected': '-143.37', 'logps_train/chosen': '-169.01', 'loss/train': '0.67892', 'examples_per_second': '30.047', 'grad_norm': '36.75', 'counters/examples': 50112, 'counters/updates': 1566}
train stats after 50144 examples: {'rewards_train/chosen': '0.027715', 'rewards_train/rejected': '0.027244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00047079', 'logps_train/rejected': '-139.53', 'logps_train/chosen': '-121.37', 'loss/train': '0.69916', 'examples_per_second': '31.518', 'grad_norm': '31.25', 'counters/examples': 50144, 'counters/updates': 1567}
train stats after 50176 examples: {'rewards_train/chosen': '0.059099', 'rewards_train/rejected': '0.015801', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043299', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-129.87', 'loss/train': '0.67642', 'examples_per_second': '32.667', 'grad_norm': '32', 'counters/examples': 50176, 'counters/updates': 1568}
skipping logging after 50208 examples to avoid logging too frequently
train stats after 50240 examples: {'rewards_train/chosen': '0.050653', 'rewards_train/rejected': '0.032619', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018034', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-126.45', 'loss/train': '0.69005', 'examples_per_second': '31.775', 'grad_norm': '28.125', 'counters/examples': 50240, 'counters/updates': 1570}
skipping logging after 50272 examples to avoid logging too frequently
train stats after 50304 examples: {'rewards_train/chosen': '0.074146', 'rewards_train/rejected': '0.061842', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012305', 'logps_train/rejected': '-106.92', 'logps_train/chosen': '-158.17', 'loss/train': '0.69089', 'examples_per_second': '31.158', 'grad_norm': '33.75', 'counters/examples': 50304, 'counters/updates': 1572}
train stats after 50336 examples: {'rewards_train/chosen': '0.07407', 'rewards_train/rejected': '0.030002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044068', 'logps_train/rejected': '-111.27', 'logps_train/chosen': '-170', 'loss/train': '0.67457', 'examples_per_second': '31.201', 'grad_norm': '30.625', 'counters/examples': 50336, 'counters/updates': 1573}
skipping logging after 50368 examples to avoid logging too frequently
train stats after 50400 examples: {'rewards_train/chosen': '0.035062', 'rewards_train/rejected': '0.064237', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029175', 'logps_train/rejected': '-106.15', 'logps_train/chosen': '-120.6', 'loss/train': '0.71206', 'examples_per_second': '31.215', 'grad_norm': '31.125', 'counters/examples': 50400, 'counters/updates': 1575}
train stats after 50432 examples: {'rewards_train/chosen': '0.088747', 'rewards_train/rejected': '0.0080599', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080687', 'logps_train/rejected': '-130.02', 'logps_train/chosen': '-154.43', 'loss/train': '0.67463', 'examples_per_second': '31.51', 'grad_norm': '33', 'counters/examples': 50432, 'counters/updates': 1576}
train stats after 50464 examples: {'rewards_train/chosen': '0.096108', 'rewards_train/rejected': '0.080117', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015991', 'logps_train/rejected': '-147.88', 'logps_train/chosen': '-138.43', 'loss/train': '0.69234', 'examples_per_second': '31.108', 'grad_norm': '35', 'counters/examples': 50464, 'counters/updates': 1577}
train stats after 50496 examples: {'rewards_train/chosen': '0.072606', 'rewards_train/rejected': '0.021791', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050815', 'logps_train/rejected': '-170.49', 'logps_train/chosen': '-173.71', 'loss/train': '0.67437', 'examples_per_second': '31.47', 'grad_norm': '39.75', 'counters/examples': 50496, 'counters/updates': 1578}
train stats after 50528 examples: {'rewards_train/chosen': '0.1068', 'rewards_train/rejected': '0.02483', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081965', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-136.19', 'loss/train': '0.66703', 'examples_per_second': '32.677', 'grad_norm': '28.625', 'counters/examples': 50528, 'counters/updates': 1579}
train stats after 50560 examples: {'rewards_train/chosen': '0.071523', 'rewards_train/rejected': '-0.00142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072943', 'logps_train/rejected': '-101.6', 'logps_train/chosen': '-106.41', 'loss/train': '0.66174', 'examples_per_second': '32.6', 'grad_norm': '22.875', 'counters/examples': 50560, 'counters/updates': 1580}
skipping logging after 50592 examples to avoid logging too frequently
train stats after 50624 examples: {'rewards_train/chosen': '0.066368', 'rewards_train/rejected': '-0.021653', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088022', 'logps_train/rejected': '-115.69', 'logps_train/chosen': '-123.57', 'loss/train': '0.65723', 'examples_per_second': '32.373', 'grad_norm': '26.875', 'counters/examples': 50624, 'counters/updates': 1582}
train stats after 50656 examples: {'rewards_train/chosen': '0.086108', 'rewards_train/rejected': '0.023875', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062233', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-129.56', 'loss/train': '0.6673', 'examples_per_second': '30.504', 'grad_norm': '30.875', 'counters/examples': 50656, 'counters/updates': 1583}
train stats after 50688 examples: {'rewards_train/chosen': '0.15559', 'rewards_train/rejected': '0.054012', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10157', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-144.49', 'loss/train': '0.65188', 'examples_per_second': '31.474', 'grad_norm': '36.75', 'counters/examples': 50688, 'counters/updates': 1584}
train stats after 50720 examples: {'rewards_train/chosen': '0.099015', 'rewards_train/rejected': '0.0080863', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090929', 'logps_train/rejected': '-151.9', 'logps_train/chosen': '-146.49', 'loss/train': '0.65553', 'examples_per_second': '31.821', 'grad_norm': '35.25', 'counters/examples': 50720, 'counters/updates': 1585}
train stats after 50752 examples: {'rewards_train/chosen': '0.027928', 'rewards_train/rejected': '0.012875', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015052', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-127.56', 'loss/train': '0.69008', 'examples_per_second': '31.508', 'grad_norm': '32', 'counters/examples': 50752, 'counters/updates': 1586}
train stats after 50784 examples: {'rewards_train/chosen': '0.052751', 'rewards_train/rejected': '0.090602', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.037851', 'logps_train/rejected': '-140.23', 'logps_train/chosen': '-137.89', 'loss/train': '0.72394', 'examples_per_second': '30.992', 'grad_norm': '41.25', 'counters/examples': 50784, 'counters/updates': 1587}
train stats after 50816 examples: {'rewards_train/chosen': '0.02569', 'rewards_train/rejected': '-0.008707', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034397', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-159.9', 'loss/train': '0.68194', 'examples_per_second': '31.101', 'grad_norm': '33.25', 'counters/examples': 50816, 'counters/updates': 1588}
train stats after 50848 examples: {'rewards_train/chosen': '0.0354', 'rewards_train/rejected': '0.0062075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.029192', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-167.53', 'loss/train': '0.68386', 'examples_per_second': '31.371', 'grad_norm': '32', 'counters/examples': 50848, 'counters/updates': 1589}
train stats after 50880 examples: {'rewards_train/chosen': '0.052449', 'rewards_train/rejected': '0.056265', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0038164', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-141.2', 'loss/train': '0.70264', 'examples_per_second': '30.586', 'grad_norm': '36.75', 'counters/examples': 50880, 'counters/updates': 1590}
train stats after 50912 examples: {'rewards_train/chosen': '0.1279', 'rewards_train/rejected': '0.042153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085744', 'logps_train/rejected': '-140.53', 'logps_train/chosen': '-134.62', 'loss/train': '0.65587', 'examples_per_second': '30.28', 'grad_norm': '30.5', 'counters/examples': 50912, 'counters/updates': 1591}
skipping logging after 50944 examples to avoid logging too frequently
train stats after 50976 examples: {'rewards_train/chosen': '0.14719', 'rewards_train/rejected': '-0.010403', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15759', 'logps_train/rejected': '-113.98', 'logps_train/chosen': '-191.73', 'loss/train': '0.62781', 'examples_per_second': '31.512', 'grad_norm': '26.875', 'counters/examples': 50976, 'counters/updates': 1593}
skipping logging after 51008 examples to avoid logging too frequently
train stats after 51040 examples: {'rewards_train/chosen': '0.1561', 'rewards_train/rejected': '0.031795', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12431', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-156.37', 'loss/train': '0.64354', 'examples_per_second': '31.579', 'grad_norm': '28.25', 'counters/examples': 51040, 'counters/updates': 1595}
train stats after 51072 examples: {'rewards_train/chosen': '0.027077', 'rewards_train/rejected': '0.034217', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0071396', 'logps_train/rejected': '-144.98', 'logps_train/chosen': '-149.8', 'loss/train': '0.69963', 'examples_per_second': '31.017', 'grad_norm': '29.25', 'counters/examples': 51072, 'counters/updates': 1596}
train stats after 51104 examples: {'rewards_train/chosen': '0.17363', 'rewards_train/rejected': '0.094182', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079444', 'logps_train/rejected': '-152.82', 'logps_train/chosen': '-187.26', 'loss/train': '0.664', 'examples_per_second': '30.513', 'grad_norm': '31.5', 'counters/examples': 51104, 'counters/updates': 1597}
train stats after 51136 examples: {'rewards_train/chosen': '0.016812', 'rewards_train/rejected': '0.015521', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0012907', 'logps_train/rejected': '-95.063', 'logps_train/chosen': '-116.25', 'loss/train': '0.69786', 'examples_per_second': '31.497', 'grad_norm': '31.5', 'counters/examples': 51136, 'counters/updates': 1598}
train stats after 51168 examples: {'rewards_train/chosen': '0.1412', 'rewards_train/rejected': '0.19926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.05806', 'logps_train/rejected': '-144.39', 'logps_train/chosen': '-161.03', 'loss/train': '0.76276', 'examples_per_second': '30.942', 'grad_norm': '104', 'counters/examples': 51168, 'counters/updates': 1599}
train stats after 51200 examples: {'rewards_train/chosen': '0.12049', 'rewards_train/rejected': '0.045455', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075038', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-141.22', 'loss/train': '0.66514', 'examples_per_second': '30.535', 'grad_norm': '57.5', 'counters/examples': 51200, 'counters/updates': 1600}
train stats after 51232 examples: {'rewards_train/chosen': '0.063698', 'rewards_train/rejected': '-0.036875', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10057', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-152.08', 'loss/train': '0.64847', 'examples_per_second': '31.52', 'grad_norm': '26.25', 'counters/examples': 51232, 'counters/updates': 1601}
skipping logging after 51264 examples to avoid logging too frequently
train stats after 51296 examples: {'rewards_train/chosen': '0.047121', 'rewards_train/rejected': '0.099042', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.051921', 'logps_train/rejected': '-136.46', 'logps_train/chosen': '-142.04', 'loss/train': '0.72823', 'examples_per_second': '31.418', 'grad_norm': '43.75', 'counters/examples': 51296, 'counters/updates': 1603}
train stats after 51328 examples: {'rewards_train/chosen': '0.096396', 'rewards_train/rejected': '0.021447', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074949', 'logps_train/rejected': '-145.14', 'logps_train/chosen': '-143.86', 'loss/train': '0.66652', 'examples_per_second': '30.068', 'grad_norm': '30.25', 'counters/examples': 51328, 'counters/updates': 1604}
train stats after 51360 examples: {'rewards_train/chosen': '0.11836', 'rewards_train/rejected': '0.02366', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094704', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-175.2', 'loss/train': '0.65587', 'examples_per_second': '29.796', 'grad_norm': '31.75', 'counters/examples': 51360, 'counters/updates': 1605}
train stats after 51392 examples: {'rewards_train/chosen': '0.057589', 'rewards_train/rejected': '0.068616', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.011028', 'logps_train/rejected': '-100.77', 'logps_train/chosen': '-134.75', 'loss/train': '0.70926', 'examples_per_second': '30.206', 'grad_norm': '31.625', 'counters/examples': 51392, 'counters/updates': 1606}
train stats after 51424 examples: {'rewards_train/chosen': '0.098086', 'rewards_train/rejected': '0.041938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056147', 'logps_train/rejected': '-123.93', 'logps_train/chosen': '-117.9', 'loss/train': '0.67458', 'examples_per_second': '32.25', 'grad_norm': '28.375', 'counters/examples': 51424, 'counters/updates': 1607}
skipping logging after 51456 examples to avoid logging too frequently
train stats after 51488 examples: {'rewards_train/chosen': '0.10911', 'rewards_train/rejected': '0.052277', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056832', 'logps_train/rejected': '-82.432', 'logps_train/chosen': '-164.06', 'loss/train': '0.66925', 'examples_per_second': '30.351', 'grad_norm': '29.25', 'counters/examples': 51488, 'counters/updates': 1609}
train stats after 51520 examples: {'rewards_train/chosen': '0.10583', 'rewards_train/rejected': '0.078438', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027397', 'logps_train/rejected': '-158.79', 'logps_train/chosen': '-136.33', 'loss/train': '0.68511', 'examples_per_second': '31.537', 'grad_norm': '33.75', 'counters/examples': 51520, 'counters/updates': 1610}
train stats after 51552 examples: {'rewards_train/chosen': '0.061743', 'rewards_train/rejected': '0.0092021', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052541', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-121.93', 'loss/train': '0.67108', 'examples_per_second': '30.237', 'grad_norm': '26', 'counters/examples': 51552, 'counters/updates': 1611}
train stats after 51584 examples: {'rewards_train/chosen': '0.090272', 'rewards_train/rejected': '0.040495', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.049777', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-141.23', 'loss/train': '0.68056', 'examples_per_second': '31.487', 'grad_norm': '34.25', 'counters/examples': 51584, 'counters/updates': 1612}
train stats after 51616 examples: {'rewards_train/chosen': '0.08618', 'rewards_train/rejected': '0.020704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065476', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-143.82', 'loss/train': '0.6676', 'examples_per_second': '32.064', 'grad_norm': '26.375', 'counters/examples': 51616, 'counters/updates': 1613}
train stats after 51648 examples: {'rewards_train/chosen': '0.11277', 'rewards_train/rejected': '0.07083', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041937', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-154.96', 'loss/train': '0.6763', 'examples_per_second': '31.472', 'grad_norm': '44', 'counters/examples': 51648, 'counters/updates': 1614}
train stats after 51680 examples: {'rewards_train/chosen': '0.029382', 'rewards_train/rejected': '0.085354', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.055972', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-163.79', 'loss/train': '0.73232', 'examples_per_second': '30.865', 'grad_norm': '57', 'counters/examples': 51680, 'counters/updates': 1615}
train stats after 51712 examples: {'rewards_train/chosen': '0.039204', 'rewards_train/rejected': '0.055843', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016639', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-176.95', 'loss/train': '0.71029', 'examples_per_second': '31.096', 'grad_norm': '33.75', 'counters/examples': 51712, 'counters/updates': 1616}
train stats after 51744 examples: {'rewards_train/chosen': '0.12795', 'rewards_train/rejected': '0.019268', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10869', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-128.2', 'loss/train': '0.65713', 'examples_per_second': '31.466', 'grad_norm': '27.75', 'counters/examples': 51744, 'counters/updates': 1617}
train stats after 51776 examples: {'rewards_train/chosen': '0.13189', 'rewards_train/rejected': '0.025637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10625', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-123.62', 'loss/train': '0.65199', 'examples_per_second': '30.73', 'grad_norm': '30.5', 'counters/examples': 51776, 'counters/updates': 1618}
train stats after 51808 examples: {'rewards_train/chosen': '0.13793', 'rewards_train/rejected': '0.066989', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070944', 'logps_train/rejected': '-195.02', 'logps_train/chosen': '-180.06', 'loss/train': '0.66924', 'examples_per_second': '30.53', 'grad_norm': '48.5', 'counters/examples': 51808, 'counters/updates': 1619}
train stats after 51840 examples: {'rewards_train/chosen': '0.10878', 'rewards_train/rejected': '0.030101', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078675', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-150.94', 'loss/train': '0.66446', 'examples_per_second': '31.502', 'grad_norm': '35.25', 'counters/examples': 51840, 'counters/updates': 1620}
train stats after 51872 examples: {'rewards_train/chosen': '0.062177', 'rewards_train/rejected': '0.049012', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013165', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-153.99', 'loss/train': '0.69205', 'examples_per_second': '32.344', 'grad_norm': '33.75', 'counters/examples': 51872, 'counters/updates': 1621}
train stats after 51904 examples: {'rewards_train/chosen': '0.032757', 'rewards_train/rejected': '0.080612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.047854', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-129.77', 'loss/train': '0.72832', 'examples_per_second': '25.24', 'grad_norm': '53.75', 'counters/examples': 51904, 'counters/updates': 1622}
train stats after 51936 examples: {'rewards_train/chosen': '0.0071156', 'rewards_train/rejected': '0.065001', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.057886', 'logps_train/rejected': '-114.5', 'logps_train/chosen': '-134', 'loss/train': '0.73003', 'examples_per_second': '30.479', 'grad_norm': '61.5', 'counters/examples': 51936, 'counters/updates': 1623}
train stats after 51968 examples: {'rewards_train/chosen': '0.071704', 'rewards_train/rejected': '0.056464', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01524', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-156.33', 'loss/train': '0.69873', 'examples_per_second': '30.116', 'grad_norm': '45.5', 'counters/examples': 51968, 'counters/updates': 1624}
train stats after 52000 examples: {'rewards_train/chosen': '0.16208', 'rewards_train/rejected': '0.06316', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098923', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-154.15', 'loss/train': '0.67231', 'examples_per_second': '25.413', 'grad_norm': '29.75', 'counters/examples': 52000, 'counters/updates': 1625}
skipping logging after 52032 examples to avoid logging too frequently
train stats after 52064 examples: {'rewards_train/chosen': '0.048753', 'rewards_train/rejected': '0.043619', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0051333', 'logps_train/rejected': '-110.35', 'logps_train/chosen': '-129.34', 'loss/train': '0.70149', 'examples_per_second': '30.877', 'grad_norm': '98', 'counters/examples': 52064, 'counters/updates': 1627}
train stats after 52096 examples: {'rewards_train/chosen': '0.059772', 'rewards_train/rejected': '0.028264', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.031508', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-175.43', 'loss/train': '0.6927', 'examples_per_second': '31.32', 'grad_norm': '28.25', 'counters/examples': 52096, 'counters/updates': 1628}
train stats after 52128 examples: {'rewards_train/chosen': '0.071625', 'rewards_train/rejected': '0.050809', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.020816', 'logps_train/rejected': '-151.48', 'logps_train/chosen': '-146.92', 'loss/train': '0.69064', 'examples_per_second': '31.165', 'grad_norm': '29.875', 'counters/examples': 52128, 'counters/updates': 1629}
train stats after 52160 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.015279', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.094963', 'logps_train/rejected': '-145.01', 'logps_train/chosen': '-141.89', 'loss/train': '0.65249', 'examples_per_second': '32.917', 'grad_norm': '28', 'counters/examples': 52160, 'counters/updates': 1630}
train stats after 52192 examples: {'rewards_train/chosen': '0.047123', 'rewards_train/rejected': '0.02588', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021243', 'logps_train/rejected': '-90.093', 'logps_train/chosen': '-118.55', 'loss/train': '0.68483', 'examples_per_second': '30.754', 'grad_norm': '24.25', 'counters/examples': 52192, 'counters/updates': 1631}
skipping logging after 52224 examples to avoid logging too frequently
train stats after 52256 examples: {'rewards_train/chosen': '0.084436', 'rewards_train/rejected': '0.030164', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054272', 'logps_train/rejected': '-118.05', 'logps_train/chosen': '-168.47', 'loss/train': '0.67212', 'examples_per_second': '33.234', 'grad_norm': '28.75', 'counters/examples': 52256, 'counters/updates': 1633}
train stats after 52288 examples: {'rewards_train/chosen': '0.02326', 'rewards_train/rejected': '0.098797', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.075537', 'logps_train/rejected': '-173.38', 'logps_train/chosen': '-179.53', 'loss/train': '0.75311', 'examples_per_second': '31.402', 'grad_norm': '57.75', 'counters/examples': 52288, 'counters/updates': 1634}
train stats after 52320 examples: {'rewards_train/chosen': '0.22368', 'rewards_train/rejected': '0.078586', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14509', 'logps_train/rejected': '-143.28', 'logps_train/chosen': '-130.61', 'loss/train': '0.66032', 'examples_per_second': '30.826', 'grad_norm': '32.75', 'counters/examples': 52320, 'counters/updates': 1635}
train stats after 52352 examples: {'rewards_train/chosen': '0.047902', 'rewards_train/rejected': '0.059653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.011751', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-113.03', 'loss/train': '0.70466', 'examples_per_second': '31.482', 'grad_norm': '30.875', 'counters/examples': 52352, 'counters/updates': 1636}
train stats after 52384 examples: {'rewards_train/chosen': '0.13367', 'rewards_train/rejected': '0.064886', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068781', 'logps_train/rejected': '-99.804', 'logps_train/chosen': '-121.93', 'loss/train': '0.66618', 'examples_per_second': '31.467', 'grad_norm': '27.5', 'counters/examples': 52384, 'counters/updates': 1637}
skipping logging after 52416 examples to avoid logging too frequently
train stats after 52448 examples: {'rewards_train/chosen': '0.13879', 'rewards_train/rejected': '-0.0044293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14322', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-144.75', 'loss/train': '0.63266', 'examples_per_second': '30.953', 'grad_norm': '28.375', 'counters/examples': 52448, 'counters/updates': 1639}
train stats after 52480 examples: {'rewards_train/chosen': '0.07963', 'rewards_train/rejected': '0.021434', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058196', 'logps_train/rejected': '-145.16', 'logps_train/chosen': '-204.75', 'loss/train': '0.67228', 'examples_per_second': '31.602', 'grad_norm': '32', 'counters/examples': 52480, 'counters/updates': 1640}
skipping logging after 52512 examples to avoid logging too frequently
train stats after 52544 examples: {'rewards_train/chosen': '0.036346', 'rewards_train/rejected': '0.0078147', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028532', 'logps_train/rejected': '-107.92', 'logps_train/chosen': '-88.438', 'loss/train': '0.68814', 'examples_per_second': '30.331', 'grad_norm': '30.25', 'counters/examples': 52544, 'counters/updates': 1642}
train stats after 52576 examples: {'rewards_train/chosen': '0.066806', 'rewards_train/rejected': '0.064638', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0021681', 'logps_train/rejected': '-86.332', 'logps_train/chosen': '-115.4', 'loss/train': '0.69861', 'examples_per_second': '32.221', 'grad_norm': '25.5', 'counters/examples': 52576, 'counters/updates': 1643}
train stats after 52608 examples: {'rewards_train/chosen': '0.031751', 'rewards_train/rejected': '0.020241', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011511', 'logps_train/rejected': '-163.04', 'logps_train/chosen': '-144.58', 'loss/train': '0.68993', 'examples_per_second': '31.238', 'grad_norm': '32.25', 'counters/examples': 52608, 'counters/updates': 1644}
skipping logging after 52640 examples to avoid logging too frequently
train stats after 52672 examples: {'rewards_train/chosen': '0.053194', 'rewards_train/rejected': '0.029053', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024141', 'logps_train/rejected': '-126.41', 'logps_train/chosen': '-145.98', 'loss/train': '0.6864', 'examples_per_second': '30.337', 'grad_norm': '29.75', 'counters/examples': 52672, 'counters/updates': 1646}
train stats after 52704 examples: {'rewards_train/chosen': '0.058968', 'rewards_train/rejected': '0.0089529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050016', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-111.3', 'loss/train': '0.67298', 'examples_per_second': '31.453', 'grad_norm': '26.5', 'counters/examples': 52704, 'counters/updates': 1647}
train stats after 52736 examples: {'rewards_train/chosen': '0.10378', 'rewards_train/rejected': '0.0040785', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099705', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-124.23', 'loss/train': '0.64879', 'examples_per_second': '29.982', 'grad_norm': '31.25', 'counters/examples': 52736, 'counters/updates': 1648}
train stats after 52768 examples: {'rewards_train/chosen': '0.20683', 'rewards_train/rejected': '0.05752', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14931', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-157.5', 'loss/train': '0.63827', 'examples_per_second': '29.899', 'grad_norm': '34.25', 'counters/examples': 52768, 'counters/updates': 1649}
train stats after 52800 examples: {'rewards_train/chosen': '0.075903', 'rewards_train/rejected': '0.077877', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0019737', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-162.81', 'loss/train': '0.70556', 'examples_per_second': '31.48', 'grad_norm': '29.75', 'counters/examples': 52800, 'counters/updates': 1650}
train stats after 52832 examples: {'rewards_train/chosen': '0.03718', 'rewards_train/rejected': '0.034254', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029263', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-168.37', 'loss/train': '0.69761', 'examples_per_second': '29.927', 'grad_norm': '32', 'counters/examples': 52832, 'counters/updates': 1651}
skipping logging after 52864 examples to avoid logging too frequently
train stats after 52896 examples: {'rewards_train/chosen': '0.054954', 'rewards_train/rejected': '0.12264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.06769', 'logps_train/rejected': '-160.92', 'logps_train/chosen': '-175.29', 'loss/train': '0.73839', 'examples_per_second': '31.151', 'grad_norm': '63.5', 'counters/examples': 52896, 'counters/updates': 1653}
train stats after 52928 examples: {'rewards_train/chosen': '0.007795', 'rewards_train/rejected': '0.14104', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.13325', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-164.55', 'loss/train': '0.78865', 'examples_per_second': '30.081', 'grad_norm': '152', 'counters/examples': 52928, 'counters/updates': 1654}
train stats after 52960 examples: {'rewards_train/chosen': '0.070896', 'rewards_train/rejected': '-0.026105', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097001', 'logps_train/rejected': '-154', 'logps_train/chosen': '-122.2', 'loss/train': '0.65084', 'examples_per_second': '31.49', 'grad_norm': '30.25', 'counters/examples': 52960, 'counters/updates': 1655}
train stats after 52992 examples: {'rewards_train/chosen': '0.048073', 'rewards_train/rejected': '0.057976', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.009903', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-225.08', 'loss/train': '0.70709', 'examples_per_second': '31.513', 'grad_norm': '38', 'counters/examples': 52992, 'counters/updates': 1656}
train stats after 53024 examples: {'rewards_train/chosen': '0.093221', 'rewards_train/rejected': '0.040864', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052357', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-143.31', 'loss/train': '0.67343', 'examples_per_second': '30.66', 'grad_norm': '33', 'counters/examples': 53024, 'counters/updates': 1657}
train stats after 53056 examples: {'rewards_train/chosen': '0.028627', 'rewards_train/rejected': '0.0056362', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02299', 'logps_train/rejected': '-96.435', 'logps_train/chosen': '-128.66', 'loss/train': '0.68391', 'examples_per_second': '24.684', 'grad_norm': '25.25', 'counters/examples': 53056, 'counters/updates': 1658}
train stats after 53088 examples: {'rewards_train/chosen': '0.0061911', 'rewards_train/rejected': '-0.024572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.030763', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-117.06', 'loss/train': '0.68049', 'examples_per_second': '32.509', 'grad_norm': '27.625', 'counters/examples': 53088, 'counters/updates': 1659}
skipping logging after 53120 examples to avoid logging too frequently
train stats after 53152 examples: {'rewards_train/chosen': '0.069772', 'rewards_train/rejected': '0.051822', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01795', 'logps_train/rejected': '-94.138', 'logps_train/chosen': '-112.17', 'loss/train': '0.68746', 'examples_per_second': '30.198', 'grad_norm': '33.5', 'counters/examples': 53152, 'counters/updates': 1661}
train stats after 53184 examples: {'rewards_train/chosen': '0.063939', 'rewards_train/rejected': '0.027979', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03596', 'logps_train/rejected': '-119.4', 'logps_train/chosen': '-116.17', 'loss/train': '0.67848', 'examples_per_second': '31.461', 'grad_norm': '34', 'counters/examples': 53184, 'counters/updates': 1662}
train stats after 53216 examples: {'rewards_train/chosen': '0.08728', 'rewards_train/rejected': '0.083568', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0037112', 'logps_train/rejected': '-94.439', 'logps_train/chosen': '-129.75', 'loss/train': '0.69666', 'examples_per_second': '32.597', 'grad_norm': '40.25', 'counters/examples': 53216, 'counters/updates': 1663}
train stats after 53248 examples: {'rewards_train/chosen': '0.03221', 'rewards_train/rejected': '0.020903', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011307', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-145.62', 'loss/train': '0.69319', 'examples_per_second': '31.125', 'grad_norm': '28.5', 'counters/examples': 53248, 'counters/updates': 1664}
train stats after 53280 examples: {'rewards_train/chosen': '0.0021744', 'rewards_train/rejected': '0.031702', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.029528', 'logps_train/rejected': '-162.96', 'logps_train/chosen': '-141.95', 'loss/train': '0.71527', 'examples_per_second': '31.466', 'grad_norm': '34', 'counters/examples': 53280, 'counters/updates': 1665}
skipping logging after 53312 examples to avoid logging too frequently
train stats after 53344 examples: {'rewards_train/chosen': '0.040231', 'rewards_train/rejected': '0.08944', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.049209', 'logps_train/rejected': '-169.61', 'logps_train/chosen': '-143.26', 'loss/train': '0.72761', 'examples_per_second': '31.538', 'grad_norm': '58.5', 'counters/examples': 53344, 'counters/updates': 1667}
train stats after 53376 examples: {'rewards_train/chosen': '0.037361', 'rewards_train/rejected': '-0.0018671', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039228', 'logps_train/rejected': '-114.16', 'logps_train/chosen': '-127.23', 'loss/train': '0.67801', 'examples_per_second': '32.345', 'grad_norm': '26.75', 'counters/examples': 53376, 'counters/updates': 1668}
train stats after 53408 examples: {'rewards_train/chosen': '0.055384', 'rewards_train/rejected': '-0.0032126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058597', 'logps_train/rejected': '-95.823', 'logps_train/chosen': '-133.35', 'loss/train': '0.66663', 'examples_per_second': '32.581', 'grad_norm': '29', 'counters/examples': 53408, 'counters/updates': 1669}
train stats after 53440 examples: {'rewards_train/chosen': '0.052638', 'rewards_train/rejected': '0.043814', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.008824', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-131.65', 'loss/train': '0.69192', 'examples_per_second': '31.451', 'grad_norm': '28.125', 'counters/examples': 53440, 'counters/updates': 1670}
train stats after 53472 examples: {'rewards_train/chosen': '0.048214', 'rewards_train/rejected': '0.061663', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013449', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-164.43', 'loss/train': '0.70574', 'examples_per_second': '30.468', 'grad_norm': '36', 'counters/examples': 53472, 'counters/updates': 1671}
train stats after 53504 examples: {'rewards_train/chosen': '0.073147', 'rewards_train/rejected': '0.043171', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029975', 'logps_train/rejected': '-128.04', 'logps_train/chosen': '-122.89', 'loss/train': '0.68267', 'examples_per_second': '30.774', 'grad_norm': '29.5', 'counters/examples': 53504, 'counters/updates': 1672}
skipping logging after 53536 examples to avoid logging too frequently
train stats after 53568 examples: {'rewards_train/chosen': '0.042444', 'rewards_train/rejected': '-0.0024678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044911', 'logps_train/rejected': '-125.47', 'logps_train/chosen': '-132.47', 'loss/train': '0.67494', 'examples_per_second': '33.977', 'grad_norm': '30.25', 'counters/examples': 53568, 'counters/updates': 1674}
train stats after 53600 examples: {'rewards_train/chosen': '0.12778', 'rewards_train/rejected': '0.016918', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11086', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-139.59', 'loss/train': '0.64564', 'examples_per_second': '32.436', 'grad_norm': '27.875', 'counters/examples': 53600, 'counters/updates': 1675}
train stats after 53632 examples: {'rewards_train/chosen': '0.035327', 'rewards_train/rejected': '0.00082518', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.034502', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-116.14', 'loss/train': '0.67907', 'examples_per_second': '32.051', 'grad_norm': '28.5', 'counters/examples': 53632, 'counters/updates': 1676}
train stats after 53664 examples: {'rewards_train/chosen': '0.19199', 'rewards_train/rejected': '0.047423', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14456', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-172.53', 'loss/train': '0.63996', 'examples_per_second': '31.161', 'grad_norm': '39.25', 'counters/examples': 53664, 'counters/updates': 1677}
train stats after 53696 examples: {'rewards_train/chosen': '0.056043', 'rewards_train/rejected': '0.040132', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015911', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-135.94', 'loss/train': '0.68925', 'examples_per_second': '30.651', 'grad_norm': '27.125', 'counters/examples': 53696, 'counters/updates': 1678}
train stats after 53728 examples: {'rewards_train/chosen': '0.048833', 'rewards_train/rejected': '0.028894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01994', 'logps_train/rejected': '-148.93', 'logps_train/chosen': '-138.05', 'loss/train': '0.68684', 'examples_per_second': '30.635', 'grad_norm': '38.25', 'counters/examples': 53728, 'counters/updates': 1679}
train stats after 53760 examples: {'rewards_train/chosen': '0.056764', 'rewards_train/rejected': '0.072818', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016053', 'logps_train/rejected': '-100.34', 'logps_train/chosen': '-174.28', 'loss/train': '0.70539', 'examples_per_second': '31.513', 'grad_norm': '56.5', 'counters/examples': 53760, 'counters/updates': 1680}
train stats after 53792 examples: {'rewards_train/chosen': '0.18654', 'rewards_train/rejected': '0.087433', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099108', 'logps_train/rejected': '-177.24', 'logps_train/chosen': '-184.35', 'loss/train': '0.66516', 'examples_per_second': '32.24', 'grad_norm': '38.25', 'counters/examples': 53792, 'counters/updates': 1681}
train stats after 53824 examples: {'rewards_train/chosen': '0.070986', 'rewards_train/rejected': '-0.018687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089673', 'logps_train/rejected': '-153.69', 'logps_train/chosen': '-193.04', 'loss/train': '0.65325', 'examples_per_second': '31.598', 'grad_norm': '30.625', 'counters/examples': 53824, 'counters/updates': 1682}
train stats after 53856 examples: {'rewards_train/chosen': '0.037871', 'rewards_train/rejected': '0.018017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019854', 'logps_train/rejected': '-80.869', 'logps_train/chosen': '-156.36', 'loss/train': '0.68822', 'examples_per_second': '31.441', 'grad_norm': '42.25', 'counters/examples': 53856, 'counters/updates': 1683}
train stats after 53888 examples: {'rewards_train/chosen': '0.012266', 'rewards_train/rejected': '0.031653', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.019387', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-149.33', 'loss/train': '0.70748', 'examples_per_second': '31.516', 'grad_norm': '41.75', 'counters/examples': 53888, 'counters/updates': 1684}
skipping logging after 53920 examples to avoid logging too frequently
train stats after 53952 examples: {'rewards_train/chosen': '0.11639', 'rewards_train/rejected': '0.0081113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10828', 'logps_train/rejected': '-134.43', 'logps_train/chosen': '-176.93', 'loss/train': '0.64655', 'examples_per_second': '31.393', 'grad_norm': '30.625', 'counters/examples': 53952, 'counters/updates': 1686}
train stats after 53984 examples: {'rewards_train/chosen': '0.088546', 'rewards_train/rejected': '0.073929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014618', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-153.07', 'loss/train': '0.69037', 'examples_per_second': '30.845', 'grad_norm': '56.5', 'counters/examples': 53984, 'counters/updates': 1687}
train stats after 54016 examples: {'rewards_train/chosen': '0.094084', 'rewards_train/rejected': '0.01985', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074235', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-133.86', 'loss/train': '0.66282', 'examples_per_second': '32.959', 'grad_norm': '26.125', 'counters/examples': 54016, 'counters/updates': 1688}
train stats after 54048 examples: {'rewards_train/chosen': '0.087591', 'rewards_train/rejected': '0.072676', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.014915', 'logps_train/rejected': '-139.17', 'logps_train/chosen': '-137.97', 'loss/train': '0.69003', 'examples_per_second': '31.623', 'grad_norm': '27.375', 'counters/examples': 54048, 'counters/updates': 1689}
train stats after 54080 examples: {'rewards_train/chosen': '0.050524', 'rewards_train/rejected': '0.060578', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010054', 'logps_train/rejected': '-149.72', 'logps_train/chosen': '-164.67', 'loss/train': '0.70915', 'examples_per_second': '31.424', 'grad_norm': '35.75', 'counters/examples': 54080, 'counters/updates': 1690}
skipping logging after 54112 examples to avoid logging too frequently
train stats after 54144 examples: {'rewards_train/chosen': '0.10331', 'rewards_train/rejected': '0.033381', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069933', 'logps_train/rejected': '-97.848', 'logps_train/chosen': '-162.49', 'loss/train': '0.66782', 'examples_per_second': '30.569', 'grad_norm': '36.75', 'counters/examples': 54144, 'counters/updates': 1692}
train stats after 54176 examples: {'rewards_train/chosen': '0.070601', 'rewards_train/rejected': '0.01813', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052471', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-126.13', 'loss/train': '0.67336', 'examples_per_second': '31.44', 'grad_norm': '39.75', 'counters/examples': 54176, 'counters/updates': 1693}
train stats after 54208 examples: {'rewards_train/chosen': '0.07463', 'rewards_train/rejected': '0.085137', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010507', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-125.95', 'loss/train': '0.70496', 'examples_per_second': '32.335', 'grad_norm': '39.25', 'counters/examples': 54208, 'counters/updates': 1694}
skipping logging after 54240 examples to avoid logging too frequently
train stats after 54272 examples: {'rewards_train/chosen': '0.11162', 'rewards_train/rejected': '-0.017994', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12962', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-169.06', 'loss/train': '0.63962', 'examples_per_second': '34.11', 'grad_norm': '29.75', 'counters/examples': 54272, 'counters/updates': 1696}
train stats after 54304 examples: {'rewards_train/chosen': '0.011473', 'rewards_train/rejected': '0.00044495', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011028', 'logps_train/rejected': '-132.8', 'logps_train/chosen': '-148.92', 'loss/train': '0.69736', 'examples_per_second': '31.476', 'grad_norm': '38.75', 'counters/examples': 54304, 'counters/updates': 1697}
train stats after 54336 examples: {'rewards_train/chosen': '0.088146', 'rewards_train/rejected': '0.056654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.031492', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-153.42', 'loss/train': '0.68542', 'examples_per_second': '31.381', 'grad_norm': '31.875', 'counters/examples': 54336, 'counters/updates': 1698}
train stats after 54368 examples: {'rewards_train/chosen': '0.10951', 'rewards_train/rejected': '0.039605', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06991', 'logps_train/rejected': '-102.93', 'logps_train/chosen': '-184.73', 'loss/train': '0.66457', 'examples_per_second': '30.69', 'grad_norm': '34.75', 'counters/examples': 54368, 'counters/updates': 1699}
train stats after 54400 examples: {'rewards_train/chosen': '0.062817', 'rewards_train/rejected': '0.048798', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014019', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-142.62', 'loss/train': '0.69279', 'examples_per_second': '31.464', 'grad_norm': '38.25', 'counters/examples': 54400, 'counters/updates': 1700}
train stats after 54432 examples: {'rewards_train/chosen': '0.14445', 'rewards_train/rejected': '0.057338', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087107', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-192.88', 'loss/train': '0.66294', 'examples_per_second': '31.472', 'grad_norm': '31.25', 'counters/examples': 54432, 'counters/updates': 1701}
train stats after 54464 examples: {'rewards_train/chosen': '0.14803', 'rewards_train/rejected': '0.014478', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13355', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-174.82', 'loss/train': '0.64058', 'examples_per_second': '31.493', 'grad_norm': '39.5', 'counters/examples': 54464, 'counters/updates': 1702}
train stats after 54496 examples: {'rewards_train/chosen': '0.087638', 'rewards_train/rejected': '0.021894', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065743', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-159.07', 'loss/train': '0.66728', 'examples_per_second': '30.029', 'grad_norm': '35', 'counters/examples': 54496, 'counters/updates': 1703}
train stats after 54528 examples: {'rewards_train/chosen': '0.13933', 'rewards_train/rejected': '0.040336', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098992', 'logps_train/rejected': '-169.29', 'logps_train/chosen': '-131.4', 'loss/train': '0.65118', 'examples_per_second': '31.453', 'grad_norm': '34.75', 'counters/examples': 54528, 'counters/updates': 1704}
train stats after 54560 examples: {'rewards_train/chosen': '0.098529', 'rewards_train/rejected': '-0.0051953', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10372', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-203.86', 'loss/train': '0.6602', 'examples_per_second': '31.336', 'grad_norm': '32', 'counters/examples': 54560, 'counters/updates': 1705}
train stats after 54592 examples: {'rewards_train/chosen': '0.10827', 'rewards_train/rejected': '0.011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097271', 'logps_train/rejected': '-118.3', 'logps_train/chosen': '-163.94', 'loss/train': '0.65558', 'examples_per_second': '30.47', 'grad_norm': '31.75', 'counters/examples': 54592, 'counters/updates': 1706}
train stats after 54624 examples: {'rewards_train/chosen': '0.070128', 'rewards_train/rejected': '0.065124', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0050038', 'logps_train/rejected': '-130.86', 'logps_train/chosen': '-146.5', 'loss/train': '0.69853', 'examples_per_second': '31.28', 'grad_norm': '42.5', 'counters/examples': 54624, 'counters/updates': 1707}
train stats after 54656 examples: {'rewards_train/chosen': '0.065492', 'rewards_train/rejected': '0.066926', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014343', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-138.21', 'loss/train': '0.69958', 'examples_per_second': '31.411', 'grad_norm': '30.75', 'counters/examples': 54656, 'counters/updates': 1708}
train stats after 54688 examples: {'rewards_train/chosen': '0.047525', 'rewards_train/rejected': '0.10501', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.057486', 'logps_train/rejected': '-149.6', 'logps_train/chosen': '-171.02', 'loss/train': '0.74046', 'examples_per_second': '32.131', 'grad_norm': '46.5', 'counters/examples': 54688, 'counters/updates': 1709}
train stats after 54720 examples: {'rewards_train/chosen': '0.080339', 'rewards_train/rejected': '0.021216', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059122', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-120.46', 'loss/train': '0.67158', 'examples_per_second': '29.941', 'grad_norm': '39', 'counters/examples': 54720, 'counters/updates': 1710}
train stats after 54752 examples: {'rewards_train/chosen': '0.077696', 'rewards_train/rejected': '0.04654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031156', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-124.87', 'loss/train': '0.68372', 'examples_per_second': '31.788', 'grad_norm': '29.125', 'counters/examples': 54752, 'counters/updates': 1711}
train stats after 54784 examples: {'rewards_train/chosen': '0.0057493', 'rewards_train/rejected': '0.10494', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.099188', 'logps_train/rejected': '-136.44', 'logps_train/chosen': '-152.86', 'loss/train': '0.76465', 'examples_per_second': '32.175', 'grad_norm': '39', 'counters/examples': 54784, 'counters/updates': 1712}
train stats after 54816 examples: {'rewards_train/chosen': '0.048955', 'rewards_train/rejected': '0.081793', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032838', 'logps_train/rejected': '-180.16', 'logps_train/chosen': '-121.49', 'loss/train': '0.71781', 'examples_per_second': '32.009', 'grad_norm': '36.25', 'counters/examples': 54816, 'counters/updates': 1713}
train stats after 54848 examples: {'rewards_train/chosen': '0.039954', 'rewards_train/rejected': '0.039096', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00085789', 'logps_train/rejected': '-128.91', 'logps_train/chosen': '-165.7', 'loss/train': '0.69973', 'examples_per_second': '30.804', 'grad_norm': '39', 'counters/examples': 54848, 'counters/updates': 1714}
skipping logging after 54880 examples to avoid logging too frequently
train stats after 54912 examples: {'rewards_train/chosen': '0.058226', 'rewards_train/rejected': '0.027489', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030737', 'logps_train/rejected': '-109.53', 'logps_train/chosen': '-158.72', 'loss/train': '0.68392', 'examples_per_second': '32.598', 'grad_norm': '32.75', 'counters/examples': 54912, 'counters/updates': 1716}
train stats after 54944 examples: {'rewards_train/chosen': '0.058036', 'rewards_train/rejected': '0.011305', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04673', 'logps_train/rejected': '-133.47', 'logps_train/chosen': '-132.35', 'loss/train': '0.67402', 'examples_per_second': '32.224', 'grad_norm': '28.75', 'counters/examples': 54944, 'counters/updates': 1717}
train stats after 54976 examples: {'rewards_train/chosen': '0.11997', 'rewards_train/rejected': '0.13029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.010321', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-149.76', 'loss/train': '0.70621', 'examples_per_second': '31.196', 'grad_norm': '58.25', 'counters/examples': 54976, 'counters/updates': 1718}
skipping logging after 55008 examples to avoid logging too frequently
train stats after 55040 examples: {'rewards_train/chosen': '0.036378', 'rewards_train/rejected': '-0.037444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073822', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-170.08', 'loss/train': '0.66197', 'examples_per_second': '32.723', 'grad_norm': '53', 'counters/examples': 55040, 'counters/updates': 1720}
train stats after 55072 examples: {'rewards_train/chosen': '0.078562', 'rewards_train/rejected': '0.0065354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072027', 'logps_train/rejected': '-88.433', 'logps_train/chosen': '-107.76', 'loss/train': '0.66248', 'examples_per_second': '31.401', 'grad_norm': '32.75', 'counters/examples': 55072, 'counters/updates': 1721}
train stats after 55104 examples: {'rewards_train/chosen': '0.052713', 'rewards_train/rejected': '0.020088', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032625', 'logps_train/rejected': '-108.19', 'logps_train/chosen': '-110.43', 'loss/train': '0.68011', 'examples_per_second': '32.898', 'grad_norm': '25.625', 'counters/examples': 55104, 'counters/updates': 1722}
train stats after 55136 examples: {'rewards_train/chosen': '0.075751', 'rewards_train/rejected': '0.0026801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073071', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-144.3', 'loss/train': '0.6608', 'examples_per_second': '31.34', 'grad_norm': '27.625', 'counters/examples': 55136, 'counters/updates': 1723}
train stats after 55168 examples: {'rewards_train/chosen': '0.10475', 'rewards_train/rejected': '0.055536', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04921', 'logps_train/rejected': '-135.69', 'logps_train/chosen': '-162.3', 'loss/train': '0.673', 'examples_per_second': '30.111', 'grad_norm': '35', 'counters/examples': 55168, 'counters/updates': 1724}
train stats after 55200 examples: {'rewards_train/chosen': '0.056231', 'rewards_train/rejected': '0.019964', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036267', 'logps_train/rejected': '-97.172', 'logps_train/chosen': '-170.69', 'loss/train': '0.6821', 'examples_per_second': '31.401', 'grad_norm': '31.75', 'counters/examples': 55200, 'counters/updates': 1725}
train stats after 55232 examples: {'rewards_train/chosen': '0.16476', 'rewards_train/rejected': '0.00072125', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16404', 'logps_train/rejected': '-173.85', 'logps_train/chosen': '-170.81', 'loss/train': '0.62596', 'examples_per_second': '32.227', 'grad_norm': '43.75', 'counters/examples': 55232, 'counters/updates': 1726}
train stats after 55264 examples: {'rewards_train/chosen': '0.095134', 'rewards_train/rejected': '0.086027', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0091072', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-146.12', 'loss/train': '0.70691', 'examples_per_second': '31.778', 'grad_norm': '30.875', 'counters/examples': 55264, 'counters/updates': 1727}
train stats after 55296 examples: {'rewards_train/chosen': '0.093194', 'rewards_train/rejected': '-0.002744', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095938', 'logps_train/rejected': '-99.989', 'logps_train/chosen': '-138.87', 'loss/train': '0.65151', 'examples_per_second': '31.776', 'grad_norm': '29.25', 'counters/examples': 55296, 'counters/updates': 1728}
skipping logging after 55328 examples to avoid logging too frequently
train stats after 55360 examples: {'rewards_train/chosen': '0.087403', 'rewards_train/rejected': '0.05869', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028713', 'logps_train/rejected': '-151.5', 'logps_train/chosen': '-154.55', 'loss/train': '0.68289', 'examples_per_second': '30.064', 'grad_norm': '98', 'counters/examples': 55360, 'counters/updates': 1730}
train stats after 55392 examples: {'rewards_train/chosen': '0.057996', 'rewards_train/rejected': '0.052789', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0052069', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-121.91', 'loss/train': '0.69233', 'examples_per_second': '30.14', 'grad_norm': '28.25', 'counters/examples': 55392, 'counters/updates': 1731}
train stats after 55424 examples: {'rewards_train/chosen': '0.05178', 'rewards_train/rejected': '0.019706', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032074', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-136.84', 'loss/train': '0.68572', 'examples_per_second': '32.654', 'grad_norm': '31.75', 'counters/examples': 55424, 'counters/updates': 1732}
train stats after 55456 examples: {'rewards_train/chosen': '0.052657', 'rewards_train/rejected': '0.052318', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00033901', 'logps_train/rejected': '-181.5', 'logps_train/chosen': '-208.79', 'loss/train': '0.70132', 'examples_per_second': '30.461', 'grad_norm': '39.75', 'counters/examples': 55456, 'counters/updates': 1733}
train stats after 55488 examples: {'rewards_train/chosen': '0.075392', 'rewards_train/rejected': '0.069997', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0053951', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-157.34', 'loss/train': '0.69444', 'examples_per_second': '30.655', 'grad_norm': '41', 'counters/examples': 55488, 'counters/updates': 1734}
skipping logging after 55520 examples to avoid logging too frequently
train stats after 55552 examples: {'rewards_train/chosen': '0.078491', 'rewards_train/rejected': '0.04999', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028501', 'logps_train/rejected': '-161.47', 'logps_train/chosen': '-176.47', 'loss/train': '0.68622', 'examples_per_second': '31.343', 'grad_norm': '38.5', 'counters/examples': 55552, 'counters/updates': 1736}
skipping logging after 55584 examples to avoid logging too frequently
train stats after 55616 examples: {'rewards_train/chosen': '-0.020742', 'rewards_train/rejected': '0.019155', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.039898', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-153.84', 'loss/train': '0.7205', 'examples_per_second': '30.111', 'grad_norm': '37.75', 'counters/examples': 55616, 'counters/updates': 1738}
skipping logging after 55648 examples to avoid logging too frequently
train stats after 55680 examples: {'rewards_train/chosen': '0.06733', 'rewards_train/rejected': '0.0019899', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06534', 'logps_train/rejected': '-157.72', 'logps_train/chosen': '-131.08', 'loss/train': '0.6716', 'examples_per_second': '31.499', 'grad_norm': '34.25', 'counters/examples': 55680, 'counters/updates': 1740}
train stats after 55712 examples: {'rewards_train/chosen': '0.10642', 'rewards_train/rejected': '0.066235', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.040184', 'logps_train/rejected': '-121.74', 'logps_train/chosen': '-165.92', 'loss/train': '0.68903', 'examples_per_second': '30.4', 'grad_norm': '32.75', 'counters/examples': 55712, 'counters/updates': 1741}
train stats after 55744 examples: {'rewards_train/chosen': '0.092353', 'rewards_train/rejected': '0.099432', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0070789', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-131.55', 'loss/train': '0.70784', 'examples_per_second': '31.535', 'grad_norm': '42.25', 'counters/examples': 55744, 'counters/updates': 1742}
train stats after 55776 examples: {'rewards_train/chosen': '0.00044629', 'rewards_train/rejected': '0.031527', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.031081', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-158.17', 'loss/train': '0.71556', 'examples_per_second': '31.504', 'grad_norm': '39.75', 'counters/examples': 55776, 'counters/updates': 1743}
train stats after 55808 examples: {'rewards_train/chosen': '0.031432', 'rewards_train/rejected': '0.0043897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027042', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-150.79', 'loss/train': '0.68709', 'examples_per_second': '31.458', 'grad_norm': '35.75', 'counters/examples': 55808, 'counters/updates': 1744}
skipping logging after 55840 examples to avoid logging too frequently
train stats after 55872 examples: {'rewards_train/chosen': '0.10099', 'rewards_train/rejected': '-0.0059898', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10698', 'logps_train/rejected': '-155.19', 'logps_train/chosen': '-146.24', 'loss/train': '0.64869', 'examples_per_second': '30.004', 'grad_norm': '33.75', 'counters/examples': 55872, 'counters/updates': 1746}
train stats after 55904 examples: {'rewards_train/chosen': '0.15335', 'rewards_train/rejected': '0.012022', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14133', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-173.95', 'loss/train': '0.6347', 'examples_per_second': '31.811', 'grad_norm': '31.375', 'counters/examples': 55904, 'counters/updates': 1747}
train stats after 55936 examples: {'rewards_train/chosen': '0.12588', 'rewards_train/rejected': '0.094669', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.031208', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-125.98', 'loss/train': '0.68377', 'examples_per_second': '30.962', 'grad_norm': '35.75', 'counters/examples': 55936, 'counters/updates': 1748}
train stats after 55968 examples: {'rewards_train/chosen': '0.033675', 'rewards_train/rejected': '0.061606', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027931', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-149.69', 'loss/train': '0.7152', 'examples_per_second': '30.719', 'grad_norm': '32.25', 'counters/examples': 55968, 'counters/updates': 1749}
train stats after 56000 examples: {'rewards_train/chosen': '0.07824', 'rewards_train/rejected': '0.022155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056086', 'logps_train/rejected': '-114.67', 'logps_train/chosen': '-161.52', 'loss/train': '0.67085', 'examples_per_second': '31.364', 'grad_norm': '30.125', 'counters/examples': 56000, 'counters/updates': 1750}
train stats after 56032 examples: {'rewards_train/chosen': '0.041407', 'rewards_train/rejected': '0.019719', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021687', 'logps_train/rejected': '-89.397', 'logps_train/chosen': '-114.17', 'loss/train': '0.68478', 'examples_per_second': '32.259', 'grad_norm': '25.125', 'counters/examples': 56032, 'counters/updates': 1751}
train stats after 56064 examples: {'rewards_train/chosen': '0.058697', 'rewards_train/rejected': '0.0030285', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055668', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-162.81', 'loss/train': '0.67151', 'examples_per_second': '30.168', 'grad_norm': '31', 'counters/examples': 56064, 'counters/updates': 1752}
train stats after 56096 examples: {'rewards_train/chosen': '0.13653', 'rewards_train/rejected': '0.04322', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093314', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-167.68', 'loss/train': '0.66209', 'examples_per_second': '30.542', 'grad_norm': '36.75', 'counters/examples': 56096, 'counters/updates': 1753}
train stats after 56128 examples: {'rewards_train/chosen': '0.039148', 'rewards_train/rejected': '-0.017111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056259', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-146.46', 'loss/train': '0.67642', 'examples_per_second': '31.215', 'grad_norm': '63.75', 'counters/examples': 56128, 'counters/updates': 1754}
train stats after 56160 examples: {'rewards_train/chosen': '0.12553', 'rewards_train/rejected': '0.086445', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039089', 'logps_train/rejected': '-113.23', 'logps_train/chosen': '-143.94', 'loss/train': '0.67658', 'examples_per_second': '31.933', 'grad_norm': '42.75', 'counters/examples': 56160, 'counters/updates': 1755}
train stats after 56192 examples: {'rewards_train/chosen': '0.13577', 'rewards_train/rejected': '0.09757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038202', 'logps_train/rejected': '-140.02', 'logps_train/chosen': '-168.15', 'loss/train': '0.68121', 'examples_per_second': '31.31', 'grad_norm': '38.75', 'counters/examples': 56192, 'counters/updates': 1756}
train stats after 56224 examples: {'rewards_train/chosen': '0.085917', 'rewards_train/rejected': '0.053586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032331', 'logps_train/rejected': '-140.92', 'logps_train/chosen': '-123.63', 'loss/train': '0.68158', 'examples_per_second': '33.186', 'grad_norm': '27.875', 'counters/examples': 56224, 'counters/updates': 1757}
train stats after 56256 examples: {'rewards_train/chosen': '0.057357', 'rewards_train/rejected': '0.12164', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.064279', 'logps_train/rejected': '-173.03', 'logps_train/chosen': '-142.82', 'loss/train': '0.7334', 'examples_per_second': '30.784', 'grad_norm': '43.25', 'counters/examples': 56256, 'counters/updates': 1758}
train stats after 56288 examples: {'rewards_train/chosen': '0.060619', 'rewards_train/rejected': '0.019817', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040802', 'logps_train/rejected': '-120.84', 'logps_train/chosen': '-133.5', 'loss/train': '0.67776', 'examples_per_second': '32.996', 'grad_norm': '31.375', 'counters/examples': 56288, 'counters/updates': 1759}
train stats after 56320 examples: {'rewards_train/chosen': '0.078627', 'rewards_train/rejected': '0.045792', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032835', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-137.6', 'loss/train': '0.68164', 'examples_per_second': '31.036', 'grad_norm': '32.5', 'counters/examples': 56320, 'counters/updates': 1760}
train stats after 56352 examples: {'rewards_train/chosen': '0.063353', 'rewards_train/rejected': '0.027255', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036098', 'logps_train/rejected': '-121.63', 'logps_train/chosen': '-149.19', 'loss/train': '0.67933', 'examples_per_second': '31.544', 'grad_norm': '32.75', 'counters/examples': 56352, 'counters/updates': 1761}
train stats after 56384 examples: {'rewards_train/chosen': '0.02326', 'rewards_train/rejected': '-0.018895', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.042155', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-170.91', 'loss/train': '0.68015', 'examples_per_second': '31.571', 'grad_norm': '31.5', 'counters/examples': 56384, 'counters/updates': 1762}
skipping logging after 56416 examples to avoid logging too frequently
train stats after 56448 examples: {'rewards_train/chosen': '0.059516', 'rewards_train/rejected': '0.010748', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.048768', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-139.49', 'loss/train': '0.67174', 'examples_per_second': '33.442', 'grad_norm': '27.5', 'counters/examples': 56448, 'counters/updates': 1764}
train stats after 56480 examples: {'rewards_train/chosen': '0.094492', 'rewards_train/rejected': '0.01365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080842', 'logps_train/rejected': '-106.12', 'logps_train/chosen': '-142.02', 'loss/train': '0.65768', 'examples_per_second': '30.25', 'grad_norm': '26.125', 'counters/examples': 56480, 'counters/updates': 1765}
skipping logging after 56512 examples to avoid logging too frequently
train stats after 56544 examples: {'rewards_train/chosen': '0.09073', 'rewards_train/rejected': '0.063793', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026937', 'logps_train/rejected': '-139.94', 'logps_train/chosen': '-151.68', 'loss/train': '0.68719', 'examples_per_second': '31.967', 'grad_norm': '33', 'counters/examples': 56544, 'counters/updates': 1767}
skipping logging after 56576 examples to avoid logging too frequently
train stats after 56608 examples: {'rewards_train/chosen': '0.075584', 'rewards_train/rejected': '0.037804', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03778', 'logps_train/rejected': '-106.53', 'logps_train/chosen': '-129.34', 'loss/train': '0.67955', 'examples_per_second': '33.204', 'grad_norm': '27.125', 'counters/examples': 56608, 'counters/updates': 1769}
skipping logging after 56640 examples to avoid logging too frequently
train stats after 56672 examples: {'rewards_train/chosen': '0.079508', 'rewards_train/rejected': '0.052176', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027332', 'logps_train/rejected': '-103.35', 'logps_train/chosen': '-129.71', 'loss/train': '0.68493', 'examples_per_second': '29.942', 'grad_norm': '32.25', 'counters/examples': 56672, 'counters/updates': 1771}
skipping logging after 56704 examples to avoid logging too frequently
train stats after 56736 examples: {'rewards_train/chosen': '0.055237', 'rewards_train/rejected': '0.087877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.03264', 'logps_train/rejected': '-102.96', 'logps_train/chosen': '-132', 'loss/train': '0.71544', 'examples_per_second': '32.002', 'grad_norm': '30.875', 'counters/examples': 56736, 'counters/updates': 1773}
train stats after 56768 examples: {'rewards_train/chosen': '0.035614', 'rewards_train/rejected': '0.011215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024399', 'logps_train/rejected': '-138.34', 'logps_train/chosen': '-163.9', 'loss/train': '0.68915', 'examples_per_second': '31.503', 'grad_norm': '33.75', 'counters/examples': 56768, 'counters/updates': 1774}
skipping logging after 56800 examples to avoid logging too frequently
train stats after 56832 examples: {'rewards_train/chosen': '0.071667', 'rewards_train/rejected': '0.047256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024411', 'logps_train/rejected': '-86.376', 'logps_train/chosen': '-112.47', 'loss/train': '0.68392', 'examples_per_second': '32.096', 'grad_norm': '26.75', 'counters/examples': 56832, 'counters/updates': 1776}
skipping logging after 56864 examples to avoid logging too frequently
train stats after 56896 examples: {'rewards_train/chosen': '0.022907', 'rewards_train/rejected': '-0.0058922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028799', 'logps_train/rejected': '-138.35', 'logps_train/chosen': '-138.56', 'loss/train': '0.6838', 'examples_per_second': '34.226', 'grad_norm': '29.375', 'counters/examples': 56896, 'counters/updates': 1778}
train stats after 56928 examples: {'rewards_train/chosen': '0.13582', 'rewards_train/rejected': '0.049945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085873', 'logps_train/rejected': '-125', 'logps_train/chosen': '-142.13', 'loss/train': '0.65759', 'examples_per_second': '31.34', 'grad_norm': '25.625', 'counters/examples': 56928, 'counters/updates': 1779}
train stats after 56960 examples: {'rewards_train/chosen': '0.048171', 'rewards_train/rejected': '0.016935', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031236', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-140.28', 'loss/train': '0.68105', 'examples_per_second': '31.559', 'grad_norm': '29.875', 'counters/examples': 56960, 'counters/updates': 1780}
train stats after 56992 examples: {'rewards_train/chosen': '0.13257', 'rewards_train/rejected': '0.026054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10651', 'logps_train/rejected': '-138.36', 'logps_train/chosen': '-157.8', 'loss/train': '0.6496', 'examples_per_second': '30.535', 'grad_norm': '31.375', 'counters/examples': 56992, 'counters/updates': 1781}
train stats after 57024 examples: {'rewards_train/chosen': '0.043838', 'rewards_train/rejected': '0.030637', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013201', 'logps_train/rejected': '-137.36', 'logps_train/chosen': '-127.17', 'loss/train': '0.6908', 'examples_per_second': '32.342', 'grad_norm': '28.25', 'counters/examples': 57024, 'counters/updates': 1782}
skipping logging after 57056 examples to avoid logging too frequently
train stats after 57088 examples: {'rewards_train/chosen': '0.064241', 'rewards_train/rejected': '0.040343', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023898', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-176.09', 'loss/train': '0.68626', 'examples_per_second': '31.568', 'grad_norm': '31.625', 'counters/examples': 57088, 'counters/updates': 1784}
train stats after 57120 examples: {'rewards_train/chosen': '0.08164', 'rewards_train/rejected': '-0.011064', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092705', 'logps_train/rejected': '-169.19', 'logps_train/chosen': '-149.22', 'loss/train': '0.6545', 'examples_per_second': '31.581', 'grad_norm': '33.5', 'counters/examples': 57120, 'counters/updates': 1785}
train stats after 57152 examples: {'rewards_train/chosen': '0.080012', 'rewards_train/rejected': '0.03747', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.042541', 'logps_train/rejected': '-117.51', 'logps_train/chosen': '-108.6', 'loss/train': '0.67762', 'examples_per_second': '30.513', 'grad_norm': '24.5', 'counters/examples': 57152, 'counters/updates': 1786}
train stats after 57184 examples: {'rewards_train/chosen': '0.080392', 'rewards_train/rejected': '-0.00092332', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081316', 'logps_train/rejected': '-123', 'logps_train/chosen': '-126.08', 'loss/train': '0.65717', 'examples_per_second': '30.104', 'grad_norm': '26.5', 'counters/examples': 57184, 'counters/updates': 1787}
train stats after 57216 examples: {'rewards_train/chosen': '0.041975', 'rewards_train/rejected': '0.018486', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02349', 'logps_train/rejected': '-122.73', 'logps_train/chosen': '-177.89', 'loss/train': '0.6869', 'examples_per_second': '31.576', 'grad_norm': '30.125', 'counters/examples': 57216, 'counters/updates': 1788}
skipping logging after 57248 examples to avoid logging too frequently
train stats after 57280 examples: {'rewards_train/chosen': '0.10905', 'rewards_train/rejected': '0.056915', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.052132', 'logps_train/rejected': '-107.5', 'logps_train/chosen': '-131.44', 'loss/train': '0.67565', 'examples_per_second': '31.561', 'grad_norm': '27.375', 'counters/examples': 57280, 'counters/updates': 1790}
train stats after 57312 examples: {'rewards_train/chosen': '0.063422', 'rewards_train/rejected': '-0.0020135', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065435', 'logps_train/rejected': '-144.16', 'logps_train/chosen': '-123.54', 'loss/train': '0.66531', 'examples_per_second': '31.494', 'grad_norm': '34.75', 'counters/examples': 57312, 'counters/updates': 1791}
train stats after 57344 examples: {'rewards_train/chosen': '0.063769', 'rewards_train/rejected': '0.073574', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0098048', 'logps_train/rejected': '-96.367', 'logps_train/chosen': '-129.98', 'loss/train': '0.70422', 'examples_per_second': '30.254', 'grad_norm': '38', 'counters/examples': 57344, 'counters/updates': 1792}
train stats after 57376 examples: {'rewards_train/chosen': '0.077462', 'rewards_train/rejected': '0.0025762', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.074886', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-157.84', 'loss/train': '0.66532', 'examples_per_second': '23.586', 'grad_norm': '34.75', 'counters/examples': 57376, 'counters/updates': 1793}
train stats after 57408 examples: {'rewards_train/chosen': '0.10056', 'rewards_train/rejected': '0.10548', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0049244', 'logps_train/rejected': '-189.24', 'logps_train/chosen': '-176.29', 'loss/train': '0.70356', 'examples_per_second': '31.5', 'grad_norm': '43.75', 'counters/examples': 57408, 'counters/updates': 1794}
skipping logging after 57440 examples to avoid logging too frequently
train stats after 57472 examples: {'rewards_train/chosen': '0.12051', 'rewards_train/rejected': '0.11521', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0052961', 'logps_train/rejected': '-155.06', 'logps_train/chosen': '-117.06', 'loss/train': '0.71801', 'examples_per_second': '24.415', 'grad_norm': '87.5', 'counters/examples': 57472, 'counters/updates': 1796}
train stats after 57504 examples: {'rewards_train/chosen': '0.15709', 'rewards_train/rejected': '0.093967', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063119', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-157.93', 'loss/train': '0.6687', 'examples_per_second': '31.629', 'grad_norm': '38.5', 'counters/examples': 57504, 'counters/updates': 1797}
train stats after 57536 examples: {'rewards_train/chosen': '0.074803', 'rewards_train/rejected': '0.047816', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026988', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-125.39', 'loss/train': '0.69159', 'examples_per_second': '32.288', 'grad_norm': '45.75', 'counters/examples': 57536, 'counters/updates': 1798}
train stats after 57568 examples: {'rewards_train/chosen': '0.11292', 'rewards_train/rejected': '0.091332', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021583', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-164.14', 'loss/train': '0.68769', 'examples_per_second': '31.213', 'grad_norm': '33', 'counters/examples': 57568, 'counters/updates': 1799}
train stats after 57600 examples: {'rewards_train/chosen': '0.15034', 'rewards_train/rejected': '0.091941', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058396', 'logps_train/rejected': '-125.71', 'logps_train/chosen': '-154.65', 'loss/train': '0.67107', 'examples_per_second': '32.331', 'grad_norm': '44', 'counters/examples': 57600, 'counters/updates': 1800}
train stats after 57632 examples: {'rewards_train/chosen': '0.14536', 'rewards_train/rejected': '0.087352', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058005', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-147.83', 'loss/train': '0.6695', 'examples_per_second': '32.311', 'grad_norm': '39.25', 'counters/examples': 57632, 'counters/updates': 1801}
train stats after 57664 examples: {'rewards_train/chosen': '0.056503', 'rewards_train/rejected': '0.012451', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044052', 'logps_train/rejected': '-142.27', 'logps_train/chosen': '-128.72', 'loss/train': '0.6837', 'examples_per_second': '30.577', 'grad_norm': '28.125', 'counters/examples': 57664, 'counters/updates': 1802}
train stats after 57696 examples: {'rewards_train/chosen': '0.094821', 'rewards_train/rejected': '0.085827', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0089934', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-173.49', 'loss/train': '0.69954', 'examples_per_second': '30.454', 'grad_norm': '38', 'counters/examples': 57696, 'counters/updates': 1803}
train stats after 57728 examples: {'rewards_train/chosen': '0.098429', 'rewards_train/rejected': '-0.0086789', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10711', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-141.21', 'loss/train': '0.65068', 'examples_per_second': '31.374', 'grad_norm': '28', 'counters/examples': 57728, 'counters/updates': 1804}
train stats after 57760 examples: {'rewards_train/chosen': '0.043106', 'rewards_train/rejected': '0.034763', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0083434', 'logps_train/rejected': '-151.23', 'logps_train/chosen': '-155.19', 'loss/train': '0.69734', 'examples_per_second': '30.011', 'grad_norm': '53.5', 'counters/examples': 57760, 'counters/updates': 1805}
train stats after 57792 examples: {'rewards_train/chosen': '0.10082', 'rewards_train/rejected': '0.066685', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034137', 'logps_train/rejected': '-158.58', 'logps_train/chosen': '-180.27', 'loss/train': '0.6863', 'examples_per_second': '31.53', 'grad_norm': '33.25', 'counters/examples': 57792, 'counters/updates': 1806}
train stats after 57824 examples: {'rewards_train/chosen': '0.030174', 'rewards_train/rejected': '0.084028', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.053854', 'logps_train/rejected': '-137.8', 'logps_train/chosen': '-189.99', 'loss/train': '0.73702', 'examples_per_second': '30.249', 'grad_norm': '47.25', 'counters/examples': 57824, 'counters/updates': 1807}
skipping logging after 57856 examples to avoid logging too frequently
train stats after 57888 examples: {'rewards_train/chosen': '0.14624', 'rewards_train/rejected': '0.053029', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09321', 'logps_train/rejected': '-102.06', 'logps_train/chosen': '-139.61', 'loss/train': '0.65362', 'examples_per_second': '31.253', 'grad_norm': '41.25', 'counters/examples': 57888, 'counters/updates': 1809}
skipping logging after 57920 examples to avoid logging too frequently
train stats after 57952 examples: {'rewards_train/chosen': '0.028389', 'rewards_train/rejected': '-0.00054993', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028939', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-127.9', 'loss/train': '0.68189', 'examples_per_second': '35.998', 'grad_norm': '27', 'counters/examples': 57952, 'counters/updates': 1811}
train stats after 57984 examples: {'rewards_train/chosen': '-0.01451', 'rewards_train/rejected': '0.032472', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.046982', 'logps_train/rejected': '-122.57', 'logps_train/chosen': '-133.52', 'loss/train': '0.72122', 'examples_per_second': '31.069', 'grad_norm': '28.375', 'counters/examples': 57984, 'counters/updates': 1812}
skipping logging after 58016 examples to avoid logging too frequently
train stats after 58048 examples: {'rewards_train/chosen': '0.064233', 'rewards_train/rejected': '0.070577', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0063437', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-124.83', 'loss/train': '0.69998', 'examples_per_second': '36.459', 'grad_norm': '26.625', 'counters/examples': 58048, 'counters/updates': 1814}
train stats after 58080 examples: {'rewards_train/chosen': '0.08883', 'rewards_train/rejected': '-0.0095908', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098421', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-167.63', 'loss/train': '0.65353', 'examples_per_second': '29.821', 'grad_norm': '34.25', 'counters/examples': 58080, 'counters/updates': 1815}
train stats after 58112 examples: {'rewards_train/chosen': '0.075558', 'rewards_train/rejected': '0.014362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061195', 'logps_train/rejected': '-106.68', 'logps_train/chosen': '-118.38', 'loss/train': '0.66762', 'examples_per_second': '31.535', 'grad_norm': '29.375', 'counters/examples': 58112, 'counters/updates': 1816}
train stats after 58144 examples: {'rewards_train/chosen': '0.12075', 'rewards_train/rejected': '-0.0038709', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12462', 'logps_train/rejected': '-112.15', 'logps_train/chosen': '-148.83', 'loss/train': '0.64324', 'examples_per_second': '30.248', 'grad_norm': '26.625', 'counters/examples': 58144, 'counters/updates': 1817}
skipping logging after 58176 examples to avoid logging too frequently
train stats after 58208 examples: {'rewards_train/chosen': '0.083885', 'rewards_train/rejected': '0.054413', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029472', 'logps_train/rejected': '-99.517', 'logps_train/chosen': '-158.28', 'loss/train': '0.68452', 'examples_per_second': '34.834', 'grad_norm': '44.5', 'counters/examples': 58208, 'counters/updates': 1819}
train stats after 58240 examples: {'rewards_train/chosen': '0.070673', 'rewards_train/rejected': '0.010647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060026', 'logps_train/rejected': '-100.69', 'logps_train/chosen': '-136.84', 'loss/train': '0.67157', 'examples_per_second': '31.594', 'grad_norm': '28.375', 'counters/examples': 58240, 'counters/updates': 1820}
train stats after 58272 examples: {'rewards_train/chosen': '0.06259', 'rewards_train/rejected': '0.043132', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019458', 'logps_train/rejected': '-103.7', 'logps_train/chosen': '-153.25', 'loss/train': '0.68821', 'examples_per_second': '31.591', 'grad_norm': '35', 'counters/examples': 58272, 'counters/updates': 1821}
train stats after 58304 examples: {'rewards_train/chosen': '0.096578', 'rewards_train/rejected': '0.17145', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.074871', 'logps_train/rejected': '-161.57', 'logps_train/chosen': '-140.51', 'loss/train': '0.74957', 'examples_per_second': '32.624', 'grad_norm': '118', 'counters/examples': 58304, 'counters/updates': 1822}
train stats after 58336 examples: {'rewards_train/chosen': '0.076502', 'rewards_train/rejected': '0.0504', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026102', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-176.89', 'loss/train': '0.68638', 'examples_per_second': '31.404', 'grad_norm': '30.75', 'counters/examples': 58336, 'counters/updates': 1823}
train stats after 58368 examples: {'rewards_train/chosen': '0.12152', 'rewards_train/rejected': '0.0028476', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11867', 'logps_train/rejected': '-114.92', 'logps_train/chosen': '-140.21', 'loss/train': '0.6394', 'examples_per_second': '31.25', 'grad_norm': '26.5', 'counters/examples': 58368, 'counters/updates': 1824}
skipping logging after 58400 examples to avoid logging too frequently
train stats after 58432 examples: {'rewards_train/chosen': '0.064351', 'rewards_train/rejected': '0.044439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019911', 'logps_train/rejected': '-103.88', 'logps_train/chosen': '-124.9', 'loss/train': '0.68837', 'examples_per_second': '33.044', 'grad_norm': '25.375', 'counters/examples': 58432, 'counters/updates': 1826}
skipping logging after 58464 examples to avoid logging too frequently
train stats after 58496 examples: {'rewards_train/chosen': '0.086836', 'rewards_train/rejected': '0.049362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037474', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-146.17', 'loss/train': '0.68254', 'examples_per_second': '30.558', 'grad_norm': '28.25', 'counters/examples': 58496, 'counters/updates': 1828}
train stats after 58528 examples: {'rewards_train/chosen': '0.11962', 'rewards_train/rejected': '0.070464', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.049152', 'logps_train/rejected': '-135.43', 'logps_train/chosen': '-146.96', 'loss/train': '0.67558', 'examples_per_second': '30.902', 'grad_norm': '31.125', 'counters/examples': 58528, 'counters/updates': 1829}
train stats after 58560 examples: {'rewards_train/chosen': '0.062871', 'rewards_train/rejected': '0.07173', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0088592', 'logps_train/rejected': '-136.51', 'logps_train/chosen': '-167.72', 'loss/train': '0.71113', 'examples_per_second': '31.559', 'grad_norm': '60', 'counters/examples': 58560, 'counters/updates': 1830}
train stats after 58592 examples: {'rewards_train/chosen': '0.062303', 'rewards_train/rejected': '0.095443', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03314', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-141.68', 'loss/train': '0.71676', 'examples_per_second': '31.542', 'grad_norm': '35.5', 'counters/examples': 58592, 'counters/updates': 1831}
train stats after 58624 examples: {'rewards_train/chosen': '0.095604', 'rewards_train/rejected': '0.067805', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027799', 'logps_train/rejected': '-121.8', 'logps_train/chosen': '-148.26', 'loss/train': '0.68894', 'examples_per_second': '26.141', 'grad_norm': '59.5', 'counters/examples': 58624, 'counters/updates': 1832}
train stats after 58656 examples: {'rewards_train/chosen': '0.063843', 'rewards_train/rejected': '0.087467', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.023624', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-137.33', 'loss/train': '0.71467', 'examples_per_second': '30.042', 'grad_norm': '42.25', 'counters/examples': 58656, 'counters/updates': 1833}
train stats after 58688 examples: {'rewards_train/chosen': '0.027381', 'rewards_train/rejected': '0.074472', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.04709', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-128.62', 'loss/train': '0.7269', 'examples_per_second': '30.097', 'grad_norm': '40.75', 'counters/examples': 58688, 'counters/updates': 1834}
train stats after 58720 examples: {'rewards_train/chosen': '0.074396', 'rewards_train/rejected': '-0.0054745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07987', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-137.51', 'loss/train': '0.6575', 'examples_per_second': '32.013', 'grad_norm': '25.875', 'counters/examples': 58720, 'counters/updates': 1835}
train stats after 58752 examples: {'rewards_train/chosen': '0.069673', 'rewards_train/rejected': '0.0069279', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062745', 'logps_train/rejected': '-117.8', 'logps_train/chosen': '-130.31', 'loss/train': '0.66621', 'examples_per_second': '30.335', 'grad_norm': '29.125', 'counters/examples': 58752, 'counters/updates': 1836}
train stats after 58784 examples: {'rewards_train/chosen': '0.046085', 'rewards_train/rejected': '0.02078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025305', 'logps_train/rejected': '-124.23', 'logps_train/chosen': '-133.25', 'loss/train': '0.6853', 'examples_per_second': '30.459', 'grad_norm': '28.625', 'counters/examples': 58784, 'counters/updates': 1837}
train stats after 58816 examples: {'rewards_train/chosen': '0.075401', 'rewards_train/rejected': '0.10046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025057', 'logps_train/rejected': '-175.79', 'logps_train/chosen': '-173.78', 'loss/train': '0.71502', 'examples_per_second': '31.531', 'grad_norm': '49.5', 'counters/examples': 58816, 'counters/updates': 1838}
train stats after 58848 examples: {'rewards_train/chosen': '0.054914', 'rewards_train/rejected': '0.023821', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031092', 'logps_train/rejected': '-134.97', 'logps_train/chosen': '-116.55', 'loss/train': '0.68239', 'examples_per_second': '30.194', 'grad_norm': '30.75', 'counters/examples': 58848, 'counters/updates': 1839}
train stats after 58880 examples: {'rewards_train/chosen': '0.11828', 'rewards_train/rejected': '0.064707', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.05357', 'logps_train/rejected': '-118.98', 'logps_train/chosen': '-146.16', 'loss/train': '0.67674', 'examples_per_second': '30.146', 'grad_norm': '26', 'counters/examples': 58880, 'counters/updates': 1840}
skipping logging after 58912 examples to avoid logging too frequently
train stats after 58944 examples: {'rewards_train/chosen': '0.056917', 'rewards_train/rejected': '0.056888', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '2.8992e-05', 'logps_train/rejected': '-126.72', 'logps_train/chosen': '-146.86', 'loss/train': '0.70328', 'examples_per_second': '31.54', 'grad_norm': '33', 'counters/examples': 58944, 'counters/updates': 1842}
train stats after 58976 examples: {'rewards_train/chosen': '0.10401', 'rewards_train/rejected': '0.034768', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069239', 'logps_train/rejected': '-171.12', 'logps_train/chosen': '-182.27', 'loss/train': '0.66891', 'examples_per_second': '30.916', 'grad_norm': '31.625', 'counters/examples': 58976, 'counters/updates': 1843}
skipping logging after 59008 examples to avoid logging too frequently
train stats after 59040 examples: {'rewards_train/chosen': '0.17024', 'rewards_train/rejected': '0.079669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090576', 'logps_train/rejected': '-175.37', 'logps_train/chosen': '-131.04', 'loss/train': '0.65813', 'examples_per_second': '30.957', 'grad_norm': '41.25', 'counters/examples': 59040, 'counters/updates': 1845}
train stats after 59072 examples: {'rewards_train/chosen': '0.064508', 'rewards_train/rejected': '0.073764', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0092555', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-132.74', 'loss/train': '0.70086', 'examples_per_second': '30.838', 'grad_norm': '37', 'counters/examples': 59072, 'counters/updates': 1846}
skipping logging after 59104 examples to avoid logging too frequently
train stats after 59136 examples: {'rewards_train/chosen': '0.1458', 'rewards_train/rejected': '0.073732', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07207', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-171.82', 'loss/train': '0.6644', 'examples_per_second': '31.989', 'grad_norm': '37.25', 'counters/examples': 59136, 'counters/updates': 1848}
train stats after 59168 examples: {'rewards_train/chosen': '0.075088', 'rewards_train/rejected': '0.031932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043156', 'logps_train/rejected': '-149.42', 'logps_train/chosen': '-145.84', 'loss/train': '0.67525', 'examples_per_second': '31.562', 'grad_norm': '32.5', 'counters/examples': 59168, 'counters/updates': 1849}
train stats after 59200 examples: {'rewards_train/chosen': '0.081877', 'rewards_train/rejected': '0.09585', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.013973', 'logps_train/rejected': '-123.08', 'logps_train/chosen': '-130.48', 'loss/train': '0.71152', 'examples_per_second': '30.855', 'grad_norm': '37.75', 'counters/examples': 59200, 'counters/updates': 1850}
train stats after 59232 examples: {'rewards_train/chosen': '0.028545', 'rewards_train/rejected': '0.053784', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.025239', 'logps_train/rejected': '-132.21', 'logps_train/chosen': '-123.99', 'loss/train': '0.71152', 'examples_per_second': '30.432', 'grad_norm': '34.5', 'counters/examples': 59232, 'counters/updates': 1851}
train stats after 59264 examples: {'rewards_train/chosen': '0.053591', 'rewards_train/rejected': '0.044732', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0088591', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-166.39', 'loss/train': '0.69704', 'examples_per_second': '30.566', 'grad_norm': '32', 'counters/examples': 59264, 'counters/updates': 1852}
skipping logging after 59296 examples to avoid logging too frequently
train stats after 59328 examples: {'rewards_train/chosen': '0.076444', 'rewards_train/rejected': '0.06331', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013133', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-131.12', 'loss/train': '0.69325', 'examples_per_second': '30.529', 'grad_norm': '39.25', 'counters/examples': 59328, 'counters/updates': 1854}
train stats after 59360 examples: {'rewards_train/chosen': '0.029748', 'rewards_train/rejected': '0.065894', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036146', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-149.78', 'loss/train': '0.71622', 'examples_per_second': '31.538', 'grad_norm': '28', 'counters/examples': 59360, 'counters/updates': 1855}
skipping logging after 59392 examples to avoid logging too frequently
train stats after 59424 examples: {'rewards_train/chosen': '0.13857', 'rewards_train/rejected': '0.04857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090001', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-160.06', 'loss/train': '0.65609', 'examples_per_second': '31.501', 'grad_norm': '36.5', 'counters/examples': 59424, 'counters/updates': 1857}
train stats after 59456 examples: {'rewards_train/chosen': '-0.0073002', 'rewards_train/rejected': '0.066147', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.073447', 'logps_train/rejected': '-167.62', 'logps_train/chosen': '-161.57', 'loss/train': '0.7362', 'examples_per_second': '30.868', 'grad_norm': '32.5', 'counters/examples': 59456, 'counters/updates': 1858}
train stats after 59488 examples: {'rewards_train/chosen': '0.12176', 'rewards_train/rejected': '0.031707', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090051', 'logps_train/rejected': '-210.8', 'logps_train/chosen': '-173.12', 'loss/train': '0.66723', 'examples_per_second': '31.503', 'grad_norm': '42', 'counters/examples': 59488, 'counters/updates': 1859}
train stats after 59520 examples: {'rewards_train/chosen': '0.12028', 'rewards_train/rejected': '0.014988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10529', 'logps_train/rejected': '-130.14', 'logps_train/chosen': '-135.96', 'loss/train': '0.65512', 'examples_per_second': '31.083', 'grad_norm': '27.875', 'counters/examples': 59520, 'counters/updates': 1860}
train stats after 59552 examples: {'rewards_train/chosen': '0.02413', 'rewards_train/rejected': '0.0038994', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02023', 'logps_train/rejected': '-115.91', 'logps_train/chosen': '-114.12', 'loss/train': '0.68618', 'examples_per_second': '31.833', 'grad_norm': '24.25', 'counters/examples': 59552, 'counters/updates': 1861}
train stats after 59584 examples: {'rewards_train/chosen': '0.10043', 'rewards_train/rejected': '0.0081478', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092287', 'logps_train/rejected': '-164.02', 'logps_train/chosen': '-160.68', 'loss/train': '0.65583', 'examples_per_second': '31.244', 'grad_norm': '38', 'counters/examples': 59584, 'counters/updates': 1862}
train stats after 59616 examples: {'rewards_train/chosen': '0.096984', 'rewards_train/rejected': '-0.017197', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11418', 'logps_train/rejected': '-84.316', 'logps_train/chosen': '-147.03', 'loss/train': '0.65138', 'examples_per_second': '31.992', 'grad_norm': '31.75', 'counters/examples': 59616, 'counters/updates': 1863}
skipping logging after 59648 examples to avoid logging too frequently
train stats after 59680 examples: {'rewards_train/chosen': '0.058834', 'rewards_train/rejected': '0.02928', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029554', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-156.79', 'loss/train': '0.68085', 'examples_per_second': '31.474', 'grad_norm': '33.25', 'counters/examples': 59680, 'counters/updates': 1865}
skipping logging after 59712 examples to avoid logging too frequently
train stats after 59744 examples: {'rewards_train/chosen': '0.053488', 'rewards_train/rejected': '0.059748', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0062604', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-125.37', 'loss/train': '0.70342', 'examples_per_second': '30.645', 'grad_norm': '32', 'counters/examples': 59744, 'counters/updates': 1867}
skipping logging after 59776 examples to avoid logging too frequently
train stats after 59808 examples: {'rewards_train/chosen': '0.071334', 'rewards_train/rejected': '0.038819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032515', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-158.25', 'loss/train': '0.68024', 'examples_per_second': '33.695', 'grad_norm': '29.625', 'counters/examples': 59808, 'counters/updates': 1869}
train stats after 59840 examples: {'rewards_train/chosen': '0.043977', 'rewards_train/rejected': '0.022609', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021369', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-119.03', 'loss/train': '0.68676', 'examples_per_second': '32.291', 'grad_norm': '25.5', 'counters/examples': 59840, 'counters/updates': 1870}
train stats after 59872 examples: {'rewards_train/chosen': '0.032168', 'rewards_train/rejected': '0.071109', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.038941', 'logps_train/rejected': '-127.26', 'logps_train/chosen': '-127.95', 'loss/train': '0.71946', 'examples_per_second': '30.072', 'grad_norm': '36.25', 'counters/examples': 59872, 'counters/updates': 1871}
train stats after 59904 examples: {'rewards_train/chosen': '0.044279', 'rewards_train/rejected': '0.012215', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032064', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-140.84', 'loss/train': '0.67961', 'examples_per_second': '29.954', 'grad_norm': '31.5', 'counters/examples': 59904, 'counters/updates': 1872}
train stats after 59936 examples: {'rewards_train/chosen': '0.12463', 'rewards_train/rejected': '0.059521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065113', 'logps_train/rejected': '-115', 'logps_train/chosen': '-143.37', 'loss/train': '0.67005', 'examples_per_second': '31.684', 'grad_norm': '30.875', 'counters/examples': 59936, 'counters/updates': 1873}
train stats after 59968 examples: {'rewards_train/chosen': '-0.0079376', 'rewards_train/rejected': '-0.067872', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059934', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-132.04', 'loss/train': '0.6779', 'examples_per_second': '30.167', 'grad_norm': '25.75', 'counters/examples': 59968, 'counters/updates': 1874}
train stats after 60000 examples: {'rewards_train/chosen': '0.037316', 'rewards_train/rejected': '-0.0091637', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04648', 'logps_train/rejected': '-151.11', 'logps_train/chosen': '-162.81', 'loss/train': '0.67647', 'examples_per_second': '30.148', 'grad_norm': '30.5', 'counters/examples': 60000, 'counters/updates': 1875}
Running evaluation after 60000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 60000: {'rewards_eval/chosen': '0.078902', 'rewards_eval/rejected': '0.034983', 'rewards_eval/accuracies': '0.55859', 'rewards_eval/margins': '0.04392', 'logps_eval/rejected': '-121.78', 'logps_eval/chosen': '-143.32', 'loss/eval': '0.6784'}
skipping save for non epoch
train stats after 60032 examples: {'rewards_train/chosen': '0.10983', 'rewards_train/rejected': '-0.0082946', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11812', 'logps_train/rejected': '-147.23', 'logps_train/chosen': '-169.61', 'loss/train': '0.64538', 'examples_per_second': '31.354', 'grad_norm': '28.875', 'counters/examples': 60032, 'counters/updates': 1876}
train stats after 60064 examples: {'rewards_train/chosen': '0.060507', 'rewards_train/rejected': '0.064136', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0036295', 'logps_train/rejected': '-138.51', 'logps_train/chosen': '-174.05', 'loss/train': '0.70034', 'examples_per_second': '32.463', 'grad_norm': '35.75', 'counters/examples': 60064, 'counters/updates': 1877}
train stats after 60096 examples: {'rewards_train/chosen': '0.14342', 'rewards_train/rejected': '0.041117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10231', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-146.62', 'loss/train': '0.65225', 'examples_per_second': '31.589', 'grad_norm': '30.125', 'counters/examples': 60096, 'counters/updates': 1878}
train stats after 60128 examples: {'rewards_train/chosen': '0.053586', 'rewards_train/rejected': '0.11056', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056978', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-154.78', 'loss/train': '0.72821', 'examples_per_second': '31.21', 'grad_norm': '39', 'counters/examples': 60128, 'counters/updates': 1879}
train stats after 60160 examples: {'rewards_train/chosen': '0.087825', 'rewards_train/rejected': '0.040799', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047027', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-161.5', 'loss/train': '0.68009', 'examples_per_second': '32.632', 'grad_norm': '28.75', 'counters/examples': 60160, 'counters/updates': 1880}
train stats after 60192 examples: {'rewards_train/chosen': '0.063818', 'rewards_train/rejected': '0.0079169', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055901', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-118.05', 'loss/train': '0.6709', 'examples_per_second': '32.507', 'grad_norm': '27.75', 'counters/examples': 60192, 'counters/updates': 1881}
train stats after 60224 examples: {'rewards_train/chosen': '0.13687', 'rewards_train/rejected': '0.048631', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08824', 'logps_train/rejected': '-120.89', 'logps_train/chosen': '-187.77', 'loss/train': '0.67421', 'examples_per_second': '31.761', 'grad_norm': '43', 'counters/examples': 60224, 'counters/updates': 1882}
train stats after 60256 examples: {'rewards_train/chosen': '0.057318', 'rewards_train/rejected': '0.057763', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00044495', 'logps_train/rejected': '-101.59', 'logps_train/chosen': '-115.78', 'loss/train': '0.69929', 'examples_per_second': '30.641', 'grad_norm': '36.75', 'counters/examples': 60256, 'counters/updates': 1883}
train stats after 60288 examples: {'rewards_train/chosen': '0.069299', 'rewards_train/rejected': '0.050762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.018537', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-141.39', 'loss/train': '0.68887', 'examples_per_second': '29.955', 'grad_norm': '29.75', 'counters/examples': 60288, 'counters/updates': 1884}
train stats after 60320 examples: {'rewards_train/chosen': '0.090507', 'rewards_train/rejected': '0.082589', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0079178', 'logps_train/rejected': '-144.17', 'logps_train/chosen': '-129.87', 'loss/train': '0.69731', 'examples_per_second': '30.16', 'grad_norm': '27.125', 'counters/examples': 60320, 'counters/updates': 1885}
train stats after 60352 examples: {'rewards_train/chosen': '0.084134', 'rewards_train/rejected': '0.0065671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077567', 'logps_train/rejected': '-106.24', 'logps_train/chosen': '-133.32', 'loss/train': '0.66186', 'examples_per_second': '30.69', 'grad_norm': '28.125', 'counters/examples': 60352, 'counters/updates': 1886}
train stats after 60384 examples: {'rewards_train/chosen': '0.019992', 'rewards_train/rejected': '-0.015842', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035834', 'logps_train/rejected': '-109.75', 'logps_train/chosen': '-147.41', 'loss/train': '0.68143', 'examples_per_second': '31.421', 'grad_norm': '39.5', 'counters/examples': 60384, 'counters/updates': 1887}
train stats after 60416 examples: {'rewards_train/chosen': '0.088782', 'rewards_train/rejected': '0.078116', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010665', 'logps_train/rejected': '-134.29', 'logps_train/chosen': '-152.2', 'loss/train': '0.70218', 'examples_per_second': '31.535', 'grad_norm': '36.25', 'counters/examples': 60416, 'counters/updates': 1888}
train stats after 60448 examples: {'rewards_train/chosen': '0.062539', 'rewards_train/rejected': '0.081416', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018877', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-146.77', 'loss/train': '0.7139', 'examples_per_second': '31.31', 'grad_norm': '40', 'counters/examples': 60448, 'counters/updates': 1889}
train stats after 60480 examples: {'rewards_train/chosen': '-0.0079601', 'rewards_train/rejected': '0.043388', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.051348', 'logps_train/rejected': '-97.111', 'logps_train/chosen': '-110.22', 'loss/train': '0.72622', 'examples_per_second': '31.7', 'grad_norm': '44', 'counters/examples': 60480, 'counters/updates': 1890}
train stats after 60512 examples: {'rewards_train/chosen': '0.061767', 'rewards_train/rejected': '0.027077', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03469', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-138.46', 'loss/train': '0.68271', 'examples_per_second': '31.828', 'grad_norm': '28.5', 'counters/examples': 60512, 'counters/updates': 1891}
train stats after 60544 examples: {'rewards_train/chosen': '0.045594', 'rewards_train/rejected': '0.080885', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035291', 'logps_train/rejected': '-106.81', 'logps_train/chosen': '-115.94', 'loss/train': '0.7216', 'examples_per_second': '32.6', 'grad_norm': '39.25', 'counters/examples': 60544, 'counters/updates': 1892}
train stats after 60576 examples: {'rewards_train/chosen': '0.098264', 'rewards_train/rejected': '0.063664', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0346', 'logps_train/rejected': '-120.61', 'logps_train/chosen': '-138.53', 'loss/train': '0.68126', 'examples_per_second': '31.043', 'grad_norm': '30.75', 'counters/examples': 60576, 'counters/updates': 1893}
train stats after 60608 examples: {'rewards_train/chosen': '0.10665', 'rewards_train/rejected': '0.063763', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042888', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-141.81', 'loss/train': '0.68524', 'examples_per_second': '30.715', 'grad_norm': '41', 'counters/examples': 60608, 'counters/updates': 1894}
train stats after 60640 examples: {'rewards_train/chosen': '0.10424', 'rewards_train/rejected': '0.031228', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.07301', 'logps_train/rejected': '-122.45', 'logps_train/chosen': '-136.38', 'loss/train': '0.67506', 'examples_per_second': '30.323', 'grad_norm': '38.25', 'counters/examples': 60640, 'counters/updates': 1895}
train stats after 60672 examples: {'rewards_train/chosen': '0.14832', 'rewards_train/rejected': '0.011046', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13727', 'logps_train/rejected': '-79.896', 'logps_train/chosen': '-152.69', 'loss/train': '0.63248', 'examples_per_second': '32.279', 'grad_norm': '24.375', 'counters/examples': 60672, 'counters/updates': 1896}
train stats after 60704 examples: {'rewards_train/chosen': '0.082387', 'rewards_train/rejected': '-0.008401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090788', 'logps_train/rejected': '-157.37', 'logps_train/chosen': '-181.89', 'loss/train': '0.65503', 'examples_per_second': '30.098', 'grad_norm': '37.25', 'counters/examples': 60704, 'counters/updates': 1897}
train stats after 60736 examples: {'rewards_train/chosen': '0.056905', 'rewards_train/rejected': '0.034281', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022624', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-124.62', 'loss/train': '0.68601', 'examples_per_second': '30.54', 'grad_norm': '28.125', 'counters/examples': 60736, 'counters/updates': 1898}
train stats after 60768 examples: {'rewards_train/chosen': '0.057843', 'rewards_train/rejected': '0.067295', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.009452', 'logps_train/rejected': '-117.84', 'logps_train/chosen': '-136.72', 'loss/train': '0.70568', 'examples_per_second': '30.095', 'grad_norm': '32', 'counters/examples': 60768, 'counters/updates': 1899}
train stats after 60800 examples: {'rewards_train/chosen': '0.086313', 'rewards_train/rejected': '0.0060863', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080226', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-119.41', 'loss/train': '0.65698', 'examples_per_second': '32.024', 'grad_norm': '35', 'counters/examples': 60800, 'counters/updates': 1900}
train stats after 60832 examples: {'rewards_train/chosen': '0.038827', 'rewards_train/rejected': '0.041041', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.0022132', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-109.7', 'loss/train': '0.69983', 'examples_per_second': '31.525', 'grad_norm': '25.875', 'counters/examples': 60832, 'counters/updates': 1901}
train stats after 60864 examples: {'rewards_train/chosen': '0.1409', 'rewards_train/rejected': '0.041113', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.099783', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-151.59', 'loss/train': '0.65196', 'examples_per_second': '30.668', 'grad_norm': '46.5', 'counters/examples': 60864, 'counters/updates': 1902}
train stats after 60896 examples: {'rewards_train/chosen': '0.072258', 'rewards_train/rejected': '0.015295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056962', 'logps_train/rejected': '-111.76', 'logps_train/chosen': '-142.66', 'loss/train': '0.66832', 'examples_per_second': '31.559', 'grad_norm': '25.875', 'counters/examples': 60896, 'counters/updates': 1903}
train stats after 60928 examples: {'rewards_train/chosen': '0.09077', 'rewards_train/rejected': '0.014394', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076376', 'logps_train/rejected': '-137.15', 'logps_train/chosen': '-154.72', 'loss/train': '0.66201', 'examples_per_second': '30.165', 'grad_norm': '36.5', 'counters/examples': 60928, 'counters/updates': 1904}
train stats after 60960 examples: {'rewards_train/chosen': '0.11747', 'rewards_train/rejected': '0.048833', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068639', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-142.02', 'loss/train': '0.6704', 'examples_per_second': '30.544', 'grad_norm': '47.75', 'counters/examples': 60960, 'counters/updates': 1905}
skipping logging after 60992 examples to avoid logging too frequently
train stats after 61024 examples: {'rewards_train/chosen': '0.070882', 'rewards_train/rejected': '0.028462', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042421', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-162.72', 'loss/train': '0.6769', 'examples_per_second': '37.257', 'grad_norm': '26.5', 'counters/examples': 61024, 'counters/updates': 1907}
skipping logging after 61056 examples to avoid logging too frequently
train stats after 61088 examples: {'rewards_train/chosen': '0.05695', 'rewards_train/rejected': '0.061071', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0041208', 'logps_train/rejected': '-153.29', 'logps_train/chosen': '-163.43', 'loss/train': '0.70301', 'examples_per_second': '30.412', 'grad_norm': '32.5', 'counters/examples': 61088, 'counters/updates': 1909}
train stats after 61120 examples: {'rewards_train/chosen': '0.11037', 'rewards_train/rejected': '0.043841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066529', 'logps_train/rejected': '-131.9', 'logps_train/chosen': '-141.78', 'loss/train': '0.66699', 'examples_per_second': '32.188', 'grad_norm': '42.25', 'counters/examples': 61120, 'counters/updates': 1910}
train stats after 61152 examples: {'rewards_train/chosen': '0.099194', 'rewards_train/rejected': '0.021244', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07795', 'logps_train/rejected': '-138.57', 'logps_train/chosen': '-169.11', 'loss/train': '0.67848', 'examples_per_second': '30.08', 'grad_norm': '30.625', 'counters/examples': 61152, 'counters/updates': 1911}
train stats after 61184 examples: {'rewards_train/chosen': '0.10418', 'rewards_train/rejected': '0.027133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077045', 'logps_train/rejected': '-151.87', 'logps_train/chosen': '-141.45', 'loss/train': '0.66175', 'examples_per_second': '29.95', 'grad_norm': '28.5', 'counters/examples': 61184, 'counters/updates': 1912}
train stats after 61216 examples: {'rewards_train/chosen': '0.088513', 'rewards_train/rejected': '0.051065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037448', 'logps_train/rejected': '-129.19', 'logps_train/chosen': '-156.83', 'loss/train': '0.68561', 'examples_per_second': '30.64', 'grad_norm': '35.25', 'counters/examples': 61216, 'counters/updates': 1913}
train stats after 61248 examples: {'rewards_train/chosen': '0.075607', 'rewards_train/rejected': '-0.033358', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10897', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-137.36', 'loss/train': '0.64627', 'examples_per_second': '32.437', 'grad_norm': '27.875', 'counters/examples': 61248, 'counters/updates': 1914}
skipping logging after 61280 examples to avoid logging too frequently
train stats after 61312 examples: {'rewards_train/chosen': '0.051376', 'rewards_train/rejected': '0.041965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0094115', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-157.33', 'loss/train': '0.69858', 'examples_per_second': '30.448', 'grad_norm': '47.5', 'counters/examples': 61312, 'counters/updates': 1916}
train stats after 61344 examples: {'rewards_train/chosen': '0.024619', 'rewards_train/rejected': '0.024887', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.00026844', 'logps_train/rejected': '-110.12', 'logps_train/chosen': '-138.1', 'loss/train': '0.69826', 'examples_per_second': '30.798', 'grad_norm': '31.75', 'counters/examples': 61344, 'counters/updates': 1917}
train stats after 61376 examples: {'rewards_train/chosen': '0.077041', 'rewards_train/rejected': '0.044509', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032532', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-144.52', 'loss/train': '0.69016', 'examples_per_second': '30', 'grad_norm': '33.5', 'counters/examples': 61376, 'counters/updates': 1918}
skipping logging after 61408 examples to avoid logging too frequently
train stats after 61440 examples: {'rewards_train/chosen': '0.034685', 'rewards_train/rejected': '0.051707', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017022', 'logps_train/rejected': '-126.76', 'logps_train/chosen': '-137.27', 'loss/train': '0.71411', 'examples_per_second': '31.526', 'grad_norm': '41.25', 'counters/examples': 61440, 'counters/updates': 1920}
train stats after 61472 examples: {'rewards_train/chosen': '0.11614', 'rewards_train/rejected': '0.011742', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1044', 'logps_train/rejected': '-126.76', 'logps_train/chosen': '-184.11', 'loss/train': '0.64874', 'examples_per_second': '30.029', 'grad_norm': '29.125', 'counters/examples': 61472, 'counters/updates': 1921}
train stats after 61504 examples: {'rewards_train/chosen': '0.070125', 'rewards_train/rejected': '0.15333', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.083206', 'logps_train/rejected': '-138.62', 'logps_train/chosen': '-112', 'loss/train': '0.75695', 'examples_per_second': '30.505', 'grad_norm': '80', 'counters/examples': 61504, 'counters/updates': 1922}
train stats after 61536 examples: {'rewards_train/chosen': '0.028689', 'rewards_train/rejected': '0.10551', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.07682', 'logps_train/rejected': '-156.25', 'logps_train/chosen': '-188.34', 'loss/train': '0.74519', 'examples_per_second': '30.016', 'grad_norm': '69.5', 'counters/examples': 61536, 'counters/updates': 1923}
train stats after 61568 examples: {'rewards_train/chosen': '0.05204', 'rewards_train/rejected': '-0.027428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079468', 'logps_train/rejected': '-146.59', 'logps_train/chosen': '-162.29', 'loss/train': '0.66183', 'examples_per_second': '31.51', 'grad_norm': '35', 'counters/examples': 61568, 'counters/updates': 1924}
train stats after 61600 examples: {'rewards_train/chosen': '0.091221', 'rewards_train/rejected': '0.059186', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032035', 'logps_train/rejected': '-146.89', 'logps_train/chosen': '-140.92', 'loss/train': '0.68362', 'examples_per_second': '31.525', 'grad_norm': '30.75', 'counters/examples': 61600, 'counters/updates': 1925}
train stats after 61632 examples: {'rewards_train/chosen': '0.11309', 'rewards_train/rejected': '0.04733', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065764', 'logps_train/rejected': '-121.58', 'logps_train/chosen': '-134.32', 'loss/train': '0.66864', 'examples_per_second': '32.075', 'grad_norm': '37', 'counters/examples': 61632, 'counters/updates': 1926}
train stats after 61664 examples: {'rewards_train/chosen': '0.060757', 'rewards_train/rejected': '-0.013024', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073781', 'logps_train/rejected': '-96.098', 'logps_train/chosen': '-111.75', 'loss/train': '0.66508', 'examples_per_second': '31.345', 'grad_norm': '30.125', 'counters/examples': 61664, 'counters/updates': 1927}
train stats after 61696 examples: {'rewards_train/chosen': '0.04639', 'rewards_train/rejected': '0.10398', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.057587', 'logps_train/rejected': '-136.7', 'logps_train/chosen': '-106.99', 'loss/train': '0.73043', 'examples_per_second': '32.079', 'grad_norm': '44.5', 'counters/examples': 61696, 'counters/updates': 1928}
train stats after 61728 examples: {'rewards_train/chosen': '0.070639', 'rewards_train/rejected': '0.04999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020649', 'logps_train/rejected': '-155.27', 'logps_train/chosen': '-179.11', 'loss/train': '0.69016', 'examples_per_second': '31.567', 'grad_norm': '41.25', 'counters/examples': 61728, 'counters/updates': 1929}
train stats after 61760 examples: {'rewards_train/chosen': '0.019016', 'rewards_train/rejected': '0.049372', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030356', 'logps_train/rejected': '-131.41', 'logps_train/chosen': '-122.41', 'loss/train': '0.7144', 'examples_per_second': '32.964', 'grad_norm': '28.25', 'counters/examples': 61760, 'counters/updates': 1930}
train stats after 61792 examples: {'rewards_train/chosen': '0.11272', 'rewards_train/rejected': '0.02106', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091661', 'logps_train/rejected': '-93.52', 'logps_train/chosen': '-146.95', 'loss/train': '0.65314', 'examples_per_second': '32.096', 'grad_norm': '28.875', 'counters/examples': 61792, 'counters/updates': 1931}
train stats after 61824 examples: {'rewards_train/chosen': '0.061029', 'rewards_train/rejected': '0.013998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047032', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-160.78', 'loss/train': '0.67599', 'examples_per_second': '31.512', 'grad_norm': '34.25', 'counters/examples': 61824, 'counters/updates': 1932}
train stats after 61856 examples: {'rewards_train/chosen': '0.12516', 'rewards_train/rejected': '0.088266', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036893', 'logps_train/rejected': '-137.57', 'logps_train/chosen': '-155.32', 'loss/train': '0.687', 'examples_per_second': '31.27', 'grad_norm': '35', 'counters/examples': 61856, 'counters/updates': 1933}
train stats after 61888 examples: {'rewards_train/chosen': '0.098809', 'rewards_train/rejected': '0.088383', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.010426', 'logps_train/rejected': '-98.053', 'logps_train/chosen': '-115.56', 'loss/train': '0.69385', 'examples_per_second': '31.571', 'grad_norm': '25.375', 'counters/examples': 61888, 'counters/updates': 1934}
train stats after 61920 examples: {'rewards_train/chosen': '0.044063', 'rewards_train/rejected': '0.081206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037143', 'logps_train/rejected': '-102.84', 'logps_train/chosen': '-142.78', 'loss/train': '0.71974', 'examples_per_second': '31.603', 'grad_norm': '48', 'counters/examples': 61920, 'counters/updates': 1935}
train stats after 61952 examples: {'rewards_train/chosen': '0.057907', 'rewards_train/rejected': '-0.0012014', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059108', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-170.16', 'loss/train': '0.66745', 'examples_per_second': '30.556', 'grad_norm': '32.5', 'counters/examples': 61952, 'counters/updates': 1936}
train stats after 61984 examples: {'rewards_train/chosen': '0.055116', 'rewards_train/rejected': '-0.0221', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077215', 'logps_train/rejected': '-137.7', 'logps_train/chosen': '-180.42', 'loss/train': '0.66129', 'examples_per_second': '32.037', 'grad_norm': '38.25', 'counters/examples': 61984, 'counters/updates': 1937}
train stats after 62016 examples: {'rewards_train/chosen': '0.090031', 'rewards_train/rejected': '0.056774', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033257', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-123.56', 'loss/train': '0.68359', 'examples_per_second': '31.498', 'grad_norm': '27.375', 'counters/examples': 62016, 'counters/updates': 1938}
train stats after 62048 examples: {'rewards_train/chosen': '0.12495', 'rewards_train/rejected': '-0.0010162', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12596', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-193.04', 'loss/train': '0.64043', 'examples_per_second': '30.819', 'grad_norm': '46.5', 'counters/examples': 62048, 'counters/updates': 1939}
train stats after 62080 examples: {'rewards_train/chosen': '0.060429', 'rewards_train/rejected': '0.096048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.035619', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-151.62', 'loss/train': '0.72131', 'examples_per_second': '30.039', 'grad_norm': '46.75', 'counters/examples': 62080, 'counters/updates': 1940}
skipping logging after 62112 examples to avoid logging too frequently
train stats after 62144 examples: {'rewards_train/chosen': '0.082388', 'rewards_train/rejected': '0.036503', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045884', 'logps_train/rejected': '-97.39', 'logps_train/chosen': '-150.21', 'loss/train': '0.6758', 'examples_per_second': '31.568', 'grad_norm': '28', 'counters/examples': 62144, 'counters/updates': 1942}
train stats after 62176 examples: {'rewards_train/chosen': '0.064953', 'rewards_train/rejected': '0.10736', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042408', 'logps_train/rejected': '-152.75', 'logps_train/chosen': '-134.58', 'loss/train': '0.71809', 'examples_per_second': '30.09', 'grad_norm': '35.75', 'counters/examples': 62176, 'counters/updates': 1943}
train stats after 62208 examples: {'rewards_train/chosen': '0.16856', 'rewards_train/rejected': '0.036614', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13194', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-158.05', 'loss/train': '0.64652', 'examples_per_second': '31.193', 'grad_norm': '35.75', 'counters/examples': 62208, 'counters/updates': 1944}
train stats after 62240 examples: {'rewards_train/chosen': '0.056493', 'rewards_train/rejected': '0.11963', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.063142', 'logps_train/rejected': '-152.6', 'logps_train/chosen': '-153.26', 'loss/train': '0.73277', 'examples_per_second': '31.545', 'grad_norm': '47', 'counters/examples': 62240, 'counters/updates': 1945}
train stats after 62272 examples: {'rewards_train/chosen': '0.12274', 'rewards_train/rejected': '0.077795', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044943', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-135.62', 'loss/train': '0.67761', 'examples_per_second': '31.774', 'grad_norm': '29.5', 'counters/examples': 62272, 'counters/updates': 1946}
train stats after 62304 examples: {'rewards_train/chosen': '0.083168', 'rewards_train/rejected': '0.02037', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062798', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-157.36', 'loss/train': '0.66805', 'examples_per_second': '31.513', 'grad_norm': '36.5', 'counters/examples': 62304, 'counters/updates': 1947}
train stats after 62336 examples: {'rewards_train/chosen': '0.089887', 'rewards_train/rejected': '0.078616', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011271', 'logps_train/rejected': '-149.89', 'logps_train/chosen': '-120.41', 'loss/train': '0.69143', 'examples_per_second': '31.053', 'grad_norm': '40', 'counters/examples': 62336, 'counters/updates': 1948}
skipping logging after 62368 examples to avoid logging too frequently
train stats after 62400 examples: {'rewards_train/chosen': '0.13011', 'rewards_train/rejected': '0.054264', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075851', 'logps_train/rejected': '-121.62', 'logps_train/chosen': '-153.39', 'loss/train': '0.66362', 'examples_per_second': '32.071', 'grad_norm': '30.875', 'counters/examples': 62400, 'counters/updates': 1950}
train stats after 62432 examples: {'rewards_train/chosen': '0.085293', 'rewards_train/rejected': '0.045839', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039454', 'logps_train/rejected': '-153.04', 'logps_train/chosen': '-159.64', 'loss/train': '0.67676', 'examples_per_second': '31.507', 'grad_norm': '41', 'counters/examples': 62432, 'counters/updates': 1951}
train stats after 62464 examples: {'rewards_train/chosen': '0.074446', 'rewards_train/rejected': '0.02915', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045296', 'logps_train/rejected': '-133.24', 'logps_train/chosen': '-169.52', 'loss/train': '0.67692', 'examples_per_second': '30.211', 'grad_norm': '38', 'counters/examples': 62464, 'counters/updates': 1952}
train stats after 62496 examples: {'rewards_train/chosen': '0.10793', 'rewards_train/rejected': '0.0026826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10525', 'logps_train/rejected': '-116.08', 'logps_train/chosen': '-137.12', 'loss/train': '0.65191', 'examples_per_second': '32.633', 'grad_norm': '31', 'counters/examples': 62496, 'counters/updates': 1953}
train stats after 62528 examples: {'rewards_train/chosen': '0.204', 'rewards_train/rejected': '0.084146', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11985', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-156.52', 'loss/train': '0.64823', 'examples_per_second': '31.971', 'grad_norm': '41', 'counters/examples': 62528, 'counters/updates': 1954}
skipping logging after 62560 examples to avoid logging too frequently
train stats after 62592 examples: {'rewards_train/chosen': '0.059672', 'rewards_train/rejected': '0.02014', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039532', 'logps_train/rejected': '-139.09', 'logps_train/chosen': '-163.56', 'loss/train': '0.67961', 'examples_per_second': '31.502', 'grad_norm': '29.5', 'counters/examples': 62592, 'counters/updates': 1956}
train stats after 62624 examples: {'rewards_train/chosen': '0.078396', 'rewards_train/rejected': '0.048939', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.029458', 'logps_train/rejected': '-153.5', 'logps_train/chosen': '-160.18', 'loss/train': '0.68385', 'examples_per_second': '32.711', 'grad_norm': '35', 'counters/examples': 62624, 'counters/updates': 1957}
train stats after 62656 examples: {'rewards_train/chosen': '0.05183', 'rewards_train/rejected': '-0.019318', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.071148', 'logps_train/rejected': '-142.03', 'logps_train/chosen': '-157.86', 'loss/train': '0.66463', 'examples_per_second': '33.086', 'grad_norm': '29.625', 'counters/examples': 62656, 'counters/updates': 1958}
train stats after 62688 examples: {'rewards_train/chosen': '0.065072', 'rewards_train/rejected': '0.042244', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022829', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-121.75', 'loss/train': '0.6882', 'examples_per_second': '30.603', 'grad_norm': '28.75', 'counters/examples': 62688, 'counters/updates': 1959}
train stats after 62720 examples: {'rewards_train/chosen': '0.12331', 'rewards_train/rejected': '0.0039574', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11936', 'logps_train/rejected': '-111.38', 'logps_train/chosen': '-144.05', 'loss/train': '0.6414', 'examples_per_second': '32.683', 'grad_norm': '31.25', 'counters/examples': 62720, 'counters/updates': 1960}
train stats after 62752 examples: {'rewards_train/chosen': '0.054237', 'rewards_train/rejected': '0.055305', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0010676', 'logps_train/rejected': '-109.93', 'logps_train/chosen': '-130.96', 'loss/train': '0.69776', 'examples_per_second': '31.717', 'grad_norm': '28', 'counters/examples': 62752, 'counters/updates': 1961}
train stats after 62784 examples: {'rewards_train/chosen': '0.14958', 'rewards_train/rejected': '0.018818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13076', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-137.23', 'loss/train': '0.65014', 'examples_per_second': '32.035', 'grad_norm': '31.375', 'counters/examples': 62784, 'counters/updates': 1962}
train stats after 62816 examples: {'rewards_train/chosen': '0.099241', 'rewards_train/rejected': '0.10993', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010693', 'logps_train/rejected': '-170.31', 'logps_train/chosen': '-134.53', 'loss/train': '0.71595', 'examples_per_second': '31.078', 'grad_norm': '37.25', 'counters/examples': 62816, 'counters/updates': 1963}
train stats after 62848 examples: {'rewards_train/chosen': '0.069944', 'rewards_train/rejected': '0.0045055', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065439', 'logps_train/rejected': '-125.84', 'logps_train/chosen': '-98.188', 'loss/train': '0.66507', 'examples_per_second': '23.245', 'grad_norm': '29.5', 'counters/examples': 62848, 'counters/updates': 1964}
skipping logging after 62880 examples to avoid logging too frequently
train stats after 62912 examples: {'rewards_train/chosen': '0.04539', 'rewards_train/rejected': '0.022714', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.022676', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-133.95', 'loss/train': '0.68672', 'examples_per_second': '32.039', 'grad_norm': '39.5', 'counters/examples': 62912, 'counters/updates': 1966}
train stats after 62944 examples: {'rewards_train/chosen': '0.10795', 'rewards_train/rejected': '0.0050783', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10287', 'logps_train/rejected': '-144.63', 'logps_train/chosen': '-147.16', 'loss/train': '0.65589', 'examples_per_second': '24.367', 'grad_norm': '35.75', 'counters/examples': 62944, 'counters/updates': 1967}
train stats after 62976 examples: {'rewards_train/chosen': '0.037981', 'rewards_train/rejected': '-0.00020009', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038181', 'logps_train/rejected': '-124', 'logps_train/chosen': '-165.27', 'loss/train': '0.67817', 'examples_per_second': '31.403', 'grad_norm': '27.375', 'counters/examples': 62976, 'counters/updates': 1968}
skipping logging after 63008 examples to avoid logging too frequently
train stats after 63040 examples: {'rewards_train/chosen': '0.06451', 'rewards_train/rejected': '0.052955', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011555', 'logps_train/rejected': '-154.47', 'logps_train/chosen': '-152.99', 'loss/train': '0.69901', 'examples_per_second': '32.462', 'grad_norm': '36.25', 'counters/examples': 63040, 'counters/updates': 1970}
train stats after 63072 examples: {'rewards_train/chosen': '0.068704', 'rewards_train/rejected': '0.018831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049873', 'logps_train/rejected': '-120.17', 'logps_train/chosen': '-141.21', 'loss/train': '0.67558', 'examples_per_second': '30.972', 'grad_norm': '43.5', 'counters/examples': 63072, 'counters/updates': 1971}
train stats after 63104 examples: {'rewards_train/chosen': '0.093489', 'rewards_train/rejected': '-0.003749', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097238', 'logps_train/rejected': '-146.91', 'logps_train/chosen': '-141.46', 'loss/train': '0.6553', 'examples_per_second': '30.751', 'grad_norm': '36', 'counters/examples': 63104, 'counters/updates': 1972}
train stats after 63136 examples: {'rewards_train/chosen': '0.084931', 'rewards_train/rejected': '0.057614', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027317', 'logps_train/rejected': '-110.53', 'logps_train/chosen': '-137.33', 'loss/train': '0.68669', 'examples_per_second': '29.888', 'grad_norm': '26.375', 'counters/examples': 63136, 'counters/updates': 1973}
train stats after 63168 examples: {'rewards_train/chosen': '0.12544', 'rewards_train/rejected': '0.10633', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019115', 'logps_train/rejected': '-117.78', 'logps_train/chosen': '-117.52', 'loss/train': '0.69354', 'examples_per_second': '31.086', 'grad_norm': '28.125', 'counters/examples': 63168, 'counters/updates': 1974}
train stats after 63200 examples: {'rewards_train/chosen': '0.077254', 'rewards_train/rejected': '0.065578', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011677', 'logps_train/rejected': '-131.06', 'logps_train/chosen': '-139.79', 'loss/train': '0.69388', 'examples_per_second': '30.872', 'grad_norm': '35.75', 'counters/examples': 63200, 'counters/updates': 1975}
skipping logging after 63232 examples to avoid logging too frequently
train stats after 63264 examples: {'rewards_train/chosen': '0.086376', 'rewards_train/rejected': '0.095555', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0091792', 'logps_train/rejected': '-144.6', 'logps_train/chosen': '-147.36', 'loss/train': '0.71066', 'examples_per_second': '31.477', 'grad_norm': '35.25', 'counters/examples': 63264, 'counters/updates': 1977}
train stats after 63296 examples: {'rewards_train/chosen': '0.14834', 'rewards_train/rejected': '0.034888', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11345', 'logps_train/rejected': '-129.03', 'logps_train/chosen': '-159.24', 'loss/train': '0.65084', 'examples_per_second': '32.541', 'grad_norm': '37.75', 'counters/examples': 63296, 'counters/updates': 1978}
train stats after 63328 examples: {'rewards_train/chosen': '0.05043', 'rewards_train/rejected': '0.0091442', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041286', 'logps_train/rejected': '-137', 'logps_train/chosen': '-131.09', 'loss/train': '0.6828', 'examples_per_second': '31.373', 'grad_norm': '32.75', 'counters/examples': 63328, 'counters/updates': 1979}
skipping logging after 63360 examples to avoid logging too frequently
train stats after 63392 examples: {'rewards_train/chosen': '0.098864', 'rewards_train/rejected': '0.027371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071493', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-166.83', 'loss/train': '0.66259', 'examples_per_second': '33.831', 'grad_norm': '29', 'counters/examples': 63392, 'counters/updates': 1981}
train stats after 63424 examples: {'rewards_train/chosen': '0.076515', 'rewards_train/rejected': '0.10667', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.030156', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-134.28', 'loss/train': '0.71536', 'examples_per_second': '32.124', 'grad_norm': '25.75', 'counters/examples': 63424, 'counters/updates': 1982}
train stats after 63456 examples: {'rewards_train/chosen': '0.18196', 'rewards_train/rejected': '0.019644', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16232', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-125.49', 'loss/train': '0.62761', 'examples_per_second': '31.372', 'grad_norm': '26.75', 'counters/examples': 63456, 'counters/updates': 1983}
train stats after 63488 examples: {'rewards_train/chosen': '0.051565', 'rewards_train/rejected': '0.10409', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.052522', 'logps_train/rejected': '-118.11', 'logps_train/chosen': '-141.5', 'loss/train': '0.72905', 'examples_per_second': '30.072', 'grad_norm': '32', 'counters/examples': 63488, 'counters/updates': 1984}
train stats after 63520 examples: {'rewards_train/chosen': '0.15189', 'rewards_train/rejected': '0.054595', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097292', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-135.34', 'loss/train': '0.66115', 'examples_per_second': '32.749', 'grad_norm': '32', 'counters/examples': 63520, 'counters/updates': 1985}
train stats after 63552 examples: {'rewards_train/chosen': '0.083475', 'rewards_train/rejected': '0.052443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031032', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-132.45', 'loss/train': '0.68405', 'examples_per_second': '31.133', 'grad_norm': '28.875', 'counters/examples': 63552, 'counters/updates': 1986}
train stats after 63584 examples: {'rewards_train/chosen': '0.11681', 'rewards_train/rejected': '0.0053922', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11142', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-131.47', 'loss/train': '0.64295', 'examples_per_second': '31.981', 'grad_norm': '30.375', 'counters/examples': 63584, 'counters/updates': 1987}
train stats after 63616 examples: {'rewards_train/chosen': '0.15098', 'rewards_train/rejected': '0.068101', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.08288', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-188.78', 'loss/train': '0.66016', 'examples_per_second': '31.482', 'grad_norm': '34', 'counters/examples': 63616, 'counters/updates': 1988}
skipping logging after 63648 examples to avoid logging too frequently
train stats after 63680 examples: {'rewards_train/chosen': '0.12158', 'rewards_train/rejected': '0.088894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032682', 'logps_train/rejected': '-137.99', 'logps_train/chosen': '-136.56', 'loss/train': '0.68819', 'examples_per_second': '30.576', 'grad_norm': '33.5', 'counters/examples': 63680, 'counters/updates': 1990}
train stats after 63712 examples: {'rewards_train/chosen': '0.11603', 'rewards_train/rejected': '-0.021067', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1371', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-154.33', 'loss/train': '0.63262', 'examples_per_second': '31.988', 'grad_norm': '26.375', 'counters/examples': 63712, 'counters/updates': 1991}
train stats after 63744 examples: {'rewards_train/chosen': '0.18053', 'rewards_train/rejected': '0.053489', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12704', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-210.29', 'loss/train': '0.65324', 'examples_per_second': '32.855', 'grad_norm': '39.5', 'counters/examples': 63744, 'counters/updates': 1992}
train stats after 63776 examples: {'rewards_train/chosen': '0.050066', 'rewards_train/rejected': '0.047559', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.002507', 'logps_train/rejected': '-160.81', 'logps_train/chosen': '-159.05', 'loss/train': '0.7015', 'examples_per_second': '31.36', 'grad_norm': '52.5', 'counters/examples': 63776, 'counters/updates': 1993}
skipping logging after 63808 examples to avoid logging too frequently
train stats after 63840 examples: {'rewards_train/chosen': '0.064377', 'rewards_train/rejected': '0.016078', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048299', 'logps_train/rejected': '-116.35', 'logps_train/chosen': '-145.92', 'loss/train': '0.67623', 'examples_per_second': '31.192', 'grad_norm': '35', 'counters/examples': 63840, 'counters/updates': 1995}
train stats after 63872 examples: {'rewards_train/chosen': '0.072504', 'rewards_train/rejected': '0.22478', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.15227', 'logps_train/rejected': '-147.33', 'logps_train/chosen': '-141.49', 'loss/train': '0.8273', 'examples_per_second': '32.604', 'grad_norm': '43.75', 'counters/examples': 63872, 'counters/updates': 1996}
train stats after 63904 examples: {'rewards_train/chosen': '0.027289', 'rewards_train/rejected': '-0.025805', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053094', 'logps_train/rejected': '-122.78', 'logps_train/chosen': '-117.6', 'loss/train': '0.67368', 'examples_per_second': '31.074', 'grad_norm': '29.75', 'counters/examples': 63904, 'counters/updates': 1997}
train stats after 63936 examples: {'rewards_train/chosen': '0.14478', 'rewards_train/rejected': '0.059521', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085259', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-154.51', 'loss/train': '0.65754', 'examples_per_second': '30.889', 'grad_norm': '37', 'counters/examples': 63936, 'counters/updates': 1998}
train stats after 63968 examples: {'rewards_train/chosen': '0.12314', 'rewards_train/rejected': '0.0038873', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11925', 'logps_train/rejected': '-116', 'logps_train/chosen': '-157.48', 'loss/train': '0.6421', 'examples_per_second': '31.29', 'grad_norm': '31.5', 'counters/examples': 63968, 'counters/updates': 1999}
train stats after 64000 examples: {'rewards_train/chosen': '0.16804', 'rewards_train/rejected': '0.031699', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13634', 'logps_train/rejected': '-124.28', 'logps_train/chosen': '-166.72', 'loss/train': '0.63378', 'examples_per_second': '31.913', 'grad_norm': '35', 'counters/examples': 64000, 'counters/updates': 2000}
train stats after 64032 examples: {'rewards_train/chosen': '0.055746', 'rewards_train/rejected': '-0.0084121', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064158', 'logps_train/rejected': '-102.14', 'logps_train/chosen': '-147.92', 'loss/train': '0.66825', 'examples_per_second': '31.857', 'grad_norm': '35.25', 'counters/examples': 64032, 'counters/updates': 2001}
train stats after 64064 examples: {'rewards_train/chosen': '0.1167', 'rewards_train/rejected': '0.02546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09124', 'logps_train/rejected': '-149.95', 'logps_train/chosen': '-162.11', 'loss/train': '0.65514', 'examples_per_second': '31.522', 'grad_norm': '33', 'counters/examples': 64064, 'counters/updates': 2002}
train stats after 64096 examples: {'rewards_train/chosen': '0.1124', 'rewards_train/rejected': '0.045719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066682', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-155.56', 'loss/train': '0.67111', 'examples_per_second': '32.556', 'grad_norm': '32.25', 'counters/examples': 64096, 'counters/updates': 2003}
train stats after 64128 examples: {'rewards_train/chosen': '0.052012', 'rewards_train/rejected': '-0.0041341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056146', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-126.32', 'loss/train': '0.67083', 'examples_per_second': '31.505', 'grad_norm': '28', 'counters/examples': 64128, 'counters/updates': 2004}
train stats after 64160 examples: {'rewards_train/chosen': '0.062315', 'rewards_train/rejected': '0.044375', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01794', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-134.22', 'loss/train': '0.6881', 'examples_per_second': '32.205', 'grad_norm': '29.125', 'counters/examples': 64160, 'counters/updates': 2005}
train stats after 64192 examples: {'rewards_train/chosen': '0.092848', 'rewards_train/rejected': '0.099085', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0062363', 'logps_train/rejected': '-134.09', 'logps_train/chosen': '-149.65', 'loss/train': '0.70373', 'examples_per_second': '25.045', 'grad_norm': '56.75', 'counters/examples': 64192, 'counters/updates': 2006}
train stats after 64224 examples: {'rewards_train/chosen': '0.10951', 'rewards_train/rejected': '0.089717', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019792', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-138.25', 'loss/train': '0.68949', 'examples_per_second': '31.434', 'grad_norm': '35.75', 'counters/examples': 64224, 'counters/updates': 2007}
skipping logging after 64256 examples to avoid logging too frequently
train stats after 64288 examples: {'rewards_train/chosen': '0.086939', 'rewards_train/rejected': '0.020556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066383', 'logps_train/rejected': '-106.18', 'logps_train/chosen': '-156.19', 'loss/train': '0.66653', 'examples_per_second': '31.478', 'grad_norm': '35', 'counters/examples': 64288, 'counters/updates': 2009}
train stats after 64320 examples: {'rewards_train/chosen': '0.098843', 'rewards_train/rejected': '0.019628', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079214', 'logps_train/rejected': '-124.33', 'logps_train/chosen': '-125.05', 'loss/train': '0.66782', 'examples_per_second': '32.78', 'grad_norm': '37', 'counters/examples': 64320, 'counters/updates': 2010}
train stats after 64352 examples: {'rewards_train/chosen': '0.065271', 'rewards_train/rejected': '0.034029', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031242', 'logps_train/rejected': '-146.65', 'logps_train/chosen': '-149.93', 'loss/train': '0.6855', 'examples_per_second': '30.422', 'grad_norm': '31', 'counters/examples': 64352, 'counters/updates': 2011}
train stats after 64384 examples: {'rewards_train/chosen': '0.1097', 'rewards_train/rejected': '0.063085', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046614', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-147.44', 'loss/train': '0.67607', 'examples_per_second': '31.267', 'grad_norm': '32.75', 'counters/examples': 64384, 'counters/updates': 2012}
skipping logging after 64416 examples to avoid logging too frequently
train stats after 64448 examples: {'rewards_train/chosen': '0.097792', 'rewards_train/rejected': '0.090669', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0071228', 'logps_train/rejected': '-181.46', 'logps_train/chosen': '-198.79', 'loss/train': '0.70467', 'examples_per_second': '32.094', 'grad_norm': '37', 'counters/examples': 64448, 'counters/updates': 2014}
train stats after 64480 examples: {'rewards_train/chosen': '0.076958', 'rewards_train/rejected': '0.041054', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035905', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-115.07', 'loss/train': '0.68276', 'examples_per_second': '31.399', 'grad_norm': '66.5', 'counters/examples': 64480, 'counters/updates': 2015}
skipping logging after 64512 examples to avoid logging too frequently
train stats after 64544 examples: {'rewards_train/chosen': '0.12503', 'rewards_train/rejected': '0.068367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056664', 'logps_train/rejected': '-126.66', 'logps_train/chosen': '-137.22', 'loss/train': '0.6681', 'examples_per_second': '32.49', 'grad_norm': '35', 'counters/examples': 64544, 'counters/updates': 2017}
train stats after 64576 examples: {'rewards_train/chosen': '0.13413', 'rewards_train/rejected': '-0.0098965', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14403', 'logps_train/rejected': '-141.92', 'logps_train/chosen': '-169.62', 'loss/train': '0.63974', 'examples_per_second': '31.569', 'grad_norm': '48', 'counters/examples': 64576, 'counters/updates': 2018}
skipping logging after 64608 examples to avoid logging too frequently
train stats after 64640 examples: {'rewards_train/chosen': '0.092508', 'rewards_train/rejected': '0.036897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05561', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-130.8', 'loss/train': '0.67336', 'examples_per_second': '31.017', 'grad_norm': '42.25', 'counters/examples': 64640, 'counters/updates': 2020}
train stats after 64672 examples: {'rewards_train/chosen': '0.046242', 'rewards_train/rejected': '0.026661', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019581', 'logps_train/rejected': '-111.02', 'logps_train/chosen': '-128.59', 'loss/train': '0.68833', 'examples_per_second': '33.252', 'grad_norm': '33.5', 'counters/examples': 64672, 'counters/updates': 2021}
skipping logging after 64704 examples to avoid logging too frequently
train stats after 64736 examples: {'rewards_train/chosen': '0.054464', 'rewards_train/rejected': '0.01852', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035944', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-136.28', 'loss/train': '0.67808', 'examples_per_second': '30.425', 'grad_norm': '27.375', 'counters/examples': 64736, 'counters/updates': 2023}
skipping logging after 64768 examples to avoid logging too frequently
train stats after 64800 examples: {'rewards_train/chosen': '0.037817', 'rewards_train/rejected': '0.024659', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.013158', 'logps_train/rejected': '-142.1', 'logps_train/chosen': '-120.39', 'loss/train': '0.69341', 'examples_per_second': '32.313', 'grad_norm': '30.5', 'counters/examples': 64800, 'counters/updates': 2025}
train stats after 64832 examples: {'rewards_train/chosen': '0.099824', 'rewards_train/rejected': '0.058781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041044', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-152.09', 'loss/train': '0.67824', 'examples_per_second': '30.508', 'grad_norm': '31.25', 'counters/examples': 64832, 'counters/updates': 2026}
train stats after 64864 examples: {'rewards_train/chosen': '0.10644', 'rewards_train/rejected': '0.050175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056263', 'logps_train/rejected': '-142.66', 'logps_train/chosen': '-162.69', 'loss/train': '0.67233', 'examples_per_second': '31.84', 'grad_norm': '37.25', 'counters/examples': 64864, 'counters/updates': 2027}
train stats after 64896 examples: {'rewards_train/chosen': '0.14668', 'rewards_train/rejected': '0.085085', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061597', 'logps_train/rejected': '-182.54', 'logps_train/chosen': '-186.6', 'loss/train': '0.68032', 'examples_per_second': '30.895', 'grad_norm': '46.25', 'counters/examples': 64896, 'counters/updates': 2028}
skipping logging after 64928 examples to avoid logging too frequently
train stats after 64960 examples: {'rewards_train/chosen': '0.091028', 'rewards_train/rejected': '0.11331', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.022286', 'logps_train/rejected': '-113.65', 'logps_train/chosen': '-127.37', 'loss/train': '0.71324', 'examples_per_second': '31.472', 'grad_norm': '52.75', 'counters/examples': 64960, 'counters/updates': 2030}
train stats after 64992 examples: {'rewards_train/chosen': '0.070019', 'rewards_train/rejected': '0.062732', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0072866', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-132.36', 'loss/train': '0.69481', 'examples_per_second': '33.117', 'grad_norm': '33.25', 'counters/examples': 64992, 'counters/updates': 2031}
train stats after 65024 examples: {'rewards_train/chosen': '0.080391', 'rewards_train/rejected': '-0.013988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094379', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-125.67', 'loss/train': '0.65011', 'examples_per_second': '30.337', 'grad_norm': '29.75', 'counters/examples': 65024, 'counters/updates': 2032}
train stats after 65056 examples: {'rewards_train/chosen': '0.13543', 'rewards_train/rejected': '0.038608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096827', 'logps_train/rejected': '-125.62', 'logps_train/chosen': '-155.28', 'loss/train': '0.6566', 'examples_per_second': '30.772', 'grad_norm': '27.375', 'counters/examples': 65056, 'counters/updates': 2033}
train stats after 65088 examples: {'rewards_train/chosen': '0.066139', 'rewards_train/rejected': '0.088312', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022173', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-141.97', 'loss/train': '0.71414', 'examples_per_second': '30.847', 'grad_norm': '52.25', 'counters/examples': 65088, 'counters/updates': 2034}
train stats after 65120 examples: {'rewards_train/chosen': '0.05901', 'rewards_train/rejected': '0.080282', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.021272', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-165.15', 'loss/train': '0.71886', 'examples_per_second': '31.573', 'grad_norm': '66', 'counters/examples': 65120, 'counters/updates': 2035}
train stats after 65152 examples: {'rewards_train/chosen': '0.10701', 'rewards_train/rejected': '0.020727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086287', 'logps_train/rejected': '-100.64', 'logps_train/chosen': '-134.32', 'loss/train': '0.66508', 'examples_per_second': '31.259', 'grad_norm': '25', 'counters/examples': 65152, 'counters/updates': 2036}
train stats after 65184 examples: {'rewards_train/chosen': '0.11257', 'rewards_train/rejected': '-0.0087266', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12129', 'logps_train/rejected': '-109.3', 'logps_train/chosen': '-157.62', 'loss/train': '0.64337', 'examples_per_second': '31.491', 'grad_norm': '37', 'counters/examples': 65184, 'counters/updates': 2037}
train stats after 65216 examples: {'rewards_train/chosen': '0.089581', 'rewards_train/rejected': '0.075367', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014215', 'logps_train/rejected': '-119.92', 'logps_train/chosen': '-162.99', 'loss/train': '0.6948', 'examples_per_second': '30.798', 'grad_norm': '43.5', 'counters/examples': 65216, 'counters/updates': 2038}
train stats after 65248 examples: {'rewards_train/chosen': '0.11803', 'rewards_train/rejected': '0.053761', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064264', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-146.81', 'loss/train': '0.67403', 'examples_per_second': '31.634', 'grad_norm': '36', 'counters/examples': 65248, 'counters/updates': 2039}
train stats after 65280 examples: {'rewards_train/chosen': '0.077663', 'rewards_train/rejected': '-0.050231', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12789', 'logps_train/rejected': '-110.06', 'logps_train/chosen': '-137.92', 'loss/train': '0.6457', 'examples_per_second': '31.538', 'grad_norm': '41', 'counters/examples': 65280, 'counters/updates': 2040}
skipping logging after 65312 examples to avoid logging too frequently
train stats after 65344 examples: {'rewards_train/chosen': '0.080579', 'rewards_train/rejected': '0.031907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048673', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-169.25', 'loss/train': '0.67755', 'examples_per_second': '30.086', 'grad_norm': '45.5', 'counters/examples': 65344, 'counters/updates': 2042}
train stats after 65376 examples: {'rewards_train/chosen': '0.089208', 'rewards_train/rejected': '-0.0008845', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090092', 'logps_train/rejected': '-99.392', 'logps_train/chosen': '-158.02', 'loss/train': '0.65441', 'examples_per_second': '30.051', 'grad_norm': '25.25', 'counters/examples': 65376, 'counters/updates': 2043}
train stats after 65408 examples: {'rewards_train/chosen': '0.13157', 'rewards_train/rejected': '0.008034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12354', 'logps_train/rejected': '-146.4', 'logps_train/chosen': '-187.58', 'loss/train': '0.64394', 'examples_per_second': '31.501', 'grad_norm': '52.25', 'counters/examples': 65408, 'counters/updates': 2044}
train stats after 65440 examples: {'rewards_train/chosen': '0.14821', 'rewards_train/rejected': '0.070997', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077215', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-151.71', 'loss/train': '0.67431', 'examples_per_second': '31.46', 'grad_norm': '39.75', 'counters/examples': 65440, 'counters/updates': 2045}
train stats after 65472 examples: {'rewards_train/chosen': '0.13322', 'rewards_train/rejected': '0.062349', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070871', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-175.22', 'loss/train': '0.66669', 'examples_per_second': '30.145', 'grad_norm': '33.75', 'counters/examples': 65472, 'counters/updates': 2046}
skipping logging after 65504 examples to avoid logging too frequently
train stats after 65536 examples: {'rewards_train/chosen': '0.038993', 'rewards_train/rejected': '0.030272', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0087209', 'logps_train/rejected': '-128.65', 'logps_train/chosen': '-122.38', 'loss/train': '0.69133', 'examples_per_second': '32.208', 'grad_norm': '28.375', 'counters/examples': 65536, 'counters/updates': 2048}
train stats after 65568 examples: {'rewards_train/chosen': '0.060586', 'rewards_train/rejected': '0.014736', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.04585', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-150.59', 'loss/train': '0.67457', 'examples_per_second': '30.609', 'grad_norm': '26.625', 'counters/examples': 65568, 'counters/updates': 2049}
train stats after 65600 examples: {'rewards_train/chosen': '0.093844', 'rewards_train/rejected': '0.05909', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.034754', 'logps_train/rejected': '-108.72', 'logps_train/chosen': '-154.49', 'loss/train': '0.68928', 'examples_per_second': '31.485', 'grad_norm': '42', 'counters/examples': 65600, 'counters/updates': 2050}
train stats after 65632 examples: {'rewards_train/chosen': '0.075615', 'rewards_train/rejected': '0.018691', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056924', 'logps_train/rejected': '-93.826', 'logps_train/chosen': '-131', 'loss/train': '0.67059', 'examples_per_second': '32.57', 'grad_norm': '24.75', 'counters/examples': 65632, 'counters/updates': 2051}
train stats after 65664 examples: {'rewards_train/chosen': '0.093225', 'rewards_train/rejected': '0.055326', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037899', 'logps_train/rejected': '-132.92', 'logps_train/chosen': '-186.32', 'loss/train': '0.68364', 'examples_per_second': '31.883', 'grad_norm': '33.25', 'counters/examples': 65664, 'counters/updates': 2052}
train stats after 65696 examples: {'rewards_train/chosen': '0.070761', 'rewards_train/rejected': '-0.0069597', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.07772', 'logps_train/rejected': '-93.129', 'logps_train/chosen': '-113.86', 'loss/train': '0.65779', 'examples_per_second': '30.424', 'grad_norm': '24.875', 'counters/examples': 65696, 'counters/updates': 2053}
train stats after 65728 examples: {'rewards_train/chosen': '0.060889', 'rewards_train/rejected': '0.043307', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017582', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-149.9', 'loss/train': '0.69359', 'examples_per_second': '33.081', 'grad_norm': '37.5', 'counters/examples': 65728, 'counters/updates': 2054}
train stats after 65760 examples: {'rewards_train/chosen': '0.084398', 'rewards_train/rejected': '0.074567', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0098307', 'logps_train/rejected': '-140.6', 'logps_train/chosen': '-111.33', 'loss/train': '0.69136', 'examples_per_second': '31.047', 'grad_norm': '27', 'counters/examples': 65760, 'counters/updates': 2055}
train stats after 65792 examples: {'rewards_train/chosen': '0.1382', 'rewards_train/rejected': '0.069918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068279', 'logps_train/rejected': '-105.69', 'logps_train/chosen': '-124.02', 'loss/train': '0.68587', 'examples_per_second': '32.93', 'grad_norm': '29.125', 'counters/examples': 65792, 'counters/updates': 2056}
train stats after 65824 examples: {'rewards_train/chosen': '0.015167', 'rewards_train/rejected': '0.032016', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016849', 'logps_train/rejected': '-146.17', 'logps_train/chosen': '-178.78', 'loss/train': '0.70914', 'examples_per_second': '31.322', 'grad_norm': '31.375', 'counters/examples': 65824, 'counters/updates': 2057}
train stats after 65856 examples: {'rewards_train/chosen': '0.11593', 'rewards_train/rejected': '0.022362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093565', 'logps_train/rejected': '-145.49', 'logps_train/chosen': '-149.49', 'loss/train': '0.66059', 'examples_per_second': '30.613', 'grad_norm': '33.5', 'counters/examples': 65856, 'counters/updates': 2058}
train stats after 65888 examples: {'rewards_train/chosen': '0.049604', 'rewards_train/rejected': '0.013938', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035665', 'logps_train/rejected': '-85.303', 'logps_train/chosen': '-124.51', 'loss/train': '0.68094', 'examples_per_second': '31.57', 'grad_norm': '26.5', 'counters/examples': 65888, 'counters/updates': 2059}
train stats after 65920 examples: {'rewards_train/chosen': '0.25043', 'rewards_train/rejected': '0.055841', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19459', 'logps_train/rejected': '-166', 'logps_train/chosen': '-165.56', 'loss/train': '0.62557', 'examples_per_second': '31.363', 'grad_norm': '39.25', 'counters/examples': 65920, 'counters/updates': 2060}
train stats after 65952 examples: {'rewards_train/chosen': '0.13639', 'rewards_train/rejected': '0.0097978', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12659', 'logps_train/rejected': '-85.083', 'logps_train/chosen': '-139.4', 'loss/train': '0.6358', 'examples_per_second': '31.409', 'grad_norm': '27.75', 'counters/examples': 65952, 'counters/updates': 2061}
train stats after 65984 examples: {'rewards_train/chosen': '0.11259', 'rewards_train/rejected': '0.056406', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.056188', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-143.35', 'loss/train': '0.67166', 'examples_per_second': '30.088', 'grad_norm': '33.75', 'counters/examples': 65984, 'counters/updates': 2062}
train stats after 66016 examples: {'rewards_train/chosen': '0.13137', 'rewards_train/rejected': '0.038135', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.093235', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-152.02', 'loss/train': '0.65438', 'examples_per_second': '31.078', 'grad_norm': '35.5', 'counters/examples': 66016, 'counters/updates': 2063}
train stats after 66048 examples: {'rewards_train/chosen': '0.10197', 'rewards_train/rejected': '0.062078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03989', 'logps_train/rejected': '-141.53', 'logps_train/chosen': '-163.18', 'loss/train': '0.68246', 'examples_per_second': '31.539', 'grad_norm': '32', 'counters/examples': 66048, 'counters/updates': 2064}
train stats after 66080 examples: {'rewards_train/chosen': '0.092304', 'rewards_train/rejected': '0.086697', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056073', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-129.34', 'loss/train': '0.69565', 'examples_per_second': '30.705', 'grad_norm': '30.5', 'counters/examples': 66080, 'counters/updates': 2065}
train stats after 66112 examples: {'rewards_train/chosen': '0.030081', 'rewards_train/rejected': '-0.022519', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0526', 'logps_train/rejected': '-120.64', 'logps_train/chosen': '-123.5', 'loss/train': '0.67063', 'examples_per_second': '31.113', 'grad_norm': '27.875', 'counters/examples': 66112, 'counters/updates': 2066}
train stats after 66144 examples: {'rewards_train/chosen': '0.074127', 'rewards_train/rejected': '0.077421', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0032939', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-134.96', 'loss/train': '0.69978', 'examples_per_second': '30.868', 'grad_norm': '33.75', 'counters/examples': 66144, 'counters/updates': 2067}
skipping logging after 66176 examples to avoid logging too frequently
train stats after 66208 examples: {'rewards_train/chosen': '0.1307', 'rewards_train/rejected': '0.010912', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11979', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-126.81', 'loss/train': '0.64026', 'examples_per_second': '32.734', 'grad_norm': '23.875', 'counters/examples': 66208, 'counters/updates': 2069}
train stats after 66240 examples: {'rewards_train/chosen': '0.10418', 'rewards_train/rejected': '0.045939', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058236', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-148.93', 'loss/train': '0.67424', 'examples_per_second': '31.688', 'grad_norm': '83.5', 'counters/examples': 66240, 'counters/updates': 2070}
train stats after 66272 examples: {'rewards_train/chosen': '0.20008', 'rewards_train/rejected': '0.0095421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19054', 'logps_train/rejected': '-136.03', 'logps_train/chosen': '-162.99', 'loss/train': '0.62361', 'examples_per_second': '29.943', 'grad_norm': '29.375', 'counters/examples': 66272, 'counters/updates': 2071}
train stats after 66304 examples: {'rewards_train/chosen': '0.055147', 'rewards_train/rejected': '0.060519', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0053713', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-129.79', 'loss/train': '0.70348', 'examples_per_second': '30.027', 'grad_norm': '33', 'counters/examples': 66304, 'counters/updates': 2072}
skipping logging after 66336 examples to avoid logging too frequently
train stats after 66368 examples: {'rewards_train/chosen': '0.11577', 'rewards_train/rejected': '0.044393', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071375', 'logps_train/rejected': '-151.77', 'logps_train/chosen': '-191.45', 'loss/train': '0.67182', 'examples_per_second': '32.572', 'grad_norm': '50.75', 'counters/examples': 66368, 'counters/updates': 2074}
train stats after 66400 examples: {'rewards_train/chosen': '0.061714', 'rewards_train/rejected': '0.08736', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.025646', 'logps_train/rejected': '-156.28', 'logps_train/chosen': '-145.5', 'loss/train': '0.71192', 'examples_per_second': '30.556', 'grad_norm': '32.75', 'counters/examples': 66400, 'counters/updates': 2075}
train stats after 66432 examples: {'rewards_train/chosen': '0.18721', 'rewards_train/rejected': '0.098812', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088398', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-137.43', 'loss/train': '0.68032', 'examples_per_second': '31.488', 'grad_norm': '36.25', 'counters/examples': 66432, 'counters/updates': 2076}
train stats after 66464 examples: {'rewards_train/chosen': '0.13864', 'rewards_train/rejected': '0.033539', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10511', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-175.16', 'loss/train': '0.65899', 'examples_per_second': '31.243', 'grad_norm': '35.25', 'counters/examples': 66464, 'counters/updates': 2077}
train stats after 66496 examples: {'rewards_train/chosen': '0.078296', 'rewards_train/rejected': '0.048235', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030062', 'logps_train/rejected': '-107.98', 'logps_train/chosen': '-139.37', 'loss/train': '0.69218', 'examples_per_second': '31.416', 'grad_norm': '45.25', 'counters/examples': 66496, 'counters/updates': 2078}
train stats after 66528 examples: {'rewards_train/chosen': '0.10288', 'rewards_train/rejected': '0.026709', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076175', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-170.12', 'loss/train': '0.66427', 'examples_per_second': '31.513', 'grad_norm': '28.125', 'counters/examples': 66528, 'counters/updates': 2079}
train stats after 66560 examples: {'rewards_train/chosen': '0.09777', 'rewards_train/rejected': '0.1082', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.010426', 'logps_train/rejected': '-110.28', 'logps_train/chosen': '-132.44', 'loss/train': '0.71189', 'examples_per_second': '30.334', 'grad_norm': '31.625', 'counters/examples': 66560, 'counters/updates': 2080}
train stats after 66592 examples: {'rewards_train/chosen': '0.033536', 'rewards_train/rejected': '0.031958', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0015779', 'logps_train/rejected': '-121.81', 'logps_train/chosen': '-120.73', 'loss/train': '0.69862', 'examples_per_second': '31.652', 'grad_norm': '33', 'counters/examples': 66592, 'counters/updates': 2081}
skipping logging after 66624 examples to avoid logging too frequently
train stats after 66656 examples: {'rewards_train/chosen': '0.028012', 'rewards_train/rejected': '-0.0088853', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036897', 'logps_train/rejected': '-86.044', 'logps_train/chosen': '-100.23', 'loss/train': '0.67847', 'examples_per_second': '32.802', 'grad_norm': '23.875', 'counters/examples': 66656, 'counters/updates': 2083}
skipping logging after 66688 examples to avoid logging too frequently
train stats after 66720 examples: {'rewards_train/chosen': '0.1001', 'rewards_train/rejected': '0.084146', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015956', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-152.12', 'loss/train': '0.69264', 'examples_per_second': '31.252', 'grad_norm': '32.25', 'counters/examples': 66720, 'counters/updates': 2085}
train stats after 66752 examples: {'rewards_train/chosen': '0.19028', 'rewards_train/rejected': '0.21148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '-0.021205', 'logps_train/rejected': '-168.68', 'logps_train/chosen': '-176.42', 'loss/train': '0.75782', 'examples_per_second': '31.433', 'grad_norm': '70', 'counters/examples': 66752, 'counters/updates': 2086}
skipping logging after 66784 examples to avoid logging too frequently
train stats after 66816 examples: {'rewards_train/chosen': '0.14789', 'rewards_train/rejected': '0.096925', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.050968', 'logps_train/rejected': '-162.94', 'logps_train/chosen': '-156.8', 'loss/train': '0.70065', 'examples_per_second': '31.099', 'grad_norm': '66', 'counters/examples': 66816, 'counters/updates': 2088}
train stats after 66848 examples: {'rewards_train/chosen': '0.051073', 'rewards_train/rejected': '0.01174', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039332', 'logps_train/rejected': '-108.65', 'logps_train/chosen': '-133.74', 'loss/train': '0.6819', 'examples_per_second': '32.91', 'grad_norm': '28.375', 'counters/examples': 66848, 'counters/updates': 2089}
train stats after 66880 examples: {'rewards_train/chosen': '0.038077', 'rewards_train/rejected': '-0.0088127', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04689', 'logps_train/rejected': '-134.61', 'logps_train/chosen': '-135.7', 'loss/train': '0.6789', 'examples_per_second': '31.483', 'grad_norm': '32.25', 'counters/examples': 66880, 'counters/updates': 2090}
skipping logging after 66912 examples to avoid logging too frequently
train stats after 66944 examples: {'rewards_train/chosen': '0.10166', 'rewards_train/rejected': '0.04587', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055786', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-139.49', 'loss/train': '0.67701', 'examples_per_second': '32.623', 'grad_norm': '33.25', 'counters/examples': 66944, 'counters/updates': 2092}
train stats after 66976 examples: {'rewards_train/chosen': '0.073882', 'rewards_train/rejected': '0.056096', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017786', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-140.33', 'loss/train': '0.6906', 'examples_per_second': '32.54', 'grad_norm': '32.75', 'counters/examples': 66976, 'counters/updates': 2093}
skipping logging after 67008 examples to avoid logging too frequently
train stats after 67040 examples: {'rewards_train/chosen': '0.08285', 'rewards_train/rejected': '0.022926', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059924', 'logps_train/rejected': '-140.7', 'logps_train/chosen': '-141.55', 'loss/train': '0.66883', 'examples_per_second': '31.229', 'grad_norm': '28.625', 'counters/examples': 67040, 'counters/updates': 2095}
train stats after 67072 examples: {'rewards_train/chosen': '0.070649', 'rewards_train/rejected': '0.048132', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022518', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-114.38', 'loss/train': '0.69108', 'examples_per_second': '31.287', 'grad_norm': '31.75', 'counters/examples': 67072, 'counters/updates': 2096}
train stats after 67104 examples: {'rewards_train/chosen': '0.15276', 'rewards_train/rejected': '0.027534', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12522', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-139.35', 'loss/train': '0.64649', 'examples_per_second': '31.688', 'grad_norm': '25.375', 'counters/examples': 67104, 'counters/updates': 2097}
train stats after 67136 examples: {'rewards_train/chosen': '0.11723', 'rewards_train/rejected': '0.084312', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032915', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-150.53', 'loss/train': '0.69347', 'examples_per_second': '30.56', 'grad_norm': '42', 'counters/examples': 67136, 'counters/updates': 2098}
train stats after 67168 examples: {'rewards_train/chosen': '0.078717', 'rewards_train/rejected': '0.079591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00087342', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-122.25', 'loss/train': '0.70052', 'examples_per_second': '30.697', 'grad_norm': '33.75', 'counters/examples': 67168, 'counters/updates': 2099}
train stats after 67200 examples: {'rewards_train/chosen': '0.087572', 'rewards_train/rejected': '0.033612', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.05396', 'logps_train/rejected': '-108.93', 'logps_train/chosen': '-110.31', 'loss/train': '0.67725', 'examples_per_second': '30.509', 'grad_norm': '44', 'counters/examples': 67200, 'counters/updates': 2100}
train stats after 67232 examples: {'rewards_train/chosen': '0.034322', 'rewards_train/rejected': '-0.029785', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064106', 'logps_train/rejected': '-122.45', 'logps_train/chosen': '-117.3', 'loss/train': '0.66862', 'examples_per_second': '33.132', 'grad_norm': '35.25', 'counters/examples': 67232, 'counters/updates': 2101}
train stats after 67264 examples: {'rewards_train/chosen': '0.060055', 'rewards_train/rejected': '0.09351', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033454', 'logps_train/rejected': '-117.86', 'logps_train/chosen': '-138.47', 'loss/train': '0.71826', 'examples_per_second': '31.382', 'grad_norm': '43.25', 'counters/examples': 67264, 'counters/updates': 2102}
skipping logging after 67296 examples to avoid logging too frequently
train stats after 67328 examples: {'rewards_train/chosen': '0.061698', 'rewards_train/rejected': '0.011577', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050121', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-123.94', 'loss/train': '0.67633', 'examples_per_second': '31.824', 'grad_norm': '27.625', 'counters/examples': 67328, 'counters/updates': 2104}
skipping logging after 67360 examples to avoid logging too frequently
train stats after 67392 examples: {'rewards_train/chosen': '0.04269', 'rewards_train/rejected': '0.024348', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.018342', 'logps_train/rejected': '-101.23', 'logps_train/chosen': '-122.38', 'loss/train': '0.6887', 'examples_per_second': '35.818', 'grad_norm': '30.375', 'counters/examples': 67392, 'counters/updates': 2106}
train stats after 67424 examples: {'rewards_train/chosen': '0.099628', 'rewards_train/rejected': '0.09536', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0042677', 'logps_train/rejected': '-142.08', 'logps_train/chosen': '-145.31', 'loss/train': '0.69918', 'examples_per_second': '31.424', 'grad_norm': '34', 'counters/examples': 67424, 'counters/updates': 2107}
skipping logging after 67456 examples to avoid logging too frequently
train stats after 67488 examples: {'rewards_train/chosen': '0.1177', 'rewards_train/rejected': '0.072212', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045484', 'logps_train/rejected': '-135.19', 'logps_train/chosen': '-154.14', 'loss/train': '0.67974', 'examples_per_second': '30.713', 'grad_norm': '30.25', 'counters/examples': 67488, 'counters/updates': 2109}
train stats after 67520 examples: {'rewards_train/chosen': '0.029067', 'rewards_train/rejected': '0.024223', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0048439', 'logps_train/rejected': '-144.07', 'logps_train/chosen': '-130.12', 'loss/train': '0.69716', 'examples_per_second': '31.301', 'grad_norm': '35.25', 'counters/examples': 67520, 'counters/updates': 2110}
train stats after 67552 examples: {'rewards_train/chosen': '0.075513', 'rewards_train/rejected': '0.048977', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026536', 'logps_train/rejected': '-95.152', 'logps_train/chosen': '-151.45', 'loss/train': '0.68755', 'examples_per_second': '30.887', 'grad_norm': '30.875', 'counters/examples': 67552, 'counters/updates': 2111}
train stats after 67584 examples: {'rewards_train/chosen': '0.014413', 'rewards_train/rejected': '0.038572', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024159', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-123.28', 'loss/train': '0.70994', 'examples_per_second': '32.475', 'grad_norm': '33.5', 'counters/examples': 67584, 'counters/updates': 2112}
train stats after 67616 examples: {'rewards_train/chosen': '0.083946', 'rewards_train/rejected': '0.089173', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0052269', 'logps_train/rejected': '-167.92', 'logps_train/chosen': '-148.39', 'loss/train': '0.70285', 'examples_per_second': '31.423', 'grad_norm': '34', 'counters/examples': 67616, 'counters/updates': 2113}
train stats after 67648 examples: {'rewards_train/chosen': '0.077411', 'rewards_train/rejected': '0.010876', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066535', 'logps_train/rejected': '-89.224', 'logps_train/chosen': '-157.62', 'loss/train': '0.66902', 'examples_per_second': '31.986', 'grad_norm': '29.25', 'counters/examples': 67648, 'counters/updates': 2114}
train stats after 67680 examples: {'rewards_train/chosen': '0.11431', 'rewards_train/rejected': '0.056229', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058085', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-139.19', 'loss/train': '0.66867', 'examples_per_second': '30.455', 'grad_norm': '28.5', 'counters/examples': 67680, 'counters/updates': 2115}
train stats after 67712 examples: {'rewards_train/chosen': '0.11539', 'rewards_train/rejected': '0.10305', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01234', 'logps_train/rejected': '-165.02', 'logps_train/chosen': '-169.98', 'loss/train': '0.6945', 'examples_per_second': '31.243', 'grad_norm': '39', 'counters/examples': 67712, 'counters/updates': 2116}
train stats after 67744 examples: {'rewards_train/chosen': '-0.0048903', 'rewards_train/rejected': '0.094742', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.099632', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-135.71', 'loss/train': '0.75244', 'examples_per_second': '31.289', 'grad_norm': '50.5', 'counters/examples': 67744, 'counters/updates': 2117}
train stats after 67776 examples: {'rewards_train/chosen': '0.10906', 'rewards_train/rejected': '0.072597', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.036463', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-130.94', 'loss/train': '0.68165', 'examples_per_second': '31.372', 'grad_norm': '32.25', 'counters/examples': 67776, 'counters/updates': 2118}
train stats after 67808 examples: {'rewards_train/chosen': '0.077635', 'rewards_train/rejected': '0.03653', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041105', 'logps_train/rejected': '-163.06', 'logps_train/chosen': '-163.94', 'loss/train': '0.68097', 'examples_per_second': '31.42', 'grad_norm': '48', 'counters/examples': 67808, 'counters/updates': 2119}
train stats after 67840 examples: {'rewards_train/chosen': '0.0092011', 'rewards_train/rejected': '0.043536', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034335', 'logps_train/rejected': '-124.35', 'logps_train/chosen': '-112.32', 'loss/train': '0.71429', 'examples_per_second': '31.426', 'grad_norm': '35.5', 'counters/examples': 67840, 'counters/updates': 2120}
skipping logging after 67872 examples to avoid logging too frequently
train stats after 67904 examples: {'rewards_train/chosen': '0.1224', 'rewards_train/rejected': '0.022987', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099417', 'logps_train/rejected': '-89.974', 'logps_train/chosen': '-117.24', 'loss/train': '0.65316', 'examples_per_second': '34.278', 'grad_norm': '24.625', 'counters/examples': 67904, 'counters/updates': 2122}
train stats after 67936 examples: {'rewards_train/chosen': '0.057482', 'rewards_train/rejected': '0.076764', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019282', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-137.63', 'loss/train': '0.7118', 'examples_per_second': '32.511', 'grad_norm': '34', 'counters/examples': 67936, 'counters/updates': 2123}
train stats after 67968 examples: {'rewards_train/chosen': '0.11012', 'rewards_train/rejected': '0.058201', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05192', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-223', 'loss/train': '0.67798', 'examples_per_second': '31.082', 'grad_norm': '39.25', 'counters/examples': 67968, 'counters/updates': 2124}
skipping logging after 68000 examples to avoid logging too frequently
train stats after 68032 examples: {'rewards_train/chosen': '0.087736', 'rewards_train/rejected': '0.049644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038092', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-150.9', 'loss/train': '0.68094', 'examples_per_second': '31.215', 'grad_norm': '36.5', 'counters/examples': 68032, 'counters/updates': 2126}
train stats after 68064 examples: {'rewards_train/chosen': '0.059223', 'rewards_train/rejected': '0.032685', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026538', 'logps_train/rejected': '-141.16', 'logps_train/chosen': '-177.73', 'loss/train': '0.68391', 'examples_per_second': '30.511', 'grad_norm': '43', 'counters/examples': 68064, 'counters/updates': 2127}
train stats after 68096 examples: {'rewards_train/chosen': '0.015165', 'rewards_train/rejected': '0.030975', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015809', 'logps_train/rejected': '-125.71', 'logps_train/chosen': '-151.36', 'loss/train': '0.70593', 'examples_per_second': '29.95', 'grad_norm': '41.5', 'counters/examples': 68096, 'counters/updates': 2128}
skipping logging after 68128 examples to avoid logging too frequently
train stats after 68160 examples: {'rewards_train/chosen': '0.054276', 'rewards_train/rejected': '-0.015241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069516', 'logps_train/rejected': '-127.93', 'logps_train/chosen': '-165.95', 'loss/train': '0.66912', 'examples_per_second': '32.437', 'grad_norm': '36', 'counters/examples': 68160, 'counters/updates': 2130}
train stats after 68192 examples: {'rewards_train/chosen': '0.082403', 'rewards_train/rejected': '0.082334', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '6.8392e-05', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-147.23', 'loss/train': '0.70405', 'examples_per_second': '29.991', 'grad_norm': '62.75', 'counters/examples': 68192, 'counters/updates': 2131}
train stats after 68224 examples: {'rewards_train/chosen': '0.044339', 'rewards_train/rejected': '0.063651', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.019313', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-135.16', 'loss/train': '0.70918', 'examples_per_second': '30.482', 'grad_norm': '29.875', 'counters/examples': 68224, 'counters/updates': 2132}
train stats after 68256 examples: {'rewards_train/chosen': '0.074555', 'rewards_train/rejected': '0.12791', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.053353', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-138.24', 'loss/train': '0.72958', 'examples_per_second': '30.273', 'grad_norm': '43.75', 'counters/examples': 68256, 'counters/updates': 2133}
train stats after 68288 examples: {'rewards_train/chosen': '0.055422', 'rewards_train/rejected': '0.057257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0018352', 'logps_train/rejected': '-116.09', 'logps_train/chosen': '-141.7', 'loss/train': '0.70648', 'examples_per_second': '31.806', 'grad_norm': '35.75', 'counters/examples': 68288, 'counters/updates': 2134}
train stats after 68320 examples: {'rewards_train/chosen': '0.072332', 'rewards_train/rejected': '0.0763', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0039686', 'logps_train/rejected': '-171.94', 'logps_train/chosen': '-164.61', 'loss/train': '0.707', 'examples_per_second': '24.162', 'grad_norm': '39', 'counters/examples': 68320, 'counters/updates': 2135}
train stats after 68352 examples: {'rewards_train/chosen': '0.20201', 'rewards_train/rejected': '0.14588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056122', 'logps_train/rejected': '-150.38', 'logps_train/chosen': '-153.97', 'loss/train': '0.68093', 'examples_per_second': '31.491', 'grad_norm': '41.25', 'counters/examples': 68352, 'counters/updates': 2136}
train stats after 68384 examples: {'rewards_train/chosen': '0.057451', 'rewards_train/rejected': '0.062747', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0052956', 'logps_train/rejected': '-137.44', 'logps_train/chosen': '-168.16', 'loss/train': '0.70388', 'examples_per_second': '32.871', 'grad_norm': '37.75', 'counters/examples': 68384, 'counters/updates': 2137}
train stats after 68416 examples: {'rewards_train/chosen': '0.15881', 'rewards_train/rejected': '0.083156', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075657', 'logps_train/rejected': '-139.93', 'logps_train/chosen': '-183.09', 'loss/train': '0.67215', 'examples_per_second': '24.376', 'grad_norm': '38', 'counters/examples': 68416, 'counters/updates': 2138}
train stats after 68448 examples: {'rewards_train/chosen': '0.078263', 'rewards_train/rejected': '0.067457', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010806', 'logps_train/rejected': '-117.27', 'logps_train/chosen': '-104.61', 'loss/train': '0.69106', 'examples_per_second': '31.999', 'grad_norm': '25.625', 'counters/examples': 68448, 'counters/updates': 2139}
skipping logging after 68480 examples to avoid logging too frequently
train stats after 68512 examples: {'rewards_train/chosen': '0.062244', 'rewards_train/rejected': '0.12872', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066475', 'logps_train/rejected': '-151.18', 'logps_train/chosen': '-151.73', 'loss/train': '0.73912', 'examples_per_second': '29.95', 'grad_norm': '42.5', 'counters/examples': 68512, 'counters/updates': 2141}
skipping logging after 68544 examples to avoid logging too frequently
train stats after 68576 examples: {'rewards_train/chosen': '0.089899', 'rewards_train/rejected': '0.053341', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036558', 'logps_train/rejected': '-156.67', 'logps_train/chosen': '-142.87', 'loss/train': '0.67953', 'examples_per_second': '30.318', 'grad_norm': '50.5', 'counters/examples': 68576, 'counters/updates': 2143}
train stats after 68608 examples: {'rewards_train/chosen': '-0.018486', 'rewards_train/rejected': '0.015912', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.034398', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-161.96', 'loss/train': '0.72775', 'examples_per_second': '31.516', 'grad_norm': '42.25', 'counters/examples': 68608, 'counters/updates': 2144}
skipping logging after 68640 examples to avoid logging too frequently
train stats after 68672 examples: {'rewards_train/chosen': '0.049085', 'rewards_train/rejected': '-0.01078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059865', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-123.47', 'loss/train': '0.66779', 'examples_per_second': '30.552', 'grad_norm': '28.5', 'counters/examples': 68672, 'counters/updates': 2146}
skipping logging after 68704 examples to avoid logging too frequently
train stats after 68736 examples: {'rewards_train/chosen': '0.076203', 'rewards_train/rejected': '0.12662', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.05042', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-179.26', 'loss/train': '0.73093', 'examples_per_second': '31.465', 'grad_norm': '79.5', 'counters/examples': 68736, 'counters/updates': 2148}
skipping logging after 68768 examples to avoid logging too frequently
train stats after 68800 examples: {'rewards_train/chosen': '0.032022', 'rewards_train/rejected': '0.055655', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023633', 'logps_train/rejected': '-149.17', 'logps_train/chosen': '-137.04', 'loss/train': '0.7144', 'examples_per_second': '32.423', 'grad_norm': '30.625', 'counters/examples': 68800, 'counters/updates': 2150}
skipping logging after 68832 examples to avoid logging too frequently
train stats after 68864 examples: {'rewards_train/chosen': '0.1545', 'rewards_train/rejected': '0.062757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091738', 'logps_train/rejected': '-122.76', 'logps_train/chosen': '-147.65', 'loss/train': '0.66464', 'examples_per_second': '30.299', 'grad_norm': '25.625', 'counters/examples': 68864, 'counters/updates': 2152}
train stats after 68896 examples: {'rewards_train/chosen': '0.091141', 'rewards_train/rejected': '0.031611', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05953', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-153.47', 'loss/train': '0.67499', 'examples_per_second': '30.742', 'grad_norm': '27.75', 'counters/examples': 68896, 'counters/updates': 2153}
train stats after 68928 examples: {'rewards_train/chosen': '0.014821', 'rewards_train/rejected': '0.026268', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.011446', 'logps_train/rejected': '-130', 'logps_train/chosen': '-174.22', 'loss/train': '0.70856', 'examples_per_second': '31.45', 'grad_norm': '39.25', 'counters/examples': 68928, 'counters/updates': 2154}
skipping logging after 68960 examples to avoid logging too frequently
train stats after 68992 examples: {'rewards_train/chosen': '0.041678', 'rewards_train/rejected': '0.077243', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.035565', 'logps_train/rejected': '-155.57', 'logps_train/chosen': '-158.61', 'loss/train': '0.71589', 'examples_per_second': '31.516', 'grad_norm': '31.625', 'counters/examples': 68992, 'counters/updates': 2156}
train stats after 69024 examples: {'rewards_train/chosen': '0.14383', 'rewards_train/rejected': '0.031751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11208', 'logps_train/rejected': '-180.24', 'logps_train/chosen': '-174.48', 'loss/train': '0.65285', 'examples_per_second': '29.458', 'grad_norm': '37', 'counters/examples': 69024, 'counters/updates': 2157}
train stats after 69056 examples: {'rewards_train/chosen': '0.080586', 'rewards_train/rejected': '0.079252', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0013343', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-127.91', 'loss/train': '0.69806', 'examples_per_second': '32.604', 'grad_norm': '28', 'counters/examples': 69056, 'counters/updates': 2158}
train stats after 69088 examples: {'rewards_train/chosen': '0.11257', 'rewards_train/rejected': '0.0043914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10818', 'logps_train/rejected': '-171.17', 'logps_train/chosen': '-148.47', 'loss/train': '0.64457', 'examples_per_second': '29.906', 'grad_norm': '30', 'counters/examples': 69088, 'counters/updates': 2159}
train stats after 69120 examples: {'rewards_train/chosen': '0.058461', 'rewards_train/rejected': '0.019942', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038519', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-126.02', 'loss/train': '0.68084', 'examples_per_second': '30.575', 'grad_norm': '27.875', 'counters/examples': 69120, 'counters/updates': 2160}
skipping logging after 69152 examples to avoid logging too frequently
train stats after 69184 examples: {'rewards_train/chosen': '0.092722', 'rewards_train/rejected': '0.082481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010241', 'logps_train/rejected': '-180.8', 'logps_train/chosen': '-200.82', 'loss/train': '0.7052', 'examples_per_second': '31.43', 'grad_norm': '57.75', 'counters/examples': 69184, 'counters/updates': 2162}
skipping logging after 69216 examples to avoid logging too frequently
train stats after 69248 examples: {'rewards_train/chosen': '0.04255', 'rewards_train/rejected': '0.055759', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013209', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-106.06', 'loss/train': '0.70518', 'examples_per_second': '30.631', 'grad_norm': '31.25', 'counters/examples': 69248, 'counters/updates': 2164}
train stats after 69280 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '-0.023153', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13756', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-129.84', 'loss/train': '0.63966', 'examples_per_second': '31.805', 'grad_norm': '42', 'counters/examples': 69280, 'counters/updates': 2165}
train stats after 69312 examples: {'rewards_train/chosen': '0.13477', 'rewards_train/rejected': '0.13197', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0027923', 'logps_train/rejected': '-180.15', 'logps_train/chosen': '-186.77', 'loss/train': '0.70189', 'examples_per_second': '31.565', 'grad_norm': '44', 'counters/examples': 69312, 'counters/updates': 2166}
train stats after 69344 examples: {'rewards_train/chosen': '0.079396', 'rewards_train/rejected': '0.092244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012848', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-153.64', 'loss/train': '0.70963', 'examples_per_second': '29.928', 'grad_norm': '29', 'counters/examples': 69344, 'counters/updates': 2167}
train stats after 69376 examples: {'rewards_train/chosen': '0.11629', 'rewards_train/rejected': '0.016323', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099964', 'logps_train/rejected': '-115.16', 'logps_train/chosen': '-111.41', 'loss/train': '0.64953', 'examples_per_second': '32.357', 'grad_norm': '26.375', 'counters/examples': 69376, 'counters/updates': 2168}
train stats after 69408 examples: {'rewards_train/chosen': '0.054305', 'rewards_train/rejected': '0.071919', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017613', 'logps_train/rejected': '-119.68', 'logps_train/chosen': '-151.55', 'loss/train': '0.70652', 'examples_per_second': '30.573', 'grad_norm': '26.5', 'counters/examples': 69408, 'counters/updates': 2169}
train stats after 69440 examples: {'rewards_train/chosen': '0.1028', 'rewards_train/rejected': '0.057903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044893', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-137.51', 'loss/train': '0.67684', 'examples_per_second': '30.749', 'grad_norm': '43.75', 'counters/examples': 69440, 'counters/updates': 2170}
train stats after 69472 examples: {'rewards_train/chosen': '0.11885', 'rewards_train/rejected': '0.019524', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099321', 'logps_train/rejected': '-153.71', 'logps_train/chosen': '-186.73', 'loss/train': '0.65077', 'examples_per_second': '30.824', 'grad_norm': '34.5', 'counters/examples': 69472, 'counters/updates': 2171}
skipping logging after 69504 examples to avoid logging too frequently
train stats after 69536 examples: {'rewards_train/chosen': '0.11138', 'rewards_train/rejected': '0.016477', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094905', 'logps_train/rejected': '-110.71', 'logps_train/chosen': '-179.12', 'loss/train': '0.65554', 'examples_per_second': '30.478', 'grad_norm': '32.75', 'counters/examples': 69536, 'counters/updates': 2173}
train stats after 69568 examples: {'rewards_train/chosen': '0.050264', 'rewards_train/rejected': '0.022946', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027318', 'logps_train/rejected': '-151.3', 'logps_train/chosen': '-152.02', 'loss/train': '0.68369', 'examples_per_second': '31.137', 'grad_norm': '30.125', 'counters/examples': 69568, 'counters/updates': 2174}
train stats after 69600 examples: {'rewards_train/chosen': '0.15488', 'rewards_train/rejected': '0.15916', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0042856', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-146.15', 'loss/train': '0.72079', 'examples_per_second': '32.077', 'grad_norm': '73', 'counters/examples': 69600, 'counters/updates': 2175}
skipping logging after 69632 examples to avoid logging too frequently
train stats after 69664 examples: {'rewards_train/chosen': '0.040894', 'rewards_train/rejected': '0.019986', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020908', 'logps_train/rejected': '-77.344', 'logps_train/chosen': '-136.38', 'loss/train': '0.69259', 'examples_per_second': '31.447', 'grad_norm': '37.75', 'counters/examples': 69664, 'counters/updates': 2177}
skipping logging after 69696 examples to avoid logging too frequently
train stats after 69728 examples: {'rewards_train/chosen': '0.11615', 'rewards_train/rejected': '0.10777', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0083844', 'logps_train/rejected': '-163.65', 'logps_train/chosen': '-163.13', 'loss/train': '0.69986', 'examples_per_second': '31.803', 'grad_norm': '33.25', 'counters/examples': 69728, 'counters/updates': 2179}
train stats after 69760 examples: {'rewards_train/chosen': '0.12006', 'rewards_train/rejected': '0.10981', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.010251', 'logps_train/rejected': '-129.5', 'logps_train/chosen': '-109.61', 'loss/train': '0.6986', 'examples_per_second': '25.754', 'grad_norm': '33.75', 'counters/examples': 69760, 'counters/updates': 2180}
train stats after 69792 examples: {'rewards_train/chosen': '0.10148', 'rewards_train/rejected': '0.035262', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066215', 'logps_train/rejected': '-144', 'logps_train/chosen': '-127.99', 'loss/train': '0.66823', 'examples_per_second': '32.074', 'grad_norm': '29.5', 'counters/examples': 69792, 'counters/updates': 2181}
train stats after 69824 examples: {'rewards_train/chosen': '0.058016', 'rewards_train/rejected': '0.04898', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0090357', 'logps_train/rejected': '-133.79', 'logps_train/chosen': '-149.53', 'loss/train': '0.6967', 'examples_per_second': '31.371', 'grad_norm': '50.25', 'counters/examples': 69824, 'counters/updates': 2182}
train stats after 69856 examples: {'rewards_train/chosen': '0.16645', 'rewards_train/rejected': '0.048822', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11763', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-144.83', 'loss/train': '0.64473', 'examples_per_second': '31.002', 'grad_norm': '38.5', 'counters/examples': 69856, 'counters/updates': 2183}
skipping logging after 69888 examples to avoid logging too frequently
train stats after 69920 examples: {'rewards_train/chosen': '0.068025', 'rewards_train/rejected': '0.018745', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04928', 'logps_train/rejected': '-94.733', 'logps_train/chosen': '-152.58', 'loss/train': '0.67534', 'examples_per_second': '31.497', 'grad_norm': '31.875', 'counters/examples': 69920, 'counters/updates': 2185}
train stats after 69952 examples: {'rewards_train/chosen': '0.10907', 'rewards_train/rejected': '0.047027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06204', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-151.28', 'loss/train': '0.67612', 'examples_per_second': '30.013', 'grad_norm': '30.875', 'counters/examples': 69952, 'counters/updates': 2186}
skipping logging after 69984 examples to avoid logging too frequently
train stats after 70016 examples: {'rewards_train/chosen': '0.043253', 'rewards_train/rejected': '0.022006', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021247', 'logps_train/rejected': '-91.245', 'logps_train/chosen': '-112.87', 'loss/train': '0.68774', 'examples_per_second': '32.571', 'grad_norm': '26.625', 'counters/examples': 70016, 'counters/updates': 2188}
train stats after 70048 examples: {'rewards_train/chosen': '-0.0042829', 'rewards_train/rejected': '0.069404', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.073687', 'logps_train/rejected': '-148.48', 'logps_train/chosen': '-147.16', 'loss/train': '0.73518', 'examples_per_second': '29.996', 'grad_norm': '32.5', 'counters/examples': 70048, 'counters/updates': 2189}
skipping logging after 70080 examples to avoid logging too frequently
train stats after 70112 examples: {'rewards_train/chosen': '0.044256', 'rewards_train/rejected': '-0.03601', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.080266', 'logps_train/rejected': '-105.09', 'logps_train/chosen': '-107.47', 'loss/train': '0.65681', 'examples_per_second': '38.007', 'grad_norm': '26.375', 'counters/examples': 70112, 'counters/updates': 2191}
train stats after 70144 examples: {'rewards_train/chosen': '0.072091', 'rewards_train/rejected': '0.01427', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05782', 'logps_train/rejected': '-115.62', 'logps_train/chosen': '-194.44', 'loss/train': '0.67083', 'examples_per_second': '29.782', 'grad_norm': '47', 'counters/examples': 70144, 'counters/updates': 2192}
train stats after 70176 examples: {'rewards_train/chosen': '0.089261', 'rewards_train/rejected': '0.04611', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043151', 'logps_train/rejected': '-108.04', 'logps_train/chosen': '-158.69', 'loss/train': '0.67854', 'examples_per_second': '32.395', 'grad_norm': '34', 'counters/examples': 70176, 'counters/updates': 2193}
train stats after 70208 examples: {'rewards_train/chosen': '0.11449', 'rewards_train/rejected': '0.098154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016338', 'logps_train/rejected': '-150.14', 'logps_train/chosen': '-123.44', 'loss/train': '0.69499', 'examples_per_second': '30.865', 'grad_norm': '32.75', 'counters/examples': 70208, 'counters/updates': 2194}
train stats after 70240 examples: {'rewards_train/chosen': '0.071087', 'rewards_train/rejected': '0.074444', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '-0.0033568', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-139.76', 'loss/train': '0.70264', 'examples_per_second': '30.019', 'grad_norm': '40.75', 'counters/examples': 70240, 'counters/updates': 2195}
train stats after 70272 examples: {'rewards_train/chosen': '0.084444', 'rewards_train/rejected': '0.10259', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018147', 'logps_train/rejected': '-129.83', 'logps_train/chosen': '-133.97', 'loss/train': '0.7044', 'examples_per_second': '32.293', 'grad_norm': '29.75', 'counters/examples': 70272, 'counters/updates': 2196}
skipping logging after 70304 examples to avoid logging too frequently
train stats after 70336 examples: {'rewards_train/chosen': '0.12529', 'rewards_train/rejected': '0.026941', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098349', 'logps_train/rejected': '-134.22', 'logps_train/chosen': '-156.19', 'loss/train': '0.65478', 'examples_per_second': '35.577', 'grad_norm': '30.875', 'counters/examples': 70336, 'counters/updates': 2198}
skipping logging after 70368 examples to avoid logging too frequently
train stats after 70400 examples: {'rewards_train/chosen': '0.11638', 'rewards_train/rejected': '0.020811', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095566', 'logps_train/rejected': '-115.7', 'logps_train/chosen': '-153.77', 'loss/train': '0.65392', 'examples_per_second': '30.203', 'grad_norm': '29.5', 'counters/examples': 70400, 'counters/updates': 2200}
skipping logging after 70432 examples to avoid logging too frequently
train stats after 70464 examples: {'rewards_train/chosen': '0.073468', 'rewards_train/rejected': '0.065224', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0082444', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-148.27', 'loss/train': '0.69528', 'examples_per_second': '29.957', 'grad_norm': '34', 'counters/examples': 70464, 'counters/updates': 2202}
train stats after 70496 examples: {'rewards_train/chosen': '0.060633', 'rewards_train/rejected': '0.058985', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0016475', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-172.26', 'loss/train': '0.69721', 'examples_per_second': '29.985', 'grad_norm': '35.5', 'counters/examples': 70496, 'counters/updates': 2203}
train stats after 70528 examples: {'rewards_train/chosen': '0.082082', 'rewards_train/rejected': '0.067659', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014423', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-150.47', 'loss/train': '0.69328', 'examples_per_second': '30.804', 'grad_norm': '32.25', 'counters/examples': 70528, 'counters/updates': 2204}
train stats after 70560 examples: {'rewards_train/chosen': '0.24873', 'rewards_train/rejected': '0.090175', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15856', 'logps_train/rejected': '-120.05', 'logps_train/chosen': '-163.37', 'loss/train': '0.63115', 'examples_per_second': '29.904', 'grad_norm': '36.25', 'counters/examples': 70560, 'counters/updates': 2205}
train stats after 70592 examples: {'rewards_train/chosen': '0.10673', 'rewards_train/rejected': '0.069414', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037313', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-144.25', 'loss/train': '0.68134', 'examples_per_second': '32.265', 'grad_norm': '38.5', 'counters/examples': 70592, 'counters/updates': 2206}
train stats after 70624 examples: {'rewards_train/chosen': '0.11126', 'rewards_train/rejected': '0.0115', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09976', 'logps_train/rejected': '-135.74', 'logps_train/chosen': '-162.74', 'loss/train': '0.65281', 'examples_per_second': '31.456', 'grad_norm': '33', 'counters/examples': 70624, 'counters/updates': 2207}
train stats after 70656 examples: {'rewards_train/chosen': '0.068296', 'rewards_train/rejected': '0.061996', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0062997', 'logps_train/rejected': '-127.61', 'logps_train/chosen': '-165.93', 'loss/train': '0.69784', 'examples_per_second': '31.039', 'grad_norm': '31.625', 'counters/examples': 70656, 'counters/updates': 2208}
skipping logging after 70688 examples to avoid logging too frequently
train stats after 70720 examples: {'rewards_train/chosen': '0.13112', 'rewards_train/rejected': '0.096924', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034196', 'logps_train/rejected': '-117.12', 'logps_train/chosen': '-154.65', 'loss/train': '0.68578', 'examples_per_second': '31.951', 'grad_norm': '40.5', 'counters/examples': 70720, 'counters/updates': 2210}
train stats after 70752 examples: {'rewards_train/chosen': '0.12883', 'rewards_train/rejected': '0.035566', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.093266', 'logps_train/rejected': '-105.1', 'logps_train/chosen': '-137.33', 'loss/train': '0.65161', 'examples_per_second': '31.487', 'grad_norm': '33', 'counters/examples': 70752, 'counters/updates': 2211}
train stats after 70784 examples: {'rewards_train/chosen': '0.041143', 'rewards_train/rejected': '-0.0014867', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04263', 'logps_train/rejected': '-110.61', 'logps_train/chosen': '-164.48', 'loss/train': '0.67853', 'examples_per_second': '30.927', 'grad_norm': '31.875', 'counters/examples': 70784, 'counters/updates': 2212}
skipping logging after 70816 examples to avoid logging too frequently
train stats after 70848 examples: {'rewards_train/chosen': '0.11867', 'rewards_train/rejected': '0.045784', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07289', 'logps_train/rejected': '-129.78', 'logps_train/chosen': '-152.19', 'loss/train': '0.6647', 'examples_per_second': '30.831', 'grad_norm': '33', 'counters/examples': 70848, 'counters/updates': 2214}
train stats after 70880 examples: {'rewards_train/chosen': '0.10515', 'rewards_train/rejected': '0.023541', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081608', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-173.7', 'loss/train': '0.66115', 'examples_per_second': '31.15', 'grad_norm': '29.75', 'counters/examples': 70880, 'counters/updates': 2215}
train stats after 70912 examples: {'rewards_train/chosen': '0.079589', 'rewards_train/rejected': '0.039472', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040117', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-150.14', 'loss/train': '0.67739', 'examples_per_second': '32.408', 'grad_norm': '28.25', 'counters/examples': 70912, 'counters/updates': 2216}
train stats after 70944 examples: {'rewards_train/chosen': '0.086507', 'rewards_train/rejected': '0.033139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053368', 'logps_train/rejected': '-97.376', 'logps_train/chosen': '-108.84', 'loss/train': '0.67445', 'examples_per_second': '32.816', 'grad_norm': '29.5', 'counters/examples': 70944, 'counters/updates': 2217}
train stats after 70976 examples: {'rewards_train/chosen': '0.065701', 'rewards_train/rejected': '0.077369', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011668', 'logps_train/rejected': '-158.77', 'logps_train/chosen': '-143.78', 'loss/train': '0.70562', 'examples_per_second': '31.482', 'grad_norm': '34.75', 'counters/examples': 70976, 'counters/updates': 2218}
skipping logging after 71008 examples to avoid logging too frequently
train stats after 71040 examples: {'rewards_train/chosen': '0.079061', 'rewards_train/rejected': '0.011202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067859', 'logps_train/rejected': '-125.37', 'logps_train/chosen': '-144.81', 'loss/train': '0.66607', 'examples_per_second': '31.455', 'grad_norm': '40.5', 'counters/examples': 71040, 'counters/updates': 2220}
skipping logging after 71072 examples to avoid logging too frequently
train stats after 71104 examples: {'rewards_train/chosen': '0.075059', 'rewards_train/rejected': '0.019291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055768', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-121.26', 'loss/train': '0.66795', 'examples_per_second': '31.99', 'grad_norm': '24.625', 'counters/examples': 71104, 'counters/updates': 2222}
skipping logging after 71136 examples to avoid logging too frequently
train stats after 71168 examples: {'rewards_train/chosen': '0.15976', 'rewards_train/rejected': '0.053621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10614', 'logps_train/rejected': '-136.18', 'logps_train/chosen': '-137.01', 'loss/train': '0.65298', 'examples_per_second': '34.05', 'grad_norm': '30.875', 'counters/examples': 71168, 'counters/updates': 2224}
train stats after 71200 examples: {'rewards_train/chosen': '0.048934', 'rewards_train/rejected': '-0.0097435', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058677', 'logps_train/rejected': '-148.43', 'logps_train/chosen': '-165.31', 'loss/train': '0.67034', 'examples_per_second': '30.523', 'grad_norm': '31.5', 'counters/examples': 71200, 'counters/updates': 2225}
train stats after 71232 examples: {'rewards_train/chosen': '0.11207', 'rewards_train/rejected': '-0.0014117', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11348', 'logps_train/rejected': '-89.057', 'logps_train/chosen': '-144.48', 'loss/train': '0.64337', 'examples_per_second': '32.497', 'grad_norm': '31.125', 'counters/examples': 71232, 'counters/updates': 2226}
train stats after 71264 examples: {'rewards_train/chosen': '0.066432', 'rewards_train/rejected': '0.059338', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0070933', 'logps_train/rejected': '-110.81', 'logps_train/chosen': '-178.27', 'loss/train': '0.69637', 'examples_per_second': '31.171', 'grad_norm': '33.25', 'counters/examples': 71264, 'counters/updates': 2227}
train stats after 71296 examples: {'rewards_train/chosen': '0.09287', 'rewards_train/rejected': '0.041418', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051452', 'logps_train/rejected': '-109.9', 'logps_train/chosen': '-150.01', 'loss/train': '0.67309', 'examples_per_second': '30.765', 'grad_norm': '29', 'counters/examples': 71296, 'counters/updates': 2228}
train stats after 71328 examples: {'rewards_train/chosen': '0.16063', 'rewards_train/rejected': '-0.0083294', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16895', 'logps_train/rejected': '-150.93', 'logps_train/chosen': '-158.76', 'loss/train': '0.62246', 'examples_per_second': '32.74', 'grad_norm': '33.25', 'counters/examples': 71328, 'counters/updates': 2229}
skipping logging after 71360 examples to avoid logging too frequently
train stats after 71392 examples: {'rewards_train/chosen': '0.16447', 'rewards_train/rejected': '0.072497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091977', 'logps_train/rejected': '-170.46', 'logps_train/chosen': '-136.74', 'loss/train': '0.65545', 'examples_per_second': '30.027', 'grad_norm': '50.25', 'counters/examples': 71392, 'counters/updates': 2231}
train stats after 71424 examples: {'rewards_train/chosen': '0.076224', 'rewards_train/rejected': '0.023947', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052277', 'logps_train/rejected': '-111.71', 'logps_train/chosen': '-107.44', 'loss/train': '0.67474', 'examples_per_second': '32.333', 'grad_norm': '31.375', 'counters/examples': 71424, 'counters/updates': 2232}
train stats after 71456 examples: {'rewards_train/chosen': '0.15778', 'rewards_train/rejected': '0.016194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14159', 'logps_train/rejected': '-123.59', 'logps_train/chosen': '-147.41', 'loss/train': '0.6345', 'examples_per_second': '31.64', 'grad_norm': '27.5', 'counters/examples': 71456, 'counters/updates': 2233}
train stats after 71488 examples: {'rewards_train/chosen': '0.12288', 'rewards_train/rejected': '0.02556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09732', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-182.21', 'loss/train': '0.653', 'examples_per_second': '30.753', 'grad_norm': '34.75', 'counters/examples': 71488, 'counters/updates': 2234}
train stats after 71520 examples: {'rewards_train/chosen': '0.019699', 'rewards_train/rejected': '0.057868', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.03817', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-127.94', 'loss/train': '0.71683', 'examples_per_second': '30.473', 'grad_norm': '28.125', 'counters/examples': 71520, 'counters/updates': 2235}
train stats after 71552 examples: {'rewards_train/chosen': '0.020432', 'rewards_train/rejected': '0.031903', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01147', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-124.64', 'loss/train': '0.70175', 'examples_per_second': '32.267', 'grad_norm': '28.375', 'counters/examples': 71552, 'counters/updates': 2236}
train stats after 71584 examples: {'rewards_train/chosen': '0.11084', 'rewards_train/rejected': '0.012357', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098481', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-164.04', 'loss/train': '0.65344', 'examples_per_second': '30.13', 'grad_norm': '30', 'counters/examples': 71584, 'counters/updates': 2237}
skipping logging after 71616 examples to avoid logging too frequently
train stats after 71648 examples: {'rewards_train/chosen': '0.15872', 'rewards_train/rejected': '0.057374', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10134', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-152.32', 'loss/train': '0.65638', 'examples_per_second': '31.484', 'grad_norm': '35', 'counters/examples': 71648, 'counters/updates': 2239}
train stats after 71680 examples: {'rewards_train/chosen': '0.12297', 'rewards_train/rejected': '0.091212', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031759', 'logps_train/rejected': '-115.08', 'logps_train/chosen': '-149.46', 'loss/train': '0.68504', 'examples_per_second': '31.386', 'grad_norm': '31', 'counters/examples': 71680, 'counters/updates': 2240}
train stats after 71712 examples: {'rewards_train/chosen': '0.071458', 'rewards_train/rejected': '0.073096', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0016379', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-128.09', 'loss/train': '0.70448', 'examples_per_second': '32.163', 'grad_norm': '32.5', 'counters/examples': 71712, 'counters/updates': 2241}
train stats after 71744 examples: {'rewards_train/chosen': '0.12699', 'rewards_train/rejected': '0.074129', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052858', 'logps_train/rejected': '-138.4', 'logps_train/chosen': '-190.14', 'loss/train': '0.67993', 'examples_per_second': '32.726', 'grad_norm': '38.25', 'counters/examples': 71744, 'counters/updates': 2242}
train stats after 71776 examples: {'rewards_train/chosen': '0.072274', 'rewards_train/rejected': '0.0028948', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069379', 'logps_train/rejected': '-98.012', 'logps_train/chosen': '-140.92', 'loss/train': '0.66514', 'examples_per_second': '30.576', 'grad_norm': '33', 'counters/examples': 71776, 'counters/updates': 2243}
skipping logging after 71808 examples to avoid logging too frequently
train stats after 71840 examples: {'rewards_train/chosen': '0.17654', 'rewards_train/rejected': '0.016857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15968', 'logps_train/rejected': '-104.96', 'logps_train/chosen': '-207.58', 'loss/train': '0.63658', 'examples_per_second': '31.743', 'grad_norm': '36.75', 'counters/examples': 71840, 'counters/updates': 2245}
train stats after 71872 examples: {'rewards_train/chosen': '0.084793', 'rewards_train/rejected': '0.039584', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045209', 'logps_train/rejected': '-122.93', 'logps_train/chosen': '-154.79', 'loss/train': '0.67849', 'examples_per_second': '31.066', 'grad_norm': '28.125', 'counters/examples': 71872, 'counters/updates': 2246}
train stats after 71904 examples: {'rewards_train/chosen': '0.042828', 'rewards_train/rejected': '0.036169', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0066589', 'logps_train/rejected': '-111.13', 'logps_train/chosen': '-131.88', 'loss/train': '0.6957', 'examples_per_second': '31.946', 'grad_norm': '29.375', 'counters/examples': 71904, 'counters/updates': 2247}
train stats after 71936 examples: {'rewards_train/chosen': '0.05883', 'rewards_train/rejected': '0.046962', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011868', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-166.94', 'loss/train': '0.70005', 'examples_per_second': '31.376', 'grad_norm': '37.5', 'counters/examples': 71936, 'counters/updates': 2248}
train stats after 71968 examples: {'rewards_train/chosen': '0.039714', 'rewards_train/rejected': '0.027308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012406', 'logps_train/rejected': '-148.61', 'logps_train/chosen': '-138.56', 'loss/train': '0.69042', 'examples_per_second': '33.043', 'grad_norm': '32.25', 'counters/examples': 71968, 'counters/updates': 2249}
train stats after 72000 examples: {'rewards_train/chosen': '0.13886', 'rewards_train/rejected': '0.022708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11616', 'logps_train/rejected': '-151.34', 'logps_train/chosen': '-145.35', 'loss/train': '0.64512', 'examples_per_second': '32.294', 'grad_norm': '34.5', 'counters/examples': 72000, 'counters/updates': 2250}
Running evaluation after 72000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.16it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.81it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.91it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.91it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 72000: {'rewards_eval/chosen': '0.099101', 'rewards_eval/rejected': '0.049537', 'rewards_eval/accuracies': '0.58984', 'rewards_eval/margins': '0.049563', 'logps_eval/rejected': '-121.64', 'logps_eval/chosen': '-143.11', 'loss/eval': '0.67831'}
skipping save for non epoch
train stats after 72032 examples: {'rewards_train/chosen': '0.061545', 'rewards_train/rejected': '0.017952', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043592', 'logps_train/rejected': '-167.17', 'logps_train/chosen': '-157.43', 'loss/train': '0.67579', 'examples_per_second': '30.558', 'grad_norm': '33.75', 'counters/examples': 72032, 'counters/updates': 2251}
train stats after 72064 examples: {'rewards_train/chosen': '0.079455', 'rewards_train/rejected': '0.029774', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049681', 'logps_train/rejected': '-93.537', 'logps_train/chosen': '-136.79', 'loss/train': '0.67254', 'examples_per_second': '29.973', 'grad_norm': '35.5', 'counters/examples': 72064, 'counters/updates': 2252}
train stats after 72096 examples: {'rewards_train/chosen': '0.12483', 'rewards_train/rejected': '0.062859', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061973', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-186.9', 'loss/train': '0.67208', 'examples_per_second': '32.615', 'grad_norm': '36.5', 'counters/examples': 72096, 'counters/updates': 2253}
skipping logging after 72128 examples to avoid logging too frequently
train stats after 72160 examples: {'rewards_train/chosen': '0.047691', 'rewards_train/rejected': '0.02678', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020911', 'logps_train/rejected': '-108.55', 'logps_train/chosen': '-117.74', 'loss/train': '0.69135', 'examples_per_second': '31.806', 'grad_norm': '26.125', 'counters/examples': 72160, 'counters/updates': 2255}
skipping logging after 72192 examples to avoid logging too frequently
train stats after 72224 examples: {'rewards_train/chosen': '0.208', 'rewards_train/rejected': '0.024695', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1833', 'logps_train/rejected': '-100.75', 'logps_train/chosen': '-166.27', 'loss/train': '0.62442', 'examples_per_second': '31.486', 'grad_norm': '25.875', 'counters/examples': 72224, 'counters/updates': 2257}
skipping logging after 72256 examples to avoid logging too frequently
train stats after 72288 examples: {'rewards_train/chosen': '0.03946', 'rewards_train/rejected': '0.14428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.10482', 'logps_train/rejected': '-125.3', 'logps_train/chosen': '-162.67', 'loss/train': '0.80498', 'examples_per_second': '31.489', 'grad_norm': '109.5', 'counters/examples': 72288, 'counters/updates': 2259}
train stats after 72320 examples: {'rewards_train/chosen': '0.13483', 'rewards_train/rejected': '0.049998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084836', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-170.24', 'loss/train': '0.66386', 'examples_per_second': '32.122', 'grad_norm': '29.5', 'counters/examples': 72320, 'counters/updates': 2260}
skipping logging after 72352 examples to avoid logging too frequently
train stats after 72384 examples: {'rewards_train/chosen': '0.1183', 'rewards_train/rejected': '0.10293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015377', 'logps_train/rejected': '-161.65', 'logps_train/chosen': '-151.62', 'loss/train': '0.69482', 'examples_per_second': '32.536', 'grad_norm': '56.25', 'counters/examples': 72384, 'counters/updates': 2262}
train stats after 72416 examples: {'rewards_train/chosen': '0.093539', 'rewards_train/rejected': '0.04194', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051599', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-133.12', 'loss/train': '0.67126', 'examples_per_second': '31.442', 'grad_norm': '28.875', 'counters/examples': 72416, 'counters/updates': 2263}
train stats after 72448 examples: {'rewards_train/chosen': '0.046837', 'rewards_train/rejected': '0.023351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023486', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-143.79', 'loss/train': '0.6885', 'examples_per_second': '32.359', 'grad_norm': '32', 'counters/examples': 72448, 'counters/updates': 2264}
train stats after 72480 examples: {'rewards_train/chosen': '0.13864', 'rewards_train/rejected': '0.091228', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047408', 'logps_train/rejected': '-113.96', 'logps_train/chosen': '-143.33', 'loss/train': '0.68079', 'examples_per_second': '30.152', 'grad_norm': '28.125', 'counters/examples': 72480, 'counters/updates': 2265}
train stats after 72512 examples: {'rewards_train/chosen': '0.10666', 'rewards_train/rejected': '0.049133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05753', 'logps_train/rejected': '-111.52', 'logps_train/chosen': '-135.65', 'loss/train': '0.67145', 'examples_per_second': '31.351', 'grad_norm': '35.75', 'counters/examples': 72512, 'counters/updates': 2266}
skipping logging after 72544 examples to avoid logging too frequently
train stats after 72576 examples: {'rewards_train/chosen': '0.19387', 'rewards_train/rejected': '0.11274', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.081128', 'logps_train/rejected': '-153.02', 'logps_train/chosen': '-179.91', 'loss/train': '0.67', 'examples_per_second': '31.423', 'grad_norm': '43.5', 'counters/examples': 72576, 'counters/updates': 2268}
train stats after 72608 examples: {'rewards_train/chosen': '0.048752', 'rewards_train/rejected': '0.0068341', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041918', 'logps_train/rejected': '-111.97', 'logps_train/chosen': '-130.83', 'loss/train': '0.67997', 'examples_per_second': '32.508', 'grad_norm': '29.625', 'counters/examples': 72608, 'counters/updates': 2269}
train stats after 72640 examples: {'rewards_train/chosen': '0.090965', 'rewards_train/rejected': '0.037627', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053338', 'logps_train/rejected': '-138.52', 'logps_train/chosen': '-131.58', 'loss/train': '0.67554', 'examples_per_second': '31.938', 'grad_norm': '33.5', 'counters/examples': 72640, 'counters/updates': 2270}
train stats after 72672 examples: {'rewards_train/chosen': '0.080434', 'rewards_train/rejected': '0.03865', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041784', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-132.39', 'loss/train': '0.67992', 'examples_per_second': '31.93', 'grad_norm': '28.75', 'counters/examples': 72672, 'counters/updates': 2271}
skipping logging after 72704 examples to avoid logging too frequently
train stats after 72736 examples: {'rewards_train/chosen': '0.060939', 'rewards_train/rejected': '0.019361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041578', 'logps_train/rejected': '-136.6', 'logps_train/chosen': '-130.39', 'loss/train': '0.67811', 'examples_per_second': '32.356', 'grad_norm': '32', 'counters/examples': 72736, 'counters/updates': 2273}
train stats after 72768 examples: {'rewards_train/chosen': '0.051387', 'rewards_train/rejected': '-0.0018039', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05319', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-135.89', 'loss/train': '0.67309', 'examples_per_second': '31.485', 'grad_norm': '36.5', 'counters/examples': 72768, 'counters/updates': 2274}
skipping logging after 72800 examples to avoid logging too frequently
train stats after 72832 examples: {'rewards_train/chosen': '0.0093972', 'rewards_train/rejected': '0.089016', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.079619', 'logps_train/rejected': '-112.66', 'logps_train/chosen': '-144.82', 'loss/train': '0.74447', 'examples_per_second': '30.559', 'grad_norm': '35', 'counters/examples': 72832, 'counters/updates': 2276}
skipping logging after 72864 examples to avoid logging too frequently
train stats after 72896 examples: {'rewards_train/chosen': '0.059442', 'rewards_train/rejected': '0.016154', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043288', 'logps_train/rejected': '-150.46', 'logps_train/chosen': '-145.96', 'loss/train': '0.6754', 'examples_per_second': '31.591', 'grad_norm': '35', 'counters/examples': 72896, 'counters/updates': 2278}
train stats after 72928 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.012044', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11585', 'logps_train/rejected': '-94.947', 'logps_train/chosen': '-135.87', 'loss/train': '0.64123', 'examples_per_second': '32.865', 'grad_norm': '50', 'counters/examples': 72928, 'counters/updates': 2279}
train stats after 72960 examples: {'rewards_train/chosen': '0.053204', 'rewards_train/rejected': '0.01309', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040114', 'logps_train/rejected': '-157.82', 'logps_train/chosen': '-109.86', 'loss/train': '0.6781', 'examples_per_second': '30.551', 'grad_norm': '29.375', 'counters/examples': 72960, 'counters/updates': 2280}
train stats after 72992 examples: {'rewards_train/chosen': '0.060985', 'rewards_train/rejected': '0.14117', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.080187', 'logps_train/rejected': '-186.84', 'logps_train/chosen': '-153.04', 'loss/train': '0.74082', 'examples_per_second': '32.429', 'grad_norm': '44', 'counters/examples': 72992, 'counters/updates': 2281}
train stats after 73024 examples: {'rewards_train/chosen': '0.13608', 'rewards_train/rejected': '0.072439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063637', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-141.24', 'loss/train': '0.67514', 'examples_per_second': '31.358', 'grad_norm': '33.75', 'counters/examples': 73024, 'counters/updates': 2282}
train stats after 73056 examples: {'rewards_train/chosen': '0.063805', 'rewards_train/rejected': '0.0094284', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054376', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-146.26', 'loss/train': '0.67702', 'examples_per_second': '33.203', 'grad_norm': '27.625', 'counters/examples': 73056, 'counters/updates': 2283}
train stats after 73088 examples: {'rewards_train/chosen': '0.12746', 'rewards_train/rejected': '0.096436', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031024', 'logps_train/rejected': '-154.08', 'logps_train/chosen': '-148.99', 'loss/train': '0.68296', 'examples_per_second': '30.454', 'grad_norm': '43.75', 'counters/examples': 73088, 'counters/updates': 2284}
train stats after 73120 examples: {'rewards_train/chosen': '0.15156', 'rewards_train/rejected': '0.041472', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11009', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-127.64', 'loss/train': '0.65186', 'examples_per_second': '31.268', 'grad_norm': '33.5', 'counters/examples': 73120, 'counters/updates': 2285}
train stats after 73152 examples: {'rewards_train/chosen': '0.13395', 'rewards_train/rejected': '0.15207', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.018125', 'logps_train/rejected': '-160.74', 'logps_train/chosen': '-184.66', 'loss/train': '0.71314', 'examples_per_second': '33.07', 'grad_norm': '40.75', 'counters/examples': 73152, 'counters/updates': 2286}
train stats after 73184 examples: {'rewards_train/chosen': '0.093213', 'rewards_train/rejected': '0.050109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043103', 'logps_train/rejected': '-111.82', 'logps_train/chosen': '-117.72', 'loss/train': '0.6765', 'examples_per_second': '32.535', 'grad_norm': '25', 'counters/examples': 73184, 'counters/updates': 2287}
train stats after 73216 examples: {'rewards_train/chosen': '0.088852', 'rewards_train/rejected': '0.018117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070735', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-135.81', 'loss/train': '0.67485', 'examples_per_second': '31.431', 'grad_norm': '32', 'counters/examples': 73216, 'counters/updates': 2288}
train stats after 73248 examples: {'rewards_train/chosen': '0.069196', 'rewards_train/rejected': '-0.0085307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077727', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-140.3', 'loss/train': '0.66072', 'examples_per_second': '31.274', 'grad_norm': '37.25', 'counters/examples': 73248, 'counters/updates': 2289}
skipping logging after 73280 examples to avoid logging too frequently
train stats after 73312 examples: {'rewards_train/chosen': '0.11611', 'rewards_train/rejected': '0.045616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070495', 'logps_train/rejected': '-133', 'logps_train/chosen': '-152.88', 'loss/train': '0.66767', 'examples_per_second': '37.313', 'grad_norm': '33.5', 'counters/examples': 73312, 'counters/updates': 2291}
skipping logging after 73344 examples to avoid logging too frequently
train stats after 73376 examples: {'rewards_train/chosen': '0.022156', 'rewards_train/rejected': '-0.015894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038049', 'logps_train/rejected': '-128.24', 'logps_train/chosen': '-137.89', 'loss/train': '0.68495', 'examples_per_second': '32.809', 'grad_norm': '29', 'counters/examples': 73376, 'counters/updates': 2293}
train stats after 73408 examples: {'rewards_train/chosen': '0.080881', 'rewards_train/rejected': '0.033492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047389', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-146.03', 'loss/train': '0.67515', 'examples_per_second': '30.355', 'grad_norm': '28.5', 'counters/examples': 73408, 'counters/updates': 2294}
skipping logging after 73440 examples to avoid logging too frequently
train stats after 73472 examples: {'rewards_train/chosen': '0.1479', 'rewards_train/rejected': '0.11914', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028764', 'logps_train/rejected': '-106.04', 'logps_train/chosen': '-121.62', 'loss/train': '0.68569', 'examples_per_second': '31.645', 'grad_norm': '34.5', 'counters/examples': 73472, 'counters/updates': 2296}
train stats after 73504 examples: {'rewards_train/chosen': '0.13496', 'rewards_train/rejected': '0.098512', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03645', 'logps_train/rejected': '-152.68', 'logps_train/chosen': '-146.02', 'loss/train': '0.68198', 'examples_per_second': '30.44', 'grad_norm': '41.75', 'counters/examples': 73504, 'counters/updates': 2297}
train stats after 73536 examples: {'rewards_train/chosen': '0.039741', 'rewards_train/rejected': '0.01112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028621', 'logps_train/rejected': '-94.259', 'logps_train/chosen': '-141.15', 'loss/train': '0.68229', 'examples_per_second': '32.326', 'grad_norm': '28.25', 'counters/examples': 73536, 'counters/updates': 2298}
train stats after 73568 examples: {'rewards_train/chosen': '0.043605', 'rewards_train/rejected': '0.014978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028627', 'logps_train/rejected': '-121.54', 'logps_train/chosen': '-162.74', 'loss/train': '0.68487', 'examples_per_second': '31.455', 'grad_norm': '33.75', 'counters/examples': 73568, 'counters/updates': 2299}
train stats after 73600 examples: {'rewards_train/chosen': '-0.0017054', 'rewards_train/rejected': '0.04388', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045585', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-169.41', 'loss/train': '0.72211', 'examples_per_second': '31.924', 'grad_norm': '41.5', 'counters/examples': 73600, 'counters/updates': 2300}
train stats after 73632 examples: {'rewards_train/chosen': '0.096874', 'rewards_train/rejected': '0.076688', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020185', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-142.78', 'loss/train': '0.6912', 'examples_per_second': '32.259', 'grad_norm': '30.25', 'counters/examples': 73632, 'counters/updates': 2301}
train stats after 73664 examples: {'rewards_train/chosen': '0.11983', 'rewards_train/rejected': '0.013811', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10602', 'logps_train/rejected': '-177.67', 'logps_train/chosen': '-160.17', 'loss/train': '0.65309', 'examples_per_second': '31.429', 'grad_norm': '31.125', 'counters/examples': 73664, 'counters/updates': 2302}
train stats after 73696 examples: {'rewards_train/chosen': '0.045115', 'rewards_train/rejected': '0.057938', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012823', 'logps_train/rejected': '-146', 'logps_train/chosen': '-132.84', 'loss/train': '0.70571', 'examples_per_second': '30.349', 'grad_norm': '45', 'counters/examples': 73696, 'counters/updates': 2303}
train stats after 73728 examples: {'rewards_train/chosen': '0.12428', 'rewards_train/rejected': '0.032652', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091632', 'logps_train/rejected': '-146.69', 'logps_train/chosen': '-137.22', 'loss/train': '0.65339', 'examples_per_second': '30.734', 'grad_norm': '29.5', 'counters/examples': 73728, 'counters/updates': 2304}
train stats after 73760 examples: {'rewards_train/chosen': '0.12296', 'rewards_train/rejected': '0.10849', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014471', 'logps_train/rejected': '-144.93', 'logps_train/chosen': '-144.91', 'loss/train': '0.69163', 'examples_per_second': '31.428', 'grad_norm': '36.5', 'counters/examples': 73760, 'counters/updates': 2305}
train stats after 73792 examples: {'rewards_train/chosen': '0.062641', 'rewards_train/rejected': '0.038539', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024102', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-125.9', 'loss/train': '0.6851', 'examples_per_second': '23.639', 'grad_norm': '29.625', 'counters/examples': 73792, 'counters/updates': 2306}
train stats after 73824 examples: {'rewards_train/chosen': '0.13707', 'rewards_train/rejected': '0.08009', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056978', 'logps_train/rejected': '-150.69', 'logps_train/chosen': '-160.54', 'loss/train': '0.6882', 'examples_per_second': '31.515', 'grad_norm': '40.25', 'counters/examples': 73824, 'counters/updates': 2307}
train stats after 73856 examples: {'rewards_train/chosen': '0.13754', 'rewards_train/rejected': '0.024355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11318', 'logps_train/rejected': '-103.57', 'logps_train/chosen': '-137.65', 'loss/train': '0.66092', 'examples_per_second': '32.019', 'grad_norm': '35.5', 'counters/examples': 73856, 'counters/updates': 2308}
train stats after 73888 examples: {'rewards_train/chosen': '0.16282', 'rewards_train/rejected': '0.02535', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13747', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-158.34', 'loss/train': '0.64655', 'examples_per_second': '23.609', 'grad_norm': '30.125', 'counters/examples': 73888, 'counters/updates': 2309}
train stats after 73920 examples: {'rewards_train/chosen': '0.063', 'rewards_train/rejected': '0.092076', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.029076', 'logps_train/rejected': '-122.3', 'logps_train/chosen': '-139.51', 'loss/train': '0.71434', 'examples_per_second': '31.949', 'grad_norm': '32.5', 'counters/examples': 73920, 'counters/updates': 2310}
train stats after 73952 examples: {'rewards_train/chosen': '0.084751', 'rewards_train/rejected': '0.11406', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.029312', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-164.08', 'loss/train': '0.71816', 'examples_per_second': '30.054', 'grad_norm': '31.625', 'counters/examples': 73952, 'counters/updates': 2311}
train stats after 73984 examples: {'rewards_train/chosen': '0.080672', 'rewards_train/rejected': '0.0079675', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072705', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-122.87', 'loss/train': '0.66194', 'examples_per_second': '30.372', 'grad_norm': '25.75', 'counters/examples': 73984, 'counters/updates': 2312}
skipping logging after 74016 examples to avoid logging too frequently
train stats after 74048 examples: {'rewards_train/chosen': '0.042843', 'rewards_train/rejected': '0.015716', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027127', 'logps_train/rejected': '-128.13', 'logps_train/chosen': '-139.2', 'loss/train': '0.68664', 'examples_per_second': '34.253', 'grad_norm': '33.25', 'counters/examples': 74048, 'counters/updates': 2314}
train stats after 74080 examples: {'rewards_train/chosen': '0.023458', 'rewards_train/rejected': '0.013863', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0095946', 'logps_train/rejected': '-103.56', 'logps_train/chosen': '-118.6', 'loss/train': '0.69465', 'examples_per_second': '31.323', 'grad_norm': '25', 'counters/examples': 74080, 'counters/updates': 2315}
train stats after 74112 examples: {'rewards_train/chosen': '0.10283', 'rewards_train/rejected': '0.1069', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0040676', 'logps_train/rejected': '-143.6', 'logps_train/chosen': '-151.78', 'loss/train': '0.70677', 'examples_per_second': '29.848', 'grad_norm': '41.5', 'counters/examples': 74112, 'counters/updates': 2316}
skipping logging after 74144 examples to avoid logging too frequently
train stats after 74176 examples: {'rewards_train/chosen': '0.065443', 'rewards_train/rejected': '0.042248', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023195', 'logps_train/rejected': '-90.221', 'logps_train/chosen': '-129.11', 'loss/train': '0.68784', 'examples_per_second': '30.997', 'grad_norm': '26', 'counters/examples': 74176, 'counters/updates': 2318}
train stats after 74208 examples: {'rewards_train/chosen': '0.02389', 'rewards_train/rejected': '0.054255', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.030365', 'logps_train/rejected': '-110.15', 'logps_train/chosen': '-131.46', 'loss/train': '0.71574', 'examples_per_second': '31.815', 'grad_norm': '39.5', 'counters/examples': 74208, 'counters/updates': 2319}
train stats after 74240 examples: {'rewards_train/chosen': '0.060645', 'rewards_train/rejected': '-0.008681', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069326', 'logps_train/rejected': '-103.9', 'logps_train/chosen': '-155.81', 'loss/train': '0.66557', 'examples_per_second': '31.526', 'grad_norm': '36.75', 'counters/examples': 74240, 'counters/updates': 2320}
skipping logging after 74272 examples to avoid logging too frequently
train stats after 74304 examples: {'rewards_train/chosen': '0.029295', 'rewards_train/rejected': '0.059393', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030098', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-144.77', 'loss/train': '0.71735', 'examples_per_second': '34.353', 'grad_norm': '40.25', 'counters/examples': 74304, 'counters/updates': 2322}
train stats after 74336 examples: {'rewards_train/chosen': '0.13005', 'rewards_train/rejected': '0.049623', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080432', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-157.3', 'loss/train': '0.66508', 'examples_per_second': '32.809', 'grad_norm': '32.5', 'counters/examples': 74336, 'counters/updates': 2323}
train stats after 74368 examples: {'rewards_train/chosen': '0.13423', 'rewards_train/rejected': '0.040939', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093286', 'logps_train/rejected': '-128.45', 'logps_train/chosen': '-177.3', 'loss/train': '0.65654', 'examples_per_second': '32.01', 'grad_norm': '33.75', 'counters/examples': 74368, 'counters/updates': 2324}
train stats after 74400 examples: {'rewards_train/chosen': '0.044684', 'rewards_train/rejected': '0.073127', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.028443', 'logps_train/rejected': '-111.43', 'logps_train/chosen': '-158.22', 'loss/train': '0.71282', 'examples_per_second': '30.363', 'grad_norm': '29.25', 'counters/examples': 74400, 'counters/updates': 2325}
train stats after 74432 examples: {'rewards_train/chosen': '0.079375', 'rewards_train/rejected': '0.1202', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.040823', 'logps_train/rejected': '-142.12', 'logps_train/chosen': '-121.25', 'loss/train': '0.72164', 'examples_per_second': '31.138', 'grad_norm': '37.5', 'counters/examples': 74432, 'counters/updates': 2326}
train stats after 74464 examples: {'rewards_train/chosen': '0.038587', 'rewards_train/rejected': '0.075598', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.037011', 'logps_train/rejected': '-143.12', 'logps_train/chosen': '-165.94', 'loss/train': '0.72649', 'examples_per_second': '31.775', 'grad_norm': '76', 'counters/examples': 74464, 'counters/updates': 2327}
train stats after 74496 examples: {'rewards_train/chosen': '0.03343', 'rewards_train/rejected': '0.046237', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012807', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-137', 'loss/train': '0.70526', 'examples_per_second': '31.52', 'grad_norm': '33.25', 'counters/examples': 74496, 'counters/updates': 2328}
train stats after 74528 examples: {'rewards_train/chosen': '0.075461', 'rewards_train/rejected': '-0.010656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086117', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-99.028', 'loss/train': '0.65555', 'examples_per_second': '30.275', 'grad_norm': '42.5', 'counters/examples': 74528, 'counters/updates': 2329}
train stats after 74560 examples: {'rewards_train/chosen': '0.067557', 'rewards_train/rejected': '0.054734', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012823', 'logps_train/rejected': '-98.14', 'logps_train/chosen': '-139.1', 'loss/train': '0.69309', 'examples_per_second': '31.572', 'grad_norm': '44.25', 'counters/examples': 74560, 'counters/updates': 2330}
train stats after 74592 examples: {'rewards_train/chosen': '0.053902', 'rewards_train/rejected': '0.010986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042915', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-135.26', 'loss/train': '0.68154', 'examples_per_second': '32.958', 'grad_norm': '51.5', 'counters/examples': 74592, 'counters/updates': 2331}
train stats after 74624 examples: {'rewards_train/chosen': '0.069875', 'rewards_train/rejected': '0.078117', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0082421', 'logps_train/rejected': '-113.96', 'logps_train/chosen': '-128.21', 'loss/train': '0.70813', 'examples_per_second': '30.83', 'grad_norm': '35.75', 'counters/examples': 74624, 'counters/updates': 2332}
train stats after 74656 examples: {'rewards_train/chosen': '0.072229', 'rewards_train/rejected': '0.11592', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043696', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-143.47', 'loss/train': '0.72715', 'examples_per_second': '31.434', 'grad_norm': '40.75', 'counters/examples': 74656, 'counters/updates': 2333}
skipping logging after 74688 examples to avoid logging too frequently
train stats after 74720 examples: {'rewards_train/chosen': '0.092417', 'rewards_train/rejected': '-0.011945', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10436', 'logps_train/rejected': '-123.07', 'logps_train/chosen': '-160.18', 'loss/train': '0.65387', 'examples_per_second': '31.444', 'grad_norm': '26.625', 'counters/examples': 74720, 'counters/updates': 2335}
train stats after 74752 examples: {'rewards_train/chosen': '0.085747', 'rewards_train/rejected': '-0.002659', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088406', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-154.17', 'loss/train': '0.65683', 'examples_per_second': '31.193', 'grad_norm': '32', 'counters/examples': 74752, 'counters/updates': 2336}
train stats after 74784 examples: {'rewards_train/chosen': '0.12054', 'rewards_train/rejected': '0.089609', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030934', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-136.44', 'loss/train': '0.68646', 'examples_per_second': '31.259', 'grad_norm': '33.5', 'counters/examples': 74784, 'counters/updates': 2337}
train stats after 74816 examples: {'rewards_train/chosen': '0.13157', 'rewards_train/rejected': '0.053057', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078511', 'logps_train/rejected': '-127.38', 'logps_train/chosen': '-125.71', 'loss/train': '0.66297', 'examples_per_second': '31.349', 'grad_norm': '29.25', 'counters/examples': 74816, 'counters/updates': 2338}
train stats after 74848 examples: {'rewards_train/chosen': '0.080474', 'rewards_train/rejected': '0.012714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06776', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-149.86', 'loss/train': '0.66796', 'examples_per_second': '30.551', 'grad_norm': '33.25', 'counters/examples': 74848, 'counters/updates': 2339}
train stats after 74880 examples: {'rewards_train/chosen': '0.091208', 'rewards_train/rejected': '0.040016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051191', 'logps_train/rejected': '-79.444', 'logps_train/chosen': '-128.18', 'loss/train': '0.67058', 'examples_per_second': '31.462', 'grad_norm': '23.125', 'counters/examples': 74880, 'counters/updates': 2340}
train stats after 74912 examples: {'rewards_train/chosen': '0.067219', 'rewards_train/rejected': '0.034035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033184', 'logps_train/rejected': '-138.12', 'logps_train/chosen': '-127.42', 'loss/train': '0.68213', 'examples_per_second': '31.482', 'grad_norm': '57.75', 'counters/examples': 74912, 'counters/updates': 2341}
train stats after 74944 examples: {'rewards_train/chosen': '0.083762', 'rewards_train/rejected': '0.063503', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020259', 'logps_train/rejected': '-160.28', 'logps_train/chosen': '-138.82', 'loss/train': '0.68868', 'examples_per_second': '33.151', 'grad_norm': '31', 'counters/examples': 74944, 'counters/updates': 2342}
train stats after 74976 examples: {'rewards_train/chosen': '0.14145', 'rewards_train/rejected': '0.06274', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078714', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-146.54', 'loss/train': '0.66397', 'examples_per_second': '32.544', 'grad_norm': '50.75', 'counters/examples': 74976, 'counters/updates': 2343}
train stats after 75008 examples: {'rewards_train/chosen': '0.072151', 'rewards_train/rejected': '0.026487', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045663', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-138.36', 'loss/train': '0.67773', 'examples_per_second': '31.493', 'grad_norm': '28', 'counters/examples': 75008, 'counters/updates': 2344}
skipping logging after 75040 examples to avoid logging too frequently
train stats after 75072 examples: {'rewards_train/chosen': '0.066012', 'rewards_train/rejected': '0.066076', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-6.4197e-05', 'logps_train/rejected': '-170.37', 'logps_train/chosen': '-161.64', 'loss/train': '0.70084', 'examples_per_second': '31.172', 'grad_norm': '34.25', 'counters/examples': 75072, 'counters/updates': 2346}
train stats after 75104 examples: {'rewards_train/chosen': '0.14767', 'rewards_train/rejected': '0.10992', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037749', 'logps_train/rejected': '-97.905', 'logps_train/chosen': '-111.96', 'loss/train': '0.68499', 'examples_per_second': '30.835', 'grad_norm': '35.75', 'counters/examples': 75104, 'counters/updates': 2347}
train stats after 75136 examples: {'rewards_train/chosen': '0.015136', 'rewards_train/rejected': '0.059502', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044366', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-131.05', 'loss/train': '0.72147', 'examples_per_second': '30.099', 'grad_norm': '33', 'counters/examples': 75136, 'counters/updates': 2348}
train stats after 75168 examples: {'rewards_train/chosen': '0.1604', 'rewards_train/rejected': '0.026588', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13381', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-146.56', 'loss/train': '0.66818', 'examples_per_second': '30.482', 'grad_norm': '34.75', 'counters/examples': 75168, 'counters/updates': 2349}
skipping logging after 75200 examples to avoid logging too frequently
train stats after 75232 examples: {'rewards_train/chosen': '0.057408', 'rewards_train/rejected': '0.01612', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041289', 'logps_train/rejected': '-112.35', 'logps_train/chosen': '-169.93', 'loss/train': '0.67675', 'examples_per_second': '31.14', 'grad_norm': '35.75', 'counters/examples': 75232, 'counters/updates': 2351}
train stats after 75264 examples: {'rewards_train/chosen': '0.049921', 'rewards_train/rejected': '-0.018757', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068679', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-144.2', 'loss/train': '0.66641', 'examples_per_second': '30.482', 'grad_norm': '37', 'counters/examples': 75264, 'counters/updates': 2352}
skipping logging after 75296 examples to avoid logging too frequently
train stats after 75328 examples: {'rewards_train/chosen': '0.084493', 'rewards_train/rejected': '0.059902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024591', 'logps_train/rejected': '-135.63', 'logps_train/chosen': '-163.42', 'loss/train': '0.6894', 'examples_per_second': '24.189', 'grad_norm': '29.375', 'counters/examples': 75328, 'counters/updates': 2354}
train stats after 75360 examples: {'rewards_train/chosen': '0.088582', 'rewards_train/rejected': '0.02586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062722', 'logps_train/rejected': '-99.601', 'logps_train/chosen': '-129.76', 'loss/train': '0.66829', 'examples_per_second': '32.516', 'grad_norm': '28.375', 'counters/examples': 75360, 'counters/updates': 2355}
train stats after 75392 examples: {'rewards_train/chosen': '0.083843', 'rewards_train/rejected': '0.081399', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0024448', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-122.49', 'loss/train': '0.70386', 'examples_per_second': '29.969', 'grad_norm': '34.25', 'counters/examples': 75392, 'counters/updates': 2356}
skipping logging after 75424 examples to avoid logging too frequently
train stats after 75456 examples: {'rewards_train/chosen': '0.1071', 'rewards_train/rejected': '0.07797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029132', 'logps_train/rejected': '-144.21', 'logps_train/chosen': '-161.31', 'loss/train': '0.68649', 'examples_per_second': '35.76', 'grad_norm': '45.25', 'counters/examples': 75456, 'counters/updates': 2358}
train stats after 75488 examples: {'rewards_train/chosen': '0.10092', 'rewards_train/rejected': '0.081424', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019498', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-151.7', 'loss/train': '0.68747', 'examples_per_second': '30.466', 'grad_norm': '29.375', 'counters/examples': 75488, 'counters/updates': 2359}
train stats after 75520 examples: {'rewards_train/chosen': '0.10349', 'rewards_train/rejected': '0.044203', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059292', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-182.93', 'loss/train': '0.67279', 'examples_per_second': '31.419', 'grad_norm': '32.75', 'counters/examples': 75520, 'counters/updates': 2360}
train stats after 75552 examples: {'rewards_train/chosen': '0.028505', 'rewards_train/rejected': '0.051339', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022833', 'logps_train/rejected': '-144.6', 'logps_train/chosen': '-121.82', 'loss/train': '0.71016', 'examples_per_second': '30.465', 'grad_norm': '34.5', 'counters/examples': 75552, 'counters/updates': 2361}
train stats after 75584 examples: {'rewards_train/chosen': '0.12234', 'rewards_train/rejected': '0.059571', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.062768', 'logps_train/rejected': '-118.97', 'logps_train/chosen': '-121.54', 'loss/train': '0.66738', 'examples_per_second': '29.998', 'grad_norm': '31', 'counters/examples': 75584, 'counters/updates': 2362}
train stats after 75616 examples: {'rewards_train/chosen': '0.059267', 'rewards_train/rejected': '0.05872', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00054692', 'logps_train/rejected': '-106.56', 'logps_train/chosen': '-126.4', 'loss/train': '0.69817', 'examples_per_second': '32.733', 'grad_norm': '26', 'counters/examples': 75616, 'counters/updates': 2363}
train stats after 75648 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.06369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037906', 'logps_train/rejected': '-150.75', 'logps_train/chosen': '-167.38', 'loss/train': '0.68451', 'examples_per_second': '31.548', 'grad_norm': '31.125', 'counters/examples': 75648, 'counters/updates': 2364}
train stats after 75680 examples: {'rewards_train/chosen': '0.064503', 'rewards_train/rejected': '0.022695', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041808', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-129.05', 'loss/train': '0.68211', 'examples_per_second': '31.494', 'grad_norm': '33.25', 'counters/examples': 75680, 'counters/updates': 2365}
skipping logging after 75712 examples to avoid logging too frequently
train stats after 75744 examples: {'rewards_train/chosen': '0.12862', 'rewards_train/rejected': '0.062818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065802', 'logps_train/rejected': '-127.41', 'logps_train/chosen': '-140.19', 'loss/train': '0.66787', 'examples_per_second': '34.244', 'grad_norm': '29', 'counters/examples': 75744, 'counters/updates': 2367}
train stats after 75776 examples: {'rewards_train/chosen': '0.092243', 'rewards_train/rejected': '0.076729', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.015514', 'logps_train/rejected': '-105.96', 'logps_train/chosen': '-152.87', 'loss/train': '0.69785', 'examples_per_second': '30.167', 'grad_norm': '37.75', 'counters/examples': 75776, 'counters/updates': 2368}
train stats after 75808 examples: {'rewards_train/chosen': '0.13385', 'rewards_train/rejected': '0.027439', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10641', 'logps_train/rejected': '-116.64', 'logps_train/chosen': '-179.44', 'loss/train': '0.65449', 'examples_per_second': '31.358', 'grad_norm': '30.25', 'counters/examples': 75808, 'counters/updates': 2369}
train stats after 75840 examples: {'rewards_train/chosen': '0.13077', 'rewards_train/rejected': '0.19035', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059578', 'logps_train/rejected': '-167.12', 'logps_train/chosen': '-137.04', 'loss/train': '0.73387', 'examples_per_second': '30.991', 'grad_norm': '62.5', 'counters/examples': 75840, 'counters/updates': 2370}
train stats after 75872 examples: {'rewards_train/chosen': '0.0067365', 'rewards_train/rejected': '0.02431', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017573', 'logps_train/rejected': '-95.794', 'logps_train/chosen': '-128.45', 'loss/train': '0.70829', 'examples_per_second': '31.363', 'grad_norm': '29.625', 'counters/examples': 75872, 'counters/updates': 2371}
train stats after 75904 examples: {'rewards_train/chosen': '0.11275', 'rewards_train/rejected': '0.061481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051274', 'logps_train/rejected': '-120.2', 'logps_train/chosen': '-152.08', 'loss/train': '0.6839', 'examples_per_second': '31.322', 'grad_norm': '39', 'counters/examples': 75904, 'counters/updates': 2372}
train stats after 75936 examples: {'rewards_train/chosen': '0.061591', 'rewards_train/rejected': '0.036951', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02464', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-145.89', 'loss/train': '0.6877', 'examples_per_second': '31.279', 'grad_norm': '30', 'counters/examples': 75936, 'counters/updates': 2373}
train stats after 75968 examples: {'rewards_train/chosen': '0.070042', 'rewards_train/rejected': '0.079284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0092417', 'logps_train/rejected': '-126.53', 'logps_train/chosen': '-136.69', 'loss/train': '0.70385', 'examples_per_second': '31.418', 'grad_norm': '32.25', 'counters/examples': 75968, 'counters/updates': 2374}
train stats after 76000 examples: {'rewards_train/chosen': '0.11198', 'rewards_train/rejected': '0.13658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.024597', 'logps_train/rejected': '-132', 'logps_train/chosen': '-121.69', 'loss/train': '0.72924', 'examples_per_second': '31.236', 'grad_norm': '68.5', 'counters/examples': 76000, 'counters/updates': 2375}
train stats after 76032 examples: {'rewards_train/chosen': '0.02797', 'rewards_train/rejected': '0.0015017', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026468', 'logps_train/rejected': '-161.54', 'logps_train/chosen': '-187.55', 'loss/train': '0.68686', 'examples_per_second': '32.769', 'grad_norm': '37.5', 'counters/examples': 76032, 'counters/updates': 2376}
train stats after 76064 examples: {'rewards_train/chosen': '0.16338', 'rewards_train/rejected': '0.11811', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045265', 'logps_train/rejected': '-156.19', 'logps_train/chosen': '-159.41', 'loss/train': '0.67686', 'examples_per_second': '30.066', 'grad_norm': '34.75', 'counters/examples': 76064, 'counters/updates': 2377}
train stats after 76096 examples: {'rewards_train/chosen': '0.087763', 'rewards_train/rejected': '0.066942', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020821', 'logps_train/rejected': '-108.49', 'logps_train/chosen': '-121.19', 'loss/train': '0.68697', 'examples_per_second': '31.413', 'grad_norm': '33.75', 'counters/examples': 76096, 'counters/updates': 2378}
skipping logging after 76128 examples to avoid logging too frequently
train stats after 76160 examples: {'rewards_train/chosen': '0.053352', 'rewards_train/rejected': '0.069736', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016385', 'logps_train/rejected': '-107.8', 'logps_train/chosen': '-118.45', 'loss/train': '0.70878', 'examples_per_second': '34.635', 'grad_norm': '33.25', 'counters/examples': 76160, 'counters/updates': 2380}
skipping logging after 76192 examples to avoid logging too frequently
train stats after 76224 examples: {'rewards_train/chosen': '-0.0064016', 'rewards_train/rejected': '0.071562', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.077964', 'logps_train/rejected': '-175.21', 'logps_train/chosen': '-168.05', 'loss/train': '0.74324', 'examples_per_second': '31.381', 'grad_norm': '38', 'counters/examples': 76224, 'counters/updates': 2382}
skipping logging after 76256 examples to avoid logging too frequently
train stats after 76288 examples: {'rewards_train/chosen': '0.089948', 'rewards_train/rejected': '0.10481', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.014865', 'logps_train/rejected': '-113.53', 'logps_train/chosen': '-119.68', 'loss/train': '0.7109', 'examples_per_second': '31.669', 'grad_norm': '34.75', 'counters/examples': 76288, 'counters/updates': 2384}
train stats after 76320 examples: {'rewards_train/chosen': '0.043158', 'rewards_train/rejected': '0.030761', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.012396', 'logps_train/rejected': '-115.92', 'logps_train/chosen': '-147.85', 'loss/train': '0.69274', 'examples_per_second': '31.116', 'grad_norm': '32.25', 'counters/examples': 76320, 'counters/updates': 2385}
train stats after 76352 examples: {'rewards_train/chosen': '0.10404', 'rewards_train/rejected': '0.025206', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078835', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-188.75', 'loss/train': '0.66105', 'examples_per_second': '30.222', 'grad_norm': '41.25', 'counters/examples': 76352, 'counters/updates': 2386}
train stats after 76384 examples: {'rewards_train/chosen': '0.057479', 'rewards_train/rejected': '0.036244', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021235', 'logps_train/rejected': '-124.24', 'logps_train/chosen': '-123.61', 'loss/train': '0.68946', 'examples_per_second': '31.967', 'grad_norm': '36', 'counters/examples': 76384, 'counters/updates': 2387}
train stats after 76416 examples: {'rewards_train/chosen': '0.14411', 'rewards_train/rejected': '0.057005', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087108', 'logps_train/rejected': '-147.1', 'logps_train/chosen': '-161.88', 'loss/train': '0.6593', 'examples_per_second': '31.476', 'grad_norm': '48.5', 'counters/examples': 76416, 'counters/updates': 2388}
train stats after 76448 examples: {'rewards_train/chosen': '0.17875', 'rewards_train/rejected': '0.14838', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030371', 'logps_train/rejected': '-135.52', 'logps_train/chosen': '-148.86', 'loss/train': '0.69484', 'examples_per_second': '31.068', 'grad_norm': '33.5', 'counters/examples': 76448, 'counters/updates': 2389}
train stats after 76480 examples: {'rewards_train/chosen': '0.12365', 'rewards_train/rejected': '0.085283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038363', 'logps_train/rejected': '-114.18', 'logps_train/chosen': '-147.25', 'loss/train': '0.68036', 'examples_per_second': '30.291', 'grad_norm': '32.5', 'counters/examples': 76480, 'counters/updates': 2390}
train stats after 76512 examples: {'rewards_train/chosen': '0.10579', 'rewards_train/rejected': '0.046218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059574', 'logps_train/rejected': '-109.81', 'logps_train/chosen': '-141.77', 'loss/train': '0.67108', 'examples_per_second': '30.179', 'grad_norm': '34.25', 'counters/examples': 76512, 'counters/updates': 2391}
train stats after 76544 examples: {'rewards_train/chosen': '0.043573', 'rewards_train/rejected': '0.014268', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.029304', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-137.88', 'loss/train': '0.68588', 'examples_per_second': '31.457', 'grad_norm': '28.75', 'counters/examples': 76544, 'counters/updates': 2392}
skipping logging after 76576 examples to avoid logging too frequently
train stats after 76608 examples: {'rewards_train/chosen': '0.098308', 'rewards_train/rejected': '0.038412', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.059896', 'logps_train/rejected': '-145.67', 'logps_train/chosen': '-151.93', 'loss/train': '0.67282', 'examples_per_second': '35.867', 'grad_norm': '33', 'counters/examples': 76608, 'counters/updates': 2394}
train stats after 76640 examples: {'rewards_train/chosen': '0.12863', 'rewards_train/rejected': '0.08021', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048421', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-110.4', 'loss/train': '0.68203', 'examples_per_second': '29.813', 'grad_norm': '33.25', 'counters/examples': 76640, 'counters/updates': 2395}
train stats after 76672 examples: {'rewards_train/chosen': '0.1554', 'rewards_train/rejected': '0.040852', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11455', 'logps_train/rejected': '-149.5', 'logps_train/chosen': '-168.85', 'loss/train': '0.65069', 'examples_per_second': '31.427', 'grad_norm': '32.75', 'counters/examples': 76672, 'counters/updates': 2396}
train stats after 76704 examples: {'rewards_train/chosen': '0.097919', 'rewards_train/rejected': '0.04767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050249', 'logps_train/rejected': '-123.45', 'logps_train/chosen': '-161.01', 'loss/train': '0.6758', 'examples_per_second': '31.377', 'grad_norm': '31', 'counters/examples': 76704, 'counters/updates': 2397}
train stats after 76736 examples: {'rewards_train/chosen': '0.11096', 'rewards_train/rejected': '0.051787', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059171', 'logps_train/rejected': '-105.46', 'logps_train/chosen': '-114.49', 'loss/train': '0.66963', 'examples_per_second': '32.048', 'grad_norm': '25.625', 'counters/examples': 76736, 'counters/updates': 2398}
train stats after 76768 examples: {'rewards_train/chosen': '0.043641', 'rewards_train/rejected': '0.050248', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0066072', 'logps_train/rejected': '-138.23', 'logps_train/chosen': '-145.35', 'loss/train': '0.70242', 'examples_per_second': '31.456', 'grad_norm': '39.75', 'counters/examples': 76768, 'counters/updates': 2399}
train stats after 76800 examples: {'rewards_train/chosen': '0.11558', 'rewards_train/rejected': '0.050999', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064585', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-143.05', 'loss/train': '0.66947', 'examples_per_second': '31.488', 'grad_norm': '30.75', 'counters/examples': 76800, 'counters/updates': 2400}
skipping logging after 76832 examples to avoid logging too frequently
train stats after 76864 examples: {'rewards_train/chosen': '0.12898', 'rewards_train/rejected': '0.146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.017018', 'logps_train/rejected': '-124.18', 'logps_train/chosen': '-128.16', 'loss/train': '0.74212', 'examples_per_second': '31.175', 'grad_norm': '39', 'counters/examples': 76864, 'counters/updates': 2402}
train stats after 76896 examples: {'rewards_train/chosen': '0.079756', 'rewards_train/rejected': '-0.10005', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1798', 'logps_train/rejected': '-139.93', 'logps_train/chosen': '-163.72', 'loss/train': '0.65444', 'examples_per_second': '31.273', 'grad_norm': '33', 'counters/examples': 76896, 'counters/updates': 2403}
skipping logging after 76928 examples to avoid logging too frequently
train stats after 76960 examples: {'rewards_train/chosen': '0.1367', 'rewards_train/rejected': '0.062861', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073834', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-163.31', 'loss/train': '0.67014', 'examples_per_second': '30.053', 'grad_norm': '31.375', 'counters/examples': 76960, 'counters/updates': 2405}
train stats after 76992 examples: {'rewards_train/chosen': '0.073882', 'rewards_train/rejected': '0.0093334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064548', 'logps_train/rejected': '-104.97', 'logps_train/chosen': '-138.28', 'loss/train': '0.66596', 'examples_per_second': '32.52', 'grad_norm': '36.25', 'counters/examples': 76992, 'counters/updates': 2406}
train stats after 77024 examples: {'rewards_train/chosen': '0.11939', 'rewards_train/rejected': '0.0026223', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11676', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-148.73', 'loss/train': '0.65375', 'examples_per_second': '32.872', 'grad_norm': '31', 'counters/examples': 77024, 'counters/updates': 2407}
train stats after 77056 examples: {'rewards_train/chosen': '0.062579', 'rewards_train/rejected': '0.066691', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0041122', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-130.32', 'loss/train': '0.70559', 'examples_per_second': '31.18', 'grad_norm': '35', 'counters/examples': 77056, 'counters/updates': 2408}
train stats after 77088 examples: {'rewards_train/chosen': '0.070688', 'rewards_train/rejected': '0.0057445', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064943', 'logps_train/rejected': '-122.7', 'logps_train/chosen': '-149.79', 'loss/train': '0.66759', 'examples_per_second': '32.511', 'grad_norm': '37.25', 'counters/examples': 77088, 'counters/updates': 2409}
skipping logging after 77120 examples to avoid logging too frequently
train stats after 77152 examples: {'rewards_train/chosen': '0.10241', 'rewards_train/rejected': '0.058505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04391', 'logps_train/rejected': '-119.4', 'logps_train/chosen': '-154', 'loss/train': '0.694', 'examples_per_second': '31.039', 'grad_norm': '35.75', 'counters/examples': 77152, 'counters/updates': 2411}
train stats after 77184 examples: {'rewards_train/chosen': '0.10835', 'rewards_train/rejected': '-0.011426', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11978', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-169.14', 'loss/train': '0.64454', 'examples_per_second': '30.899', 'grad_norm': '33.25', 'counters/examples': 77184, 'counters/updates': 2412}
train stats after 77216 examples: {'rewards_train/chosen': '0.20554', 'rewards_train/rejected': '0.060161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14538', 'logps_train/rejected': '-140.68', 'logps_train/chosen': '-149.81', 'loss/train': '0.64906', 'examples_per_second': '31.463', 'grad_norm': '31.25', 'counters/examples': 77216, 'counters/updates': 2413}
train stats after 77248 examples: {'rewards_train/chosen': '0.079364', 'rewards_train/rejected': '0.073444', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0059198', 'logps_train/rejected': '-133.19', 'logps_train/chosen': '-161.98', 'loss/train': '0.6971', 'examples_per_second': '29.993', 'grad_norm': '33.75', 'counters/examples': 77248, 'counters/updates': 2414}
train stats after 77280 examples: {'rewards_train/chosen': '0.055193', 'rewards_train/rejected': '0.00072068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054472', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-120.63', 'loss/train': '0.67599', 'examples_per_second': '31.522', 'grad_norm': '32.25', 'counters/examples': 77280, 'counters/updates': 2415}
train stats after 77312 examples: {'rewards_train/chosen': '0.13095', 'rewards_train/rejected': '0.060118', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.070831', 'logps_train/rejected': '-161.56', 'logps_train/chosen': '-158.03', 'loss/train': '0.66495', 'examples_per_second': '30.094', 'grad_norm': '34.75', 'counters/examples': 77312, 'counters/updates': 2416}
train stats after 77344 examples: {'rewards_train/chosen': '0.122', 'rewards_train/rejected': '0.056704', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065299', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-133.41', 'loss/train': '0.67047', 'examples_per_second': '31.514', 'grad_norm': '45.25', 'counters/examples': 77344, 'counters/updates': 2417}
train stats after 77376 examples: {'rewards_train/chosen': '0.11587', 'rewards_train/rejected': '0.052142', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063724', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-127.46', 'loss/train': '0.66522', 'examples_per_second': '31.5', 'grad_norm': '28.25', 'counters/examples': 77376, 'counters/updates': 2418}
train stats after 77408 examples: {'rewards_train/chosen': '0.058719', 'rewards_train/rejected': '-0.0066668', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065385', 'logps_train/rejected': '-107.3', 'logps_train/chosen': '-125.81', 'loss/train': '0.66881', 'examples_per_second': '31.541', 'grad_norm': '31.625', 'counters/examples': 77408, 'counters/updates': 2419}
skipping logging after 77440 examples to avoid logging too frequently
train stats after 77472 examples: {'rewards_train/chosen': '0.087518', 'rewards_train/rejected': '0.066111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021407', 'logps_train/rejected': '-130.63', 'logps_train/chosen': '-111.87', 'loss/train': '0.68848', 'examples_per_second': '32.586', 'grad_norm': '26.375', 'counters/examples': 77472, 'counters/updates': 2421}
train stats after 77504 examples: {'rewards_train/chosen': '0.055889', 'rewards_train/rejected': '0.0094257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046463', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-121.16', 'loss/train': '0.6746', 'examples_per_second': '31.211', 'grad_norm': '28.75', 'counters/examples': 77504, 'counters/updates': 2422}
skipping logging after 77536 examples to avoid logging too frequently
train stats after 77568 examples: {'rewards_train/chosen': '0.072876', 'rewards_train/rejected': '0.018859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054017', 'logps_train/rejected': '-121.55', 'logps_train/chosen': '-129.95', 'loss/train': '0.67001', 'examples_per_second': '30.061', 'grad_norm': '25.5', 'counters/examples': 77568, 'counters/updates': 2424}
train stats after 77600 examples: {'rewards_train/chosen': '0.049854', 'rewards_train/rejected': '0.0035561', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.046298', 'logps_train/rejected': '-128.23', 'logps_train/chosen': '-147.99', 'loss/train': '0.67384', 'examples_per_second': '31.018', 'grad_norm': '29.375', 'counters/examples': 77600, 'counters/updates': 2425}
train stats after 77632 examples: {'rewards_train/chosen': '0.064057', 'rewards_train/rejected': '0.056952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.007105', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-135.98', 'loss/train': '0.69192', 'examples_per_second': '30.492', 'grad_norm': '45', 'counters/examples': 77632, 'counters/updates': 2426}
train stats after 77664 examples: {'rewards_train/chosen': '0.096041', 'rewards_train/rejected': '0.070881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025159', 'logps_train/rejected': '-111.34', 'logps_train/chosen': '-137.21', 'loss/train': '0.68533', 'examples_per_second': '31.688', 'grad_norm': '26.125', 'counters/examples': 77664, 'counters/updates': 2427}
train stats after 77696 examples: {'rewards_train/chosen': '0.062101', 'rewards_train/rejected': '0.048619', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.013482', 'logps_train/rejected': '-147.45', 'logps_train/chosen': '-155.32', 'loss/train': '0.69391', 'examples_per_second': '31.49', 'grad_norm': '32.75', 'counters/examples': 77696, 'counters/updates': 2428}
train stats after 77728 examples: {'rewards_train/chosen': '0.053001', 'rewards_train/rejected': '0.1097', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0567', 'logps_train/rejected': '-150.82', 'logps_train/chosen': '-135.43', 'loss/train': '0.73829', 'examples_per_second': '32.367', 'grad_norm': '48', 'counters/examples': 77728, 'counters/updates': 2429}
train stats after 77760 examples: {'rewards_train/chosen': '0.091471', 'rewards_train/rejected': '0.10063', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0091593', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-168.18', 'loss/train': '0.70708', 'examples_per_second': '31.423', 'grad_norm': '35.5', 'counters/examples': 77760, 'counters/updates': 2430}
skipping logging after 77792 examples to avoid logging too frequently
train stats after 77824 examples: {'rewards_train/chosen': '0.11361', 'rewards_train/rejected': '0.027797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08581', 'logps_train/rejected': '-143.79', 'logps_train/chosen': '-169.76', 'loss/train': '0.66057', 'examples_per_second': '29.91', 'grad_norm': '34.75', 'counters/examples': 77824, 'counters/updates': 2432}
train stats after 77856 examples: {'rewards_train/chosen': '0.11466', 'rewards_train/rejected': '0.097301', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.017357', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-136.4', 'loss/train': '0.69473', 'examples_per_second': '30.468', 'grad_norm': '34.5', 'counters/examples': 77856, 'counters/updates': 2433}
train stats after 77888 examples: {'rewards_train/chosen': '0.13902', 'rewards_train/rejected': '0.087411', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05161', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-132.53', 'loss/train': '0.67753', 'examples_per_second': '30.385', 'grad_norm': '31.125', 'counters/examples': 77888, 'counters/updates': 2434}
train stats after 77920 examples: {'rewards_train/chosen': '0.15372', 'rewards_train/rejected': '0.065355', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088368', 'logps_train/rejected': '-119.69', 'logps_train/chosen': '-157', 'loss/train': '0.66368', 'examples_per_second': '32.14', 'grad_norm': '33.75', 'counters/examples': 77920, 'counters/updates': 2435}
train stats after 77952 examples: {'rewards_train/chosen': '0.087296', 'rewards_train/rejected': '0.084138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.003158', 'logps_train/rejected': '-117.02', 'logps_train/chosen': '-141.06', 'loss/train': '0.70137', 'examples_per_second': '32.015', 'grad_norm': '27.25', 'counters/examples': 77952, 'counters/updates': 2436}
train stats after 77984 examples: {'rewards_train/chosen': '0.042739', 'rewards_train/rejected': '0.028755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013984', 'logps_train/rejected': '-100.93', 'logps_train/chosen': '-112.51', 'loss/train': '0.69135', 'examples_per_second': '30.12', 'grad_norm': '27', 'counters/examples': 77984, 'counters/updates': 2437}
train stats after 78016 examples: {'rewards_train/chosen': '0.081393', 'rewards_train/rejected': '-0.00092733', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08232', 'logps_train/rejected': '-139.81', 'logps_train/chosen': '-164.83', 'loss/train': '0.65843', 'examples_per_second': '30.951', 'grad_norm': '27', 'counters/examples': 78016, 'counters/updates': 2438}
train stats after 78048 examples: {'rewards_train/chosen': '0.1454', 'rewards_train/rejected': '0.10758', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037822', 'logps_train/rejected': '-105.69', 'logps_train/chosen': '-128.33', 'loss/train': '0.68757', 'examples_per_second': '31.369', 'grad_norm': '34', 'counters/examples': 78048, 'counters/updates': 2439}
train stats after 78080 examples: {'rewards_train/chosen': '0.16053', 'rewards_train/rejected': '0.066034', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094499', 'logps_train/rejected': '-168.13', 'logps_train/chosen': '-145.97', 'loss/train': '0.6526', 'examples_per_second': '31.2', 'grad_norm': '41.75', 'counters/examples': 78080, 'counters/updates': 2440}
skipping logging after 78112 examples to avoid logging too frequently
train stats after 78144 examples: {'rewards_train/chosen': '0.091397', 'rewards_train/rejected': '0.085289', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.006108', 'logps_train/rejected': '-127.37', 'logps_train/chosen': '-166.98', 'loss/train': '0.70292', 'examples_per_second': '36.176', 'grad_norm': '35.75', 'counters/examples': 78144, 'counters/updates': 2442}
train stats after 78176 examples: {'rewards_train/chosen': '0.10534', 'rewards_train/rejected': '0.1312', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.025862', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-151.37', 'loss/train': '0.72117', 'examples_per_second': '30.018', 'grad_norm': '53.5', 'counters/examples': 78176, 'counters/updates': 2443}
train stats after 78208 examples: {'rewards_train/chosen': '0.058867', 'rewards_train/rejected': '0.038673', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020193', 'logps_train/rejected': '-147.96', 'logps_train/chosen': '-147.76', 'loss/train': '0.68813', 'examples_per_second': '31.215', 'grad_norm': '35.5', 'counters/examples': 78208, 'counters/updates': 2444}
train stats after 78240 examples: {'rewards_train/chosen': '0.081856', 'rewards_train/rejected': '0.081297', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.00055937', 'logps_train/rejected': '-126.6', 'logps_train/chosen': '-137.33', 'loss/train': '0.70489', 'examples_per_second': '29.93', 'grad_norm': '33.25', 'counters/examples': 78240, 'counters/updates': 2445}
train stats after 78272 examples: {'rewards_train/chosen': '0.1267', 'rewards_train/rejected': '0.02255', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10415', 'logps_train/rejected': '-103.93', 'logps_train/chosen': '-159.82', 'loss/train': '0.6547', 'examples_per_second': '32.536', 'grad_norm': '46.5', 'counters/examples': 78272, 'counters/updates': 2446}
skipping logging after 78304 examples to avoid logging too frequently
train stats after 78336 examples: {'rewards_train/chosen': '0.10707', 'rewards_train/rejected': '0.055613', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051458', 'logps_train/rejected': '-137.2', 'logps_train/chosen': '-152.93', 'loss/train': '0.67209', 'examples_per_second': '31.532', 'grad_norm': '30.125', 'counters/examples': 78336, 'counters/updates': 2448}
train stats after 78368 examples: {'rewards_train/chosen': '0.080486', 'rewards_train/rejected': '0.026282', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054204', 'logps_train/rejected': '-169.03', 'logps_train/chosen': '-129.59', 'loss/train': '0.6734', 'examples_per_second': '30.853', 'grad_norm': '38.5', 'counters/examples': 78368, 'counters/updates': 2449}
skipping logging after 78400 examples to avoid logging too frequently
train stats after 78432 examples: {'rewards_train/chosen': '0.10568', 'rewards_train/rejected': '0.031516', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074163', 'logps_train/rejected': '-114.12', 'logps_train/chosen': '-133.66', 'loss/train': '0.66161', 'examples_per_second': '33.721', 'grad_norm': '33.5', 'counters/examples': 78432, 'counters/updates': 2451}
train stats after 78464 examples: {'rewards_train/chosen': '0.07851', 'rewards_train/rejected': '0.053477', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025033', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-115.7', 'loss/train': '0.68516', 'examples_per_second': '31.479', 'grad_norm': '29', 'counters/examples': 78464, 'counters/updates': 2452}
train stats after 78496 examples: {'rewards_train/chosen': '0.1734', 'rewards_train/rejected': '0.075111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098291', 'logps_train/rejected': '-97.249', 'logps_train/chosen': '-152.76', 'loss/train': '0.65614', 'examples_per_second': '31.419', 'grad_norm': '37.5', 'counters/examples': 78496, 'counters/updates': 2453}
skipping logging after 78528 examples to avoid logging too frequently
train stats after 78560 examples: {'rewards_train/chosen': '0.11442', 'rewards_train/rejected': '0.080245', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034173', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-106.92', 'loss/train': '0.68265', 'examples_per_second': '31.117', 'grad_norm': '31.375', 'counters/examples': 78560, 'counters/updates': 2455}
skipping logging after 78592 examples to avoid logging too frequently
train stats after 78624 examples: {'rewards_train/chosen': '0.099108', 'rewards_train/rejected': '0.032268', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.06684', 'logps_train/rejected': '-144.49', 'logps_train/chosen': '-130.92', 'loss/train': '0.66674', 'examples_per_second': '31.158', 'grad_norm': '32', 'counters/examples': 78624, 'counters/updates': 2457}
train stats after 78656 examples: {'rewards_train/chosen': '0.11441', 'rewards_train/rejected': '-0.024499', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13891', 'logps_train/rejected': '-121.19', 'logps_train/chosen': '-187.95', 'loss/train': '0.63052', 'examples_per_second': '31.497', 'grad_norm': '29.625', 'counters/examples': 78656, 'counters/updates': 2458}
train stats after 78688 examples: {'rewards_train/chosen': '0.091116', 'rewards_train/rejected': '0.042428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048688', 'logps_train/rejected': '-99.419', 'logps_train/chosen': '-130.09', 'loss/train': '0.67493', 'examples_per_second': '31.517', 'grad_norm': '26.875', 'counters/examples': 78688, 'counters/updates': 2459}
train stats after 78720 examples: {'rewards_train/chosen': '0.10718', 'rewards_train/rejected': '0.093891', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013293', 'logps_train/rejected': '-129.72', 'logps_train/chosen': '-177.53', 'loss/train': '0.70177', 'examples_per_second': '30.569', 'grad_norm': '31.125', 'counters/examples': 78720, 'counters/updates': 2460}
train stats after 78752 examples: {'rewards_train/chosen': '0.11527', 'rewards_train/rejected': '0.026339', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08893', 'logps_train/rejected': '-91.789', 'logps_train/chosen': '-168.23', 'loss/train': '0.65876', 'examples_per_second': '30.915', 'grad_norm': '40.25', 'counters/examples': 78752, 'counters/updates': 2461}
train stats after 78784 examples: {'rewards_train/chosen': '0.16184', 'rewards_train/rejected': '-0.029938', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19178', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-131.77', 'loss/train': '0.60911', 'examples_per_second': '31.249', 'grad_norm': '24.125', 'counters/examples': 78784, 'counters/updates': 2462}
train stats after 78816 examples: {'rewards_train/chosen': '0.15244', 'rewards_train/rejected': '0.1151', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037339', 'logps_train/rejected': '-126.85', 'logps_train/chosen': '-171.29', 'loss/train': '0.68652', 'examples_per_second': '32.785', 'grad_norm': '33.75', 'counters/examples': 78816, 'counters/updates': 2463}
train stats after 78848 examples: {'rewards_train/chosen': '0.088125', 'rewards_train/rejected': '0.067723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020402', 'logps_train/rejected': '-167.85', 'logps_train/chosen': '-144.02', 'loss/train': '0.69246', 'examples_per_second': '33.186', 'grad_norm': '45', 'counters/examples': 78848, 'counters/updates': 2464}
train stats after 78880 examples: {'rewards_train/chosen': '0.07203', 'rewards_train/rejected': '-0.0023547', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074384', 'logps_train/rejected': '-97.643', 'logps_train/chosen': '-122.83', 'loss/train': '0.66021', 'examples_per_second': '32.284', 'grad_norm': '24.875', 'counters/examples': 78880, 'counters/updates': 2465}
train stats after 78912 examples: {'rewards_train/chosen': '0.093642', 'rewards_train/rejected': '-0.013513', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10715', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-156.32', 'loss/train': '0.6484', 'examples_per_second': '31.712', 'grad_norm': '30.75', 'counters/examples': 78912, 'counters/updates': 2466}
train stats after 78944 examples: {'rewards_train/chosen': '0.11682', 'rewards_train/rejected': '0.037034', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.079782', 'logps_train/rejected': '-109.5', 'logps_train/chosen': '-146', 'loss/train': '0.6606', 'examples_per_second': '30.847', 'grad_norm': '30.75', 'counters/examples': 78944, 'counters/updates': 2467}
train stats after 78976 examples: {'rewards_train/chosen': '0.10016', 'rewards_train/rejected': '0.0098755', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090286', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-136.63', 'loss/train': '0.65485', 'examples_per_second': '29.998', 'grad_norm': '29.875', 'counters/examples': 78976, 'counters/updates': 2468}
train stats after 79008 examples: {'rewards_train/chosen': '0.061793', 'rewards_train/rejected': '0.040741', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021052', 'logps_train/rejected': '-95.769', 'logps_train/chosen': '-112.96', 'loss/train': '0.68445', 'examples_per_second': '31.178', 'grad_norm': '24', 'counters/examples': 79008, 'counters/updates': 2469}
train stats after 79040 examples: {'rewards_train/chosen': '0.11014', 'rewards_train/rejected': '0.081938', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028201', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-151.87', 'loss/train': '0.68752', 'examples_per_second': '30.587', 'grad_norm': '30.375', 'counters/examples': 79040, 'counters/updates': 2470}
skipping logging after 79072 examples to avoid logging too frequently
train stats after 79104 examples: {'rewards_train/chosen': '0.097677', 'rewards_train/rejected': '-0.060509', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15819', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-145.16', 'loss/train': '0.6253', 'examples_per_second': '34.758', 'grad_norm': '32.5', 'counters/examples': 79104, 'counters/updates': 2472}
skipping logging after 79136 examples to avoid logging too frequently
train stats after 79168 examples: {'rewards_train/chosen': '0.10775', 'rewards_train/rejected': '0.061204', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046541', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-163.58', 'loss/train': '0.67666', 'examples_per_second': '31.688', 'grad_norm': '29.625', 'counters/examples': 79168, 'counters/updates': 2474}
train stats after 79200 examples: {'rewards_train/chosen': '0.10783', 'rewards_train/rejected': '0.078905', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028924', 'logps_train/rejected': '-156.66', 'logps_train/chosen': '-197.93', 'loss/train': '0.68776', 'examples_per_second': '30.509', 'grad_norm': '37.75', 'counters/examples': 79200, 'counters/updates': 2475}
skipping logging after 79232 examples to avoid logging too frequently
train stats after 79264 examples: {'rewards_train/chosen': '0.06594', 'rewards_train/rejected': '0.016318', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049622', 'logps_train/rejected': '-121.38', 'logps_train/chosen': '-141.61', 'loss/train': '0.67876', 'examples_per_second': '23.492', 'grad_norm': '30.125', 'counters/examples': 79264, 'counters/updates': 2477}
train stats after 79296 examples: {'rewards_train/chosen': '0.10527', 'rewards_train/rejected': '0.069025', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036244', 'logps_train/rejected': '-153.63', 'logps_train/chosen': '-150.22', 'loss/train': '0.68187', 'examples_per_second': '30.259', 'grad_norm': '30.25', 'counters/examples': 79296, 'counters/updates': 2478}
train stats after 79328 examples: {'rewards_train/chosen': '0.061989', 'rewards_train/rejected': '-0.032882', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094871', 'logps_train/rejected': '-127.31', 'logps_train/chosen': '-141.44', 'loss/train': '0.65115', 'examples_per_second': '30.967', 'grad_norm': '25.625', 'counters/examples': 79328, 'counters/updates': 2479}
train stats after 79360 examples: {'rewards_train/chosen': '0.082279', 'rewards_train/rejected': '0.03259', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049689', 'logps_train/rejected': '-112.97', 'logps_train/chosen': '-129.42', 'loss/train': '0.67405', 'examples_per_second': '27.116', 'grad_norm': '31.5', 'counters/examples': 79360, 'counters/updates': 2480}
train stats after 79392 examples: {'rewards_train/chosen': '0.085289', 'rewards_train/rejected': '0.072094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013195', 'logps_train/rejected': '-131.7', 'logps_train/chosen': '-131.7', 'loss/train': '0.69184', 'examples_per_second': '31.081', 'grad_norm': '36.5', 'counters/examples': 79392, 'counters/updates': 2481}
train stats after 79424 examples: {'rewards_train/chosen': '0.12145', 'rewards_train/rejected': '0.054723', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06673', 'logps_train/rejected': '-117.03', 'logps_train/chosen': '-141.1', 'loss/train': '0.66442', 'examples_per_second': '31.804', 'grad_norm': '26.5', 'counters/examples': 79424, 'counters/updates': 2482}
train stats after 79456 examples: {'rewards_train/chosen': '0.064617', 'rewards_train/rejected': '0.060462', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0041548', 'logps_train/rejected': '-133.87', 'logps_train/chosen': '-141.57', 'loss/train': '0.70234', 'examples_per_second': '31.977', 'grad_norm': '33', 'counters/examples': 79456, 'counters/updates': 2483}
train stats after 79488 examples: {'rewards_train/chosen': '0.094895', 'rewards_train/rejected': '0.06126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033635', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-153.18', 'loss/train': '0.68712', 'examples_per_second': '30.864', 'grad_norm': '29', 'counters/examples': 79488, 'counters/updates': 2484}
train stats after 79520 examples: {'rewards_train/chosen': '0.14364', 'rewards_train/rejected': '0.1185', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025137', 'logps_train/rejected': '-146.89', 'logps_train/chosen': '-136.5', 'loss/train': '0.70457', 'examples_per_second': '31.578', 'grad_norm': '51.25', 'counters/examples': 79520, 'counters/updates': 2485}
skipping logging after 79552 examples to avoid logging too frequently
train stats after 79584 examples: {'rewards_train/chosen': '0.069677', 'rewards_train/rejected': '0.045442', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.024235', 'logps_train/rejected': '-128.97', 'logps_train/chosen': '-187.46', 'loss/train': '0.68459', 'examples_per_second': '31.512', 'grad_norm': '30.5', 'counters/examples': 79584, 'counters/updates': 2487}
train stats after 79616 examples: {'rewards_train/chosen': '0.078804', 'rewards_train/rejected': '0.089098', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010294', 'logps_train/rejected': '-174.94', 'logps_train/chosen': '-170.94', 'loss/train': '0.70334', 'examples_per_second': '31.536', 'grad_norm': '54.5', 'counters/examples': 79616, 'counters/updates': 2488}
train stats after 79648 examples: {'rewards_train/chosen': '0.16103', 'rewards_train/rejected': '0.1227', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038335', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-157.53', 'loss/train': '0.68865', 'examples_per_second': '31.984', 'grad_norm': '38', 'counters/examples': 79648, 'counters/updates': 2489}
train stats after 79680 examples: {'rewards_train/chosen': '0.17525', 'rewards_train/rejected': '0.066161', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10908', 'logps_train/rejected': '-118.06', 'logps_train/chosen': '-146.04', 'loss/train': '0.65572', 'examples_per_second': '31.441', 'grad_norm': '29.875', 'counters/examples': 79680, 'counters/updates': 2490}
train stats after 79712 examples: {'rewards_train/chosen': '0.045208', 'rewards_train/rejected': '0.029723', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015485', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-160.84', 'loss/train': '0.69467', 'examples_per_second': '31.097', 'grad_norm': '41.75', 'counters/examples': 79712, 'counters/updates': 2491}
train stats after 79744 examples: {'rewards_train/chosen': '0.053777', 'rewards_train/rejected': '0.058554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0047772', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-185.96', 'loss/train': '0.70574', 'examples_per_second': '30.505', 'grad_norm': '35.5', 'counters/examples': 79744, 'counters/updates': 2492}
train stats after 79776 examples: {'rewards_train/chosen': '0.034618', 'rewards_train/rejected': '0.058902', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024285', 'logps_train/rejected': '-159.73', 'logps_train/chosen': '-145.15', 'loss/train': '0.71143', 'examples_per_second': '31.547', 'grad_norm': '45', 'counters/examples': 79776, 'counters/updates': 2493}
train stats after 79808 examples: {'rewards_train/chosen': '0.10402', 'rewards_train/rejected': '0.044256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059765', 'logps_train/rejected': '-145.31', 'logps_train/chosen': '-165.92', 'loss/train': '0.669', 'examples_per_second': '32.149', 'grad_norm': '29.625', 'counters/examples': 79808, 'counters/updates': 2494}
train stats after 79840 examples: {'rewards_train/chosen': '0.11709', 'rewards_train/rejected': '0.065731', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.051361', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-141.74', 'loss/train': '0.69787', 'examples_per_second': '30.82', 'grad_norm': '40.75', 'counters/examples': 79840, 'counters/updates': 2495}
train stats after 79872 examples: {'rewards_train/chosen': '0.038564', 'rewards_train/rejected': '0.039129', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00056503', 'logps_train/rejected': '-158.46', 'logps_train/chosen': '-117.82', 'loss/train': '0.70298', 'examples_per_second': '31.329', 'grad_norm': '40.25', 'counters/examples': 79872, 'counters/updates': 2496}
train stats after 79904 examples: {'rewards_train/chosen': '0.11244', 'rewards_train/rejected': '0.063037', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0494', 'logps_train/rejected': '-146.52', 'logps_train/chosen': '-143.46', 'loss/train': '0.68487', 'examples_per_second': '29.964', 'grad_norm': '35.25', 'counters/examples': 79904, 'counters/updates': 2497}
train stats after 79936 examples: {'rewards_train/chosen': '0.14547', 'rewards_train/rejected': '0.092051', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053424', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-158.28', 'loss/train': '0.67228', 'examples_per_second': '31.179', 'grad_norm': '36.25', 'counters/examples': 79936, 'counters/updates': 2498}
train stats after 79968 examples: {'rewards_train/chosen': '0.12527', 'rewards_train/rejected': '0.054156', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071116', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-122.01', 'loss/train': '0.66829', 'examples_per_second': '31.377', 'grad_norm': '29.25', 'counters/examples': 79968, 'counters/updates': 2499}
train stats after 80000 examples: {'rewards_train/chosen': '0.10729', 'rewards_train/rejected': '0.069195', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.03809', 'logps_train/rejected': '-172.9', 'logps_train/chosen': '-149.04', 'loss/train': '0.70085', 'examples_per_second': '31.475', 'grad_norm': '69', 'counters/examples': 80000, 'counters/updates': 2500}
train stats after 80032 examples: {'rewards_train/chosen': '0.12084', 'rewards_train/rejected': '0.069277', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051567', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-139.87', 'loss/train': '0.6735', 'examples_per_second': '30.229', 'grad_norm': '36.75', 'counters/examples': 80032, 'counters/updates': 2501}
train stats after 80064 examples: {'rewards_train/chosen': '0.11148', 'rewards_train/rejected': '0.16996', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.058478', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-158.06', 'loss/train': '0.72996', 'examples_per_second': '33.106', 'grad_norm': '41', 'counters/examples': 80064, 'counters/updates': 2502}
skipping logging after 80096 examples to avoid logging too frequently
train stats after 80128 examples: {'rewards_train/chosen': '0.063068', 'rewards_train/rejected': '0.044642', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018426', 'logps_train/rejected': '-114.7', 'logps_train/chosen': '-127.88', 'loss/train': '0.69076', 'examples_per_second': '34.111', 'grad_norm': '27.75', 'counters/examples': 80128, 'counters/updates': 2504}
skipping logging after 80160 examples to avoid logging too frequently
train stats after 80192 examples: {'rewards_train/chosen': '0.10047', 'rewards_train/rejected': '0.041139', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059335', 'logps_train/rejected': '-104.07', 'logps_train/chosen': '-122.44', 'loss/train': '0.67133', 'examples_per_second': '31.697', 'grad_norm': '26.125', 'counters/examples': 80192, 'counters/updates': 2506}
train stats after 80224 examples: {'rewards_train/chosen': '0.12355', 'rewards_train/rejected': '0.034459', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089091', 'logps_train/rejected': '-114.51', 'logps_train/chosen': '-140.34', 'loss/train': '0.65334', 'examples_per_second': '32.891', 'grad_norm': '35.25', 'counters/examples': 80224, 'counters/updates': 2507}
train stats after 80256 examples: {'rewards_train/chosen': '0.12693', 'rewards_train/rejected': '0.16435', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.037412', 'logps_train/rejected': '-135.84', 'logps_train/chosen': '-146.46', 'loss/train': '0.72544', 'examples_per_second': '31.302', 'grad_norm': '47.25', 'counters/examples': 80256, 'counters/updates': 2508}
train stats after 80288 examples: {'rewards_train/chosen': '0.046721', 'rewards_train/rejected': '0.031412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015309', 'logps_train/rejected': '-113.11', 'logps_train/chosen': '-120.44', 'loss/train': '0.69055', 'examples_per_second': '32.317', 'grad_norm': '36.5', 'counters/examples': 80288, 'counters/updates': 2509}
train stats after 80320 examples: {'rewards_train/chosen': '0.10852', 'rewards_train/rejected': '0.11674', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0082165', 'logps_train/rejected': '-171.48', 'logps_train/chosen': '-139.91', 'loss/train': '0.70598', 'examples_per_second': '30.241', 'grad_norm': '80.5', 'counters/examples': 80320, 'counters/updates': 2510}
train stats after 80352 examples: {'rewards_train/chosen': '0.061841', 'rewards_train/rejected': '0.031222', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030619', 'logps_train/rejected': '-117.28', 'logps_train/chosen': '-166.66', 'loss/train': '0.68436', 'examples_per_second': '30.049', 'grad_norm': '35', 'counters/examples': 80352, 'counters/updates': 2511}
skipping logging after 80384 examples to avoid logging too frequently
train stats after 80416 examples: {'rewards_train/chosen': '0.065009', 'rewards_train/rejected': '0.076872', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.011863', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-122.14', 'loss/train': '0.71179', 'examples_per_second': '33.354', 'grad_norm': '33.75', 'counters/examples': 80416, 'counters/updates': 2513}
train stats after 80448 examples: {'rewards_train/chosen': '0.069664', 'rewards_train/rejected': '0.096532', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.026869', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-123.36', 'loss/train': '0.71567', 'examples_per_second': '31.474', 'grad_norm': '35.75', 'counters/examples': 80448, 'counters/updates': 2514}
train stats after 80480 examples: {'rewards_train/chosen': '0.074103', 'rewards_train/rejected': '0.030614', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043489', 'logps_train/rejected': '-95.69', 'logps_train/chosen': '-125.52', 'loss/train': '0.67468', 'examples_per_second': '31.576', 'grad_norm': '27.125', 'counters/examples': 80480, 'counters/updates': 2515}
skipping logging after 80512 examples to avoid logging too frequently
train stats after 80544 examples: {'rewards_train/chosen': '0.07542', 'rewards_train/rejected': '0.12056', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.045136', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-112.7', 'loss/train': '0.72239', 'examples_per_second': '30.114', 'grad_norm': '42.25', 'counters/examples': 80544, 'counters/updates': 2517}
train stats after 80576 examples: {'rewards_train/chosen': '0.05835', 'rewards_train/rejected': '0.040346', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '0.018004', 'logps_train/rejected': '-94.837', 'logps_train/chosen': '-113.63', 'loss/train': '0.69411', 'examples_per_second': '30.515', 'grad_norm': '27.25', 'counters/examples': 80576, 'counters/updates': 2518}
train stats after 80608 examples: {'rewards_train/chosen': '0.062651', 'rewards_train/rejected': '0.058457', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0041939', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-168.52', 'loss/train': '0.69622', 'examples_per_second': '31.304', 'grad_norm': '35.5', 'counters/examples': 80608, 'counters/updates': 2519}
train stats after 80640 examples: {'rewards_train/chosen': '0.072816', 'rewards_train/rejected': '0.031101', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041714', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-131.75', 'loss/train': '0.6805', 'examples_per_second': '30.663', 'grad_norm': '34.25', 'counters/examples': 80640, 'counters/updates': 2520}
train stats after 80672 examples: {'rewards_train/chosen': '0.11641', 'rewards_train/rejected': '0.077767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.038647', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-131.31', 'loss/train': '0.68176', 'examples_per_second': '30.569', 'grad_norm': '30.5', 'counters/examples': 80672, 'counters/updates': 2521}
train stats after 80704 examples: {'rewards_train/chosen': '0.035845', 'rewards_train/rejected': '0.011219', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024627', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-172.37', 'loss/train': '0.69633', 'examples_per_second': '31.532', 'grad_norm': '34', 'counters/examples': 80704, 'counters/updates': 2522}
train stats after 80736 examples: {'rewards_train/chosen': '0.1179', 'rewards_train/rejected': '0.078156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039746', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-133.9', 'loss/train': '0.68246', 'examples_per_second': '32.78', 'grad_norm': '37.75', 'counters/examples': 80736, 'counters/updates': 2523}
train stats after 80768 examples: {'rewards_train/chosen': '0.16641', 'rewards_train/rejected': '0.020417', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14599', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-133.33', 'loss/train': '0.62949', 'examples_per_second': '30.459', 'grad_norm': '27.5', 'counters/examples': 80768, 'counters/updates': 2524}
train stats after 80800 examples: {'rewards_train/chosen': '0.14961', 'rewards_train/rejected': '0.068141', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.081469', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-148.39', 'loss/train': '0.66508', 'examples_per_second': '32.956', 'grad_norm': '35.75', 'counters/examples': 80800, 'counters/updates': 2525}
train stats after 80832 examples: {'rewards_train/chosen': '0.12251', 'rewards_train/rejected': '0.080838', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.041677', 'logps_train/rejected': '-126', 'logps_train/chosen': '-150.73', 'loss/train': '0.68624', 'examples_per_second': '31.253', 'grad_norm': '53', 'counters/examples': 80832, 'counters/updates': 2526}
train stats after 80864 examples: {'rewards_train/chosen': '0.072901', 'rewards_train/rejected': '0.026505', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046396', 'logps_train/rejected': '-127.2', 'logps_train/chosen': '-145.41', 'loss/train': '0.68031', 'examples_per_second': '31.481', 'grad_norm': '38.5', 'counters/examples': 80864, 'counters/updates': 2527}
train stats after 80896 examples: {'rewards_train/chosen': '0.056034', 'rewards_train/rejected': '0.099703', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043669', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-138.92', 'loss/train': '0.72019', 'examples_per_second': '24.717', 'grad_norm': '26.5', 'counters/examples': 80896, 'counters/updates': 2528}
train stats after 80928 examples: {'rewards_train/chosen': '0.095199', 'rewards_train/rejected': '0.072168', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023031', 'logps_train/rejected': '-160.74', 'logps_train/chosen': '-168.15', 'loss/train': '0.69389', 'examples_per_second': '30.977', 'grad_norm': '50.25', 'counters/examples': 80928, 'counters/updates': 2529}
train stats after 80960 examples: {'rewards_train/chosen': '0.032791', 'rewards_train/rejected': '0.071911', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.03912', 'logps_train/rejected': '-109.8', 'logps_train/chosen': '-161.63', 'loss/train': '0.72325', 'examples_per_second': '31.446', 'grad_norm': '37.5', 'counters/examples': 80960, 'counters/updates': 2530}
skipping logging after 80992 examples to avoid logging too frequently
train stats after 81024 examples: {'rewards_train/chosen': '0.15793', 'rewards_train/rejected': '0.065262', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092668', 'logps_train/rejected': '-104.51', 'logps_train/chosen': '-154.52', 'loss/train': '0.66798', 'examples_per_second': '33.983', 'grad_norm': '26.5', 'counters/examples': 81024, 'counters/updates': 2532}
train stats after 81056 examples: {'rewards_train/chosen': '0.10862', 'rewards_train/rejected': '0.032183', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076441', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-136.79', 'loss/train': '0.66318', 'examples_per_second': '30.931', 'grad_norm': '41', 'counters/examples': 81056, 'counters/updates': 2533}
train stats after 81088 examples: {'rewards_train/chosen': '0.10973', 'rewards_train/rejected': '0.018933', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090794', 'logps_train/rejected': '-145.12', 'logps_train/chosen': '-158.49', 'loss/train': '0.6595', 'examples_per_second': '31.321', 'grad_norm': '47', 'counters/examples': 81088, 'counters/updates': 2534}
train stats after 81120 examples: {'rewards_train/chosen': '0.1062', 'rewards_train/rejected': '0.017042', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089154', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-147.08', 'loss/train': '0.65927', 'examples_per_second': '31.626', 'grad_norm': '43', 'counters/examples': 81120, 'counters/updates': 2535}
train stats after 81152 examples: {'rewards_train/chosen': '0.094073', 'rewards_train/rejected': '-0.049094', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14317', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-148.49', 'loss/train': '0.63493', 'examples_per_second': '30.696', 'grad_norm': '29.875', 'counters/examples': 81152, 'counters/updates': 2536}
skipping logging after 81184 examples to avoid logging too frequently
train stats after 81216 examples: {'rewards_train/chosen': '0.11468', 'rewards_train/rejected': '0.055288', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.059397', 'logps_train/rejected': '-107.48', 'logps_train/chosen': '-144.35', 'loss/train': '0.67339', 'examples_per_second': '31.887', 'grad_norm': '27.5', 'counters/examples': 81216, 'counters/updates': 2538}
train stats after 81248 examples: {'rewards_train/chosen': '0.12228', 'rewards_train/rejected': '0.09378', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028498', 'logps_train/rejected': '-150.57', 'logps_train/chosen': '-176.24', 'loss/train': '0.69096', 'examples_per_second': '31.552', 'grad_norm': '42', 'counters/examples': 81248, 'counters/updates': 2539}
train stats after 81280 examples: {'rewards_train/chosen': '0.1139', 'rewards_train/rejected': '0.030768', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083128', 'logps_train/rejected': '-124.14', 'logps_train/chosen': '-118.63', 'loss/train': '0.65897', 'examples_per_second': '31.394', 'grad_norm': '31.625', 'counters/examples': 81280, 'counters/updates': 2540}
train stats after 81312 examples: {'rewards_train/chosen': '0.073995', 'rewards_train/rejected': '-0.04678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12077', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-137.25', 'loss/train': '0.65309', 'examples_per_second': '30.427', 'grad_norm': '24.25', 'counters/examples': 81312, 'counters/updates': 2541}
skipping logging after 81344 examples to avoid logging too frequently
train stats after 81376 examples: {'rewards_train/chosen': '0.074775', 'rewards_train/rejected': '0.016802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057973', 'logps_train/rejected': '-105.45', 'logps_train/chosen': '-122.63', 'loss/train': '0.67001', 'examples_per_second': '30.889', 'grad_norm': '29', 'counters/examples': 81376, 'counters/updates': 2543}
train stats after 81408 examples: {'rewards_train/chosen': '0.042238', 'rewards_train/rejected': '0.04562', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0033818', 'logps_train/rejected': '-148.22', 'logps_train/chosen': '-151.44', 'loss/train': '0.70018', 'examples_per_second': '31.297', 'grad_norm': '34.25', 'counters/examples': 81408, 'counters/updates': 2544}
skipping logging after 81440 examples to avoid logging too frequently
train stats after 81472 examples: {'rewards_train/chosen': '0.15454', 'rewards_train/rejected': '0.040876', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11366', 'logps_train/rejected': '-131.81', 'logps_train/chosen': '-178.39', 'loss/train': '0.6474', 'examples_per_second': '30.342', 'grad_norm': '32', 'counters/examples': 81472, 'counters/updates': 2546}
train stats after 81504 examples: {'rewards_train/chosen': '0.090687', 'rewards_train/rejected': '0.049917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04077', 'logps_train/rejected': '-128.02', 'logps_train/chosen': '-130.27', 'loss/train': '0.67994', 'examples_per_second': '31.565', 'grad_norm': '30', 'counters/examples': 81504, 'counters/updates': 2547}
skipping logging after 81536 examples to avoid logging too frequently
train stats after 81568 examples: {'rewards_train/chosen': '0.051781', 'rewards_train/rejected': '0.087748', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.035967', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-100.42', 'loss/train': '0.71995', 'examples_per_second': '38.387', 'grad_norm': '31.75', 'counters/examples': 81568, 'counters/updates': 2549}
train stats after 81600 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.1314', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019822', 'logps_train/rejected': '-132.7', 'logps_train/chosen': '-121.9', 'loss/train': '0.71255', 'examples_per_second': '30.017', 'grad_norm': '37', 'counters/examples': 81600, 'counters/updates': 2550}
train stats after 81632 examples: {'rewards_train/chosen': '0.051786', 'rewards_train/rejected': '-0.027806', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079593', 'logps_train/rejected': '-120.37', 'logps_train/chosen': '-154.07', 'loss/train': '0.65929', 'examples_per_second': '31.5', 'grad_norm': '26.5', 'counters/examples': 81632, 'counters/updates': 2551}
train stats after 81664 examples: {'rewards_train/chosen': '0.080598', 'rewards_train/rejected': '0.027059', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053539', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-129', 'loss/train': '0.6742', 'examples_per_second': '30.495', 'grad_norm': '30.625', 'counters/examples': 81664, 'counters/updates': 2552}
train stats after 81696 examples: {'rewards_train/chosen': '0.079553', 'rewards_train/rejected': '0.026805', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052748', 'logps_train/rejected': '-107.22', 'logps_train/chosen': '-100.98', 'loss/train': '0.67665', 'examples_per_second': '32.976', 'grad_norm': '31.25', 'counters/examples': 81696, 'counters/updates': 2553}
train stats after 81728 examples: {'rewards_train/chosen': '0.1306', 'rewards_train/rejected': '0.011886', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11872', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-171.15', 'loss/train': '0.64203', 'examples_per_second': '31.297', 'grad_norm': '24.75', 'counters/examples': 81728, 'counters/updates': 2554}
train stats after 81760 examples: {'rewards_train/chosen': '0.11766', 'rewards_train/rejected': '0.076811', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040851', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-176.1', 'loss/train': '0.67987', 'examples_per_second': '30.856', 'grad_norm': '32.75', 'counters/examples': 81760, 'counters/updates': 2555}
train stats after 81792 examples: {'rewards_train/chosen': '0.036748', 'rewards_train/rejected': '0.064252', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.027504', 'logps_train/rejected': '-143.24', 'logps_train/chosen': '-138.78', 'loss/train': '0.71398', 'examples_per_second': '31.789', 'grad_norm': '32.25', 'counters/examples': 81792, 'counters/updates': 2556}
train stats after 81824 examples: {'rewards_train/chosen': '0.076555', 'rewards_train/rejected': '0.012883', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063672', 'logps_train/rejected': '-123.25', 'logps_train/chosen': '-155.16', 'loss/train': '0.66495', 'examples_per_second': '31.494', 'grad_norm': '27', 'counters/examples': 81824, 'counters/updates': 2557}
train stats after 81856 examples: {'rewards_train/chosen': '0.088939', 'rewards_train/rejected': '0.044954', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043985', 'logps_train/rejected': '-99.274', 'logps_train/chosen': '-99.301', 'loss/train': '0.67801', 'examples_per_second': '31.531', 'grad_norm': '24.625', 'counters/examples': 81856, 'counters/updates': 2558}
train stats after 81888 examples: {'rewards_train/chosen': '0.036826', 'rewards_train/rejected': '-0.0036419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040468', 'logps_train/rejected': '-152.5', 'logps_train/chosen': '-159.24', 'loss/train': '0.68331', 'examples_per_second': '31.535', 'grad_norm': '37', 'counters/examples': 81888, 'counters/updates': 2559}
train stats after 81920 examples: {'rewards_train/chosen': '0.12422', 'rewards_train/rejected': '0.09062', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033601', 'logps_train/rejected': '-132.12', 'logps_train/chosen': '-132.92', 'loss/train': '0.69288', 'examples_per_second': '30.85', 'grad_norm': '27.875', 'counters/examples': 81920, 'counters/updates': 2560}
train stats after 81952 examples: {'rewards_train/chosen': '0.06124', 'rewards_train/rejected': '0.035987', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025252', 'logps_train/rejected': '-99.555', 'logps_train/chosen': '-131.41', 'loss/train': '0.68606', 'examples_per_second': '32.44', 'grad_norm': '41.25', 'counters/examples': 81952, 'counters/updates': 2561}
train stats after 81984 examples: {'rewards_train/chosen': '0.12221', 'rewards_train/rejected': '0.070783', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051427', 'logps_train/rejected': '-150.48', 'logps_train/chosen': '-185.01', 'loss/train': '0.69217', 'examples_per_second': '31.482', 'grad_norm': '31.125', 'counters/examples': 81984, 'counters/updates': 2562}
train stats after 82016 examples: {'rewards_train/chosen': '0.14436', 'rewards_train/rejected': '0.0011546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1432', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-160.52', 'loss/train': '0.63123', 'examples_per_second': '31.538', 'grad_norm': '34.75', 'counters/examples': 82016, 'counters/updates': 2563}
skipping logging after 82048 examples to avoid logging too frequently
train stats after 82080 examples: {'rewards_train/chosen': '0.045707', 'rewards_train/rejected': '0.07927', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.033563', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-116.2', 'loss/train': '0.71706', 'examples_per_second': '31.994', 'grad_norm': '28', 'counters/examples': 82080, 'counters/updates': 2565}
skipping logging after 82112 examples to avoid logging too frequently
train stats after 82144 examples: {'rewards_train/chosen': '0.08155', 'rewards_train/rejected': '0.074851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0066993', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-127.04', 'loss/train': '0.69642', 'examples_per_second': '31.436', 'grad_norm': '37.25', 'counters/examples': 82144, 'counters/updates': 2567}
train stats after 82176 examples: {'rewards_train/chosen': '-0.010153', 'rewards_train/rejected': '0.052351', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.062504', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-151.35', 'loss/train': '0.73196', 'examples_per_second': '31.391', 'grad_norm': '33.25', 'counters/examples': 82176, 'counters/updates': 2568}
train stats after 82208 examples: {'rewards_train/chosen': '0.10346', 'rewards_train/rejected': '0.061403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042055', 'logps_train/rejected': '-125.42', 'logps_train/chosen': '-163.67', 'loss/train': '0.67908', 'examples_per_second': '32.49', 'grad_norm': '35.25', 'counters/examples': 82208, 'counters/updates': 2569}
train stats after 82240 examples: {'rewards_train/chosen': '0.15008', 'rewards_train/rejected': '0.066669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083414', 'logps_train/rejected': '-89.435', 'logps_train/chosen': '-184.87', 'loss/train': '0.66025', 'examples_per_second': '32.463', 'grad_norm': '30.625', 'counters/examples': 82240, 'counters/updates': 2570}
train stats after 82272 examples: {'rewards_train/chosen': '0.097262', 'rewards_train/rejected': '0.063261', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034002', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-137.7', 'loss/train': '0.69262', 'examples_per_second': '31.682', 'grad_norm': '31.375', 'counters/examples': 82272, 'counters/updates': 2571}
train stats after 82304 examples: {'rewards_train/chosen': '0.11982', 'rewards_train/rejected': '0.060322', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059503', 'logps_train/rejected': '-141.73', 'logps_train/chosen': '-152.04', 'loss/train': '0.67205', 'examples_per_second': '31.708', 'grad_norm': '28.375', 'counters/examples': 82304, 'counters/updates': 2572}
skipping logging after 82336 examples to avoid logging too frequently
train stats after 82368 examples: {'rewards_train/chosen': '0.084593', 'rewards_train/rejected': '0.0062992', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078294', 'logps_train/rejected': '-112.58', 'logps_train/chosen': '-150.25', 'loss/train': '0.66865', 'examples_per_second': '33.237', 'grad_norm': '38.5', 'counters/examples': 82368, 'counters/updates': 2574}
train stats after 82400 examples: {'rewards_train/chosen': '0.082596', 'rewards_train/rejected': '0.052354', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030242', 'logps_train/rejected': '-172.85', 'logps_train/chosen': '-167.7', 'loss/train': '0.68487', 'examples_per_second': '31.486', 'grad_norm': '36.75', 'counters/examples': 82400, 'counters/updates': 2575}
train stats after 82432 examples: {'rewards_train/chosen': '0.055172', 'rewards_train/rejected': '0.12994', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.074764', 'logps_train/rejected': '-154.14', 'logps_train/chosen': '-150.69', 'loss/train': '0.74193', 'examples_per_second': '31.893', 'grad_norm': '51', 'counters/examples': 82432, 'counters/updates': 2576}
train stats after 82464 examples: {'rewards_train/chosen': '0.10083', 'rewards_train/rejected': '0.063799', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.037033', 'logps_train/rejected': '-113.86', 'logps_train/chosen': '-127.94', 'loss/train': '0.68051', 'examples_per_second': '30.588', 'grad_norm': '26.5', 'counters/examples': 82464, 'counters/updates': 2577}
train stats after 82496 examples: {'rewards_train/chosen': '0.11677', 'rewards_train/rejected': '0.042365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074404', 'logps_train/rejected': '-166.1', 'logps_train/chosen': '-167.06', 'loss/train': '0.66179', 'examples_per_second': '31.488', 'grad_norm': '29', 'counters/examples': 82496, 'counters/updates': 2578}
train stats after 82528 examples: {'rewards_train/chosen': '0.07291', 'rewards_train/rejected': '0.059419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.01349', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-154.46', 'loss/train': '0.69406', 'examples_per_second': '30.513', 'grad_norm': '30.125', 'counters/examples': 82528, 'counters/updates': 2579}
train stats after 82560 examples: {'rewards_train/chosen': '0.055606', 'rewards_train/rejected': '-0.0026771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058283', 'logps_train/rejected': '-108.82', 'logps_train/chosen': '-160.97', 'loss/train': '0.66877', 'examples_per_second': '30.523', 'grad_norm': '48.5', 'counters/examples': 82560, 'counters/updates': 2580}
train stats after 82592 examples: {'rewards_train/chosen': '0.16983', 'rewards_train/rejected': '0.10031', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.069518', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-160.68', 'loss/train': '0.66857', 'examples_per_second': '31.309', 'grad_norm': '51.5', 'counters/examples': 82592, 'counters/updates': 2581}
skipping logging after 82624 examples to avoid logging too frequently
train stats after 82656 examples: {'rewards_train/chosen': '0.12987', 'rewards_train/rejected': '0.030985', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098889', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-118.46', 'loss/train': '0.65352', 'examples_per_second': '31.469', 'grad_norm': '26.625', 'counters/examples': 82656, 'counters/updates': 2583}
train stats after 82688 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '0.074556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040268', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-149.94', 'loss/train': '0.68318', 'examples_per_second': '30.033', 'grad_norm': '36', 'counters/examples': 82688, 'counters/updates': 2584}
train stats after 82720 examples: {'rewards_train/chosen': '0.065139', 'rewards_train/rejected': '0.0086709', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056468', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-125.01', 'loss/train': '0.6709', 'examples_per_second': '31.684', 'grad_norm': '28.375', 'counters/examples': 82720, 'counters/updates': 2585}
train stats after 82752 examples: {'rewards_train/chosen': '0.11419', 'rewards_train/rejected': '0.027309', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086884', 'logps_train/rejected': '-133.85', 'logps_train/chosen': '-150.29', 'loss/train': '0.65849', 'examples_per_second': '30.45', 'grad_norm': '29.125', 'counters/examples': 82752, 'counters/updates': 2586}
train stats after 82784 examples: {'rewards_train/chosen': '0.048648', 'rewards_train/rejected': '0.023619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025029', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-137.48', 'loss/train': '0.69181', 'examples_per_second': '31.479', 'grad_norm': '40', 'counters/examples': 82784, 'counters/updates': 2587}
train stats after 82816 examples: {'rewards_train/chosen': '0.10125', 'rewards_train/rejected': '0.017994', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083257', 'logps_train/rejected': '-107.1', 'logps_train/chosen': '-132.73', 'loss/train': '0.6641', 'examples_per_second': '32.342', 'grad_norm': '50', 'counters/examples': 82816, 'counters/updates': 2588}
train stats after 82848 examples: {'rewards_train/chosen': '0.19811', 'rewards_train/rejected': '0.065231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13288', 'logps_train/rejected': '-112.57', 'logps_train/chosen': '-160.51', 'loss/train': '0.63704', 'examples_per_second': '31.47', 'grad_norm': '31.75', 'counters/examples': 82848, 'counters/updates': 2589}
train stats after 82880 examples: {'rewards_train/chosen': '0.10708', 'rewards_train/rejected': '0.025409', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.081671', 'logps_train/rejected': '-122.89', 'logps_train/chosen': '-129.9', 'loss/train': '0.6657', 'examples_per_second': '31.272', 'grad_norm': '41.5', 'counters/examples': 82880, 'counters/updates': 2590}
train stats after 82912 examples: {'rewards_train/chosen': '0.065082', 'rewards_train/rejected': '0.00098649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064096', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-155.39', 'loss/train': '0.66757', 'examples_per_second': '31.471', 'grad_norm': '29.875', 'counters/examples': 82912, 'counters/updates': 2591}
train stats after 82944 examples: {'rewards_train/chosen': '0.095092', 'rewards_train/rejected': '0.039416', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055676', 'logps_train/rejected': '-168.64', 'logps_train/chosen': '-173.55', 'loss/train': '0.6723', 'examples_per_second': '31.5', 'grad_norm': '29.125', 'counters/examples': 82944, 'counters/updates': 2592}
train stats after 82976 examples: {'rewards_train/chosen': '0.13549', 'rewards_train/rejected': '0.10469', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030802', 'logps_train/rejected': '-154.71', 'logps_train/chosen': '-177.73', 'loss/train': '0.69395', 'examples_per_second': '31.48', 'grad_norm': '47.25', 'counters/examples': 82976, 'counters/updates': 2593}
skipping logging after 83008 examples to avoid logging too frequently
train stats after 83040 examples: {'rewards_train/chosen': '0.12093', 'rewards_train/rejected': '0.068118', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052812', 'logps_train/rejected': '-124.61', 'logps_train/chosen': '-153.44', 'loss/train': '0.67622', 'examples_per_second': '31.591', 'grad_norm': '30.625', 'counters/examples': 83040, 'counters/updates': 2595}
train stats after 83072 examples: {'rewards_train/chosen': '0.0992', 'rewards_train/rejected': '0.017991', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081208', 'logps_train/rejected': '-141.95', 'logps_train/chosen': '-144.69', 'loss/train': '0.65703', 'examples_per_second': '30.081', 'grad_norm': '34', 'counters/examples': 83072, 'counters/updates': 2596}
train stats after 83104 examples: {'rewards_train/chosen': '0.043835', 'rewards_train/rejected': '0.030374', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013461', 'logps_train/rejected': '-121.72', 'logps_train/chosen': '-129.72', 'loss/train': '0.69012', 'examples_per_second': '29.988', 'grad_norm': '30.75', 'counters/examples': 83104, 'counters/updates': 2597}
train stats after 83136 examples: {'rewards_train/chosen': '0.091106', 'rewards_train/rejected': '-0.002799', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093905', 'logps_train/rejected': '-141.67', 'logps_train/chosen': '-125.95', 'loss/train': '0.6531', 'examples_per_second': '30.725', 'grad_norm': '30.375', 'counters/examples': 83136, 'counters/updates': 2598}
train stats after 83168 examples: {'rewards_train/chosen': '0.06804', 'rewards_train/rejected': '0.10705', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039009', 'logps_train/rejected': '-151.92', 'logps_train/chosen': '-136.35', 'loss/train': '0.72702', 'examples_per_second': '30.008', 'grad_norm': '43', 'counters/examples': 83168, 'counters/updates': 2599}
skipping logging after 83200 examples to avoid logging too frequently
train stats after 83232 examples: {'rewards_train/chosen': '0.075539', 'rewards_train/rejected': '0.0353', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.040239', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-109.07', 'loss/train': '0.67758', 'examples_per_second': '33.749', 'grad_norm': '27.875', 'counters/examples': 83232, 'counters/updates': 2601}
train stats after 83264 examples: {'rewards_train/chosen': '0.078856', 'rewards_train/rejected': '0.083744', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0048884', 'logps_train/rejected': '-131.16', 'logps_train/chosen': '-142.25', 'loss/train': '0.70093', 'examples_per_second': '31.252', 'grad_norm': '31.375', 'counters/examples': 83264, 'counters/updates': 2602}
train stats after 83296 examples: {'rewards_train/chosen': '0.027927', 'rewards_train/rejected': '0.098389', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.070461', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-152.18', 'loss/train': '0.7366', 'examples_per_second': '32.475', 'grad_norm': '38.25', 'counters/examples': 83296, 'counters/updates': 2603}
skipping logging after 83328 examples to avoid logging too frequently
train stats after 83360 examples: {'rewards_train/chosen': '0.20136', 'rewards_train/rejected': '0.0071842', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19418', 'logps_train/rejected': '-85.806', 'logps_train/chosen': '-162.33', 'loss/train': '0.63106', 'examples_per_second': '31.498', 'grad_norm': '38.25', 'counters/examples': 83360, 'counters/updates': 2605}
skipping logging after 83392 examples to avoid logging too frequently
train stats after 83424 examples: {'rewards_train/chosen': '0.07871', 'rewards_train/rejected': '0.012072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066638', 'logps_train/rejected': '-141.58', 'logps_train/chosen': '-150.1', 'loss/train': '0.66547', 'examples_per_second': '30.879', 'grad_norm': '29', 'counters/examples': 83424, 'counters/updates': 2607}
train stats after 83456 examples: {'rewards_train/chosen': '0.13663', 'rewards_train/rejected': '0.05025', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086377', 'logps_train/rejected': '-118.27', 'logps_train/chosen': '-153.86', 'loss/train': '0.65837', 'examples_per_second': '31.382', 'grad_norm': '33.5', 'counters/examples': 83456, 'counters/updates': 2608}
skipping logging after 83488 examples to avoid logging too frequently
train stats after 83520 examples: {'rewards_train/chosen': '0.074773', 'rewards_train/rejected': '0.019957', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054816', 'logps_train/rejected': '-184.53', 'logps_train/chosen': '-157.65', 'loss/train': '0.67533', 'examples_per_second': '32.654', 'grad_norm': '33', 'counters/examples': 83520, 'counters/updates': 2610}
train stats after 83552 examples: {'rewards_train/chosen': '0.1122', 'rewards_train/rejected': '0.062715', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049488', 'logps_train/rejected': '-148.61', 'logps_train/chosen': '-136.25', 'loss/train': '0.67801', 'examples_per_second': '33.154', 'grad_norm': '33', 'counters/examples': 83552, 'counters/updates': 2611}
skipping logging after 83584 examples to avoid logging too frequently
train stats after 83616 examples: {'rewards_train/chosen': '0.12413', 'rewards_train/rejected': '0.090256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033873', 'logps_train/rejected': '-146.07', 'logps_train/chosen': '-131.77', 'loss/train': '0.68318', 'examples_per_second': '33.382', 'grad_norm': '30.125', 'counters/examples': 83616, 'counters/updates': 2613}
skipping logging after 83648 examples to avoid logging too frequently
train stats after 83680 examples: {'rewards_train/chosen': '0.09472', 'rewards_train/rejected': '0.059095', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035625', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-124.19', 'loss/train': '0.68203', 'examples_per_second': '32.213', 'grad_norm': '33.25', 'counters/examples': 83680, 'counters/updates': 2615}
train stats after 83712 examples: {'rewards_train/chosen': '0.12746', 'rewards_train/rejected': '0.060009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067451', 'logps_train/rejected': '-117.29', 'logps_train/chosen': '-142.48', 'loss/train': '0.66444', 'examples_per_second': '30.902', 'grad_norm': '26.5', 'counters/examples': 83712, 'counters/updates': 2616}
train stats after 83744 examples: {'rewards_train/chosen': '0.10429', 'rewards_train/rejected': '0.074965', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029324', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-164.71', 'loss/train': '0.68878', 'examples_per_second': '30.274', 'grad_norm': '30', 'counters/examples': 83744, 'counters/updates': 2617}
skipping logging after 83776 examples to avoid logging too frequently
train stats after 83808 examples: {'rewards_train/chosen': '0.10539', 'rewards_train/rejected': '0.0071402', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098251', 'logps_train/rejected': '-92.895', 'logps_train/chosen': '-91.298', 'loss/train': '0.65296', 'examples_per_second': '33.293', 'grad_norm': '36.5', 'counters/examples': 83808, 'counters/updates': 2619}
skipping logging after 83840 examples to avoid logging too frequently
train stats after 83872 examples: {'rewards_train/chosen': '0.05221', 'rewards_train/rejected': '0.026606', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025604', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-145.23', 'loss/train': '0.68874', 'examples_per_second': '30.289', 'grad_norm': '33.5', 'counters/examples': 83872, 'counters/updates': 2621}
train stats after 83904 examples: {'rewards_train/chosen': '0.1012', 'rewards_train/rejected': '0.0035203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097678', 'logps_train/rejected': '-121.11', 'logps_train/chosen': '-144.57', 'loss/train': '0.6526', 'examples_per_second': '32.036', 'grad_norm': '26.875', 'counters/examples': 83904, 'counters/updates': 2622}
train stats after 83936 examples: {'rewards_train/chosen': '0.10176', 'rewards_train/rejected': '0.017314', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084451', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-146.67', 'loss/train': '0.65646', 'examples_per_second': '32.082', 'grad_norm': '29.5', 'counters/examples': 83936, 'counters/updates': 2623}
train stats after 83968 examples: {'rewards_train/chosen': '0.10244', 'rewards_train/rejected': '0.046094', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056345', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-153.99', 'loss/train': '0.67618', 'examples_per_second': '30.929', 'grad_norm': '36.5', 'counters/examples': 83968, 'counters/updates': 2624}
skipping logging after 84000 examples to avoid logging too frequently
Running evaluation after 84000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 84000: {'rewards_eval/chosen': '0.11376', 'rewards_eval/rejected': '0.049063', 'rewards_eval/accuracies': '0.55859', 'rewards_eval/margins': '0.064697', 'logps_eval/rejected': '-121.64', 'logps_eval/chosen': '-142.97', 'loss/eval': '0.67024'}
skipping save for non epoch
train stats after 84032 examples: {'rewards_train/chosen': '0.10262', 'rewards_train/rejected': '0.03569', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06693', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-163.53', 'loss/train': '0.66581', 'examples_per_second': '31.393', 'grad_norm': '36', 'counters/examples': 84032, 'counters/updates': 2626}
train stats after 84064 examples: {'rewards_train/chosen': '0.016243', 'rewards_train/rejected': '0.014473', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0017697', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-164.66', 'loss/train': '0.69939', 'examples_per_second': '30.438', 'grad_norm': '31.875', 'counters/examples': 84064, 'counters/updates': 2627}
train stats after 84096 examples: {'rewards_train/chosen': '0.1324', 'rewards_train/rejected': '0.013312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11909', 'logps_train/rejected': '-157.4', 'logps_train/chosen': '-158.25', 'loss/train': '0.64427', 'examples_per_second': '31.827', 'grad_norm': '31', 'counters/examples': 84096, 'counters/updates': 2628}
train stats after 84128 examples: {'rewards_train/chosen': '0.086209', 'rewards_train/rejected': '0.015371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070838', 'logps_train/rejected': '-84.427', 'logps_train/chosen': '-141.83', 'loss/train': '0.66472', 'examples_per_second': '31.619', 'grad_norm': '25.875', 'counters/examples': 84128, 'counters/updates': 2629}
skipping logging after 84160 examples to avoid logging too frequently
train stats after 84192 examples: {'rewards_train/chosen': '0.075276', 'rewards_train/rejected': '-0.011509', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086785', 'logps_train/rejected': '-143.79', 'logps_train/chosen': '-105.42', 'loss/train': '0.65937', 'examples_per_second': '31.5', 'grad_norm': '34.25', 'counters/examples': 84192, 'counters/updates': 2631}
train stats after 84224 examples: {'rewards_train/chosen': '0.13816', 'rewards_train/rejected': '-0.020145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1583', 'logps_train/rejected': '-111.27', 'logps_train/chosen': '-172.91', 'loss/train': '0.63223', 'examples_per_second': '31.228', 'grad_norm': '27.75', 'counters/examples': 84224, 'counters/updates': 2632}
train stats after 84256 examples: {'rewards_train/chosen': '0.11665', 'rewards_train/rejected': '0.12803', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011378', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-103.8', 'loss/train': '0.71245', 'examples_per_second': '30.81', 'grad_norm': '32.25', 'counters/examples': 84256, 'counters/updates': 2633}
skipping logging after 84288 examples to avoid logging too frequently
train stats after 84320 examples: {'rewards_train/chosen': '0.095441', 'rewards_train/rejected': '0.053801', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041639', 'logps_train/rejected': '-120.36', 'logps_train/chosen': '-132.44', 'loss/train': '0.67563', 'examples_per_second': '31.453', 'grad_norm': '33.25', 'counters/examples': 84320, 'counters/updates': 2635}
train stats after 84352 examples: {'rewards_train/chosen': '0.083642', 'rewards_train/rejected': '0.0011121', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08253', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-158.49', 'loss/train': '0.65616', 'examples_per_second': '29.954', 'grad_norm': '35.5', 'counters/examples': 84352, 'counters/updates': 2636}
train stats after 84384 examples: {'rewards_train/chosen': '0.12677', 'rewards_train/rejected': '0.04747', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079299', 'logps_train/rejected': '-122.59', 'logps_train/chosen': '-174.7', 'loss/train': '0.65956', 'examples_per_second': '32.4', 'grad_norm': '30.125', 'counters/examples': 84384, 'counters/updates': 2637}
train stats after 84416 examples: {'rewards_train/chosen': '0.13081', 'rewards_train/rejected': '0.10201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028798', 'logps_train/rejected': '-140.01', 'logps_train/chosen': '-149.04', 'loss/train': '0.68927', 'examples_per_second': '30.32', 'grad_norm': '36.75', 'counters/examples': 84416, 'counters/updates': 2638}
train stats after 84448 examples: {'rewards_train/chosen': '0.067233', 'rewards_train/rejected': '0.10773', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.040501', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-159.16', 'loss/train': '0.73851', 'examples_per_second': '30.391', 'grad_norm': '48.25', 'counters/examples': 84448, 'counters/updates': 2639}
train stats after 84480 examples: {'rewards_train/chosen': '0.12675', 'rewards_train/rejected': '0.07466', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.05209', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-136.25', 'loss/train': '0.67664', 'examples_per_second': '29.966', 'grad_norm': '33.25', 'counters/examples': 84480, 'counters/updates': 2640}
train stats after 84512 examples: {'rewards_train/chosen': '0.15522', 'rewards_train/rejected': '0.06544', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089777', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-166.53', 'loss/train': '0.66749', 'examples_per_second': '33.17', 'grad_norm': '33.75', 'counters/examples': 84512, 'counters/updates': 2641}
train stats after 84544 examples: {'rewards_train/chosen': '0.02309', 'rewards_train/rejected': '0.088222', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.065132', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-168.76', 'loss/train': '0.74024', 'examples_per_second': '31.383', 'grad_norm': '48.75', 'counters/examples': 84544, 'counters/updates': 2642}
skipping logging after 84576 examples to avoid logging too frequently
train stats after 84608 examples: {'rewards_train/chosen': '0.10828', 'rewards_train/rejected': '0.13547', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.02719', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-112.19', 'loss/train': '0.7153', 'examples_per_second': '31.463', 'grad_norm': '31.875', 'counters/examples': 84608, 'counters/updates': 2644}
train stats after 84640 examples: {'rewards_train/chosen': '0.08906', 'rewards_train/rejected': '0.050704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038355', 'logps_train/rejected': '-112.63', 'logps_train/chosen': '-128.53', 'loss/train': '0.68315', 'examples_per_second': '31.794', 'grad_norm': '31.75', 'counters/examples': 84640, 'counters/updates': 2645}
train stats after 84672 examples: {'rewards_train/chosen': '0.11617', 'rewards_train/rejected': '0.043932', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072242', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-127.36', 'loss/train': '0.66309', 'examples_per_second': '30.324', 'grad_norm': '24.75', 'counters/examples': 84672, 'counters/updates': 2646}
skipping logging after 84704 examples to avoid logging too frequently
train stats after 84736 examples: {'rewards_train/chosen': '0.19906', 'rewards_train/rejected': '0.12268', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076377', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-144.18', 'loss/train': '0.66649', 'examples_per_second': '23.853', 'grad_norm': '34.75', 'counters/examples': 84736, 'counters/updates': 2648}
train stats after 84768 examples: {'rewards_train/chosen': '0.14272', 'rewards_train/rejected': '0.031465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11126', 'logps_train/rejected': '-93.019', 'logps_train/chosen': '-138.36', 'loss/train': '0.64718', 'examples_per_second': '30.736', 'grad_norm': '25.875', 'counters/examples': 84768, 'counters/updates': 2649}
train stats after 84800 examples: {'rewards_train/chosen': '0.12322', 'rewards_train/rejected': '0.018083', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10514', 'logps_train/rejected': '-129.97', 'logps_train/chosen': '-133.66', 'loss/train': '0.6501', 'examples_per_second': '31.173', 'grad_norm': '32.75', 'counters/examples': 84800, 'counters/updates': 2650}
train stats after 84832 examples: {'rewards_train/chosen': '0.085829', 'rewards_train/rejected': '0.099181', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013352', 'logps_train/rejected': '-94.109', 'logps_train/chosen': '-116.84', 'loss/train': '0.70494', 'examples_per_second': '23.862', 'grad_norm': '28.5', 'counters/examples': 84832, 'counters/updates': 2651}
train stats after 84864 examples: {'rewards_train/chosen': '0.15191', 'rewards_train/rejected': '0.026988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12492', 'logps_train/rejected': '-141.87', 'logps_train/chosen': '-140.75', 'loss/train': '0.64191', 'examples_per_second': '30.116', 'grad_norm': '35.75', 'counters/examples': 84864, 'counters/updates': 2652}
train stats after 84896 examples: {'rewards_train/chosen': '0.15955', 'rewards_train/rejected': '0.14337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.016175', 'logps_train/rejected': '-155.88', 'logps_train/chosen': '-148.89', 'loss/train': '0.69412', 'examples_per_second': '31.504', 'grad_norm': '40', 'counters/examples': 84896, 'counters/updates': 2653}
train stats after 84928 examples: {'rewards_train/chosen': '0.059267', 'rewards_train/rejected': '0.0070457', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052221', 'logps_train/rejected': '-99.319', 'logps_train/chosen': '-140.45', 'loss/train': '0.67738', 'examples_per_second': '31.606', 'grad_norm': '28.375', 'counters/examples': 84928, 'counters/updates': 2654}
train stats after 84960 examples: {'rewards_train/chosen': '0.039247', 'rewards_train/rejected': '0.049484', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.010238', 'logps_train/rejected': '-128.13', 'logps_train/chosen': '-132.73', 'loss/train': '0.7034', 'examples_per_second': '31.437', 'grad_norm': '27.75', 'counters/examples': 84960, 'counters/updates': 2655}
skipping logging after 84992 examples to avoid logging too frequently
train stats after 85024 examples: {'rewards_train/chosen': '0.11753', 'rewards_train/rejected': '0.054128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063402', 'logps_train/rejected': '-114.03', 'logps_train/chosen': '-135.34', 'loss/train': '0.67209', 'examples_per_second': '36.309', 'grad_norm': '29.875', 'counters/examples': 85024, 'counters/updates': 2657}
train stats after 85056 examples: {'rewards_train/chosen': '0.0879', 'rewards_train/rejected': '0.033291', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054609', 'logps_train/rejected': '-117.75', 'logps_train/chosen': '-159.6', 'loss/train': '0.67392', 'examples_per_second': '31.479', 'grad_norm': '29.75', 'counters/examples': 85056, 'counters/updates': 2658}
train stats after 85088 examples: {'rewards_train/chosen': '0.16384', 'rewards_train/rejected': '0.089914', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073923', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-113.96', 'loss/train': '0.66446', 'examples_per_second': '30.383', 'grad_norm': '27', 'counters/examples': 85088, 'counters/updates': 2659}
train stats after 85120 examples: {'rewards_train/chosen': '0.13205', 'rewards_train/rejected': '0.010811', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12124', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-175.78', 'loss/train': '0.63931', 'examples_per_second': '31.502', 'grad_norm': '42.25', 'counters/examples': 85120, 'counters/updates': 2660}
train stats after 85152 examples: {'rewards_train/chosen': '0.060454', 'rewards_train/rejected': '0.058618', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0018368', 'logps_train/rejected': '-129.67', 'logps_train/chosen': '-121.12', 'loss/train': '0.70001', 'examples_per_second': '31.426', 'grad_norm': '31.75', 'counters/examples': 85152, 'counters/updates': 2661}
train stats after 85184 examples: {'rewards_train/chosen': '0.048875', 'rewards_train/rejected': '0.054594', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0057191', 'logps_train/rejected': '-143.19', 'logps_train/chosen': '-123.16', 'loss/train': '0.70215', 'examples_per_second': '31.632', 'grad_norm': '51.5', 'counters/examples': 85184, 'counters/updates': 2662}
train stats after 85216 examples: {'rewards_train/chosen': '0.11477', 'rewards_train/rejected': '0.069165', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045604', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-137.43', 'loss/train': '0.67496', 'examples_per_second': '31.409', 'grad_norm': '41.75', 'counters/examples': 85216, 'counters/updates': 2663}
train stats after 85248 examples: {'rewards_train/chosen': '0.11245', 'rewards_train/rejected': '0.090359', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022092', 'logps_train/rejected': '-133.72', 'logps_train/chosen': '-150.74', 'loss/train': '0.69745', 'examples_per_second': '32.194', 'grad_norm': '40.75', 'counters/examples': 85248, 'counters/updates': 2664}
train stats after 85280 examples: {'rewards_train/chosen': '0.12195', 'rewards_train/rejected': '0.029498', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092448', 'logps_train/rejected': '-128.49', 'logps_train/chosen': '-146.82', 'loss/train': '0.65427', 'examples_per_second': '29.955', 'grad_norm': '35', 'counters/examples': 85280, 'counters/updates': 2665}
train stats after 85312 examples: {'rewards_train/chosen': '0.060712', 'rewards_train/rejected': '0.0653', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0045889', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-159.26', 'loss/train': '0.70733', 'examples_per_second': '31.788', 'grad_norm': '34.5', 'counters/examples': 85312, 'counters/updates': 2666}
skipping logging after 85344 examples to avoid logging too frequently
train stats after 85376 examples: {'rewards_train/chosen': '0.06108', 'rewards_train/rejected': '0.0074556', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053625', 'logps_train/rejected': '-166.69', 'logps_train/chosen': '-145.35', 'loss/train': '0.67652', 'examples_per_second': '32.343', 'grad_norm': '28.625', 'counters/examples': 85376, 'counters/updates': 2668}
train stats after 85408 examples: {'rewards_train/chosen': '0.03451', 'rewards_train/rejected': '0.030162', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0043481', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-171.18', 'loss/train': '0.69479', 'examples_per_second': '31.502', 'grad_norm': '34.25', 'counters/examples': 85408, 'counters/updates': 2669}
train stats after 85440 examples: {'rewards_train/chosen': '0.10183', 'rewards_train/rejected': '0.041054', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.060771', 'logps_train/rejected': '-107.53', 'logps_train/chosen': '-141.79', 'loss/train': '0.67158', 'examples_per_second': '32.692', 'grad_norm': '29.875', 'counters/examples': 85440, 'counters/updates': 2670}
train stats after 85472 examples: {'rewards_train/chosen': '0.1674', 'rewards_train/rejected': '0.030644', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13676', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-162.38', 'loss/train': '0.64195', 'examples_per_second': '30.834', 'grad_norm': '29.5', 'counters/examples': 85472, 'counters/updates': 2671}
skipping logging after 85504 examples to avoid logging too frequently
train stats after 85536 examples: {'rewards_train/chosen': '0.13622', 'rewards_train/rejected': '0.12107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01515', 'logps_train/rejected': '-124.98', 'logps_train/chosen': '-160.5', 'loss/train': '0.69639', 'examples_per_second': '31.567', 'grad_norm': '33.75', 'counters/examples': 85536, 'counters/updates': 2673}
train stats after 85568 examples: {'rewards_train/chosen': '0.022611', 'rewards_train/rejected': '0.016919', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056929', 'logps_train/rejected': '-94.325', 'logps_train/chosen': '-108.02', 'loss/train': '0.69656', 'examples_per_second': '31.458', 'grad_norm': '27.625', 'counters/examples': 85568, 'counters/updates': 2674}
train stats after 85600 examples: {'rewards_train/chosen': '0.10175', 'rewards_train/rejected': '0.062094', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039657', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-136.37', 'loss/train': '0.68572', 'examples_per_second': '31.496', 'grad_norm': '37', 'counters/examples': 85600, 'counters/updates': 2675}
train stats after 85632 examples: {'rewards_train/chosen': '0.11949', 'rewards_train/rejected': '0.11105', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0084365', 'logps_train/rejected': '-145.99', 'logps_train/chosen': '-156.23', 'loss/train': '0.6945', 'examples_per_second': '31.316', 'grad_norm': '32.25', 'counters/examples': 85632, 'counters/updates': 2676}
train stats after 85664 examples: {'rewards_train/chosen': '0.10496', 'rewards_train/rejected': '0.13497', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030008', 'logps_train/rejected': '-153.07', 'logps_train/chosen': '-205.69', 'loss/train': '0.7228', 'examples_per_second': '31.405', 'grad_norm': '44.5', 'counters/examples': 85664, 'counters/updates': 2677}
train stats after 85696 examples: {'rewards_train/chosen': '0.069991', 'rewards_train/rejected': '-0.0014759', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071467', 'logps_train/rejected': '-118.71', 'logps_train/chosen': '-156.35', 'loss/train': '0.66462', 'examples_per_second': '32.239', 'grad_norm': '36.5', 'counters/examples': 85696, 'counters/updates': 2678}
skipping logging after 85728 examples to avoid logging too frequently
train stats after 85760 examples: {'rewards_train/chosen': '0.091701', 'rewards_train/rejected': '0.055362', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036339', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-144.81', 'loss/train': '0.68452', 'examples_per_second': '31.437', 'grad_norm': '45', 'counters/examples': 85760, 'counters/updates': 2680}
skipping logging after 85792 examples to avoid logging too frequently
train stats after 85824 examples: {'rewards_train/chosen': '0.12942', 'rewards_train/rejected': '-0.014509', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14393', 'logps_train/rejected': '-109.63', 'logps_train/chosen': '-156.17', 'loss/train': '0.63122', 'examples_per_second': '30.038', 'grad_norm': '26.875', 'counters/examples': 85824, 'counters/updates': 2682}
train stats after 85856 examples: {'rewards_train/chosen': '0.041152', 'rewards_train/rejected': '0.035902', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0052496', 'logps_train/rejected': '-147.34', 'logps_train/chosen': '-166.07', 'loss/train': '0.69645', 'examples_per_second': '30.024', 'grad_norm': '35.5', 'counters/examples': 85856, 'counters/updates': 2683}
train stats after 85888 examples: {'rewards_train/chosen': '0.17522', 'rewards_train/rejected': '0.12082', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054402', 'logps_train/rejected': '-109.41', 'logps_train/chosen': '-157.67', 'loss/train': '0.67425', 'examples_per_second': '32.427', 'grad_norm': '30.375', 'counters/examples': 85888, 'counters/updates': 2684}
train stats after 85920 examples: {'rewards_train/chosen': '0.16324', 'rewards_train/rejected': '0.06705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096192', 'logps_train/rejected': '-121.02', 'logps_train/chosen': '-162.22', 'loss/train': '0.65769', 'examples_per_second': '30.672', 'grad_norm': '30.125', 'counters/examples': 85920, 'counters/updates': 2685}
train stats after 85952 examples: {'rewards_train/chosen': '0.10771', 'rewards_train/rejected': '0.042655', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06506', 'logps_train/rejected': '-130.39', 'logps_train/chosen': '-151.78', 'loss/train': '0.6709', 'examples_per_second': '31.149', 'grad_norm': '28.125', 'counters/examples': 85952, 'counters/updates': 2686}
train stats after 85984 examples: {'rewards_train/chosen': '0.047108', 'rewards_train/rejected': '0.02641', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.020698', 'logps_train/rejected': '-151.24', 'logps_train/chosen': '-131.2', 'loss/train': '0.70284', 'examples_per_second': '31.394', 'grad_norm': '52.75', 'counters/examples': 85984, 'counters/updates': 2687}
train stats after 86016 examples: {'rewards_train/chosen': '0.050902', 'rewards_train/rejected': '0.044804', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0060986', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-153.42', 'loss/train': '0.7041', 'examples_per_second': '31.287', 'grad_norm': '38', 'counters/examples': 86016, 'counters/updates': 2688}
train stats after 86048 examples: {'rewards_train/chosen': '0.11098', 'rewards_train/rejected': '0.026295', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084689', 'logps_train/rejected': '-152.25', 'logps_train/chosen': '-141.38', 'loss/train': '0.6621', 'examples_per_second': '31.415', 'grad_norm': '29.375', 'counters/examples': 86048, 'counters/updates': 2689}
train stats after 86080 examples: {'rewards_train/chosen': '0.10141', 'rewards_train/rejected': '-0.0032228', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10463', 'logps_train/rejected': '-182.59', 'logps_train/chosen': '-193.27', 'loss/train': '0.65735', 'examples_per_second': '29.918', 'grad_norm': '32.75', 'counters/examples': 86080, 'counters/updates': 2690}
train stats after 86112 examples: {'rewards_train/chosen': '-0.0023837', 'rewards_train/rejected': '0.087491', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.089875', 'logps_train/rejected': '-119.52', 'logps_train/chosen': '-125.93', 'loss/train': '0.77102', 'examples_per_second': '31.135', 'grad_norm': '47', 'counters/examples': 86112, 'counters/updates': 2691}
train stats after 86144 examples: {'rewards_train/chosen': '0.093076', 'rewards_train/rejected': '0.073564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019511', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-152.71', 'loss/train': '0.68941', 'examples_per_second': '33.042', 'grad_norm': '28.25', 'counters/examples': 86144, 'counters/updates': 2692}
train stats after 86176 examples: {'rewards_train/chosen': '0.10409', 'rewards_train/rejected': '0.079377', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024711', 'logps_train/rejected': '-126.42', 'logps_train/chosen': '-143.05', 'loss/train': '0.69543', 'examples_per_second': '31.651', 'grad_norm': '41', 'counters/examples': 86176, 'counters/updates': 2693}
train stats after 86208 examples: {'rewards_train/chosen': '0.089889', 'rewards_train/rejected': '0.090986', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0010969', 'logps_train/rejected': '-104.27', 'logps_train/chosen': '-129.63', 'loss/train': '0.70328', 'examples_per_second': '32.442', 'grad_norm': '40.25', 'counters/examples': 86208, 'counters/updates': 2694}
train stats after 86240 examples: {'rewards_train/chosen': '0.065389', 'rewards_train/rejected': '0.037615', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027773', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-118.76', 'loss/train': '0.68631', 'examples_per_second': '31.541', 'grad_norm': '29.25', 'counters/examples': 86240, 'counters/updates': 2695}
skipping logging after 86272 examples to avoid logging too frequently
train stats after 86304 examples: {'rewards_train/chosen': '0.059717', 'rewards_train/rejected': '0.10307', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043352', 'logps_train/rejected': '-130.69', 'logps_train/chosen': '-114.19', 'loss/train': '0.72871', 'examples_per_second': '30.868', 'grad_norm': '36', 'counters/examples': 86304, 'counters/updates': 2697}
train stats after 86336 examples: {'rewards_train/chosen': '0.094408', 'rewards_train/rejected': '0.076168', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01824', 'logps_train/rejected': '-155.97', 'logps_train/chosen': '-112.21', 'loss/train': '0.68883', 'examples_per_second': '32.667', 'grad_norm': '36', 'counters/examples': 86336, 'counters/updates': 2698}
train stats after 86368 examples: {'rewards_train/chosen': '0.12823', 'rewards_train/rejected': '0.061315', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066911', 'logps_train/rejected': '-155.98', 'logps_train/chosen': '-174.38', 'loss/train': '0.67617', 'examples_per_second': '31.472', 'grad_norm': '37', 'counters/examples': 86368, 'counters/updates': 2699}
train stats after 86400 examples: {'rewards_train/chosen': '0.074532', 'rewards_train/rejected': '0.047984', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.026548', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-124.91', 'loss/train': '0.68701', 'examples_per_second': '30.414', 'grad_norm': '29', 'counters/examples': 86400, 'counters/updates': 2700}
train stats after 86432 examples: {'rewards_train/chosen': '0.10818', 'rewards_train/rejected': '0.088893', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019284', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-136.27', 'loss/train': '0.68814', 'examples_per_second': '31.459', 'grad_norm': '39', 'counters/examples': 86432, 'counters/updates': 2701}
train stats after 86464 examples: {'rewards_train/chosen': '0.1803', 'rewards_train/rejected': '0.0061065', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1742', 'logps_train/rejected': '-103.17', 'logps_train/chosen': '-163.98', 'loss/train': '0.61975', 'examples_per_second': '24.659', 'grad_norm': '25.875', 'counters/examples': 86464, 'counters/updates': 2702}
train stats after 86496 examples: {'rewards_train/chosen': '0.082459', 'rewards_train/rejected': '0.026134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056325', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-160.94', 'loss/train': '0.67403', 'examples_per_second': '31.5', 'grad_norm': '32', 'counters/examples': 86496, 'counters/updates': 2703}
train stats after 86528 examples: {'rewards_train/chosen': '0.13273', 'rewards_train/rejected': '0.10081', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.03192', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-129.24', 'loss/train': '0.683', 'examples_per_second': '31.451', 'grad_norm': '28.25', 'counters/examples': 86528, 'counters/updates': 2704}
train stats after 86560 examples: {'rewards_train/chosen': '0.12283', 'rewards_train/rejected': '0.048042', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074783', 'logps_train/rejected': '-140.17', 'logps_train/chosen': '-146.05', 'loss/train': '0.67399', 'examples_per_second': '31.015', 'grad_norm': '30', 'counters/examples': 86560, 'counters/updates': 2705}
skipping logging after 86592 examples to avoid logging too frequently
train stats after 86624 examples: {'rewards_train/chosen': '0.095048', 'rewards_train/rejected': '0.070446', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024602', 'logps_train/rejected': '-126.9', 'logps_train/chosen': '-174.14', 'loss/train': '0.68482', 'examples_per_second': '30.923', 'grad_norm': '42', 'counters/examples': 86624, 'counters/updates': 2707}
train stats after 86656 examples: {'rewards_train/chosen': '0.12892', 'rewards_train/rejected': '0.038129', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090792', 'logps_train/rejected': '-176.12', 'logps_train/chosen': '-183.75', 'loss/train': '0.66936', 'examples_per_second': '31.428', 'grad_norm': '62.25', 'counters/examples': 86656, 'counters/updates': 2708}
train stats after 86688 examples: {'rewards_train/chosen': '0.054711', 'rewards_train/rejected': '0.047461', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0072508', 'logps_train/rejected': '-109.68', 'logps_train/chosen': '-117.44', 'loss/train': '0.69409', 'examples_per_second': '30.912', 'grad_norm': '39.25', 'counters/examples': 86688, 'counters/updates': 2709}
train stats after 86720 examples: {'rewards_train/chosen': '0.08845', 'rewards_train/rejected': '0.032861', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.055589', 'logps_train/rejected': '-113.81', 'logps_train/chosen': '-119.44', 'loss/train': '0.67599', 'examples_per_second': '29.805', 'grad_norm': '25.5', 'counters/examples': 86720, 'counters/updates': 2710}
train stats after 86752 examples: {'rewards_train/chosen': '0.12857', 'rewards_train/rejected': '0.094828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033746', 'logps_train/rejected': '-120.63', 'logps_train/chosen': '-128.8', 'loss/train': '0.68473', 'examples_per_second': '30.166', 'grad_norm': '32.75', 'counters/examples': 86752, 'counters/updates': 2711}
train stats after 86784 examples: {'rewards_train/chosen': '0.085101', 'rewards_train/rejected': '0.14012', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.055017', 'logps_train/rejected': '-149.27', 'logps_train/chosen': '-119.38', 'loss/train': '0.72737', 'examples_per_second': '29.864', 'grad_norm': '37', 'counters/examples': 86784, 'counters/updates': 2712}
train stats after 86816 examples: {'rewards_train/chosen': '0.17915', 'rewards_train/rejected': '-0.0064987', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18565', 'logps_train/rejected': '-90.785', 'logps_train/chosen': '-155.09', 'loss/train': '0.63197', 'examples_per_second': '31.469', 'grad_norm': '28', 'counters/examples': 86816, 'counters/updates': 2713}
train stats after 86848 examples: {'rewards_train/chosen': '0.059121', 'rewards_train/rejected': '0.082293', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023171', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-138.83', 'loss/train': '0.71636', 'examples_per_second': '32.251', 'grad_norm': '30.25', 'counters/examples': 86848, 'counters/updates': 2714}
train stats after 86880 examples: {'rewards_train/chosen': '0.1986', 'rewards_train/rejected': '0.091187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10741', 'logps_train/rejected': '-168.92', 'logps_train/chosen': '-206.63', 'loss/train': '0.65346', 'examples_per_second': '31.458', 'grad_norm': '38.5', 'counters/examples': 86880, 'counters/updates': 2715}
train stats after 86912 examples: {'rewards_train/chosen': '0.10119', 'rewards_train/rejected': '0.060485', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.040701', 'logps_train/rejected': '-139.49', 'logps_train/chosen': '-115.97', 'loss/train': '0.67892', 'examples_per_second': '32.716', 'grad_norm': '28.25', 'counters/examples': 86912, 'counters/updates': 2716}
train stats after 86944 examples: {'rewards_train/chosen': '0.11762', 'rewards_train/rejected': '0.062534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05509', 'logps_train/rejected': '-152.84', 'logps_train/chosen': '-150.68', 'loss/train': '0.6753', 'examples_per_second': '31.079', 'grad_norm': '35.25', 'counters/examples': 86944, 'counters/updates': 2717}
skipping logging after 86976 examples to avoid logging too frequently
train stats after 87008 examples: {'rewards_train/chosen': '0.057565', 'rewards_train/rejected': '0.04868', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0088845', 'logps_train/rejected': '-178.99', 'logps_train/chosen': '-149.44', 'loss/train': '0.69434', 'examples_per_second': '31.186', 'grad_norm': '41.25', 'counters/examples': 87008, 'counters/updates': 2719}
skipping logging after 87040 examples to avoid logging too frequently
train stats after 87072 examples: {'rewards_train/chosen': '0.058062', 'rewards_train/rejected': '0.023326', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.034736', 'logps_train/rejected': '-143.93', 'logps_train/chosen': '-103.3', 'loss/train': '0.67931', 'examples_per_second': '31.228', 'grad_norm': '28.625', 'counters/examples': 87072, 'counters/updates': 2721}
train stats after 87104 examples: {'rewards_train/chosen': '0.09361', 'rewards_train/rejected': '0.0026416', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090968', 'logps_train/rejected': '-123.88', 'logps_train/chosen': '-158.27', 'loss/train': '0.65414', 'examples_per_second': '31.382', 'grad_norm': '32', 'counters/examples': 87104, 'counters/updates': 2722}
train stats after 87136 examples: {'rewards_train/chosen': '0.11253', 'rewards_train/rejected': '0.11064', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0018887', 'logps_train/rejected': '-150.25', 'logps_train/chosen': '-126.3', 'loss/train': '0.69893', 'examples_per_second': '29.985', 'grad_norm': '27.75', 'counters/examples': 87136, 'counters/updates': 2723}
train stats after 87168 examples: {'rewards_train/chosen': '0.079181', 'rewards_train/rejected': '0.019017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060164', 'logps_train/rejected': '-158.35', 'logps_train/chosen': '-131.48', 'loss/train': '0.67294', 'examples_per_second': '30.774', 'grad_norm': '31.875', 'counters/examples': 87168, 'counters/updates': 2724}
train stats after 87200 examples: {'rewards_train/chosen': '0.081656', 'rewards_train/rejected': '0.04194', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039716', 'logps_train/rejected': '-132.93', 'logps_train/chosen': '-140.74', 'loss/train': '0.67885', 'examples_per_second': '32.05', 'grad_norm': '28.75', 'counters/examples': 87200, 'counters/updates': 2725}
skipping logging after 87232 examples to avoid logging too frequently
train stats after 87264 examples: {'rewards_train/chosen': '0.044069', 'rewards_train/rejected': '0.071903', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.027834', 'logps_train/rejected': '-152.49', 'logps_train/chosen': '-171.06', 'loss/train': '0.71403', 'examples_per_second': '31.275', 'grad_norm': '33', 'counters/examples': 87264, 'counters/updates': 2727}
skipping logging after 87296 examples to avoid logging too frequently
train stats after 87328 examples: {'rewards_train/chosen': '0.083231', 'rewards_train/rejected': '0.089975', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0067443', 'logps_train/rejected': '-129.8', 'logps_train/chosen': '-124.17', 'loss/train': '0.70398', 'examples_per_second': '32.126', 'grad_norm': '44.5', 'counters/examples': 87328, 'counters/updates': 2729}
train stats after 87360 examples: {'rewards_train/chosen': '0.063988', 'rewards_train/rejected': '0.011563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052425', 'logps_train/rejected': '-89.526', 'logps_train/chosen': '-136.35', 'loss/train': '0.67148', 'examples_per_second': '30.076', 'grad_norm': '26.25', 'counters/examples': 87360, 'counters/updates': 2730}
train stats after 87392 examples: {'rewards_train/chosen': '0.14115', 'rewards_train/rejected': '0.069565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071586', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-143.91', 'loss/train': '0.66271', 'examples_per_second': '31.888', 'grad_norm': '35', 'counters/examples': 87392, 'counters/updates': 2731}
train stats after 87424 examples: {'rewards_train/chosen': '0.046679', 'rewards_train/rejected': '0.077975', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.031295', 'logps_train/rejected': '-150.24', 'logps_train/chosen': '-119.73', 'loss/train': '0.71475', 'examples_per_second': '32.009', 'grad_norm': '35.25', 'counters/examples': 87424, 'counters/updates': 2732}
train stats after 87456 examples: {'rewards_train/chosen': '0.067321', 'rewards_train/rejected': '0.10347', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036154', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-166.07', 'loss/train': '0.72536', 'examples_per_second': '32.548', 'grad_norm': '45', 'counters/examples': 87456, 'counters/updates': 2733}
skipping logging after 87488 examples to avoid logging too frequently
train stats after 87520 examples: {'rewards_train/chosen': '0.059906', 'rewards_train/rejected': '0.13647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.076569', 'logps_train/rejected': '-134.41', 'logps_train/chosen': '-143.52', 'loss/train': '0.76883', 'examples_per_second': '31.324', 'grad_norm': '87.5', 'counters/examples': 87520, 'counters/updates': 2735}
train stats after 87552 examples: {'rewards_train/chosen': '0.12157', 'rewards_train/rejected': '0.063045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058528', 'logps_train/rejected': '-134.31', 'logps_train/chosen': '-145.67', 'loss/train': '0.672', 'examples_per_second': '31.979', 'grad_norm': '37.75', 'counters/examples': 87552, 'counters/updates': 2736}
train stats after 87584 examples: {'rewards_train/chosen': '0.062191', 'rewards_train/rejected': '0.087272', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.025081', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-148.86', 'loss/train': '0.717', 'examples_per_second': '31.161', 'grad_norm': '49.25', 'counters/examples': 87584, 'counters/updates': 2737}
train stats after 87616 examples: {'rewards_train/chosen': '0.10353', 'rewards_train/rejected': '0.062796', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040731', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-127.08', 'loss/train': '0.67769', 'examples_per_second': '30.459', 'grad_norm': '30.875', 'counters/examples': 87616, 'counters/updates': 2738}
train stats after 87648 examples: {'rewards_train/chosen': '0.10035', 'rewards_train/rejected': '-0.025705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12605', 'logps_train/rejected': '-97.564', 'logps_train/chosen': '-124.56', 'loss/train': '0.63842', 'examples_per_second': '31.448', 'grad_norm': '28.875', 'counters/examples': 87648, 'counters/updates': 2739}
train stats after 87680 examples: {'rewards_train/chosen': '0.16169', 'rewards_train/rejected': '0.078802', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08289', 'logps_train/rejected': '-135.19', 'logps_train/chosen': '-139.01', 'loss/train': '0.66013', 'examples_per_second': '31.453', 'grad_norm': '37', 'counters/examples': 87680, 'counters/updates': 2740}
skipping logging after 87712 examples to avoid logging too frequently
train stats after 87744 examples: {'rewards_train/chosen': '0.16784', 'rewards_train/rejected': '0.036104', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13173', 'logps_train/rejected': '-104.77', 'logps_train/chosen': '-172.78', 'loss/train': '0.64446', 'examples_per_second': '32.478', 'grad_norm': '29.875', 'counters/examples': 87744, 'counters/updates': 2742}
train stats after 87776 examples: {'rewards_train/chosen': '0.040801', 'rewards_train/rejected': '0.051508', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.010707', 'logps_train/rejected': '-117.65', 'logps_train/chosen': '-122.81', 'loss/train': '0.70457', 'examples_per_second': '31.561', 'grad_norm': '27', 'counters/examples': 87776, 'counters/updates': 2743}
skipping logging after 87808 examples to avoid logging too frequently
train stats after 87840 examples: {'rewards_train/chosen': '0.098121', 'rewards_train/rejected': '-0.0067234', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10484', 'logps_train/rejected': '-145.26', 'logps_train/chosen': '-132.95', 'loss/train': '0.64487', 'examples_per_second': '38.708', 'grad_norm': '26.25', 'counters/examples': 87840, 'counters/updates': 2745}
skipping logging after 87872 examples to avoid logging too frequently
train stats after 87904 examples: {'rewards_train/chosen': '0.15909', 'rewards_train/rejected': '0.074128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084965', 'logps_train/rejected': '-107.41', 'logps_train/chosen': '-115.4', 'loss/train': '0.664', 'examples_per_second': '31.46', 'grad_norm': '23.375', 'counters/examples': 87904, 'counters/updates': 2747}
train stats after 87936 examples: {'rewards_train/chosen': '0.12649', 'rewards_train/rejected': '0.069871', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056621', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-130.89', 'loss/train': '0.67074', 'examples_per_second': '29.908', 'grad_norm': '30.375', 'counters/examples': 87936, 'counters/updates': 2748}
train stats after 87968 examples: {'rewards_train/chosen': '0.1022', 'rewards_train/rejected': '-0.044277', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14648', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-140.31', 'loss/train': '0.65522', 'examples_per_second': '30.234', 'grad_norm': '30', 'counters/examples': 87968, 'counters/updates': 2749}
skipping logging after 88000 examples to avoid logging too frequently
train stats after 88032 examples: {'rewards_train/chosen': '0.1518', 'rewards_train/rejected': '0.054161', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097637', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-149.85', 'loss/train': '0.65386', 'examples_per_second': '31.4', 'grad_norm': '47.25', 'counters/examples': 88032, 'counters/updates': 2751}
train stats after 88064 examples: {'rewards_train/chosen': '0.15074', 'rewards_train/rejected': '0.029248', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12149', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-163.78', 'loss/train': '0.6416', 'examples_per_second': '32.232', 'grad_norm': '25.625', 'counters/examples': 88064, 'counters/updates': 2752}
train stats after 88096 examples: {'rewards_train/chosen': '0.027997', 'rewards_train/rejected': '0.019634', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0083631', 'logps_train/rejected': '-97.464', 'logps_train/chosen': '-124.3', 'loss/train': '0.69381', 'examples_per_second': '32.566', 'grad_norm': '26.125', 'counters/examples': 88096, 'counters/updates': 2753}
train stats after 88128 examples: {'rewards_train/chosen': '0.0018422', 'rewards_train/rejected': '0.077836', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.075994', 'logps_train/rejected': '-119.04', 'logps_train/chosen': '-134.84', 'loss/train': '0.74061', 'examples_per_second': '31.538', 'grad_norm': '52.75', 'counters/examples': 88128, 'counters/updates': 2754}
train stats after 88160 examples: {'rewards_train/chosen': '0.062495', 'rewards_train/rejected': '0.014977', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047517', 'logps_train/rejected': '-127.09', 'logps_train/chosen': '-184.81', 'loss/train': '0.67558', 'examples_per_second': '31.474', 'grad_norm': '48.75', 'counters/examples': 88160, 'counters/updates': 2755}
train stats after 88192 examples: {'rewards_train/chosen': '0.078878', 'rewards_train/rejected': '0.016893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061985', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-114.26', 'loss/train': '0.66781', 'examples_per_second': '29.876', 'grad_norm': '25.75', 'counters/examples': 88192, 'counters/updates': 2756}
train stats after 88224 examples: {'rewards_train/chosen': '0.12515', 'rewards_train/rejected': '0.070386', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05476', 'logps_train/rejected': '-100.43', 'logps_train/chosen': '-127.54', 'loss/train': '0.67418', 'examples_per_second': '31.446', 'grad_norm': '30.375', 'counters/examples': 88224, 'counters/updates': 2757}
train stats after 88256 examples: {'rewards_train/chosen': '0.083402', 'rewards_train/rejected': '-0.014316', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097719', 'logps_train/rejected': '-105.69', 'logps_train/chosen': '-141.65', 'loss/train': '0.65005', 'examples_per_second': '31.777', 'grad_norm': '30.375', 'counters/examples': 88256, 'counters/updates': 2758}
train stats after 88288 examples: {'rewards_train/chosen': '0.18811', 'rewards_train/rejected': '0.052179', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13593', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-158.3', 'loss/train': '0.64407', 'examples_per_second': '31.162', 'grad_norm': '41.75', 'counters/examples': 88288, 'counters/updates': 2759}
train stats after 88320 examples: {'rewards_train/chosen': '0.092718', 'rewards_train/rejected': '0.095913', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.003195', 'logps_train/rejected': '-145.31', 'logps_train/chosen': '-128.35', 'loss/train': '0.70165', 'examples_per_second': '30.881', 'grad_norm': '33.75', 'counters/examples': 88320, 'counters/updates': 2760}
train stats after 88352 examples: {'rewards_train/chosen': '0.10876', 'rewards_train/rejected': '-0.0089567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11772', 'logps_train/rejected': '-116.41', 'logps_train/chosen': '-97.866', 'loss/train': '0.64546', 'examples_per_second': '31.254', 'grad_norm': '27.375', 'counters/examples': 88352, 'counters/updates': 2761}
train stats after 88384 examples: {'rewards_train/chosen': '0.041377', 'rewards_train/rejected': '0.1095', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.068126', 'logps_train/rejected': '-124.57', 'logps_train/chosen': '-131.95', 'loss/train': '0.73411', 'examples_per_second': '29.981', 'grad_norm': '48.75', 'counters/examples': 88384, 'counters/updates': 2762}
train stats after 88416 examples: {'rewards_train/chosen': '0.1045', 'rewards_train/rejected': '0.059761', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044741', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-174.53', 'loss/train': '0.67823', 'examples_per_second': '31.476', 'grad_norm': '32', 'counters/examples': 88416, 'counters/updates': 2763}
train stats after 88448 examples: {'rewards_train/chosen': '0.080711', 'rewards_train/rejected': '-0.015641', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096351', 'logps_train/rejected': '-117.39', 'logps_train/chosen': '-143.45', 'loss/train': '0.65475', 'examples_per_second': '32.758', 'grad_norm': '31.5', 'counters/examples': 88448, 'counters/updates': 2764}
train stats after 88480 examples: {'rewards_train/chosen': '0.13997', 'rewards_train/rejected': '0.052429', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087537', 'logps_train/rejected': '-128.93', 'logps_train/chosen': '-170.1', 'loss/train': '0.65757', 'examples_per_second': '30.556', 'grad_norm': '38.25', 'counters/examples': 88480, 'counters/updates': 2765}
train stats after 88512 examples: {'rewards_train/chosen': '0.12484', 'rewards_train/rejected': '0.059975', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.064865', 'logps_train/rejected': '-151.56', 'logps_train/chosen': '-171.94', 'loss/train': '0.67433', 'examples_per_second': '29.966', 'grad_norm': '33.5', 'counters/examples': 88512, 'counters/updates': 2766}
train stats after 88544 examples: {'rewards_train/chosen': '0.09929', 'rewards_train/rejected': '-0.015515', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11481', 'logps_train/rejected': '-126.12', 'logps_train/chosen': '-137.93', 'loss/train': '0.64975', 'examples_per_second': '31.502', 'grad_norm': '24.375', 'counters/examples': 88544, 'counters/updates': 2767}
train stats after 88576 examples: {'rewards_train/chosen': '0.12949', 'rewards_train/rejected': '-0.023202', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15269', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-160.87', 'loss/train': '0.62622', 'examples_per_second': '31.535', 'grad_norm': '32.5', 'counters/examples': 88576, 'counters/updates': 2768}
train stats after 88608 examples: {'rewards_train/chosen': '0.17287', 'rewards_train/rejected': '0.076442', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096425', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-146.19', 'loss/train': '0.68127', 'examples_per_second': '32.948', 'grad_norm': '28.75', 'counters/examples': 88608, 'counters/updates': 2769}
train stats after 88640 examples: {'rewards_train/chosen': '0.11456', 'rewards_train/rejected': '0.018118', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096445', 'logps_train/rejected': '-118.71', 'logps_train/chosen': '-144.64', 'loss/train': '0.65632', 'examples_per_second': '30.406', 'grad_norm': '45.75', 'counters/examples': 88640, 'counters/updates': 2770}
skipping logging after 88672 examples to avoid logging too frequently
train stats after 88704 examples: {'rewards_train/chosen': '0.078592', 'rewards_train/rejected': '0.0088764', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069716', 'logps_train/rejected': '-112.42', 'logps_train/chosen': '-131.15', 'loss/train': '0.66654', 'examples_per_second': '33.65', 'grad_norm': '24.875', 'counters/examples': 88704, 'counters/updates': 2772}
skipping logging after 88736 examples to avoid logging too frequently
train stats after 88768 examples: {'rewards_train/chosen': '0.10275', 'rewards_train/rejected': '0.04096', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061785', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-131.37', 'loss/train': '0.66691', 'examples_per_second': '31.525', 'grad_norm': '30.875', 'counters/examples': 88768, 'counters/updates': 2774}
train stats after 88800 examples: {'rewards_train/chosen': '0.17312', 'rewards_train/rejected': '0.056436', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11669', 'logps_train/rejected': '-137.83', 'logps_train/chosen': '-137.47', 'loss/train': '0.6474', 'examples_per_second': '30.193', 'grad_norm': '33', 'counters/examples': 88800, 'counters/updates': 2775}
train stats after 88832 examples: {'rewards_train/chosen': '0.12798', 'rewards_train/rejected': '0.020078', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10791', 'logps_train/rejected': '-97.28', 'logps_train/chosen': '-150.53', 'loss/train': '0.65254', 'examples_per_second': '32.633', 'grad_norm': '28.25', 'counters/examples': 88832, 'counters/updates': 2776}
train stats after 88864 examples: {'rewards_train/chosen': '0.19105', 'rewards_train/rejected': '0.06692', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12413', 'logps_train/rejected': '-146.19', 'logps_train/chosen': '-151.86', 'loss/train': '0.64186', 'examples_per_second': '31.202', 'grad_norm': '49.5', 'counters/examples': 88864, 'counters/updates': 2777}
train stats after 88896 examples: {'rewards_train/chosen': '0.10218', 'rewards_train/rejected': '0.031783', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070392', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-135.88', 'loss/train': '0.6641', 'examples_per_second': '31.095', 'grad_norm': '36.75', 'counters/examples': 88896, 'counters/updates': 2778}
train stats after 88928 examples: {'rewards_train/chosen': '0.12991', 'rewards_train/rejected': '0.054858', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075052', 'logps_train/rejected': '-151.43', 'logps_train/chosen': '-153.88', 'loss/train': '0.66916', 'examples_per_second': '30.498', 'grad_norm': '31.5', 'counters/examples': 88928, 'counters/updates': 2779}
skipping logging after 88960 examples to avoid logging too frequently
train stats after 88992 examples: {'rewards_train/chosen': '0.12645', 'rewards_train/rejected': '0.15031', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023857', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-137.58', 'loss/train': '0.73349', 'examples_per_second': '31.514', 'grad_norm': '43.75', 'counters/examples': 88992, 'counters/updates': 2781}
train stats after 89024 examples: {'rewards_train/chosen': '0.063258', 'rewards_train/rejected': '0.092262', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.029003', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-123.95', 'loss/train': '0.71641', 'examples_per_second': '32.532', 'grad_norm': '49.5', 'counters/examples': 89024, 'counters/updates': 2782}
train stats after 89056 examples: {'rewards_train/chosen': '0.19587', 'rewards_train/rejected': '0.09707', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098804', 'logps_train/rejected': '-116.5', 'logps_train/chosen': '-138.28', 'loss/train': '0.65557', 'examples_per_second': '31.493', 'grad_norm': '41.25', 'counters/examples': 89056, 'counters/updates': 2783}
train stats after 89088 examples: {'rewards_train/chosen': '0.14119', 'rewards_train/rejected': '0.03688', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10431', 'logps_train/rejected': '-130.82', 'logps_train/chosen': '-168.28', 'loss/train': '0.65098', 'examples_per_second': '31.551', 'grad_norm': '32.5', 'counters/examples': 89088, 'counters/updates': 2784}
train stats after 89120 examples: {'rewards_train/chosen': '0.043946', 'rewards_train/rejected': '0.088216', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.04427', 'logps_train/rejected': '-109.64', 'logps_train/chosen': '-116.62', 'loss/train': '0.72084', 'examples_per_second': '31.972', 'grad_norm': '36.5', 'counters/examples': 89120, 'counters/updates': 2785}
train stats after 89152 examples: {'rewards_train/chosen': '0.097865', 'rewards_train/rejected': '0.109', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01113', 'logps_train/rejected': '-138.69', 'logps_train/chosen': '-170.77', 'loss/train': '0.70555', 'examples_per_second': '30.306', 'grad_norm': '35.25', 'counters/examples': 89152, 'counters/updates': 2786}
train stats after 89184 examples: {'rewards_train/chosen': '0.098817', 'rewards_train/rejected': '0.084363', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014454', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-194.54', 'loss/train': '0.6916', 'examples_per_second': '29.857', 'grad_norm': '37', 'counters/examples': 89184, 'counters/updates': 2787}
train stats after 89216 examples: {'rewards_train/chosen': '0.13467', 'rewards_train/rejected': '0.0046594', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13001', 'logps_train/rejected': '-112.29', 'logps_train/chosen': '-158.23', 'loss/train': '0.6382', 'examples_per_second': '31.479', 'grad_norm': '29.625', 'counters/examples': 89216, 'counters/updates': 2788}
train stats after 89248 examples: {'rewards_train/chosen': '0.095854', 'rewards_train/rejected': '0.055016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040838', 'logps_train/rejected': '-86.391', 'logps_train/chosen': '-113.14', 'loss/train': '0.67651', 'examples_per_second': '33.012', 'grad_norm': '37.75', 'counters/examples': 89248, 'counters/updates': 2789}
train stats after 89280 examples: {'rewards_train/chosen': '0.15226', 'rewards_train/rejected': '0.033898', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11837', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-147.31', 'loss/train': '0.64616', 'examples_per_second': '31.501', 'grad_norm': '31.5', 'counters/examples': 89280, 'counters/updates': 2790}
train stats after 89312 examples: {'rewards_train/chosen': '0.13614', 'rewards_train/rejected': '0.13984', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0036994', 'logps_train/rejected': '-139.14', 'logps_train/chosen': '-140.52', 'loss/train': '0.70225', 'examples_per_second': '30.653', 'grad_norm': '47.75', 'counters/examples': 89312, 'counters/updates': 2791}
train stats after 89344 examples: {'rewards_train/chosen': '0.18265', 'rewards_train/rejected': '0.098388', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084262', 'logps_train/rejected': '-144.33', 'logps_train/chosen': '-189.8', 'loss/train': '0.67755', 'examples_per_second': '31.408', 'grad_norm': '39', 'counters/examples': 89344, 'counters/updates': 2792}
train stats after 89376 examples: {'rewards_train/chosen': '0.21605', 'rewards_train/rejected': '0.08407', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13198', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-185.97', 'loss/train': '0.63882', 'examples_per_second': '31.399', 'grad_norm': '27.25', 'counters/examples': 89376, 'counters/updates': 2793}
train stats after 89408 examples: {'rewards_train/chosen': '0.1015', 'rewards_train/rejected': '0.03969', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061812', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-130.38', 'loss/train': '0.67221', 'examples_per_second': '31.937', 'grad_norm': '34.25', 'counters/examples': 89408, 'counters/updates': 2794}
train stats after 89440 examples: {'rewards_train/chosen': '0.15074', 'rewards_train/rejected': '0.17537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.024627', 'logps_train/rejected': '-137.18', 'logps_train/chosen': '-151.92', 'loss/train': '0.71949', 'examples_per_second': '32.419', 'grad_norm': '38', 'counters/examples': 89440, 'counters/updates': 2795}
train stats after 89472 examples: {'rewards_train/chosen': '0.16801', 'rewards_train/rejected': '0.065332', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10268', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-141.33', 'loss/train': '0.66157', 'examples_per_second': '32.342', 'grad_norm': '39.5', 'counters/examples': 89472, 'counters/updates': 2796}
train stats after 89504 examples: {'rewards_train/chosen': '0.18777', 'rewards_train/rejected': '0.053022', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13475', 'logps_train/rejected': '-149.62', 'logps_train/chosen': '-160.66', 'loss/train': '0.64305', 'examples_per_second': '31.484', 'grad_norm': '30', 'counters/examples': 89504, 'counters/updates': 2797}
train stats after 89536 examples: {'rewards_train/chosen': '0.11629', 'rewards_train/rejected': '0.059648', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05664', 'logps_train/rejected': '-124.5', 'logps_train/chosen': '-155.9', 'loss/train': '0.67953', 'examples_per_second': '31.45', 'grad_norm': '47.5', 'counters/examples': 89536, 'counters/updates': 2798}
train stats after 89568 examples: {'rewards_train/chosen': '0.10799', 'rewards_train/rejected': '-0.016512', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1245', 'logps_train/rejected': '-89.619', 'logps_train/chosen': '-142.16', 'loss/train': '0.64644', 'examples_per_second': '30.571', 'grad_norm': '23.75', 'counters/examples': 89568, 'counters/updates': 2799}
train stats after 89600 examples: {'rewards_train/chosen': '0.040155', 'rewards_train/rejected': '0.023042', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017112', 'logps_train/rejected': '-154.2', 'logps_train/chosen': '-129.1', 'loss/train': '0.68794', 'examples_per_second': '30.872', 'grad_norm': '34.25', 'counters/examples': 89600, 'counters/updates': 2800}
skipping logging after 89632 examples to avoid logging too frequently
train stats after 89664 examples: {'rewards_train/chosen': '0.062039', 'rewards_train/rejected': '0.015442', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046597', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-147.61', 'loss/train': '0.67356', 'examples_per_second': '31.989', 'grad_norm': '27.875', 'counters/examples': 89664, 'counters/updates': 2802}
skipping logging after 89696 examples to avoid logging too frequently
train stats after 89728 examples: {'rewards_train/chosen': '0.077837', 'rewards_train/rejected': '0.093568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.015731', 'logps_train/rejected': '-145.62', 'logps_train/chosen': '-164.6', 'loss/train': '0.70654', 'examples_per_second': '31.461', 'grad_norm': '43.75', 'counters/examples': 89728, 'counters/updates': 2804}
skipping logging after 89760 examples to avoid logging too frequently
train stats after 89792 examples: {'rewards_train/chosen': '0.089975', 'rewards_train/rejected': '0.042128', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047847', 'logps_train/rejected': '-176.49', 'logps_train/chosen': '-126.46', 'loss/train': '0.68798', 'examples_per_second': '31.487', 'grad_norm': '39', 'counters/examples': 89792, 'counters/updates': 2806}
train stats after 89824 examples: {'rewards_train/chosen': '0.14031', 'rewards_train/rejected': '0.18135', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041033', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-128.18', 'loss/train': '0.73662', 'examples_per_second': '29.957', 'grad_norm': '39.75', 'counters/examples': 89824, 'counters/updates': 2807}
train stats after 89856 examples: {'rewards_train/chosen': '0.17227', 'rewards_train/rejected': '0.0089465', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16332', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-157.26', 'loss/train': '0.62737', 'examples_per_second': '31.517', 'grad_norm': '27.5', 'counters/examples': 89856, 'counters/updates': 2808}
train stats after 89888 examples: {'rewards_train/chosen': '0.22174', 'rewards_train/rejected': '0.092698', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12905', 'logps_train/rejected': '-157.46', 'logps_train/chosen': '-135.23', 'loss/train': '0.64856', 'examples_per_second': '30.343', 'grad_norm': '51.75', 'counters/examples': 89888, 'counters/updates': 2809}
train stats after 89920 examples: {'rewards_train/chosen': '0.097474', 'rewards_train/rejected': '0.060306', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037167', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-150.47', 'loss/train': '0.6793', 'examples_per_second': '30.27', 'grad_norm': '35.25', 'counters/examples': 89920, 'counters/updates': 2810}
train stats after 89952 examples: {'rewards_train/chosen': '0.16285', 'rewards_train/rejected': '0.015988', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14686', 'logps_train/rejected': '-134.95', 'logps_train/chosen': '-172.8', 'loss/train': '0.6515', 'examples_per_second': '31.516', 'grad_norm': '35.5', 'counters/examples': 89952, 'counters/updates': 2811}
train stats after 89984 examples: {'rewards_train/chosen': '0.079835', 'rewards_train/rejected': '0.090909', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.011074', 'logps_train/rejected': '-132.07', 'logps_train/chosen': '-183.7', 'loss/train': '0.71221', 'examples_per_second': '31.965', 'grad_norm': '33.5', 'counters/examples': 89984, 'counters/updates': 2812}
train stats after 90016 examples: {'rewards_train/chosen': '0.074851', 'rewards_train/rejected': '-0.0074914', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082342', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-150.69', 'loss/train': '0.6619', 'examples_per_second': '31.253', 'grad_norm': '33', 'counters/examples': 90016, 'counters/updates': 2813}
train stats after 90048 examples: {'rewards_train/chosen': '0.087642', 'rewards_train/rejected': '0.093852', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0062104', 'logps_train/rejected': '-172.5', 'logps_train/chosen': '-136.62', 'loss/train': '0.70848', 'examples_per_second': '30.453', 'grad_norm': '61', 'counters/examples': 90048, 'counters/updates': 2814}
train stats after 90080 examples: {'rewards_train/chosen': '0.12929', 'rewards_train/rejected': '0.10188', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027403', 'logps_train/rejected': '-156.03', 'logps_train/chosen': '-162.2', 'loss/train': '0.68711', 'examples_per_second': '32.902', 'grad_norm': '37.5', 'counters/examples': 90080, 'counters/updates': 2815}
train stats after 90112 examples: {'rewards_train/chosen': '0.11454', 'rewards_train/rejected': '-0.021952', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13649', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-157.51', 'loss/train': '0.63371', 'examples_per_second': '31.463', 'grad_norm': '35', 'counters/examples': 90112, 'counters/updates': 2816}
train stats after 90144 examples: {'rewards_train/chosen': '0.1114', 'rewards_train/rejected': '0.05971', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051691', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-150.09', 'loss/train': '0.67885', 'examples_per_second': '32.866', 'grad_norm': '41.5', 'counters/examples': 90144, 'counters/updates': 2817}
train stats after 90176 examples: {'rewards_train/chosen': '0.13237', 'rewards_train/rejected': '0.11515', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017225', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-150.56', 'loss/train': '0.69516', 'examples_per_second': '30.591', 'grad_norm': '36.75', 'counters/examples': 90176, 'counters/updates': 2818}
train stats after 90208 examples: {'rewards_train/chosen': '0.10119', 'rewards_train/rejected': '0.068729', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032461', 'logps_train/rejected': '-121.32', 'logps_train/chosen': '-155.7', 'loss/train': '0.68788', 'examples_per_second': '25.033', 'grad_norm': '33.25', 'counters/examples': 90208, 'counters/updates': 2819}
train stats after 90240 examples: {'rewards_train/chosen': '0.093027', 'rewards_train/rejected': '0.062029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030999', 'logps_train/rejected': '-106.84', 'logps_train/chosen': '-183.59', 'loss/train': '0.68476', 'examples_per_second': '30.563', 'grad_norm': '44.25', 'counters/examples': 90240, 'counters/updates': 2820}
train stats after 90272 examples: {'rewards_train/chosen': '0.094699', 'rewards_train/rejected': '0.040506', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054194', 'logps_train/rejected': '-153.24', 'logps_train/chosen': '-128.6', 'loss/train': '0.67311', 'examples_per_second': '30.964', 'grad_norm': '26.875', 'counters/examples': 90272, 'counters/updates': 2821}
train stats after 90304 examples: {'rewards_train/chosen': '0.088626', 'rewards_train/rejected': '0.091342', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0027158', 'logps_train/rejected': '-132.86', 'logps_train/chosen': '-151.69', 'loss/train': '0.70099', 'examples_per_second': '23.932', 'grad_norm': '39.75', 'counters/examples': 90304, 'counters/updates': 2822}
train stats after 90336 examples: {'rewards_train/chosen': '0.076217', 'rewards_train/rejected': '0.089706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.01349', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-137.88', 'loss/train': '0.71504', 'examples_per_second': '30.93', 'grad_norm': '41.5', 'counters/examples': 90336, 'counters/updates': 2823}
train stats after 90368 examples: {'rewards_train/chosen': '0.043954', 'rewards_train/rejected': '0.005866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038088', 'logps_train/rejected': '-104.1', 'logps_train/chosen': '-116.44', 'loss/train': '0.68097', 'examples_per_second': '31.89', 'grad_norm': '33.25', 'counters/examples': 90368, 'counters/updates': 2824}
train stats after 90400 examples: {'rewards_train/chosen': '0.042866', 'rewards_train/rejected': '0.058408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.015542', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-126.39', 'loss/train': '0.71154', 'examples_per_second': '32.69', 'grad_norm': '34', 'counters/examples': 90400, 'counters/updates': 2825}
skipping logging after 90432 examples to avoid logging too frequently
train stats after 90464 examples: {'rewards_train/chosen': '0.093968', 'rewards_train/rejected': '0.068719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025249', 'logps_train/rejected': '-108.15', 'logps_train/chosen': '-134.86', 'loss/train': '0.68464', 'examples_per_second': '30.216', 'grad_norm': '25.5', 'counters/examples': 90464, 'counters/updates': 2827}
train stats after 90496 examples: {'rewards_train/chosen': '0.08949', 'rewards_train/rejected': '0.072411', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01708', 'logps_train/rejected': '-134.36', 'logps_train/chosen': '-118.28', 'loss/train': '0.69466', 'examples_per_second': '30.088', 'grad_norm': '39.75', 'counters/examples': 90496, 'counters/updates': 2828}
train stats after 90528 examples: {'rewards_train/chosen': '0.064562', 'rewards_train/rejected': '0.036675', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027888', 'logps_train/rejected': '-147.78', 'logps_train/chosen': '-156.88', 'loss/train': '0.68606', 'examples_per_second': '29.924', 'grad_norm': '37.75', 'counters/examples': 90528, 'counters/updates': 2829}
train stats after 90560 examples: {'rewards_train/chosen': '0.12657', 'rewards_train/rejected': '0.081997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044569', 'logps_train/rejected': '-118.04', 'logps_train/chosen': '-157.3', 'loss/train': '0.67964', 'examples_per_second': '29.993', 'grad_norm': '29.375', 'counters/examples': 90560, 'counters/updates': 2830}
train stats after 90592 examples: {'rewards_train/chosen': '0.10189', 'rewards_train/rejected': '-0.041706', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1436', 'logps_train/rejected': '-131.23', 'logps_train/chosen': '-122.1', 'loss/train': '0.63302', 'examples_per_second': '32.007', 'grad_norm': '34.75', 'counters/examples': 90592, 'counters/updates': 2831}
train stats after 90624 examples: {'rewards_train/chosen': '0.073624', 'rewards_train/rejected': '0.066825', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0067987', 'logps_train/rejected': '-132.82', 'logps_train/chosen': '-111.26', 'loss/train': '0.69436', 'examples_per_second': '32.554', 'grad_norm': '29.625', 'counters/examples': 90624, 'counters/updates': 2832}
skipping logging after 90656 examples to avoid logging too frequently
train stats after 90688 examples: {'rewards_train/chosen': '0.15431', 'rewards_train/rejected': '0.072849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081457', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-122.83', 'loss/train': '0.6619', 'examples_per_second': '34.116', 'grad_norm': '31.5', 'counters/examples': 90688, 'counters/updates': 2834}
train stats after 90720 examples: {'rewards_train/chosen': '0.13501', 'rewards_train/rejected': '0.1168', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018204', 'logps_train/rejected': '-153.52', 'logps_train/chosen': '-165.3', 'loss/train': '0.69071', 'examples_per_second': '31.389', 'grad_norm': '28.875', 'counters/examples': 90720, 'counters/updates': 2835}
train stats after 90752 examples: {'rewards_train/chosen': '0.067085', 'rewards_train/rejected': '0.069259', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0021735', 'logps_train/rejected': '-78.346', 'logps_train/chosen': '-113.69', 'loss/train': '0.69853', 'examples_per_second': '32.065', 'grad_norm': '28.5', 'counters/examples': 90752, 'counters/updates': 2836}
skipping logging after 90784 examples to avoid logging too frequently
train stats after 90816 examples: {'rewards_train/chosen': '0.10949', 'rewards_train/rejected': '0.17979', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.070301', 'logps_train/rejected': '-172.17', 'logps_train/chosen': '-146.32', 'loss/train': '0.74289', 'examples_per_second': '31.474', 'grad_norm': '50.75', 'counters/examples': 90816, 'counters/updates': 2838}
train stats after 90848 examples: {'rewards_train/chosen': '0.034763', 'rewards_train/rejected': '0.030587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0041761', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-157.01', 'loss/train': '0.70073', 'examples_per_second': '30.825', 'grad_norm': '35.5', 'counters/examples': 90848, 'counters/updates': 2839}
train stats after 90880 examples: {'rewards_train/chosen': '0.15374', 'rewards_train/rejected': '0.13063', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023111', 'logps_train/rejected': '-188.23', 'logps_train/chosen': '-176.19', 'loss/train': '0.69636', 'examples_per_second': '31.504', 'grad_norm': '38', 'counters/examples': 90880, 'counters/updates': 2840}
train stats after 90912 examples: {'rewards_train/chosen': '0.18967', 'rewards_train/rejected': '0.067825', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12185', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-168.18', 'loss/train': '0.64184', 'examples_per_second': '30.976', 'grad_norm': '29.5', 'counters/examples': 90912, 'counters/updates': 2841}
train stats after 90944 examples: {'rewards_train/chosen': '0.059605', 'rewards_train/rejected': '0.041559', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018046', 'logps_train/rejected': '-114.85', 'logps_train/chosen': '-151.27', 'loss/train': '0.69902', 'examples_per_second': '32.94', 'grad_norm': '37.25', 'counters/examples': 90944, 'counters/updates': 2842}
train stats after 90976 examples: {'rewards_train/chosen': '0.038342', 'rewards_train/rejected': '0.017145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021197', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-129.06', 'loss/train': '0.68781', 'examples_per_second': '31.273', 'grad_norm': '30.125', 'counters/examples': 90976, 'counters/updates': 2843}
train stats after 91008 examples: {'rewards_train/chosen': '0.1564', 'rewards_train/rejected': '0.15121', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0051952', 'logps_train/rejected': '-143.2', 'logps_train/chosen': '-133', 'loss/train': '0.70009', 'examples_per_second': '30.967', 'grad_norm': '39.25', 'counters/examples': 91008, 'counters/updates': 2844}
train stats after 91040 examples: {'rewards_train/chosen': '0.1083', 'rewards_train/rejected': '0.080516', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.027787', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-121.53', 'loss/train': '0.694', 'examples_per_second': '32.175', 'grad_norm': '33.75', 'counters/examples': 91040, 'counters/updates': 2845}
train stats after 91072 examples: {'rewards_train/chosen': '0.055537', 'rewards_train/rejected': '0.060206', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0046694', 'logps_train/rejected': '-135.15', 'logps_train/chosen': '-150.52', 'loss/train': '0.70163', 'examples_per_second': '32.036', 'grad_norm': '31.875', 'counters/examples': 91072, 'counters/updates': 2846}
train stats after 91104 examples: {'rewards_train/chosen': '0.16947', 'rewards_train/rejected': '0.10759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061883', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-142.67', 'loss/train': '0.67891', 'examples_per_second': '32.353', 'grad_norm': '33.75', 'counters/examples': 91104, 'counters/updates': 2847}
train stats after 91136 examples: {'rewards_train/chosen': '0.13417', 'rewards_train/rejected': '0.037223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096948', 'logps_train/rejected': '-99.692', 'logps_train/chosen': '-146.4', 'loss/train': '0.65176', 'examples_per_second': '31.327', 'grad_norm': '34.5', 'counters/examples': 91136, 'counters/updates': 2848}
train stats after 91168 examples: {'rewards_train/chosen': '0.1603', 'rewards_train/rejected': '0.11735', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042945', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-136.11', 'loss/train': '0.67727', 'examples_per_second': '31.702', 'grad_norm': '27.5', 'counters/examples': 91168, 'counters/updates': 2849}
skipping logging after 91200 examples to avoid logging too frequently
train stats after 91232 examples: {'rewards_train/chosen': '0.11851', 'rewards_train/rejected': '0.030229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088276', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-153.16', 'loss/train': '0.65965', 'examples_per_second': '32.093', 'grad_norm': '27.25', 'counters/examples': 91232, 'counters/updates': 2851}
train stats after 91264 examples: {'rewards_train/chosen': '0.16988', 'rewards_train/rejected': '0.049378', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1205', 'logps_train/rejected': '-137.54', 'logps_train/chosen': '-132.68', 'loss/train': '0.64511', 'examples_per_second': '30.976', 'grad_norm': '26.25', 'counters/examples': 91264, 'counters/updates': 2852}
train stats after 91296 examples: {'rewards_train/chosen': '0.051549', 'rewards_train/rejected': '0.076108', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.02456', 'logps_train/rejected': '-139.63', 'logps_train/chosen': '-131.54', 'loss/train': '0.7183', 'examples_per_second': '29.875', 'grad_norm': '30.875', 'counters/examples': 91296, 'counters/updates': 2853}
train stats after 91328 examples: {'rewards_train/chosen': '0.091614', 'rewards_train/rejected': '0.054386', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037229', 'logps_train/rejected': '-106.65', 'logps_train/chosen': '-137.66', 'loss/train': '0.68066', 'examples_per_second': '31.44', 'grad_norm': '25.375', 'counters/examples': 91328, 'counters/updates': 2854}
train stats after 91360 examples: {'rewards_train/chosen': '0.098673', 'rewards_train/rejected': '0.11913', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.020458', 'logps_train/rejected': '-137.59', 'logps_train/chosen': '-135.09', 'loss/train': '0.70875', 'examples_per_second': '32.107', 'grad_norm': '31.75', 'counters/examples': 91360, 'counters/updates': 2855}
train stats after 91392 examples: {'rewards_train/chosen': '0.061537', 'rewards_train/rejected': '0.034941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026596', 'logps_train/rejected': '-119.27', 'logps_train/chosen': '-143.52', 'loss/train': '0.68575', 'examples_per_second': '30.395', 'grad_norm': '25.375', 'counters/examples': 91392, 'counters/updates': 2856}
skipping logging after 91424 examples to avoid logging too frequently
train stats after 91456 examples: {'rewards_train/chosen': '0.051209', 'rewards_train/rejected': '0.017433', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033776', 'logps_train/rejected': '-108.88', 'logps_train/chosen': '-163.56', 'loss/train': '0.68144', 'examples_per_second': '31.489', 'grad_norm': '26.625', 'counters/examples': 91456, 'counters/updates': 2858}
train stats after 91488 examples: {'rewards_train/chosen': '0.080892', 'rewards_train/rejected': '0.035143', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045749', 'logps_train/rejected': '-141.27', 'logps_train/chosen': '-120.34', 'loss/train': '0.6884', 'examples_per_second': '32.37', 'grad_norm': '33.25', 'counters/examples': 91488, 'counters/updates': 2859}
skipping logging after 91520 examples to avoid logging too frequently
train stats after 91552 examples: {'rewards_train/chosen': '0.10862', 'rewards_train/rejected': '0.033009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075612', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-141.61', 'loss/train': '0.66915', 'examples_per_second': '30.836', 'grad_norm': '29.125', 'counters/examples': 91552, 'counters/updates': 2861}
train stats after 91584 examples: {'rewards_train/chosen': '0.16069', 'rewards_train/rejected': '0.13006', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030625', 'logps_train/rejected': '-162.45', 'logps_train/chosen': '-151.85', 'loss/train': '0.68546', 'examples_per_second': '31.348', 'grad_norm': '37', 'counters/examples': 91584, 'counters/updates': 2862}
train stats after 91616 examples: {'rewards_train/chosen': '0.10086', 'rewards_train/rejected': '0.074275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026585', 'logps_train/rejected': '-115.29', 'logps_train/chosen': '-119.04', 'loss/train': '0.68677', 'examples_per_second': '32.179', 'grad_norm': '35.5', 'counters/examples': 91616, 'counters/updates': 2863}
train stats after 91648 examples: {'rewards_train/chosen': '0.17178', 'rewards_train/rejected': '0.020531', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15125', 'logps_train/rejected': '-112.84', 'logps_train/chosen': '-135.4', 'loss/train': '0.62953', 'examples_per_second': '30.766', 'grad_norm': '27.5', 'counters/examples': 91648, 'counters/updates': 2864}
skipping logging after 91680 examples to avoid logging too frequently
train stats after 91712 examples: {'rewards_train/chosen': '0.091543', 'rewards_train/rejected': '0.027451', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064092', 'logps_train/rejected': '-106.5', 'logps_train/chosen': '-143.52', 'loss/train': '0.67185', 'examples_per_second': '31.506', 'grad_norm': '30', 'counters/examples': 91712, 'counters/updates': 2866}
train stats after 91744 examples: {'rewards_train/chosen': '0.076512', 'rewards_train/rejected': '0.061241', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015271', 'logps_train/rejected': '-118.93', 'logps_train/chosen': '-131.41', 'loss/train': '0.69348', 'examples_per_second': '30.841', 'grad_norm': '29', 'counters/examples': 91744, 'counters/updates': 2867}
train stats after 91776 examples: {'rewards_train/chosen': '0.14916', 'rewards_train/rejected': '0.07745', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.071706', 'logps_train/rejected': '-92.6', 'logps_train/chosen': '-138.93', 'loss/train': '0.66654', 'examples_per_second': '30.993', 'grad_norm': '25.75', 'counters/examples': 91776, 'counters/updates': 2868}
train stats after 91808 examples: {'rewards_train/chosen': '0.12165', 'rewards_train/rejected': '0.047527', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074119', 'logps_train/rejected': '-107', 'logps_train/chosen': '-151', 'loss/train': '0.66942', 'examples_per_second': '32.091', 'grad_norm': '28', 'counters/examples': 91808, 'counters/updates': 2869}
train stats after 91840 examples: {'rewards_train/chosen': '0.096289', 'rewards_train/rejected': '0.031902', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064387', 'logps_train/rejected': '-116.4', 'logps_train/chosen': '-163.79', 'loss/train': '0.67258', 'examples_per_second': '33.024', 'grad_norm': '36.5', 'counters/examples': 91840, 'counters/updates': 2870}
train stats after 91872 examples: {'rewards_train/chosen': '0.0025305', 'rewards_train/rejected': '0.10546', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.10293', 'logps_train/rejected': '-156.39', 'logps_train/chosen': '-125', 'loss/train': '0.76204', 'examples_per_second': '31.121', 'grad_norm': '51.75', 'counters/examples': 91872, 'counters/updates': 2871}
train stats after 91904 examples: {'rewards_train/chosen': '0.078382', 'rewards_train/rejected': '0.10201', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.023626', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-158.57', 'loss/train': '0.71619', 'examples_per_second': '32.252', 'grad_norm': '35', 'counters/examples': 91904, 'counters/updates': 2872}
train stats after 91936 examples: {'rewards_train/chosen': '0.087853', 'rewards_train/rejected': '0.019441', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068412', 'logps_train/rejected': '-87.683', 'logps_train/chosen': '-119.15', 'loss/train': '0.66419', 'examples_per_second': '30.936', 'grad_norm': '27', 'counters/examples': 91936, 'counters/updates': 2873}
train stats after 91968 examples: {'rewards_train/chosen': '0.10132', 'rewards_train/rejected': '0.046562', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.054758', 'logps_train/rejected': '-145.42', 'logps_train/chosen': '-165.99', 'loss/train': '0.67304', 'examples_per_second': '31.567', 'grad_norm': '33.75', 'counters/examples': 91968, 'counters/updates': 2874}
train stats after 92000 examples: {'rewards_train/chosen': '0.075542', 'rewards_train/rejected': '-0.0057874', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081329', 'logps_train/rejected': '-160.86', 'logps_train/chosen': '-149.56', 'loss/train': '0.66454', 'examples_per_second': '30.076', 'grad_norm': '27.5', 'counters/examples': 92000, 'counters/updates': 2875}
train stats after 92032 examples: {'rewards_train/chosen': '0.052999', 'rewards_train/rejected': '-0.014882', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06788', 'logps_train/rejected': '-108.01', 'logps_train/chosen': '-142.02', 'loss/train': '0.66685', 'examples_per_second': '26.891', 'grad_norm': '35.75', 'counters/examples': 92032, 'counters/updates': 2876}
train stats after 92064 examples: {'rewards_train/chosen': '0.087118', 'rewards_train/rejected': '0.074909', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012209', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-149.52', 'loss/train': '0.69657', 'examples_per_second': '32.023', 'grad_norm': '35.75', 'counters/examples': 92064, 'counters/updates': 2877}
train stats after 92096 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.099144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024612', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-141.12', 'loss/train': '0.6887', 'examples_per_second': '31.885', 'grad_norm': '39.75', 'counters/examples': 92096, 'counters/updates': 2878}
train stats after 92128 examples: {'rewards_train/chosen': '0.095892', 'rewards_train/rejected': '0.015884', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080009', 'logps_train/rejected': '-113.26', 'logps_train/chosen': '-153.02', 'loss/train': '0.66299', 'examples_per_second': '31.016', 'grad_norm': '30.75', 'counters/examples': 92128, 'counters/updates': 2879}
skipping logging after 92160 examples to avoid logging too frequently
train stats after 92192 examples: {'rewards_train/chosen': '0.091807', 'rewards_train/rejected': '0.065443', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026364', 'logps_train/rejected': '-138.28', 'logps_train/chosen': '-165.03', 'loss/train': '0.68872', 'examples_per_second': '30.733', 'grad_norm': '28.5', 'counters/examples': 92192, 'counters/updates': 2881}
train stats after 92224 examples: {'rewards_train/chosen': '0.082931', 'rewards_train/rejected': '0.13835', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.055414', 'logps_train/rejected': '-150.49', 'logps_train/chosen': '-151.42', 'loss/train': '0.73521', 'examples_per_second': '31.46', 'grad_norm': '36.5', 'counters/examples': 92224, 'counters/updates': 2882}
skipping logging after 92256 examples to avoid logging too frequently
train stats after 92288 examples: {'rewards_train/chosen': '0.077727', 'rewards_train/rejected': '0.044409', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033317', 'logps_train/rejected': '-103.24', 'logps_train/chosen': '-145.78', 'loss/train': '0.68976', 'examples_per_second': '31.358', 'grad_norm': '38', 'counters/examples': 92288, 'counters/updates': 2884}
train stats after 92320 examples: {'rewards_train/chosen': '0.12512', 'rewards_train/rejected': '0.025345', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099774', 'logps_train/rejected': '-112.27', 'logps_train/chosen': '-183.08', 'loss/train': '0.65414', 'examples_per_second': '32.219', 'grad_norm': '39.25', 'counters/examples': 92320, 'counters/updates': 2885}
train stats after 92352 examples: {'rewards_train/chosen': '0.057326', 'rewards_train/rejected': '0.0087577', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048569', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-180.23', 'loss/train': '0.68294', 'examples_per_second': '30.52', 'grad_norm': '27.375', 'counters/examples': 92352, 'counters/updates': 2886}
train stats after 92384 examples: {'rewards_train/chosen': '0.13239', 'rewards_train/rejected': '0.097065', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035322', 'logps_train/rejected': '-154.96', 'logps_train/chosen': '-129.85', 'loss/train': '0.69022', 'examples_per_second': '31.343', 'grad_norm': '53', 'counters/examples': 92384, 'counters/updates': 2887}
skipping logging after 92416 examples to avoid logging too frequently
train stats after 92448 examples: {'rewards_train/chosen': '0.11207', 'rewards_train/rejected': '0.025663', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086403', 'logps_train/rejected': '-142.27', 'logps_train/chosen': '-144.13', 'loss/train': '0.65792', 'examples_per_second': '31.411', 'grad_norm': '41', 'counters/examples': 92448, 'counters/updates': 2889}
train stats after 92480 examples: {'rewards_train/chosen': '0.10914', 'rewards_train/rejected': '0.029059', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.080078', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-158.6', 'loss/train': '0.65889', 'examples_per_second': '31.417', 'grad_norm': '32', 'counters/examples': 92480, 'counters/updates': 2890}
skipping logging after 92512 examples to avoid logging too frequently
train stats after 92544 examples: {'rewards_train/chosen': '0.03856', 'rewards_train/rejected': '0.040079', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0015182', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-146.43', 'loss/train': '0.70163', 'examples_per_second': '31.86', 'grad_norm': '44', 'counters/examples': 92544, 'counters/updates': 2892}
train stats after 92576 examples: {'rewards_train/chosen': '0.084965', 'rewards_train/rejected': '0.057399', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027566', 'logps_train/rejected': '-86.199', 'logps_train/chosen': '-133.23', 'loss/train': '0.68522', 'examples_per_second': '31.08', 'grad_norm': '23.875', 'counters/examples': 92576, 'counters/updates': 2893}
train stats after 92608 examples: {'rewards_train/chosen': '0.15212', 'rewards_train/rejected': '0.085652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066473', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-172.97', 'loss/train': '0.68512', 'examples_per_second': '29.814', 'grad_norm': '38', 'counters/examples': 92608, 'counters/updates': 2894}
train stats after 92640 examples: {'rewards_train/chosen': '0.088876', 'rewards_train/rejected': '0.027945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.06093', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-150.86', 'loss/train': '0.67272', 'examples_per_second': '29.81', 'grad_norm': '33', 'counters/examples': 92640, 'counters/updates': 2895}
skipping logging after 92672 examples to avoid logging too frequently
train stats after 92704 examples: {'rewards_train/chosen': '0.095762', 'rewards_train/rejected': '0.032013', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063749', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-151.16', 'loss/train': '0.66928', 'examples_per_second': '31.858', 'grad_norm': '27.375', 'counters/examples': 92704, 'counters/updates': 2897}
train stats after 92736 examples: {'rewards_train/chosen': '0.074954', 'rewards_train/rejected': '0.076724', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00177', 'logps_train/rejected': '-105.06', 'logps_train/chosen': '-123.68', 'loss/train': '0.70213', 'examples_per_second': '32.867', 'grad_norm': '27.375', 'counters/examples': 92736, 'counters/updates': 2898}
train stats after 92768 examples: {'rewards_train/chosen': '0.086365', 'rewards_train/rejected': '0.14215', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.05578', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-112.17', 'loss/train': '0.73229', 'examples_per_second': '31.046', 'grad_norm': '35.25', 'counters/examples': 92768, 'counters/updates': 2899}
skipping logging after 92800 examples to avoid logging too frequently
train stats after 92832 examples: {'rewards_train/chosen': '0.21596', 'rewards_train/rejected': '0.09448', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12148', 'logps_train/rejected': '-158.35', 'logps_train/chosen': '-174.57', 'loss/train': '0.64287', 'examples_per_second': '33.648', 'grad_norm': '37.25', 'counters/examples': 92832, 'counters/updates': 2901}
train stats after 92864 examples: {'rewards_train/chosen': '0.083067', 'rewards_train/rejected': '0.048354', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034713', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-154.5', 'loss/train': '0.67855', 'examples_per_second': '30.072', 'grad_norm': '29', 'counters/examples': 92864, 'counters/updates': 2902}
train stats after 92896 examples: {'rewards_train/chosen': '0.086246', 'rewards_train/rejected': '0.079337', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0069096', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-102.85', 'loss/train': '0.69716', 'examples_per_second': '31.387', 'grad_norm': '34.25', 'counters/examples': 92896, 'counters/updates': 2903}
train stats after 92928 examples: {'rewards_train/chosen': '0.12649', 'rewards_train/rejected': '0.026669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099826', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-139.12', 'loss/train': '0.65671', 'examples_per_second': '30.726', 'grad_norm': '28.875', 'counters/examples': 92928, 'counters/updates': 2904}
train stats after 92960 examples: {'rewards_train/chosen': '0.13359', 'rewards_train/rejected': '-0.0054497', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13904', 'logps_train/rejected': '-84.765', 'logps_train/chosen': '-151.67', 'loss/train': '0.63642', 'examples_per_second': '30.91', 'grad_norm': '29', 'counters/examples': 92960, 'counters/updates': 2905}
skipping logging after 92992 examples to avoid logging too frequently
train stats after 93024 examples: {'rewards_train/chosen': '0.20354', 'rewards_train/rejected': '0.064799', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13874', 'logps_train/rejected': '-140.29', 'logps_train/chosen': '-170.77', 'loss/train': '0.64136', 'examples_per_second': '30.771', 'grad_norm': '29.375', 'counters/examples': 93024, 'counters/updates': 2907}
skipping logging after 93056 examples to avoid logging too frequently
train stats after 93088 examples: {'rewards_train/chosen': '0.083666', 'rewards_train/rejected': '0.047941', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035725', 'logps_train/rejected': '-172.74', 'logps_train/chosen': '-155.73', 'loss/train': '0.68746', 'examples_per_second': '33.178', 'grad_norm': '36.5', 'counters/examples': 93088, 'counters/updates': 2909}
train stats after 93120 examples: {'rewards_train/chosen': '0.14528', 'rewards_train/rejected': '0.099661', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045619', 'logps_train/rejected': '-146.44', 'logps_train/chosen': '-152.94', 'loss/train': '0.67441', 'examples_per_second': '30.292', 'grad_norm': '29.5', 'counters/examples': 93120, 'counters/updates': 2910}
train stats after 93152 examples: {'rewards_train/chosen': '0.073128', 'rewards_train/rejected': '0.019062', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054066', 'logps_train/rejected': '-80.994', 'logps_train/chosen': '-125.2', 'loss/train': '0.67467', 'examples_per_second': '31.422', 'grad_norm': '35.25', 'counters/examples': 93152, 'counters/updates': 2911}
skipping logging after 93184 examples to avoid logging too frequently
train stats after 93216 examples: {'rewards_train/chosen': '0.064558', 'rewards_train/rejected': '0.051206', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013352', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-174.4', 'loss/train': '0.69199', 'examples_per_second': '33.29', 'grad_norm': '41.75', 'counters/examples': 93216, 'counters/updates': 2913}
train stats after 93248 examples: {'rewards_train/chosen': '0.078163', 'rewards_train/rejected': '0.054142', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.02402', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-126.46', 'loss/train': '0.68479', 'examples_per_second': '31.251', 'grad_norm': '31.75', 'counters/examples': 93248, 'counters/updates': 2914}
train stats after 93280 examples: {'rewards_train/chosen': '0.14627', 'rewards_train/rejected': '0.1057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040574', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-166.22', 'loss/train': '0.68742', 'examples_per_second': '31.461', 'grad_norm': '40.5', 'counters/examples': 93280, 'counters/updates': 2915}
train stats after 93312 examples: {'rewards_train/chosen': '0.15665', 'rewards_train/rejected': '0.047589', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10906', 'logps_train/rejected': '-116.97', 'logps_train/chosen': '-104.92', 'loss/train': '0.64686', 'examples_per_second': '31.128', 'grad_norm': '28.375', 'counters/examples': 93312, 'counters/updates': 2916}
train stats after 93344 examples: {'rewards_train/chosen': '0.17003', 'rewards_train/rejected': '0.14226', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027763', 'logps_train/rejected': '-133.98', 'logps_train/chosen': '-158.79', 'loss/train': '0.71169', 'examples_per_second': '31.473', 'grad_norm': '64', 'counters/examples': 93344, 'counters/updates': 2917}
train stats after 93376 examples: {'rewards_train/chosen': '0.1363', 'rewards_train/rejected': '0.19921', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.062902', 'logps_train/rejected': '-168.74', 'logps_train/chosen': '-152.12', 'loss/train': '0.73819', 'examples_per_second': '31.476', 'grad_norm': '40.25', 'counters/examples': 93376, 'counters/updates': 2918}
train stats after 93408 examples: {'rewards_train/chosen': '-0.0098444', 'rewards_train/rejected': '0.0426', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.052445', 'logps_train/rejected': '-153.12', 'logps_train/chosen': '-171.04', 'loss/train': '0.731', 'examples_per_second': '31.417', 'grad_norm': '41.25', 'counters/examples': 93408, 'counters/updates': 2919}
train stats after 93440 examples: {'rewards_train/chosen': '0.1092', 'rewards_train/rejected': '0.078951', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030253', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-157.03', 'loss/train': '0.68291', 'examples_per_second': '31.22', 'grad_norm': '31.5', 'counters/examples': 93440, 'counters/updates': 2920}
train stats after 93472 examples: {'rewards_train/chosen': '0.1023', 'rewards_train/rejected': '0.061038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041262', 'logps_train/rejected': '-111.48', 'logps_train/chosen': '-135.51', 'loss/train': '0.6756', 'examples_per_second': '30.825', 'grad_norm': '24.5', 'counters/examples': 93472, 'counters/updates': 2921}
train stats after 93504 examples: {'rewards_train/chosen': '0.17102', 'rewards_train/rejected': '0.029213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14181', 'logps_train/rejected': '-126.98', 'logps_train/chosen': '-157.86', 'loss/train': '0.6339', 'examples_per_second': '31.22', 'grad_norm': '35', 'counters/examples': 93504, 'counters/updates': 2922}
train stats after 93536 examples: {'rewards_train/chosen': '0.062009', 'rewards_train/rejected': '0.086547', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024538', 'logps_train/rejected': '-120.58', 'logps_train/chosen': '-178.69', 'loss/train': '0.71258', 'examples_per_second': '31.406', 'grad_norm': '34.75', 'counters/examples': 93536, 'counters/updates': 2923}
train stats after 93568 examples: {'rewards_train/chosen': '0.073426', 'rewards_train/rejected': '-0.07658', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15001', 'logps_train/rejected': '-180.84', 'logps_train/chosen': '-181.18', 'loss/train': '0.63553', 'examples_per_second': '31.511', 'grad_norm': '41', 'counters/examples': 93568, 'counters/updates': 2924}
train stats after 93600 examples: {'rewards_train/chosen': '0.09026', 'rewards_train/rejected': '0.051453', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038807', 'logps_train/rejected': '-135.1', 'logps_train/chosen': '-153.98', 'loss/train': '0.67804', 'examples_per_second': '29.967', 'grad_norm': '32', 'counters/examples': 93600, 'counters/updates': 2925}
train stats after 93632 examples: {'rewards_train/chosen': '0.12196', 'rewards_train/rejected': '-0.029548', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1515', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-156.65', 'loss/train': '0.63067', 'examples_per_second': '30.999', 'grad_norm': '45.25', 'counters/examples': 93632, 'counters/updates': 2926}
train stats after 93664 examples: {'rewards_train/chosen': '0.14362', 'rewards_train/rejected': '0.069464', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074155', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-159.2', 'loss/train': '0.66053', 'examples_per_second': '30.071', 'grad_norm': '29.375', 'counters/examples': 93664, 'counters/updates': 2927}
train stats after 93696 examples: {'rewards_train/chosen': '0.20532', 'rewards_train/rejected': '0.082507', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12281', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-172.3', 'loss/train': '0.64633', 'examples_per_second': '31.277', 'grad_norm': '28.75', 'counters/examples': 93696, 'counters/updates': 2928}
train stats after 93728 examples: {'rewards_train/chosen': '0.088655', 'rewards_train/rejected': '0.16665', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.077996', 'logps_train/rejected': '-158.75', 'logps_train/chosen': '-114.99', 'loss/train': '0.74579', 'examples_per_second': '31.436', 'grad_norm': '46.5', 'counters/examples': 93728, 'counters/updates': 2929}
skipping logging after 93760 examples to avoid logging too frequently
train stats after 93792 examples: {'rewards_train/chosen': '0.068652', 'rewards_train/rejected': '0.24193', 'rewards_train/accuracies': '0.28125', 'rewards_train/margins': '-0.17328', 'logps_train/rejected': '-162.62', 'logps_train/chosen': '-142.07', 'loss/train': '0.81154', 'examples_per_second': '29.932', 'grad_norm': '107', 'counters/examples': 93792, 'counters/updates': 2931}
train stats after 93824 examples: {'rewards_train/chosen': '0.042307', 'rewards_train/rejected': '0.030246', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012061', 'logps_train/rejected': '-92.806', 'logps_train/chosen': '-129.22', 'loss/train': '0.69399', 'examples_per_second': '31.472', 'grad_norm': '28.75', 'counters/examples': 93824, 'counters/updates': 2932}
train stats after 93856 examples: {'rewards_train/chosen': '0.061981', 'rewards_train/rejected': '0.1007', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.038722', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-158.8', 'loss/train': '0.71685', 'examples_per_second': '31.763', 'grad_norm': '29.125', 'counters/examples': 93856, 'counters/updates': 2933}
skipping logging after 93888 examples to avoid logging too frequently
train stats after 93920 examples: {'rewards_train/chosen': '0.18196', 'rewards_train/rejected': '0.11742', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064534', 'logps_train/rejected': '-138.01', 'logps_train/chosen': '-159.53', 'loss/train': '0.68154', 'examples_per_second': '29.914', 'grad_norm': '50.25', 'counters/examples': 93920, 'counters/updates': 2935}
train stats after 93952 examples: {'rewards_train/chosen': '0.11977', 'rewards_train/rejected': '0.043067', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076705', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-138.74', 'loss/train': '0.66683', 'examples_per_second': '32.709', 'grad_norm': '26.75', 'counters/examples': 93952, 'counters/updates': 2936}
train stats after 93984 examples: {'rewards_train/chosen': '0.15801', 'rewards_train/rejected': '0.10855', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049459', 'logps_train/rejected': '-149.93', 'logps_train/chosen': '-159.44', 'loss/train': '0.68815', 'examples_per_second': '31.903', 'grad_norm': '35', 'counters/examples': 93984, 'counters/updates': 2937}
train stats after 94016 examples: {'rewards_train/chosen': '0.071816', 'rewards_train/rejected': '0.036824', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034992', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-142.64', 'loss/train': '0.68439', 'examples_per_second': '29.945', 'grad_norm': '27', 'counters/examples': 94016, 'counters/updates': 2938}
skipping logging after 94048 examples to avoid logging too frequently
train stats after 94080 examples: {'rewards_train/chosen': '0.1141', 'rewards_train/rejected': '0.068217', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045879', 'logps_train/rejected': '-153.57', 'logps_train/chosen': '-167.46', 'loss/train': '0.69001', 'examples_per_second': '31.416', 'grad_norm': '53.5', 'counters/examples': 94080, 'counters/updates': 2940}
skipping logging after 94112 examples to avoid logging too frequently
train stats after 94144 examples: {'rewards_train/chosen': '0.17467', 'rewards_train/rejected': '0.091418', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083256', 'logps_train/rejected': '-133.81', 'logps_train/chosen': '-183.62', 'loss/train': '0.66537', 'examples_per_second': '31.402', 'grad_norm': '34.75', 'counters/examples': 94144, 'counters/updates': 2942}
train stats after 94176 examples: {'rewards_train/chosen': '0.21047', 'rewards_train/rejected': '0.17519', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.035282', 'logps_train/rejected': '-144.43', 'logps_train/chosen': '-164.4', 'loss/train': '0.71424', 'examples_per_second': '31.302', 'grad_norm': '63', 'counters/examples': 94176, 'counters/updates': 2943}
train stats after 94208 examples: {'rewards_train/chosen': '0.065737', 'rewards_train/rejected': '0.056403', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0093338', 'logps_train/rejected': '-138.89', 'logps_train/chosen': '-176.99', 'loss/train': '0.69768', 'examples_per_second': '31.412', 'grad_norm': '55', 'counters/examples': 94208, 'counters/updates': 2944}
train stats after 94240 examples: {'rewards_train/chosen': '0.0948', 'rewards_train/rejected': '-0.026155', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12096', 'logps_train/rejected': '-89.42', 'logps_train/chosen': '-111.37', 'loss/train': '0.63919', 'examples_per_second': '29.493', 'grad_norm': '22.875', 'counters/examples': 94240, 'counters/updates': 2945}
train stats after 94272 examples: {'rewards_train/chosen': '0.10279', 'rewards_train/rejected': '0.05728', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045514', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-168.47', 'loss/train': '0.68367', 'examples_per_second': '33.114', 'grad_norm': '43.25', 'counters/examples': 94272, 'counters/updates': 2946}
train stats after 94304 examples: {'rewards_train/chosen': '0.085004', 'rewards_train/rejected': '0.0231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061903', 'logps_train/rejected': '-102.65', 'logps_train/chosen': '-144.98', 'loss/train': '0.66987', 'examples_per_second': '32.101', 'grad_norm': '29.875', 'counters/examples': 94304, 'counters/updates': 2947}
train stats after 94336 examples: {'rewards_train/chosen': '0.027843', 'rewards_train/rejected': '0.078805', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.050962', 'logps_train/rejected': '-112.73', 'logps_train/chosen': '-121.01', 'loss/train': '0.72476', 'examples_per_second': '30.571', 'grad_norm': '30.25', 'counters/examples': 94336, 'counters/updates': 2948}
train stats after 94368 examples: {'rewards_train/chosen': '0.11629', 'rewards_train/rejected': '0.041831', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074459', 'logps_train/rejected': '-106.73', 'logps_train/chosen': '-110.31', 'loss/train': '0.66531', 'examples_per_second': '32.194', 'grad_norm': '27.25', 'counters/examples': 94368, 'counters/updates': 2949}
skipping logging after 94400 examples to avoid logging too frequently
train stats after 94432 examples: {'rewards_train/chosen': '0.12588', 'rewards_train/rejected': '0.014353', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11153', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-158.3', 'loss/train': '0.66028', 'examples_per_second': '31.735', 'grad_norm': '27.875', 'counters/examples': 94432, 'counters/updates': 2951}
train stats after 94464 examples: {'rewards_train/chosen': '0.15794', 'rewards_train/rejected': '0.16469', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0067504', 'logps_train/rejected': '-171.13', 'logps_train/chosen': '-155.82', 'loss/train': '0.71409', 'examples_per_second': '31.444', 'grad_norm': '54.25', 'counters/examples': 94464, 'counters/updates': 2952}
train stats after 94496 examples: {'rewards_train/chosen': '0.084826', 'rewards_train/rejected': '0.11613', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0313', 'logps_train/rejected': '-159.04', 'logps_train/chosen': '-165.67', 'loss/train': '0.71236', 'examples_per_second': '31.227', 'grad_norm': '47.75', 'counters/examples': 94496, 'counters/updates': 2953}
train stats after 94528 examples: {'rewards_train/chosen': '0.12034', 'rewards_train/rejected': '0.13624', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015902', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-119.01', 'loss/train': '0.70984', 'examples_per_second': '30.188', 'grad_norm': '34', 'counters/examples': 94528, 'counters/updates': 2954}
train stats after 94560 examples: {'rewards_train/chosen': '0.15853', 'rewards_train/rejected': '0.05904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099485', 'logps_train/rejected': '-105.09', 'logps_train/chosen': '-149.17', 'loss/train': '0.65538', 'examples_per_second': '31.298', 'grad_norm': '26.25', 'counters/examples': 94560, 'counters/updates': 2955}
train stats after 94592 examples: {'rewards_train/chosen': '0.14181', 'rewards_train/rejected': '0.12504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016774', 'logps_train/rejected': '-151.53', 'logps_train/chosen': '-144.62', 'loss/train': '0.69268', 'examples_per_second': '31.227', 'grad_norm': '36.5', 'counters/examples': 94592, 'counters/updates': 2956}
train stats after 94624 examples: {'rewards_train/chosen': '0.12025', 'rewards_train/rejected': '0.06412', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05613', 'logps_train/rejected': '-138.95', 'logps_train/chosen': '-182.65', 'loss/train': '0.67204', 'examples_per_second': '30.132', 'grad_norm': '30.75', 'counters/examples': 94624, 'counters/updates': 2957}
train stats after 94656 examples: {'rewards_train/chosen': '0.10004', 'rewards_train/rejected': '0.025269', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07477', 'logps_train/rejected': '-146.22', 'logps_train/chosen': '-162.67', 'loss/train': '0.66488', 'examples_per_second': '33.058', 'grad_norm': '29.375', 'counters/examples': 94656, 'counters/updates': 2958}
train stats after 94688 examples: {'rewards_train/chosen': '0.084198', 'rewards_train/rejected': '0.033184', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051014', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-104.59', 'loss/train': '0.6727', 'examples_per_second': '31.288', 'grad_norm': '30.375', 'counters/examples': 94688, 'counters/updates': 2959}
skipping logging after 94720 examples to avoid logging too frequently
train stats after 94752 examples: {'rewards_train/chosen': '0.1397', 'rewards_train/rejected': '0.12051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019197', 'logps_train/rejected': '-152.53', 'logps_train/chosen': '-142.99', 'loss/train': '0.69341', 'examples_per_second': '30.963', 'grad_norm': '44.25', 'counters/examples': 94752, 'counters/updates': 2961}
train stats after 94784 examples: {'rewards_train/chosen': '0.1031', 'rewards_train/rejected': '0.023387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079709', 'logps_train/rejected': '-112.11', 'logps_train/chosen': '-128.17', 'loss/train': '0.66136', 'examples_per_second': '31.459', 'grad_norm': '28.375', 'counters/examples': 94784, 'counters/updates': 2962}
skipping logging after 94816 examples to avoid logging too frequently
train stats after 94848 examples: {'rewards_train/chosen': '0.086286', 'rewards_train/rejected': '0.061963', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024323', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-140.62', 'loss/train': '0.69717', 'examples_per_second': '32.212', 'grad_norm': '38', 'counters/examples': 94848, 'counters/updates': 2964}
train stats after 94880 examples: {'rewards_train/chosen': '0.062237', 'rewards_train/rejected': '-0.024413', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08665', 'logps_train/rejected': '-97.594', 'logps_train/chosen': '-138.48', 'loss/train': '0.65521', 'examples_per_second': '31.486', 'grad_norm': '36.25', 'counters/examples': 94880, 'counters/updates': 2965}
train stats after 94912 examples: {'rewards_train/chosen': '0.13678', 'rewards_train/rejected': '0.17357', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.036797', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-129.59', 'loss/train': '0.71983', 'examples_per_second': '31.851', 'grad_norm': '31.5', 'counters/examples': 94912, 'counters/updates': 2966}
skipping logging after 94944 examples to avoid logging too frequently
train stats after 94976 examples: {'rewards_train/chosen': '0.062704', 'rewards_train/rejected': '0.089231', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026527', 'logps_train/rejected': '-161.82', 'logps_train/chosen': '-136.23', 'loss/train': '0.71385', 'examples_per_second': '32.235', 'grad_norm': '42.25', 'counters/examples': 94976, 'counters/updates': 2968}
train stats after 95008 examples: {'rewards_train/chosen': '0.14578', 'rewards_train/rejected': '0.076022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069753', 'logps_train/rejected': '-143.57', 'logps_train/chosen': '-144.27', 'loss/train': '0.67068', 'examples_per_second': '32.271', 'grad_norm': '34.25', 'counters/examples': 95008, 'counters/updates': 2969}
train stats after 95040 examples: {'rewards_train/chosen': '0.10342', 'rewards_train/rejected': '0.18427', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.08085', 'logps_train/rejected': '-147.43', 'logps_train/chosen': '-126.68', 'loss/train': '0.74802', 'examples_per_second': '30.697', 'grad_norm': '65', 'counters/examples': 95040, 'counters/updates': 2970}
train stats after 95072 examples: {'rewards_train/chosen': '0.078645', 'rewards_train/rejected': '0.020309', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.058336', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-161.6', 'loss/train': '0.67221', 'examples_per_second': '31.486', 'grad_norm': '30.375', 'counters/examples': 95072, 'counters/updates': 2971}
train stats after 95104 examples: {'rewards_train/chosen': '0.21521', 'rewards_train/rejected': '0.11321', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.102', 'logps_train/rejected': '-105.05', 'logps_train/chosen': '-159.89', 'loss/train': '0.65932', 'examples_per_second': '31.484', 'grad_norm': '26.125', 'counters/examples': 95104, 'counters/updates': 2972}
train stats after 95136 examples: {'rewards_train/chosen': '0.16343', 'rewards_train/rejected': '0.045601', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11783', 'logps_train/rejected': '-112.65', 'logps_train/chosen': '-120', 'loss/train': '0.64684', 'examples_per_second': '31.718', 'grad_norm': '39.5', 'counters/examples': 95136, 'counters/updates': 2973}
skipping logging after 95168 examples to avoid logging too frequently
train stats after 95200 examples: {'rewards_train/chosen': '0.098432', 'rewards_train/rejected': '0.061187', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037245', 'logps_train/rejected': '-95.99', 'logps_train/chosen': '-136.77', 'loss/train': '0.6833', 'examples_per_second': '32.406', 'grad_norm': '54', 'counters/examples': 95200, 'counters/updates': 2975}
train stats after 95232 examples: {'rewards_train/chosen': '0.076575', 'rewards_train/rejected': '0.12311', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046532', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-126.43', 'loss/train': '0.72412', 'examples_per_second': '33.017', 'grad_norm': '39', 'counters/examples': 95232, 'counters/updates': 2976}
train stats after 95264 examples: {'rewards_train/chosen': '0.10944', 'rewards_train/rejected': '0.041169', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068274', 'logps_train/rejected': '-114.45', 'logps_train/chosen': '-129.7', 'loss/train': '0.66397', 'examples_per_second': '30.844', 'grad_norm': '26.625', 'counters/examples': 95264, 'counters/updates': 2977}
train stats after 95296 examples: {'rewards_train/chosen': '0.14328', 'rewards_train/rejected': '0.095445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047832', 'logps_train/rejected': '-110.17', 'logps_train/chosen': '-135.21', 'loss/train': '0.67817', 'examples_per_second': '31.49', 'grad_norm': '33.5', 'counters/examples': 95296, 'counters/updates': 2978}
train stats after 95328 examples: {'rewards_train/chosen': '0.15101', 'rewards_train/rejected': '0.13152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01949', 'logps_train/rejected': '-153.82', 'logps_train/chosen': '-162.04', 'loss/train': '0.69658', 'examples_per_second': '32.189', 'grad_norm': '29.375', 'counters/examples': 95328, 'counters/updates': 2979}
train stats after 95360 examples: {'rewards_train/chosen': '0.049612', 'rewards_train/rejected': '0.059651', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010039', 'logps_train/rejected': '-149.09', 'logps_train/chosen': '-119.98', 'loss/train': '0.70414', 'examples_per_second': '31.474', 'grad_norm': '41.25', 'counters/examples': 95360, 'counters/updates': 2980}
train stats after 95392 examples: {'rewards_train/chosen': '0.085678', 'rewards_train/rejected': '0.10305', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017368', 'logps_train/rejected': '-130.42', 'logps_train/chosen': '-142.53', 'loss/train': '0.70869', 'examples_per_second': '31.465', 'grad_norm': '37', 'counters/examples': 95392, 'counters/updates': 2981}
train stats after 95424 examples: {'rewards_train/chosen': '0.09376', 'rewards_train/rejected': '0.049352', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044408', 'logps_train/rejected': '-151.94', 'logps_train/chosen': '-159.32', 'loss/train': '0.68918', 'examples_per_second': '30.233', 'grad_norm': '48.5', 'counters/examples': 95424, 'counters/updates': 2982}
train stats after 95456 examples: {'rewards_train/chosen': '0.066683', 'rewards_train/rejected': '0.085863', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.01918', 'logps_train/rejected': '-98.241', 'logps_train/chosen': '-135.1', 'loss/train': '0.70915', 'examples_per_second': '30.08', 'grad_norm': '28.75', 'counters/examples': 95456, 'counters/updates': 2983}
train stats after 95488 examples: {'rewards_train/chosen': '0.10084', 'rewards_train/rejected': '0.058425', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042417', 'logps_train/rejected': '-168.16', 'logps_train/chosen': '-136.94', 'loss/train': '0.67846', 'examples_per_second': '31.496', 'grad_norm': '27.75', 'counters/examples': 95488, 'counters/updates': 2984}
train stats after 95520 examples: {'rewards_train/chosen': '0.14255', 'rewards_train/rejected': '0.033803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10875', 'logps_train/rejected': '-136.95', 'logps_train/chosen': '-140.95', 'loss/train': '0.6515', 'examples_per_second': '33.13', 'grad_norm': '31.625', 'counters/examples': 95520, 'counters/updates': 2985}
train stats after 95552 examples: {'rewards_train/chosen': '0.097427', 'rewards_train/rejected': '0.1359', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.038473', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-154.76', 'loss/train': '0.72195', 'examples_per_second': '31.522', 'grad_norm': '30.625', 'counters/examples': 95552, 'counters/updates': 2986}
train stats after 95584 examples: {'rewards_train/chosen': '0.11209', 'rewards_train/rejected': '0.085876', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02621', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-138.3', 'loss/train': '0.68542', 'examples_per_second': '30.984', 'grad_norm': '31.875', 'counters/examples': 95584, 'counters/updates': 2987}
train stats after 95616 examples: {'rewards_train/chosen': '0.16006', 'rewards_train/rejected': '0.016565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14349', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-187.76', 'loss/train': '0.63421', 'examples_per_second': '31.421', 'grad_norm': '30.875', 'counters/examples': 95616, 'counters/updates': 2988}
train stats after 95648 examples: {'rewards_train/chosen': '0.094926', 'rewards_train/rejected': '0.048788', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046138', 'logps_train/rejected': '-126.83', 'logps_train/chosen': '-162.08', 'loss/train': '0.67611', 'examples_per_second': '32.921', 'grad_norm': '34', 'counters/examples': 95648, 'counters/updates': 2989}
train stats after 95680 examples: {'rewards_train/chosen': '0.014671', 'rewards_train/rejected': '0.032925', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.018255', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-175.08', 'loss/train': '0.7155', 'examples_per_second': '23.901', 'grad_norm': '56.75', 'counters/examples': 95680, 'counters/updates': 2990}
train stats after 95712 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '0.12496', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0052197', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-125.26', 'loss/train': '0.71347', 'examples_per_second': '32.236', 'grad_norm': '40.5', 'counters/examples': 95712, 'counters/updates': 2991}
train stats after 95744 examples: {'rewards_train/chosen': '0.046725', 'rewards_train/rejected': '-0.016902', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063627', 'logps_train/rejected': '-154.44', 'logps_train/chosen': '-145.89', 'loss/train': '0.67043', 'examples_per_second': '29.787', 'grad_norm': '35.25', 'counters/examples': 95744, 'counters/updates': 2992}
train stats after 95776 examples: {'rewards_train/chosen': '0.080283', 'rewards_train/rejected': '0.12202', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.041739', 'logps_train/rejected': '-124.06', 'logps_train/chosen': '-140.63', 'loss/train': '0.72238', 'examples_per_second': '24.236', 'grad_norm': '33.75', 'counters/examples': 95776, 'counters/updates': 2993}
train stats after 95808 examples: {'rewards_train/chosen': '0.031809', 'rewards_train/rejected': '0.085872', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.054063', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-147.36', 'loss/train': '0.73946', 'examples_per_second': '30.09', 'grad_norm': '52.5', 'counters/examples': 95808, 'counters/updates': 2994}
skipping logging after 95840 examples to avoid logging too frequently
train stats after 95872 examples: {'rewards_train/chosen': '0.14385', 'rewards_train/rejected': '0.095936', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047915', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-139.15', 'loss/train': '0.67731', 'examples_per_second': '30.971', 'grad_norm': '28', 'counters/examples': 95872, 'counters/updates': 2996}
train stats after 95904 examples: {'rewards_train/chosen': '0.11206', 'rewards_train/rejected': '0.14049', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.028437', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-164.47', 'loss/train': '0.7231', 'examples_per_second': '31.188', 'grad_norm': '41', 'counters/examples': 95904, 'counters/updates': 2997}
train stats after 95936 examples: {'rewards_train/chosen': '0.057309', 'rewards_train/rejected': '0.14098', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.083671', 'logps_train/rejected': '-106.95', 'logps_train/chosen': '-105.07', 'loss/train': '0.76679', 'examples_per_second': '30.523', 'grad_norm': '89.5', 'counters/examples': 95936, 'counters/updates': 2998}
skipping logging after 95968 examples to avoid logging too frequently
train stats after 96000 examples: {'rewards_train/chosen': '0.12186', 'rewards_train/rejected': '0.091724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030137', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-164.51', 'loss/train': '0.68735', 'examples_per_second': '31.455', 'grad_norm': '53.5', 'counters/examples': 96000, 'counters/updates': 3000}
Running evaluation after 96000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 96000: {'rewards_eval/chosen': '0.11123', 'rewards_eval/rejected': '0.061947', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.04928', 'logps_eval/rejected': '-121.51', 'logps_eval/chosen': '-142.99', 'loss/eval': '0.67894'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349/step-96000...
writing checkpoint to .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349/step-96000/policy.pt...
train stats after 96032 examples: {'rewards_train/chosen': '0.094658', 'rewards_train/rejected': '0.034256', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060402', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-181.04', 'loss/train': '0.67423', 'examples_per_second': '14.102', 'grad_norm': '30.125', 'counters/examples': 96032, 'counters/updates': 3001}
train stats after 96064 examples: {'rewards_train/chosen': '0.11871', 'rewards_train/rejected': '0.0099964', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10871', 'logps_train/rejected': '-101.05', 'logps_train/chosen': '-152.84', 'loss/train': '0.64403', 'examples_per_second': '30.33', 'grad_norm': '27', 'counters/examples': 96064, 'counters/updates': 3002}
train stats after 96096 examples: {'rewards_train/chosen': '0.1685', 'rewards_train/rejected': '0.072457', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.096043', 'logps_train/rejected': '-106.35', 'logps_train/chosen': '-162.99', 'loss/train': '0.65734', 'examples_per_second': '31.843', 'grad_norm': '29.875', 'counters/examples': 96096, 'counters/updates': 3003}
train stats after 96128 examples: {'rewards_train/chosen': '0.12849', 'rewards_train/rejected': '0.052513', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075978', 'logps_train/rejected': '-141.87', 'logps_train/chosen': '-157.82', 'loss/train': '0.66334', 'examples_per_second': '31.419', 'grad_norm': '31.125', 'counters/examples': 96128, 'counters/updates': 3004}
train stats after 96160 examples: {'rewards_train/chosen': '0.049043', 'rewards_train/rejected': '0.030819', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018224', 'logps_train/rejected': '-106.86', 'logps_train/chosen': '-127.9', 'loss/train': '0.69479', 'examples_per_second': '30.154', 'grad_norm': '29.5', 'counters/examples': 96160, 'counters/updates': 3005}
train stats after 96192 examples: {'rewards_train/chosen': '0.10783', 'rewards_train/rejected': '0.052573', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055258', 'logps_train/rejected': '-107.08', 'logps_train/chosen': '-124.75', 'loss/train': '0.67752', 'examples_per_second': '31.577', 'grad_norm': '29.25', 'counters/examples': 96192, 'counters/updates': 3006}
train stats after 96224 examples: {'rewards_train/chosen': '0.11761', 'rewards_train/rejected': '0.054603', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063009', 'logps_train/rejected': '-122.4', 'logps_train/chosen': '-172.64', 'loss/train': '0.67165', 'examples_per_second': '30.394', 'grad_norm': '46.25', 'counters/examples': 96224, 'counters/updates': 3007}
train stats after 96256 examples: {'rewards_train/chosen': '0.14669', 'rewards_train/rejected': '0.024305', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12238', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-165.72', 'loss/train': '0.64044', 'examples_per_second': '32.448', 'grad_norm': '25', 'counters/examples': 96256, 'counters/updates': 3008}
train stats after 96288 examples: {'rewards_train/chosen': '0.1296', 'rewards_train/rejected': '0.034922', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094673', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-128.38', 'loss/train': '0.6531', 'examples_per_second': '31.341', 'grad_norm': '35.25', 'counters/examples': 96288, 'counters/updates': 3009}
train stats after 96320 examples: {'rewards_train/chosen': '0.13077', 'rewards_train/rejected': '0.088566', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042203', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-153.36', 'loss/train': '0.68395', 'examples_per_second': '31.025', 'grad_norm': '29.125', 'counters/examples': 96320, 'counters/updates': 3010}
train stats after 96352 examples: {'rewards_train/chosen': '0.17602', 'rewards_train/rejected': '0.10463', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071387', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-144.94', 'loss/train': '0.66541', 'examples_per_second': '30.535', 'grad_norm': '33.5', 'counters/examples': 96352, 'counters/updates': 3011}
train stats after 96384 examples: {'rewards_train/chosen': '0.20356', 'rewards_train/rejected': '0.088309', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11525', 'logps_train/rejected': '-140.01', 'logps_train/chosen': '-186.16', 'loss/train': '0.65493', 'examples_per_second': '31.075', 'grad_norm': '36.5', 'counters/examples': 96384, 'counters/updates': 3012}
train stats after 96416 examples: {'rewards_train/chosen': '0.16062', 'rewards_train/rejected': '0.040971', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11965', 'logps_train/rejected': '-122.76', 'logps_train/chosen': '-123.9', 'loss/train': '0.64546', 'examples_per_second': '30.227', 'grad_norm': '36.25', 'counters/examples': 96416, 'counters/updates': 3013}
train stats after 96448 examples: {'rewards_train/chosen': '0.095222', 'rewards_train/rejected': '0.050242', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04498', 'logps_train/rejected': '-98.49', 'logps_train/chosen': '-136.25', 'loss/train': '0.67859', 'examples_per_second': '30.748', 'grad_norm': '35.75', 'counters/examples': 96448, 'counters/updates': 3014}
train stats after 96480 examples: {'rewards_train/chosen': '0.10369', 'rewards_train/rejected': '0.061299', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042395', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-189.4', 'loss/train': '0.68334', 'examples_per_second': '31.473', 'grad_norm': '41.75', 'counters/examples': 96480, 'counters/updates': 3015}
train stats after 96512 examples: {'rewards_train/chosen': '0.11955', 'rewards_train/rejected': '0.094419', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025128', 'logps_train/rejected': '-131.78', 'logps_train/chosen': '-147.81', 'loss/train': '0.6964', 'examples_per_second': '31.399', 'grad_norm': '39.75', 'counters/examples': 96512, 'counters/updates': 3016}
train stats after 96544 examples: {'rewards_train/chosen': '0.083033', 'rewards_train/rejected': '-0.0014306', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.084464', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-131.56', 'loss/train': '0.6592', 'examples_per_second': '31.576', 'grad_norm': '26.125', 'counters/examples': 96544, 'counters/updates': 3017}
skipping logging after 96576 examples to avoid logging too frequently
train stats after 96608 examples: {'rewards_train/chosen': '0.101', 'rewards_train/rejected': '0.057606', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043389', 'logps_train/rejected': '-168.37', 'logps_train/chosen': '-153.74', 'loss/train': '0.68156', 'examples_per_second': '31.297', 'grad_norm': '56.5', 'counters/examples': 96608, 'counters/updates': 3019}
train stats after 96640 examples: {'rewards_train/chosen': '0.13486', 'rewards_train/rejected': '0.099677', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035179', 'logps_train/rejected': '-165.32', 'logps_train/chosen': '-148.4', 'loss/train': '0.69441', 'examples_per_second': '31.455', 'grad_norm': '43.5', 'counters/examples': 96640, 'counters/updates': 3020}
train stats after 96672 examples: {'rewards_train/chosen': '0.10409', 'rewards_train/rejected': '0.084463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019628', 'logps_train/rejected': '-144.91', 'logps_train/chosen': '-157.84', 'loss/train': '0.69438', 'examples_per_second': '31.533', 'grad_norm': '49.25', 'counters/examples': 96672, 'counters/updates': 3021}
train stats after 96704 examples: {'rewards_train/chosen': '0.078623', 'rewards_train/rejected': '0.066073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012551', 'logps_train/rejected': '-100.82', 'logps_train/chosen': '-125.34', 'loss/train': '0.6953', 'examples_per_second': '31.148', 'grad_norm': '28.5', 'counters/examples': 96704, 'counters/updates': 3022}
train stats after 96736 examples: {'rewards_train/chosen': '0.11311', 'rewards_train/rejected': '0.01689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096216', 'logps_train/rejected': '-114.43', 'logps_train/chosen': '-153.34', 'loss/train': '0.65417', 'examples_per_second': '32.29', 'grad_norm': '32.5', 'counters/examples': 96736, 'counters/updates': 3023}
train stats after 96768 examples: {'rewards_train/chosen': '0.13249', 'rewards_train/rejected': '0.067721', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064769', 'logps_train/rejected': '-157.39', 'logps_train/chosen': '-158.69', 'loss/train': '0.67495', 'examples_per_second': '31.194', 'grad_norm': '29.75', 'counters/examples': 96768, 'counters/updates': 3024}
train stats after 96800 examples: {'rewards_train/chosen': '0.055705', 'rewards_train/rejected': '0.064061', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0083556', 'logps_train/rejected': '-127.04', 'logps_train/chosen': '-130.1', 'loss/train': '0.70283', 'examples_per_second': '30.99', 'grad_norm': '28.125', 'counters/examples': 96800, 'counters/updates': 3025}
skipping logging after 96832 examples to avoid logging too frequently
train stats after 96864 examples: {'rewards_train/chosen': '0.13225', 'rewards_train/rejected': '0.10959', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02266', 'logps_train/rejected': '-150.03', 'logps_train/chosen': '-153.23', 'loss/train': '0.68983', 'examples_per_second': '31.573', 'grad_norm': '31.5', 'counters/examples': 96864, 'counters/updates': 3027}
train stats after 96896 examples: {'rewards_train/chosen': '0.24255', 'rewards_train/rejected': '0.16816', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074387', 'logps_train/rejected': '-123.26', 'logps_train/chosen': '-150.29', 'loss/train': '0.66865', 'examples_per_second': '31.119', 'grad_norm': '41.75', 'counters/examples': 96896, 'counters/updates': 3028}
train stats after 96928 examples: {'rewards_train/chosen': '0.098621', 'rewards_train/rejected': '0.013308', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085313', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-166.36', 'loss/train': '0.66051', 'examples_per_second': '32.16', 'grad_norm': '28.625', 'counters/examples': 96928, 'counters/updates': 3029}
train stats after 96960 examples: {'rewards_train/chosen': '0.13245', 'rewards_train/rejected': '0.061438', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.071009', 'logps_train/rejected': '-121.95', 'logps_train/chosen': '-131.27', 'loss/train': '0.66355', 'examples_per_second': '32.394', 'grad_norm': '29.75', 'counters/examples': 96960, 'counters/updates': 3030}
train stats after 96992 examples: {'rewards_train/chosen': '0.063472', 'rewards_train/rejected': '0.040103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023369', 'logps_train/rejected': '-134.79', 'logps_train/chosen': '-153.9', 'loss/train': '0.68686', 'examples_per_second': '31.087', 'grad_norm': '34.25', 'counters/examples': 96992, 'counters/updates': 3031}
train stats after 97024 examples: {'rewards_train/chosen': '0.0035857', 'rewards_train/rejected': '-0.037461', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041047', 'logps_train/rejected': '-117.46', 'logps_train/chosen': '-118.41', 'loss/train': '0.67646', 'examples_per_second': '31.496', 'grad_norm': '37.5', 'counters/examples': 97024, 'counters/updates': 3032}
train stats after 97056 examples: {'rewards_train/chosen': '0.015777', 'rewards_train/rejected': '0.037723', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021946', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-134.44', 'loss/train': '0.71542', 'examples_per_second': '31.455', 'grad_norm': '35.5', 'counters/examples': 97056, 'counters/updates': 3033}
train stats after 97088 examples: {'rewards_train/chosen': '0.12377', 'rewards_train/rejected': '0.061386', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062388', 'logps_train/rejected': '-112.22', 'logps_train/chosen': '-142.46', 'loss/train': '0.67319', 'examples_per_second': '31.21', 'grad_norm': '32.25', 'counters/examples': 97088, 'counters/updates': 3034}
train stats after 97120 examples: {'rewards_train/chosen': '0.066822', 'rewards_train/rejected': '0.093268', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026446', 'logps_train/rejected': '-152.77', 'logps_train/chosen': '-137.09', 'loss/train': '0.71608', 'examples_per_second': '32.784', 'grad_norm': '45.75', 'counters/examples': 97120, 'counters/updates': 3035}
train stats after 97152 examples: {'rewards_train/chosen': '0.12908', 'rewards_train/rejected': '0.023982', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10509', 'logps_train/rejected': '-182.62', 'logps_train/chosen': '-120.08', 'loss/train': '0.66182', 'examples_per_second': '31.345', 'grad_norm': '34', 'counters/examples': 97152, 'counters/updates': 3036}
train stats after 97184 examples: {'rewards_train/chosen': '0.10926', 'rewards_train/rejected': '0.0091792', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10009', 'logps_train/rejected': '-107.13', 'logps_train/chosen': '-132.89', 'loss/train': '0.64892', 'examples_per_second': '30.593', 'grad_norm': '29.375', 'counters/examples': 97184, 'counters/updates': 3037}
train stats after 97216 examples: {'rewards_train/chosen': '0.16191', 'rewards_train/rejected': '0.11658', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045326', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-143.14', 'loss/train': '0.67593', 'examples_per_second': '32.801', 'grad_norm': '37', 'counters/examples': 97216, 'counters/updates': 3038}
train stats after 97248 examples: {'rewards_train/chosen': '0.10892', 'rewards_train/rejected': '0.17558', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.066656', 'logps_train/rejected': '-143.19', 'logps_train/chosen': '-115.71', 'loss/train': '0.74901', 'examples_per_second': '31.214', 'grad_norm': '42.25', 'counters/examples': 97248, 'counters/updates': 3039}
train stats after 97280 examples: {'rewards_train/chosen': '0.065643', 'rewards_train/rejected': '0.014218', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051425', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-128.31', 'loss/train': '0.67488', 'examples_per_second': '29.969', 'grad_norm': '37.25', 'counters/examples': 97280, 'counters/updates': 3040}
train stats after 97312 examples: {'rewards_train/chosen': '0.14112', 'rewards_train/rejected': '0.13889', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0022351', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-165.02', 'loss/train': '0.70345', 'examples_per_second': '31.447', 'grad_norm': '48.5', 'counters/examples': 97312, 'counters/updates': 3041}
train stats after 97344 examples: {'rewards_train/chosen': '0.16543', 'rewards_train/rejected': '0.051948', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11348', 'logps_train/rejected': '-100.96', 'logps_train/chosen': '-147.33', 'loss/train': '0.65702', 'examples_per_second': '30.811', 'grad_norm': '32.25', 'counters/examples': 97344, 'counters/updates': 3042}
train stats after 97376 examples: {'rewards_train/chosen': '0.17947', 'rewards_train/rejected': '0.058525', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12095', 'logps_train/rejected': '-93.141', 'logps_train/chosen': '-140.71', 'loss/train': '0.64034', 'examples_per_second': '31.41', 'grad_norm': '28.875', 'counters/examples': 97376, 'counters/updates': 3043}
train stats after 97408 examples: {'rewards_train/chosen': '0.11161', 'rewards_train/rejected': '0.062892', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048719', 'logps_train/rejected': '-149.86', 'logps_train/chosen': '-130.39', 'loss/train': '0.67933', 'examples_per_second': '30.497', 'grad_norm': '35', 'counters/examples': 97408, 'counters/updates': 3044}
train stats after 97440 examples: {'rewards_train/chosen': '0.070559', 'rewards_train/rejected': '0.025799', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04476', 'logps_train/rejected': '-192.76', 'logps_train/chosen': '-138.16', 'loss/train': '0.68226', 'examples_per_second': '30.453', 'grad_norm': '36.25', 'counters/examples': 97440, 'counters/updates': 3045}
train stats after 97472 examples: {'rewards_train/chosen': '0.16585', 'rewards_train/rejected': '0.10128', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064574', 'logps_train/rejected': '-129.62', 'logps_train/chosen': '-136.86', 'loss/train': '0.67686', 'examples_per_second': '31.443', 'grad_norm': '39.75', 'counters/examples': 97472, 'counters/updates': 3046}
train stats after 97504 examples: {'rewards_train/chosen': '0.18838', 'rewards_train/rejected': '0.06186', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12652', 'logps_train/rejected': '-107.65', 'logps_train/chosen': '-159.93', 'loss/train': '0.64227', 'examples_per_second': '30.568', 'grad_norm': '33.25', 'counters/examples': 97504, 'counters/updates': 3047}
train stats after 97536 examples: {'rewards_train/chosen': '0.15802', 'rewards_train/rejected': '0.10869', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049328', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-145.78', 'loss/train': '0.67822', 'examples_per_second': '31.544', 'grad_norm': '29.375', 'counters/examples': 97536, 'counters/updates': 3048}
train stats after 97568 examples: {'rewards_train/chosen': '0.19094', 'rewards_train/rejected': '0.082845', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1081', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-125.01', 'loss/train': '0.64688', 'examples_per_second': '32.002', 'grad_norm': '23.75', 'counters/examples': 97568, 'counters/updates': 3049}
train stats after 97600 examples: {'rewards_train/chosen': '0.13661', 'rewards_train/rejected': '0.12181', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.014799', 'logps_train/rejected': '-149.77', 'logps_train/chosen': '-160.13', 'loss/train': '0.69475', 'examples_per_second': '31.309', 'grad_norm': '42', 'counters/examples': 97600, 'counters/updates': 3050}
train stats after 97632 examples: {'rewards_train/chosen': '0.083647', 'rewards_train/rejected': '0.028967', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.05468', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-135.03', 'loss/train': '0.66964', 'examples_per_second': '23.517', 'grad_norm': '24', 'counters/examples': 97632, 'counters/updates': 3051}
train stats after 97664 examples: {'rewards_train/chosen': '0.1241', 'rewards_train/rejected': '0.071593', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05251', 'logps_train/rejected': '-135.44', 'logps_train/chosen': '-134.9', 'loss/train': '0.67566', 'examples_per_second': '31.348', 'grad_norm': '30.875', 'counters/examples': 97664, 'counters/updates': 3052}
train stats after 97696 examples: {'rewards_train/chosen': '0.15918', 'rewards_train/rejected': '0.11621', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042971', 'logps_train/rejected': '-137.78', 'logps_train/chosen': '-161.64', 'loss/train': '0.68211', 'examples_per_second': '31.421', 'grad_norm': '43.75', 'counters/examples': 97696, 'counters/updates': 3053}
train stats after 97728 examples: {'rewards_train/chosen': '0.10634', 'rewards_train/rejected': '0.066377', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039965', 'logps_train/rejected': '-110.15', 'logps_train/chosen': '-166.42', 'loss/train': '0.6792', 'examples_per_second': '31.352', 'grad_norm': '45.25', 'counters/examples': 97728, 'counters/updates': 3054}
skipping logging after 97760 examples to avoid logging too frequently
train stats after 97792 examples: {'rewards_train/chosen': '0.11864', 'rewards_train/rejected': '0.047139', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.071497', 'logps_train/rejected': '-145.67', 'logps_train/chosen': '-172.97', 'loss/train': '0.66878', 'examples_per_second': '31.376', 'grad_norm': '33.5', 'counters/examples': 97792, 'counters/updates': 3056}
skipping logging after 97824 examples to avoid logging too frequently
train stats after 97856 examples: {'rewards_train/chosen': '0.24264', 'rewards_train/rejected': '0.012851', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22978', 'logps_train/rejected': '-141.38', 'logps_train/chosen': '-170.18', 'loss/train': '0.60276', 'examples_per_second': '31.261', 'grad_norm': '32.5', 'counters/examples': 97856, 'counters/updates': 3058}
skipping logging after 97888 examples to avoid logging too frequently
train stats after 97920 examples: {'rewards_train/chosen': '0.1977', 'rewards_train/rejected': '0.071283', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12642', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-130.56', 'loss/train': '0.63867', 'examples_per_second': '32.277', 'grad_norm': '53.75', 'counters/examples': 97920, 'counters/updates': 3060}
train stats after 97952 examples: {'rewards_train/chosen': '0.082822', 'rewards_train/rejected': '0.095619', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012797', 'logps_train/rejected': '-103.56', 'logps_train/chosen': '-103.44', 'loss/train': '0.70515', 'examples_per_second': '32.205', 'grad_norm': '39.5', 'counters/examples': 97952, 'counters/updates': 3061}
train stats after 97984 examples: {'rewards_train/chosen': '0.05094', 'rewards_train/rejected': '0.016401', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034539', 'logps_train/rejected': '-116.21', 'logps_train/chosen': '-108.5', 'loss/train': '0.68298', 'examples_per_second': '31.303', 'grad_norm': '23.5', 'counters/examples': 97984, 'counters/updates': 3062}
train stats after 98016 examples: {'rewards_train/chosen': '0.24409', 'rewards_train/rejected': '0.047211', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19688', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-159.79', 'loss/train': '0.62453', 'examples_per_second': '30.378', 'grad_norm': '29.5', 'counters/examples': 98016, 'counters/updates': 3063}
train stats after 98048 examples: {'rewards_train/chosen': '0.10037', 'rewards_train/rejected': '0.048263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052108', 'logps_train/rejected': '-117.89', 'logps_train/chosen': '-124.94', 'loss/train': '0.67438', 'examples_per_second': '31.86', 'grad_norm': '27', 'counters/examples': 98048, 'counters/updates': 3064}
train stats after 98080 examples: {'rewards_train/chosen': '0.10378', 'rewards_train/rejected': '0.088805', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.014978', 'logps_train/rejected': '-154.12', 'logps_train/chosen': '-136.63', 'loss/train': '0.69533', 'examples_per_second': '30.27', 'grad_norm': '31.25', 'counters/examples': 98080, 'counters/updates': 3065}
train stats after 98112 examples: {'rewards_train/chosen': '0.12438', 'rewards_train/rejected': '-0.023123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1475', 'logps_train/rejected': '-114.57', 'logps_train/chosen': '-169.07', 'loss/train': '0.63402', 'examples_per_second': '31.454', 'grad_norm': '31', 'counters/examples': 98112, 'counters/updates': 3066}
train stats after 98144 examples: {'rewards_train/chosen': '0.15296', 'rewards_train/rejected': '0.071109', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081849', 'logps_train/rejected': '-141.08', 'logps_train/chosen': '-180.01', 'loss/train': '0.6617', 'examples_per_second': '29.925', 'grad_norm': '32.5', 'counters/examples': 98144, 'counters/updates': 3067}
train stats after 98176 examples: {'rewards_train/chosen': '0.14814', 'rewards_train/rejected': '0.048238', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099897', 'logps_train/rejected': '-96.742', 'logps_train/chosen': '-136.49', 'loss/train': '0.65349', 'examples_per_second': '31.441', 'grad_norm': '33.5', 'counters/examples': 98176, 'counters/updates': 3068}
train stats after 98208 examples: {'rewards_train/chosen': '0.049313', 'rewards_train/rejected': '0.026138', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023175', 'logps_train/rejected': '-140.59', 'logps_train/chosen': '-123.4', 'loss/train': '0.68655', 'examples_per_second': '31.397', 'grad_norm': '42', 'counters/examples': 98208, 'counters/updates': 3069}
train stats after 98240 examples: {'rewards_train/chosen': '0.098535', 'rewards_train/rejected': '0.027687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070848', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-131.57', 'loss/train': '0.66692', 'examples_per_second': '30.587', 'grad_norm': '28.375', 'counters/examples': 98240, 'counters/updates': 3070}
train stats after 98272 examples: {'rewards_train/chosen': '0.095664', 'rewards_train/rejected': '0.032843', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062821', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-104.32', 'loss/train': '0.66869', 'examples_per_second': '31.984', 'grad_norm': '31', 'counters/examples': 98272, 'counters/updates': 3071}
train stats after 98304 examples: {'rewards_train/chosen': '0.10225', 'rewards_train/rejected': '0.073559', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028688', 'logps_train/rejected': '-164.06', 'logps_train/chosen': '-126.32', 'loss/train': '0.68641', 'examples_per_second': '31.475', 'grad_norm': '42', 'counters/examples': 98304, 'counters/updates': 3072}
train stats after 98336 examples: {'rewards_train/chosen': '0.1141', 'rewards_train/rejected': '0.032165', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081937', 'logps_train/rejected': '-105.48', 'logps_train/chosen': '-172.83', 'loss/train': '0.65899', 'examples_per_second': '31.406', 'grad_norm': '29', 'counters/examples': 98336, 'counters/updates': 3073}
train stats after 98368 examples: {'rewards_train/chosen': '0.16907', 'rewards_train/rejected': '0.070326', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098741', 'logps_train/rejected': '-145.78', 'logps_train/chosen': '-178.1', 'loss/train': '0.65597', 'examples_per_second': '31.981', 'grad_norm': '37.25', 'counters/examples': 98368, 'counters/updates': 3074}
train stats after 98400 examples: {'rewards_train/chosen': '0.10639', 'rewards_train/rejected': '0.10332', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0030752', 'logps_train/rejected': '-161.06', 'logps_train/chosen': '-147.61', 'loss/train': '0.70374', 'examples_per_second': '32.706', 'grad_norm': '44.5', 'counters/examples': 98400, 'counters/updates': 3075}
train stats after 98432 examples: {'rewards_train/chosen': '0.24592', 'rewards_train/rejected': '0.07067', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.17525', 'logps_train/rejected': '-140.86', 'logps_train/chosen': '-168.07', 'loss/train': '0.61729', 'examples_per_second': '31.438', 'grad_norm': '34.75', 'counters/examples': 98432, 'counters/updates': 3076}
skipping logging after 98464 examples to avoid logging too frequently
train stats after 98496 examples: {'rewards_train/chosen': '0.1981', 'rewards_train/rejected': '0.10108', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097025', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-150.13', 'loss/train': '0.65743', 'examples_per_second': '29.878', 'grad_norm': '41.75', 'counters/examples': 98496, 'counters/updates': 3078}
train stats after 98528 examples: {'rewards_train/chosen': '0.078658', 'rewards_train/rejected': '0.054484', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.024174', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-155.49', 'loss/train': '0.68405', 'examples_per_second': '29.853', 'grad_norm': '30.5', 'counters/examples': 98528, 'counters/updates': 3079}
train stats after 98560 examples: {'rewards_train/chosen': '0.14084', 'rewards_train/rejected': '0.052356', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088485', 'logps_train/rejected': '-133.57', 'logps_train/chosen': '-151.21', 'loss/train': '0.66206', 'examples_per_second': '31.3', 'grad_norm': '47.25', 'counters/examples': 98560, 'counters/updates': 3080}
skipping logging after 98592 examples to avoid logging too frequently
train stats after 98624 examples: {'rewards_train/chosen': '0.10244', 'rewards_train/rejected': '0.057476', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.044961', 'logps_train/rejected': '-139.45', 'logps_train/chosen': '-145.04', 'loss/train': '0.67573', 'examples_per_second': '31.05', 'grad_norm': '29.25', 'counters/examples': 98624, 'counters/updates': 3082}
train stats after 98656 examples: {'rewards_train/chosen': '0.1656', 'rewards_train/rejected': '0.013303', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15229', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-147.71', 'loss/train': '0.62828', 'examples_per_second': '30.701', 'grad_norm': '26.5', 'counters/examples': 98656, 'counters/updates': 3083}
train stats after 98688 examples: {'rewards_train/chosen': '0.08669', 'rewards_train/rejected': '0.03949', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047201', 'logps_train/rejected': '-115.19', 'logps_train/chosen': '-140.35', 'loss/train': '0.6763', 'examples_per_second': '31.117', 'grad_norm': '32.75', 'counters/examples': 98688, 'counters/updates': 3084}
train stats after 98720 examples: {'rewards_train/chosen': '0.036335', 'rewards_train/rejected': '0.039236', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0029004', 'logps_train/rejected': '-123.69', 'logps_train/chosen': '-152.91', 'loss/train': '0.70513', 'examples_per_second': '30.345', 'grad_norm': '40.25', 'counters/examples': 98720, 'counters/updates': 3085}
train stats after 98752 examples: {'rewards_train/chosen': '0.085204', 'rewards_train/rejected': '0.057121', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028083', 'logps_train/rejected': '-157.31', 'logps_train/chosen': '-134.71', 'loss/train': '0.68655', 'examples_per_second': '31.438', 'grad_norm': '27.625', 'counters/examples': 98752, 'counters/updates': 3086}
skipping logging after 98784 examples to avoid logging too frequently
train stats after 98816 examples: {'rewards_train/chosen': '0.093313', 'rewards_train/rejected': '0.16319', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.069875', 'logps_train/rejected': '-144.24', 'logps_train/chosen': '-125.6', 'loss/train': '0.75417', 'examples_per_second': '31.577', 'grad_norm': '50.25', 'counters/examples': 98816, 'counters/updates': 3088}
train stats after 98848 examples: {'rewards_train/chosen': '0.19632', 'rewards_train/rejected': '0.093023', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1033', 'logps_train/rejected': '-174.3', 'logps_train/chosen': '-168.19', 'loss/train': '0.65972', 'examples_per_second': '31.425', 'grad_norm': '29.75', 'counters/examples': 98848, 'counters/updates': 3089}
train stats after 98880 examples: {'rewards_train/chosen': '0.13066', 'rewards_train/rejected': '0.032385', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098276', 'logps_train/rejected': '-103.77', 'logps_train/chosen': '-133.87', 'loss/train': '0.65711', 'examples_per_second': '31.463', 'grad_norm': '26.25', 'counters/examples': 98880, 'counters/updates': 3090}
train stats after 98912 examples: {'rewards_train/chosen': '0.15444', 'rewards_train/rejected': '0.029746', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1247', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-160', 'loss/train': '0.64362', 'examples_per_second': '31.428', 'grad_norm': '28', 'counters/examples': 98912, 'counters/updates': 3091}
skipping logging after 98944 examples to avoid logging too frequently
train stats after 98976 examples: {'rewards_train/chosen': '0.10665', 'rewards_train/rejected': '0.064762', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041892', 'logps_train/rejected': '-111.49', 'logps_train/chosen': '-143.42', 'loss/train': '0.67894', 'examples_per_second': '33.975', 'grad_norm': '29', 'counters/examples': 98976, 'counters/updates': 3093}
train stats after 99008 examples: {'rewards_train/chosen': '0.077056', 'rewards_train/rejected': '-0.0085262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085582', 'logps_train/rejected': '-134.5', 'logps_train/chosen': '-137.36', 'loss/train': '0.66789', 'examples_per_second': '30.201', 'grad_norm': '32.75', 'counters/examples': 99008, 'counters/updates': 3094}
train stats after 99040 examples: {'rewards_train/chosen': '0.11567', 'rewards_train/rejected': '0.017211', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098455', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-123.31', 'loss/train': '0.65026', 'examples_per_second': '31.419', 'grad_norm': '45.25', 'counters/examples': 99040, 'counters/updates': 3095}
train stats after 99072 examples: {'rewards_train/chosen': '0.10036', 'rewards_train/rejected': '0.14448', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.044123', 'logps_train/rejected': '-134.84', 'logps_train/chosen': '-138.46', 'loss/train': '0.7241', 'examples_per_second': '32.338', 'grad_norm': '41', 'counters/examples': 99072, 'counters/updates': 3096}
train stats after 99104 examples: {'rewards_train/chosen': '0.093844', 'rewards_train/rejected': '0.0091877', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084656', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-144.31', 'loss/train': '0.65558', 'examples_per_second': '30.409', 'grad_norm': '32.5', 'counters/examples': 99104, 'counters/updates': 3097}
skipping logging after 99136 examples to avoid logging too frequently
train stats after 99168 examples: {'rewards_train/chosen': '0.13441', 'rewards_train/rejected': '0.034809', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099602', 'logps_train/rejected': '-108.11', 'logps_train/chosen': '-122.94', 'loss/train': '0.65476', 'examples_per_second': '32.439', 'grad_norm': '24.75', 'counters/examples': 99168, 'counters/updates': 3099}
train stats after 99200 examples: {'rewards_train/chosen': '0.079516', 'rewards_train/rejected': '0.13581', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.056291', 'logps_train/rejected': '-145.69', 'logps_train/chosen': '-137.41', 'loss/train': '0.73453', 'examples_per_second': '30.22', 'grad_norm': '32', 'counters/examples': 99200, 'counters/updates': 3100}
train stats after 99232 examples: {'rewards_train/chosen': '0.11795', 'rewards_train/rejected': '0.025433', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092517', 'logps_train/rejected': '-119.16', 'logps_train/chosen': '-141.74', 'loss/train': '0.65276', 'examples_per_second': '32.473', 'grad_norm': '29.375', 'counters/examples': 99232, 'counters/updates': 3101}
train stats after 99264 examples: {'rewards_train/chosen': '0.084188', 'rewards_train/rejected': '0.034781', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.049407', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-150.73', 'loss/train': '0.67699', 'examples_per_second': '30.848', 'grad_norm': '26', 'counters/examples': 99264, 'counters/updates': 3102}
train stats after 99296 examples: {'rewards_train/chosen': '0.12454', 'rewards_train/rejected': '0.08692', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037621', 'logps_train/rejected': '-142.09', 'logps_train/chosen': '-125.66', 'loss/train': '0.68553', 'examples_per_second': '31.415', 'grad_norm': '38.25', 'counters/examples': 99296, 'counters/updates': 3103}
train stats after 99328 examples: {'rewards_train/chosen': '0.10417', 'rewards_train/rejected': '0.031993', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072172', 'logps_train/rejected': '-129.61', 'logps_train/chosen': '-160.13', 'loss/train': '0.66536', 'examples_per_second': '30.436', 'grad_norm': '34.5', 'counters/examples': 99328, 'counters/updates': 3104}
train stats after 99360 examples: {'rewards_train/chosen': '0.070616', 'rewards_train/rejected': '-0.036236', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10685', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-120.09', 'loss/train': '0.64708', 'examples_per_second': '32.572', 'grad_norm': '36.5', 'counters/examples': 99360, 'counters/updates': 3105}
train stats after 99392 examples: {'rewards_train/chosen': '0.10779', 'rewards_train/rejected': '0.030887', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076899', 'logps_train/rejected': '-139.55', 'logps_train/chosen': '-147.39', 'loss/train': '0.66528', 'examples_per_second': '29.937', 'grad_norm': '65', 'counters/examples': 99392, 'counters/updates': 3106}
train stats after 99424 examples: {'rewards_train/chosen': '0.071522', 'rewards_train/rejected': '0.037359', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034162', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-157.82', 'loss/train': '0.68471', 'examples_per_second': '32.329', 'grad_norm': '29.875', 'counters/examples': 99424, 'counters/updates': 3107}
train stats after 99456 examples: {'rewards_train/chosen': '0.10567', 'rewards_train/rejected': '0.0023857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10329', 'logps_train/rejected': '-135', 'logps_train/chosen': '-140.61', 'loss/train': '0.64982', 'examples_per_second': '32.269', 'grad_norm': '34.25', 'counters/examples': 99456, 'counters/updates': 3108}
train stats after 99488 examples: {'rewards_train/chosen': '0.12451', 'rewards_train/rejected': '-0.017389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14189', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-126.04', 'loss/train': '0.63315', 'examples_per_second': '33.014', 'grad_norm': '36.25', 'counters/examples': 99488, 'counters/updates': 3109}
train stats after 99520 examples: {'rewards_train/chosen': '0.085721', 'rewards_train/rejected': '0.13211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.046387', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-132.8', 'loss/train': '0.73743', 'examples_per_second': '32.182', 'grad_norm': '33.75', 'counters/examples': 99520, 'counters/updates': 3110}
train stats after 99552 examples: {'rewards_train/chosen': '0.092532', 'rewards_train/rejected': '-0.024899', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11743', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-128.92', 'loss/train': '0.64783', 'examples_per_second': '32.61', 'grad_norm': '30', 'counters/examples': 99552, 'counters/updates': 3111}
train stats after 99584 examples: {'rewards_train/chosen': '0.13027', 'rewards_train/rejected': '0.020257', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11001', 'logps_train/rejected': '-144.42', 'logps_train/chosen': '-165.52', 'loss/train': '0.6465', 'examples_per_second': '31.567', 'grad_norm': '27.5', 'counters/examples': 99584, 'counters/updates': 3112}
skipping logging after 99616 examples to avoid logging too frequently
train stats after 99648 examples: {'rewards_train/chosen': '0.025457', 'rewards_train/rejected': '0.043938', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.01848', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-128.28', 'loss/train': '0.71313', 'examples_per_second': '32.686', 'grad_norm': '44.25', 'counters/examples': 99648, 'counters/updates': 3114}
train stats after 99680 examples: {'rewards_train/chosen': '0.076508', 'rewards_train/rejected': '0.019816', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056693', 'logps_train/rejected': '-103.78', 'logps_train/chosen': '-126.74', 'loss/train': '0.67131', 'examples_per_second': '31.383', 'grad_norm': '33.75', 'counters/examples': 99680, 'counters/updates': 3115}
skipping logging after 99712 examples to avoid logging too frequently
train stats after 99744 examples: {'rewards_train/chosen': '0.082761', 'rewards_train/rejected': '0.038414', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044347', 'logps_train/rejected': '-92.947', 'logps_train/chosen': '-115.36', 'loss/train': '0.67389', 'examples_per_second': '31.446', 'grad_norm': '23.75', 'counters/examples': 99744, 'counters/updates': 3117}
train stats after 99776 examples: {'rewards_train/chosen': '0.066734', 'rewards_train/rejected': '0.07236', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0056257', 'logps_train/rejected': '-150.86', 'logps_train/chosen': '-118.94', 'loss/train': '0.70355', 'examples_per_second': '30.398', 'grad_norm': '43.5', 'counters/examples': 99776, 'counters/updates': 3118}
skipping logging after 99808 examples to avoid logging too frequently
train stats after 99840 examples: {'rewards_train/chosen': '0.18386', 'rewards_train/rejected': '0.080075', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10378', 'logps_train/rejected': '-168.83', 'logps_train/chosen': '-182.94', 'loss/train': '0.65816', 'examples_per_second': '29.873', 'grad_norm': '33.5', 'counters/examples': 99840, 'counters/updates': 3120}
train stats after 99872 examples: {'rewards_train/chosen': '0.057045', 'rewards_train/rejected': '0.029507', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027538', 'logps_train/rejected': '-146.66', 'logps_train/chosen': '-138.88', 'loss/train': '0.68642', 'examples_per_second': '30.318', 'grad_norm': '30.75', 'counters/examples': 99872, 'counters/updates': 3121}
train stats after 99904 examples: {'rewards_train/chosen': '0.15886', 'rewards_train/rejected': '0.093683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065181', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-180.39', 'loss/train': '0.66952', 'examples_per_second': '31.497', 'grad_norm': '33.5', 'counters/examples': 99904, 'counters/updates': 3122}
train stats after 99936 examples: {'rewards_train/chosen': '0.10149', 'rewards_train/rejected': '0.062759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038736', 'logps_train/rejected': '-171.37', 'logps_train/chosen': '-192.23', 'loss/train': '0.6846', 'examples_per_second': '31.883', 'grad_norm': '42.25', 'counters/examples': 99936, 'counters/updates': 3123}
skipping logging after 99968 examples to avoid logging too frequently
train stats after 100000 examples: {'rewards_train/chosen': '0.13582', 'rewards_train/rejected': '0.013134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12269', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-153.13', 'loss/train': '0.64204', 'examples_per_second': '31.465', 'grad_norm': '31.625', 'counters/examples': 100000, 'counters/updates': 3125}
train stats after 100032 examples: {'rewards_train/chosen': '0.11413', 'rewards_train/rejected': '0.049696', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.064432', 'logps_train/rejected': '-112.2', 'logps_train/chosen': '-129.37', 'loss/train': '0.67411', 'examples_per_second': '32.689', 'grad_norm': '34.5', 'counters/examples': 100032, 'counters/updates': 3126}
skipping logging after 100064 examples to avoid logging too frequently
train stats after 100096 examples: {'rewards_train/chosen': '0.082777', 'rewards_train/rejected': '0.0041508', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078627', 'logps_train/rejected': '-116.02', 'logps_train/chosen': '-133.41', 'loss/train': '0.65974', 'examples_per_second': '33.627', 'grad_norm': '29.125', 'counters/examples': 100096, 'counters/updates': 3128}
skipping logging after 100128 examples to avoid logging too frequently
train stats after 100160 examples: {'rewards_train/chosen': '0.13104', 'rewards_train/rejected': '0.012058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11898', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-141.24', 'loss/train': '0.64421', 'examples_per_second': '31.483', 'grad_norm': '28.125', 'counters/examples': 100160, 'counters/updates': 3130}
train stats after 100192 examples: {'rewards_train/chosen': '0.09008', 'rewards_train/rejected': '0.11215', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.022075', 'logps_train/rejected': '-125.55', 'logps_train/chosen': '-115.44', 'loss/train': '0.71854', 'examples_per_second': '31.706', 'grad_norm': '39.5', 'counters/examples': 100192, 'counters/updates': 3131}
skipping logging after 100224 examples to avoid logging too frequently
train stats after 100256 examples: {'rewards_train/chosen': '0.20949', 'rewards_train/rejected': '0.055365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15413', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-135.06', 'loss/train': '0.63272', 'examples_per_second': '31.5', 'grad_norm': '29.625', 'counters/examples': 100256, 'counters/updates': 3133}
skipping logging after 100288 examples to avoid logging too frequently
train stats after 100320 examples: {'rewards_train/chosen': '0.14878', 'rewards_train/rejected': '0.10697', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041806', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-122.97', 'loss/train': '0.68251', 'examples_per_second': '31.524', 'grad_norm': '33.25', 'counters/examples': 100320, 'counters/updates': 3135}
train stats after 100352 examples: {'rewards_train/chosen': '0.169', 'rewards_train/rejected': '0.031419', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13758', 'logps_train/rejected': '-123.36', 'logps_train/chosen': '-134.25', 'loss/train': '0.64097', 'examples_per_second': '31.119', 'grad_norm': '31.75', 'counters/examples': 100352, 'counters/updates': 3136}
train stats after 100384 examples: {'rewards_train/chosen': '0.14123', 'rewards_train/rejected': '0.066857', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074371', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-149.82', 'loss/train': '0.66458', 'examples_per_second': '30.498', 'grad_norm': '27.875', 'counters/examples': 100384, 'counters/updates': 3137}
train stats after 100416 examples: {'rewards_train/chosen': '0.14173', 'rewards_train/rejected': '0.045289', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096439', 'logps_train/rejected': '-108.66', 'logps_train/chosen': '-117.4', 'loss/train': '0.65611', 'examples_per_second': '32.776', 'grad_norm': '31.25', 'counters/examples': 100416, 'counters/updates': 3138}
train stats after 100448 examples: {'rewards_train/chosen': '0.090424', 'rewards_train/rejected': '0.090268', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.00015552', 'logps_train/rejected': '-156.23', 'logps_train/chosen': '-146.28', 'loss/train': '0.70017', 'examples_per_second': '31.159', 'grad_norm': '37.5', 'counters/examples': 100448, 'counters/updates': 3139}
train stats after 100480 examples: {'rewards_train/chosen': '0.14259', 'rewards_train/rejected': '0.10209', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.040506', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-115.13', 'loss/train': '0.67992', 'examples_per_second': '30.062', 'grad_norm': '28.75', 'counters/examples': 100480, 'counters/updates': 3140}
train stats after 100512 examples: {'rewards_train/chosen': '0.14837', 'rewards_train/rejected': '0.027209', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12116', 'logps_train/rejected': '-117.93', 'logps_train/chosen': '-133.85', 'loss/train': '0.64663', 'examples_per_second': '31.9', 'grad_norm': '29.25', 'counters/examples': 100512, 'counters/updates': 3141}
train stats after 100544 examples: {'rewards_train/chosen': '0.032992', 'rewards_train/rejected': '-0.012514', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045506', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-147.88', 'loss/train': '0.67946', 'examples_per_second': '30.801', 'grad_norm': '30.25', 'counters/examples': 100544, 'counters/updates': 3142}
train stats after 100576 examples: {'rewards_train/chosen': '0.13112', 'rewards_train/rejected': '0.045213', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085911', 'logps_train/rejected': '-143.43', 'logps_train/chosen': '-162.83', 'loss/train': '0.65718', 'examples_per_second': '32.941', 'grad_norm': '29.875', 'counters/examples': 100576, 'counters/updates': 3143}
train stats after 100608 examples: {'rewards_train/chosen': '0.060075', 'rewards_train/rejected': '0.03099', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029085', 'logps_train/rejected': '-144.31', 'logps_train/chosen': '-173.68', 'loss/train': '0.68881', 'examples_per_second': '30.514', 'grad_norm': '36', 'counters/examples': 100608, 'counters/updates': 3144}
train stats after 100640 examples: {'rewards_train/chosen': '0.13176', 'rewards_train/rejected': '0.067914', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.063849', 'logps_train/rejected': '-107.23', 'logps_train/chosen': '-174.33', 'loss/train': '0.67735', 'examples_per_second': '31.466', 'grad_norm': '32', 'counters/examples': 100640, 'counters/updates': 3145}
skipping logging after 100672 examples to avoid logging too frequently
train stats after 100704 examples: {'rewards_train/chosen': '0.1344', 'rewards_train/rejected': '0.034453', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.099947', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-157.83', 'loss/train': '0.65123', 'examples_per_second': '30.114', 'grad_norm': '27.875', 'counters/examples': 100704, 'counters/updates': 3147}
train stats after 100736 examples: {'rewards_train/chosen': '0.17985', 'rewards_train/rejected': '0.13932', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04053', 'logps_train/rejected': '-151.93', 'logps_train/chosen': '-205.11', 'loss/train': '0.69206', 'examples_per_second': '30.396', 'grad_norm': '47.75', 'counters/examples': 100736, 'counters/updates': 3148}
train stats after 100768 examples: {'rewards_train/chosen': '0.057522', 'rewards_train/rejected': '-0.011781', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069303', 'logps_train/rejected': '-137.94', 'logps_train/chosen': '-121.62', 'loss/train': '0.66416', 'examples_per_second': '32.131', 'grad_norm': '27.375', 'counters/examples': 100768, 'counters/updates': 3149}
train stats after 100800 examples: {'rewards_train/chosen': '0.15072', 'rewards_train/rejected': '-0.013453', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16417', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-144.95', 'loss/train': '0.63398', 'examples_per_second': '31.412', 'grad_norm': '30.125', 'counters/examples': 100800, 'counters/updates': 3150}
train stats after 100832 examples: {'rewards_train/chosen': '0.16592', 'rewards_train/rejected': '0.059006', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10691', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-196.98', 'loss/train': '0.64958', 'examples_per_second': '30.344', 'grad_norm': '40.25', 'counters/examples': 100832, 'counters/updates': 3151}
train stats after 100864 examples: {'rewards_train/chosen': '0.10378', 'rewards_train/rejected': '0.0037455', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10003', 'logps_train/rejected': '-92.794', 'logps_train/chosen': '-132.29', 'loss/train': '0.65395', 'examples_per_second': '31.078', 'grad_norm': '32.25', 'counters/examples': 100864, 'counters/updates': 3152}
train stats after 100896 examples: {'rewards_train/chosen': '0.17812', 'rewards_train/rejected': '0.087016', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091109', 'logps_train/rejected': '-143.36', 'logps_train/chosen': '-143.79', 'loss/train': '0.65656', 'examples_per_second': '30.135', 'grad_norm': '28.25', 'counters/examples': 100896, 'counters/updates': 3153}
train stats after 100928 examples: {'rewards_train/chosen': '0.12215', 'rewards_train/rejected': '0.04673', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075422', 'logps_train/rejected': '-157.15', 'logps_train/chosen': '-114.59', 'loss/train': '0.66271', 'examples_per_second': '30.876', 'grad_norm': '28.375', 'counters/examples': 100928, 'counters/updates': 3154}
train stats after 100960 examples: {'rewards_train/chosen': '0.11001', 'rewards_train/rejected': '0.025846', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084161', 'logps_train/rejected': '-127.47', 'logps_train/chosen': '-159.49', 'loss/train': '0.6634', 'examples_per_second': '30.392', 'grad_norm': '28.25', 'counters/examples': 100960, 'counters/updates': 3155}
train stats after 100992 examples: {'rewards_train/chosen': '0.098039', 'rewards_train/rejected': '0.109', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010965', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-115.26', 'loss/train': '0.7025', 'examples_per_second': '31.421', 'grad_norm': '31.125', 'counters/examples': 100992, 'counters/updates': 3156}
train stats after 101024 examples: {'rewards_train/chosen': '0.17066', 'rewards_train/rejected': '0.11673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053927', 'logps_train/rejected': '-146.2', 'logps_train/chosen': '-139.09', 'loss/train': '0.67515', 'examples_per_second': '30.134', 'grad_norm': '36.5', 'counters/examples': 101024, 'counters/updates': 3157}
skipping logging after 101056 examples to avoid logging too frequently
train stats after 101088 examples: {'rewards_train/chosen': '0.026709', 'rewards_train/rejected': '0.074429', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.04772', 'logps_train/rejected': '-116.18', 'logps_train/chosen': '-135.51', 'loss/train': '0.72328', 'examples_per_second': '29.968', 'grad_norm': '28.625', 'counters/examples': 101088, 'counters/updates': 3159}
train stats after 101120 examples: {'rewards_train/chosen': '0.10063', 'rewards_train/rejected': '-0.010169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1108', 'logps_train/rejected': '-151.71', 'logps_train/chosen': '-174.22', 'loss/train': '0.64756', 'examples_per_second': '31.775', 'grad_norm': '41.75', 'counters/examples': 101120, 'counters/updates': 3160}
train stats after 101152 examples: {'rewards_train/chosen': '0.1523', 'rewards_train/rejected': '0.046242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10606', 'logps_train/rejected': '-110.11', 'logps_train/chosen': '-138.15', 'loss/train': '0.6538', 'examples_per_second': '31.093', 'grad_norm': '25.25', 'counters/examples': 101152, 'counters/updates': 3161}
train stats after 101184 examples: {'rewards_train/chosen': '0.096393', 'rewards_train/rejected': '0.088172', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0082213', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-151.91', 'loss/train': '0.69712', 'examples_per_second': '31.642', 'grad_norm': '32.75', 'counters/examples': 101184, 'counters/updates': 3162}
train stats after 101216 examples: {'rewards_train/chosen': '0.1452', 'rewards_train/rejected': '0.053442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091763', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-164.98', 'loss/train': '0.66398', 'examples_per_second': '31.483', 'grad_norm': '43', 'counters/examples': 101216, 'counters/updates': 3163}
train stats after 101248 examples: {'rewards_train/chosen': '0.14788', 'rewards_train/rejected': '0.10248', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045407', 'logps_train/rejected': '-113.63', 'logps_train/chosen': '-126.51', 'loss/train': '0.67701', 'examples_per_second': '33.18', 'grad_norm': '27.875', 'counters/examples': 101248, 'counters/updates': 3164}
train stats after 101280 examples: {'rewards_train/chosen': '0.12636', 'rewards_train/rejected': '0.089257', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037101', 'logps_train/rejected': '-122.22', 'logps_train/chosen': '-134.16', 'loss/train': '0.6852', 'examples_per_second': '31.04', 'grad_norm': '33.75', 'counters/examples': 101280, 'counters/updates': 3165}
skipping logging after 101312 examples to avoid logging too frequently
train stats after 101344 examples: {'rewards_train/chosen': '0.093546', 'rewards_train/rejected': '0.082969', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010577', 'logps_train/rejected': '-83.596', 'logps_train/chosen': '-112.22', 'loss/train': '0.69236', 'examples_per_second': '30.161', 'grad_norm': '24.125', 'counters/examples': 101344, 'counters/updates': 3167}
skipping logging after 101376 examples to avoid logging too frequently
train stats after 101408 examples: {'rewards_train/chosen': '0.10863', 'rewards_train/rejected': '0.10099', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0076427', 'logps_train/rejected': '-181.72', 'logps_train/chosen': '-160.48', 'loss/train': '0.70102', 'examples_per_second': '30.461', 'grad_norm': '42', 'counters/examples': 101408, 'counters/updates': 3169}
train stats after 101440 examples: {'rewards_train/chosen': '0.16333', 'rewards_train/rejected': '0.0015133', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16182', 'logps_train/rejected': '-142.5', 'logps_train/chosen': '-117.41', 'loss/train': '0.62976', 'examples_per_second': '30.532', 'grad_norm': '30.375', 'counters/examples': 101440, 'counters/updates': 3170}
train stats after 101472 examples: {'rewards_train/chosen': '0.093087', 'rewards_train/rejected': '0.013065', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080022', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-150.62', 'loss/train': '0.66295', 'examples_per_second': '31.492', 'grad_norm': '40', 'counters/examples': 101472, 'counters/updates': 3171}
train stats after 101504 examples: {'rewards_train/chosen': '0.072286', 'rewards_train/rejected': '0.048941', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023345', 'logps_train/rejected': '-146.04', 'logps_train/chosen': '-164.51', 'loss/train': '0.68895', 'examples_per_second': '31.462', 'grad_norm': '34.5', 'counters/examples': 101504, 'counters/updates': 3172}
train stats after 101536 examples: {'rewards_train/chosen': '0.10948', 'rewards_train/rejected': '0.066747', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042734', 'logps_train/rejected': '-109.39', 'logps_train/chosen': '-122.74', 'loss/train': '0.67826', 'examples_per_second': '30.274', 'grad_norm': '38.5', 'counters/examples': 101536, 'counters/updates': 3173}
train stats after 101568 examples: {'rewards_train/chosen': '0.10632', 'rewards_train/rejected': '0.038715', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067604', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-152.62', 'loss/train': '0.66657', 'examples_per_second': '30.075', 'grad_norm': '28.625', 'counters/examples': 101568, 'counters/updates': 3174}
skipping logging after 101600 examples to avoid logging too frequently
train stats after 101632 examples: {'rewards_train/chosen': '0.13908', 'rewards_train/rejected': '0.20215', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.063072', 'logps_train/rejected': '-138.91', 'logps_train/chosen': '-150.54', 'loss/train': '0.7373', 'examples_per_second': '30.156', 'grad_norm': '40.5', 'counters/examples': 101632, 'counters/updates': 3176}
train stats after 101664 examples: {'rewards_train/chosen': '0.12733', 'rewards_train/rejected': '0.043179', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084153', 'logps_train/rejected': '-97.933', 'logps_train/chosen': '-125.59', 'loss/train': '0.65756', 'examples_per_second': '31.516', 'grad_norm': '25.25', 'counters/examples': 101664, 'counters/updates': 3177}
train stats after 101696 examples: {'rewards_train/chosen': '0.12814', 'rewards_train/rejected': '0.097705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.030434', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-108.35', 'loss/train': '0.68781', 'examples_per_second': '30.644', 'grad_norm': '40.75', 'counters/examples': 101696, 'counters/updates': 3178}
train stats after 101728 examples: {'rewards_train/chosen': '0.20451', 'rewards_train/rejected': '0.11423', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090274', 'logps_train/rejected': '-132.79', 'logps_train/chosen': '-140.59', 'loss/train': '0.66174', 'examples_per_second': '31.003', 'grad_norm': '30.125', 'counters/examples': 101728, 'counters/updates': 3179}
train stats after 101760 examples: {'rewards_train/chosen': '0.084692', 'rewards_train/rejected': '0.012163', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072529', 'logps_train/rejected': '-113.22', 'logps_train/chosen': '-111.89', 'loss/train': '0.66222', 'examples_per_second': '31.33', 'grad_norm': '24.875', 'counters/examples': 101760, 'counters/updates': 3180}
train stats after 101792 examples: {'rewards_train/chosen': '0.092123', 'rewards_train/rejected': '0.071633', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020491', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-129.72', 'loss/train': '0.69189', 'examples_per_second': '31.537', 'grad_norm': '29.75', 'counters/examples': 101792, 'counters/updates': 3181}
train stats after 101824 examples: {'rewards_train/chosen': '0.20211', 'rewards_train/rejected': '0.053995', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14812', 'logps_train/rejected': '-105.4', 'logps_train/chosen': '-154.95', 'loss/train': '0.63883', 'examples_per_second': '32.954', 'grad_norm': '25.5', 'counters/examples': 101824, 'counters/updates': 3182}
skipping logging after 101856 examples to avoid logging too frequently
train stats after 101888 examples: {'rewards_train/chosen': '0.171', 'rewards_train/rejected': '0.079805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091196', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-158.31', 'loss/train': '0.66145', 'examples_per_second': '32.49', 'grad_norm': '37.25', 'counters/examples': 101888, 'counters/updates': 3184}
train stats after 101920 examples: {'rewards_train/chosen': '0.083314', 'rewards_train/rejected': '0.075602', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0077126', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-175.05', 'loss/train': '0.70412', 'examples_per_second': '31.491', 'grad_norm': '31.125', 'counters/examples': 101920, 'counters/updates': 3185}
train stats after 101952 examples: {'rewards_train/chosen': '0.17394', 'rewards_train/rejected': '0.0026395', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1713', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-148.63', 'loss/train': '0.62564', 'examples_per_second': '29.939', 'grad_norm': '34.25', 'counters/examples': 101952, 'counters/updates': 3186}
train stats after 101984 examples: {'rewards_train/chosen': '0.2379', 'rewards_train/rejected': '0.083151', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15475', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-171.41', 'loss/train': '0.64318', 'examples_per_second': '31.493', 'grad_norm': '30.125', 'counters/examples': 101984, 'counters/updates': 3187}
train stats after 102016 examples: {'rewards_train/chosen': '0.081691', 'rewards_train/rejected': '0.12533', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043635', 'logps_train/rejected': '-155.28', 'logps_train/chosen': '-131.79', 'loss/train': '0.73369', 'examples_per_second': '31.483', 'grad_norm': '58.25', 'counters/examples': 102016, 'counters/updates': 3188}
skipping logging after 102048 examples to avoid logging too frequently
train stats after 102080 examples: {'rewards_train/chosen': '0.11079', 'rewards_train/rejected': '0.053282', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057509', 'logps_train/rejected': '-146.3', 'logps_train/chosen': '-160.54', 'loss/train': '0.67657', 'examples_per_second': '31.353', 'grad_norm': '45.5', 'counters/examples': 102080, 'counters/updates': 3190}
train stats after 102112 examples: {'rewards_train/chosen': '0.13448', 'rewards_train/rejected': '0.094049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040431', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-161.12', 'loss/train': '0.68622', 'examples_per_second': '30.198', 'grad_norm': '32.75', 'counters/examples': 102112, 'counters/updates': 3191}
train stats after 102144 examples: {'rewards_train/chosen': '0.045925', 'rewards_train/rejected': '0.038517', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0074081', 'logps_train/rejected': '-97.62', 'logps_train/chosen': '-110.21', 'loss/train': '0.69292', 'examples_per_second': '32.527', 'grad_norm': '29.5', 'counters/examples': 102144, 'counters/updates': 3192}
train stats after 102176 examples: {'rewards_train/chosen': '0.12174', 'rewards_train/rejected': '0.003547', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11819', 'logps_train/rejected': '-113.15', 'logps_train/chosen': '-146.27', 'loss/train': '0.64284', 'examples_per_second': '31.288', 'grad_norm': '28.875', 'counters/examples': 102176, 'counters/updates': 3193}
train stats after 102208 examples: {'rewards_train/chosen': '0.12805', 'rewards_train/rejected': '0.05538', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.072673', 'logps_train/rejected': '-116.13', 'logps_train/chosen': '-157.95', 'loss/train': '0.66494', 'examples_per_second': '31.393', 'grad_norm': '31.375', 'counters/examples': 102208, 'counters/updates': 3194}
train stats after 102240 examples: {'rewards_train/chosen': '0.047101', 'rewards_train/rejected': '0.032268', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014833', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-129.4', 'loss/train': '0.68887', 'examples_per_second': '32.339', 'grad_norm': '25.375', 'counters/examples': 102240, 'counters/updates': 3195}
train stats after 102272 examples: {'rewards_train/chosen': '0.12444', 'rewards_train/rejected': '0.036255', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088182', 'logps_train/rejected': '-85.381', 'logps_train/chosen': '-129.45', 'loss/train': '0.65507', 'examples_per_second': '30.679', 'grad_norm': '30.875', 'counters/examples': 102272, 'counters/updates': 3196}
train stats after 102304 examples: {'rewards_train/chosen': '0.13541', 'rewards_train/rejected': '0.096194', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.039216', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-186.43', 'loss/train': '0.6959', 'examples_per_second': '32.849', 'grad_norm': '44.5', 'counters/examples': 102304, 'counters/updates': 3197}
skipping logging after 102336 examples to avoid logging too frequently
train stats after 102368 examples: {'rewards_train/chosen': '0.067698', 'rewards_train/rejected': '0.015429', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.052269', 'logps_train/rejected': '-103.14', 'logps_train/chosen': '-123.15', 'loss/train': '0.68873', 'examples_per_second': '31.768', 'grad_norm': '32.5', 'counters/examples': 102368, 'counters/updates': 3199}
train stats after 102400 examples: {'rewards_train/chosen': '0.04442', 'rewards_train/rejected': '0.1709', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.12648', 'logps_train/rejected': '-143.23', 'logps_train/chosen': '-108.97', 'loss/train': '0.7758', 'examples_per_second': '31.163', 'grad_norm': '50.25', 'counters/examples': 102400, 'counters/updates': 3200}
train stats after 102432 examples: {'rewards_train/chosen': '0.11549', 'rewards_train/rejected': '0.066585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048909', 'logps_train/rejected': '-144.48', 'logps_train/chosen': '-141.17', 'loss/train': '0.68076', 'examples_per_second': '30.126', 'grad_norm': '37.75', 'counters/examples': 102432, 'counters/updates': 3201}
train stats after 102464 examples: {'rewards_train/chosen': '0.10305', 'rewards_train/rejected': '0.076198', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026849', 'logps_train/rejected': '-140.11', 'logps_train/chosen': '-149.31', 'loss/train': '0.68484', 'examples_per_second': '29.958', 'grad_norm': '26.875', 'counters/examples': 102464, 'counters/updates': 3202}
skipping logging after 102496 examples to avoid logging too frequently
train stats after 102528 examples: {'rewards_train/chosen': '0.096684', 'rewards_train/rejected': '-0.0026206', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099304', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-113.48', 'loss/train': '0.6489', 'examples_per_second': '32.48', 'grad_norm': '26.625', 'counters/examples': 102528, 'counters/updates': 3204}
train stats after 102560 examples: {'rewards_train/chosen': '0.15982', 'rewards_train/rejected': '0.10362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056197', 'logps_train/rejected': '-149.56', 'logps_train/chosen': '-142.57', 'loss/train': '0.69626', 'examples_per_second': '23.733', 'grad_norm': '63.25', 'counters/examples': 102560, 'counters/updates': 3205}
train stats after 102592 examples: {'rewards_train/chosen': '0.19526', 'rewards_train/rejected': '0.057211', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13805', 'logps_train/rejected': '-133.42', 'logps_train/chosen': '-158.1', 'loss/train': '0.63168', 'examples_per_second': '30.03', 'grad_norm': '36.5', 'counters/examples': 102592, 'counters/updates': 3206}
train stats after 102624 examples: {'rewards_train/chosen': '0.23221', 'rewards_train/rejected': '0.097482', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13473', 'logps_train/rejected': '-138.65', 'logps_train/chosen': '-166.75', 'loss/train': '0.6487', 'examples_per_second': '30.267', 'grad_norm': '33', 'counters/examples': 102624, 'counters/updates': 3207}
train stats after 102656 examples: {'rewards_train/chosen': '0.072944', 'rewards_train/rejected': '0.015548', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057395', 'logps_train/rejected': '-149.47', 'logps_train/chosen': '-161.04', 'loss/train': '0.67228', 'examples_per_second': '23.955', 'grad_norm': '36', 'counters/examples': 102656, 'counters/updates': 3208}
train stats after 102688 examples: {'rewards_train/chosen': '0.14828', 'rewards_train/rejected': '0.074111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074165', 'logps_train/rejected': '-137.95', 'logps_train/chosen': '-142.71', 'loss/train': '0.6687', 'examples_per_second': '32.461', 'grad_norm': '28.5', 'counters/examples': 102688, 'counters/updates': 3209}
train stats after 102720 examples: {'rewards_train/chosen': '0.080483', 'rewards_train/rejected': '0.0069316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073552', 'logps_train/rejected': '-122.33', 'logps_train/chosen': '-147.99', 'loss/train': '0.66167', 'examples_per_second': '30.997', 'grad_norm': '27', 'counters/examples': 102720, 'counters/updates': 3210}
train stats after 102752 examples: {'rewards_train/chosen': '0.1011', 'rewards_train/rejected': '-0.0023348', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10343', 'logps_train/rejected': '-119.35', 'logps_train/chosen': '-136.89', 'loss/train': '0.65523', 'examples_per_second': '31.475', 'grad_norm': '28', 'counters/examples': 102752, 'counters/updates': 3211}
train stats after 102784 examples: {'rewards_train/chosen': '0.12522', 'rewards_train/rejected': '0.10025', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024966', 'logps_train/rejected': '-127.12', 'logps_train/chosen': '-147.8', 'loss/train': '0.69523', 'examples_per_second': '30.843', 'grad_norm': '40.5', 'counters/examples': 102784, 'counters/updates': 3212}
train stats after 102816 examples: {'rewards_train/chosen': '0.1767', 'rewards_train/rejected': '0.15873', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017972', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-159.97', 'loss/train': '0.69963', 'examples_per_second': '31.493', 'grad_norm': '33.5', 'counters/examples': 102816, 'counters/updates': 3213}
train stats after 102848 examples: {'rewards_train/chosen': '0.12696', 'rewards_train/rejected': '0.069659', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.0573', 'logps_train/rejected': '-115.56', 'logps_train/chosen': '-140.96', 'loss/train': '0.66767', 'examples_per_second': '31.202', 'grad_norm': '42.25', 'counters/examples': 102848, 'counters/updates': 3214}
skipping logging after 102880 examples to avoid logging too frequently
train stats after 102912 examples: {'rewards_train/chosen': '0.18371', 'rewards_train/rejected': '0.019102', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16461', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-161.63', 'loss/train': '0.6338', 'examples_per_second': '31.332', 'grad_norm': '31', 'counters/examples': 102912, 'counters/updates': 3216}
train stats after 102944 examples: {'rewards_train/chosen': '0.11401', 'rewards_train/rejected': '0.035748', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078266', 'logps_train/rejected': '-91.098', 'logps_train/chosen': '-124.13', 'loss/train': '0.66093', 'examples_per_second': '30.763', 'grad_norm': '28.875', 'counters/examples': 102944, 'counters/updates': 3217}
train stats after 102976 examples: {'rewards_train/chosen': '0.12814', 'rewards_train/rejected': '0.11098', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017161', 'logps_train/rejected': '-172.86', 'logps_train/chosen': '-146.14', 'loss/train': '0.69494', 'examples_per_second': '30.773', 'grad_norm': '36.25', 'counters/examples': 102976, 'counters/updates': 3218}
train stats after 103008 examples: {'rewards_train/chosen': '0.066026', 'rewards_train/rejected': '0.012078', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053948', 'logps_train/rejected': '-118.17', 'logps_train/chosen': '-120.74', 'loss/train': '0.67321', 'examples_per_second': '30.096', 'grad_norm': '24.25', 'counters/examples': 103008, 'counters/updates': 3219}
train stats after 103040 examples: {'rewards_train/chosen': '0.14989', 'rewards_train/rejected': '0.12554', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024348', 'logps_train/rejected': '-197.83', 'logps_train/chosen': '-187.95', 'loss/train': '0.69473', 'examples_per_second': '31.467', 'grad_norm': '54.75', 'counters/examples': 103040, 'counters/updates': 3220}
skipping logging after 103072 examples to avoid logging too frequently
train stats after 103104 examples: {'rewards_train/chosen': '0.099385', 'rewards_train/rejected': '0.10665', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0072673', 'logps_train/rejected': '-132.67', 'logps_train/chosen': '-138.54', 'loss/train': '0.70861', 'examples_per_second': '29.986', 'grad_norm': '44.25', 'counters/examples': 103104, 'counters/updates': 3222}
skipping logging after 103136 examples to avoid logging too frequently
train stats after 103168 examples: {'rewards_train/chosen': '0.092511', 'rewards_train/rejected': '0.11873', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02622', 'logps_train/rejected': '-149.8', 'logps_train/chosen': '-170.73', 'loss/train': '0.71502', 'examples_per_second': '29.885', 'grad_norm': '35.5', 'counters/examples': 103168, 'counters/updates': 3224}
train stats after 103200 examples: {'rewards_train/chosen': '0.15943', 'rewards_train/rejected': '0.073531', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085895', 'logps_train/rejected': '-103.72', 'logps_train/chosen': '-141.87', 'loss/train': '0.66238', 'examples_per_second': '32.906', 'grad_norm': '26', 'counters/examples': 103200, 'counters/updates': 3225}
train stats after 103232 examples: {'rewards_train/chosen': '0.052459', 'rewards_train/rejected': '0.0011313', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.051327', 'logps_train/rejected': '-108.31', 'logps_train/chosen': '-121.47', 'loss/train': '0.67273', 'examples_per_second': '32.531', 'grad_norm': '31.875', 'counters/examples': 103232, 'counters/updates': 3226}
train stats after 103264 examples: {'rewards_train/chosen': '0.11778', 'rewards_train/rejected': '0.098312', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.019469', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-155.66', 'loss/train': '0.69005', 'examples_per_second': '30.396', 'grad_norm': '45.75', 'counters/examples': 103264, 'counters/updates': 3227}
train stats after 103296 examples: {'rewards_train/chosen': '0.17116', 'rewards_train/rejected': '0.06974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10142', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-180.12', 'loss/train': '0.65258', 'examples_per_second': '30.114', 'grad_norm': '33.5', 'counters/examples': 103296, 'counters/updates': 3228}
train stats after 103328 examples: {'rewards_train/chosen': '0.12966', 'rewards_train/rejected': '0.043224', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086436', 'logps_train/rejected': '-148.75', 'logps_train/chosen': '-138.47', 'loss/train': '0.66257', 'examples_per_second': '31.438', 'grad_norm': '29', 'counters/examples': 103328, 'counters/updates': 3229}
train stats after 103360 examples: {'rewards_train/chosen': '0.10515', 'rewards_train/rejected': '0.13199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.026846', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-122.79', 'loss/train': '0.71309', 'examples_per_second': '30.741', 'grad_norm': '31.375', 'counters/examples': 103360, 'counters/updates': 3230}
train stats after 103392 examples: {'rewards_train/chosen': '0.11544', 'rewards_train/rejected': '0.0037053', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11174', 'logps_train/rejected': '-99.562', 'logps_train/chosen': '-156.35', 'loss/train': '0.65236', 'examples_per_second': '32.489', 'grad_norm': '28.25', 'counters/examples': 103392, 'counters/updates': 3231}
skipping logging after 103424 examples to avoid logging too frequently
train stats after 103456 examples: {'rewards_train/chosen': '0.17417', 'rewards_train/rejected': '0.086147', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088025', 'logps_train/rejected': '-138.17', 'logps_train/chosen': '-164.96', 'loss/train': '0.6602', 'examples_per_second': '33.695', 'grad_norm': '50.5', 'counters/examples': 103456, 'counters/updates': 3233}
train stats after 103488 examples: {'rewards_train/chosen': '0.14508', 'rewards_train/rejected': '0.045322', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09976', 'logps_train/rejected': '-181.58', 'logps_train/chosen': '-135.6', 'loss/train': '0.66568', 'examples_per_second': '31.075', 'grad_norm': '55.75', 'counters/examples': 103488, 'counters/updates': 3234}
skipping logging after 103520 examples to avoid logging too frequently
train stats after 103552 examples: {'rewards_train/chosen': '0.089671', 'rewards_train/rejected': '0.022511', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06716', 'logps_train/rejected': '-148.52', 'logps_train/chosen': '-156.95', 'loss/train': '0.66807', 'examples_per_second': '32.901', 'grad_norm': '34.25', 'counters/examples': 103552, 'counters/updates': 3236}
train stats after 103584 examples: {'rewards_train/chosen': '0.16797', 'rewards_train/rejected': '0.045175', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1228', 'logps_train/rejected': '-108.27', 'logps_train/chosen': '-160.59', 'loss/train': '0.64822', 'examples_per_second': '33.173', 'grad_norm': '31.25', 'counters/examples': 103584, 'counters/updates': 3237}
skipping logging after 103616 examples to avoid logging too frequently
train stats after 103648 examples: {'rewards_train/chosen': '0.18039', 'rewards_train/rejected': '0.045954', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13444', 'logps_train/rejected': '-91.451', 'logps_train/chosen': '-146.36', 'loss/train': '0.63427', 'examples_per_second': '31.681', 'grad_norm': '24.625', 'counters/examples': 103648, 'counters/updates': 3239}
train stats after 103680 examples: {'rewards_train/chosen': '0.1209', 'rewards_train/rejected': '0.067123', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053773', 'logps_train/rejected': '-147.27', 'logps_train/chosen': '-188.31', 'loss/train': '0.67976', 'examples_per_second': '30.037', 'grad_norm': '30.875', 'counters/examples': 103680, 'counters/updates': 3240}
train stats after 103712 examples: {'rewards_train/chosen': '0.11846', 'rewards_train/rejected': '0.03946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079', 'logps_train/rejected': '-113.27', 'logps_train/chosen': '-108.16', 'loss/train': '0.66012', 'examples_per_second': '30.726', 'grad_norm': '25.875', 'counters/examples': 103712, 'counters/updates': 3241}
skipping logging after 103744 examples to avoid logging too frequently
train stats after 103776 examples: {'rewards_train/chosen': '0.065836', 'rewards_train/rejected': '0.012396', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053439', 'logps_train/rejected': '-102.19', 'logps_train/chosen': '-131.56', 'loss/train': '0.6721', 'examples_per_second': '31.484', 'grad_norm': '45.5', 'counters/examples': 103776, 'counters/updates': 3243}
train stats after 103808 examples: {'rewards_train/chosen': '0.19928', 'rewards_train/rejected': '0.059861', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13941', 'logps_train/rejected': '-144.55', 'logps_train/chosen': '-153.47', 'loss/train': '0.63858', 'examples_per_second': '32.753', 'grad_norm': '25.5', 'counters/examples': 103808, 'counters/updates': 3244}
train stats after 103840 examples: {'rewards_train/chosen': '0.11927', 'rewards_train/rejected': '0.096265', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023005', 'logps_train/rejected': '-107.75', 'logps_train/chosen': '-121.06', 'loss/train': '0.68941', 'examples_per_second': '30.75', 'grad_norm': '35', 'counters/examples': 103840, 'counters/updates': 3245}
train stats after 103872 examples: {'rewards_train/chosen': '0.047108', 'rewards_train/rejected': '-0.056953', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10406', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-96.724', 'loss/train': '0.64761', 'examples_per_second': '31.373', 'grad_norm': '31.625', 'counters/examples': 103872, 'counters/updates': 3246}
train stats after 103904 examples: {'rewards_train/chosen': '0.15221', 'rewards_train/rejected': '0.064294', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087915', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-145.87', 'loss/train': '0.65737', 'examples_per_second': '30.841', 'grad_norm': '28.625', 'counters/examples': 103904, 'counters/updates': 3247}
train stats after 103936 examples: {'rewards_train/chosen': '0.052939', 'rewards_train/rejected': '0.11902', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.066081', 'logps_train/rejected': '-205.09', 'logps_train/chosen': '-195.12', 'loss/train': '0.74086', 'examples_per_second': '30.34', 'grad_norm': '67.5', 'counters/examples': 103936, 'counters/updates': 3248}
train stats after 103968 examples: {'rewards_train/chosen': '0.15164', 'rewards_train/rejected': '0.065171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08647', 'logps_train/rejected': '-110.03', 'logps_train/chosen': '-131', 'loss/train': '0.66341', 'examples_per_second': '31.444', 'grad_norm': '30.375', 'counters/examples': 103968, 'counters/updates': 3249}
train stats after 104000 examples: {'rewards_train/chosen': '0.16517', 'rewards_train/rejected': '0.06445', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10072', 'logps_train/rejected': '-126', 'logps_train/chosen': '-140.88', 'loss/train': '0.65705', 'examples_per_second': '30.488', 'grad_norm': '53.5', 'counters/examples': 104000, 'counters/updates': 3250}
train stats after 104032 examples: {'rewards_train/chosen': '0.10277', 'rewards_train/rejected': '0.089941', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.012829', 'logps_train/rejected': '-150.92', 'logps_train/chosen': '-188.53', 'loss/train': '0.69498', 'examples_per_second': '30.924', 'grad_norm': '31', 'counters/examples': 104032, 'counters/updates': 3251}
train stats after 104064 examples: {'rewards_train/chosen': '0.1277', 'rewards_train/rejected': '0.078073', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049623', 'logps_train/rejected': '-143.82', 'logps_train/chosen': '-154.08', 'loss/train': '0.67751', 'examples_per_second': '30.83', 'grad_norm': '40.75', 'counters/examples': 104064, 'counters/updates': 3252}
skipping logging after 104096 examples to avoid logging too frequently
train stats after 104128 examples: {'rewards_train/chosen': '0.1163', 'rewards_train/rejected': '0.062092', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054206', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-173.07', 'loss/train': '0.67491', 'examples_per_second': '29.957', 'grad_norm': '26.125', 'counters/examples': 104128, 'counters/updates': 3254}
train stats after 104160 examples: {'rewards_train/chosen': '0.15181', 'rewards_train/rejected': '0.085709', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066101', 'logps_train/rejected': '-127.95', 'logps_train/chosen': '-139.53', 'loss/train': '0.66818', 'examples_per_second': '32.897', 'grad_norm': '40.25', 'counters/examples': 104160, 'counters/updates': 3255}
train stats after 104192 examples: {'rewards_train/chosen': '0.11487', 'rewards_train/rejected': '0.040662', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074207', 'logps_train/rejected': '-146.13', 'logps_train/chosen': '-150.87', 'loss/train': '0.66139', 'examples_per_second': '32.353', 'grad_norm': '29.25', 'counters/examples': 104192, 'counters/updates': 3256}
train stats after 104224 examples: {'rewards_train/chosen': '0.065208', 'rewards_train/rejected': '0.047658', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01755', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-123.88', 'loss/train': '0.69102', 'examples_per_second': '31.331', 'grad_norm': '37', 'counters/examples': 104224, 'counters/updates': 3257}
train stats after 104256 examples: {'rewards_train/chosen': '0.14184', 'rewards_train/rejected': '0.052143', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089693', 'logps_train/rejected': '-127.07', 'logps_train/chosen': '-140.07', 'loss/train': '0.66164', 'examples_per_second': '29.834', 'grad_norm': '28.625', 'counters/examples': 104256, 'counters/updates': 3258}
skipping logging after 104288 examples to avoid logging too frequently
train stats after 104320 examples: {'rewards_train/chosen': '0.26942', 'rewards_train/rejected': '0.10837', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16106', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-194.93', 'loss/train': '0.62571', 'examples_per_second': '31.457', 'grad_norm': '35.5', 'counters/examples': 104320, 'counters/updates': 3260}
train stats after 104352 examples: {'rewards_train/chosen': '0.082673', 'rewards_train/rejected': '0.017321', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065351', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-107.02', 'loss/train': '0.66572', 'examples_per_second': '30.998', 'grad_norm': '28.75', 'counters/examples': 104352, 'counters/updates': 3261}
train stats after 104384 examples: {'rewards_train/chosen': '0.0021119', 'rewards_train/rejected': '0.031032', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.02892', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-117.06', 'loss/train': '0.72196', 'examples_per_second': '31.27', 'grad_norm': '44', 'counters/examples': 104384, 'counters/updates': 3262}
train stats after 104416 examples: {'rewards_train/chosen': '0.11998', 'rewards_train/rejected': '0.10674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013237', 'logps_train/rejected': '-143.14', 'logps_train/chosen': '-130.54', 'loss/train': '0.70547', 'examples_per_second': '30.43', 'grad_norm': '39', 'counters/examples': 104416, 'counters/updates': 3263}
train stats after 104448 examples: {'rewards_train/chosen': '0.14796', 'rewards_train/rejected': '0.042673', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10529', 'logps_train/rejected': '-163.65', 'logps_train/chosen': '-130.18', 'loss/train': '0.64712', 'examples_per_second': '30.561', 'grad_norm': '31.625', 'counters/examples': 104448, 'counters/updates': 3264}
train stats after 104480 examples: {'rewards_train/chosen': '-0.054393', 'rewards_train/rejected': '-0.012675', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.041718', 'logps_train/rejected': '-89.076', 'logps_train/chosen': '-158.31', 'loss/train': '0.74343', 'examples_per_second': '33.132', 'grad_norm': '41.25', 'counters/examples': 104480, 'counters/updates': 3265}
train stats after 104512 examples: {'rewards_train/chosen': '0.12555', 'rewards_train/rejected': '0.033289', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092261', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-138.22', 'loss/train': '0.65898', 'examples_per_second': '31.129', 'grad_norm': '28', 'counters/examples': 104512, 'counters/updates': 3266}
train stats after 104544 examples: {'rewards_train/chosen': '0.15211', 'rewards_train/rejected': '0.0030735', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14904', 'logps_train/rejected': '-129.63', 'logps_train/chosen': '-141.29', 'loss/train': '0.63196', 'examples_per_second': '31.312', 'grad_norm': '35', 'counters/examples': 104544, 'counters/updates': 3267}
train stats after 104576 examples: {'rewards_train/chosen': '0.13816', 'rewards_train/rejected': '-0.015464', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15363', 'logps_train/rejected': '-130.28', 'logps_train/chosen': '-137.74', 'loss/train': '0.62776', 'examples_per_second': '31.337', 'grad_norm': '34', 'counters/examples': 104576, 'counters/updates': 3268}
train stats after 104608 examples: {'rewards_train/chosen': '0.11929', 'rewards_train/rejected': '0.046959', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072332', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-158.22', 'loss/train': '0.66981', 'examples_per_second': '31.361', 'grad_norm': '39.5', 'counters/examples': 104608, 'counters/updates': 3269}
train stats after 104640 examples: {'rewards_train/chosen': '0.081924', 'rewards_train/rejected': '0.080702', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0012217', 'logps_train/rejected': '-101.43', 'logps_train/chosen': '-123.02', 'loss/train': '0.6985', 'examples_per_second': '23.891', 'grad_norm': '28.875', 'counters/examples': 104640, 'counters/updates': 3270}
train stats after 104672 examples: {'rewards_train/chosen': '0.14441', 'rewards_train/rejected': '0.034417', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10999', 'logps_train/rejected': '-129.65', 'logps_train/chosen': '-160.48', 'loss/train': '0.65237', 'examples_per_second': '29.896', 'grad_norm': '30.5', 'counters/examples': 104672, 'counters/updates': 3271}
train stats after 104704 examples: {'rewards_train/chosen': '0.17879', 'rewards_train/rejected': '0.17952', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00072509', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-153.28', 'loss/train': '0.70267', 'examples_per_second': '31.304', 'grad_norm': '53', 'counters/examples': 104704, 'counters/updates': 3272}
train stats after 104736 examples: {'rewards_train/chosen': '0.13546', 'rewards_train/rejected': '0.11239', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023069', 'logps_train/rejected': '-118.63', 'logps_train/chosen': '-127.07', 'loss/train': '0.69981', 'examples_per_second': '31.411', 'grad_norm': '31.5', 'counters/examples': 104736, 'counters/updates': 3273}
train stats after 104768 examples: {'rewards_train/chosen': '0.10925', 'rewards_train/rejected': '0.080365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028881', 'logps_train/rejected': '-127', 'logps_train/chosen': '-154.49', 'loss/train': '0.68564', 'examples_per_second': '31.906', 'grad_norm': '33.25', 'counters/examples': 104768, 'counters/updates': 3274}
train stats after 104800 examples: {'rewards_train/chosen': '0.16008', 'rewards_train/rejected': '0.049171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11091', 'logps_train/rejected': '-111.88', 'logps_train/chosen': '-125.91', 'loss/train': '0.65013', 'examples_per_second': '31.415', 'grad_norm': '27.625', 'counters/examples': 104800, 'counters/updates': 3275}
train stats after 104832 examples: {'rewards_train/chosen': '0.11367', 'rewards_train/rejected': '0.052089', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061578', 'logps_train/rejected': '-109.24', 'logps_train/chosen': '-154.6', 'loss/train': '0.67336', 'examples_per_second': '31.061', 'grad_norm': '29.5', 'counters/examples': 104832, 'counters/updates': 3276}
skipping logging after 104864 examples to avoid logging too frequently
train stats after 104896 examples: {'rewards_train/chosen': '0.16117', 'rewards_train/rejected': '0.063676', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.097497', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-140.68', 'loss/train': '0.6516', 'examples_per_second': '33.298', 'grad_norm': '30.75', 'counters/examples': 104896, 'counters/updates': 3278}
train stats after 104928 examples: {'rewards_train/chosen': '0.12974', 'rewards_train/rejected': '0.032954', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09679', 'logps_train/rejected': '-110.37', 'logps_train/chosen': '-144.12', 'loss/train': '0.65447', 'examples_per_second': '31.543', 'grad_norm': '25.375', 'counters/examples': 104928, 'counters/updates': 3279}
train stats after 104960 examples: {'rewards_train/chosen': '0.13983', 'rewards_train/rejected': '0.03599', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10384', 'logps_train/rejected': '-113.72', 'logps_train/chosen': '-160.29', 'loss/train': '0.65517', 'examples_per_second': '31.387', 'grad_norm': '31.75', 'counters/examples': 104960, 'counters/updates': 3280}
train stats after 104992 examples: {'rewards_train/chosen': '0.099756', 'rewards_train/rejected': '0.075523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024233', 'logps_train/rejected': '-154.93', 'logps_train/chosen': '-138.29', 'loss/train': '0.68779', 'examples_per_second': '32.836', 'grad_norm': '36', 'counters/examples': 104992, 'counters/updates': 3281}
train stats after 105024 examples: {'rewards_train/chosen': '0.1085', 'rewards_train/rejected': '0.041752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06675', 'logps_train/rejected': '-114.6', 'logps_train/chosen': '-141.93', 'loss/train': '0.66588', 'examples_per_second': '30.159', 'grad_norm': '39.25', 'counters/examples': 105024, 'counters/updates': 3282}
train stats after 105056 examples: {'rewards_train/chosen': '0.10197', 'rewards_train/rejected': '0.038293', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063681', 'logps_train/rejected': '-124.78', 'logps_train/chosen': '-154.29', 'loss/train': '0.67193', 'examples_per_second': '29.896', 'grad_norm': '40', 'counters/examples': 105056, 'counters/updates': 3283}
train stats after 105088 examples: {'rewards_train/chosen': '0.18417', 'rewards_train/rejected': '0.13133', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.052844', 'logps_train/rejected': '-158.25', 'logps_train/chosen': '-146.11', 'loss/train': '0.69044', 'examples_per_second': '31.381', 'grad_norm': '54.75', 'counters/examples': 105088, 'counters/updates': 3284}
train stats after 105120 examples: {'rewards_train/chosen': '0.16048', 'rewards_train/rejected': '0.050909', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10957', 'logps_train/rejected': '-144.52', 'logps_train/chosen': '-136.58', 'loss/train': '0.65344', 'examples_per_second': '31.576', 'grad_norm': '42.25', 'counters/examples': 105120, 'counters/updates': 3285}
train stats after 105152 examples: {'rewards_train/chosen': '0.14422', 'rewards_train/rejected': '0.05513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089089', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-125.55', 'loss/train': '0.65669', 'examples_per_second': '30.474', 'grad_norm': '35.75', 'counters/examples': 105152, 'counters/updates': 3286}
train stats after 105184 examples: {'rewards_train/chosen': '0.15645', 'rewards_train/rejected': '0.023342', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13311', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-154.25', 'loss/train': '0.64773', 'examples_per_second': '31.377', 'grad_norm': '90.5', 'counters/examples': 105184, 'counters/updates': 3287}
train stats after 105216 examples: {'rewards_train/chosen': '0.13113', 'rewards_train/rejected': '0.15156', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.020423', 'logps_train/rejected': '-143.54', 'logps_train/chosen': '-116.01', 'loss/train': '0.71765', 'examples_per_second': '32.488', 'grad_norm': '49.75', 'counters/examples': 105216, 'counters/updates': 3288}
train stats after 105248 examples: {'rewards_train/chosen': '0.065504', 'rewards_train/rejected': '0.07173', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0062267', 'logps_train/rejected': '-121.84', 'logps_train/chosen': '-106.25', 'loss/train': '0.70569', 'examples_per_second': '30.938', 'grad_norm': '58.75', 'counters/examples': 105248, 'counters/updates': 3289}
train stats after 105280 examples: {'rewards_train/chosen': '0.040633', 'rewards_train/rejected': '-0.042673', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083306', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-130.26', 'loss/train': '0.65973', 'examples_per_second': '31.426', 'grad_norm': '28.375', 'counters/examples': 105280, 'counters/updates': 3290}
train stats after 105312 examples: {'rewards_train/chosen': '0.08043', 'rewards_train/rejected': '0.057233', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.023197', 'logps_train/rejected': '-161.59', 'logps_train/chosen': '-146.74', 'loss/train': '0.69585', 'examples_per_second': '31.5', 'grad_norm': '38.5', 'counters/examples': 105312, 'counters/updates': 3291}
train stats after 105344 examples: {'rewards_train/chosen': '0.14366', 'rewards_train/rejected': '0.032313', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11134', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-149.07', 'loss/train': '0.64515', 'examples_per_second': '30.561', 'grad_norm': '32.5', 'counters/examples': 105344, 'counters/updates': 3292}
train stats after 105376 examples: {'rewards_train/chosen': '0.057762', 'rewards_train/rejected': '0.083463', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025701', 'logps_train/rejected': '-160.89', 'logps_train/chosen': '-137.88', 'loss/train': '0.71212', 'examples_per_second': '32.188', 'grad_norm': '42.75', 'counters/examples': 105376, 'counters/updates': 3293}
skipping logging after 105408 examples to avoid logging too frequently
train stats after 105440 examples: {'rewards_train/chosen': '0.12852', 'rewards_train/rejected': '0.064615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063901', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-129.27', 'loss/train': '0.67408', 'examples_per_second': '33.027', 'grad_norm': '31.5', 'counters/examples': 105440, 'counters/updates': 3295}
skipping logging after 105472 examples to avoid logging too frequently
train stats after 105504 examples: {'rewards_train/chosen': '0.08611', 'rewards_train/rejected': '-0.0043186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090429', 'logps_train/rejected': '-91.187', 'logps_train/chosen': '-129.31', 'loss/train': '0.65261', 'examples_per_second': '31.781', 'grad_norm': '34.5', 'counters/examples': 105504, 'counters/updates': 3297}
skipping logging after 105536 examples to avoid logging too frequently
train stats after 105568 examples: {'rewards_train/chosen': '0.088435', 'rewards_train/rejected': '-0.010593', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099027', 'logps_train/rejected': '-109.08', 'logps_train/chosen': '-134.97', 'loss/train': '0.65175', 'examples_per_second': '31.761', 'grad_norm': '37.25', 'counters/examples': 105568, 'counters/updates': 3299}
train stats after 105600 examples: {'rewards_train/chosen': '0.1333', 'rewards_train/rejected': '0.078805', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054493', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-141.55', 'loss/train': '0.67378', 'examples_per_second': '31.382', 'grad_norm': '32.25', 'counters/examples': 105600, 'counters/updates': 3300}
train stats after 105632 examples: {'rewards_train/chosen': '0.1744', 'rewards_train/rejected': '0.041025', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-97.623', 'logps_train/chosen': '-149.08', 'loss/train': '0.63644', 'examples_per_second': '32.22', 'grad_norm': '26.125', 'counters/examples': 105632, 'counters/updates': 3301}
train stats after 105664 examples: {'rewards_train/chosen': '0.074097', 'rewards_train/rejected': '-0.0031484', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077246', 'logps_train/rejected': '-96.931', 'logps_train/chosen': '-107.22', 'loss/train': '0.66196', 'examples_per_second': '31.503', 'grad_norm': '23.5', 'counters/examples': 105664, 'counters/updates': 3302}
train stats after 105696 examples: {'rewards_train/chosen': '0.10281', 'rewards_train/rejected': '0.041038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061772', 'logps_train/rejected': '-102.27', 'logps_train/chosen': '-134.03', 'loss/train': '0.6659', 'examples_per_second': '30.962', 'grad_norm': '37.5', 'counters/examples': 105696, 'counters/updates': 3303}
train stats after 105728 examples: {'rewards_train/chosen': '0.10812', 'rewards_train/rejected': '0.038678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.069444', 'logps_train/rejected': '-98.523', 'logps_train/chosen': '-151.54', 'loss/train': '0.66297', 'examples_per_second': '31.221', 'grad_norm': '29.625', 'counters/examples': 105728, 'counters/updates': 3304}
train stats after 105760 examples: {'rewards_train/chosen': '0.17756', 'rewards_train/rejected': '0.034714', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14285', 'logps_train/rejected': '-118.55', 'logps_train/chosen': '-169.32', 'loss/train': '0.6314', 'examples_per_second': '30.386', 'grad_norm': '29.5', 'counters/examples': 105760, 'counters/updates': 3305}
train stats after 105792 examples: {'rewards_train/chosen': '0.28161', 'rewards_train/rejected': '0.11985', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16176', 'logps_train/rejected': '-133.99', 'logps_train/chosen': '-202.18', 'loss/train': '0.6807', 'examples_per_second': '30.507', 'grad_norm': '31', 'counters/examples': 105792, 'counters/updates': 3306}
train stats after 105824 examples: {'rewards_train/chosen': '0.068097', 'rewards_train/rejected': '0.023915', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044182', 'logps_train/rejected': '-126.34', 'logps_train/chosen': '-142.7', 'loss/train': '0.67768', 'examples_per_second': '29.962', 'grad_norm': '31.625', 'counters/examples': 105824, 'counters/updates': 3307}
train stats after 105856 examples: {'rewards_train/chosen': '0.1414', 'rewards_train/rejected': '0.080352', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061043', 'logps_train/rejected': '-118', 'logps_train/chosen': '-166.6', 'loss/train': '0.67359', 'examples_per_second': '29.91', 'grad_norm': '44', 'counters/examples': 105856, 'counters/updates': 3308}
train stats after 105888 examples: {'rewards_train/chosen': '0.091707', 'rewards_train/rejected': '0.12757', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.035868', 'logps_train/rejected': '-145.97', 'logps_train/chosen': '-139.2', 'loss/train': '0.72337', 'examples_per_second': '31.45', 'grad_norm': '49.25', 'counters/examples': 105888, 'counters/updates': 3309}
train stats after 105920 examples: {'rewards_train/chosen': '0.13181', 'rewards_train/rejected': '0.036439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095368', 'logps_train/rejected': '-134.42', 'logps_train/chosen': '-168.76', 'loss/train': '0.65619', 'examples_per_second': '30.533', 'grad_norm': '29', 'counters/examples': 105920, 'counters/updates': 3310}
train stats after 105952 examples: {'rewards_train/chosen': '0.12304', 'rewards_train/rejected': '0.028954', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094085', 'logps_train/rejected': '-147.24', 'logps_train/chosen': '-158.7', 'loss/train': '0.65797', 'examples_per_second': '31.459', 'grad_norm': '40', 'counters/examples': 105952, 'counters/updates': 3311}
train stats after 105984 examples: {'rewards_train/chosen': '0.097678', 'rewards_train/rejected': '0.14042', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042737', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-140.04', 'loss/train': '0.74096', 'examples_per_second': '33.109', 'grad_norm': '90', 'counters/examples': 105984, 'counters/updates': 3312}
train stats after 106016 examples: {'rewards_train/chosen': '0.067461', 'rewards_train/rejected': '0.040541', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.02692', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-190.69', 'loss/train': '0.68979', 'examples_per_second': '31.464', 'grad_norm': '46', 'counters/examples': 106016, 'counters/updates': 3313}
train stats after 106048 examples: {'rewards_train/chosen': '0.087292', 'rewards_train/rejected': '0.020466', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066826', 'logps_train/rejected': '-104.81', 'logps_train/chosen': '-150.75', 'loss/train': '0.66815', 'examples_per_second': '30.064', 'grad_norm': '28.75', 'counters/examples': 106048, 'counters/updates': 3314}
train stats after 106080 examples: {'rewards_train/chosen': '0.047618', 'rewards_train/rejected': '0.036175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011443', 'logps_train/rejected': '-111.7', 'logps_train/chosen': '-141.5', 'loss/train': '0.69426', 'examples_per_second': '31.271', 'grad_norm': '32.5', 'counters/examples': 106080, 'counters/updates': 3315}
train stats after 106112 examples: {'rewards_train/chosen': '0.12061', 'rewards_train/rejected': '0.051314', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069294', 'logps_train/rejected': '-135.36', 'logps_train/chosen': '-122.69', 'loss/train': '0.66717', 'examples_per_second': '33.095', 'grad_norm': '31.375', 'counters/examples': 106112, 'counters/updates': 3316}
train stats after 106144 examples: {'rewards_train/chosen': '0.075301', 'rewards_train/rejected': '0.076833', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.001532', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-162.66', 'loss/train': '0.70213', 'examples_per_second': '31.729', 'grad_norm': '41.25', 'counters/examples': 106144, 'counters/updates': 3317}
skipping logging after 106176 examples to avoid logging too frequently
train stats after 106208 examples: {'rewards_train/chosen': '0.078903', 'rewards_train/rejected': '0.039018', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039885', 'logps_train/rejected': '-96.913', 'logps_train/chosen': '-125', 'loss/train': '0.67974', 'examples_per_second': '30.49', 'grad_norm': '28.875', 'counters/examples': 106208, 'counters/updates': 3319}
train stats after 106240 examples: {'rewards_train/chosen': '0.14787', 'rewards_train/rejected': '0.048267', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0996', 'logps_train/rejected': '-100.17', 'logps_train/chosen': '-134.92', 'loss/train': '0.65103', 'examples_per_second': '31.018', 'grad_norm': '28', 'counters/examples': 106240, 'counters/updates': 3320}
train stats after 106272 examples: {'rewards_train/chosen': '0.067474', 'rewards_train/rejected': '0.11818', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.050702', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-136.32', 'loss/train': '0.73544', 'examples_per_second': '31.957', 'grad_norm': '41.25', 'counters/examples': 106272, 'counters/updates': 3321}
skipping logging after 106304 examples to avoid logging too frequently
train stats after 106336 examples: {'rewards_train/chosen': '0.03321', 'rewards_train/rejected': '0.07196', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03875', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-119.84', 'loss/train': '0.71979', 'examples_per_second': '32.668', 'grad_norm': '43.25', 'counters/examples': 106336, 'counters/updates': 3323}
train stats after 106368 examples: {'rewards_train/chosen': '0.11893', 'rewards_train/rejected': '0.069704', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04923', 'logps_train/rejected': '-110.44', 'logps_train/chosen': '-147.94', 'loss/train': '0.67874', 'examples_per_second': '31.469', 'grad_norm': '32.75', 'counters/examples': 106368, 'counters/updates': 3324}
train stats after 106400 examples: {'rewards_train/chosen': '0.1467', 'rewards_train/rejected': '0.0099577', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13674', 'logps_train/rejected': '-130.69', 'logps_train/chosen': '-140.8', 'loss/train': '0.63757', 'examples_per_second': '29.965', 'grad_norm': '38.25', 'counters/examples': 106400, 'counters/updates': 3325}
skipping logging after 106432 examples to avoid logging too frequently
train stats after 106464 examples: {'rewards_train/chosen': '0.24974', 'rewards_train/rejected': '0.11194', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1378', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-170.81', 'loss/train': '0.63903', 'examples_per_second': '30.026', 'grad_norm': '42', 'counters/examples': 106464, 'counters/updates': 3327}
train stats after 106496 examples: {'rewards_train/chosen': '0.11394', 'rewards_train/rejected': '0.069402', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044541', 'logps_train/rejected': '-145.17', 'logps_train/chosen': '-135.83', 'loss/train': '0.68474', 'examples_per_second': '31.48', 'grad_norm': '40.25', 'counters/examples': 106496, 'counters/updates': 3328}
train stats after 106528 examples: {'rewards_train/chosen': '0.098527', 'rewards_train/rejected': '0.049791', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048735', 'logps_train/rejected': '-164.79', 'logps_train/chosen': '-184.22', 'loss/train': '0.67651', 'examples_per_second': '31.341', 'grad_norm': '37', 'counters/examples': 106528, 'counters/updates': 3329}
train stats after 106560 examples: {'rewards_train/chosen': '0.029494', 'rewards_train/rejected': '0.066371', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036877', 'logps_train/rejected': '-149.64', 'logps_train/chosen': '-165.45', 'loss/train': '0.71854', 'examples_per_second': '30.701', 'grad_norm': '39.75', 'counters/examples': 106560, 'counters/updates': 3330}
train stats after 106592 examples: {'rewards_train/chosen': '0.14323', 'rewards_train/rejected': '0.044286', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098945', 'logps_train/rejected': '-117.45', 'logps_train/chosen': '-129.69', 'loss/train': '0.65188', 'examples_per_second': '33.269', 'grad_norm': '33.5', 'counters/examples': 106592, 'counters/updates': 3331}
train stats after 106624 examples: {'rewards_train/chosen': '0.094846', 'rewards_train/rejected': '0.015566', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07928', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-117.96', 'loss/train': '0.66594', 'examples_per_second': '31.492', 'grad_norm': '29.375', 'counters/examples': 106624, 'counters/updates': 3332}
train stats after 106656 examples: {'rewards_train/chosen': '0.040983', 'rewards_train/rejected': '0.019822', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021161', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-130.36', 'loss/train': '0.68791', 'examples_per_second': '31.779', 'grad_norm': '29.625', 'counters/examples': 106656, 'counters/updates': 3333}
train stats after 106688 examples: {'rewards_train/chosen': '0.15757', 'rewards_train/rejected': '0.033002', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12457', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-144.76', 'loss/train': '0.65532', 'examples_per_second': '31.233', 'grad_norm': '30.625', 'counters/examples': 106688, 'counters/updates': 3334}
train stats after 106720 examples: {'rewards_train/chosen': '0.14139', 'rewards_train/rejected': '0.037221', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10417', 'logps_train/rejected': '-119.63', 'logps_train/chosen': '-137.93', 'loss/train': '0.64789', 'examples_per_second': '29.839', 'grad_norm': '25', 'counters/examples': 106720, 'counters/updates': 3335}
train stats after 106752 examples: {'rewards_train/chosen': '0.1748', 'rewards_train/rejected': '0.080915', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093884', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-139.6', 'loss/train': '0.65969', 'examples_per_second': '31.336', 'grad_norm': '30.625', 'counters/examples': 106752, 'counters/updates': 3336}
skipping logging after 106784 examples to avoid logging too frequently
train stats after 106816 examples: {'rewards_train/chosen': '0.091533', 'rewards_train/rejected': '-0.0028762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094409', 'logps_train/rejected': '-132.94', 'logps_train/chosen': '-108.09', 'loss/train': '0.66317', 'examples_per_second': '31.453', 'grad_norm': '30.875', 'counters/examples': 106816, 'counters/updates': 3338}
train stats after 106848 examples: {'rewards_train/chosen': '0.14766', 'rewards_train/rejected': '0.17488', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02722', 'logps_train/rejected': '-171.51', 'logps_train/chosen': '-142.1', 'loss/train': '0.72244', 'examples_per_second': '31.168', 'grad_norm': '39.5', 'counters/examples': 106848, 'counters/updates': 3339}
train stats after 106880 examples: {'rewards_train/chosen': '0.12223', 'rewards_train/rejected': '0.029194', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093039', 'logps_train/rejected': '-111.81', 'logps_train/chosen': '-167.38', 'loss/train': '0.65612', 'examples_per_second': '31.421', 'grad_norm': '54.25', 'counters/examples': 106880, 'counters/updates': 3340}
train stats after 106912 examples: {'rewards_train/chosen': '0.13656', 'rewards_train/rejected': '0.03882', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097744', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-152.08', 'loss/train': '0.64968', 'examples_per_second': '29.907', 'grad_norm': '29.5', 'counters/examples': 106912, 'counters/updates': 3341}
train stats after 106944 examples: {'rewards_train/chosen': '0.13664', 'rewards_train/rejected': '0.038919', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.097719', 'logps_train/rejected': '-140.61', 'logps_train/chosen': '-136.96', 'loss/train': '0.66569', 'examples_per_second': '31.408', 'grad_norm': '100.5', 'counters/examples': 106944, 'counters/updates': 3342}
train stats after 106976 examples: {'rewards_train/chosen': '0.059135', 'rewards_train/rejected': '-0.047066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1062', 'logps_train/rejected': '-92.099', 'logps_train/chosen': '-129.01', 'loss/train': '0.64866', 'examples_per_second': '31.817', 'grad_norm': '26.125', 'counters/examples': 106976, 'counters/updates': 3343}
train stats after 107008 examples: {'rewards_train/chosen': '0.11807', 'rewards_train/rejected': '0.046958', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071114', 'logps_train/rejected': '-133.86', 'logps_train/chosen': '-169.56', 'loss/train': '0.66614', 'examples_per_second': '32.045', 'grad_norm': '34.25', 'counters/examples': 107008, 'counters/updates': 3344}
train stats after 107040 examples: {'rewards_train/chosen': '0.085767', 'rewards_train/rejected': '0.0086605', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077106', 'logps_train/rejected': '-109.61', 'logps_train/chosen': '-132.44', 'loss/train': '0.66346', 'examples_per_second': '30.139', 'grad_norm': '28.875', 'counters/examples': 107040, 'counters/updates': 3345}
skipping logging after 107072 examples to avoid logging too frequently
train stats after 107104 examples: {'rewards_train/chosen': '0.14391', 'rewards_train/rejected': '0.10314', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040767', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-126.34', 'loss/train': '0.68735', 'examples_per_second': '32.33', 'grad_norm': '36.5', 'counters/examples': 107104, 'counters/updates': 3347}
train stats after 107136 examples: {'rewards_train/chosen': '0.12196', 'rewards_train/rejected': '0.014792', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10716', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-133.61', 'loss/train': '0.64758', 'examples_per_second': '31.249', 'grad_norm': '31.125', 'counters/examples': 107136, 'counters/updates': 3348}
train stats after 107168 examples: {'rewards_train/chosen': '0.17587', 'rewards_train/rejected': '0.026607', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14926', 'logps_train/rejected': '-115.28', 'logps_train/chosen': '-164.78', 'loss/train': '0.6311', 'examples_per_second': '32.148', 'grad_norm': '34.5', 'counters/examples': 107168, 'counters/updates': 3349}
train stats after 107200 examples: {'rewards_train/chosen': '0.051187', 'rewards_train/rejected': '0.069476', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.01829', 'logps_train/rejected': '-117.07', 'logps_train/chosen': '-108.49', 'loss/train': '0.71124', 'examples_per_second': '31.461', 'grad_norm': '29.125', 'counters/examples': 107200, 'counters/updates': 3350}
skipping logging after 107232 examples to avoid logging too frequently
train stats after 107264 examples: {'rewards_train/chosen': '0.10613', 'rewards_train/rejected': '0.096596', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0095341', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-127.93', 'loss/train': '0.70307', 'examples_per_second': '31.443', 'grad_norm': '80.5', 'counters/examples': 107264, 'counters/updates': 3352}
train stats after 107296 examples: {'rewards_train/chosen': '0.11435', 'rewards_train/rejected': '0.093847', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020502', 'logps_train/rejected': '-151.42', 'logps_train/chosen': '-98.992', 'loss/train': '0.69228', 'examples_per_second': '29.941', 'grad_norm': '61', 'counters/examples': 107296, 'counters/updates': 3353}
train stats after 107328 examples: {'rewards_train/chosen': '0.08544', 'rewards_train/rejected': '0.033727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051713', 'logps_train/rejected': '-102.36', 'logps_train/chosen': '-128.95', 'loss/train': '0.67215', 'examples_per_second': '30.862', 'grad_norm': '24.5', 'counters/examples': 107328, 'counters/updates': 3354}
skipping logging after 107360 examples to avoid logging too frequently
train stats after 107392 examples: {'rewards_train/chosen': '0.12854', 'rewards_train/rejected': '0.12664', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0019011', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-152.05', 'loss/train': '0.70163', 'examples_per_second': '31.16', 'grad_norm': '46.75', 'counters/examples': 107392, 'counters/updates': 3356}
train stats after 107424 examples: {'rewards_train/chosen': '0.17154', 'rewards_train/rejected': '0.096272', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07527', 'logps_train/rejected': '-157.81', 'logps_train/chosen': '-184.62', 'loss/train': '0.67063', 'examples_per_second': '31.261', 'grad_norm': '38.5', 'counters/examples': 107424, 'counters/updates': 3357}
train stats after 107456 examples: {'rewards_train/chosen': '0.068924', 'rewards_train/rejected': '0.10867', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.039746', 'logps_train/rejected': '-122.42', 'logps_train/chosen': '-143.73', 'loss/train': '0.7236', 'examples_per_second': '31.225', 'grad_norm': '39.25', 'counters/examples': 107456, 'counters/updates': 3358}
train stats after 107488 examples: {'rewards_train/chosen': '0.098321', 'rewards_train/rejected': '0.15677', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.058453', 'logps_train/rejected': '-154.52', 'logps_train/chosen': '-126.09', 'loss/train': '0.73114', 'examples_per_second': '31.741', 'grad_norm': '50', 'counters/examples': 107488, 'counters/updates': 3359}
train stats after 107520 examples: {'rewards_train/chosen': '0.17157', 'rewards_train/rejected': '0.093009', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078559', 'logps_train/rejected': '-134.77', 'logps_train/chosen': '-164.62', 'loss/train': '0.66627', 'examples_per_second': '31.482', 'grad_norm': '33.75', 'counters/examples': 107520, 'counters/updates': 3360}
train stats after 107552 examples: {'rewards_train/chosen': '0.16585', 'rewards_train/rejected': '0.051584', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11426', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-171.25', 'loss/train': '0.64419', 'examples_per_second': '31.533', 'grad_norm': '43', 'counters/examples': 107552, 'counters/updates': 3361}
skipping logging after 107584 examples to avoid logging too frequently
train stats after 107616 examples: {'rewards_train/chosen': '0.076586', 'rewards_train/rejected': '-0.0084567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085043', 'logps_train/rejected': '-147.96', 'logps_train/chosen': '-150.85', 'loss/train': '0.6585', 'examples_per_second': '31.477', 'grad_norm': '46.5', 'counters/examples': 107616, 'counters/updates': 3363}
skipping logging after 107648 examples to avoid logging too frequently
train stats after 107680 examples: {'rewards_train/chosen': '0.07065', 'rewards_train/rejected': '0.035756', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034894', 'logps_train/rejected': '-143.85', 'logps_train/chosen': '-119.69', 'loss/train': '0.68341', 'examples_per_second': '31.501', 'grad_norm': '35.75', 'counters/examples': 107680, 'counters/updates': 3365}
train stats after 107712 examples: {'rewards_train/chosen': '0.085376', 'rewards_train/rejected': '0.16651', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.081131', 'logps_train/rejected': '-125.94', 'logps_train/chosen': '-135.23', 'loss/train': '0.74687', 'examples_per_second': '31.054', 'grad_norm': '34.5', 'counters/examples': 107712, 'counters/updates': 3366}
train stats after 107744 examples: {'rewards_train/chosen': '0.12571', 'rewards_train/rejected': '0.0091573', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11656', 'logps_train/rejected': '-163.66', 'logps_train/chosen': '-113.09', 'loss/train': '0.64779', 'examples_per_second': '31.447', 'grad_norm': '31.625', 'counters/examples': 107744, 'counters/updates': 3367}
train stats after 107776 examples: {'rewards_train/chosen': '0.12279', 'rewards_train/rejected': '0.042878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07991', 'logps_train/rejected': '-116.27', 'logps_train/chosen': '-142.25', 'loss/train': '0.66804', 'examples_per_second': '32.262', 'grad_norm': '27.125', 'counters/examples': 107776, 'counters/updates': 3368}
skipping logging after 107808 examples to avoid logging too frequently
train stats after 107840 examples: {'rewards_train/chosen': '0.057484', 'rewards_train/rejected': '0.065751', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0082674', 'logps_train/rejected': '-145.58', 'logps_train/chosen': '-115.94', 'loss/train': '0.70201', 'examples_per_second': '31.307', 'grad_norm': '30.125', 'counters/examples': 107840, 'counters/updates': 3370}
skipping logging after 107872 examples to avoid logging too frequently
train stats after 107904 examples: {'rewards_train/chosen': '0.13308', 'rewards_train/rejected': '0.070653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062427', 'logps_train/rejected': '-138.48', 'logps_train/chosen': '-144.7', 'loss/train': '0.67396', 'examples_per_second': '31.356', 'grad_norm': '32.25', 'counters/examples': 107904, 'counters/updates': 3372}
train stats after 107936 examples: {'rewards_train/chosen': '0.042331', 'rewards_train/rejected': '0.021837', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020494', 'logps_train/rejected': '-100.15', 'logps_train/chosen': '-127.93', 'loss/train': '0.69128', 'examples_per_second': '32.053', 'grad_norm': '28.25', 'counters/examples': 107936, 'counters/updates': 3373}
skipping logging after 107968 examples to avoid logging too frequently
train stats after 108000 examples: {'rewards_train/chosen': '0.23787', 'rewards_train/rejected': '0.16856', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069304', 'logps_train/rejected': '-148', 'logps_train/chosen': '-150.92', 'loss/train': '0.68294', 'examples_per_second': '31.744', 'grad_norm': '32.25', 'counters/examples': 108000, 'counters/updates': 3375}
Running evaluation after 108000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 108000: {'rewards_eval/chosen': '0.12982', 'rewards_eval/rejected': '0.053927', 'rewards_eval/accuracies': '0.57812', 'rewards_eval/margins': '0.075888', 'logps_eval/rejected': '-121.59', 'logps_eval/chosen': '-142.81', 'loss/eval': '0.66553'}
skipping save for non epoch
train stats after 108032 examples: {'rewards_train/chosen': '0.079109', 'rewards_train/rejected': '0.043383', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.035726', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-119.6', 'loss/train': '0.68207', 'examples_per_second': '34.102', 'grad_norm': '32.75', 'counters/examples': 108032, 'counters/updates': 3376}
train stats after 108064 examples: {'rewards_train/chosen': '0.12214', 'rewards_train/rejected': '0.09825', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02389', 'logps_train/rejected': '-131.26', 'logps_train/chosen': '-159.28', 'loss/train': '0.69152', 'examples_per_second': '33.236', 'grad_norm': '46.25', 'counters/examples': 108064, 'counters/updates': 3377}
train stats after 108096 examples: {'rewards_train/chosen': '0.13652', 'rewards_train/rejected': '0.015355', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12116', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-132.72', 'loss/train': '0.63858', 'examples_per_second': '30.525', 'grad_norm': '28.75', 'counters/examples': 108096, 'counters/updates': 3378}
skipping logging after 108128 examples to avoid logging too frequently
train stats after 108160 examples: {'rewards_train/chosen': '0.14429', 'rewards_train/rejected': '0.026027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11826', 'logps_train/rejected': '-89.422', 'logps_train/chosen': '-147.95', 'loss/train': '0.64256', 'examples_per_second': '26.028', 'grad_norm': '27.375', 'counters/examples': 108160, 'counters/updates': 3380}
train stats after 108192 examples: {'rewards_train/chosen': '0.15644', 'rewards_train/rejected': '0.099496', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056942', 'logps_train/rejected': '-142.39', 'logps_train/chosen': '-154.79', 'loss/train': '0.67696', 'examples_per_second': '32.31', 'grad_norm': '35.25', 'counters/examples': 108192, 'counters/updates': 3381}
train stats after 108224 examples: {'rewards_train/chosen': '0.13414', 'rewards_train/rejected': '0.029858', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10428', 'logps_train/rejected': '-184.57', 'logps_train/chosen': '-154.94', 'loss/train': '0.6542', 'examples_per_second': '31.154', 'grad_norm': '44', 'counters/examples': 108224, 'counters/updates': 3382}
train stats after 108256 examples: {'rewards_train/chosen': '0.13217', 'rewards_train/rejected': '0.06139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070775', 'logps_train/rejected': '-138.81', 'logps_train/chosen': '-145.27', 'loss/train': '0.66572', 'examples_per_second': '24.189', 'grad_norm': '33.25', 'counters/examples': 108256, 'counters/updates': 3383}
skipping logging after 108288 examples to avoid logging too frequently
train stats after 108320 examples: {'rewards_train/chosen': '0.13196', 'rewards_train/rejected': '0.026429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10553', 'logps_train/rejected': '-110.74', 'logps_train/chosen': '-124.05', 'loss/train': '0.65282', 'examples_per_second': '31.207', 'grad_norm': '27.625', 'counters/examples': 108320, 'counters/updates': 3385}
skipping logging after 108352 examples to avoid logging too frequently
train stats after 108384 examples: {'rewards_train/chosen': '0.044006', 'rewards_train/rejected': '0.09839', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.054384', 'logps_train/rejected': '-119.12', 'logps_train/chosen': '-145.72', 'loss/train': '0.73166', 'examples_per_second': '30.058', 'grad_norm': '33.5', 'counters/examples': 108384, 'counters/updates': 3387}
skipping logging after 108416 examples to avoid logging too frequently
train stats after 108448 examples: {'rewards_train/chosen': '0.15521', 'rewards_train/rejected': '0.062886', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.092325', 'logps_train/rejected': '-149.51', 'logps_train/chosen': '-182.97', 'loss/train': '0.66385', 'examples_per_second': '29.931', 'grad_norm': '34.5', 'counters/examples': 108448, 'counters/updates': 3389}
train stats after 108480 examples: {'rewards_train/chosen': '0.057211', 'rewards_train/rejected': '0.047738', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0094733', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-134.42', 'loss/train': '0.69614', 'examples_per_second': '30.714', 'grad_norm': '33', 'counters/examples': 108480, 'counters/updates': 3390}
train stats after 108512 examples: {'rewards_train/chosen': '0.10044', 'rewards_train/rejected': '0.056186', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04425', 'logps_train/rejected': '-160.26', 'logps_train/chosen': '-167.81', 'loss/train': '0.679', 'examples_per_second': '31.21', 'grad_norm': '41.5', 'counters/examples': 108512, 'counters/updates': 3391}
skipping logging after 108544 examples to avoid logging too frequently
train stats after 108576 examples: {'rewards_train/chosen': '0.21781', 'rewards_train/rejected': '0.03601', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1818', 'logps_train/rejected': '-155.24', 'logps_train/chosen': '-173.85', 'loss/train': '0.62954', 'examples_per_second': '31.401', 'grad_norm': '34.5', 'counters/examples': 108576, 'counters/updates': 3393}
train stats after 108608 examples: {'rewards_train/chosen': '0.12351', 'rewards_train/rejected': '0.10151', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022008', 'logps_train/rejected': '-125.48', 'logps_train/chosen': '-136.88', 'loss/train': '0.69183', 'examples_per_second': '29.978', 'grad_norm': '34.75', 'counters/examples': 108608, 'counters/updates': 3394}
skipping logging after 108640 examples to avoid logging too frequently
train stats after 108672 examples: {'rewards_train/chosen': '0.14892', 'rewards_train/rejected': '0.029637', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11928', 'logps_train/rejected': '-127.6', 'logps_train/chosen': '-134.66', 'loss/train': '0.64333', 'examples_per_second': '31.873', 'grad_norm': '26.375', 'counters/examples': 108672, 'counters/updates': 3396}
train stats after 108704 examples: {'rewards_train/chosen': '0.05677', 'rewards_train/rejected': '0.034458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.022312', 'logps_train/rejected': '-124.55', 'logps_train/chosen': '-140.43', 'loss/train': '0.69491', 'examples_per_second': '30.902', 'grad_norm': '29.125', 'counters/examples': 108704, 'counters/updates': 3397}
train stats after 108736 examples: {'rewards_train/chosen': '0.14421', 'rewards_train/rejected': '0.060001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084205', 'logps_train/rejected': '-142.42', 'logps_train/chosen': '-163.29', 'loss/train': '0.66086', 'examples_per_second': '30.278', 'grad_norm': '29.625', 'counters/examples': 108736, 'counters/updates': 3398}
train stats after 108768 examples: {'rewards_train/chosen': '0.17609', 'rewards_train/rejected': '0.038027', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13806', 'logps_train/rejected': '-140.43', 'logps_train/chosen': '-157.52', 'loss/train': '0.6445', 'examples_per_second': '30.094', 'grad_norm': '35', 'counters/examples': 108768, 'counters/updates': 3399}
train stats after 108800 examples: {'rewards_train/chosen': '0.043117', 'rewards_train/rejected': '-0.00033302', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04345', 'logps_train/rejected': '-86.549', 'logps_train/chosen': '-108.82', 'loss/train': '0.67692', 'examples_per_second': '30.3', 'grad_norm': '22.875', 'counters/examples': 108800, 'counters/updates': 3400}
skipping logging after 108832 examples to avoid logging too frequently
train stats after 108864 examples: {'rewards_train/chosen': '0.090189', 'rewards_train/rejected': '0.023818', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066371', 'logps_train/rejected': '-85.671', 'logps_train/chosen': '-118.23', 'loss/train': '0.66564', 'examples_per_second': '31.245', 'grad_norm': '21.875', 'counters/examples': 108864, 'counters/updates': 3402}
train stats after 108896 examples: {'rewards_train/chosen': '0.076342', 'rewards_train/rejected': '0.006406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069936', 'logps_train/rejected': '-151.12', 'logps_train/chosen': '-175.74', 'loss/train': '0.66684', 'examples_per_second': '31.521', 'grad_norm': '43.25', 'counters/examples': 108896, 'counters/updates': 3403}
skipping logging after 108928 examples to avoid logging too frequently
train stats after 108960 examples: {'rewards_train/chosen': '0.080519', 'rewards_train/rejected': '0.071123', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0093955', 'logps_train/rejected': '-148.12', 'logps_train/chosen': '-135.01', 'loss/train': '0.70352', 'examples_per_second': '35.649', 'grad_norm': '62.75', 'counters/examples': 108960, 'counters/updates': 3405}
train stats after 108992 examples: {'rewards_train/chosen': '0.088759', 'rewards_train/rejected': '0.12167', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.032913', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-174.87', 'loss/train': '0.72136', 'examples_per_second': '30.528', 'grad_norm': '54', 'counters/examples': 108992, 'counters/updates': 3406}
train stats after 109024 examples: {'rewards_train/chosen': '0.1889', 'rewards_train/rejected': '0.011596', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1773', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-159.18', 'loss/train': '0.61827', 'examples_per_second': '32.925', 'grad_norm': '41.75', 'counters/examples': 109024, 'counters/updates': 3407}
train stats after 109056 examples: {'rewards_train/chosen': '0.13702', 'rewards_train/rejected': '-0.020964', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15799', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-204.73', 'loss/train': '0.63235', 'examples_per_second': '31.362', 'grad_norm': '44', 'counters/examples': 109056, 'counters/updates': 3408}
train stats after 109088 examples: {'rewards_train/chosen': '0.12803', 'rewards_train/rejected': '0.07859', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049435', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-153.77', 'loss/train': '0.67679', 'examples_per_second': '32.038', 'grad_norm': '37.75', 'counters/examples': 109088, 'counters/updates': 3409}
train stats after 109120 examples: {'rewards_train/chosen': '0.076173', 'rewards_train/rejected': '-0.0062871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08246', 'logps_train/rejected': '-118.39', 'logps_train/chosen': '-143.8', 'loss/train': '0.6574', 'examples_per_second': '32.482', 'grad_norm': '27', 'counters/examples': 109120, 'counters/updates': 3410}
train stats after 109152 examples: {'rewards_train/chosen': '0.031303', 'rewards_train/rejected': '0.0069216', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024382', 'logps_train/rejected': '-93.285', 'logps_train/chosen': '-132.82', 'loss/train': '0.69044', 'examples_per_second': '32.732', 'grad_norm': '29.375', 'counters/examples': 109152, 'counters/updates': 3411}
skipping logging after 109184 examples to avoid logging too frequently
train stats after 109216 examples: {'rewards_train/chosen': '0.12678', 'rewards_train/rejected': '0.033843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092937', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-110.25', 'loss/train': '0.65548', 'examples_per_second': '33.028', 'grad_norm': '24.5', 'counters/examples': 109216, 'counters/updates': 3413}
skipping logging after 109248 examples to avoid logging too frequently
train stats after 109280 examples: {'rewards_train/chosen': '0.10036', 'rewards_train/rejected': '0.061517', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.038847', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-161.3', 'loss/train': '0.67795', 'examples_per_second': '32.818', 'grad_norm': '33.5', 'counters/examples': 109280, 'counters/updates': 3415}
train stats after 109312 examples: {'rewards_train/chosen': '0.11722', 'rewards_train/rejected': '0.030889', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086329', 'logps_train/rejected': '-130.87', 'logps_train/chosen': '-160.79', 'loss/train': '0.66589', 'examples_per_second': '31.281', 'grad_norm': '31.5', 'counters/examples': 109312, 'counters/updates': 3416}
train stats after 109344 examples: {'rewards_train/chosen': '0.099475', 'rewards_train/rejected': '0.00278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096695', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-141.76', 'loss/train': '0.65896', 'examples_per_second': '30.533', 'grad_norm': '36.75', 'counters/examples': 109344, 'counters/updates': 3417}
train stats after 109376 examples: {'rewards_train/chosen': '0.047189', 'rewards_train/rejected': '0.00096177', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046227', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-154.48', 'loss/train': '0.68035', 'examples_per_second': '31.433', 'grad_norm': '67.5', 'counters/examples': 109376, 'counters/updates': 3418}
train stats after 109408 examples: {'rewards_train/chosen': '0.084819', 'rewards_train/rejected': '0.035511', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.049309', 'logps_train/rejected': '-141.89', 'logps_train/chosen': '-128.34', 'loss/train': '0.68302', 'examples_per_second': '30.879', 'grad_norm': '41.5', 'counters/examples': 109408, 'counters/updates': 3419}
train stats after 109440 examples: {'rewards_train/chosen': '0.1173', 'rewards_train/rejected': '0.025173', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092124', 'logps_train/rejected': '-140.5', 'logps_train/chosen': '-131.03', 'loss/train': '0.66118', 'examples_per_second': '31.475', 'grad_norm': '31.375', 'counters/examples': 109440, 'counters/updates': 3420}
train stats after 109472 examples: {'rewards_train/chosen': '0.11053', 'rewards_train/rejected': '0.0026773', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10786', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-146.86', 'loss/train': '0.65066', 'examples_per_second': '31.964', 'grad_norm': '34.25', 'counters/examples': 109472, 'counters/updates': 3421}
train stats after 109504 examples: {'rewards_train/chosen': '0.10894', 'rewards_train/rejected': '-0.036824', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14576', 'logps_train/rejected': '-116.64', 'logps_train/chosen': '-160.69', 'loss/train': '0.6348', 'examples_per_second': '32.49', 'grad_norm': '29.125', 'counters/examples': 109504, 'counters/updates': 3422}
train stats after 109536 examples: {'rewards_train/chosen': '0.1426', 'rewards_train/rejected': '0.14784', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0052415', 'logps_train/rejected': '-149.01', 'logps_train/chosen': '-151.35', 'loss/train': '0.70495', 'examples_per_second': '30.906', 'grad_norm': '33.25', 'counters/examples': 109536, 'counters/updates': 3423}
skipping logging after 109568 examples to avoid logging too frequently
train stats after 109600 examples: {'rewards_train/chosen': '0.16867', 'rewards_train/rejected': '0.024031', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14464', 'logps_train/rejected': '-133.09', 'logps_train/chosen': '-151.43', 'loss/train': '0.63717', 'examples_per_second': '30.502', 'grad_norm': '41.25', 'counters/examples': 109600, 'counters/updates': 3425}
train stats after 109632 examples: {'rewards_train/chosen': '0.094926', 'rewards_train/rejected': '-0.0032986', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.098224', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-140.16', 'loss/train': '0.65165', 'examples_per_second': '31.146', 'grad_norm': '36.75', 'counters/examples': 109632, 'counters/updates': 3426}
train stats after 109664 examples: {'rewards_train/chosen': '0.11003', 'rewards_train/rejected': '0.099719', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010312', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-167.38', 'loss/train': '0.69684', 'examples_per_second': '31.453', 'grad_norm': '28.25', 'counters/examples': 109664, 'counters/updates': 3427}
train stats after 109696 examples: {'rewards_train/chosen': '0.12282', 'rewards_train/rejected': '0.040949', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081872', 'logps_train/rejected': '-183.13', 'logps_train/chosen': '-180.75', 'loss/train': '0.66331', 'examples_per_second': '30.498', 'grad_norm': '41.5', 'counters/examples': 109696, 'counters/updates': 3428}
skipping logging after 109728 examples to avoid logging too frequently
train stats after 109760 examples: {'rewards_train/chosen': '0.11158', 'rewards_train/rejected': '0.12215', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.010566', 'logps_train/rejected': '-143.99', 'logps_train/chosen': '-121.33', 'loss/train': '0.7115', 'examples_per_second': '30.876', 'grad_norm': '39.25', 'counters/examples': 109760, 'counters/updates': 3430}
train stats after 109792 examples: {'rewards_train/chosen': '0.072827', 'rewards_train/rejected': '0.089083', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.016257', 'logps_train/rejected': '-119.33', 'logps_train/chosen': '-125.78', 'loss/train': '0.71303', 'examples_per_second': '31.729', 'grad_norm': '49.25', 'counters/examples': 109792, 'counters/updates': 3431}
train stats after 109824 examples: {'rewards_train/chosen': '0.0732', 'rewards_train/rejected': '0.030189', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043011', 'logps_train/rejected': '-107.38', 'logps_train/chosen': '-189.76', 'loss/train': '0.68505', 'examples_per_second': '31.429', 'grad_norm': '37', 'counters/examples': 109824, 'counters/updates': 3432}
train stats after 109856 examples: {'rewards_train/chosen': '0.079745', 'rewards_train/rejected': '0.023379', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056366', 'logps_train/rejected': '-111.96', 'logps_train/chosen': '-153.75', 'loss/train': '0.67396', 'examples_per_second': '30.599', 'grad_norm': '30.75', 'counters/examples': 109856, 'counters/updates': 3433}
train stats after 109888 examples: {'rewards_train/chosen': '0.084557', 'rewards_train/rejected': '0.03801', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046547', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-193.12', 'loss/train': '0.6766', 'examples_per_second': '32.331', 'grad_norm': '27.75', 'counters/examples': 109888, 'counters/updates': 3434}
train stats after 109920 examples: {'rewards_train/chosen': '0.038819', 'rewards_train/rejected': '0.077507', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.038689', 'logps_train/rejected': '-143.9', 'logps_train/chosen': '-169.44', 'loss/train': '0.72444', 'examples_per_second': '30.248', 'grad_norm': '73.5', 'counters/examples': 109920, 'counters/updates': 3435}
train stats after 109952 examples: {'rewards_train/chosen': '0.08094', 'rewards_train/rejected': '-0.038917', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11986', 'logps_train/rejected': '-162.33', 'logps_train/chosen': '-135.82', 'loss/train': '0.64109', 'examples_per_second': '31.432', 'grad_norm': '50', 'counters/examples': 109952, 'counters/updates': 3436}
train stats after 109984 examples: {'rewards_train/chosen': '0.032414', 'rewards_train/rejected': '0.072744', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.040329', 'logps_train/rejected': '-155.04', 'logps_train/chosen': '-120.68', 'loss/train': '0.72015', 'examples_per_second': '31.213', 'grad_norm': '32', 'counters/examples': 109984, 'counters/updates': 3437}
train stats after 110016 examples: {'rewards_train/chosen': '0.11003', 'rewards_train/rejected': '0.033166', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076862', 'logps_train/rejected': '-116.23', 'logps_train/chosen': '-125.46', 'loss/train': '0.66263', 'examples_per_second': '31.584', 'grad_norm': '28.75', 'counters/examples': 110016, 'counters/updates': 3438}
train stats after 110048 examples: {'rewards_train/chosen': '0.09241', 'rewards_train/rejected': '0.084352', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0080584', 'logps_train/rejected': '-107.19', 'logps_train/chosen': '-139.13', 'loss/train': '0.69545', 'examples_per_second': '30.982', 'grad_norm': '33', 'counters/examples': 110048, 'counters/updates': 3439}
train stats after 110080 examples: {'rewards_train/chosen': '0.055047', 'rewards_train/rejected': '0.060931', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0058835', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-126.42', 'loss/train': '0.70241', 'examples_per_second': '30.431', 'grad_norm': '29.125', 'counters/examples': 110080, 'counters/updates': 3440}
train stats after 110112 examples: {'rewards_train/chosen': '0.058556', 'rewards_train/rejected': '0.043432', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015123', 'logps_train/rejected': '-139.57', 'logps_train/chosen': '-140.48', 'loss/train': '0.691', 'examples_per_second': '31.439', 'grad_norm': '32.5', 'counters/examples': 110112, 'counters/updates': 3441}
train stats after 110144 examples: {'rewards_train/chosen': '-0.051618', 'rewards_train/rejected': '0.03428', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.085898', 'logps_train/rejected': '-101.67', 'logps_train/chosen': '-121.5', 'loss/train': '0.75298', 'examples_per_second': '31.382', 'grad_norm': '50.75', 'counters/examples': 110144, 'counters/updates': 3442}
train stats after 110176 examples: {'rewards_train/chosen': '0.14032', 'rewards_train/rejected': '0.069789', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.070533', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-129.63', 'loss/train': '0.66646', 'examples_per_second': '29.966', 'grad_norm': '34.25', 'counters/examples': 110176, 'counters/updates': 3443}
train stats after 110208 examples: {'rewards_train/chosen': '0.11955', 'rewards_train/rejected': '0.08276', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036791', 'logps_train/rejected': '-108.2', 'logps_train/chosen': '-145.66', 'loss/train': '0.70027', 'examples_per_second': '30.768', 'grad_norm': '31', 'counters/examples': 110208, 'counters/updates': 3444}
train stats after 110240 examples: {'rewards_train/chosen': '0.1096', 'rewards_train/rejected': '0.015021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094579', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-148.79', 'loss/train': '0.6567', 'examples_per_second': '24.69', 'grad_norm': '49.75', 'counters/examples': 110240, 'counters/updates': 3445}
train stats after 110272 examples: {'rewards_train/chosen': '0.17943', 'rewards_train/rejected': '0.079861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099566', 'logps_train/rejected': '-120.57', 'logps_train/chosen': '-145.38', 'loss/train': '0.65843', 'examples_per_second': '31.33', 'grad_norm': '36.5', 'counters/examples': 110272, 'counters/updates': 3446}
skipping logging after 110304 examples to avoid logging too frequently
train stats after 110336 examples: {'rewards_train/chosen': '0.12708', 'rewards_train/rejected': '-0.01173', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13881', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-119.4', 'loss/train': '0.63391', 'examples_per_second': '35.368', 'grad_norm': '28.75', 'counters/examples': 110336, 'counters/updates': 3448}
train stats after 110368 examples: {'rewards_train/chosen': '0.23095', 'rewards_train/rejected': '0.030943', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20001', 'logps_train/rejected': '-148.69', 'logps_train/chosen': '-141.99', 'loss/train': '0.61639', 'examples_per_second': '30.115', 'grad_norm': '38.5', 'counters/examples': 110368, 'counters/updates': 3449}
train stats after 110400 examples: {'rewards_train/chosen': '0.047864', 'rewards_train/rejected': '0.01207', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.035793', 'logps_train/rejected': '-131.17', 'logps_train/chosen': '-150.95', 'loss/train': '0.68643', 'examples_per_second': '31.032', 'grad_norm': '29.625', 'counters/examples': 110400, 'counters/updates': 3450}
train stats after 110432 examples: {'rewards_train/chosen': '0.13642', 'rewards_train/rejected': '-0.013953', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15037', 'logps_train/rejected': '-131.33', 'logps_train/chosen': '-147.46', 'loss/train': '0.63822', 'examples_per_second': '31.26', 'grad_norm': '28.25', 'counters/examples': 110432, 'counters/updates': 3451}
train stats after 110464 examples: {'rewards_train/chosen': '0.2079', 'rewards_train/rejected': '0.089438', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11846', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-158.77', 'loss/train': '0.65638', 'examples_per_second': '32.406', 'grad_norm': '42.25', 'counters/examples': 110464, 'counters/updates': 3452}
skipping logging after 110496 examples to avoid logging too frequently
train stats after 110528 examples: {'rewards_train/chosen': '0.074891', 'rewards_train/rejected': '0.024575', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050316', 'logps_train/rejected': '-119.95', 'logps_train/chosen': '-159.33', 'loss/train': '0.67626', 'examples_per_second': '31.435', 'grad_norm': '39.75', 'counters/examples': 110528, 'counters/updates': 3454}
train stats after 110560 examples: {'rewards_train/chosen': '0.074317', 'rewards_train/rejected': '-0.0084793', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082796', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-140.45', 'loss/train': '0.66188', 'examples_per_second': '32.375', 'grad_norm': '52.75', 'counters/examples': 110560, 'counters/updates': 3455}
train stats after 110592 examples: {'rewards_train/chosen': '0.079354', 'rewards_train/rejected': '0.068062', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011291', 'logps_train/rejected': '-156.01', 'logps_train/chosen': '-151.49', 'loss/train': '0.70845', 'examples_per_second': '31.689', 'grad_norm': '76.5', 'counters/examples': 110592, 'counters/updates': 3456}
skipping logging after 110624 examples to avoid logging too frequently
train stats after 110656 examples: {'rewards_train/chosen': '0.13293', 'rewards_train/rejected': '0.081115', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.051816', 'logps_train/rejected': '-105.36', 'logps_train/chosen': '-146.59', 'loss/train': '0.67903', 'examples_per_second': '31.445', 'grad_norm': '54.25', 'counters/examples': 110656, 'counters/updates': 3458}
train stats after 110688 examples: {'rewards_train/chosen': '0.037529', 'rewards_train/rejected': '0.042261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0047317', 'logps_train/rejected': '-151.87', 'logps_train/chosen': '-168.02', 'loss/train': '0.71357', 'examples_per_second': '31.289', 'grad_norm': '33.5', 'counters/examples': 110688, 'counters/updates': 3459}
skipping logging after 110720 examples to avoid logging too frequently
train stats after 110752 examples: {'rewards_train/chosen': '0.04464', 'rewards_train/rejected': '-0.024529', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069169', 'logps_train/rejected': '-100.88', 'logps_train/chosen': '-100.17', 'loss/train': '0.66526', 'examples_per_second': '33.227', 'grad_norm': '32.5', 'counters/examples': 110752, 'counters/updates': 3461}
train stats after 110784 examples: {'rewards_train/chosen': '0.06547', 'rewards_train/rejected': '0.039094', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026376', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-155.44', 'loss/train': '0.68957', 'examples_per_second': '32.603', 'grad_norm': '28.75', 'counters/examples': 110784, 'counters/updates': 3462}
train stats after 110816 examples: {'rewards_train/chosen': '0.1073', 'rewards_train/rejected': '0.077635', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029665', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-169.51', 'loss/train': '0.68742', 'examples_per_second': '31.453', 'grad_norm': '31.625', 'counters/examples': 110816, 'counters/updates': 3463}
skipping logging after 110848 examples to avoid logging too frequently
train stats after 110880 examples: {'rewards_train/chosen': '0.024287', 'rewards_train/rejected': '0.032389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.0081019', 'logps_train/rejected': '-124.61', 'logps_train/chosen': '-146.9', 'loss/train': '0.73458', 'examples_per_second': '31.482', 'grad_norm': '39.5', 'counters/examples': 110880, 'counters/updates': 3465}
train stats after 110912 examples: {'rewards_train/chosen': '0.071474', 'rewards_train/rejected': '0.049756', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.021718', 'logps_train/rejected': '-128.15', 'logps_train/chosen': '-138.76', 'loss/train': '0.68719', 'examples_per_second': '31.32', 'grad_norm': '27.375', 'counters/examples': 110912, 'counters/updates': 3466}
train stats after 110944 examples: {'rewards_train/chosen': '0.10403', 'rewards_train/rejected': '0.029308', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074727', 'logps_train/rejected': '-142.92', 'logps_train/chosen': '-123.61', 'loss/train': '0.66516', 'examples_per_second': '31.457', 'grad_norm': '29', 'counters/examples': 110944, 'counters/updates': 3467}
train stats after 110976 examples: {'rewards_train/chosen': '0.20552', 'rewards_train/rejected': '-0.0077425', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21326', 'logps_train/rejected': '-121.15', 'logps_train/chosen': '-169.38', 'loss/train': '0.60746', 'examples_per_second': '32.931', 'grad_norm': '26.875', 'counters/examples': 110976, 'counters/updates': 3468}
train stats after 111008 examples: {'rewards_train/chosen': '0.17671', 'rewards_train/rejected': '0.064041', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11267', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-144.13', 'loss/train': '0.6666', 'examples_per_second': '32.33', 'grad_norm': '28', 'counters/examples': 111008, 'counters/updates': 3469}
train stats after 111040 examples: {'rewards_train/chosen': '0.023371', 'rewards_train/rejected': '0.042655', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.019284', 'logps_train/rejected': '-130.49', 'logps_train/chosen': '-151.39', 'loss/train': '0.7116', 'examples_per_second': '31.454', 'grad_norm': '44.75', 'counters/examples': 111040, 'counters/updates': 3470}
train stats after 111072 examples: {'rewards_train/chosen': '0.094046', 'rewards_train/rejected': '0.086611', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0074342', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-139.93', 'loss/train': '0.70283', 'examples_per_second': '30.028', 'grad_norm': '65', 'counters/examples': 111072, 'counters/updates': 3471}
skipping logging after 111104 examples to avoid logging too frequently
train stats after 111136 examples: {'rewards_train/chosen': '0.060724', 'rewards_train/rejected': '0.075677', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014953', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-134.25', 'loss/train': '0.70702', 'examples_per_second': '29.725', 'grad_norm': '33.5', 'counters/examples': 111136, 'counters/updates': 3473}
train stats after 111168 examples: {'rewards_train/chosen': '0.06756', 'rewards_train/rejected': '0.060849', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0067117', 'logps_train/rejected': '-116.56', 'logps_train/chosen': '-122.41', 'loss/train': '0.69404', 'examples_per_second': '31.357', 'grad_norm': '30.75', 'counters/examples': 111168, 'counters/updates': 3474}
train stats after 111200 examples: {'rewards_train/chosen': '0.094036', 'rewards_train/rejected': '0.015335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078701', 'logps_train/rejected': '-150.92', 'logps_train/chosen': '-213.06', 'loss/train': '0.65937', 'examples_per_second': '31.365', 'grad_norm': '38.25', 'counters/examples': 111200, 'counters/updates': 3475}
train stats after 111232 examples: {'rewards_train/chosen': '0.13104', 'rewards_train/rejected': '0.044588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086453', 'logps_train/rejected': '-114.03', 'logps_train/chosen': '-140.72', 'loss/train': '0.6609', 'examples_per_second': '30.675', 'grad_norm': '25.875', 'counters/examples': 111232, 'counters/updates': 3476}
skipping logging after 111264 examples to avoid logging too frequently
train stats after 111296 examples: {'rewards_train/chosen': '0.10689', 'rewards_train/rejected': '0.090061', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016834', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-122.81', 'loss/train': '0.69172', 'examples_per_second': '31.985', 'grad_norm': '32.25', 'counters/examples': 111296, 'counters/updates': 3478}
train stats after 111328 examples: {'rewards_train/chosen': '0.1352', 'rewards_train/rejected': '0.060265', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074935', 'logps_train/rejected': '-115.09', 'logps_train/chosen': '-168.36', 'loss/train': '0.66262', 'examples_per_second': '31.046', 'grad_norm': '28', 'counters/examples': 111328, 'counters/updates': 3479}
skipping logging after 111360 examples to avoid logging too frequently
train stats after 111392 examples: {'rewards_train/chosen': '0.12667', 'rewards_train/rejected': '0.10878', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017888', 'logps_train/rejected': '-164.22', 'logps_train/chosen': '-143.23', 'loss/train': '0.70406', 'examples_per_second': '31.403', 'grad_norm': '45.25', 'counters/examples': 111392, 'counters/updates': 3481}
train stats after 111424 examples: {'rewards_train/chosen': '0.13051', 'rewards_train/rejected': '0.035626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09488', 'logps_train/rejected': '-125.76', 'logps_train/chosen': '-171.31', 'loss/train': '0.66319', 'examples_per_second': '31.006', 'grad_norm': '29', 'counters/examples': 111424, 'counters/updates': 3482}
train stats after 111456 examples: {'rewards_train/chosen': '0.12743', 'rewards_train/rejected': '0.065081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062348', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-166.49', 'loss/train': '0.66927', 'examples_per_second': '30.525', 'grad_norm': '29.625', 'counters/examples': 111456, 'counters/updates': 3483}
skipping logging after 111488 examples to avoid logging too frequently
train stats after 111520 examples: {'rewards_train/chosen': '0.10392', 'rewards_train/rejected': '0.038959', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.064962', 'logps_train/rejected': '-91.143', 'logps_train/chosen': '-147.45', 'loss/train': '0.6676', 'examples_per_second': '31.45', 'grad_norm': '37', 'counters/examples': 111520, 'counters/updates': 3485}
train stats after 111552 examples: {'rewards_train/chosen': '0.067255', 'rewards_train/rejected': '-0.0077839', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.075039', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-122.11', 'loss/train': '0.6664', 'examples_per_second': '32.548', 'grad_norm': '26.75', 'counters/examples': 111552, 'counters/updates': 3486}
train stats after 111584 examples: {'rewards_train/chosen': '0.13519', 'rewards_train/rejected': '0.016566', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11862', 'logps_train/rejected': '-153.26', 'logps_train/chosen': '-128.63', 'loss/train': '0.65575', 'examples_per_second': '29.943', 'grad_norm': '30.625', 'counters/examples': 111584, 'counters/updates': 3487}
train stats after 111616 examples: {'rewards_train/chosen': '0.042549', 'rewards_train/rejected': '0.021252', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021297', 'logps_train/rejected': '-89.697', 'logps_train/chosen': '-159.92', 'loss/train': '0.69395', 'examples_per_second': '29.896', 'grad_norm': '46.75', 'counters/examples': 111616, 'counters/updates': 3488}
train stats after 111648 examples: {'rewards_train/chosen': '0.10748', 'rewards_train/rejected': '0.021309', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086167', 'logps_train/rejected': '-103.23', 'logps_train/chosen': '-108.98', 'loss/train': '0.65805', 'examples_per_second': '32.518', 'grad_norm': '30.375', 'counters/examples': 111648, 'counters/updates': 3489}
train stats after 111680 examples: {'rewards_train/chosen': '0.072829', 'rewards_train/rejected': '-0.027666', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1005', 'logps_train/rejected': '-86.596', 'logps_train/chosen': '-151.4', 'loss/train': '0.6514', 'examples_per_second': '31.832', 'grad_norm': '27.125', 'counters/examples': 111680, 'counters/updates': 3490}
train stats after 111712 examples: {'rewards_train/chosen': '0.11702', 'rewards_train/rejected': '0.067283', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049733', 'logps_train/rejected': '-109.26', 'logps_train/chosen': '-151.79', 'loss/train': '0.67553', 'examples_per_second': '29.979', 'grad_norm': '36.25', 'counters/examples': 111712, 'counters/updates': 3491}
train stats after 111744 examples: {'rewards_train/chosen': '0.20336', 'rewards_train/rejected': '0.018314', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18504', 'logps_train/rejected': '-102.99', 'logps_train/chosen': '-157.98', 'loss/train': '0.62996', 'examples_per_second': '30.155', 'grad_norm': '25.25', 'counters/examples': 111744, 'counters/updates': 3492}
train stats after 111776 examples: {'rewards_train/chosen': '0.084546', 'rewards_train/rejected': '0.02019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064356', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-119.48', 'loss/train': '0.66713', 'examples_per_second': '31.502', 'grad_norm': '26.375', 'counters/examples': 111776, 'counters/updates': 3493}
skipping logging after 111808 examples to avoid logging too frequently
train stats after 111840 examples: {'rewards_train/chosen': '0.13698', 'rewards_train/rejected': '0.036014', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10096', 'logps_train/rejected': '-114.27', 'logps_train/chosen': '-151.87', 'loss/train': '0.65883', 'examples_per_second': '30.18', 'grad_norm': '24.75', 'counters/examples': 111840, 'counters/updates': 3495}
train stats after 111872 examples: {'rewards_train/chosen': '0.12472', 'rewards_train/rejected': '0.079389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.045327', 'logps_train/rejected': '-89.378', 'logps_train/chosen': '-127.2', 'loss/train': '0.67674', 'examples_per_second': '31.119', 'grad_norm': '25.25', 'counters/examples': 111872, 'counters/updates': 3496}
train stats after 111904 examples: {'rewards_train/chosen': '0.058153', 'rewards_train/rejected': '0.086355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.028203', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-124.28', 'loss/train': '0.71356', 'examples_per_second': '32.108', 'grad_norm': '32.5', 'counters/examples': 111904, 'counters/updates': 3497}
train stats after 111936 examples: {'rewards_train/chosen': '0.11181', 'rewards_train/rejected': '0.091479', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020328', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-120.12', 'loss/train': '0.69111', 'examples_per_second': '31.437', 'grad_norm': '37.75', 'counters/examples': 111936, 'counters/updates': 3498}
train stats after 111968 examples: {'rewards_train/chosen': '0.096884', 'rewards_train/rejected': '0.00095959', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095925', 'logps_train/rejected': '-145.23', 'logps_train/chosen': '-152.68', 'loss/train': '0.65309', 'examples_per_second': '31.98', 'grad_norm': '34.5', 'counters/examples': 111968, 'counters/updates': 3499}
train stats after 112000 examples: {'rewards_train/chosen': '0.010578', 'rewards_train/rejected': '0.070155', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.059577', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-119.16', 'loss/train': '0.7294', 'examples_per_second': '30.298', 'grad_norm': '37', 'counters/examples': 112000, 'counters/updates': 3500}
train stats after 112032 examples: {'rewards_train/chosen': '0.24339', 'rewards_train/rejected': '0.075745', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16764', 'logps_train/rejected': '-137.17', 'logps_train/chosen': '-167.27', 'loss/train': '0.64214', 'examples_per_second': '31.487', 'grad_norm': '31.625', 'counters/examples': 112032, 'counters/updates': 3501}
train stats after 112064 examples: {'rewards_train/chosen': '0.020056', 'rewards_train/rejected': '0.035921', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015865', 'logps_train/rejected': '-164.16', 'logps_train/chosen': '-194.17', 'loss/train': '0.71159', 'examples_per_second': '30.138', 'grad_norm': '39.75', 'counters/examples': 112064, 'counters/updates': 3502}
train stats after 112096 examples: {'rewards_train/chosen': '0.1259', 'rewards_train/rejected': '0.11258', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013317', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-152.52', 'loss/train': '0.69847', 'examples_per_second': '30.804', 'grad_norm': '34.75', 'counters/examples': 112096, 'counters/updates': 3503}
train stats after 112128 examples: {'rewards_train/chosen': '0.18364', 'rewards_train/rejected': '0.048535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1351', 'logps_train/rejected': '-121.51', 'logps_train/chosen': '-158.43', 'loss/train': '0.63729', 'examples_per_second': '30.806', 'grad_norm': '32.25', 'counters/examples': 112128, 'counters/updates': 3504}
train stats after 112160 examples: {'rewards_train/chosen': '0.12568', 'rewards_train/rejected': '-0.014535', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14021', 'logps_train/rejected': '-148.92', 'logps_train/chosen': '-196.39', 'loss/train': '0.63253', 'examples_per_second': '31.072', 'grad_norm': '40.25', 'counters/examples': 112160, 'counters/updates': 3505}
train stats after 112192 examples: {'rewards_train/chosen': '0.11762', 'rewards_train/rejected': '0.097614', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020011', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-170.07', 'loss/train': '0.69229', 'examples_per_second': '31.589', 'grad_norm': '34.75', 'counters/examples': 112192, 'counters/updates': 3506}
train stats after 112224 examples: {'rewards_train/chosen': '0.10267', 'rewards_train/rejected': '0.018847', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083822', 'logps_train/rejected': '-127.97', 'logps_train/chosen': '-141.08', 'loss/train': '0.66388', 'examples_per_second': '31.468', 'grad_norm': '26.375', 'counters/examples': 112224, 'counters/updates': 3507}
train stats after 112256 examples: {'rewards_train/chosen': '0.085258', 'rewards_train/rejected': '0.052452', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032806', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-163.86', 'loss/train': '0.68348', 'examples_per_second': '33.048', 'grad_norm': '30.625', 'counters/examples': 112256, 'counters/updates': 3508}
train stats after 112288 examples: {'rewards_train/chosen': '0.12811', 'rewards_train/rejected': '0.053504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074611', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-115.78', 'loss/train': '0.6617', 'examples_per_second': '32.509', 'grad_norm': '37', 'counters/examples': 112288, 'counters/updates': 3509}
train stats after 112320 examples: {'rewards_train/chosen': '0.064256', 'rewards_train/rejected': '0.037025', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027231', 'logps_train/rejected': '-112.2', 'logps_train/chosen': '-133.55', 'loss/train': '0.68281', 'examples_per_second': '30.705', 'grad_norm': '26.875', 'counters/examples': 112320, 'counters/updates': 3510}
train stats after 112352 examples: {'rewards_train/chosen': '0.09681', 'rewards_train/rejected': '0.033334', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063476', 'logps_train/rejected': '-95.556', 'logps_train/chosen': '-112.35', 'loss/train': '0.66775', 'examples_per_second': '32.202', 'grad_norm': '28.625', 'counters/examples': 112352, 'counters/updates': 3511}
skipping logging after 112384 examples to avoid logging too frequently
train stats after 112416 examples: {'rewards_train/chosen': '0.15393', 'rewards_train/rejected': '0.14587', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0080652', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-165.44', 'loss/train': '0.70082', 'examples_per_second': '31.421', 'grad_norm': '44', 'counters/examples': 112416, 'counters/updates': 3513}
train stats after 112448 examples: {'rewards_train/chosen': '0.15247', 'rewards_train/rejected': '0.073354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079121', 'logps_train/rejected': '-158.61', 'logps_train/chosen': '-167.53', 'loss/train': '0.66688', 'examples_per_second': '31.415', 'grad_norm': '41.75', 'counters/examples': 112448, 'counters/updates': 3514}
train stats after 112480 examples: {'rewards_train/chosen': '0.13334', 'rewards_train/rejected': '0.040102', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093242', 'logps_train/rejected': '-104.05', 'logps_train/chosen': '-130.25', 'loss/train': '0.6529', 'examples_per_second': '32.92', 'grad_norm': '28.625', 'counters/examples': 112480, 'counters/updates': 3515}
train stats after 112512 examples: {'rewards_train/chosen': '0.12983', 'rewards_train/rejected': '0.033681', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096147', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-143.66', 'loss/train': '0.65222', 'examples_per_second': '29.944', 'grad_norm': '28.75', 'counters/examples': 112512, 'counters/updates': 3516}
train stats after 112544 examples: {'rewards_train/chosen': '0.10768', 'rewards_train/rejected': '0.020601', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.087082', 'logps_train/rejected': '-108.97', 'logps_train/chosen': '-147.12', 'loss/train': '0.65965', 'examples_per_second': '31.969', 'grad_norm': '33.5', 'counters/examples': 112544, 'counters/updates': 3517}
train stats after 112576 examples: {'rewards_train/chosen': '0.033732', 'rewards_train/rejected': '0.19309', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.15936', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-138.88', 'loss/train': '0.82202', 'examples_per_second': '31.434', 'grad_norm': '55.75', 'counters/examples': 112576, 'counters/updates': 3518}
train stats after 112608 examples: {'rewards_train/chosen': '0.11274', 'rewards_train/rejected': '0.06176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050984', 'logps_train/rejected': '-101.07', 'logps_train/chosen': '-144.66', 'loss/train': '0.67387', 'examples_per_second': '29.88', 'grad_norm': '32.5', 'counters/examples': 112608, 'counters/updates': 3519}
train stats after 112640 examples: {'rewards_train/chosen': '0.15592', 'rewards_train/rejected': '0.083596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072325', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-146.11', 'loss/train': '0.66938', 'examples_per_second': '31.416', 'grad_norm': '37.75', 'counters/examples': 112640, 'counters/updates': 3520}
train stats after 112672 examples: {'rewards_train/chosen': '0.098287', 'rewards_train/rejected': '0.050195', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048092', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-158.67', 'loss/train': '0.6762', 'examples_per_second': '30.331', 'grad_norm': '34.75', 'counters/examples': 112672, 'counters/updates': 3521}
skipping logging after 112704 examples to avoid logging too frequently
train stats after 112736 examples: {'rewards_train/chosen': '0.12984', 'rewards_train/rejected': '0.034897', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094942', 'logps_train/rejected': '-102.1', 'logps_train/chosen': '-146.9', 'loss/train': '0.65385', 'examples_per_second': '33.704', 'grad_norm': '30.625', 'counters/examples': 112736, 'counters/updates': 3523}
train stats after 112768 examples: {'rewards_train/chosen': '0.097874', 'rewards_train/rejected': '0.037376', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060498', 'logps_train/rejected': '-144.8', 'logps_train/chosen': '-118.96', 'loss/train': '0.66847', 'examples_per_second': '31.069', 'grad_norm': '32.75', 'counters/examples': 112768, 'counters/updates': 3524}
skipping logging after 112800 examples to avoid logging too frequently
train stats after 112832 examples: {'rewards_train/chosen': '0.10738', 'rewards_train/rejected': '0.055396', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.051985', 'logps_train/rejected': '-163.03', 'logps_train/chosen': '-153', 'loss/train': '0.68188', 'examples_per_second': '30.971', 'grad_norm': '35.75', 'counters/examples': 112832, 'counters/updates': 3526}
skipping logging after 112864 examples to avoid logging too frequently
train stats after 112896 examples: {'rewards_train/chosen': '0.16996', 'rewards_train/rejected': '0.047724', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12223', 'logps_train/rejected': '-131.26', 'logps_train/chosen': '-180.09', 'loss/train': '0.64169', 'examples_per_second': '31.716', 'grad_norm': '35.75', 'counters/examples': 112896, 'counters/updates': 3528}
skipping logging after 112928 examples to avoid logging too frequently
train stats after 112960 examples: {'rewards_train/chosen': '0.077161', 'rewards_train/rejected': '0.029633', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047528', 'logps_train/rejected': '-170.29', 'logps_train/chosen': '-145', 'loss/train': '0.67697', 'examples_per_second': '30.887', 'grad_norm': '35', 'counters/examples': 112960, 'counters/updates': 3530}
train stats after 112992 examples: {'rewards_train/chosen': '0.072969', 'rewards_train/rejected': '0.015458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057512', 'logps_train/rejected': '-135.67', 'logps_train/chosen': '-148.76', 'loss/train': '0.67143', 'examples_per_second': '30.044', 'grad_norm': '33', 'counters/examples': 112992, 'counters/updates': 3531}
train stats after 113024 examples: {'rewards_train/chosen': '0.13819', 'rewards_train/rejected': '0.0027174', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13547', 'logps_train/rejected': '-154.68', 'logps_train/chosen': '-206.51', 'loss/train': '0.64012', 'examples_per_second': '31.447', 'grad_norm': '32.75', 'counters/examples': 113024, 'counters/updates': 3532}
train stats after 113056 examples: {'rewards_train/chosen': '0.11705', 'rewards_train/rejected': '0.068686', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.048363', 'logps_train/rejected': '-103.9', 'logps_train/chosen': '-119.48', 'loss/train': '0.67694', 'examples_per_second': '30.014', 'grad_norm': '27', 'counters/examples': 113056, 'counters/updates': 3533}
train stats after 113088 examples: {'rewards_train/chosen': '0.028202', 'rewards_train/rejected': '-0.014208', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04241', 'logps_train/rejected': '-138.08', 'logps_train/chosen': '-163.34', 'loss/train': '0.68289', 'examples_per_second': '31.362', 'grad_norm': '43.5', 'counters/examples': 113088, 'counters/updates': 3534}
train stats after 113120 examples: {'rewards_train/chosen': '0.14185', 'rewards_train/rejected': '0.037931', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10392', 'logps_train/rejected': '-111.6', 'logps_train/chosen': '-140.35', 'loss/train': '0.65487', 'examples_per_second': '30.164', 'grad_norm': '37.5', 'counters/examples': 113120, 'counters/updates': 3535}
train stats after 113152 examples: {'rewards_train/chosen': '0.17293', 'rewards_train/rejected': '0.17491', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0019787', 'logps_train/rejected': '-152.18', 'logps_train/chosen': '-179.18', 'loss/train': '0.70283', 'examples_per_second': '31.362', 'grad_norm': '48.5', 'counters/examples': 113152, 'counters/updates': 3536}
train stats after 113184 examples: {'rewards_train/chosen': '0.10763', 'rewards_train/rejected': '0.0069923', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10064', 'logps_train/rejected': '-148.87', 'logps_train/chosen': '-149.92', 'loss/train': '0.65067', 'examples_per_second': '31.853', 'grad_norm': '30', 'counters/examples': 113184, 'counters/updates': 3537}
train stats after 113216 examples: {'rewards_train/chosen': '0.12885', 'rewards_train/rejected': '0.068452', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060398', 'logps_train/rejected': '-145.7', 'logps_train/chosen': '-174.49', 'loss/train': '0.68103', 'examples_per_second': '30.669', 'grad_norm': '32.5', 'counters/examples': 113216, 'counters/updates': 3538}
train stats after 113248 examples: {'rewards_train/chosen': '0.12312', 'rewards_train/rejected': '0.062624', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060491', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-127.48', 'loss/train': '0.67774', 'examples_per_second': '31.445', 'grad_norm': '35', 'counters/examples': 113248, 'counters/updates': 3539}
skipping logging after 113280 examples to avoid logging too frequently
train stats after 113312 examples: {'rewards_train/chosen': '0.10627', 'rewards_train/rejected': '0.0038139', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10246', 'logps_train/rejected': '-115.78', 'logps_train/chosen': '-152.3', 'loss/train': '0.65327', 'examples_per_second': '31.481', 'grad_norm': '34.5', 'counters/examples': 113312, 'counters/updates': 3541}
skipping logging after 113344 examples to avoid logging too frequently
train stats after 113376 examples: {'rewards_train/chosen': '0.18208', 'rewards_train/rejected': '0.10819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.073885', 'logps_train/rejected': '-100.87', 'logps_train/chosen': '-141.38', 'loss/train': '0.66349', 'examples_per_second': '31.272', 'grad_norm': '32.75', 'counters/examples': 113376, 'counters/updates': 3543}
train stats after 113408 examples: {'rewards_train/chosen': '0.18392', 'rewards_train/rejected': '0.083101', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10082', 'logps_train/rejected': '-108.78', 'logps_train/chosen': '-154.5', 'loss/train': '0.65525', 'examples_per_second': '30.022', 'grad_norm': '29.25', 'counters/examples': 113408, 'counters/updates': 3544}
train stats after 113440 examples: {'rewards_train/chosen': '0.12873', 'rewards_train/rejected': '0.050856', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077877', 'logps_train/rejected': '-101.62', 'logps_train/chosen': '-155.52', 'loss/train': '0.66626', 'examples_per_second': '30.073', 'grad_norm': '27.875', 'counters/examples': 113440, 'counters/updates': 3545}
train stats after 113472 examples: {'rewards_train/chosen': '0.22874', 'rewards_train/rejected': '0.16338', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065361', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-153.77', 'loss/train': '0.67734', 'examples_per_second': '30.395', 'grad_norm': '35.75', 'counters/examples': 113472, 'counters/updates': 3546}
train stats after 113504 examples: {'rewards_train/chosen': '0.085612', 'rewards_train/rejected': '0.0067481', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078864', 'logps_train/rejected': '-126.33', 'logps_train/chosen': '-139.34', 'loss/train': '0.66298', 'examples_per_second': '32.827', 'grad_norm': '26.25', 'counters/examples': 113504, 'counters/updates': 3547}
train stats after 113536 examples: {'rewards_train/chosen': '0.15301', 'rewards_train/rejected': '0.041341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11167', 'logps_train/rejected': '-148.46', 'logps_train/chosen': '-174.66', 'loss/train': '0.65524', 'examples_per_second': '31.201', 'grad_norm': '29.5', 'counters/examples': 113536, 'counters/updates': 3548}
train stats after 113568 examples: {'rewards_train/chosen': '0.13124', 'rewards_train/rejected': '0.066788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064455', 'logps_train/rejected': '-118.53', 'logps_train/chosen': '-116.93', 'loss/train': '0.66781', 'examples_per_second': '31.436', 'grad_norm': '27.625', 'counters/examples': 113568, 'counters/updates': 3549}
train stats after 113600 examples: {'rewards_train/chosen': '0.0867', 'rewards_train/rejected': '0.026895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059805', 'logps_train/rejected': '-127.83', 'logps_train/chosen': '-125.42', 'loss/train': '0.67012', 'examples_per_second': '31.316', 'grad_norm': '26.125', 'counters/examples': 113600, 'counters/updates': 3550}
train stats after 113632 examples: {'rewards_train/chosen': '0.1414', 'rewards_train/rejected': '0.020396', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.121', 'logps_train/rejected': '-146.32', 'logps_train/chosen': '-178.23', 'loss/train': '0.64071', 'examples_per_second': '23.992', 'grad_norm': '34', 'counters/examples': 113632, 'counters/updates': 3551}
train stats after 113664 examples: {'rewards_train/chosen': '0.095604', 'rewards_train/rejected': '0.016557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.079047', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-164.04', 'loss/train': '0.66355', 'examples_per_second': '31.787', 'grad_norm': '27.875', 'counters/examples': 113664, 'counters/updates': 3552}
train stats after 113696 examples: {'rewards_train/chosen': '0.096382', 'rewards_train/rejected': '0.017644', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.078738', 'logps_train/rejected': '-80.172', 'logps_train/chosen': '-115.81', 'loss/train': '0.65783', 'examples_per_second': '32.25', 'grad_norm': '24.5', 'counters/examples': 113696, 'counters/updates': 3553}
train stats after 113728 examples: {'rewards_train/chosen': '0.094432', 'rewards_train/rejected': '0.065561', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028871', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-124.44', 'loss/train': '0.68605', 'examples_per_second': '24.923', 'grad_norm': '30.5', 'counters/examples': 113728, 'counters/updates': 3554}
train stats after 113760 examples: {'rewards_train/chosen': '0.16633', 'rewards_train/rejected': '0.052251', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11408', 'logps_train/rejected': '-120.98', 'logps_train/chosen': '-126.41', 'loss/train': '0.6512', 'examples_per_second': '32.327', 'grad_norm': '34.25', 'counters/examples': 113760, 'counters/updates': 3555}
train stats after 113792 examples: {'rewards_train/chosen': '0.15857', 'rewards_train/rejected': '-0.036426', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19499', 'logps_train/rejected': '-101.85', 'logps_train/chosen': '-159.57', 'loss/train': '0.6133', 'examples_per_second': '31.456', 'grad_norm': '27.75', 'counters/examples': 113792, 'counters/updates': 3556}
skipping logging after 113824 examples to avoid logging too frequently
train stats after 113856 examples: {'rewards_train/chosen': '0.15246', 'rewards_train/rejected': '0.081065', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071395', 'logps_train/rejected': '-158.69', 'logps_train/chosen': '-162.26', 'loss/train': '0.66384', 'examples_per_second': '31.438', 'grad_norm': '33.75', 'counters/examples': 113856, 'counters/updates': 3558}
train stats after 113888 examples: {'rewards_train/chosen': '0.16369', 'rewards_train/rejected': '0.10777', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05592', 'logps_train/rejected': '-145.67', 'logps_train/chosen': '-162.73', 'loss/train': '0.68423', 'examples_per_second': '30.616', 'grad_norm': '33.5', 'counters/examples': 113888, 'counters/updates': 3559}
train stats after 113920 examples: {'rewards_train/chosen': '0.12529', 'rewards_train/rejected': '0.074919', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050367', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-145.26', 'loss/train': '0.67594', 'examples_per_second': '32.333', 'grad_norm': '59', 'counters/examples': 113920, 'counters/updates': 3560}
skipping logging after 113952 examples to avoid logging too frequently
train stats after 113984 examples: {'rewards_train/chosen': '0.10452', 'rewards_train/rejected': '0.07903', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.025491', 'logps_train/rejected': '-136.5', 'logps_train/chosen': '-129.76', 'loss/train': '0.68397', 'examples_per_second': '30.744', 'grad_norm': '32.5', 'counters/examples': 113984, 'counters/updates': 3562}
skipping logging after 114016 examples to avoid logging too frequently
train stats after 114048 examples: {'rewards_train/chosen': '0.069602', 'rewards_train/rejected': '-0.01468', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084282', 'logps_train/rejected': '-99.565', 'logps_train/chosen': '-110.71', 'loss/train': '0.65851', 'examples_per_second': '34.514', 'grad_norm': '22.875', 'counters/examples': 114048, 'counters/updates': 3564}
skipping logging after 114080 examples to avoid logging too frequently
train stats after 114112 examples: {'rewards_train/chosen': '0.19466', 'rewards_train/rejected': '0.086977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10769', 'logps_train/rejected': '-127.03', 'logps_train/chosen': '-129.36', 'loss/train': '0.66107', 'examples_per_second': '32.385', 'grad_norm': '28.5', 'counters/examples': 114112, 'counters/updates': 3566}
skipping logging after 114144 examples to avoid logging too frequently
train stats after 114176 examples: {'rewards_train/chosen': '0.10219', 'rewards_train/rejected': '0.064521', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.03767', 'logps_train/rejected': '-125.36', 'logps_train/chosen': '-150.67', 'loss/train': '0.67802', 'examples_per_second': '34.246', 'grad_norm': '30.5', 'counters/examples': 114176, 'counters/updates': 3568}
train stats after 114208 examples: {'rewards_train/chosen': '0.09962', 'rewards_train/rejected': '0.078027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.021593', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-104.63', 'loss/train': '0.68612', 'examples_per_second': '32.306', 'grad_norm': '25.875', 'counters/examples': 114208, 'counters/updates': 3569}
train stats after 114240 examples: {'rewards_train/chosen': '0.085102', 'rewards_train/rejected': '0.087952', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028502', 'logps_train/rejected': '-162.48', 'logps_train/chosen': '-147.27', 'loss/train': '0.71145', 'examples_per_second': '29.943', 'grad_norm': '44.5', 'counters/examples': 114240, 'counters/updates': 3570}
train stats after 114272 examples: {'rewards_train/chosen': '0.023228', 'rewards_train/rejected': '0.00036141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022866', 'logps_train/rejected': '-123.05', 'logps_train/chosen': '-128.18', 'loss/train': '0.69142', 'examples_per_second': '30.785', 'grad_norm': '33.25', 'counters/examples': 114272, 'counters/updates': 3571}
train stats after 114304 examples: {'rewards_train/chosen': '0.07842', 'rewards_train/rejected': '0.0058206', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.072599', 'logps_train/rejected': '-99.326', 'logps_train/chosen': '-123.38', 'loss/train': '0.66706', 'examples_per_second': '29.828', 'grad_norm': '24.25', 'counters/examples': 114304, 'counters/updates': 3572}
train stats after 114336 examples: {'rewards_train/chosen': '0.15114', 'rewards_train/rejected': '0.053755', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097382', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-152.91', 'loss/train': '0.65481', 'examples_per_second': '31.447', 'grad_norm': '33.5', 'counters/examples': 114336, 'counters/updates': 3573}
train stats after 114368 examples: {'rewards_train/chosen': '0.11037', 'rewards_train/rejected': '0.085041', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025333', 'logps_train/rejected': '-139.68', 'logps_train/chosen': '-152.73', 'loss/train': '0.69186', 'examples_per_second': '31.416', 'grad_norm': '32.25', 'counters/examples': 114368, 'counters/updates': 3574}
train stats after 114400 examples: {'rewards_train/chosen': '0.076312', 'rewards_train/rejected': '0.081273', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0049616', 'logps_train/rejected': '-111.01', 'logps_train/chosen': '-135.03', 'loss/train': '0.70446', 'examples_per_second': '32.497', 'grad_norm': '27.375', 'counters/examples': 114400, 'counters/updates': 3575}
train stats after 114432 examples: {'rewards_train/chosen': '0.05658', 'rewards_train/rejected': '-0.012754', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069334', 'logps_train/rejected': '-115.54', 'logps_train/chosen': '-152.05', 'loss/train': '0.67412', 'examples_per_second': '31.441', 'grad_norm': '27.875', 'counters/examples': 114432, 'counters/updates': 3576}
train stats after 114464 examples: {'rewards_train/chosen': '0.070751', 'rewards_train/rejected': '0.057709', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013042', 'logps_train/rejected': '-156.15', 'logps_train/chosen': '-134.45', 'loss/train': '0.69895', 'examples_per_second': '29.933', 'grad_norm': '28.75', 'counters/examples': 114464, 'counters/updates': 3577}
train stats after 114496 examples: {'rewards_train/chosen': '0.084847', 'rewards_train/rejected': '0.038968', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.045879', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-161.88', 'loss/train': '0.68843', 'examples_per_second': '30.559', 'grad_norm': '42.5', 'counters/examples': 114496, 'counters/updates': 3578}
skipping logging after 114528 examples to avoid logging too frequently
train stats after 114560 examples: {'rewards_train/chosen': '0.18371', 'rewards_train/rejected': '0.094151', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089562', 'logps_train/rejected': '-145.23', 'logps_train/chosen': '-204.64', 'loss/train': '0.67042', 'examples_per_second': '31.27', 'grad_norm': '41.75', 'counters/examples': 114560, 'counters/updates': 3580}
train stats after 114592 examples: {'rewards_train/chosen': '0.1386', 'rewards_train/rejected': '0.047644', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090956', 'logps_train/rejected': '-107.03', 'logps_train/chosen': '-149.8', 'loss/train': '0.6577', 'examples_per_second': '31.559', 'grad_norm': '26.125', 'counters/examples': 114592, 'counters/updates': 3581}
skipping logging after 114624 examples to avoid logging too frequently
train stats after 114656 examples: {'rewards_train/chosen': '0.12892', 'rewards_train/rejected': '0.057121', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0718', 'logps_train/rejected': '-115.18', 'logps_train/chosen': '-125.3', 'loss/train': '0.67265', 'examples_per_second': '32.439', 'grad_norm': '30.625', 'counters/examples': 114656, 'counters/updates': 3583}
train stats after 114688 examples: {'rewards_train/chosen': '0.13392', 'rewards_train/rejected': '0.081614', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052311', 'logps_train/rejected': '-146.05', 'logps_train/chosen': '-151.63', 'loss/train': '0.67469', 'examples_per_second': '31.231', 'grad_norm': '53.25', 'counters/examples': 114688, 'counters/updates': 3584}
train stats after 114720 examples: {'rewards_train/chosen': '0.1353', 'rewards_train/rejected': '-0.012044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14734', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-146.19', 'loss/train': '0.63411', 'examples_per_second': '31.373', 'grad_norm': '29.5', 'counters/examples': 114720, 'counters/updates': 3585}
train stats after 114752 examples: {'rewards_train/chosen': '0.083692', 'rewards_train/rejected': '0.020457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063235', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-157.06', 'loss/train': '0.67038', 'examples_per_second': '31.513', 'grad_norm': '37.5', 'counters/examples': 114752, 'counters/updates': 3586}
train stats after 114784 examples: {'rewards_train/chosen': '0.10735', 'rewards_train/rejected': '0.026866', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080487', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-153.76', 'loss/train': '0.66419', 'examples_per_second': '31.365', 'grad_norm': '29', 'counters/examples': 114784, 'counters/updates': 3587}
train stats after 114816 examples: {'rewards_train/chosen': '0.15481', 'rewards_train/rejected': '0.083358', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071456', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-188.99', 'loss/train': '0.67459', 'examples_per_second': '31.168', 'grad_norm': '39.25', 'counters/examples': 114816, 'counters/updates': 3588}
skipping logging after 114848 examples to avoid logging too frequently
train stats after 114880 examples: {'rewards_train/chosen': '0.17179', 'rewards_train/rejected': '0.034563', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13722', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-135.9', 'loss/train': '0.63719', 'examples_per_second': '32.18', 'grad_norm': '31.625', 'counters/examples': 114880, 'counters/updates': 3590}
train stats after 114912 examples: {'rewards_train/chosen': '0.11102', 'rewards_train/rejected': '0.007653', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10337', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-175.05', 'loss/train': '0.65614', 'examples_per_second': '29.953', 'grad_norm': '28.625', 'counters/examples': 114912, 'counters/updates': 3591}
train stats after 114944 examples: {'rewards_train/chosen': '0.13502', 'rewards_train/rejected': '0.0045636', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13046', 'logps_train/rejected': '-107.56', 'logps_train/chosen': '-162.41', 'loss/train': '0.64206', 'examples_per_second': '32.351', 'grad_norm': '30.125', 'counters/examples': 114944, 'counters/updates': 3592}
train stats after 114976 examples: {'rewards_train/chosen': '0.064081', 'rewards_train/rejected': '0.0069449', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.057137', 'logps_train/rejected': '-111.26', 'logps_train/chosen': '-123.41', 'loss/train': '0.66932', 'examples_per_second': '30.995', 'grad_norm': '25', 'counters/examples': 114976, 'counters/updates': 3593}
train stats after 115008 examples: {'rewards_train/chosen': '0.049551', 'rewards_train/rejected': '0.026124', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.023427', 'logps_train/rejected': '-116.66', 'logps_train/chosen': '-125.5', 'loss/train': '0.68698', 'examples_per_second': '30.993', 'grad_norm': '27.75', 'counters/examples': 115008, 'counters/updates': 3594}
skipping logging after 115040 examples to avoid logging too frequently
train stats after 115072 examples: {'rewards_train/chosen': '0.14321', 'rewards_train/rejected': '-8.657e-05', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1433', 'logps_train/rejected': '-132.19', 'logps_train/chosen': '-156.03', 'loss/train': '0.64581', 'examples_per_second': '31.18', 'grad_norm': '27.625', 'counters/examples': 115072, 'counters/updates': 3596}
train stats after 115104 examples: {'rewards_train/chosen': '0.12427', 'rewards_train/rejected': '0.052759', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071511', 'logps_train/rejected': '-111.23', 'logps_train/chosen': '-112.02', 'loss/train': '0.66545', 'examples_per_second': '30.471', 'grad_norm': '32.25', 'counters/examples': 115104, 'counters/updates': 3597}
skipping logging after 115136 examples to avoid logging too frequently
train stats after 115168 examples: {'rewards_train/chosen': '0.068666', 'rewards_train/rejected': '0.0018205', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066845', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-141.96', 'loss/train': '0.6662', 'examples_per_second': '31.732', 'grad_norm': '27.875', 'counters/examples': 115168, 'counters/updates': 3599}
skipping logging after 115200 examples to avoid logging too frequently
train stats after 115232 examples: {'rewards_train/chosen': '0.03098', 'rewards_train/rejected': '0.024037', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '0.0069422', 'logps_train/rejected': '-99.456', 'logps_train/chosen': '-119.27', 'loss/train': '0.69867', 'examples_per_second': '34.737', 'grad_norm': '24.375', 'counters/examples': 115232, 'counters/updates': 3601}
train stats after 115264 examples: {'rewards_train/chosen': '0.093243', 'rewards_train/rejected': '-0.040924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13417', 'logps_train/rejected': '-131.62', 'logps_train/chosen': '-163.36', 'loss/train': '0.64602', 'examples_per_second': '33.086', 'grad_norm': '33.25', 'counters/examples': 115264, 'counters/updates': 3602}
train stats after 115296 examples: {'rewards_train/chosen': '0.15752', 'rewards_train/rejected': '0.085078', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072442', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-151.55', 'loss/train': '0.66814', 'examples_per_second': '31.315', 'grad_norm': '33.25', 'counters/examples': 115296, 'counters/updates': 3603}
train stats after 115328 examples: {'rewards_train/chosen': '0.053435', 'rewards_train/rejected': '-0.037782', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091217', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-104.79', 'loss/train': '0.65565', 'examples_per_second': '31.449', 'grad_norm': '24.25', 'counters/examples': 115328, 'counters/updates': 3604}
train stats after 115360 examples: {'rewards_train/chosen': '0.050411', 'rewards_train/rejected': '-0.0044875', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054898', 'logps_train/rejected': '-115.17', 'logps_train/chosen': '-169.42', 'loss/train': '0.67171', 'examples_per_second': '32.502', 'grad_norm': '28.5', 'counters/examples': 115360, 'counters/updates': 3605}
skipping logging after 115392 examples to avoid logging too frequently
train stats after 115424 examples: {'rewards_train/chosen': '0.09013', 'rewards_train/rejected': '0.058817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.031312', 'logps_train/rejected': '-129', 'logps_train/chosen': '-142.59', 'loss/train': '0.68384', 'examples_per_second': '29.976', 'grad_norm': '58.75', 'counters/examples': 115424, 'counters/updates': 3607}
train stats after 115456 examples: {'rewards_train/chosen': '0.17511', 'rewards_train/rejected': '0.088146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086965', 'logps_train/rejected': '-119.82', 'logps_train/chosen': '-153.42', 'loss/train': '0.68203', 'examples_per_second': '30.314', 'grad_norm': '42.5', 'counters/examples': 115456, 'counters/updates': 3608}
skipping logging after 115488 examples to avoid logging too frequently
train stats after 115520 examples: {'rewards_train/chosen': '0.13196', 'rewards_train/rejected': '0.088344', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04362', 'logps_train/rejected': '-150.53', 'logps_train/chosen': '-174.09', 'loss/train': '0.6882', 'examples_per_second': '30.151', 'grad_norm': '34.5', 'counters/examples': 115520, 'counters/updates': 3610}
train stats after 115552 examples: {'rewards_train/chosen': '0.19048', 'rewards_train/rejected': '0.039145', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15133', 'logps_train/rejected': '-115.57', 'logps_train/chosen': '-164.23', 'loss/train': '0.63219', 'examples_per_second': '32.316', 'grad_norm': '34', 'counters/examples': 115552, 'counters/updates': 3611}
skipping logging after 115584 examples to avoid logging too frequently
train stats after 115616 examples: {'rewards_train/chosen': '0.04911', 'rewards_train/rejected': '0.020952', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028158', 'logps_train/rejected': '-141.63', 'logps_train/chosen': '-138.83', 'loss/train': '0.69084', 'examples_per_second': '31.424', 'grad_norm': '50.25', 'counters/examples': 115616, 'counters/updates': 3613}
train stats after 115648 examples: {'rewards_train/chosen': '0.078221', 'rewards_train/rejected': '0.083522', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0053008', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-123.85', 'loss/train': '0.71481', 'examples_per_second': '31.248', 'grad_norm': '33.75', 'counters/examples': 115648, 'counters/updates': 3614}
train stats after 115680 examples: {'rewards_train/chosen': '0.18826', 'rewards_train/rejected': '0.11837', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069896', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-161.8', 'loss/train': '0.67238', 'examples_per_second': '30.971', 'grad_norm': '28.25', 'counters/examples': 115680, 'counters/updates': 3615}
train stats after 115712 examples: {'rewards_train/chosen': '0.15636', 'rewards_train/rejected': '0.050622', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10573', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-143.27', 'loss/train': '0.65421', 'examples_per_second': '29.899', 'grad_norm': '25.5', 'counters/examples': 115712, 'counters/updates': 3616}
train stats after 115744 examples: {'rewards_train/chosen': '0.091393', 'rewards_train/rejected': '0.11632', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.024922', 'logps_train/rejected': '-114.28', 'logps_train/chosen': '-108.17', 'loss/train': '0.71457', 'examples_per_second': '30.491', 'grad_norm': '49.25', 'counters/examples': 115744, 'counters/updates': 3617}
train stats after 115776 examples: {'rewards_train/chosen': '0.13647', 'rewards_train/rejected': '0.016678', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11979', 'logps_train/rejected': '-148.41', 'logps_train/chosen': '-199.53', 'loss/train': '0.65279', 'examples_per_second': '31.451', 'grad_norm': '43.25', 'counters/examples': 115776, 'counters/updates': 3618}
train stats after 115808 examples: {'rewards_train/chosen': '0.18411', 'rewards_train/rejected': '0.013413', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1707', 'logps_train/rejected': '-168.41', 'logps_train/chosen': '-159.53', 'loss/train': '0.61764', 'examples_per_second': '24.073', 'grad_norm': '29.625', 'counters/examples': 115808, 'counters/updates': 3619}
train stats after 115840 examples: {'rewards_train/chosen': '0.13307', 'rewards_train/rejected': '0.039582', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.09349', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-138.1', 'loss/train': '0.65182', 'examples_per_second': '32.106', 'grad_norm': '39.75', 'counters/examples': 115840, 'counters/updates': 3620}
skipping logging after 115872 examples to avoid logging too frequently
train stats after 115904 examples: {'rewards_train/chosen': '0.18413', 'rewards_train/rejected': '0.080795', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10334', 'logps_train/rejected': '-118.43', 'logps_train/chosen': '-180.02', 'loss/train': '0.65217', 'examples_per_second': '31.474', 'grad_norm': '32.75', 'counters/examples': 115904, 'counters/updates': 3622}
skipping logging after 115936 examples to avoid logging too frequently
train stats after 115968 examples: {'rewards_train/chosen': '0.15937', 'rewards_train/rejected': '0.031034', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12833', 'logps_train/rejected': '-112.49', 'logps_train/chosen': '-158.63', 'loss/train': '0.6483', 'examples_per_second': '29.971', 'grad_norm': '28.5', 'counters/examples': 115968, 'counters/updates': 3624}
train stats after 116000 examples: {'rewards_train/chosen': '0.16941', 'rewards_train/rejected': '0.026559', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14285', 'logps_train/rejected': '-122.29', 'logps_train/chosen': '-145.72', 'loss/train': '0.63494', 'examples_per_second': '30.682', 'grad_norm': '28.5', 'counters/examples': 116000, 'counters/updates': 3625}
train stats after 116032 examples: {'rewards_train/chosen': '0.084485', 'rewards_train/rejected': '0.052548', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031937', 'logps_train/rejected': '-132.46', 'logps_train/chosen': '-124.43', 'loss/train': '0.68836', 'examples_per_second': '30.464', 'grad_norm': '42.5', 'counters/examples': 116032, 'counters/updates': 3626}
train stats after 116064 examples: {'rewards_train/chosen': '0.098494', 'rewards_train/rejected': '0.037531', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060962', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-126.26', 'loss/train': '0.67049', 'examples_per_second': '31.544', 'grad_norm': '28.625', 'counters/examples': 116064, 'counters/updates': 3627}
train stats after 116096 examples: {'rewards_train/chosen': '0.1543', 'rewards_train/rejected': '0.1003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053992', 'logps_train/rejected': '-152.02', 'logps_train/chosen': '-157.79', 'loss/train': '0.67437', 'examples_per_second': '31.508', 'grad_norm': '37.5', 'counters/examples': 116096, 'counters/updates': 3628}
skipping logging after 116128 examples to avoid logging too frequently
train stats after 116160 examples: {'rewards_train/chosen': '0.17778', 'rewards_train/rejected': '0.070424', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10736', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-128.25', 'loss/train': '0.65121', 'examples_per_second': '30.459', 'grad_norm': '31.875', 'counters/examples': 116160, 'counters/updates': 3630}
train stats after 116192 examples: {'rewards_train/chosen': '0.31159', 'rewards_train/rejected': '0.049548', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.26204', 'logps_train/rejected': '-150.18', 'logps_train/chosen': '-134.72', 'loss/train': '0.58353', 'examples_per_second': '29.51', 'grad_norm': '31.625', 'counters/examples': 116192, 'counters/updates': 3631}
train stats after 116224 examples: {'rewards_train/chosen': '0.14663', 'rewards_train/rejected': '0.11764', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.028991', 'logps_train/rejected': '-113.78', 'logps_train/chosen': '-157.95', 'loss/train': '0.6966', 'examples_per_second': '30.556', 'grad_norm': '49', 'counters/examples': 116224, 'counters/updates': 3632}
skipping logging after 116256 examples to avoid logging too frequently
train stats after 116288 examples: {'rewards_train/chosen': '0.070161', 'rewards_train/rejected': '0.033952', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036209', 'logps_train/rejected': '-141.23', 'logps_train/chosen': '-144.15', 'loss/train': '0.68307', 'examples_per_second': '29.961', 'grad_norm': '28.5', 'counters/examples': 116288, 'counters/updates': 3634}
train stats after 116320 examples: {'rewards_train/chosen': '0.093211', 'rewards_train/rejected': '0.043553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049658', 'logps_train/rejected': '-128.64', 'logps_train/chosen': '-146.25', 'loss/train': '0.67699', 'examples_per_second': '31.495', 'grad_norm': '33.5', 'counters/examples': 116320, 'counters/updates': 3635}
train stats after 116352 examples: {'rewards_train/chosen': '0.10237', 'rewards_train/rejected': '0.020824', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081542', 'logps_train/rejected': '-100.98', 'logps_train/chosen': '-127.31', 'loss/train': '0.66134', 'examples_per_second': '32.022', 'grad_norm': '23', 'counters/examples': 116352, 'counters/updates': 3636}
train stats after 116384 examples: {'rewards_train/chosen': '0.12003', 'rewards_train/rejected': '0.062889', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05714', 'logps_train/rejected': '-157.53', 'logps_train/chosen': '-131.94', 'loss/train': '0.67317', 'examples_per_second': '29.92', 'grad_norm': '38.75', 'counters/examples': 116384, 'counters/updates': 3637}
train stats after 116416 examples: {'rewards_train/chosen': '0.26111', 'rewards_train/rejected': '0.079916', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1812', 'logps_train/rejected': '-138.21', 'logps_train/chosen': '-169.07', 'loss/train': '0.61936', 'examples_per_second': '31.406', 'grad_norm': '32.25', 'counters/examples': 116416, 'counters/updates': 3638}
train stats after 116448 examples: {'rewards_train/chosen': '0.053037', 'rewards_train/rejected': '0.013715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039322', 'logps_train/rejected': '-128.52', 'logps_train/chosen': '-132.21', 'loss/train': '0.67761', 'examples_per_second': '31.745', 'grad_norm': '32.5', 'counters/examples': 116448, 'counters/updates': 3639}
train stats after 116480 examples: {'rewards_train/chosen': '0.12592', 'rewards_train/rejected': '0.095542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030376', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-146.49', 'loss/train': '0.69143', 'examples_per_second': '30.417', 'grad_norm': '35.75', 'counters/examples': 116480, 'counters/updates': 3640}
skipping logging after 116512 examples to avoid logging too frequently
train stats after 116544 examples: {'rewards_train/chosen': '0.157', 'rewards_train/rejected': '0.014269', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14273', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-162.91', 'loss/train': '0.63148', 'examples_per_second': '29.924', 'grad_norm': '33', 'counters/examples': 116544, 'counters/updates': 3642}
train stats after 116576 examples: {'rewards_train/chosen': '0.10145', 'rewards_train/rejected': '0.051381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05007', 'logps_train/rejected': '-102.35', 'logps_train/chosen': '-124.19', 'loss/train': '0.67688', 'examples_per_second': '31.393', 'grad_norm': '27.375', 'counters/examples': 116576, 'counters/updates': 3643}
train stats after 116608 examples: {'rewards_train/chosen': '0.077462', 'rewards_train/rejected': '-0.013734', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091196', 'logps_train/rejected': '-109.25', 'logps_train/chosen': '-119.62', 'loss/train': '0.65935', 'examples_per_second': '31.693', 'grad_norm': '24.75', 'counters/examples': 116608, 'counters/updates': 3644}
train stats after 116640 examples: {'rewards_train/chosen': '0.14018', 'rewards_train/rejected': '0.042105', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098079', 'logps_train/rejected': '-113.75', 'logps_train/chosen': '-146.56', 'loss/train': '0.65208', 'examples_per_second': '31.175', 'grad_norm': '41', 'counters/examples': 116640, 'counters/updates': 3645}
train stats after 116672 examples: {'rewards_train/chosen': '0.062447', 'rewards_train/rejected': '0.038879', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023568', 'logps_train/rejected': '-102.5', 'logps_train/chosen': '-118.48', 'loss/train': '0.69018', 'examples_per_second': '32.347', 'grad_norm': '27.5', 'counters/examples': 116672, 'counters/updates': 3646}
train stats after 116704 examples: {'rewards_train/chosen': '0.094472', 'rewards_train/rejected': '0.073442', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02103', 'logps_train/rejected': '-115.42', 'logps_train/chosen': '-140.56', 'loss/train': '0.69489', 'examples_per_second': '33.329', 'grad_norm': '42', 'counters/examples': 116704, 'counters/updates': 3647}
train stats after 116736 examples: {'rewards_train/chosen': '0.1527', 'rewards_train/rejected': '0.078587', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074109', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-143.85', 'loss/train': '0.67112', 'examples_per_second': '30.799', 'grad_norm': '32.5', 'counters/examples': 116736, 'counters/updates': 3648}
train stats after 116768 examples: {'rewards_train/chosen': '0.17788', 'rewards_train/rejected': '0.067028', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11085', 'logps_train/rejected': '-150.94', 'logps_train/chosen': '-117.15', 'loss/train': '0.64429', 'examples_per_second': '31.439', 'grad_norm': '29.125', 'counters/examples': 116768, 'counters/updates': 3649}
skipping logging after 116800 examples to avoid logging too frequently
train stats after 116832 examples: {'rewards_train/chosen': '0.15461', 'rewards_train/rejected': '0.074896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079718', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-162.41', 'loss/train': '0.66506', 'examples_per_second': '30.784', 'grad_norm': '37.75', 'counters/examples': 116832, 'counters/updates': 3651}
train stats after 116864 examples: {'rewards_train/chosen': '0.15912', 'rewards_train/rejected': '0.077405', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081713', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-104.67', 'loss/train': '0.67022', 'examples_per_second': '31.453', 'grad_norm': '35.75', 'counters/examples': 116864, 'counters/updates': 3652}
train stats after 116896 examples: {'rewards_train/chosen': '0.15603', 'rewards_train/rejected': '0.0012922', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15474', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-141.33', 'loss/train': '0.62895', 'examples_per_second': '29.762', 'grad_norm': '40.25', 'counters/examples': 116896, 'counters/updates': 3653}
train stats after 116928 examples: {'rewards_train/chosen': '0.11741', 'rewards_train/rejected': '0.047128', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070283', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-141.39', 'loss/train': '0.66677', 'examples_per_second': '31.462', 'grad_norm': '30.125', 'counters/examples': 116928, 'counters/updates': 3654}
train stats after 116960 examples: {'rewards_train/chosen': '0.13084', 'rewards_train/rejected': '0.087161', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.043674', 'logps_train/rejected': '-159.4', 'logps_train/chosen': '-135.43', 'loss/train': '0.68781', 'examples_per_second': '31.466', 'grad_norm': '47.25', 'counters/examples': 116960, 'counters/updates': 3655}
train stats after 116992 examples: {'rewards_train/chosen': '0.063137', 'rewards_train/rejected': '-0.0029464', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066083', 'logps_train/rejected': '-127.88', 'logps_train/chosen': '-174.43', 'loss/train': '0.67475', 'examples_per_second': '31.423', 'grad_norm': '36.25', 'counters/examples': 116992, 'counters/updates': 3656}
train stats after 117024 examples: {'rewards_train/chosen': '0.083854', 'rewards_train/rejected': '0.049869', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033985', 'logps_train/rejected': '-104.97', 'logps_train/chosen': '-149.36', 'loss/train': '0.68856', 'examples_per_second': '31.924', 'grad_norm': '30.125', 'counters/examples': 117024, 'counters/updates': 3657}
train stats after 117056 examples: {'rewards_train/chosen': '0.066218', 'rewards_train/rejected': '-0.0081602', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074378', 'logps_train/rejected': '-103.78', 'logps_train/chosen': '-139.96', 'loss/train': '0.66215', 'examples_per_second': '31.436', 'grad_norm': '27.125', 'counters/examples': 117056, 'counters/updates': 3658}
train stats after 117088 examples: {'rewards_train/chosen': '0.021382', 'rewards_train/rejected': '0.078268', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.056886', 'logps_train/rejected': '-108.45', 'logps_train/chosen': '-122.13', 'loss/train': '0.73083', 'examples_per_second': '30.768', 'grad_norm': '29.75', 'counters/examples': 117088, 'counters/updates': 3659}
train stats after 117120 examples: {'rewards_train/chosen': '0.079711', 'rewards_train/rejected': '0.066302', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013409', 'logps_train/rejected': '-146.73', 'logps_train/chosen': '-164.73', 'loss/train': '0.69682', 'examples_per_second': '32.704', 'grad_norm': '37.5', 'counters/examples': 117120, 'counters/updates': 3660}
skipping logging after 117152 examples to avoid logging too frequently
train stats after 117184 examples: {'rewards_train/chosen': '0.13171', 'rewards_train/rejected': '0.022415', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10929', 'logps_train/rejected': '-121.46', 'logps_train/chosen': '-118.27', 'loss/train': '0.64857', 'examples_per_second': '30.67', 'grad_norm': '23.75', 'counters/examples': 117184, 'counters/updates': 3662}
train stats after 117216 examples: {'rewards_train/chosen': '0.090021', 'rewards_train/rejected': '0.039391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05063', 'logps_train/rejected': '-116.88', 'logps_train/chosen': '-144.62', 'loss/train': '0.67387', 'examples_per_second': '32.035', 'grad_norm': '35', 'counters/examples': 117216, 'counters/updates': 3663}
train stats after 117248 examples: {'rewards_train/chosen': '0.085126', 'rewards_train/rejected': '-0.014084', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09921', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-118.72', 'loss/train': '0.64934', 'examples_per_second': '30.174', 'grad_norm': '26.75', 'counters/examples': 117248, 'counters/updates': 3664}
train stats after 117280 examples: {'rewards_train/chosen': '0.24234', 'rewards_train/rejected': '0.13361', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10873', 'logps_train/rejected': '-148.13', 'logps_train/chosen': '-169.73', 'loss/train': '0.66989', 'examples_per_second': '30.176', 'grad_norm': '37', 'counters/examples': 117280, 'counters/updates': 3665}
train stats after 117312 examples: {'rewards_train/chosen': '0.10493', 'rewards_train/rejected': '-0.029652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13458', 'logps_train/rejected': '-121.6', 'logps_train/chosen': '-138.79', 'loss/train': '0.65201', 'examples_per_second': '31.339', 'grad_norm': '28.125', 'counters/examples': 117312, 'counters/updates': 3666}
train stats after 117344 examples: {'rewards_train/chosen': '0.22425', 'rewards_train/rejected': '0.034382', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18987', 'logps_train/rejected': '-102.64', 'logps_train/chosen': '-141.87', 'loss/train': '0.62447', 'examples_per_second': '30.158', 'grad_norm': '27.875', 'counters/examples': 117344, 'counters/updates': 3667}
train stats after 117376 examples: {'rewards_train/chosen': '0.11351', 'rewards_train/rejected': '0.037945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075569', 'logps_train/rejected': '-127.32', 'logps_train/chosen': '-178.55', 'loss/train': '0.66795', 'examples_per_second': '32.111', 'grad_norm': '30.75', 'counters/examples': 117376, 'counters/updates': 3668}
skipping logging after 117408 examples to avoid logging too frequently
train stats after 117440 examples: {'rewards_train/chosen': '0.10909', 'rewards_train/rejected': '0.013998', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095092', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-118.99', 'loss/train': '0.65658', 'examples_per_second': '31.347', 'grad_norm': '30', 'counters/examples': 117440, 'counters/updates': 3670}
train stats after 117472 examples: {'rewards_train/chosen': '0.054623', 'rewards_train/rejected': '0.033468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.021155', 'logps_train/rejected': '-152.06', 'logps_train/chosen': '-160.22', 'loss/train': '0.69268', 'examples_per_second': '31.464', 'grad_norm': '42.75', 'counters/examples': 117472, 'counters/updates': 3671}
skipping logging after 117504 examples to avoid logging too frequently
train stats after 117536 examples: {'rewards_train/chosen': '0.11907', 'rewards_train/rejected': '0.051232', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067836', 'logps_train/rejected': '-111.68', 'logps_train/chosen': '-150.12', 'loss/train': '0.66687', 'examples_per_second': '32.578', 'grad_norm': '28.875', 'counters/examples': 117536, 'counters/updates': 3673}
train stats after 117568 examples: {'rewards_train/chosen': '0.0661', 'rewards_train/rejected': '0.0065391', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059561', 'logps_train/rejected': '-101.72', 'logps_train/chosen': '-113.72', 'loss/train': '0.66989', 'examples_per_second': '32.806', 'grad_norm': '23.375', 'counters/examples': 117568, 'counters/updates': 3674}
train stats after 117600 examples: {'rewards_train/chosen': '0.18889', 'rewards_train/rejected': '0.074244', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11464', 'logps_train/rejected': '-104.67', 'logps_train/chosen': '-166.99', 'loss/train': '0.64747', 'examples_per_second': '31.517', 'grad_norm': '31.875', 'counters/examples': 117600, 'counters/updates': 3675}
skipping logging after 117632 examples to avoid logging too frequently
train stats after 117664 examples: {'rewards_train/chosen': '0.10683', 'rewards_train/rejected': '0.029817', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077012', 'logps_train/rejected': '-102.81', 'logps_train/chosen': '-114.64', 'loss/train': '0.66108', 'examples_per_second': '30.372', 'grad_norm': '24.875', 'counters/examples': 117664, 'counters/updates': 3677}
train stats after 117696 examples: {'rewards_train/chosen': '0.13921', 'rewards_train/rejected': '0.00075281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13845', 'logps_train/rejected': '-130.53', 'logps_train/chosen': '-170.27', 'loss/train': '0.63613', 'examples_per_second': '30.449', 'grad_norm': '27.875', 'counters/examples': 117696, 'counters/updates': 3678}
skipping logging after 117728 examples to avoid logging too frequently
train stats after 117760 examples: {'rewards_train/chosen': '0.093794', 'rewards_train/rejected': '-0.00065172', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.094446', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-126.78', 'loss/train': '0.6594', 'examples_per_second': '31.469', 'grad_norm': '30', 'counters/examples': 117760, 'counters/updates': 3680}
skipping logging after 117792 examples to avoid logging too frequently
train stats after 117824 examples: {'rewards_train/chosen': '0.10393', 'rewards_train/rejected': '0.038463', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065463', 'logps_train/rejected': '-109.17', 'logps_train/chosen': '-142.21', 'loss/train': '0.67018', 'examples_per_second': '30.408', 'grad_norm': '26', 'counters/examples': 117824, 'counters/updates': 3682}
skipping logging after 117856 examples to avoid logging too frequently
train stats after 117888 examples: {'rewards_train/chosen': '0.12391', 'rewards_train/rejected': '0.020854', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10305', 'logps_train/rejected': '-130.99', 'logps_train/chosen': '-157.9', 'loss/train': '0.65151', 'examples_per_second': '30.34', 'grad_norm': '36', 'counters/examples': 117888, 'counters/updates': 3684}
train stats after 117920 examples: {'rewards_train/chosen': '0.074338', 'rewards_train/rejected': '0.024881', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049457', 'logps_train/rejected': '-148.82', 'logps_train/chosen': '-123', 'loss/train': '0.6757', 'examples_per_second': '31.426', 'grad_norm': '34', 'counters/examples': 117920, 'counters/updates': 3685}
train stats after 117952 examples: {'rewards_train/chosen': '0.13649', 'rewards_train/rejected': '0.02528', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11121', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-122.65', 'loss/train': '0.64956', 'examples_per_second': '32.169', 'grad_norm': '35', 'counters/examples': 117952, 'counters/updates': 3686}
train stats after 117984 examples: {'rewards_train/chosen': '0.18615', 'rewards_train/rejected': '0.053904', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13225', 'logps_train/rejected': '-107.15', 'logps_train/chosen': '-119.46', 'loss/train': '0.63365', 'examples_per_second': '31.974', 'grad_norm': '26.5', 'counters/examples': 117984, 'counters/updates': 3687}
train stats after 118016 examples: {'rewards_train/chosen': '0.071781', 'rewards_train/rejected': '0.10826', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.036477', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-128.17', 'loss/train': '0.71865', 'examples_per_second': '31.368', 'grad_norm': '42', 'counters/examples': 118016, 'counters/updates': 3688}
train stats after 118048 examples: {'rewards_train/chosen': '0.10615', 'rewards_train/rejected': '0.058207', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.047942', 'logps_train/rejected': '-104.24', 'logps_train/chosen': '-109.41', 'loss/train': '0.67457', 'examples_per_second': '31.495', 'grad_norm': '27.125', 'counters/examples': 118048, 'counters/updates': 3689}
train stats after 118080 examples: {'rewards_train/chosen': '0.14048', 'rewards_train/rejected': '0.11197', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028514', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-148.7', 'loss/train': '0.69605', 'examples_per_second': '29.83', 'grad_norm': '35.25', 'counters/examples': 118080, 'counters/updates': 3690}
train stats after 118112 examples: {'rewards_train/chosen': '0.060571', 'rewards_train/rejected': '0.08517', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0246', 'logps_train/rejected': '-143.45', 'logps_train/chosen': '-141.67', 'loss/train': '0.71175', 'examples_per_second': '30.523', 'grad_norm': '33.5', 'counters/examples': 118112, 'counters/updates': 3691}
skipping logging after 118144 examples to avoid logging too frequently
train stats after 118176 examples: {'rewards_train/chosen': '0.065458', 'rewards_train/rejected': '0.043107', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022351', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-106.67', 'loss/train': '0.68905', 'examples_per_second': '31.417', 'grad_norm': '36.5', 'counters/examples': 118176, 'counters/updates': 3693}
train stats after 118208 examples: {'rewards_train/chosen': '0.13235', 'rewards_train/rejected': '0.12082', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.011533', 'logps_train/rejected': '-165.68', 'logps_train/chosen': '-172.61', 'loss/train': '0.70611', 'examples_per_second': '31.414', 'grad_norm': '43.5', 'counters/examples': 118208, 'counters/updates': 3694}
train stats after 118240 examples: {'rewards_train/chosen': '0.13177', 'rewards_train/rejected': '0.04571', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086061', 'logps_train/rejected': '-137.97', 'logps_train/chosen': '-156.28', 'loss/train': '0.65773', 'examples_per_second': '33.128', 'grad_norm': '30.125', 'counters/examples': 118240, 'counters/updates': 3695}
train stats after 118272 examples: {'rewards_train/chosen': '0.13218', 'rewards_train/rejected': '0.10942', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022754', 'logps_train/rejected': '-96.178', 'logps_train/chosen': '-154.56', 'loss/train': '0.69072', 'examples_per_second': '29.906', 'grad_norm': '43.5', 'counters/examples': 118272, 'counters/updates': 3696}
train stats after 118304 examples: {'rewards_train/chosen': '0.079177', 'rewards_train/rejected': '0.032412', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.046766', 'logps_train/rejected': '-96.428', 'logps_train/chosen': '-107.47', 'loss/train': '0.67568', 'examples_per_second': '32.913', 'grad_norm': '25.125', 'counters/examples': 118304, 'counters/updates': 3697}
train stats after 118336 examples: {'rewards_train/chosen': '0.14774', 'rewards_train/rejected': '0.070565', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077175', 'logps_train/rejected': '-138.69', 'logps_train/chosen': '-132', 'loss/train': '0.66317', 'examples_per_second': '30.803', 'grad_norm': '29.75', 'counters/examples': 118336, 'counters/updates': 3698}
train stats after 118368 examples: {'rewards_train/chosen': '0.17224', 'rewards_train/rejected': '0.072447', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099798', 'logps_train/rejected': '-113.59', 'logps_train/chosen': '-152.03', 'loss/train': '0.6561', 'examples_per_second': '31.915', 'grad_norm': '26.375', 'counters/examples': 118368, 'counters/updates': 3699}
train stats after 118400 examples: {'rewards_train/chosen': '0.089978', 'rewards_train/rejected': '0.014751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075227', 'logps_train/rejected': '-120.11', 'logps_train/chosen': '-113.97', 'loss/train': '0.66545', 'examples_per_second': '31.748', 'grad_norm': '39.75', 'counters/examples': 118400, 'counters/updates': 3700}
skipping logging after 118432 examples to avoid logging too frequently
train stats after 118464 examples: {'rewards_train/chosen': '0.14668', 'rewards_train/rejected': '0.077203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069477', 'logps_train/rejected': '-161.16', 'logps_train/chosen': '-142.36', 'loss/train': '0.67025', 'examples_per_second': '31.407', 'grad_norm': '28.625', 'counters/examples': 118464, 'counters/updates': 3702}
train stats after 118496 examples: {'rewards_train/chosen': '0.23277', 'rewards_train/rejected': '0.10066', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13211', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-143.72', 'loss/train': '0.64286', 'examples_per_second': '31.027', 'grad_norm': '53', 'counters/examples': 118496, 'counters/updates': 3703}
skipping logging after 118528 examples to avoid logging too frequently
train stats after 118560 examples: {'rewards_train/chosen': '0.095227', 'rewards_train/rejected': '0.0054681', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089759', 'logps_train/rejected': '-115.38', 'logps_train/chosen': '-131.13', 'loss/train': '0.65483', 'examples_per_second': '33.258', 'grad_norm': '27.25', 'counters/examples': 118560, 'counters/updates': 3705}
train stats after 118592 examples: {'rewards_train/chosen': '0.1192', 'rewards_train/rejected': '-0.029692', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14889', 'logps_train/rejected': '-106.54', 'logps_train/chosen': '-103.33', 'loss/train': '0.62862', 'examples_per_second': '31.51', 'grad_norm': '28.125', 'counters/examples': 118592, 'counters/updates': 3706}
train stats after 118624 examples: {'rewards_train/chosen': '0.08407', 'rewards_train/rejected': '-0.0070648', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091134', 'logps_train/rejected': '-104.57', 'logps_train/chosen': '-148.44', 'loss/train': '0.65541', 'examples_per_second': '32.064', 'grad_norm': '28.25', 'counters/examples': 118624, 'counters/updates': 3707}
skipping logging after 118656 examples to avoid logging too frequently
train stats after 118688 examples: {'rewards_train/chosen': '0.080426', 'rewards_train/rejected': '0.081719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0012928', 'logps_train/rejected': '-103.64', 'logps_train/chosen': '-125.25', 'loss/train': '0.69701', 'examples_per_second': '35.548', 'grad_norm': '35', 'counters/examples': 118688, 'counters/updates': 3709}
train stats after 118720 examples: {'rewards_train/chosen': '0.11198', 'rewards_train/rejected': '0.11004', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.001932', 'logps_train/rejected': '-162.41', 'logps_train/chosen': '-152.19', 'loss/train': '0.7102', 'examples_per_second': '32.791', 'grad_norm': '43.25', 'counters/examples': 118720, 'counters/updates': 3710}
train stats after 118752 examples: {'rewards_train/chosen': '0.057911', 'rewards_train/rejected': '0.070737', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.012826', 'logps_train/rejected': '-118.03', 'logps_train/chosen': '-164.3', 'loss/train': '0.70401', 'examples_per_second': '31.418', 'grad_norm': '29.75', 'counters/examples': 118752, 'counters/updates': 3711}
train stats after 118784 examples: {'rewards_train/chosen': '0.077324', 'rewards_train/rejected': '0.10222', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024893', 'logps_train/rejected': '-133.02', 'logps_train/chosen': '-142.94', 'loss/train': '0.72141', 'examples_per_second': '31.442', 'grad_norm': '40.75', 'counters/examples': 118784, 'counters/updates': 3712}
skipping logging after 118816 examples to avoid logging too frequently
train stats after 118848 examples: {'rewards_train/chosen': '0.16435', 'rewards_train/rejected': '0.059355', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.105', 'logps_train/rejected': '-134.2', 'logps_train/chosen': '-163.58', 'loss/train': '0.66052', 'examples_per_second': '30.913', 'grad_norm': '33.25', 'counters/examples': 118848, 'counters/updates': 3714}
train stats after 118880 examples: {'rewards_train/chosen': '0.05831', 'rewards_train/rejected': '0.0088056', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049504', 'logps_train/rejected': '-117.08', 'logps_train/chosen': '-138.02', 'loss/train': '0.67172', 'examples_per_second': '31.532', 'grad_norm': '25.5', 'counters/examples': 118880, 'counters/updates': 3715}
skipping logging after 118912 examples to avoid logging too frequently
train stats after 118944 examples: {'rewards_train/chosen': '0.17906', 'rewards_train/rejected': '0.14093', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038133', 'logps_train/rejected': '-93.575', 'logps_train/chosen': '-116.52', 'loss/train': '0.6837', 'examples_per_second': '33.032', 'grad_norm': '32.5', 'counters/examples': 118944, 'counters/updates': 3717}
train stats after 118976 examples: {'rewards_train/chosen': '0.066939', 'rewards_train/rejected': '0.026434', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040505', 'logps_train/rejected': '-113.36', 'logps_train/chosen': '-150.66', 'loss/train': '0.68206', 'examples_per_second': '29.73', 'grad_norm': '37.75', 'counters/examples': 118976, 'counters/updates': 3718}
train stats after 119008 examples: {'rewards_train/chosen': '0.14275', 'rewards_train/rejected': '0.091073', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051679', 'logps_train/rejected': '-89.467', 'logps_train/chosen': '-144.1', 'loss/train': '0.67203', 'examples_per_second': '31.048', 'grad_norm': '33', 'counters/examples': 119008, 'counters/updates': 3719}
skipping logging after 119040 examples to avoid logging too frequently
train stats after 119072 examples: {'rewards_train/chosen': '0.046369', 'rewards_train/rejected': '-0.011584', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057953', 'logps_train/rejected': '-111.35', 'logps_train/chosen': '-136.66', 'loss/train': '0.67107', 'examples_per_second': '35.62', 'grad_norm': '25.5', 'counters/examples': 119072, 'counters/updates': 3721}
train stats after 119104 examples: {'rewards_train/chosen': '0.091928', 'rewards_train/rejected': '0.010502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081426', 'logps_train/rejected': '-116.8', 'logps_train/chosen': '-124', 'loss/train': '0.65951', 'examples_per_second': '25.091', 'grad_norm': '26.5', 'counters/examples': 119104, 'counters/updates': 3722}
train stats after 119136 examples: {'rewards_train/chosen': '0.10505', 'rewards_train/rejected': '0.022719', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082332', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-143.73', 'loss/train': '0.66038', 'examples_per_second': '30.69', 'grad_norm': '33', 'counters/examples': 119136, 'counters/updates': 3723}
train stats after 119168 examples: {'rewards_train/chosen': '0.10828', 'rewards_train/rejected': '0.071046', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037234', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-132.85', 'loss/train': '0.68651', 'examples_per_second': '31.486', 'grad_norm': '44.25', 'counters/examples': 119168, 'counters/updates': 3724}
train stats after 119200 examples: {'rewards_train/chosen': '0.13907', 'rewards_train/rejected': '0.038311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10076', 'logps_train/rejected': '-105.52', 'logps_train/chosen': '-135.69', 'loss/train': '0.64977', 'examples_per_second': '24.776', 'grad_norm': '26.375', 'counters/examples': 119200, 'counters/updates': 3725}
train stats after 119232 examples: {'rewards_train/chosen': '0.10206', 'rewards_train/rejected': '0.082443', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019618', 'logps_train/rejected': '-144.97', 'logps_train/chosen': '-159.78', 'loss/train': '0.69248', 'examples_per_second': '30.521', 'grad_norm': '34', 'counters/examples': 119232, 'counters/updates': 3726}
train stats after 119264 examples: {'rewards_train/chosen': '0.072362', 'rewards_train/rejected': '0.025396', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046966', 'logps_train/rejected': '-131.36', 'logps_train/chosen': '-161.85', 'loss/train': '0.67937', 'examples_per_second': '31.474', 'grad_norm': '39.5', 'counters/examples': 119264, 'counters/updates': 3727}
train stats after 119296 examples: {'rewards_train/chosen': '0.18046', 'rewards_train/rejected': '0.023216', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15724', 'logps_train/rejected': '-150.69', 'logps_train/chosen': '-130.26', 'loss/train': '0.62778', 'examples_per_second': '29.979', 'grad_norm': '27.25', 'counters/examples': 119296, 'counters/updates': 3728}
train stats after 119328 examples: {'rewards_train/chosen': '0.076154', 'rewards_train/rejected': '0.05414', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.022013', 'logps_train/rejected': '-101.97', 'logps_train/chosen': '-130.42', 'loss/train': '0.69393', 'examples_per_second': '32.499', 'grad_norm': '32.75', 'counters/examples': 119328, 'counters/updates': 3729}
skipping logging after 119360 examples to avoid logging too frequently
train stats after 119392 examples: {'rewards_train/chosen': '0.11101', 'rewards_train/rejected': '0.061923', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049084', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-132.29', 'loss/train': '0.68488', 'examples_per_second': '32.959', 'grad_norm': '34.25', 'counters/examples': 119392, 'counters/updates': 3731}
train stats after 119424 examples: {'rewards_train/chosen': '0.069386', 'rewards_train/rejected': '0.0072615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062125', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-171.12', 'loss/train': '0.66892', 'examples_per_second': '31.21', 'grad_norm': '36', 'counters/examples': 119424, 'counters/updates': 3732}
skipping logging after 119456 examples to avoid logging too frequently
train stats after 119488 examples: {'rewards_train/chosen': '0.15161', 'rewards_train/rejected': '0.075803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075802', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-152.02', 'loss/train': '0.66489', 'examples_per_second': '31.468', 'grad_norm': '29.25', 'counters/examples': 119488, 'counters/updates': 3734}
train stats after 119520 examples: {'rewards_train/chosen': '0.16513', 'rewards_train/rejected': '0.074291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090842', 'logps_train/rejected': '-132.18', 'logps_train/chosen': '-141.46', 'loss/train': '0.65715', 'examples_per_second': '30.874', 'grad_norm': '26.5', 'counters/examples': 119520, 'counters/updates': 3735}
train stats after 119552 examples: {'rewards_train/chosen': '0.14078', 'rewards_train/rejected': '0.14158', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00079315', 'logps_train/rejected': '-99.816', 'logps_train/chosen': '-146.1', 'loss/train': '0.70823', 'examples_per_second': '31.612', 'grad_norm': '36.75', 'counters/examples': 119552, 'counters/updates': 3736}
train stats after 119584 examples: {'rewards_train/chosen': '0.12683', 'rewards_train/rejected': '0.088836', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037991', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-174.76', 'loss/train': '0.6771', 'examples_per_second': '31.23', 'grad_norm': '36.25', 'counters/examples': 119584, 'counters/updates': 3737}
train stats after 119616 examples: {'rewards_train/chosen': '0.18471', 'rewards_train/rejected': '0.11447', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07024', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-182.26', 'loss/train': '0.67028', 'examples_per_second': '30.711', 'grad_norm': '39.5', 'counters/examples': 119616, 'counters/updates': 3738}
train stats after 119648 examples: {'rewards_train/chosen': '0.12224', 'rewards_train/rejected': '-0.0086369', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13088', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-138.75', 'loss/train': '0.63825', 'examples_per_second': '32.281', 'grad_norm': '26.625', 'counters/examples': 119648, 'counters/updates': 3739}
train stats after 119680 examples: {'rewards_train/chosen': '0.071389', 'rewards_train/rejected': '0.036455', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034934', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-148.02', 'loss/train': '0.68416', 'examples_per_second': '32.133', 'grad_norm': '33.25', 'counters/examples': 119680, 'counters/updates': 3740}
train stats after 119712 examples: {'rewards_train/chosen': '0.026393', 'rewards_train/rejected': '0.030812', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0044197', 'logps_train/rejected': '-106.42', 'logps_train/chosen': '-171.46', 'loss/train': '0.70487', 'examples_per_second': '30.512', 'grad_norm': '32.5', 'counters/examples': 119712, 'counters/updates': 3741}
train stats after 119744 examples: {'rewards_train/chosen': '0.17344', 'rewards_train/rejected': '0.090376', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08306', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-156.73', 'loss/train': '0.6632', 'examples_per_second': '30.281', 'grad_norm': '48.75', 'counters/examples': 119744, 'counters/updates': 3742}
train stats after 119776 examples: {'rewards_train/chosen': '0.081261', 'rewards_train/rejected': '0.10728', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02602', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-143.53', 'loss/train': '0.71723', 'examples_per_second': '32.016', 'grad_norm': '38.75', 'counters/examples': 119776, 'counters/updates': 3743}
train stats after 119808 examples: {'rewards_train/chosen': '0.058547', 'rewards_train/rejected': '0.0061269', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.05242', 'logps_train/rejected': '-135.35', 'logps_train/chosen': '-149.9', 'loss/train': '0.67701', 'examples_per_second': '31.34', 'grad_norm': '33.25', 'counters/examples': 119808, 'counters/updates': 3744}
train stats after 119840 examples: {'rewards_train/chosen': '0.2001', 'rewards_train/rejected': '0.05036', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14974', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-154.18', 'loss/train': '0.63498', 'examples_per_second': '30.125', 'grad_norm': '27.5', 'counters/examples': 119840, 'counters/updates': 3745}
train stats after 119872 examples: {'rewards_train/chosen': '0.091126', 'rewards_train/rejected': '0.00119', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089936', 'logps_train/rejected': '-119.31', 'logps_train/chosen': '-140.78', 'loss/train': '0.65639', 'examples_per_second': '31.49', 'grad_norm': '34.25', 'counters/examples': 119872, 'counters/updates': 3746}
train stats after 119904 examples: {'rewards_train/chosen': '0.14622', 'rewards_train/rejected': '0.0051715', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14105', 'logps_train/rejected': '-149.34', 'logps_train/chosen': '-156.21', 'loss/train': '0.6337', 'examples_per_second': '29.929', 'grad_norm': '33', 'counters/examples': 119904, 'counters/updates': 3747}
train stats after 119936 examples: {'rewards_train/chosen': '0.1367', 'rewards_train/rejected': '0.012831', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12386', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-143.54', 'loss/train': '0.64094', 'examples_per_second': '31.979', 'grad_norm': '27.375', 'counters/examples': 119936, 'counters/updates': 3748}
skipping logging after 119968 examples to avoid logging too frequently
train stats after 120000 examples: {'rewards_train/chosen': '0.098896', 'rewards_train/rejected': '0.018184', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080712', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-123.07', 'loss/train': '0.66585', 'examples_per_second': '31.159', 'grad_norm': '31.125', 'counters/examples': 120000, 'counters/updates': 3750}
Running evaluation after 120000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 120000: {'rewards_eval/chosen': '0.13208', 'rewards_eval/rejected': '0.056006', 'rewards_eval/accuracies': '0.58203', 'rewards_eval/margins': '0.076078', 'logps_eval/rejected': '-121.57', 'logps_eval/chosen': '-142.78', 'loss/eval': '0.66702'}
skipping save for non epoch
train stats after 120032 examples: {'rewards_train/chosen': '0.10104', 'rewards_train/rejected': '0.064237', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036806', 'logps_train/rejected': '-93.223', 'logps_train/chosen': '-149.68', 'loss/train': '0.68632', 'examples_per_second': '33.917', 'grad_norm': '30.375', 'counters/examples': 120032, 'counters/updates': 3751}
skipping logging after 120064 examples to avoid logging too frequently
train stats after 120096 examples: {'rewards_train/chosen': '0.1019', 'rewards_train/rejected': '0.061852', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040045', 'logps_train/rejected': '-97.202', 'logps_train/chosen': '-144.67', 'loss/train': '0.6793', 'examples_per_second': '32.728', 'grad_norm': '40.5', 'counters/examples': 120096, 'counters/updates': 3753}
train stats after 120128 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.079703', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053726', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-154.83', 'loss/train': '0.67576', 'examples_per_second': '31.433', 'grad_norm': '35.5', 'counters/examples': 120128, 'counters/updates': 3754}
skipping logging after 120160 examples to avoid logging too frequently
train stats after 120192 examples: {'rewards_train/chosen': '0.19666', 'rewards_train/rejected': '0.066925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12974', 'logps_train/rejected': '-121.67', 'logps_train/chosen': '-133.77', 'loss/train': '0.65351', 'examples_per_second': '32.293', 'grad_norm': '26.25', 'counters/examples': 120192, 'counters/updates': 3756}
train stats after 120224 examples: {'rewards_train/chosen': '0.17995', 'rewards_train/rejected': '0.10308', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076867', 'logps_train/rejected': '-143.8', 'logps_train/chosen': '-183.92', 'loss/train': '0.69175', 'examples_per_second': '33.07', 'grad_norm': '47', 'counters/examples': 120224, 'counters/updates': 3757}
train stats after 120256 examples: {'rewards_train/chosen': '0.13101', 'rewards_train/rejected': '0.090914', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040092', 'logps_train/rejected': '-113.88', 'logps_train/chosen': '-175.97', 'loss/train': '0.6843', 'examples_per_second': '29.949', 'grad_norm': '32.5', 'counters/examples': 120256, 'counters/updates': 3758}
train stats after 120288 examples: {'rewards_train/chosen': '0.11869', 'rewards_train/rejected': '-0.010899', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12959', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-133.77', 'loss/train': '0.64428', 'examples_per_second': '30.432', 'grad_norm': '26.75', 'counters/examples': 120288, 'counters/updates': 3759}
train stats after 120320 examples: {'rewards_train/chosen': '0.1901', 'rewards_train/rejected': '0.1267', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.063397', 'logps_train/rejected': '-124.07', 'logps_train/chosen': '-150.8', 'loss/train': '0.68177', 'examples_per_second': '30.965', 'grad_norm': '34.25', 'counters/examples': 120320, 'counters/updates': 3760}
train stats after 120352 examples: {'rewards_train/chosen': '0.20079', 'rewards_train/rejected': '0.054175', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14661', 'logps_train/rejected': '-134.72', 'logps_train/chosen': '-150.76', 'loss/train': '0.63863', 'examples_per_second': '30.318', 'grad_norm': '34', 'counters/examples': 120352, 'counters/updates': 3761}
train stats after 120384 examples: {'rewards_train/chosen': '0.17962', 'rewards_train/rejected': '0.025199', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15442', 'logps_train/rejected': '-171.85', 'logps_train/chosen': '-194.81', 'loss/train': '0.63206', 'examples_per_second': '31.295', 'grad_norm': '36.5', 'counters/examples': 120384, 'counters/updates': 3762}
train stats after 120416 examples: {'rewards_train/chosen': '0.17032', 'rewards_train/rejected': '-0.0078537', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17818', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-136.77', 'loss/train': '0.62421', 'examples_per_second': '31.941', 'grad_norm': '30.625', 'counters/examples': 120416, 'counters/updates': 3763}
train stats after 120448 examples: {'rewards_train/chosen': '0.11008', 'rewards_train/rejected': '0.022071', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088007', 'logps_train/rejected': '-101.06', 'logps_train/chosen': '-137.68', 'loss/train': '0.66267', 'examples_per_second': '30.465', 'grad_norm': '26.875', 'counters/examples': 120448, 'counters/updates': 3764}
skipping logging after 120480 examples to avoid logging too frequently
train stats after 120512 examples: {'rewards_train/chosen': '0.11204', 'rewards_train/rejected': '0.051142', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.060897', 'logps_train/rejected': '-110.54', 'logps_train/chosen': '-142.03', 'loss/train': '0.6709', 'examples_per_second': '30.378', 'grad_norm': '32.25', 'counters/examples': 120512, 'counters/updates': 3766}
train stats after 120544 examples: {'rewards_train/chosen': '0.12061', 'rewards_train/rejected': '0.0088903', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11172', 'logps_train/rejected': '-136.31', 'logps_train/chosen': '-166.01', 'loss/train': '0.64585', 'examples_per_second': '31.001', 'grad_norm': '27.625', 'counters/examples': 120544, 'counters/updates': 3767}
skipping logging after 120576 examples to avoid logging too frequently
train stats after 120608 examples: {'rewards_train/chosen': '0.065804', 'rewards_train/rejected': '0.038861', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026943', 'logps_train/rejected': '-186.53', 'logps_train/chosen': '-139.68', 'loss/train': '0.68783', 'examples_per_second': '31.272', 'grad_norm': '44.5', 'counters/examples': 120608, 'counters/updates': 3769}
train stats after 120640 examples: {'rewards_train/chosen': '0.078904', 'rewards_train/rejected': '-0.027228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10613', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-144.22', 'loss/train': '0.65011', 'examples_per_second': '30.695', 'grad_norm': '33', 'counters/examples': 120640, 'counters/updates': 3770}
train stats after 120672 examples: {'rewards_train/chosen': '0.066519', 'rewards_train/rejected': '0.12', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.053485', 'logps_train/rejected': '-157.52', 'logps_train/chosen': '-154.13', 'loss/train': '0.73259', 'examples_per_second': '32.4', 'grad_norm': '41.75', 'counters/examples': 120672, 'counters/updates': 3771}
train stats after 120704 examples: {'rewards_train/chosen': '0.1572', 'rewards_train/rejected': '0.20822', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.051022', 'logps_train/rejected': '-139.55', 'logps_train/chosen': '-155.56', 'loss/train': '0.73487', 'examples_per_second': '32.785', 'grad_norm': '57.25', 'counters/examples': 120704, 'counters/updates': 3772}
train stats after 120736 examples: {'rewards_train/chosen': '0.19203', 'rewards_train/rejected': '0.02536', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16667', 'logps_train/rejected': '-159.96', 'logps_train/chosen': '-191.35', 'loss/train': '0.62096', 'examples_per_second': '31.154', 'grad_norm': '60', 'counters/examples': 120736, 'counters/updates': 3773}
train stats after 120768 examples: {'rewards_train/chosen': '0.14734', 'rewards_train/rejected': '0.083474', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063862', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-152.52', 'loss/train': '0.67124', 'examples_per_second': '30.441', 'grad_norm': '44.75', 'counters/examples': 120768, 'counters/updates': 3774}
train stats after 120800 examples: {'rewards_train/chosen': '0.18949', 'rewards_train/rejected': '0.029557', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15993', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-185.43', 'loss/train': '0.63457', 'examples_per_second': '31.239', 'grad_norm': '32.5', 'counters/examples': 120800, 'counters/updates': 3775}
skipping logging after 120832 examples to avoid logging too frequently
train stats after 120864 examples: {'rewards_train/chosen': '0.1463', 'rewards_train/rejected': '0.029231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11707', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-149.61', 'loss/train': '0.64507', 'examples_per_second': '31.415', 'grad_norm': '27.625', 'counters/examples': 120864, 'counters/updates': 3777}
train stats after 120896 examples: {'rewards_train/chosen': '0.049168', 'rewards_train/rejected': '-0.0044099', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053578', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-159.73', 'loss/train': '0.67223', 'examples_per_second': '31.291', 'grad_norm': '35', 'counters/examples': 120896, 'counters/updates': 3778}
skipping logging after 120928 examples to avoid logging too frequently
train stats after 120960 examples: {'rewards_train/chosen': '0.13308', 'rewards_train/rejected': '0.030072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10301', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-156.62', 'loss/train': '0.65139', 'examples_per_second': '30.354', 'grad_norm': '28.25', 'counters/examples': 120960, 'counters/updates': 3780}
train stats after 120992 examples: {'rewards_train/chosen': '0.15094', 'rewards_train/rejected': '0.021796', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12915', 'logps_train/rejected': '-141.62', 'logps_train/chosen': '-169.5', 'loss/train': '0.6439', 'examples_per_second': '31.419', 'grad_norm': '35.25', 'counters/examples': 120992, 'counters/updates': 3781}
train stats after 121024 examples: {'rewards_train/chosen': '0.1851', 'rewards_train/rejected': '0.15039', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034711', 'logps_train/rejected': '-152.24', 'logps_train/chosen': '-151.11', 'loss/train': '0.69367', 'examples_per_second': '33.121', 'grad_norm': '52.5', 'counters/examples': 121024, 'counters/updates': 3782}
train stats after 121056 examples: {'rewards_train/chosen': '0.15682', 'rewards_train/rejected': '0.068872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087951', 'logps_train/rejected': '-114.38', 'logps_train/chosen': '-149.07', 'loss/train': '0.65947', 'examples_per_second': '32.775', 'grad_norm': '27.5', 'counters/examples': 121056, 'counters/updates': 3783}
train stats after 121088 examples: {'rewards_train/chosen': '0.15898', 'rewards_train/rejected': '0.053865', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10511', 'logps_train/rejected': '-121.45', 'logps_train/chosen': '-141.33', 'loss/train': '0.65207', 'examples_per_second': '31.887', 'grad_norm': '33.75', 'counters/examples': 121088, 'counters/updates': 3784}
train stats after 121120 examples: {'rewards_train/chosen': '0.13508', 'rewards_train/rejected': '0.045141', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089936', 'logps_train/rejected': '-110.86', 'logps_train/chosen': '-155.22', 'loss/train': '0.66055', 'examples_per_second': '30.48', 'grad_norm': '29', 'counters/examples': 121120, 'counters/updates': 3785}
train stats after 121152 examples: {'rewards_train/chosen': '0.22626', 'rewards_train/rejected': '0.017618', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20864', 'logps_train/rejected': '-101.71', 'logps_train/chosen': '-145.4', 'loss/train': '0.6052', 'examples_per_second': '31.434', 'grad_norm': '26.75', 'counters/examples': 121152, 'counters/updates': 3786}
skipping logging after 121184 examples to avoid logging too frequently
train stats after 121216 examples: {'rewards_train/chosen': '0.096848', 'rewards_train/rejected': '0.038379', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058469', 'logps_train/rejected': '-160.22', 'logps_train/chosen': '-135.1', 'loss/train': '0.68281', 'examples_per_second': '31.397', 'grad_norm': '77', 'counters/examples': 121216, 'counters/updates': 3788}
train stats after 121248 examples: {'rewards_train/chosen': '0.17977', 'rewards_train/rejected': '0.017362', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16241', 'logps_train/rejected': '-107.82', 'logps_train/chosen': '-170.24', 'loss/train': '0.62962', 'examples_per_second': '31.181', 'grad_norm': '35.75', 'counters/examples': 121248, 'counters/updates': 3789}
skipping logging after 121280 examples to avoid logging too frequently
train stats after 121312 examples: {'rewards_train/chosen': '0.049415', 'rewards_train/rejected': '0.045445', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0039695', 'logps_train/rejected': '-147.12', 'logps_train/chosen': '-139.4', 'loss/train': '0.69993', 'examples_per_second': '31.038', 'grad_norm': '36', 'counters/examples': 121312, 'counters/updates': 3791}
train stats after 121344 examples: {'rewards_train/chosen': '0.20041', 'rewards_train/rejected': '0.042322', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15808', 'logps_train/rejected': '-118.47', 'logps_train/chosen': '-164.83', 'loss/train': '0.63025', 'examples_per_second': '31.395', 'grad_norm': '36', 'counters/examples': 121344, 'counters/updates': 3792}
train stats after 121376 examples: {'rewards_train/chosen': '0.16286', 'rewards_train/rejected': '0.13744', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.025426', 'logps_train/rejected': '-163.99', 'logps_train/chosen': '-153.7', 'loss/train': '0.69256', 'examples_per_second': '24.325', 'grad_norm': '45', 'counters/examples': 121376, 'counters/updates': 3793}
skipping logging after 121408 examples to avoid logging too frequently
train stats after 121440 examples: {'rewards_train/chosen': '0.082745', 'rewards_train/rejected': '0.057669', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025077', 'logps_train/rejected': '-104.16', 'logps_train/chosen': '-129.52', 'loss/train': '0.68788', 'examples_per_second': '30.446', 'grad_norm': '31', 'counters/examples': 121440, 'counters/updates': 3795}
skipping logging after 121472 examples to avoid logging too frequently
train stats after 121504 examples: {'rewards_train/chosen': '0.13226', 'rewards_train/rejected': '0.066923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065339', 'logps_train/rejected': '-120.27', 'logps_train/chosen': '-139.04', 'loss/train': '0.683', 'examples_per_second': '34.174', 'grad_norm': '42.5', 'counters/examples': 121504, 'counters/updates': 3797}
train stats after 121536 examples: {'rewards_train/chosen': '0.074175', 'rewards_train/rejected': '0.11148', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.037303', 'logps_train/rejected': '-116.34', 'logps_train/chosen': '-161.9', 'loss/train': '0.71647', 'examples_per_second': '29.897', 'grad_norm': '28.25', 'counters/examples': 121536, 'counters/updates': 3798}
train stats after 121568 examples: {'rewards_train/chosen': '0.12361', 'rewards_train/rejected': '0.045747', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.077865', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-120.79', 'loss/train': '0.66658', 'examples_per_second': '32.098', 'grad_norm': '75.5', 'counters/examples': 121568, 'counters/updates': 3799}
skipping logging after 121600 examples to avoid logging too frequently
train stats after 121632 examples: {'rewards_train/chosen': '0.108', 'rewards_train/rejected': '0.0033589', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10464', 'logps_train/rejected': '-121.41', 'logps_train/chosen': '-122.48', 'loss/train': '0.64961', 'examples_per_second': '32.894', 'grad_norm': '32.75', 'counters/examples': 121632, 'counters/updates': 3801}
train stats after 121664 examples: {'rewards_train/chosen': '0.10888', 'rewards_train/rejected': '0.065196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043684', 'logps_train/rejected': '-127.76', 'logps_train/chosen': '-132.04', 'loss/train': '0.68674', 'examples_per_second': '31.147', 'grad_norm': '33.25', 'counters/examples': 121664, 'counters/updates': 3802}
train stats after 121696 examples: {'rewards_train/chosen': '0.0027574', 'rewards_train/rejected': '0.022609', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.019851', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-138.44', 'loss/train': '0.71037', 'examples_per_second': '29.89', 'grad_norm': '31.625', 'counters/examples': 121696, 'counters/updates': 3803}
train stats after 121728 examples: {'rewards_train/chosen': '0.16938', 'rewards_train/rejected': '0.10694', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062445', 'logps_train/rejected': '-149.05', 'logps_train/chosen': '-172.18', 'loss/train': '0.67188', 'examples_per_second': '31.415', 'grad_norm': '31.5', 'counters/examples': 121728, 'counters/updates': 3804}
train stats after 121760 examples: {'rewards_train/chosen': '0.10595', 'rewards_train/rejected': '-0.026974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13293', 'logps_train/rejected': '-128.38', 'logps_train/chosen': '-151.59', 'loss/train': '0.64589', 'examples_per_second': '30.746', 'grad_norm': '27.125', 'counters/examples': 121760, 'counters/updates': 3805}
train stats after 121792 examples: {'rewards_train/chosen': '0.15909', 'rewards_train/rejected': '0.029788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1293', 'logps_train/rejected': '-129.21', 'logps_train/chosen': '-163.55', 'loss/train': '0.64061', 'examples_per_second': '31.684', 'grad_norm': '46.5', 'counters/examples': 121792, 'counters/updates': 3806}
train stats after 121824 examples: {'rewards_train/chosen': '0.22047', 'rewards_train/rejected': '0.051758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16871', 'logps_train/rejected': '-153.07', 'logps_train/chosen': '-173.93', 'loss/train': '0.63192', 'examples_per_second': '29.912', 'grad_norm': '30.75', 'counters/examples': 121824, 'counters/updates': 3807}
train stats after 121856 examples: {'rewards_train/chosen': '0.11136', 'rewards_train/rejected': '0.028554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082808', 'logps_train/rejected': '-100.06', 'logps_train/chosen': '-129.88', 'loss/train': '0.66109', 'examples_per_second': '31.997', 'grad_norm': '26.25', 'counters/examples': 121856, 'counters/updates': 3808}
train stats after 121888 examples: {'rewards_train/chosen': '0.084312', 'rewards_train/rejected': '-0.02975', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11406', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-131.29', 'loss/train': '0.6429', 'examples_per_second': '28.207', 'grad_norm': '26.625', 'counters/examples': 121888, 'counters/updates': 3809}
skipping logging after 121920 examples to avoid logging too frequently
train stats after 121952 examples: {'rewards_train/chosen': '0.032931', 'rewards_train/rejected': '0.031931', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.001', 'logps_train/rejected': '-94.174', 'logps_train/chosen': '-135.65', 'loss/train': '0.70134', 'examples_per_second': '32.319', 'grad_norm': '30.25', 'counters/examples': 121952, 'counters/updates': 3811}
train stats after 121984 examples: {'rewards_train/chosen': '0.16299', 'rewards_train/rejected': '0.042612', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12038', 'logps_train/rejected': '-137.54', 'logps_train/chosen': '-127.01', 'loss/train': '0.6439', 'examples_per_second': '29.906', 'grad_norm': '42.5', 'counters/examples': 121984, 'counters/updates': 3812}
train stats after 122016 examples: {'rewards_train/chosen': '0.07946', 'rewards_train/rejected': '0.042791', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036669', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-129.36', 'loss/train': '0.68332', 'examples_per_second': '31.557', 'grad_norm': '25.25', 'counters/examples': 122016, 'counters/updates': 3813}
train stats after 122048 examples: {'rewards_train/chosen': '0.18246', 'rewards_train/rejected': '0.04265', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13981', 'logps_train/rejected': '-126.91', 'logps_train/chosen': '-155.2', 'loss/train': '0.63421', 'examples_per_second': '31.266', 'grad_norm': '31.25', 'counters/examples': 122048, 'counters/updates': 3814}
skipping logging after 122080 examples to avoid logging too frequently
train stats after 122112 examples: {'rewards_train/chosen': '0.059759', 'rewards_train/rejected': '-0.053717', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11348', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-126.38', 'loss/train': '0.65181', 'examples_per_second': '31.405', 'grad_norm': '31', 'counters/examples': 122112, 'counters/updates': 3816}
train stats after 122144 examples: {'rewards_train/chosen': '0.13759', 'rewards_train/rejected': '-0.026397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16399', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-174.62', 'loss/train': '0.63577', 'examples_per_second': '33.08', 'grad_norm': '33.75', 'counters/examples': 122144, 'counters/updates': 3817}
skipping logging after 122176 examples to avoid logging too frequently
train stats after 122208 examples: {'rewards_train/chosen': '0.11831', 'rewards_train/rejected': '0.058626', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059679', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-133.61', 'loss/train': '0.67789', 'examples_per_second': '30.555', 'grad_norm': '32.25', 'counters/examples': 122208, 'counters/updates': 3819}
skipping logging after 122240 examples to avoid logging too frequently
train stats after 122272 examples: {'rewards_train/chosen': '0.19688', 'rewards_train/rejected': '0.062283', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1346', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-159.68', 'loss/train': '0.64351', 'examples_per_second': '35.707', 'grad_norm': '28.375', 'counters/examples': 122272, 'counters/updates': 3821}
skipping logging after 122304 examples to avoid logging too frequently
train stats after 122336 examples: {'rewards_train/chosen': '0.071782', 'rewards_train/rejected': '-0.011169', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08295', 'logps_train/rejected': '-135.05', 'logps_train/chosen': '-113.37', 'loss/train': '0.65925', 'examples_per_second': '31.42', 'grad_norm': '35.75', 'counters/examples': 122336, 'counters/updates': 3823}
train stats after 122368 examples: {'rewards_train/chosen': '0.062399', 'rewards_train/rejected': '0.019439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04296', 'logps_train/rejected': '-142.97', 'logps_train/chosen': '-122.93', 'loss/train': '0.67982', 'examples_per_second': '31.954', 'grad_norm': '29.125', 'counters/examples': 122368, 'counters/updates': 3824}
skipping logging after 122400 examples to avoid logging too frequently
train stats after 122432 examples: {'rewards_train/chosen': '0.11518', 'rewards_train/rejected': '0.0032546', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11192', 'logps_train/rejected': '-122.71', 'logps_train/chosen': '-143.79', 'loss/train': '0.65419', 'examples_per_second': '31.268', 'grad_norm': '25.625', 'counters/examples': 122432, 'counters/updates': 3826}
train stats after 122464 examples: {'rewards_train/chosen': '0.082082', 'rewards_train/rejected': '0.12115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.039064', 'logps_train/rejected': '-118.38', 'logps_train/chosen': '-112.24', 'loss/train': '0.72265', 'examples_per_second': '31.676', 'grad_norm': '31.375', 'counters/examples': 122464, 'counters/updates': 3827}
train stats after 122496 examples: {'rewards_train/chosen': '0.19988', 'rewards_train/rejected': '0.0089305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19095', 'logps_train/rejected': '-107.14', 'logps_train/chosen': '-149.88', 'loss/train': '0.61683', 'examples_per_second': '31.101', 'grad_norm': '34.25', 'counters/examples': 122496, 'counters/updates': 3828}
train stats after 122528 examples: {'rewards_train/chosen': '0.16791', 'rewards_train/rejected': '0.10587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062036', 'logps_train/rejected': '-102.94', 'logps_train/chosen': '-125.19', 'loss/train': '0.67332', 'examples_per_second': '30.401', 'grad_norm': '49.75', 'counters/examples': 122528, 'counters/updates': 3829}
train stats after 122560 examples: {'rewards_train/chosen': '0.11635', 'rewards_train/rejected': '0.081895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034455', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-138.66', 'loss/train': '0.68647', 'examples_per_second': '30.872', 'grad_norm': '27.125', 'counters/examples': 122560, 'counters/updates': 3830}
train stats after 122592 examples: {'rewards_train/chosen': '0.21349', 'rewards_train/rejected': '0.079479', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13401', 'logps_train/rejected': '-118.2', 'logps_train/chosen': '-149.62', 'loss/train': '0.64121', 'examples_per_second': '30.371', 'grad_norm': '41.75', 'counters/examples': 122592, 'counters/updates': 3831}
train stats after 122624 examples: {'rewards_train/chosen': '0.10008', 'rewards_train/rejected': '0.051042', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049034', 'logps_train/rejected': '-158.86', 'logps_train/chosen': '-162.53', 'loss/train': '0.68243', 'examples_per_second': '31.4', 'grad_norm': '40.25', 'counters/examples': 122624, 'counters/updates': 3832}
skipping logging after 122656 examples to avoid logging too frequently
train stats after 122688 examples: {'rewards_train/chosen': '0.14284', 'rewards_train/rejected': '0.093373', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04947', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-155.94', 'loss/train': '0.67727', 'examples_per_second': '30.049', 'grad_norm': '37.5', 'counters/examples': 122688, 'counters/updates': 3834}
skipping logging after 122720 examples to avoid logging too frequently
train stats after 122752 examples: {'rewards_train/chosen': '0.037294', 'rewards_train/rejected': '-0.012998', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050292', 'logps_train/rejected': '-98.334', 'logps_train/chosen': '-109.27', 'loss/train': '0.67145', 'examples_per_second': '31.566', 'grad_norm': '33.5', 'counters/examples': 122752, 'counters/updates': 3836}
train stats after 122784 examples: {'rewards_train/chosen': '0.082974', 'rewards_train/rejected': '-0.024241', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10721', 'logps_train/rejected': '-89.441', 'logps_train/chosen': '-99.46', 'loss/train': '0.64932', 'examples_per_second': '30.799', 'grad_norm': '24.125', 'counters/examples': 122784, 'counters/updates': 3837}
skipping logging after 122816 examples to avoid logging too frequently
train stats after 122848 examples: {'rewards_train/chosen': '0.082299', 'rewards_train/rejected': '0.084987', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0026874', 'logps_train/rejected': '-120.77', 'logps_train/chosen': '-127.49', 'loss/train': '0.69921', 'examples_per_second': '32.247', 'grad_norm': '27.75', 'counters/examples': 122848, 'counters/updates': 3839}
train stats after 122880 examples: {'rewards_train/chosen': '0.19526', 'rewards_train/rejected': '0.10268', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092585', 'logps_train/rejected': '-200.47', 'logps_train/chosen': '-205.42', 'loss/train': '0.656', 'examples_per_second': '31.418', 'grad_norm': '66', 'counters/examples': 122880, 'counters/updates': 3840}
train stats after 122912 examples: {'rewards_train/chosen': '0.092902', 'rewards_train/rejected': '0.084581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0083207', 'logps_train/rejected': '-125.05', 'logps_train/chosen': '-130.42', 'loss/train': '0.69372', 'examples_per_second': '30.443', 'grad_norm': '30.375', 'counters/examples': 122912, 'counters/updates': 3841}
train stats after 122944 examples: {'rewards_train/chosen': '0.055312', 'rewards_train/rejected': '0.051848', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0034631', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-134.48', 'loss/train': '0.69941', 'examples_per_second': '33.066', 'grad_norm': '32.25', 'counters/examples': 122944, 'counters/updates': 3842}
skipping logging after 122976 examples to avoid logging too frequently
train stats after 123008 examples: {'rewards_train/chosen': '0.090061', 'rewards_train/rejected': '0.078982', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.01108', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-101.08', 'loss/train': '0.69347', 'examples_per_second': '36.016', 'grad_norm': '27.875', 'counters/examples': 123008, 'counters/updates': 3844}
train stats after 123040 examples: {'rewards_train/chosen': '0.12382', 'rewards_train/rejected': '0.12217', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0016524', 'logps_train/rejected': '-110.78', 'logps_train/chosen': '-134.47', 'loss/train': '0.71083', 'examples_per_second': '31.241', 'grad_norm': '54', 'counters/examples': 123040, 'counters/updates': 3845}
train stats after 123072 examples: {'rewards_train/chosen': '0.159', 'rewards_train/rejected': '0.024354', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13465', 'logps_train/rejected': '-124.81', 'logps_train/chosen': '-159.57', 'loss/train': '0.6381', 'examples_per_second': '32.004', 'grad_norm': '31.625', 'counters/examples': 123072, 'counters/updates': 3846}
train stats after 123104 examples: {'rewards_train/chosen': '0.12391', 'rewards_train/rejected': '0.12386', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '4.6888e-05', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-129.76', 'loss/train': '0.6975', 'examples_per_second': '30.394', 'grad_norm': '29.25', 'counters/examples': 123104, 'counters/updates': 3847}
skipping logging after 123136 examples to avoid logging too frequently
train stats after 123168 examples: {'rewards_train/chosen': '0.26222', 'rewards_train/rejected': '-0.010777', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.273', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-144.24', 'loss/train': '0.58305', 'examples_per_second': '31.412', 'grad_norm': '26.125', 'counters/examples': 123168, 'counters/updates': 3849}
train stats after 123200 examples: {'rewards_train/chosen': '0.10037', 'rewards_train/rejected': '0.035363', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065002', 'logps_train/rejected': '-123.97', 'logps_train/chosen': '-155.89', 'loss/train': '0.67045', 'examples_per_second': '31.453', 'grad_norm': '31.75', 'counters/examples': 123200, 'counters/updates': 3850}
train stats after 123232 examples: {'rewards_train/chosen': '0.1276', 'rewards_train/rejected': '0.065693', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.061908', 'logps_train/rejected': '-120.47', 'logps_train/chosen': '-142.88', 'loss/train': '0.67506', 'examples_per_second': '31.462', 'grad_norm': '38.5', 'counters/examples': 123232, 'counters/updates': 3851}
train stats after 123264 examples: {'rewards_train/chosen': '0.20157', 'rewards_train/rejected': '0.068409', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13316', 'logps_train/rejected': '-138.51', 'logps_train/chosen': '-160.65', 'loss/train': '0.6348', 'examples_per_second': '30.307', 'grad_norm': '38.75', 'counters/examples': 123264, 'counters/updates': 3852}
train stats after 123296 examples: {'rewards_train/chosen': '0.13085', 'rewards_train/rejected': '0.076882', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.05397', 'logps_train/rejected': '-133.94', 'logps_train/chosen': '-122.53', 'loss/train': '0.67423', 'examples_per_second': '32.313', 'grad_norm': '34.75', 'counters/examples': 123296, 'counters/updates': 3853}
train stats after 123328 examples: {'rewards_train/chosen': '0.12444', 'rewards_train/rejected': '-0.022743', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14718', 'logps_train/rejected': '-146.05', 'logps_train/chosen': '-143.73', 'loss/train': '0.63866', 'examples_per_second': '31.451', 'grad_norm': '62.75', 'counters/examples': 123328, 'counters/updates': 3854}
train stats after 123360 examples: {'rewards_train/chosen': '0.11184', 'rewards_train/rejected': '0.074927', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.036912', 'logps_train/rejected': '-129.85', 'logps_train/chosen': '-185.49', 'loss/train': '0.68296', 'examples_per_second': '30.934', 'grad_norm': '52', 'counters/examples': 123360, 'counters/updates': 3855}
train stats after 123392 examples: {'rewards_train/chosen': '0.13168', 'rewards_train/rejected': '0.071948', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05973', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-174.3', 'loss/train': '0.67249', 'examples_per_second': '31.116', 'grad_norm': '33.75', 'counters/examples': 123392, 'counters/updates': 3856}
skipping logging after 123424 examples to avoid logging too frequently
train stats after 123456 examples: {'rewards_train/chosen': '0.086379', 'rewards_train/rejected': '0.11281', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.026428', 'logps_train/rejected': '-171.42', 'logps_train/chosen': '-181.58', 'loss/train': '0.71731', 'examples_per_second': '31.129', 'grad_norm': '41.5', 'counters/examples': 123456, 'counters/updates': 3858}
train stats after 123488 examples: {'rewards_train/chosen': '0.14951', 'rewards_train/rejected': '0.086593', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062914', 'logps_train/rejected': '-127.36', 'logps_train/chosen': '-199.32', 'loss/train': '0.67598', 'examples_per_second': '30.381', 'grad_norm': '32.75', 'counters/examples': 123488, 'counters/updates': 3859}
train stats after 123520 examples: {'rewards_train/chosen': '0.086224', 'rewards_train/rejected': '0.04575', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040474', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-141.02', 'loss/train': '0.68147', 'examples_per_second': '31.435', 'grad_norm': '29.375', 'counters/examples': 123520, 'counters/updates': 3860}
train stats after 123552 examples: {'rewards_train/chosen': '0.12829', 'rewards_train/rejected': '0.074373', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053917', 'logps_train/rejected': '-114.26', 'logps_train/chosen': '-133.33', 'loss/train': '0.68201', 'examples_per_second': '30.403', 'grad_norm': '32.25', 'counters/examples': 123552, 'counters/updates': 3861}
train stats after 123584 examples: {'rewards_train/chosen': '0.11901', 'rewards_train/rejected': '0.10017', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.018846', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-172.93', 'loss/train': '0.69143', 'examples_per_second': '31.419', 'grad_norm': '39.25', 'counters/examples': 123584, 'counters/updates': 3862}
train stats after 123616 examples: {'rewards_train/chosen': '0.082684', 'rewards_train/rejected': '0.034066', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048618', 'logps_train/rejected': '-114.11', 'logps_train/chosen': '-165.22', 'loss/train': '0.68582', 'examples_per_second': '31.405', 'grad_norm': '35.75', 'counters/examples': 123616, 'counters/updates': 3863}
train stats after 123648 examples: {'rewards_train/chosen': '0.13459', 'rewards_train/rejected': '0.13951', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0049259', 'logps_train/rejected': '-139.24', 'logps_train/chosen': '-130.68', 'loss/train': '0.70869', 'examples_per_second': '31.445', 'grad_norm': '49.75', 'counters/examples': 123648, 'counters/updates': 3864}
train stats after 123680 examples: {'rewards_train/chosen': '0.043495', 'rewards_train/rejected': '0.044452', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00095749', 'logps_train/rejected': '-133.49', 'logps_train/chosen': '-130.16', 'loss/train': '0.70447', 'examples_per_second': '30.389', 'grad_norm': '39.5', 'counters/examples': 123680, 'counters/updates': 3865}
train stats after 123712 examples: {'rewards_train/chosen': '0.14361', 'rewards_train/rejected': '0.038695', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10491', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-145.83', 'loss/train': '0.65096', 'examples_per_second': '31.443', 'grad_norm': '43.5', 'counters/examples': 123712, 'counters/updates': 3866}
train stats after 123744 examples: {'rewards_train/chosen': '0.08962', 'rewards_train/rejected': '0.063139', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.026481', 'logps_train/rejected': '-130.33', 'logps_train/chosen': '-139.64', 'loss/train': '0.69227', 'examples_per_second': '30.528', 'grad_norm': '71.5', 'counters/examples': 123744, 'counters/updates': 3867}
train stats after 123776 examples: {'rewards_train/chosen': '0.2608', 'rewards_train/rejected': '0.073924', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18688', 'logps_train/rejected': '-108.47', 'logps_train/chosen': '-138.91', 'loss/train': '0.61389', 'examples_per_second': '30.947', 'grad_norm': '32.75', 'counters/examples': 123776, 'counters/updates': 3868}
train stats after 123808 examples: {'rewards_train/chosen': '0.14334', 'rewards_train/rejected': '0.022722', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12062', 'logps_train/rejected': '-120.86', 'logps_train/chosen': '-165.12', 'loss/train': '0.64264', 'examples_per_second': '32.54', 'grad_norm': '30.125', 'counters/examples': 123808, 'counters/updates': 3869}
train stats after 123840 examples: {'rewards_train/chosen': '0.087736', 'rewards_train/rejected': '0.083344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0043918', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-119.74', 'loss/train': '0.70538', 'examples_per_second': '30.618', 'grad_norm': '38.5', 'counters/examples': 123840, 'counters/updates': 3870}
train stats after 123872 examples: {'rewards_train/chosen': '0.10677', 'rewards_train/rejected': '0.00084283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10593', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-134.55', 'loss/train': '0.65208', 'examples_per_second': '30.493', 'grad_norm': '28.25', 'counters/examples': 123872, 'counters/updates': 3871}
train stats after 123904 examples: {'rewards_train/chosen': '0.092032', 'rewards_train/rejected': '-0.0090577', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10109', 'logps_train/rejected': '-129.68', 'logps_train/chosen': '-156.59', 'loss/train': '0.65136', 'examples_per_second': '31.817', 'grad_norm': '32.5', 'counters/examples': 123904, 'counters/updates': 3872}
skipping logging after 123936 examples to avoid logging too frequently
train stats after 123968 examples: {'rewards_train/chosen': '0.19388', 'rewards_train/rejected': '0.13496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058924', 'logps_train/rejected': '-112.27', 'logps_train/chosen': '-130.01', 'loss/train': '0.67706', 'examples_per_second': '33.34', 'grad_norm': '41.5', 'counters/examples': 123968, 'counters/updates': 3874}
train stats after 124000 examples: {'rewards_train/chosen': '0.083327', 'rewards_train/rejected': '-0.018875', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1022', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-150.21', 'loss/train': '0.65198', 'examples_per_second': '29.983', 'grad_norm': '34.5', 'counters/examples': 124000, 'counters/updates': 3875}
skipping logging after 124032 examples to avoid logging too frequently
train stats after 124064 examples: {'rewards_train/chosen': '0.13846', 'rewards_train/rejected': '0.028812', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10964', 'logps_train/rejected': '-159.03', 'logps_train/chosen': '-175.16', 'loss/train': '0.64948', 'examples_per_second': '31.448', 'grad_norm': '31.25', 'counters/examples': 124064, 'counters/updates': 3877}
train stats after 124096 examples: {'rewards_train/chosen': '0.16512', 'rewards_train/rejected': '0.017369', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14775', 'logps_train/rejected': '-135.19', 'logps_train/chosen': '-149.13', 'loss/train': '0.63326', 'examples_per_second': '32.496', 'grad_norm': '33', 'counters/examples': 124096, 'counters/updates': 3878}
train stats after 124128 examples: {'rewards_train/chosen': '0.13857', 'rewards_train/rejected': '0.025958', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11261', 'logps_train/rejected': '-126', 'logps_train/chosen': '-144.78', 'loss/train': '0.64564', 'examples_per_second': '31.415', 'grad_norm': '31.25', 'counters/examples': 124128, 'counters/updates': 3879}
train stats after 124160 examples: {'rewards_train/chosen': '0.19885', 'rewards_train/rejected': '0.098092', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10076', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-180.63', 'loss/train': '0.65395', 'examples_per_second': '30.366', 'grad_norm': '37', 'counters/examples': 124160, 'counters/updates': 3880}
train stats after 124192 examples: {'rewards_train/chosen': '0.097502', 'rewards_train/rejected': '0.052605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044897', 'logps_train/rejected': '-143.41', 'logps_train/chosen': '-161.47', 'loss/train': '0.68118', 'examples_per_second': '32.283', 'grad_norm': '34.75', 'counters/examples': 124192, 'counters/updates': 3881}
train stats after 124224 examples: {'rewards_train/chosen': '0.15246', 'rewards_train/rejected': '0.095468', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056991', 'logps_train/rejected': '-126.8', 'logps_train/chosen': '-178.39', 'loss/train': '0.67098', 'examples_per_second': '32.869', 'grad_norm': '30.75', 'counters/examples': 124224, 'counters/updates': 3882}
train stats after 124256 examples: {'rewards_train/chosen': '0.14278', 'rewards_train/rejected': '0.03103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11176', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-188.54', 'loss/train': '0.64745', 'examples_per_second': '31.393', 'grad_norm': '34.5', 'counters/examples': 124256, 'counters/updates': 3883}
train stats after 124288 examples: {'rewards_train/chosen': '0.10891', 'rewards_train/rejected': '0.010599', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09831', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-142.13', 'loss/train': '0.65272', 'examples_per_second': '31.397', 'grad_norm': '28.375', 'counters/examples': 124288, 'counters/updates': 3884}
train stats after 124320 examples: {'rewards_train/chosen': '0.080501', 'rewards_train/rejected': '0.032056', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048445', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-178.48', 'loss/train': '0.67811', 'examples_per_second': '31.405', 'grad_norm': '38', 'counters/examples': 124320, 'counters/updates': 3885}
train stats after 124352 examples: {'rewards_train/chosen': '0.16969', 'rewards_train/rejected': '0.044947', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12474', 'logps_train/rejected': '-101.59', 'logps_train/chosen': '-139.74', 'loss/train': '0.65318', 'examples_per_second': '29.911', 'grad_norm': '28.5', 'counters/examples': 124352, 'counters/updates': 3886}
skipping logging after 124384 examples to avoid logging too frequently
train stats after 124416 examples: {'rewards_train/chosen': '0.24872', 'rewards_train/rejected': '0.048803', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19991', 'logps_train/rejected': '-118.28', 'logps_train/chosen': '-173.23', 'loss/train': '0.61229', 'examples_per_second': '33.695', 'grad_norm': '32.25', 'counters/examples': 124416, 'counters/updates': 3888}
train stats after 124448 examples: {'rewards_train/chosen': '0.09652', 'rewards_train/rejected': '0.053523', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.042997', 'logps_train/rejected': '-157.06', 'logps_train/chosen': '-128.49', 'loss/train': '0.67977', 'examples_per_second': '31.34', 'grad_norm': '37.75', 'counters/examples': 124448, 'counters/updates': 3889}
train stats after 124480 examples: {'rewards_train/chosen': '0.079378', 'rewards_train/rejected': '0.063764', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.015614', 'logps_train/rejected': '-126.63', 'logps_train/chosen': '-156.02', 'loss/train': '0.69193', 'examples_per_second': '32.317', 'grad_norm': '57.5', 'counters/examples': 124480, 'counters/updates': 3890}
train stats after 124512 examples: {'rewards_train/chosen': '0.16046', 'rewards_train/rejected': '0.11582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044639', 'logps_train/rejected': '-153.54', 'logps_train/chosen': '-106.22', 'loss/train': '0.68314', 'examples_per_second': '32.215', 'grad_norm': '30.125', 'counters/examples': 124512, 'counters/updates': 3891}
skipping logging after 124544 examples to avoid logging too frequently
train stats after 124576 examples: {'rewards_train/chosen': '0.077434', 'rewards_train/rejected': '0.021832', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055602', 'logps_train/rejected': '-112.68', 'logps_train/chosen': '-135.7', 'loss/train': '0.6717', 'examples_per_second': '26.632', 'grad_norm': '31.75', 'counters/examples': 124576, 'counters/updates': 3893}
train stats after 124608 examples: {'rewards_train/chosen': '0.058299', 'rewards_train/rejected': '-0.072291', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13059', 'logps_train/rejected': '-134.67', 'logps_train/chosen': '-139.14', 'loss/train': '0.6407', 'examples_per_second': '30.78', 'grad_norm': '39.5', 'counters/examples': 124608, 'counters/updates': 3894}
train stats after 124640 examples: {'rewards_train/chosen': '0.047012', 'rewards_train/rejected': '0.075471', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.028458', 'logps_train/rejected': '-188.63', 'logps_train/chosen': '-141.36', 'loss/train': '0.72122', 'examples_per_second': '33.011', 'grad_norm': '39.5', 'counters/examples': 124640, 'counters/updates': 3895}
train stats after 124672 examples: {'rewards_train/chosen': '0.14008', 'rewards_train/rejected': '0.13993', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.00014894', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-156.5', 'loss/train': '0.6993', 'examples_per_second': '23.788', 'grad_norm': '31.25', 'counters/examples': 124672, 'counters/updates': 3896}
train stats after 124704 examples: {'rewards_train/chosen': '0.11838', 'rewards_train/rejected': '0.029936', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088448', 'logps_train/rejected': '-119.09', 'logps_train/chosen': '-148.64', 'loss/train': '0.65592', 'examples_per_second': '30.498', 'grad_norm': '29.25', 'counters/examples': 124704, 'counters/updates': 3897}
train stats after 124736 examples: {'rewards_train/chosen': '0.12856', 'rewards_train/rejected': '0.070319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058246', 'logps_train/rejected': '-150.91', 'logps_train/chosen': '-175.25', 'loss/train': '0.67357', 'examples_per_second': '30.354', 'grad_norm': '34.5', 'counters/examples': 124736, 'counters/updates': 3898}
train stats after 124768 examples: {'rewards_train/chosen': '0.16269', 'rewards_train/rejected': '0.06596', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096729', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-131.72', 'loss/train': '0.65561', 'examples_per_second': '30.702', 'grad_norm': '23.625', 'counters/examples': 124768, 'counters/updates': 3899}
skipping logging after 124800 examples to avoid logging too frequently
train stats after 124832 examples: {'rewards_train/chosen': '0.10578', 'rewards_train/rejected': '0.069961', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035823', 'logps_train/rejected': '-130.69', 'logps_train/chosen': '-133.72', 'loss/train': '0.68431', 'examples_per_second': '32.133', 'grad_norm': '27.375', 'counters/examples': 124832, 'counters/updates': 3901}
train stats after 124864 examples: {'rewards_train/chosen': '0.10344', 'rewards_train/rejected': '0.047155', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056285', 'logps_train/rejected': '-116.38', 'logps_train/chosen': '-194.97', 'loss/train': '0.67577', 'examples_per_second': '31.327', 'grad_norm': '35.25', 'counters/examples': 124864, 'counters/updates': 3902}
train stats after 124896 examples: {'rewards_train/chosen': '0.10757', 'rewards_train/rejected': '0.028308', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07926', 'logps_train/rejected': '-133.82', 'logps_train/chosen': '-143.67', 'loss/train': '0.66277', 'examples_per_second': '31.391', 'grad_norm': '28.875', 'counters/examples': 124896, 'counters/updates': 3903}
train stats after 124928 examples: {'rewards_train/chosen': '0.12098', 'rewards_train/rejected': '0.082253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038732', 'logps_train/rejected': '-127.11', 'logps_train/chosen': '-118.56', 'loss/train': '0.68566', 'examples_per_second': '31.453', 'grad_norm': '27.75', 'counters/examples': 124928, 'counters/updates': 3904}
train stats after 124960 examples: {'rewards_train/chosen': '0.15623', 'rewards_train/rejected': '0.10871', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047515', 'logps_train/rejected': '-133.71', 'logps_train/chosen': '-141.3', 'loss/train': '0.67584', 'examples_per_second': '33.029', 'grad_norm': '35.5', 'counters/examples': 124960, 'counters/updates': 3905}
train stats after 124992 examples: {'rewards_train/chosen': '0.15413', 'rewards_train/rejected': '0.10133', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052802', 'logps_train/rejected': '-145.21', 'logps_train/chosen': '-169.8', 'loss/train': '0.69144', 'examples_per_second': '31.443', 'grad_norm': '43', 'counters/examples': 124992, 'counters/updates': 3906}
train stats after 125024 examples: {'rewards_train/chosen': '0.13522', 'rewards_train/rejected': '0.063423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071794', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-123.1', 'loss/train': '0.66769', 'examples_per_second': '31.463', 'grad_norm': '78', 'counters/examples': 125024, 'counters/updates': 3907}
train stats after 125056 examples: {'rewards_train/chosen': '0.14187', 'rewards_train/rejected': '0.056299', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.085575', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-171.77', 'loss/train': '0.66696', 'examples_per_second': '31.35', 'grad_norm': '33.25', 'counters/examples': 125056, 'counters/updates': 3908}
train stats after 125088 examples: {'rewards_train/chosen': '0.16866', 'rewards_train/rejected': '0.065073', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10359', 'logps_train/rejected': '-135.98', 'logps_train/chosen': '-140.96', 'loss/train': '0.65432', 'examples_per_second': '30.977', 'grad_norm': '31.125', 'counters/examples': 125088, 'counters/updates': 3909}
train stats after 125120 examples: {'rewards_train/chosen': '0.21872', 'rewards_train/rejected': '0.12138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097344', 'logps_train/rejected': '-122.25', 'logps_train/chosen': '-186.03', 'loss/train': '0.66018', 'examples_per_second': '29.932', 'grad_norm': '35.25', 'counters/examples': 125120, 'counters/updates': 3910}
train stats after 125152 examples: {'rewards_train/chosen': '0.23578', 'rewards_train/rejected': '0.12655', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10923', 'logps_train/rejected': '-139.87', 'logps_train/chosen': '-186.01', 'loss/train': '0.65515', 'examples_per_second': '30.04', 'grad_norm': '44.75', 'counters/examples': 125152, 'counters/updates': 3911}
skipping logging after 125184 examples to avoid logging too frequently
train stats after 125216 examples: {'rewards_train/chosen': '0.1652', 'rewards_train/rejected': '0.10563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059564', 'logps_train/rejected': '-117.25', 'logps_train/chosen': '-134.14', 'loss/train': '0.67808', 'examples_per_second': '34.196', 'grad_norm': '32.25', 'counters/examples': 125216, 'counters/updates': 3913}
train stats after 125248 examples: {'rewards_train/chosen': '0.14377', 'rewards_train/rejected': '0.10907', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.034695', 'logps_train/rejected': '-131.14', 'logps_train/chosen': '-153.44', 'loss/train': '0.70705', 'examples_per_second': '32.108', 'grad_norm': '46.75', 'counters/examples': 125248, 'counters/updates': 3914}
train stats after 125280 examples: {'rewards_train/chosen': '0.20094', 'rewards_train/rejected': '0.09646', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10448', 'logps_train/rejected': '-108.64', 'logps_train/chosen': '-126.4', 'loss/train': '0.65089', 'examples_per_second': '30.997', 'grad_norm': '25.125', 'counters/examples': 125280, 'counters/updates': 3915}
skipping logging after 125312 examples to avoid logging too frequently
train stats after 125344 examples: {'rewards_train/chosen': '0.16532', 'rewards_train/rejected': '0.03855', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12677', 'logps_train/rejected': '-131.89', 'logps_train/chosen': '-148.44', 'loss/train': '0.64158', 'examples_per_second': '33.835', 'grad_norm': '37', 'counters/examples': 125344, 'counters/updates': 3917}
train stats after 125376 examples: {'rewards_train/chosen': '0.097171', 'rewards_train/rejected': '0.038836', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058335', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-158.23', 'loss/train': '0.67444', 'examples_per_second': '33.238', 'grad_norm': '33.75', 'counters/examples': 125376, 'counters/updates': 3918}
skipping logging after 125408 examples to avoid logging too frequently
train stats after 125440 examples: {'rewards_train/chosen': '0.11821', 'rewards_train/rejected': '0.042888', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075322', 'logps_train/rejected': '-96.299', 'logps_train/chosen': '-153.04', 'loss/train': '0.66322', 'examples_per_second': '30.414', 'grad_norm': '29.5', 'counters/examples': 125440, 'counters/updates': 3920}
train stats after 125472 examples: {'rewards_train/chosen': '0.11989', 'rewards_train/rejected': '0.10749', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012395', 'logps_train/rejected': '-125.74', 'logps_train/chosen': '-127.22', 'loss/train': '0.70318', 'examples_per_second': '31.352', 'grad_norm': '35', 'counters/examples': 125472, 'counters/updates': 3921}
train stats after 125504 examples: {'rewards_train/chosen': '0.11728', 'rewards_train/rejected': '0.039275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078006', 'logps_train/rejected': '-104.64', 'logps_train/chosen': '-104.6', 'loss/train': '0.66263', 'examples_per_second': '30.047', 'grad_norm': '24.625', 'counters/examples': 125504, 'counters/updates': 3922}
train stats after 125536 examples: {'rewards_train/chosen': '0.15909', 'rewards_train/rejected': '0.086329', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.072761', 'logps_train/rejected': '-104.95', 'logps_train/chosen': '-126.3', 'loss/train': '0.6637', 'examples_per_second': '31.294', 'grad_norm': '23.125', 'counters/examples': 125536, 'counters/updates': 3923}
train stats after 125568 examples: {'rewards_train/chosen': '0.10165', 'rewards_train/rejected': '0.029261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072387', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-158.29', 'loss/train': '0.66295', 'examples_per_second': '30.319', 'grad_norm': '35', 'counters/examples': 125568, 'counters/updates': 3924}
train stats after 125600 examples: {'rewards_train/chosen': '0.081811', 'rewards_train/rejected': '0.046078', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035733', 'logps_train/rejected': '-112.2', 'logps_train/chosen': '-144.63', 'loss/train': '0.68019', 'examples_per_second': '31.498', 'grad_norm': '36', 'counters/examples': 125600, 'counters/updates': 3925}
train stats after 125632 examples: {'rewards_train/chosen': '0.1269', 'rewards_train/rejected': '0.033116', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09378', 'logps_train/rejected': '-99.043', 'logps_train/chosen': '-168.72', 'loss/train': '0.6584', 'examples_per_second': '30.11', 'grad_norm': '28.25', 'counters/examples': 125632, 'counters/updates': 3926}
train stats after 125664 examples: {'rewards_train/chosen': '0.044453', 'rewards_train/rejected': '0.011776', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032677', 'logps_train/rejected': '-139.81', 'logps_train/chosen': '-143.76', 'loss/train': '0.68372', 'examples_per_second': '29.823', 'grad_norm': '39', 'counters/examples': 125664, 'counters/updates': 3927}
train stats after 125696 examples: {'rewards_train/chosen': '0.13126', 'rewards_train/rejected': '0.10152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029746', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-131.55', 'loss/train': '0.6899', 'examples_per_second': '31.519', 'grad_norm': '29.5', 'counters/examples': 125696, 'counters/updates': 3928}
train stats after 125728 examples: {'rewards_train/chosen': '0.10761', 'rewards_train/rejected': '0.069842', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037773', 'logps_train/rejected': '-149.36', 'logps_train/chosen': '-171.1', 'loss/train': '0.689', 'examples_per_second': '31.367', 'grad_norm': '32.25', 'counters/examples': 125728, 'counters/updates': 3929}
train stats after 125760 examples: {'rewards_train/chosen': '0.25966', 'rewards_train/rejected': '0.067113', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19255', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-159.02', 'loss/train': '0.61173', 'examples_per_second': '31.299', 'grad_norm': '26.75', 'counters/examples': 125760, 'counters/updates': 3930}
train stats after 125792 examples: {'rewards_train/chosen': '0.148', 'rewards_train/rejected': '0.11283', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035163', 'logps_train/rejected': '-158.65', 'logps_train/chosen': '-175.51', 'loss/train': '0.68948', 'examples_per_second': '31.013', 'grad_norm': '36.5', 'counters/examples': 125792, 'counters/updates': 3931}
train stats after 125824 examples: {'rewards_train/chosen': '0.11066', 'rewards_train/rejected': '0.0039483', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10671', 'logps_train/rejected': '-142.74', 'logps_train/chosen': '-145.63', 'loss/train': '0.64775', 'examples_per_second': '32.055', 'grad_norm': '31.75', 'counters/examples': 125824, 'counters/updates': 3932}
train stats after 125856 examples: {'rewards_train/chosen': '0.09868', 'rewards_train/rejected': '0.041974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056706', 'logps_train/rejected': '-142.18', 'logps_train/chosen': '-138.51', 'loss/train': '0.66899', 'examples_per_second': '31.506', 'grad_norm': '32', 'counters/examples': 125856, 'counters/updates': 3933}
skipping logging after 125888 examples to avoid logging too frequently
train stats after 125920 examples: {'rewards_train/chosen': '0.25591', 'rewards_train/rejected': '0.065134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19078', 'logps_train/rejected': '-125.42', 'logps_train/chosen': '-187.84', 'loss/train': '0.63461', 'examples_per_second': '30.386', 'grad_norm': '34.75', 'counters/examples': 125920, 'counters/updates': 3935}
train stats after 125952 examples: {'rewards_train/chosen': '0.11259', 'rewards_train/rejected': '0.067624', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044963', 'logps_train/rejected': '-126.29', 'logps_train/chosen': '-157.22', 'loss/train': '0.67417', 'examples_per_second': '31.191', 'grad_norm': '41.25', 'counters/examples': 125952, 'counters/updates': 3936}
train stats after 125984 examples: {'rewards_train/chosen': '0.14741', 'rewards_train/rejected': '0.028909', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1185', 'logps_train/rejected': '-95.891', 'logps_train/chosen': '-120.72', 'loss/train': '0.64313', 'examples_per_second': '32.873', 'grad_norm': '29.25', 'counters/examples': 125984, 'counters/updates': 3937}
train stats after 126016 examples: {'rewards_train/chosen': '0.10857', 'rewards_train/rejected': '0.037503', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071064', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-139.39', 'loss/train': '0.66458', 'examples_per_second': '31.571', 'grad_norm': '31.375', 'counters/examples': 126016, 'counters/updates': 3938}
train stats after 126048 examples: {'rewards_train/chosen': '0.088981', 'rewards_train/rejected': '0.10226', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.013284', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-130.67', 'loss/train': '0.7052', 'examples_per_second': '30.87', 'grad_norm': '31', 'counters/examples': 126048, 'counters/updates': 3939}
train stats after 126080 examples: {'rewards_train/chosen': '0.13588', 'rewards_train/rejected': '0.12489', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010986', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-187.08', 'loss/train': '0.69886', 'examples_per_second': '31.077', 'grad_norm': '42.5', 'counters/examples': 126080, 'counters/updates': 3940}
skipping logging after 126112 examples to avoid logging too frequently
train stats after 126144 examples: {'rewards_train/chosen': '0.169', 'rewards_train/rejected': '0.033648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13535', 'logps_train/rejected': '-121.21', 'logps_train/chosen': '-159.24', 'loss/train': '0.64197', 'examples_per_second': '29.889', 'grad_norm': '28.5', 'counters/examples': 126144, 'counters/updates': 3942}
train stats after 126176 examples: {'rewards_train/chosen': '0.045609', 'rewards_train/rejected': '0.096271', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.050662', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-118.18', 'loss/train': '0.73352', 'examples_per_second': '32.037', 'grad_norm': '40', 'counters/examples': 126176, 'counters/updates': 3943}
train stats after 126208 examples: {'rewards_train/chosen': '0.074082', 'rewards_train/rejected': '0.051803', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.022279', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-132.27', 'loss/train': '0.69049', 'examples_per_second': '33.008', 'grad_norm': '31.75', 'counters/examples': 126208, 'counters/updates': 3944}
train stats after 126240 examples: {'rewards_train/chosen': '0.11018', 'rewards_train/rejected': '0.092766', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017418', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-144.14', 'loss/train': '0.6923', 'examples_per_second': '31.442', 'grad_norm': '29', 'counters/examples': 126240, 'counters/updates': 3945}
train stats after 126272 examples: {'rewards_train/chosen': '0.14692', 'rewards_train/rejected': '0.12759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019332', 'logps_train/rejected': '-138.93', 'logps_train/chosen': '-173.24', 'loss/train': '0.70541', 'examples_per_second': '30.228', 'grad_norm': '63', 'counters/examples': 126272, 'counters/updates': 3946}
train stats after 126304 examples: {'rewards_train/chosen': '0.18762', 'rewards_train/rejected': '0.16817', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01945', 'logps_train/rejected': '-154.55', 'logps_train/chosen': '-197.05', 'loss/train': '0.71304', 'examples_per_second': '30.302', 'grad_norm': '175', 'counters/examples': 126304, 'counters/updates': 3947}
train stats after 126336 examples: {'rewards_train/chosen': '0.093151', 'rewards_train/rejected': '0.12751', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.034361', 'logps_train/rejected': '-161.72', 'logps_train/chosen': '-116.95', 'loss/train': '0.71717', 'examples_per_second': '30.453', 'grad_norm': '32.5', 'counters/examples': 126336, 'counters/updates': 3948}
train stats after 126368 examples: {'rewards_train/chosen': '0.22273', 'rewards_train/rejected': '0.036861', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18587', 'logps_train/rejected': '-145.95', 'logps_train/chosen': '-193.46', 'loss/train': '0.61225', 'examples_per_second': '30.284', 'grad_norm': '31.25', 'counters/examples': 126368, 'counters/updates': 3949}
train stats after 126400 examples: {'rewards_train/chosen': '0.13458', 'rewards_train/rejected': '0.09308', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041504', 'logps_train/rejected': '-115.89', 'logps_train/chosen': '-189.44', 'loss/train': '0.69212', 'examples_per_second': '30.558', 'grad_norm': '60.75', 'counters/examples': 126400, 'counters/updates': 3950}
train stats after 126432 examples: {'rewards_train/chosen': '0.10034', 'rewards_train/rejected': '0.0886', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.011736', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-159.49', 'loss/train': '0.70832', 'examples_per_second': '30.991', 'grad_norm': '50', 'counters/examples': 126432, 'counters/updates': 3951}
train stats after 126464 examples: {'rewards_train/chosen': '0.15244', 'rewards_train/rejected': '0.078166', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.074278', 'logps_train/rejected': '-165.91', 'logps_train/chosen': '-171', 'loss/train': '0.667', 'examples_per_second': '31.415', 'grad_norm': '41.75', 'counters/examples': 126464, 'counters/updates': 3952}
train stats after 126496 examples: {'rewards_train/chosen': '0.12365', 'rewards_train/rejected': '0.099174', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024476', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-127.88', 'loss/train': '0.68775', 'examples_per_second': '31.362', 'grad_norm': '34', 'counters/examples': 126496, 'counters/updates': 3953}
train stats after 126528 examples: {'rewards_train/chosen': '0.089659', 'rewards_train/rejected': '0.045219', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04444', 'logps_train/rejected': '-94.237', 'logps_train/chosen': '-132.72', 'loss/train': '0.68095', 'examples_per_second': '31.469', 'grad_norm': '28.625', 'counters/examples': 126528, 'counters/updates': 3954}
train stats after 126560 examples: {'rewards_train/chosen': '0.12298', 'rewards_train/rejected': '0.092551', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.030433', 'logps_train/rejected': '-114.75', 'logps_train/chosen': '-121.03', 'loss/train': '0.68298', 'examples_per_second': '31.073', 'grad_norm': '27.375', 'counters/examples': 126560, 'counters/updates': 3955}
train stats after 126592 examples: {'rewards_train/chosen': '0.14756', 'rewards_train/rejected': '0.056875', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090684', 'logps_train/rejected': '-97.123', 'logps_train/chosen': '-123.22', 'loss/train': '0.65458', 'examples_per_second': '30.453', 'grad_norm': '26.125', 'counters/examples': 126592, 'counters/updates': 3956}
train stats after 126624 examples: {'rewards_train/chosen': '0.064348', 'rewards_train/rejected': '0.12869', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.064343', 'logps_train/rejected': '-110.86', 'logps_train/chosen': '-134.52', 'loss/train': '0.74044', 'examples_per_second': '30.101', 'grad_norm': '37.75', 'counters/examples': 126624, 'counters/updates': 3957}
train stats after 126656 examples: {'rewards_train/chosen': '0.14318', 'rewards_train/rejected': '0.090126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053057', 'logps_train/rejected': '-163.98', 'logps_train/chosen': '-161.48', 'loss/train': '0.67486', 'examples_per_second': '32.155', 'grad_norm': '32', 'counters/examples': 126656, 'counters/updates': 3958}
skipping logging after 126688 examples to avoid logging too frequently
train stats after 126720 examples: {'rewards_train/chosen': '0.13142', 'rewards_train/rejected': '0.11026', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021163', 'logps_train/rejected': '-146.18', 'logps_train/chosen': '-154.54', 'loss/train': '0.69744', 'examples_per_second': '35.381', 'grad_norm': '29.125', 'counters/examples': 126720, 'counters/updates': 3960}
skipping logging after 126752 examples to avoid logging too frequently
train stats after 126784 examples: {'rewards_train/chosen': '0.10414', 'rewards_train/rejected': '0.062671', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04147', 'logps_train/rejected': '-100.1', 'logps_train/chosen': '-116.64', 'loss/train': '0.67813', 'examples_per_second': '31.27', 'grad_norm': '27.125', 'counters/examples': 126784, 'counters/updates': 3962}
train stats after 126816 examples: {'rewards_train/chosen': '0.088655', 'rewards_train/rejected': '0.037566', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051088', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-140.81', 'loss/train': '0.68191', 'examples_per_second': '30.51', 'grad_norm': '37.25', 'counters/examples': 126816, 'counters/updates': 3963}
train stats after 126848 examples: {'rewards_train/chosen': '0.13371', 'rewards_train/rejected': '0.098411', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035299', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-152.02', 'loss/train': '0.68304', 'examples_per_second': '30.4', 'grad_norm': '30.875', 'counters/examples': 126848, 'counters/updates': 3964}
train stats after 126880 examples: {'rewards_train/chosen': '0.16012', 'rewards_train/rejected': '0.06319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096928', 'logps_train/rejected': '-99.493', 'logps_train/chosen': '-150.24', 'loss/train': '0.6571', 'examples_per_second': '31.439', 'grad_norm': '24.5', 'counters/examples': 126880, 'counters/updates': 3965}
train stats after 126912 examples: {'rewards_train/chosen': '0.11059', 'rewards_train/rejected': '0.042393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068196', 'logps_train/rejected': '-137.05', 'logps_train/chosen': '-105.23', 'loss/train': '0.66533', 'examples_per_second': '31.171', 'grad_norm': '27.875', 'counters/examples': 126912, 'counters/updates': 3966}
train stats after 126944 examples: {'rewards_train/chosen': '0.09821', 'rewards_train/rejected': '0.053045', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045165', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-135.09', 'loss/train': '0.67741', 'examples_per_second': '24.539', 'grad_norm': '26.125', 'counters/examples': 126944, 'counters/updates': 3967}
train stats after 126976 examples: {'rewards_train/chosen': '0.18551', 'rewards_train/rejected': '0.12044', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065074', 'logps_train/rejected': '-118.24', 'logps_train/chosen': '-143.82', 'loss/train': '0.66927', 'examples_per_second': '31.145', 'grad_norm': '27.125', 'counters/examples': 126976, 'counters/updates': 3968}
train stats after 127008 examples: {'rewards_train/chosen': '0.09919', 'rewards_train/rejected': '0.084933', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014257', 'logps_train/rejected': '-113.54', 'logps_train/chosen': '-146.4', 'loss/train': '0.69334', 'examples_per_second': '32.927', 'grad_norm': '36.25', 'counters/examples': 127008, 'counters/updates': 3969}
train stats after 127040 examples: {'rewards_train/chosen': '0.10928', 'rewards_train/rejected': '0.04467', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064606', 'logps_train/rejected': '-103.49', 'logps_train/chosen': '-128.06', 'loss/train': '0.66989', 'examples_per_second': '30.986', 'grad_norm': '31', 'counters/examples': 127040, 'counters/updates': 3970}
train stats after 127072 examples: {'rewards_train/chosen': '0.20362', 'rewards_train/rejected': '0.12616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077456', 'logps_train/rejected': '-159.14', 'logps_train/chosen': '-188.14', 'loss/train': '0.6823', 'examples_per_second': '31.547', 'grad_norm': '35.75', 'counters/examples': 127072, 'counters/updates': 3971}
train stats after 127104 examples: {'rewards_train/chosen': '0.20456', 'rewards_train/rejected': '0.098975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10559', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-130.74', 'loss/train': '0.69097', 'examples_per_second': '30.348', 'grad_norm': '58.5', 'counters/examples': 127104, 'counters/updates': 3972}
train stats after 127136 examples: {'rewards_train/chosen': '0.12873', 'rewards_train/rejected': '0.096127', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032605', 'logps_train/rejected': '-97.775', 'logps_train/chosen': '-167.77', 'loss/train': '0.68731', 'examples_per_second': '32.462', 'grad_norm': '32.5', 'counters/examples': 127136, 'counters/updates': 3973}
skipping logging after 127168 examples to avoid logging too frequently
train stats after 127200 examples: {'rewards_train/chosen': '0.085809', 'rewards_train/rejected': '0.089992', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0041827', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-124.46', 'loss/train': '0.70599', 'examples_per_second': '29.857', 'grad_norm': '31.25', 'counters/examples': 127200, 'counters/updates': 3975}
train stats after 127232 examples: {'rewards_train/chosen': '0.12946', 'rewards_train/rejected': '0.051439', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.078017', 'logps_train/rejected': '-95.271', 'logps_train/chosen': '-176.32', 'loss/train': '0.67241', 'examples_per_second': '30.608', 'grad_norm': '51.75', 'counters/examples': 127232, 'counters/updates': 3976}
train stats after 127264 examples: {'rewards_train/chosen': '0.11266', 'rewards_train/rejected': '0.066567', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046097', 'logps_train/rejected': '-145.33', 'logps_train/chosen': '-191.53', 'loss/train': '0.67729', 'examples_per_second': '31.393', 'grad_norm': '31.875', 'counters/examples': 127264, 'counters/updates': 3977}
train stats after 127296 examples: {'rewards_train/chosen': '0.11084', 'rewards_train/rejected': '-0.0040439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11489', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-168.54', 'loss/train': '0.65031', 'examples_per_second': '31.342', 'grad_norm': '27.375', 'counters/examples': 127296, 'counters/updates': 3978}
train stats after 127328 examples: {'rewards_train/chosen': '0.16993', 'rewards_train/rejected': '0.065626', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1043', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-144', 'loss/train': '0.65805', 'examples_per_second': '32.144', 'grad_norm': '28.875', 'counters/examples': 127328, 'counters/updates': 3979}
train stats after 127360 examples: {'rewards_train/chosen': '0.13962', 'rewards_train/rejected': '0.098096', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.041528', 'logps_train/rejected': '-111.63', 'logps_train/chosen': '-124.76', 'loss/train': '0.67974', 'examples_per_second': '30.86', 'grad_norm': '30.125', 'counters/examples': 127360, 'counters/updates': 3980}
train stats after 127392 examples: {'rewards_train/chosen': '0.18457', 'rewards_train/rejected': '0.030814', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15376', 'logps_train/rejected': '-163.3', 'logps_train/chosen': '-181.36', 'loss/train': '0.63447', 'examples_per_second': '31.438', 'grad_norm': '30.625', 'counters/examples': 127392, 'counters/updates': 3981}
train stats after 127424 examples: {'rewards_train/chosen': '0.094606', 'rewards_train/rejected': '0.16571', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.071106', 'logps_train/rejected': '-119.71', 'logps_train/chosen': '-170.48', 'loss/train': '0.738', 'examples_per_second': '30.236', 'grad_norm': '34.5', 'counters/examples': 127424, 'counters/updates': 3982}
skipping logging after 127456 examples to avoid logging too frequently
train stats after 127488 examples: {'rewards_train/chosen': '0.12953', 'rewards_train/rejected': '0.057061', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.072466', 'logps_train/rejected': '-114.46', 'logps_train/chosen': '-126.48', 'loss/train': '0.67058', 'examples_per_second': '29.891', 'grad_norm': '26', 'counters/examples': 127488, 'counters/updates': 3984}
train stats after 127520 examples: {'rewards_train/chosen': '0.11307', 'rewards_train/rejected': '0.12648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.013408', 'logps_train/rejected': '-154.23', 'logps_train/chosen': '-142.55', 'loss/train': '0.70676', 'examples_per_second': '32.139', 'grad_norm': '31.875', 'counters/examples': 127520, 'counters/updates': 3985}
train stats after 127552 examples: {'rewards_train/chosen': '0.086572', 'rewards_train/rejected': '0.038837', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047736', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-135.08', 'loss/train': '0.67856', 'examples_per_second': '31.252', 'grad_norm': '28.875', 'counters/examples': 127552, 'counters/updates': 3986}
skipping logging after 127584 examples to avoid logging too frequently
train stats after 127616 examples: {'rewards_train/chosen': '0.10502', 'rewards_train/rejected': '0.027228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07779', 'logps_train/rejected': '-123.83', 'logps_train/chosen': '-136.16', 'loss/train': '0.66006', 'examples_per_second': '30.21', 'grad_norm': '32', 'counters/examples': 127616, 'counters/updates': 3988}
skipping logging after 127648 examples to avoid logging too frequently
train stats after 127680 examples: {'rewards_train/chosen': '0.2301', 'rewards_train/rejected': '0.089375', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14073', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-135.43', 'loss/train': '0.65065', 'examples_per_second': '32.436', 'grad_norm': '29.75', 'counters/examples': 127680, 'counters/updates': 3990}
train stats after 127712 examples: {'rewards_train/chosen': '0.16346', 'rewards_train/rejected': '0.17031', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0068564', 'logps_train/rejected': '-147.06', 'logps_train/chosen': '-176.97', 'loss/train': '0.73048', 'examples_per_second': '31.333', 'grad_norm': '63.25', 'counters/examples': 127712, 'counters/updates': 3991}
train stats after 127744 examples: {'rewards_train/chosen': '0.20115', 'rewards_train/rejected': '0.065457', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1357', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-140.12', 'loss/train': '0.65725', 'examples_per_second': '30.028', 'grad_norm': '28.5', 'counters/examples': 127744, 'counters/updates': 3992}
train stats after 127776 examples: {'rewards_train/chosen': '0.070547', 'rewards_train/rejected': '0.042101', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028446', 'logps_train/rejected': '-125.95', 'logps_train/chosen': '-114.55', 'loss/train': '0.68523', 'examples_per_second': '31.437', 'grad_norm': '27.125', 'counters/examples': 127776, 'counters/updates': 3993}
train stats after 127808 examples: {'rewards_train/chosen': '0.12361', 'rewards_train/rejected': '0.050818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072789', 'logps_train/rejected': '-146.5', 'logps_train/chosen': '-146.49', 'loss/train': '0.66497', 'examples_per_second': '29.826', 'grad_norm': '31.125', 'counters/examples': 127808, 'counters/updates': 3994}
train stats after 127840 examples: {'rewards_train/chosen': '0.081958', 'rewards_train/rejected': '0.063125', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018833', 'logps_train/rejected': '-147.36', 'logps_train/chosen': '-175.1', 'loss/train': '0.68993', 'examples_per_second': '32.247', 'grad_norm': '31.25', 'counters/examples': 127840, 'counters/updates': 3995}
train stats after 127872 examples: {'rewards_train/chosen': '0.067069', 'rewards_train/rejected': '0.079276', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.012207', 'logps_train/rejected': '-126.81', 'logps_train/chosen': '-130.01', 'loss/train': '0.70872', 'examples_per_second': '31.504', 'grad_norm': '26', 'counters/examples': 127872, 'counters/updates': 3996}
train stats after 127904 examples: {'rewards_train/chosen': '0.077794', 'rewards_train/rejected': '0.063941', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013853', 'logps_train/rejected': '-86.803', 'logps_train/chosen': '-94.247', 'loss/train': '0.69038', 'examples_per_second': '32.477', 'grad_norm': '25.375', 'counters/examples': 127904, 'counters/updates': 3997}
train stats after 127936 examples: {'rewards_train/chosen': '0.066668', 'rewards_train/rejected': '0.044837', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021831', 'logps_train/rejected': '-158.13', 'logps_train/chosen': '-160.2', 'loss/train': '0.69268', 'examples_per_second': '33.068', 'grad_norm': '33.5', 'counters/examples': 127936, 'counters/updates': 3998}
train stats after 127968 examples: {'rewards_train/chosen': '0.19747', 'rewards_train/rejected': '0.10583', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091634', 'logps_train/rejected': '-114.82', 'logps_train/chosen': '-172.66', 'loss/train': '0.66186', 'examples_per_second': '31.51', 'grad_norm': '40.25', 'counters/examples': 127968, 'counters/updates': 3999}
train stats after 128000 examples: {'rewards_train/chosen': '0.1091', 'rewards_train/rejected': '0.13841', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029317', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-127.31', 'loss/train': '0.72473', 'examples_per_second': '30.482', 'grad_norm': '147', 'counters/examples': 128000, 'counters/updates': 4000}
skipping logging after 128032 examples to avoid logging too frequently
train stats after 128064 examples: {'rewards_train/chosen': '0.13054', 'rewards_train/rejected': '0.12877', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0017675', 'logps_train/rejected': '-153.76', 'logps_train/chosen': '-124.69', 'loss/train': '0.70536', 'examples_per_second': '31.947', 'grad_norm': '36.75', 'counters/examples': 128064, 'counters/updates': 4002}
train stats after 128096 examples: {'rewards_train/chosen': '0.19383', 'rewards_train/rejected': '0.11061', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083218', 'logps_train/rejected': '-170.59', 'logps_train/chosen': '-131.43', 'loss/train': '0.66757', 'examples_per_second': '31.464', 'grad_norm': '36', 'counters/examples': 128096, 'counters/updates': 4003}
train stats after 128128 examples: {'rewards_train/chosen': '0.079327', 'rewards_train/rejected': '0.048498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030829', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-115.98', 'loss/train': '0.6833', 'examples_per_second': '31.4', 'grad_norm': '28', 'counters/examples': 128128, 'counters/updates': 4004}
train stats after 128160 examples: {'rewards_train/chosen': '0.12452', 'rewards_train/rejected': '0.037743', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086775', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-120.45', 'loss/train': '0.66415', 'examples_per_second': '31.186', 'grad_norm': '27.25', 'counters/examples': 128160, 'counters/updates': 4005}
train stats after 128192 examples: {'rewards_train/chosen': '0.080207', 'rewards_train/rejected': '0.038576', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041631', 'logps_train/rejected': '-107.25', 'logps_train/chosen': '-131.9', 'loss/train': '0.67713', 'examples_per_second': '31.83', 'grad_norm': '28.875', 'counters/examples': 128192, 'counters/updates': 4006}
train stats after 128224 examples: {'rewards_train/chosen': '0.15129', 'rewards_train/rejected': '0.0069381', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14435', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-137.64', 'loss/train': '0.63387', 'examples_per_second': '31.364', 'grad_norm': '24.625', 'counters/examples': 128224, 'counters/updates': 4007}
train stats after 128256 examples: {'rewards_train/chosen': '0.1364', 'rewards_train/rejected': '0.10201', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034389', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-168.45', 'loss/train': '0.69626', 'examples_per_second': '31.5', 'grad_norm': '36.75', 'counters/examples': 128256, 'counters/updates': 4008}
skipping logging after 128288 examples to avoid logging too frequently
train stats after 128320 examples: {'rewards_train/chosen': '0.18662', 'rewards_train/rejected': '-0.0050435', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19166', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-176.17', 'loss/train': '0.62036', 'examples_per_second': '29.791', 'grad_norm': '44.5', 'counters/examples': 128320, 'counters/updates': 4010}
train stats after 128352 examples: {'rewards_train/chosen': '0.15703', 'rewards_train/rejected': '0.026907', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13013', 'logps_train/rejected': '-117.62', 'logps_train/chosen': '-121.02', 'loss/train': '0.6416', 'examples_per_second': '30.772', 'grad_norm': '25.75', 'counters/examples': 128352, 'counters/updates': 4011}
train stats after 128384 examples: {'rewards_train/chosen': '0.071817', 'rewards_train/rejected': '0.14298', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.071165', 'logps_train/rejected': '-151.51', 'logps_train/chosen': '-166.62', 'loss/train': '0.73828', 'examples_per_second': '31.394', 'grad_norm': '52.75', 'counters/examples': 128384, 'counters/updates': 4012}
train stats after 128416 examples: {'rewards_train/chosen': '0.17112', 'rewards_train/rejected': '0.071769', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099348', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-134.12', 'loss/train': '0.65531', 'examples_per_second': '31.472', 'grad_norm': '29.75', 'counters/examples': 128416, 'counters/updates': 4013}
train stats after 128448 examples: {'rewards_train/chosen': '0.034208', 'rewards_train/rejected': '0.024915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0092923', 'logps_train/rejected': '-140.72', 'logps_train/chosen': '-118.23', 'loss/train': '0.70351', 'examples_per_second': '30.735', 'grad_norm': '28.75', 'counters/examples': 128448, 'counters/updates': 4014}
skipping logging after 128480 examples to avoid logging too frequently
train stats after 128512 examples: {'rewards_train/chosen': '0.27532', 'rewards_train/rejected': '0.07202', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2033', 'logps_train/rejected': '-147.21', 'logps_train/chosen': '-152.77', 'loss/train': '0.61385', 'examples_per_second': '30.902', 'grad_norm': '27.875', 'counters/examples': 128512, 'counters/updates': 4016}
train stats after 128544 examples: {'rewards_train/chosen': '0.078653', 'rewards_train/rejected': '0.051167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.027486', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-134.34', 'loss/train': '0.69892', 'examples_per_second': '32.255', 'grad_norm': '54.25', 'counters/examples': 128544, 'counters/updates': 4017}
train stats after 128576 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '0.079062', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04395', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-146.5', 'loss/train': '0.68273', 'examples_per_second': '30.695', 'grad_norm': '33.5', 'counters/examples': 128576, 'counters/updates': 4018}
train stats after 128608 examples: {'rewards_train/chosen': '0.11885', 'rewards_train/rejected': '0.020308', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098546', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-130.04', 'loss/train': '0.65584', 'examples_per_second': '31.474', 'grad_norm': '29', 'counters/examples': 128608, 'counters/updates': 4019}
train stats after 128640 examples: {'rewards_train/chosen': '0.15651', 'rewards_train/rejected': '0.12649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03002', 'logps_train/rejected': '-145.99', 'logps_train/chosen': '-168.73', 'loss/train': '0.69023', 'examples_per_second': '30.126', 'grad_norm': '47', 'counters/examples': 128640, 'counters/updates': 4020}
train stats after 128672 examples: {'rewards_train/chosen': '0.11032', 'rewards_train/rejected': '0.11848', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0081618', 'logps_train/rejected': '-166.6', 'logps_train/chosen': '-161.82', 'loss/train': '0.70241', 'examples_per_second': '30.955', 'grad_norm': '34', 'counters/examples': 128672, 'counters/updates': 4021}
train stats after 128704 examples: {'rewards_train/chosen': '0.12419', 'rewards_train/rejected': '0.17674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.052544', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-120.01', 'loss/train': '0.76044', 'examples_per_second': '31.783', 'grad_norm': '47', 'counters/examples': 128704, 'counters/updates': 4022}
skipping logging after 128736 examples to avoid logging too frequently
train stats after 128768 examples: {'rewards_train/chosen': '0.16941', 'rewards_train/rejected': '0.016196', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15321', 'logps_train/rejected': '-145.99', 'logps_train/chosen': '-181.87', 'loss/train': '0.63291', 'examples_per_second': '31.161', 'grad_norm': '34.75', 'counters/examples': 128768, 'counters/updates': 4024}
train stats after 128800 examples: {'rewards_train/chosen': '0.12874', 'rewards_train/rejected': '0.0060283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12272', 'logps_train/rejected': '-101.06', 'logps_train/chosen': '-113.6', 'loss/train': '0.64713', 'examples_per_second': '30.673', 'grad_norm': '27.75', 'counters/examples': 128800, 'counters/updates': 4025}
train stats after 128832 examples: {'rewards_train/chosen': '0.11864', 'rewards_train/rejected': '0.049063', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069581', 'logps_train/rejected': '-152.57', 'logps_train/chosen': '-157.67', 'loss/train': '0.66815', 'examples_per_second': '31.479', 'grad_norm': '33', 'counters/examples': 128832, 'counters/updates': 4026}
skipping logging after 128864 examples to avoid logging too frequently
train stats after 128896 examples: {'rewards_train/chosen': '0.11832', 'rewards_train/rejected': '0.062943', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.055382', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-143.79', 'loss/train': '0.67899', 'examples_per_second': '31.825', 'grad_norm': '49', 'counters/examples': 128896, 'counters/updates': 4028}
train stats after 128928 examples: {'rewards_train/chosen': '0.10652', 'rewards_train/rejected': '0.053492', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053024', 'logps_train/rejected': '-160.97', 'logps_train/chosen': '-153.9', 'loss/train': '0.67354', 'examples_per_second': '30.66', 'grad_norm': '43.5', 'counters/examples': 128928, 'counters/updates': 4029}
train stats after 128960 examples: {'rewards_train/chosen': '0.085921', 'rewards_train/rejected': '0.11088', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02496', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-143.14', 'loss/train': '0.72071', 'examples_per_second': '32.012', 'grad_norm': '38.25', 'counters/examples': 128960, 'counters/updates': 4030}
train stats after 128992 examples: {'rewards_train/chosen': '0.10796', 'rewards_train/rejected': '0.049281', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.058675', 'logps_train/rejected': '-144.06', 'logps_train/chosen': '-181.47', 'loss/train': '0.67691', 'examples_per_second': '32.021', 'grad_norm': '39.5', 'counters/examples': 128992, 'counters/updates': 4031}
train stats after 129024 examples: {'rewards_train/chosen': '0.045194', 'rewards_train/rejected': '0.084699', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.039505', 'logps_train/rejected': '-115.52', 'logps_train/chosen': '-105.33', 'loss/train': '0.72082', 'examples_per_second': '31.061', 'grad_norm': '37.25', 'counters/examples': 129024, 'counters/updates': 4032}
train stats after 129056 examples: {'rewards_train/chosen': '0.11797', 'rewards_train/rejected': '0.059992', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05798', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-127.63', 'loss/train': '0.677', 'examples_per_second': '32.81', 'grad_norm': '44.75', 'counters/examples': 129056, 'counters/updates': 4033}
train stats after 129088 examples: {'rewards_train/chosen': '0.082726', 'rewards_train/rejected': '-0.00083238', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083559', 'logps_train/rejected': '-105.61', 'logps_train/chosen': '-141.89', 'loss/train': '0.66039', 'examples_per_second': '32.248', 'grad_norm': '31.125', 'counters/examples': 129088, 'counters/updates': 4034}
train stats after 129120 examples: {'rewards_train/chosen': '0.090465', 'rewards_train/rejected': '0.044791', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045674', 'logps_train/rejected': '-152.04', 'logps_train/chosen': '-133.65', 'loss/train': '0.68061', 'examples_per_second': '32.575', 'grad_norm': '33.75', 'counters/examples': 129120, 'counters/updates': 4035}
train stats after 129152 examples: {'rewards_train/chosen': '0.13161', 'rewards_train/rejected': '0.18036', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.04875', 'logps_train/rejected': '-105.92', 'logps_train/chosen': '-128.88', 'loss/train': '0.75615', 'examples_per_second': '31.475', 'grad_norm': '67.5', 'counters/examples': 129152, 'counters/updates': 4036}
train stats after 129184 examples: {'rewards_train/chosen': '0.08559', 'rewards_train/rejected': '0.028746', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056843', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-145.3', 'loss/train': '0.67428', 'examples_per_second': '31.396', 'grad_norm': '28.125', 'counters/examples': 129184, 'counters/updates': 4037}
skipping logging after 129216 examples to avoid logging too frequently
train stats after 129248 examples: {'rewards_train/chosen': '0.062277', 'rewards_train/rejected': '0.07005', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0077731', 'logps_train/rejected': '-140.19', 'logps_train/chosen': '-156.49', 'loss/train': '0.70602', 'examples_per_second': '34.237', 'grad_norm': '38.5', 'counters/examples': 129248, 'counters/updates': 4039}
train stats after 129280 examples: {'rewards_train/chosen': '0.078453', 'rewards_train/rejected': '0.051852', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.026601', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-125.42', 'loss/train': '0.69024', 'examples_per_second': '31.436', 'grad_norm': '27.25', 'counters/examples': 129280, 'counters/updates': 4040}
train stats after 129312 examples: {'rewards_train/chosen': '0.058878', 'rewards_train/rejected': '0.024513', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034365', 'logps_train/rejected': '-125.11', 'logps_train/chosen': '-146.16', 'loss/train': '0.68175', 'examples_per_second': '32.479', 'grad_norm': '28.25', 'counters/examples': 129312, 'counters/updates': 4041}
train stats after 129344 examples: {'rewards_train/chosen': '0.11938', 'rewards_train/rejected': '0.051934', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.067448', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-127.35', 'loss/train': '0.67321', 'examples_per_second': '29.731', 'grad_norm': '27.125', 'counters/examples': 129344, 'counters/updates': 4042}
train stats after 129376 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.028142', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086722', 'logps_train/rejected': '-126.35', 'logps_train/chosen': '-115.48', 'loss/train': '0.65784', 'examples_per_second': '30.932', 'grad_norm': '27', 'counters/examples': 129376, 'counters/updates': 4043}
train stats after 129408 examples: {'rewards_train/chosen': '0.15869', 'rewards_train/rejected': '0.11081', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.047883', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-155.21', 'loss/train': '0.67997', 'examples_per_second': '32.746', 'grad_norm': '33', 'counters/examples': 129408, 'counters/updates': 4044}
train stats after 129440 examples: {'rewards_train/chosen': '0.10781', 'rewards_train/rejected': '0.051072', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056742', 'logps_train/rejected': '-177.7', 'logps_train/chosen': '-129.09', 'loss/train': '0.67796', 'examples_per_second': '31.127', 'grad_norm': '32', 'counters/examples': 129440, 'counters/updates': 4045}
train stats after 129472 examples: {'rewards_train/chosen': '0.13865', 'rewards_train/rejected': '0.064043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07461', 'logps_train/rejected': '-123.49', 'logps_train/chosen': '-113.04', 'loss/train': '0.66673', 'examples_per_second': '31.441', 'grad_norm': '34.75', 'counters/examples': 129472, 'counters/updates': 4046}
train stats after 129504 examples: {'rewards_train/chosen': '0.16949', 'rewards_train/rejected': '0.089974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079515', 'logps_train/rejected': '-165.52', 'logps_train/chosen': '-166.07', 'loss/train': '0.66716', 'examples_per_second': '31.387', 'grad_norm': '64', 'counters/examples': 129504, 'counters/updates': 4047}
train stats after 129536 examples: {'rewards_train/chosen': '0.11237', 'rewards_train/rejected': '0.056252', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056117', 'logps_train/rejected': '-113.57', 'logps_train/chosen': '-155.05', 'loss/train': '0.69441', 'examples_per_second': '31.481', 'grad_norm': '34', 'counters/examples': 129536, 'counters/updates': 4048}
train stats after 129568 examples: {'rewards_train/chosen': '0.10278', 'rewards_train/rejected': '0.047312', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055469', 'logps_train/rejected': '-160.18', 'logps_train/chosen': '-123.61', 'loss/train': '0.6765', 'examples_per_second': '31.437', 'grad_norm': '30.75', 'counters/examples': 129568, 'counters/updates': 4049}
train stats after 129600 examples: {'rewards_train/chosen': '0.236', 'rewards_train/rejected': '0.21071', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.025287', 'logps_train/rejected': '-156.78', 'logps_train/chosen': '-184.95', 'loss/train': '0.69785', 'examples_per_second': '31.47', 'grad_norm': '30.875', 'counters/examples': 129600, 'counters/updates': 4050}
train stats after 129632 examples: {'rewards_train/chosen': '0.11126', 'rewards_train/rejected': '0.089502', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021757', 'logps_train/rejected': '-163.5', 'logps_train/chosen': '-160.26', 'loss/train': '0.69209', 'examples_per_second': '32.871', 'grad_norm': '35.5', 'counters/examples': 129632, 'counters/updates': 4051}
train stats after 129664 examples: {'rewards_train/chosen': '0.19333', 'rewards_train/rejected': '0.062145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13119', 'logps_train/rejected': '-128.32', 'logps_train/chosen': '-140.04', 'loss/train': '0.64806', 'examples_per_second': '30.597', 'grad_norm': '27', 'counters/examples': 129664, 'counters/updates': 4052}
train stats after 129696 examples: {'rewards_train/chosen': '0.15495', 'rewards_train/rejected': '0.087337', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067613', 'logps_train/rejected': '-120.7', 'logps_train/chosen': '-136.36', 'loss/train': '0.67262', 'examples_per_second': '31.271', 'grad_norm': '36.5', 'counters/examples': 129696, 'counters/updates': 4053}
skipping logging after 129728 examples to avoid logging too frequently
train stats after 129760 examples: {'rewards_train/chosen': '0.16734', 'rewards_train/rejected': '0.04904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1183', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-147.57', 'loss/train': '0.64161', 'examples_per_second': '31.591', 'grad_norm': '29.375', 'counters/examples': 129760, 'counters/updates': 4055}
train stats after 129792 examples: {'rewards_train/chosen': '0.16287', 'rewards_train/rejected': '0.082563', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080306', 'logps_train/rejected': '-178.15', 'logps_train/chosen': '-160.67', 'loss/train': '0.66291', 'examples_per_second': '31.064', 'grad_norm': '45.75', 'counters/examples': 129792, 'counters/updates': 4056}
train stats after 129824 examples: {'rewards_train/chosen': '0.23391', 'rewards_train/rejected': '0.11736', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11654', 'logps_train/rejected': '-151.65', 'logps_train/chosen': '-145.07', 'loss/train': '0.65373', 'examples_per_second': '31.429', 'grad_norm': '29.5', 'counters/examples': 129824, 'counters/updates': 4057}
skipping logging after 129856 examples to avoid logging too frequently
train stats after 129888 examples: {'rewards_train/chosen': '0.065557', 'rewards_train/rejected': '0.075263', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0097061', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-145.47', 'loss/train': '0.71233', 'examples_per_second': '33.964', 'grad_norm': '54', 'counters/examples': 129888, 'counters/updates': 4059}
skipping logging after 129920 examples to avoid logging too frequently
train stats after 129952 examples: {'rewards_train/chosen': '0.18589', 'rewards_train/rejected': '0.071847', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11405', 'logps_train/rejected': '-112.84', 'logps_train/chosen': '-126.12', 'loss/train': '0.65268', 'examples_per_second': '30.692', 'grad_norm': '27.75', 'counters/examples': 129952, 'counters/updates': 4061}
train stats after 129984 examples: {'rewards_train/chosen': '0.080443', 'rewards_train/rejected': '0.039361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041082', 'logps_train/rejected': '-90.559', 'logps_train/chosen': '-138.29', 'loss/train': '0.68037', 'examples_per_second': '31.974', 'grad_norm': '25.25', 'counters/examples': 129984, 'counters/updates': 4062}
train stats after 130016 examples: {'rewards_train/chosen': '0.08981', 'rewards_train/rejected': '0.088636', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0011736', 'logps_train/rejected': '-160.58', 'logps_train/chosen': '-127.78', 'loss/train': '0.70829', 'examples_per_second': '31.442', 'grad_norm': '53.5', 'counters/examples': 130016, 'counters/updates': 4063}
train stats after 130048 examples: {'rewards_train/chosen': '0.16971', 'rewards_train/rejected': '0.061141', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.10857', 'logps_train/rejected': '-119.49', 'logps_train/chosen': '-113.52', 'loss/train': '0.64666', 'examples_per_second': '23.26', 'grad_norm': '30.25', 'counters/examples': 130048, 'counters/updates': 4064}
train stats after 130080 examples: {'rewards_train/chosen': '0.047665', 'rewards_train/rejected': '0.028', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019665', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-121.42', 'loss/train': '0.69092', 'examples_per_second': '30.011', 'grad_norm': '29.375', 'counters/examples': 130080, 'counters/updates': 4065}
skipping logging after 130112 examples to avoid logging too frequently
train stats after 130144 examples: {'rewards_train/chosen': '0.073842', 'rewards_train/rejected': '0.046189', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027653', 'logps_train/rejected': '-102.75', 'logps_train/chosen': '-103.81', 'loss/train': '0.68538', 'examples_per_second': '29.093', 'grad_norm': '23', 'counters/examples': 130144, 'counters/updates': 4067}
skipping logging after 130176 examples to avoid logging too frequently
train stats after 130208 examples: {'rewards_train/chosen': '0.076542', 'rewards_train/rejected': '0.034754', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041788', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-158.09', 'loss/train': '0.6784', 'examples_per_second': '32.068', 'grad_norm': '42.5', 'counters/examples': 130208, 'counters/updates': 4069}
train stats after 130240 examples: {'rewards_train/chosen': '0.046502', 'rewards_train/rejected': '0.045186', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.001316', 'logps_train/rejected': '-94.66', 'logps_train/chosen': '-141.32', 'loss/train': '0.69802', 'examples_per_second': '32.376', 'grad_norm': '29.125', 'counters/examples': 130240, 'counters/updates': 4070}
skipping logging after 130272 examples to avoid logging too frequently
train stats after 130304 examples: {'rewards_train/chosen': '0.12318', 'rewards_train/rejected': '0.012257', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11092', 'logps_train/rejected': '-102.05', 'logps_train/chosen': '-131.47', 'loss/train': '0.64757', 'examples_per_second': '31.436', 'grad_norm': '25.375', 'counters/examples': 130304, 'counters/updates': 4072}
train stats after 130336 examples: {'rewards_train/chosen': '0.22742', 'rewards_train/rejected': '0.0529', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17452', 'logps_train/rejected': '-153.44', 'logps_train/chosen': '-190.67', 'loss/train': '0.62737', 'examples_per_second': '31.459', 'grad_norm': '31.875', 'counters/examples': 130336, 'counters/updates': 4073}
train stats after 130368 examples: {'rewards_train/chosen': '0.1646', 'rewards_train/rejected': '0.040812', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12379', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-93.944', 'loss/train': '0.65193', 'examples_per_second': '30.381', 'grad_norm': '23.625', 'counters/examples': 130368, 'counters/updates': 4074}
train stats after 130400 examples: {'rewards_train/chosen': '0.028198', 'rewards_train/rejected': '0.0090543', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019144', 'logps_train/rejected': '-102.69', 'logps_train/chosen': '-106.57', 'loss/train': '0.69191', 'examples_per_second': '32.303', 'grad_norm': '30', 'counters/examples': 130400, 'counters/updates': 4075}
train stats after 130432 examples: {'rewards_train/chosen': '0.079549', 'rewards_train/rejected': '0.009623', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.069926', 'logps_train/rejected': '-149.97', 'logps_train/chosen': '-112.63', 'loss/train': '0.68386', 'examples_per_second': '32.362', 'grad_norm': '47.75', 'counters/examples': 130432, 'counters/updates': 4076}
skipping logging after 130464 examples to avoid logging too frequently
train stats after 130496 examples: {'rewards_train/chosen': '0.077945', 'rewards_train/rejected': '0.087053', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0091077', 'logps_train/rejected': '-145.6', 'logps_train/chosen': '-157', 'loss/train': '0.70407', 'examples_per_second': '30.754', 'grad_norm': '33.5', 'counters/examples': 130496, 'counters/updates': 4078}
train stats after 130528 examples: {'rewards_train/chosen': '0.13738', 'rewards_train/rejected': '0.0050042', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13237', 'logps_train/rejected': '-115.35', 'logps_train/chosen': '-166.36', 'loss/train': '0.64741', 'examples_per_second': '30.662', 'grad_norm': '25.25', 'counters/examples': 130528, 'counters/updates': 4079}
train stats after 130560 examples: {'rewards_train/chosen': '0.18594', 'rewards_train/rejected': '0.097383', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088555', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-176.56', 'loss/train': '0.65913', 'examples_per_second': '31.482', 'grad_norm': '28', 'counters/examples': 130560, 'counters/updates': 4080}
train stats after 130592 examples: {'rewards_train/chosen': '0.093937', 'rewards_train/rejected': '0.046474', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047463', 'logps_train/rejected': '-98.238', 'logps_train/chosen': '-105.48', 'loss/train': '0.67762', 'examples_per_second': '30.466', 'grad_norm': '26.75', 'counters/examples': 130592, 'counters/updates': 4081}
train stats after 130624 examples: {'rewards_train/chosen': '0.13311', 'rewards_train/rejected': '0.043559', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08955', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-145.2', 'loss/train': '0.65544', 'examples_per_second': '32.873', 'grad_norm': '27.25', 'counters/examples': 130624, 'counters/updates': 4082}
skipping logging after 130656 examples to avoid logging too frequently
train stats after 130688 examples: {'rewards_train/chosen': '0.15795', 'rewards_train/rejected': '0.087986', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069964', 'logps_train/rejected': '-141.08', 'logps_train/chosen': '-148.3', 'loss/train': '0.66933', 'examples_per_second': '32.548', 'grad_norm': '48.25', 'counters/examples': 130688, 'counters/updates': 4084}
train stats after 130720 examples: {'rewards_train/chosen': '0.1519', 'rewards_train/rejected': '0.089994', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061908', 'logps_train/rejected': '-140.24', 'logps_train/chosen': '-181.64', 'loss/train': '0.67072', 'examples_per_second': '31.449', 'grad_norm': '40', 'counters/examples': 130720, 'counters/updates': 4085}
train stats after 130752 examples: {'rewards_train/chosen': '0.22222', 'rewards_train/rejected': '0.026363', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19586', 'logps_train/rejected': '-112.13', 'logps_train/chosen': '-137.69', 'loss/train': '0.61215', 'examples_per_second': '30.952', 'grad_norm': '26.25', 'counters/examples': 130752, 'counters/updates': 4086}
train stats after 130784 examples: {'rewards_train/chosen': '0.16353', 'rewards_train/rejected': '0.068673', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094861', 'logps_train/rejected': '-154.46', 'logps_train/chosen': '-135.37', 'loss/train': '0.65433', 'examples_per_second': '31.407', 'grad_norm': '32.75', 'counters/examples': 130784, 'counters/updates': 4087}
train stats after 130816 examples: {'rewards_train/chosen': '0.13568', 'rewards_train/rejected': '0.042153', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093523', 'logps_train/rejected': '-126.47', 'logps_train/chosen': '-129.74', 'loss/train': '0.66847', 'examples_per_second': '30.758', 'grad_norm': '40.5', 'counters/examples': 130816, 'counters/updates': 4088}
skipping logging after 130848 examples to avoid logging too frequently
train stats after 130880 examples: {'rewards_train/chosen': '0.12065', 'rewards_train/rejected': '0.073369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047283', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-146.63', 'loss/train': '0.67762', 'examples_per_second': '29.951', 'grad_norm': '29.375', 'counters/examples': 130880, 'counters/updates': 4090}
train stats after 130912 examples: {'rewards_train/chosen': '0.13757', 'rewards_train/rejected': '0.068066', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069509', 'logps_train/rejected': '-122.86', 'logps_train/chosen': '-163.67', 'loss/train': '0.67192', 'examples_per_second': '31.438', 'grad_norm': '29.5', 'counters/examples': 130912, 'counters/updates': 4091}
skipping logging after 130944 examples to avoid logging too frequently
train stats after 130976 examples: {'rewards_train/chosen': '0.10533', 'rewards_train/rejected': '0.0088387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096491', 'logps_train/rejected': '-101.63', 'logps_train/chosen': '-133.93', 'loss/train': '0.65397', 'examples_per_second': '30.426', 'grad_norm': '23.25', 'counters/examples': 130976, 'counters/updates': 4093}
train stats after 131008 examples: {'rewards_train/chosen': '0.11442', 'rewards_train/rejected': '0.099099', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015319', 'logps_train/rejected': '-140', 'logps_train/chosen': '-144.48', 'loss/train': '0.69838', 'examples_per_second': '30.024', 'grad_norm': '40.25', 'counters/examples': 131008, 'counters/updates': 4094}
train stats after 131040 examples: {'rewards_train/chosen': '0.26414', 'rewards_train/rejected': '0.16868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095464', 'logps_train/rejected': '-184.56', 'logps_train/chosen': '-206.71', 'loss/train': '0.65479', 'examples_per_second': '31.2', 'grad_norm': '42', 'counters/examples': 131040, 'counters/updates': 4095}
train stats after 131072 examples: {'rewards_train/chosen': '0.094312', 'rewards_train/rejected': '0.050467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043845', 'logps_train/rejected': '-152.62', 'logps_train/chosen': '-134.78', 'loss/train': '0.68596', 'examples_per_second': '30.936', 'grad_norm': '38.5', 'counters/examples': 131072, 'counters/updates': 4096}
skipping logging after 131104 examples to avoid logging too frequently
train stats after 131136 examples: {'rewards_train/chosen': '0.13389', 'rewards_train/rejected': '0.062077', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07181', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-156.9', 'loss/train': '0.66857', 'examples_per_second': '30.424', 'grad_norm': '29.875', 'counters/examples': 131136, 'counters/updates': 4098}
train stats after 131168 examples: {'rewards_train/chosen': '0.16587', 'rewards_train/rejected': '0.15662', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0092533', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-159.98', 'loss/train': '0.7081', 'examples_per_second': '31.492', 'grad_norm': '48.25', 'counters/examples': 131168, 'counters/updates': 4099}
train stats after 131200 examples: {'rewards_train/chosen': '0.13127', 'rewards_train/rejected': '0.015906', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11536', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-166.54', 'loss/train': '0.6486', 'examples_per_second': '31.141', 'grad_norm': '34.5', 'counters/examples': 131200, 'counters/updates': 4100}
train stats after 131232 examples: {'rewards_train/chosen': '0.068521', 'rewards_train/rejected': '0.050714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017808', 'logps_train/rejected': '-139.86', 'logps_train/chosen': '-145.37', 'loss/train': '0.6981', 'examples_per_second': '30.845', 'grad_norm': '33.25', 'counters/examples': 131232, 'counters/updates': 4101}
train stats after 131264 examples: {'rewards_train/chosen': '0.16442', 'rewards_train/rejected': '0.062842', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10158', 'logps_train/rejected': '-145.65', 'logps_train/chosen': '-172.83', 'loss/train': '0.65364', 'examples_per_second': '29.907', 'grad_norm': '30.75', 'counters/examples': 131264, 'counters/updates': 4102}
skipping logging after 131296 examples to avoid logging too frequently
train stats after 131328 examples: {'rewards_train/chosen': '0.1384', 'rewards_train/rejected': '0.050232', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088168', 'logps_train/rejected': '-177.28', 'logps_train/chosen': '-143.05', 'loss/train': '0.65743', 'examples_per_second': '31.253', 'grad_norm': '30.875', 'counters/examples': 131328, 'counters/updates': 4104}
train stats after 131360 examples: {'rewards_train/chosen': '0.15676', 'rewards_train/rejected': '0.0731', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083663', 'logps_train/rejected': '-130.09', 'logps_train/chosen': '-115.62', 'loss/train': '0.66007', 'examples_per_second': '29.895', 'grad_norm': '33.25', 'counters/examples': 131360, 'counters/updates': 4105}
train stats after 131392 examples: {'rewards_train/chosen': '0.21665', 'rewards_train/rejected': '0.097245', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11941', 'logps_train/rejected': '-141.82', 'logps_train/chosen': '-174.94', 'loss/train': '0.64466', 'examples_per_second': '30.449', 'grad_norm': '35', 'counters/examples': 131392, 'counters/updates': 4106}
train stats after 131424 examples: {'rewards_train/chosen': '0.14151', 'rewards_train/rejected': '-0.040655', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18216', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-138.53', 'loss/train': '0.62079', 'examples_per_second': '31.455', 'grad_norm': '24.5', 'counters/examples': 131424, 'counters/updates': 4107}
skipping logging after 131456 examples to avoid logging too frequently
train stats after 131488 examples: {'rewards_train/chosen': '0.1542', 'rewards_train/rejected': '0.062859', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091341', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-183.33', 'loss/train': '0.65694', 'examples_per_second': '31.392', 'grad_norm': '33.5', 'counters/examples': 131488, 'counters/updates': 4109}
train stats after 131520 examples: {'rewards_train/chosen': '0.055891', 'rewards_train/rejected': '-0.03047', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086361', 'logps_train/rejected': '-107.54', 'logps_train/chosen': '-143.86', 'loss/train': '0.66574', 'examples_per_second': '33.294', 'grad_norm': '30.25', 'counters/examples': 131520, 'counters/updates': 4110}
train stats after 131552 examples: {'rewards_train/chosen': '0.045273', 'rewards_train/rejected': '0.064081', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018808', 'logps_train/rejected': '-170.32', 'logps_train/chosen': '-157.42', 'loss/train': '0.71766', 'examples_per_second': '32.812', 'grad_norm': '47.75', 'counters/examples': 131552, 'counters/updates': 4111}
train stats after 131584 examples: {'rewards_train/chosen': '0.12006', 'rewards_train/rejected': '0.090399', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.029656', 'logps_train/rejected': '-125.59', 'logps_train/chosen': '-125.86', 'loss/train': '0.68591', 'examples_per_second': '30.651', 'grad_norm': '37.25', 'counters/examples': 131584, 'counters/updates': 4112}
skipping logging after 131616 examples to avoid logging too frequently
train stats after 131648 examples: {'rewards_train/chosen': '0.12238', 'rewards_train/rejected': '0.071688', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050693', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-114.11', 'loss/train': '0.68415', 'examples_per_second': '31.404', 'grad_norm': '30.625', 'counters/examples': 131648, 'counters/updates': 4114}
train stats after 131680 examples: {'rewards_train/chosen': '0.060755', 'rewards_train/rejected': '0.059516', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0012395', 'logps_train/rejected': '-136.98', 'logps_train/chosen': '-133.28', 'loss/train': '0.7056', 'examples_per_second': '30.857', 'grad_norm': '32.75', 'counters/examples': 131680, 'counters/updates': 4115}
train stats after 131712 examples: {'rewards_train/chosen': '0.18121', 'rewards_train/rejected': '0.07368', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10753', 'logps_train/rejected': '-135.52', 'logps_train/chosen': '-147.95', 'loss/train': '0.64968', 'examples_per_second': '32.546', 'grad_norm': '39.75', 'counters/examples': 131712, 'counters/updates': 4116}
train stats after 131744 examples: {'rewards_train/chosen': '0.1392', 'rewards_train/rejected': '0.036853', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10235', 'logps_train/rejected': '-87.018', 'logps_train/chosen': '-127.59', 'loss/train': '0.64823', 'examples_per_second': '29.997', 'grad_norm': '24.625', 'counters/examples': 131744, 'counters/updates': 4117}
skipping logging after 131776 examples to avoid logging too frequently
train stats after 131808 examples: {'rewards_train/chosen': '0.20542', 'rewards_train/rejected': '0.096296', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10913', 'logps_train/rejected': '-130.15', 'logps_train/chosen': '-139.29', 'loss/train': '0.66327', 'examples_per_second': '29.907', 'grad_norm': '34.5', 'counters/examples': 131808, 'counters/updates': 4119}
skipping logging after 131840 examples to avoid logging too frequently
train stats after 131872 examples: {'rewards_train/chosen': '0.12628', 'rewards_train/rejected': '0.050817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075459', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-131.55', 'loss/train': '0.66425', 'examples_per_second': '37.881', 'grad_norm': '27.625', 'counters/examples': 131872, 'counters/updates': 4121}
train stats after 131904 examples: {'rewards_train/chosen': '0.15318', 'rewards_train/rejected': '0.12067', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032518', 'logps_train/rejected': '-141.01', 'logps_train/chosen': '-147.61', 'loss/train': '0.69422', 'examples_per_second': '32.122', 'grad_norm': '54.25', 'counters/examples': 131904, 'counters/updates': 4122}
skipping logging after 131936 examples to avoid logging too frequently
train stats after 131968 examples: {'rewards_train/chosen': '0.15005', 'rewards_train/rejected': '0.090461', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.059587', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-140.2', 'loss/train': '0.67228', 'examples_per_second': '32.526', 'grad_norm': '49.5', 'counters/examples': 131968, 'counters/updates': 4124}
skipping logging after 132000 examples to avoid logging too frequently
Running evaluation after 132000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.20it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.92it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.83it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.07it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 132000: {'rewards_eval/chosen': '0.14497', 'rewards_eval/rejected': '0.049621', 'rewards_eval/accuracies': '0.62891', 'rewards_eval/margins': '0.095346', 'logps_eval/rejected': '-121.64', 'logps_eval/chosen': '-142.66', 'loss/eval': '0.65869'}
skipping save for non epoch
train stats after 132032 examples: {'rewards_train/chosen': '0.18197', 'rewards_train/rejected': '0.091333', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090636', 'logps_train/rejected': '-125.27', 'logps_train/chosen': '-164.09', 'loss/train': '0.66582', 'examples_per_second': '30.726', 'grad_norm': '31.625', 'counters/examples': 132032, 'counters/updates': 4126}
train stats after 132064 examples: {'rewards_train/chosen': '0.1462', 'rewards_train/rejected': '0.07733', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068875', 'logps_train/rejected': '-116.91', 'logps_train/chosen': '-132.15', 'loss/train': '0.66971', 'examples_per_second': '30.685', 'grad_norm': '28.375', 'counters/examples': 132064, 'counters/updates': 4127}
skipping logging after 132096 examples to avoid logging too frequently
train stats after 132128 examples: {'rewards_train/chosen': '0.064812', 'rewards_train/rejected': '0.072189', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0073776', 'logps_train/rejected': '-137.24', 'logps_train/chosen': '-95.937', 'loss/train': '0.70276', 'examples_per_second': '30.642', 'grad_norm': '26', 'counters/examples': 132128, 'counters/updates': 4129}
train stats after 132160 examples: {'rewards_train/chosen': '0.039369', 'rewards_train/rejected': '0.12844', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.089075', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-135.09', 'loss/train': '0.7494', 'examples_per_second': '31.349', 'grad_norm': '50.5', 'counters/examples': 132160, 'counters/updates': 4130}
train stats after 132192 examples: {'rewards_train/chosen': '0.13578', 'rewards_train/rejected': '0.035625', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10015', 'logps_train/rejected': '-140.3', 'logps_train/chosen': '-173.64', 'loss/train': '0.66065', 'examples_per_second': '30.867', 'grad_norm': '29.625', 'counters/examples': 132192, 'counters/updates': 4131}
train stats after 132224 examples: {'rewards_train/chosen': '0.11481', 'rewards_train/rejected': '0.14968', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034863', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-140.36', 'loss/train': '0.72458', 'examples_per_second': '31.387', 'grad_norm': '38.25', 'counters/examples': 132224, 'counters/updates': 4132}
train stats after 132256 examples: {'rewards_train/chosen': '0.12648', 'rewards_train/rejected': '0.042592', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083887', 'logps_train/rejected': '-140.39', 'logps_train/chosen': '-152.52', 'loss/train': '0.66311', 'examples_per_second': '32.291', 'grad_norm': '31.5', 'counters/examples': 132256, 'counters/updates': 4133}
train stats after 132288 examples: {'rewards_train/chosen': '0.25917', 'rewards_train/rejected': '0.23871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.020462', 'logps_train/rejected': '-161.35', 'logps_train/chosen': '-168.93', 'loss/train': '0.72739', 'examples_per_second': '29.793', 'grad_norm': '123.5', 'counters/examples': 132288, 'counters/updates': 4134}
train stats after 132320 examples: {'rewards_train/chosen': '0.23319', 'rewards_train/rejected': '0.024855', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20834', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-146.13', 'loss/train': '0.63303', 'examples_per_second': '32.533', 'grad_norm': '29.375', 'counters/examples': 132320, 'counters/updates': 4135}
train stats after 132352 examples: {'rewards_train/chosen': '0.2238', 'rewards_train/rejected': '0.032229', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19157', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-173.62', 'loss/train': '0.61217', 'examples_per_second': '31.982', 'grad_norm': '28.875', 'counters/examples': 132352, 'counters/updates': 4136}
train stats after 132384 examples: {'rewards_train/chosen': '0.18229', 'rewards_train/rejected': '0.061951', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12034', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-121.79', 'loss/train': '0.64426', 'examples_per_second': '31.328', 'grad_norm': '29.375', 'counters/examples': 132384, 'counters/updates': 4137}
train stats after 132416 examples: {'rewards_train/chosen': '0.2659', 'rewards_train/rejected': '0.13887', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12703', 'logps_train/rejected': '-135.91', 'logps_train/chosen': '-192.07', 'loss/train': '0.65793', 'examples_per_second': '31.451', 'grad_norm': '31.875', 'counters/examples': 132416, 'counters/updates': 4138}
train stats after 132448 examples: {'rewards_train/chosen': '0.18336', 'rewards_train/rejected': '0.032449', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15091', 'logps_train/rejected': '-112.11', 'logps_train/chosen': '-169.19', 'loss/train': '0.64223', 'examples_per_second': '30.46', 'grad_norm': '32.5', 'counters/examples': 132448, 'counters/updates': 4139}
train stats after 132480 examples: {'rewards_train/chosen': '0.19016', 'rewards_train/rejected': '0.04478', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14538', 'logps_train/rejected': '-117.88', 'logps_train/chosen': '-150.76', 'loss/train': '0.63222', 'examples_per_second': '31.422', 'grad_norm': '25.375', 'counters/examples': 132480, 'counters/updates': 4140}
train stats after 132512 examples: {'rewards_train/chosen': '0.15697', 'rewards_train/rejected': '0.10506', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051908', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-147.03', 'loss/train': '0.67578', 'examples_per_second': '23.064', 'grad_norm': '46.75', 'counters/examples': 132512, 'counters/updates': 4141}
train stats after 132544 examples: {'rewards_train/chosen': '0.16146', 'rewards_train/rejected': '0.11666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044801', 'logps_train/rejected': '-164.93', 'logps_train/chosen': '-164.17', 'loss/train': '0.69261', 'examples_per_second': '30.95', 'grad_norm': '136', 'counters/examples': 132544, 'counters/updates': 4142}
train stats after 132576 examples: {'rewards_train/chosen': '0.11641', 'rewards_train/rejected': '0.10957', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0068414', 'logps_train/rejected': '-113.92', 'logps_train/chosen': '-114.02', 'loss/train': '0.69535', 'examples_per_second': '30.359', 'grad_norm': '32.75', 'counters/examples': 132576, 'counters/updates': 4143}
train stats after 132608 examples: {'rewards_train/chosen': '0.10592', 'rewards_train/rejected': '0.046838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059079', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-129.68', 'loss/train': '0.67277', 'examples_per_second': '30.693', 'grad_norm': '29.5', 'counters/examples': 132608, 'counters/updates': 4144}
skipping logging after 132640 examples to avoid logging too frequently
train stats after 132672 examples: {'rewards_train/chosen': '0.11751', 'rewards_train/rejected': '0.049254', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068253', 'logps_train/rejected': '-115.98', 'logps_train/chosen': '-159.44', 'loss/train': '0.66651', 'examples_per_second': '31.388', 'grad_norm': '44', 'counters/examples': 132672, 'counters/updates': 4146}
train stats after 132704 examples: {'rewards_train/chosen': '0.15302', 'rewards_train/rejected': '0.069612', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083406', 'logps_train/rejected': '-137.46', 'logps_train/chosen': '-130.13', 'loss/train': '0.66051', 'examples_per_second': '30.824', 'grad_norm': '28', 'counters/examples': 132704, 'counters/updates': 4147}
train stats after 132736 examples: {'rewards_train/chosen': '0.16759', 'rewards_train/rejected': '0.001551', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16604', 'logps_train/rejected': '-86.21', 'logps_train/chosen': '-142.31', 'loss/train': '0.62429', 'examples_per_second': '29.958', 'grad_norm': '26', 'counters/examples': 132736, 'counters/updates': 4148}
train stats after 132768 examples: {'rewards_train/chosen': '0.052621', 'rewards_train/rejected': '0.10064', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.048021', 'logps_train/rejected': '-153.05', 'logps_train/chosen': '-153.24', 'loss/train': '0.73101', 'examples_per_second': '30.01', 'grad_norm': '125', 'counters/examples': 132768, 'counters/updates': 4149}
train stats after 132800 examples: {'rewards_train/chosen': '0.14416', 'rewards_train/rejected': '-0.010166', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15432', 'logps_train/rejected': '-112', 'logps_train/chosen': '-143.16', 'loss/train': '0.62763', 'examples_per_second': '31.354', 'grad_norm': '30', 'counters/examples': 132800, 'counters/updates': 4150}
train stats after 132832 examples: {'rewards_train/chosen': '0.11383', 'rewards_train/rejected': '0.03797', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075858', 'logps_train/rejected': '-162.32', 'logps_train/chosen': '-194.56', 'loss/train': '0.66385', 'examples_per_second': '31.746', 'grad_norm': '30.75', 'counters/examples': 132832, 'counters/updates': 4151}
train stats after 132864 examples: {'rewards_train/chosen': '0.093961', 'rewards_train/rejected': '0.0050216', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08894', 'logps_train/rejected': '-136.55', 'logps_train/chosen': '-151.75', 'loss/train': '0.6688', 'examples_per_second': '31.647', 'grad_norm': '28.25', 'counters/examples': 132864, 'counters/updates': 4152}
train stats after 132896 examples: {'rewards_train/chosen': '0.091579', 'rewards_train/rejected': '0.090446', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.0011336', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-153.07', 'loss/train': '0.70091', 'examples_per_second': '31.216', 'grad_norm': '30.375', 'counters/examples': 132896, 'counters/updates': 4153}
train stats after 132928 examples: {'rewards_train/chosen': '0.17548', 'rewards_train/rejected': '0.12657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048915', 'logps_train/rejected': '-122.5', 'logps_train/chosen': '-139.92', 'loss/train': '0.68158', 'examples_per_second': '29.763', 'grad_norm': '36.5', 'counters/examples': 132928, 'counters/updates': 4154}
train stats after 132960 examples: {'rewards_train/chosen': '0.13487', 'rewards_train/rejected': '-0.0042663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13914', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-150.58', 'loss/train': '0.64023', 'examples_per_second': '29.894', 'grad_norm': '25.75', 'counters/examples': 132960, 'counters/updates': 4155}
train stats after 132992 examples: {'rewards_train/chosen': '0.08604', 'rewards_train/rejected': '0.041484', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044555', 'logps_train/rejected': '-146.01', 'logps_train/chosen': '-170.82', 'loss/train': '0.68252', 'examples_per_second': '31.921', 'grad_norm': '53.5', 'counters/examples': 132992, 'counters/updates': 4156}
train stats after 133024 examples: {'rewards_train/chosen': '0.15303', 'rewards_train/rejected': '0.044187', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10884', 'logps_train/rejected': '-145.18', 'logps_train/chosen': '-155.29', 'loss/train': '0.64935', 'examples_per_second': '30.843', 'grad_norm': '28.375', 'counters/examples': 133024, 'counters/updates': 4157}
train stats after 133056 examples: {'rewards_train/chosen': '0.15711', 'rewards_train/rejected': '0.0081702', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14894', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-159.19', 'loss/train': '0.63403', 'examples_per_second': '31.546', 'grad_norm': '30.5', 'counters/examples': 133056, 'counters/updates': 4158}
skipping logging after 133088 examples to avoid logging too frequently
train stats after 133120 examples: {'rewards_train/chosen': '0.13395', 'rewards_train/rejected': '0.071991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061963', 'logps_train/rejected': '-163.37', 'logps_train/chosen': '-154.57', 'loss/train': '0.68359', 'examples_per_second': '31.556', 'grad_norm': '41.25', 'counters/examples': 133120, 'counters/updates': 4160}
train stats after 133152 examples: {'rewards_train/chosen': '0.12415', 'rewards_train/rejected': '0.078372', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045778', 'logps_train/rejected': '-103.83', 'logps_train/chosen': '-105.79', 'loss/train': '0.67887', 'examples_per_second': '31.418', 'grad_norm': '34.5', 'counters/examples': 133152, 'counters/updates': 4161}
train stats after 133184 examples: {'rewards_train/chosen': '0.11712', 'rewards_train/rejected': '0.030371', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086751', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-120.89', 'loss/train': '0.65955', 'examples_per_second': '31.231', 'grad_norm': '31.625', 'counters/examples': 133184, 'counters/updates': 4162}
skipping logging after 133216 examples to avoid logging too frequently
train stats after 133248 examples: {'rewards_train/chosen': '0.18429', 'rewards_train/rejected': '0.054928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12936', 'logps_train/rejected': '-117.13', 'logps_train/chosen': '-160.18', 'loss/train': '0.63825', 'examples_per_second': '31.439', 'grad_norm': '35.75', 'counters/examples': 133248, 'counters/updates': 4164}
skipping logging after 133280 examples to avoid logging too frequently
train stats after 133312 examples: {'rewards_train/chosen': '0.033752', 'rewards_train/rejected': '0.0061996', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027553', 'logps_train/rejected': '-131.93', 'logps_train/chosen': '-150.07', 'loss/train': '0.69466', 'examples_per_second': '31.491', 'grad_norm': '33', 'counters/examples': 133312, 'counters/updates': 4166}
train stats after 133344 examples: {'rewards_train/chosen': '0.16071', 'rewards_train/rejected': '0.050668', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11004', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-193.19', 'loss/train': '0.6516', 'examples_per_second': '29.909', 'grad_norm': '29.75', 'counters/examples': 133344, 'counters/updates': 4167}
train stats after 133376 examples: {'rewards_train/chosen': '0.13186', 'rewards_train/rejected': '0.026326', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10554', 'logps_train/rejected': '-108.17', 'logps_train/chosen': '-125.8', 'loss/train': '0.6525', 'examples_per_second': '31.617', 'grad_norm': '25.75', 'counters/examples': 133376, 'counters/updates': 4168}
train stats after 133408 examples: {'rewards_train/chosen': '0.071641', 'rewards_train/rejected': '0.02036', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05128', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-159', 'loss/train': '0.67703', 'examples_per_second': '31.456', 'grad_norm': '26.125', 'counters/examples': 133408, 'counters/updates': 4169}
skipping logging after 133440 examples to avoid logging too frequently
train stats after 133472 examples: {'rewards_train/chosen': '0.0436', 'rewards_train/rejected': '-0.012651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05625', 'logps_train/rejected': '-88.13', 'logps_train/chosen': '-115.24', 'loss/train': '0.67438', 'examples_per_second': '31.418', 'grad_norm': '25.75', 'counters/examples': 133472, 'counters/updates': 4171}
train stats after 133504 examples: {'rewards_train/chosen': '0.096221', 'rewards_train/rejected': '0.13251', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.036286', 'logps_train/rejected': '-146.67', 'logps_train/chosen': '-131.95', 'loss/train': '0.72512', 'examples_per_second': '30.077', 'grad_norm': '34.75', 'counters/examples': 133504, 'counters/updates': 4172}
train stats after 133536 examples: {'rewards_train/chosen': '0.11469', 'rewards_train/rejected': '0.057621', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057067', 'logps_train/rejected': '-144.79', 'logps_train/chosen': '-139.11', 'loss/train': '0.67379', 'examples_per_second': '31.426', 'grad_norm': '35', 'counters/examples': 133536, 'counters/updates': 4173}
train stats after 133568 examples: {'rewards_train/chosen': '0.11636', 'rewards_train/rejected': '0.074819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041545', 'logps_train/rejected': '-133.62', 'logps_train/chosen': '-142.78', 'loss/train': '0.68235', 'examples_per_second': '31.828', 'grad_norm': '29.75', 'counters/examples': 133568, 'counters/updates': 4174}
train stats after 133600 examples: {'rewards_train/chosen': '0.29421', 'rewards_train/rejected': '0.27151', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022695', 'logps_train/rejected': '-148.58', 'logps_train/chosen': '-190.5', 'loss/train': '0.69992', 'examples_per_second': '30.567', 'grad_norm': '51.5', 'counters/examples': 133600, 'counters/updates': 4175}
train stats after 133632 examples: {'rewards_train/chosen': '0.10358', 'rewards_train/rejected': '0.017888', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085688', 'logps_train/rejected': '-121.08', 'logps_train/chosen': '-144.28', 'loss/train': '0.65429', 'examples_per_second': '31.335', 'grad_norm': '37.5', 'counters/examples': 133632, 'counters/updates': 4176}
train stats after 133664 examples: {'rewards_train/chosen': '0.07583', 'rewards_train/rejected': '0.018934', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.056895', 'logps_train/rejected': '-92.754', 'logps_train/chosen': '-125.5', 'loss/train': '0.66882', 'examples_per_second': '30.382', 'grad_norm': '23.125', 'counters/examples': 133664, 'counters/updates': 4177}
train stats after 133696 examples: {'rewards_train/chosen': '0.083094', 'rewards_train/rejected': '0.058236', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024858', 'logps_train/rejected': '-140.06', 'logps_train/chosen': '-181.18', 'loss/train': '0.69022', 'examples_per_second': '32.849', 'grad_norm': '35.25', 'counters/examples': 133696, 'counters/updates': 4178}
train stats after 133728 examples: {'rewards_train/chosen': '0.10857', 'rewards_train/rejected': '0.0045925', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10397', 'logps_train/rejected': '-119.28', 'logps_train/chosen': '-119.36', 'loss/train': '0.65128', 'examples_per_second': '31.412', 'grad_norm': '29.125', 'counters/examples': 133728, 'counters/updates': 4179}
train stats after 133760 examples: {'rewards_train/chosen': '0.15584', 'rewards_train/rejected': '0.054798', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10104', 'logps_train/rejected': '-97.204', 'logps_train/chosen': '-170.2', 'loss/train': '0.65729', 'examples_per_second': '32.052', 'grad_norm': '49.5', 'counters/examples': 133760, 'counters/updates': 4180}
train stats after 133792 examples: {'rewards_train/chosen': '0.16938', 'rewards_train/rejected': '0.054777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1146', 'logps_train/rejected': '-115.92', 'logps_train/chosen': '-136.85', 'loss/train': '0.65213', 'examples_per_second': '32.465', 'grad_norm': '57.5', 'counters/examples': 133792, 'counters/updates': 4181}
train stats after 133824 examples: {'rewards_train/chosen': '0.046992', 'rewards_train/rejected': '0.068172', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02118', 'logps_train/rejected': '-128.2', 'logps_train/chosen': '-134.48', 'loss/train': '0.72132', 'examples_per_second': '31.811', 'grad_norm': '30.875', 'counters/examples': 133824, 'counters/updates': 4182}
train stats after 133856 examples: {'rewards_train/chosen': '0.15854', 'rewards_train/rejected': '0.070486', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088058', 'logps_train/rejected': '-147.07', 'logps_train/chosen': '-137.51', 'loss/train': '0.66142', 'examples_per_second': '31.449', 'grad_norm': '33.75', 'counters/examples': 133856, 'counters/updates': 4183}
train stats after 133888 examples: {'rewards_train/chosen': '0.088744', 'rewards_train/rejected': '0.034091', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054653', 'logps_train/rejected': '-105.85', 'logps_train/chosen': '-153.77', 'loss/train': '0.67208', 'examples_per_second': '31.421', 'grad_norm': '30', 'counters/examples': 133888, 'counters/updates': 4184}
train stats after 133920 examples: {'rewards_train/chosen': '0.093888', 'rewards_train/rejected': '0.02806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065827', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-140.86', 'loss/train': '0.67034', 'examples_per_second': '32.511', 'grad_norm': '30.125', 'counters/examples': 133920, 'counters/updates': 4185}
train stats after 133952 examples: {'rewards_train/chosen': '0.18531', 'rewards_train/rejected': '0.041653', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14366', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-144.01', 'loss/train': '0.63881', 'examples_per_second': '32.328', 'grad_norm': '32.75', 'counters/examples': 133952, 'counters/updates': 4186}
train stats after 133984 examples: {'rewards_train/chosen': '0.1295', 'rewards_train/rejected': '0.060807', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06869', 'logps_train/rejected': '-112.98', 'logps_train/chosen': '-105.42', 'loss/train': '0.66739', 'examples_per_second': '32.668', 'grad_norm': '29.125', 'counters/examples': 133984, 'counters/updates': 4187}
train stats after 134016 examples: {'rewards_train/chosen': '0.19803', 'rewards_train/rejected': '0.084497', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11353', 'logps_train/rejected': '-166.48', 'logps_train/chosen': '-154.54', 'loss/train': '0.66829', 'examples_per_second': '29.96', 'grad_norm': '41', 'counters/examples': 134016, 'counters/updates': 4188}
train stats after 134048 examples: {'rewards_train/chosen': '0.086737', 'rewards_train/rejected': '0.026344', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060392', 'logps_train/rejected': '-110.18', 'logps_train/chosen': '-142.79', 'loss/train': '0.67283', 'examples_per_second': '31.447', 'grad_norm': '32.25', 'counters/examples': 134048, 'counters/updates': 4189}
train stats after 134080 examples: {'rewards_train/chosen': '0.21561', 'rewards_train/rejected': '0.1274', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08821', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-150', 'loss/train': '0.66886', 'examples_per_second': '31.166', 'grad_norm': '35.75', 'counters/examples': 134080, 'counters/updates': 4190}
train stats after 134112 examples: {'rewards_train/chosen': '0.10584', 'rewards_train/rejected': '-0.0083427', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11418', 'logps_train/rejected': '-113.94', 'logps_train/chosen': '-148.84', 'loss/train': '0.64797', 'examples_per_second': '31.521', 'grad_norm': '25.125', 'counters/examples': 134112, 'counters/updates': 4191}
train stats after 134144 examples: {'rewards_train/chosen': '0.1326', 'rewards_train/rejected': '0.07316', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059435', 'logps_train/rejected': '-112.6', 'logps_train/chosen': '-153.63', 'loss/train': '0.68144', 'examples_per_second': '31.344', 'grad_norm': '29.125', 'counters/examples': 134144, 'counters/updates': 4192}
train stats after 134176 examples: {'rewards_train/chosen': '0.074304', 'rewards_train/rejected': '0.060425', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.013878', 'logps_train/rejected': '-107.93', 'logps_train/chosen': '-109.5', 'loss/train': '0.69328', 'examples_per_second': '30.556', 'grad_norm': '34', 'counters/examples': 134176, 'counters/updates': 4193}
train stats after 134208 examples: {'rewards_train/chosen': '0.10556', 'rewards_train/rejected': '0.077134', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.028424', 'logps_train/rejected': '-137.84', 'logps_train/chosen': '-183.02', 'loss/train': '0.6888', 'examples_per_second': '30.513', 'grad_norm': '35.25', 'counters/examples': 134208, 'counters/updates': 4194}
train stats after 134240 examples: {'rewards_train/chosen': '0.28904', 'rewards_train/rejected': '0.077763', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21128', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-152.92', 'loss/train': '0.62479', 'examples_per_second': '31.213', 'grad_norm': '32.75', 'counters/examples': 134240, 'counters/updates': 4195}
train stats after 134272 examples: {'rewards_train/chosen': '0.16568', 'rewards_train/rejected': '0.053914', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11176', 'logps_train/rejected': '-121.71', 'logps_train/chosen': '-157.81', 'loss/train': '0.64668', 'examples_per_second': '31.437', 'grad_norm': '30.125', 'counters/examples': 134272, 'counters/updates': 4196}
train stats after 134304 examples: {'rewards_train/chosen': '0.096728', 'rewards_train/rejected': '0.080253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016476', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-128.76', 'loss/train': '0.6906', 'examples_per_second': '31.467', 'grad_norm': '28.5', 'counters/examples': 134304, 'counters/updates': 4197}
train stats after 134336 examples: {'rewards_train/chosen': '0.14723', 'rewards_train/rejected': '0.017235', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12999', 'logps_train/rejected': '-110.98', 'logps_train/chosen': '-143.49', 'loss/train': '0.65477', 'examples_per_second': '31.435', 'grad_norm': '24.875', 'counters/examples': 134336, 'counters/updates': 4198}
train stats after 134368 examples: {'rewards_train/chosen': '0.13989', 'rewards_train/rejected': '0.04126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098632', 'logps_train/rejected': '-161.46', 'logps_train/chosen': '-132.26', 'loss/train': '0.65211', 'examples_per_second': '30.578', 'grad_norm': '27.5', 'counters/examples': 134368, 'counters/updates': 4199}
train stats after 134400 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.042053', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098107', 'logps_train/rejected': '-91.957', 'logps_train/chosen': '-154.2', 'loss/train': '0.65148', 'examples_per_second': '32.583', 'grad_norm': '25.375', 'counters/examples': 134400, 'counters/updates': 4200}
train stats after 134432 examples: {'rewards_train/chosen': '0.084804', 'rewards_train/rejected': '0.027628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057175', 'logps_train/rejected': '-102.04', 'logps_train/chosen': '-122.55', 'loss/train': '0.67066', 'examples_per_second': '31.527', 'grad_norm': '34.5', 'counters/examples': 134432, 'counters/updates': 4201}
train stats after 134464 examples: {'rewards_train/chosen': '0.14536', 'rewards_train/rejected': '0.020317', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12505', 'logps_train/rejected': '-119.77', 'logps_train/chosen': '-147.1', 'loss/train': '0.64511', 'examples_per_second': '30.918', 'grad_norm': '34', 'counters/examples': 134464, 'counters/updates': 4202}
train stats after 134496 examples: {'rewards_train/chosen': '0.15326', 'rewards_train/rejected': '0.025409', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12785', 'logps_train/rejected': '-124.61', 'logps_train/chosen': '-152.5', 'loss/train': '0.64039', 'examples_per_second': '30.627', 'grad_norm': '28.5', 'counters/examples': 134496, 'counters/updates': 4203}
train stats after 134528 examples: {'rewards_train/chosen': '0.18296', 'rewards_train/rejected': '0.077085', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10587', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-145.94', 'loss/train': '0.65858', 'examples_per_second': '30.768', 'grad_norm': '34.25', 'counters/examples': 134528, 'counters/updates': 4204}
train stats after 134560 examples: {'rewards_train/chosen': '0.097871', 'rewards_train/rejected': '-0.012324', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11019', 'logps_train/rejected': '-113.39', 'logps_train/chosen': '-116.1', 'loss/train': '0.64443', 'examples_per_second': '31.376', 'grad_norm': '26.875', 'counters/examples': 134560, 'counters/updates': 4205}
train stats after 134592 examples: {'rewards_train/chosen': '0.18329', 'rewards_train/rejected': '0.17905', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0042463', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-142.68', 'loss/train': '0.73383', 'examples_per_second': '31.28', 'grad_norm': '85.5', 'counters/examples': 134592, 'counters/updates': 4206}
train stats after 134624 examples: {'rewards_train/chosen': '0.10248', 'rewards_train/rejected': '0.030967', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071511', 'logps_train/rejected': '-144.2', 'logps_train/chosen': '-165.02', 'loss/train': '0.66569', 'examples_per_second': '31.43', 'grad_norm': '31.375', 'counters/examples': 134624, 'counters/updates': 4207}
train stats after 134656 examples: {'rewards_train/chosen': '0.060368', 'rewards_train/rejected': '0.063057', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0026888', 'logps_train/rejected': '-136.68', 'logps_train/chosen': '-162.09', 'loss/train': '0.71075', 'examples_per_second': '32.014', 'grad_norm': '41', 'counters/examples': 134656, 'counters/updates': 4208}
train stats after 134688 examples: {'rewards_train/chosen': '0.099553', 'rewards_train/rejected': '-0.02831', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12786', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-155.09', 'loss/train': '0.63581', 'examples_per_second': '31.545', 'grad_norm': '25.75', 'counters/examples': 134688, 'counters/updates': 4209}
train stats after 134720 examples: {'rewards_train/chosen': '0.14723', 'rewards_train/rejected': '0.016877', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13035', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-150', 'loss/train': '0.64085', 'examples_per_second': '30.027', 'grad_norm': '27.125', 'counters/examples': 134720, 'counters/updates': 4210}
train stats after 134752 examples: {'rewards_train/chosen': '0.21321', 'rewards_train/rejected': '0.036742', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17647', 'logps_train/rejected': '-126.18', 'logps_train/chosen': '-165.17', 'loss/train': '0.62223', 'examples_per_second': '31.412', 'grad_norm': '28.375', 'counters/examples': 134752, 'counters/updates': 4211}
skipping logging after 134784 examples to avoid logging too frequently
train stats after 134816 examples: {'rewards_train/chosen': '0.15443', 'rewards_train/rejected': '0.012349', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14208', 'logps_train/rejected': '-146.15', 'logps_train/chosen': '-141.58', 'loss/train': '0.6391', 'examples_per_second': '30.699', 'grad_norm': '27.875', 'counters/examples': 134816, 'counters/updates': 4213}
train stats after 134848 examples: {'rewards_train/chosen': '0.10689', 'rewards_train/rejected': '0.06779', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.039103', 'logps_train/rejected': '-145.34', 'logps_train/chosen': '-157.38', 'loss/train': '0.68303', 'examples_per_second': '30.719', 'grad_norm': '39.25', 'counters/examples': 134848, 'counters/updates': 4214}
train stats after 134880 examples: {'rewards_train/chosen': '0.084006', 'rewards_train/rejected': '-0.026478', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11048', 'logps_train/rejected': '-116.23', 'logps_train/chosen': '-136.25', 'loss/train': '0.6472', 'examples_per_second': '30.341', 'grad_norm': '28.125', 'counters/examples': 134880, 'counters/updates': 4215}
skipping logging after 134912 examples to avoid logging too frequently
train stats after 134944 examples: {'rewards_train/chosen': '0.20025', 'rewards_train/rejected': '0.099437', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10081', 'logps_train/rejected': '-148.42', 'logps_train/chosen': '-156.74', 'loss/train': '0.66297', 'examples_per_second': '34.324', 'grad_norm': '37.25', 'counters/examples': 134944, 'counters/updates': 4217}
train stats after 134976 examples: {'rewards_train/chosen': '0.21899', 'rewards_train/rejected': '0.085611', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13338', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-111.87', 'loss/train': '0.64004', 'examples_per_second': '31.491', 'grad_norm': '34.75', 'counters/examples': 134976, 'counters/updates': 4218}
train stats after 135008 examples: {'rewards_train/chosen': '0.053178', 'rewards_train/rejected': '-0.020155', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073333', 'logps_train/rejected': '-110.96', 'logps_train/chosen': '-134.61', 'loss/train': '0.66595', 'examples_per_second': '30.789', 'grad_norm': '29.875', 'counters/examples': 135008, 'counters/updates': 4219}
train stats after 135040 examples: {'rewards_train/chosen': '0.14235', 'rewards_train/rejected': '0.094294', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048059', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-135.94', 'loss/train': '0.67715', 'examples_per_second': '30.228', 'grad_norm': '32', 'counters/examples': 135040, 'counters/updates': 4220}
skipping logging after 135072 examples to avoid logging too frequently
train stats after 135104 examples: {'rewards_train/chosen': '0.18132', 'rewards_train/rejected': '0.0065209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1748', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-167.07', 'loss/train': '0.62511', 'examples_per_second': '32.286', 'grad_norm': '28.75', 'counters/examples': 135104, 'counters/updates': 4222}
train stats after 135136 examples: {'rewards_train/chosen': '0.18932', 'rewards_train/rejected': '0.14452', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044805', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-137.26', 'loss/train': '0.69387', 'examples_per_second': '30.036', 'grad_norm': '41', 'counters/examples': 135136, 'counters/updates': 4223}
skipping logging after 135168 examples to avoid logging too frequently
train stats after 135200 examples: {'rewards_train/chosen': '0.10279', 'rewards_train/rejected': '0.083543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019243', 'logps_train/rejected': '-163.49', 'logps_train/chosen': '-136.18', 'loss/train': '0.69574', 'examples_per_second': '31.486', 'grad_norm': '39.75', 'counters/examples': 135200, 'counters/updates': 4225}
skipping logging after 135232 examples to avoid logging too frequently
train stats after 135264 examples: {'rewards_train/chosen': '0.15296', 'rewards_train/rejected': '0.042379', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11059', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-160.05', 'loss/train': '0.64879', 'examples_per_second': '31.337', 'grad_norm': '31.5', 'counters/examples': 135264, 'counters/updates': 4227}
train stats after 135296 examples: {'rewards_train/chosen': '0.073949', 'rewards_train/rejected': '0.087704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.013754', 'logps_train/rejected': '-120.75', 'logps_train/chosen': '-139.88', 'loss/train': '0.70965', 'examples_per_second': '31.982', 'grad_norm': '31.375', 'counters/examples': 135296, 'counters/updates': 4228}
train stats after 135328 examples: {'rewards_train/chosen': '0.20001', 'rewards_train/rejected': '0.10615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093854', 'logps_train/rejected': '-127.33', 'logps_train/chosen': '-147.93', 'loss/train': '0.66327', 'examples_per_second': '31.479', 'grad_norm': '31.625', 'counters/examples': 135328, 'counters/updates': 4229}
train stats after 135360 examples: {'rewards_train/chosen': '0.16963', 'rewards_train/rejected': '0.078414', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091211', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-158.78', 'loss/train': '0.65586', 'examples_per_second': '32.421', 'grad_norm': '26.125', 'counters/examples': 135360, 'counters/updates': 4230}
skipping logging after 135392 examples to avoid logging too frequently
train stats after 135424 examples: {'rewards_train/chosen': '0.23207', 'rewards_train/rejected': '0.076489', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15558', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-158.4', 'loss/train': '0.63576', 'examples_per_second': '30.359', 'grad_norm': '36.75', 'counters/examples': 135424, 'counters/updates': 4232}
train stats after 135456 examples: {'rewards_train/chosen': '0.094009', 'rewards_train/rejected': '-0.0085865', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1026', 'logps_train/rejected': '-109.94', 'logps_train/chosen': '-115.89', 'loss/train': '0.65798', 'examples_per_second': '32.516', 'grad_norm': '25.25', 'counters/examples': 135456, 'counters/updates': 4233}
train stats after 135488 examples: {'rewards_train/chosen': '0.045088', 'rewards_train/rejected': '0.078284', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.033196', 'logps_train/rejected': '-96.937', 'logps_train/chosen': '-117.68', 'loss/train': '0.72046', 'examples_per_second': '30.913', 'grad_norm': '27.125', 'counters/examples': 135488, 'counters/updates': 4234}
train stats after 135520 examples: {'rewards_train/chosen': '0.15456', 'rewards_train/rejected': '0.048958', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1056', 'logps_train/rejected': '-105.32', 'logps_train/chosen': '-149.58', 'loss/train': '0.64936', 'examples_per_second': '23.524', 'grad_norm': '33.75', 'counters/examples': 135520, 'counters/updates': 4235}
train stats after 135552 examples: {'rewards_train/chosen': '0.16052', 'rewards_train/rejected': '0.13281', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027713', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-140.1', 'loss/train': '0.70542', 'examples_per_second': '31.37', 'grad_norm': '40.75', 'counters/examples': 135552, 'counters/updates': 4236}
train stats after 135584 examples: {'rewards_train/chosen': '0.17497', 'rewards_train/rejected': '0.054988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11998', 'logps_train/rejected': '-102.68', 'logps_train/chosen': '-125.49', 'loss/train': '0.64621', 'examples_per_second': '31.501', 'grad_norm': '32.25', 'counters/examples': 135584, 'counters/updates': 4237}
train stats after 135616 examples: {'rewards_train/chosen': '0.14601', 'rewards_train/rejected': '0.027338', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11867', 'logps_train/rejected': '-126.06', 'logps_train/chosen': '-135.95', 'loss/train': '0.64906', 'examples_per_second': '24.805', 'grad_norm': '30.75', 'counters/examples': 135616, 'counters/updates': 4238}
train stats after 135648 examples: {'rewards_train/chosen': '0.068363', 'rewards_train/rejected': '0.13073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.062367', 'logps_train/rejected': '-132.72', 'logps_train/chosen': '-150.92', 'loss/train': '0.74511', 'examples_per_second': '32.798', 'grad_norm': '52.5', 'counters/examples': 135648, 'counters/updates': 4239}
train stats after 135680 examples: {'rewards_train/chosen': '0.21229', 'rewards_train/rejected': '0.090106', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12218', 'logps_train/rejected': '-114.22', 'logps_train/chosen': '-160.96', 'loss/train': '0.64417', 'examples_per_second': '31.511', 'grad_norm': '34.25', 'counters/examples': 135680, 'counters/updates': 4240}
train stats after 135712 examples: {'rewards_train/chosen': '0.19743', 'rewards_train/rejected': '0.097597', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099832', 'logps_train/rejected': '-155.87', 'logps_train/chosen': '-174.03', 'loss/train': '0.66105', 'examples_per_second': '30.86', 'grad_norm': '33.5', 'counters/examples': 135712, 'counters/updates': 4241}
train stats after 135744 examples: {'rewards_train/chosen': '0.12899', 'rewards_train/rejected': '0.086885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042106', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-130.53', 'loss/train': '0.6981', 'examples_per_second': '32.011', 'grad_norm': '42', 'counters/examples': 135744, 'counters/updates': 4242}
train stats after 135776 examples: {'rewards_train/chosen': '0.28387', 'rewards_train/rejected': '0.059169', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2247', 'logps_train/rejected': '-132.58', 'logps_train/chosen': '-156.09', 'loss/train': '0.60371', 'examples_per_second': '31.235', 'grad_norm': '29.25', 'counters/examples': 135776, 'counters/updates': 4243}
train stats after 135808 examples: {'rewards_train/chosen': '0.14183', 'rewards_train/rejected': '0.10491', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036924', 'logps_train/rejected': '-129.5', 'logps_train/chosen': '-153.29', 'loss/train': '0.68038', 'examples_per_second': '31.566', 'grad_norm': '59.25', 'counters/examples': 135808, 'counters/updates': 4244}
train stats after 135840 examples: {'rewards_train/chosen': '0.25359', 'rewards_train/rejected': '0.013136', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24046', 'logps_train/rejected': '-146.48', 'logps_train/chosen': '-145.98', 'loss/train': '0.63802', 'examples_per_second': '30.536', 'grad_norm': '25.25', 'counters/examples': 135840, 'counters/updates': 4245}
train stats after 135872 examples: {'rewards_train/chosen': '0.11442', 'rewards_train/rejected': '0.10306', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011357', 'logps_train/rejected': '-125.28', 'logps_train/chosen': '-120.97', 'loss/train': '0.69573', 'examples_per_second': '32.609', 'grad_norm': '28.375', 'counters/examples': 135872, 'counters/updates': 4246}
train stats after 135904 examples: {'rewards_train/chosen': '0.1053', 'rewards_train/rejected': '0.089929', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015375', 'logps_train/rejected': '-96.233', 'logps_train/chosen': '-133', 'loss/train': '0.69132', 'examples_per_second': '32.088', 'grad_norm': '35.5', 'counters/examples': 135904, 'counters/updates': 4247}
train stats after 135936 examples: {'rewards_train/chosen': '0.12711', 'rewards_train/rejected': '0.028666', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.098441', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-182.22', 'loss/train': '0.65133', 'examples_per_second': '30.729', 'grad_norm': '32.75', 'counters/examples': 135936, 'counters/updates': 4248}
train stats after 135968 examples: {'rewards_train/chosen': '0.10787', 'rewards_train/rejected': '0.05195', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055917', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-137.96', 'loss/train': '0.67714', 'examples_per_second': '32.923', 'grad_norm': '30.125', 'counters/examples': 135968, 'counters/updates': 4249}
train stats after 136000 examples: {'rewards_train/chosen': '0.067949', 'rewards_train/rejected': '0.083793', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015844', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-127.86', 'loss/train': '0.7178', 'examples_per_second': '30.635', 'grad_norm': '48', 'counters/examples': 136000, 'counters/updates': 4250}
train stats after 136032 examples: {'rewards_train/chosen': '0.14877', 'rewards_train/rejected': '0.048977', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099791', 'logps_train/rejected': '-164.64', 'logps_train/chosen': '-160.17', 'loss/train': '0.66131', 'examples_per_second': '33.064', 'grad_norm': '35.75', 'counters/examples': 136032, 'counters/updates': 4251}
train stats after 136064 examples: {'rewards_train/chosen': '0.066914', 'rewards_train/rejected': '0.065144', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0017699', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-132.82', 'loss/train': '0.69753', 'examples_per_second': '30.341', 'grad_norm': '27.125', 'counters/examples': 136064, 'counters/updates': 4252}
train stats after 136096 examples: {'rewards_train/chosen': '0.17254', 'rewards_train/rejected': '0.1237', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.048843', 'logps_train/rejected': '-150.48', 'logps_train/chosen': '-182.59', 'loss/train': '0.68595', 'examples_per_second': '31.629', 'grad_norm': '34.5', 'counters/examples': 136096, 'counters/updates': 4253}
skipping logging after 136128 examples to avoid logging too frequently
train stats after 136160 examples: {'rewards_train/chosen': '0.08238', 'rewards_train/rejected': '-0.0013894', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083769', 'logps_train/rejected': '-97.718', 'logps_train/chosen': '-156.06', 'loss/train': '0.65658', 'examples_per_second': '33.256', 'grad_norm': '31.875', 'counters/examples': 136160, 'counters/updates': 4255}
train stats after 136192 examples: {'rewards_train/chosen': '0.12833', 'rewards_train/rejected': '0.098616', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029711', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-153.23', 'loss/train': '0.69549', 'examples_per_second': '31.445', 'grad_norm': '32.75', 'counters/examples': 136192, 'counters/updates': 4256}
train stats after 136224 examples: {'rewards_train/chosen': '0.13627', 'rewards_train/rejected': '0.16088', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.024611', 'logps_train/rejected': '-146.8', 'logps_train/chosen': '-134.53', 'loss/train': '0.75671', 'examples_per_second': '30.687', 'grad_norm': '51.75', 'counters/examples': 136224, 'counters/updates': 4257}
train stats after 136256 examples: {'rewards_train/chosen': '0.10386', 'rewards_train/rejected': '-0.047759', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15162', 'logps_train/rejected': '-111.11', 'logps_train/chosen': '-129.16', 'loss/train': '0.63662', 'examples_per_second': '31.393', 'grad_norm': '31.625', 'counters/examples': 136256, 'counters/updates': 4258}
train stats after 136288 examples: {'rewards_train/chosen': '0.21666', 'rewards_train/rejected': '0.12582', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090836', 'logps_train/rejected': '-152.26', 'logps_train/chosen': '-150.15', 'loss/train': '0.67111', 'examples_per_second': '30.982', 'grad_norm': '33.75', 'counters/examples': 136288, 'counters/updates': 4259}
skipping logging after 136320 examples to avoid logging too frequently
train stats after 136352 examples: {'rewards_train/chosen': '0.15911', 'rewards_train/rejected': '-0.019723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17884', 'logps_train/rejected': '-138.47', 'logps_train/chosen': '-171.47', 'loss/train': '0.61665', 'examples_per_second': '31.421', 'grad_norm': '28.5', 'counters/examples': 136352, 'counters/updates': 4261}
skipping logging after 136384 examples to avoid logging too frequently
train stats after 136416 examples: {'rewards_train/chosen': '0.18164', 'rewards_train/rejected': '0.15728', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.024366', 'logps_train/rejected': '-146.97', 'logps_train/chosen': '-142.54', 'loss/train': '0.69451', 'examples_per_second': '31.502', 'grad_norm': '35', 'counters/examples': 136416, 'counters/updates': 4263}
train stats after 136448 examples: {'rewards_train/chosen': '0.12006', 'rewards_train/rejected': '0.1097', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010362', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-135.38', 'loss/train': '0.70029', 'examples_per_second': '30.172', 'grad_norm': '35', 'counters/examples': 136448, 'counters/updates': 4264}
train stats after 136480 examples: {'rewards_train/chosen': '0.16002', 'rewards_train/rejected': '0.087006', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073009', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-167.54', 'loss/train': '0.66526', 'examples_per_second': '31.166', 'grad_norm': '36.5', 'counters/examples': 136480, 'counters/updates': 4265}
skipping logging after 136512 examples to avoid logging too frequently
train stats after 136544 examples: {'rewards_train/chosen': '0.16398', 'rewards_train/rejected': '0.10651', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057468', 'logps_train/rejected': '-130.73', 'logps_train/chosen': '-161.45', 'loss/train': '0.67788', 'examples_per_second': '31.007', 'grad_norm': '34.75', 'counters/examples': 136544, 'counters/updates': 4267}
train stats after 136576 examples: {'rewards_train/chosen': '0.16019', 'rewards_train/rejected': '0.07815', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082035', 'logps_train/rejected': '-148.05', 'logps_train/chosen': '-140.4', 'loss/train': '0.66829', 'examples_per_second': '30.145', 'grad_norm': '44.5', 'counters/examples': 136576, 'counters/updates': 4268}
train stats after 136608 examples: {'rewards_train/chosen': '0.1353', 'rewards_train/rejected': '0.11603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01927', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-126.72', 'loss/train': '0.69814', 'examples_per_second': '30.989', 'grad_norm': '27.375', 'counters/examples': 136608, 'counters/updates': 4269}
train stats after 136640 examples: {'rewards_train/chosen': '0.11213', 'rewards_train/rejected': '0.082048', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03008', 'logps_train/rejected': '-114.03', 'logps_train/chosen': '-126.74', 'loss/train': '0.69075', 'examples_per_second': '31.27', 'grad_norm': '24.125', 'counters/examples': 136640, 'counters/updates': 4270}
train stats after 136672 examples: {'rewards_train/chosen': '0.17768', 'rewards_train/rejected': '0.0033599', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17432', 'logps_train/rejected': '-124.41', 'logps_train/chosen': '-167.03', 'loss/train': '0.62317', 'examples_per_second': '30.62', 'grad_norm': '33.25', 'counters/examples': 136672, 'counters/updates': 4271}
train stats after 136704 examples: {'rewards_train/chosen': '0.089462', 'rewards_train/rejected': '0.026066', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063396', 'logps_train/rejected': '-143.3', 'logps_train/chosen': '-166.05', 'loss/train': '0.67511', 'examples_per_second': '31.666', 'grad_norm': '35.5', 'counters/examples': 136704, 'counters/updates': 4272}
skipping logging after 136736 examples to avoid logging too frequently
train stats after 136768 examples: {'rewards_train/chosen': '0.086953', 'rewards_train/rejected': '0.027719', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059234', 'logps_train/rejected': '-118.62', 'logps_train/chosen': '-125.95', 'loss/train': '0.67623', 'examples_per_second': '33.188', 'grad_norm': '27.125', 'counters/examples': 136768, 'counters/updates': 4274}
train stats after 136800 examples: {'rewards_train/chosen': '0.11576', 'rewards_train/rejected': '-0.00078273', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11654', 'logps_train/rejected': '-94.427', 'logps_train/chosen': '-116.1', 'loss/train': '0.64327', 'examples_per_second': '30.566', 'grad_norm': '50.5', 'counters/examples': 136800, 'counters/updates': 4275}
train stats after 136832 examples: {'rewards_train/chosen': '0.19274', 'rewards_train/rejected': '0.04233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15041', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-150.86', 'loss/train': '0.63905', 'examples_per_second': '32.576', 'grad_norm': '33', 'counters/examples': 136832, 'counters/updates': 4276}
train stats after 136864 examples: {'rewards_train/chosen': '0.10591', 'rewards_train/rejected': '0.074564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031349', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-140.78', 'loss/train': '0.68757', 'examples_per_second': '30.822', 'grad_norm': '32.25', 'counters/examples': 136864, 'counters/updates': 4277}
train stats after 136896 examples: {'rewards_train/chosen': '0.14347', 'rewards_train/rejected': '0.14424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.00076735', 'logps_train/rejected': '-143.63', 'logps_train/chosen': '-140.29', 'loss/train': '0.71039', 'examples_per_second': '31.37', 'grad_norm': '30.875', 'counters/examples': 136896, 'counters/updates': 4278}
train stats after 136928 examples: {'rewards_train/chosen': '0.086515', 'rewards_train/rejected': '0.0063786', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080136', 'logps_train/rejected': '-127.24', 'logps_train/chosen': '-143.82', 'loss/train': '0.65988', 'examples_per_second': '31.504', 'grad_norm': '40.5', 'counters/examples': 136928, 'counters/updates': 4279}
train stats after 136960 examples: {'rewards_train/chosen': '0.14061', 'rewards_train/rejected': '0.080793', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059819', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-154.37', 'loss/train': '0.66943', 'examples_per_second': '32.958', 'grad_norm': '28.875', 'counters/examples': 136960, 'counters/updates': 4280}
train stats after 136992 examples: {'rewards_train/chosen': '0.19329', 'rewards_train/rejected': '0.072918', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12037', 'logps_train/rejected': '-87.685', 'logps_train/chosen': '-150.92', 'loss/train': '0.65549', 'examples_per_second': '29.729', 'grad_norm': '37.75', 'counters/examples': 136992, 'counters/updates': 4281}
train stats after 137024 examples: {'rewards_train/chosen': '0.16645', 'rewards_train/rejected': '0.13538', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031072', 'logps_train/rejected': '-174.75', 'logps_train/chosen': '-182.24', 'loss/train': '0.68856', 'examples_per_second': '32.806', 'grad_norm': '49', 'counters/examples': 137024, 'counters/updates': 4282}
train stats after 137056 examples: {'rewards_train/chosen': '0.17912', 'rewards_train/rejected': '-0.010658', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18978', 'logps_train/rejected': '-124.69', 'logps_train/chosen': '-157.57', 'loss/train': '0.62369', 'examples_per_second': '31.481', 'grad_norm': '30', 'counters/examples': 137056, 'counters/updates': 4283}
train stats after 137088 examples: {'rewards_train/chosen': '0.089858', 'rewards_train/rejected': '0.060106', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029752', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-129.07', 'loss/train': '0.6862', 'examples_per_second': '30.55', 'grad_norm': '32', 'counters/examples': 137088, 'counters/updates': 4284}
train stats after 137120 examples: {'rewards_train/chosen': '0.1821', 'rewards_train/rejected': '0.16585', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01625', 'logps_train/rejected': '-127.67', 'logps_train/chosen': '-189.2', 'loss/train': '0.70961', 'examples_per_second': '31.46', 'grad_norm': '48.75', 'counters/examples': 137120, 'counters/updates': 4285}
train stats after 137152 examples: {'rewards_train/chosen': '0.051721', 'rewards_train/rejected': '0.055933', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0042117', 'logps_train/rejected': '-136.32', 'logps_train/chosen': '-166.99', 'loss/train': '0.70457', 'examples_per_second': '31.306', 'grad_norm': '29.5', 'counters/examples': 137152, 'counters/updates': 4286}
train stats after 137184 examples: {'rewards_train/chosen': '0.068141', 'rewards_train/rejected': '-0.0044588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0726', 'logps_train/rejected': '-113.76', 'logps_train/chosen': '-172.26', 'loss/train': '0.67554', 'examples_per_second': '31.492', 'grad_norm': '30.625', 'counters/examples': 137184, 'counters/updates': 4287}
train stats after 137216 examples: {'rewards_train/chosen': '0.11752', 'rewards_train/rejected': '0.12073', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0032083', 'logps_train/rejected': '-94.385', 'logps_train/chosen': '-137.97', 'loss/train': '0.70479', 'examples_per_second': '31.715', 'grad_norm': '30.125', 'counters/examples': 137216, 'counters/updates': 4288}
train stats after 137248 examples: {'rewards_train/chosen': '0.21317', 'rewards_train/rejected': '0.089085', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12408', 'logps_train/rejected': '-144.82', 'logps_train/chosen': '-178.94', 'loss/train': '0.66645', 'examples_per_second': '30.043', 'grad_norm': '37.25', 'counters/examples': 137248, 'counters/updates': 4289}
skipping logging after 137280 examples to avoid logging too frequently
train stats after 137312 examples: {'rewards_train/chosen': '0.10856', 'rewards_train/rejected': '0.013453', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095108', 'logps_train/rejected': '-111.41', 'logps_train/chosen': '-140.04', 'loss/train': '0.65207', 'examples_per_second': '35.592', 'grad_norm': '25.375', 'counters/examples': 137312, 'counters/updates': 4291}
train stats after 137344 examples: {'rewards_train/chosen': '0.12059', 'rewards_train/rejected': '0.11227', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0083151', 'logps_train/rejected': '-143.34', 'logps_train/chosen': '-137.13', 'loss/train': '0.70179', 'examples_per_second': '30.098', 'grad_norm': '37.75', 'counters/examples': 137344, 'counters/updates': 4292}
train stats after 137376 examples: {'rewards_train/chosen': '0.13491', 'rewards_train/rejected': '0.058961', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075951', 'logps_train/rejected': '-136.66', 'logps_train/chosen': '-171.32', 'loss/train': '0.67196', 'examples_per_second': '31.889', 'grad_norm': '32.75', 'counters/examples': 137376, 'counters/updates': 4293}
train stats after 137408 examples: {'rewards_train/chosen': '0.091049', 'rewards_train/rejected': '0.091101', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-5.2291e-05', 'logps_train/rejected': '-170.45', 'logps_train/chosen': '-164.34', 'loss/train': '0.7088', 'examples_per_second': '32.27', 'grad_norm': '48', 'counters/examples': 137408, 'counters/updates': 4294}
skipping logging after 137440 examples to avoid logging too frequently
train stats after 137472 examples: {'rewards_train/chosen': '0.15247', 'rewards_train/rejected': '0.069819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082648', 'logps_train/rejected': '-148.71', 'logps_train/chosen': '-162.47', 'loss/train': '0.66418', 'examples_per_second': '31.24', 'grad_norm': '33.75', 'counters/examples': 137472, 'counters/updates': 4296}
train stats after 137504 examples: {'rewards_train/chosen': '0.078551', 'rewards_train/rejected': '0.050279', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028272', 'logps_train/rejected': '-106.48', 'logps_train/chosen': '-132.21', 'loss/train': '0.68754', 'examples_per_second': '31.552', 'grad_norm': '28.625', 'counters/examples': 137504, 'counters/updates': 4297}
train stats after 137536 examples: {'rewards_train/chosen': '0.086128', 'rewards_train/rejected': '-0.037054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12318', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-128.07', 'loss/train': '0.64303', 'examples_per_second': '31.571', 'grad_norm': '31.25', 'counters/examples': 137536, 'counters/updates': 4298}
train stats after 137568 examples: {'rewards_train/chosen': '0.19343', 'rewards_train/rejected': '0.095616', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097813', 'logps_train/rejected': '-164.31', 'logps_train/chosen': '-181.3', 'loss/train': '0.67456', 'examples_per_second': '30.024', 'grad_norm': '42.25', 'counters/examples': 137568, 'counters/updates': 4299}
skipping logging after 137600 examples to avoid logging too frequently
train stats after 137632 examples: {'rewards_train/chosen': '0.15087', 'rewards_train/rejected': '0.010821', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14005', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-142.99', 'loss/train': '0.63009', 'examples_per_second': '31.575', 'grad_norm': '26.25', 'counters/examples': 137632, 'counters/updates': 4301}
train stats after 137664 examples: {'rewards_train/chosen': '0.094452', 'rewards_train/rejected': '0.0069692', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087483', 'logps_train/rejected': '-84.457', 'logps_train/chosen': '-116.99', 'loss/train': '0.65775', 'examples_per_second': '31.953', 'grad_norm': '28.5', 'counters/examples': 137664, 'counters/updates': 4302}
train stats after 137696 examples: {'rewards_train/chosen': '0.18387', 'rewards_train/rejected': '0.087892', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095977', 'logps_train/rejected': '-133.51', 'logps_train/chosen': '-168.37', 'loss/train': '0.6584', 'examples_per_second': '31.529', 'grad_norm': '36', 'counters/examples': 137696, 'counters/updates': 4303}
train stats after 137728 examples: {'rewards_train/chosen': '0.11757', 'rewards_train/rejected': '0.015827', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10174', 'logps_train/rejected': '-120.41', 'logps_train/chosen': '-149.13', 'loss/train': '0.65198', 'examples_per_second': '32.799', 'grad_norm': '33.25', 'counters/examples': 137728, 'counters/updates': 4304}
train stats after 137760 examples: {'rewards_train/chosen': '0.10516', 'rewards_train/rejected': '0.068019', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037144', 'logps_train/rejected': '-128.59', 'logps_train/chosen': '-127.37', 'loss/train': '0.68457', 'examples_per_second': '30.513', 'grad_norm': '40.25', 'counters/examples': 137760, 'counters/updates': 4305}
train stats after 137792 examples: {'rewards_train/chosen': '0.081561', 'rewards_train/rejected': '0.0108', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070761', 'logps_train/rejected': '-98.529', 'logps_train/chosen': '-125.26', 'loss/train': '0.66399', 'examples_per_second': '32.187', 'grad_norm': '24.125', 'counters/examples': 137792, 'counters/updates': 4306}
train stats after 137824 examples: {'rewards_train/chosen': '0.22182', 'rewards_train/rejected': '0.011783', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21003', 'logps_train/rejected': '-105.22', 'logps_train/chosen': '-159.01', 'loss/train': '0.60908', 'examples_per_second': '31.568', 'grad_norm': '28.125', 'counters/examples': 137824, 'counters/updates': 4307}
train stats after 137856 examples: {'rewards_train/chosen': '0.1914', 'rewards_train/rejected': '-0.016413', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20781', 'logps_train/rejected': '-102.15', 'logps_train/chosen': '-123.79', 'loss/train': '0.60466', 'examples_per_second': '30.388', 'grad_norm': '37', 'counters/examples': 137856, 'counters/updates': 4308}
train stats after 137888 examples: {'rewards_train/chosen': '0.13319', 'rewards_train/rejected': '0.044712', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088473', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-122.63', 'loss/train': '0.65785', 'examples_per_second': '32.644', 'grad_norm': '26.875', 'counters/examples': 137888, 'counters/updates': 4309}
train stats after 137920 examples: {'rewards_train/chosen': '0.18064', 'rewards_train/rejected': '0.06215', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11849', 'logps_train/rejected': '-122.75', 'logps_train/chosen': '-135.68', 'loss/train': '0.64615', 'examples_per_second': '30.988', 'grad_norm': '26', 'counters/examples': 137920, 'counters/updates': 4310}
train stats after 137952 examples: {'rewards_train/chosen': '0.17749', 'rewards_train/rejected': '0.10145', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076036', 'logps_train/rejected': '-146.17', 'logps_train/chosen': '-154.21', 'loss/train': '0.67861', 'examples_per_second': '30.469', 'grad_norm': '33.5', 'counters/examples': 137952, 'counters/updates': 4311}
train stats after 137984 examples: {'rewards_train/chosen': '0.197', 'rewards_train/rejected': '0.0068123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19019', 'logps_train/rejected': '-143.57', 'logps_train/chosen': '-178.43', 'loss/train': '0.63733', 'examples_per_second': '30.916', 'grad_norm': '30.375', 'counters/examples': 137984, 'counters/updates': 4312}
train stats after 138016 examples: {'rewards_train/chosen': '0.17019', 'rewards_train/rejected': '0.052956', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11723', 'logps_train/rejected': '-129.47', 'logps_train/chosen': '-141.93', 'loss/train': '0.64716', 'examples_per_second': '30.398', 'grad_norm': '26.375', 'counters/examples': 138016, 'counters/updates': 4313}
train stats after 138048 examples: {'rewards_train/chosen': '0.11925', 'rewards_train/rejected': '0.026361', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092886', 'logps_train/rejected': '-119.46', 'logps_train/chosen': '-130.48', 'loss/train': '0.65366', 'examples_per_second': '31.078', 'grad_norm': '26.125', 'counters/examples': 138048, 'counters/updates': 4314}
train stats after 138080 examples: {'rewards_train/chosen': '0.10041', 'rewards_train/rejected': '0.028423', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071988', 'logps_train/rejected': '-107.88', 'logps_train/chosen': '-143.65', 'loss/train': '0.66227', 'examples_per_second': '22.412', 'grad_norm': '32.5', 'counters/examples': 138080, 'counters/updates': 4315}
train stats after 138112 examples: {'rewards_train/chosen': '0.18613', 'rewards_train/rejected': '0.041777', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14435', 'logps_train/rejected': '-167.1', 'logps_train/chosen': '-191.47', 'loss/train': '0.63399', 'examples_per_second': '31.132', 'grad_norm': '38.25', 'counters/examples': 138112, 'counters/updates': 4316}
train stats after 138144 examples: {'rewards_train/chosen': '0.16942', 'rewards_train/rejected': '0.12558', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043841', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-175.6', 'loss/train': '0.69143', 'examples_per_second': '32.346', 'grad_norm': '41.25', 'counters/examples': 138144, 'counters/updates': 4317}
skipping logging after 138176 examples to avoid logging too frequently
train stats after 138208 examples: {'rewards_train/chosen': '0.1423', 'rewards_train/rejected': '0.039947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10235', 'logps_train/rejected': '-168.31', 'logps_train/chosen': '-157.4', 'loss/train': '0.65318', 'examples_per_second': '29.798', 'grad_norm': '30.5', 'counters/examples': 138208, 'counters/updates': 4319}
train stats after 138240 examples: {'rewards_train/chosen': '0.13693', 'rewards_train/rejected': '0.050977', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085954', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-132.78', 'loss/train': '0.66193', 'examples_per_second': '31.989', 'grad_norm': '23', 'counters/examples': 138240, 'counters/updates': 4320}
train stats after 138272 examples: {'rewards_train/chosen': '0.069376', 'rewards_train/rejected': '0.061713', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0076633', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-97.353', 'loss/train': '0.69746', 'examples_per_second': '31.063', 'grad_norm': '26.625', 'counters/examples': 138272, 'counters/updates': 4321}
train stats after 138304 examples: {'rewards_train/chosen': '0.098042', 'rewards_train/rejected': '0.047538', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050504', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-132.46', 'loss/train': '0.67746', 'examples_per_second': '30.047', 'grad_norm': '29.25', 'counters/examples': 138304, 'counters/updates': 4322}
train stats after 138336 examples: {'rewards_train/chosen': '0.053159', 'rewards_train/rejected': '0.13812', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.084958', 'logps_train/rejected': '-135.79', 'logps_train/chosen': '-139.53', 'loss/train': '0.76076', 'examples_per_second': '31.199', 'grad_norm': '37.5', 'counters/examples': 138336, 'counters/updates': 4323}
train stats after 138368 examples: {'rewards_train/chosen': '0.17213', 'rewards_train/rejected': '0.084047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088081', 'logps_train/rejected': '-133.11', 'logps_train/chosen': '-163.28', 'loss/train': '0.65999', 'examples_per_second': '30.863', 'grad_norm': '30.375', 'counters/examples': 138368, 'counters/updates': 4324}
train stats after 138400 examples: {'rewards_train/chosen': '0.090488', 'rewards_train/rejected': '-0.0012545', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091743', 'logps_train/rejected': '-105.04', 'logps_train/chosen': '-127.05', 'loss/train': '0.67585', 'examples_per_second': '30.797', 'grad_norm': '46.25', 'counters/examples': 138400, 'counters/updates': 4325}
train stats after 138432 examples: {'rewards_train/chosen': '0.10718', 'rewards_train/rejected': '0.12403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.016847', 'logps_train/rejected': '-124.83', 'logps_train/chosen': '-106.47', 'loss/train': '0.70849', 'examples_per_second': '30.778', 'grad_norm': '36.25', 'counters/examples': 138432, 'counters/updates': 4326}
train stats after 138464 examples: {'rewards_train/chosen': '0.21647', 'rewards_train/rejected': '0.050572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16589', 'logps_train/rejected': '-163.94', 'logps_train/chosen': '-178.09', 'loss/train': '0.62834', 'examples_per_second': '31.489', 'grad_norm': '35.5', 'counters/examples': 138464, 'counters/updates': 4327}
train stats after 138496 examples: {'rewards_train/chosen': '0.15055', 'rewards_train/rejected': '0.040455', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1101', 'logps_train/rejected': '-131.97', 'logps_train/chosen': '-143.51', 'loss/train': '0.65199', 'examples_per_second': '31.429', 'grad_norm': '34.25', 'counters/examples': 138496, 'counters/updates': 4328}
train stats after 138528 examples: {'rewards_train/chosen': '0.29227', 'rewards_train/rejected': '0.068887', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22338', 'logps_train/rejected': '-165.45', 'logps_train/chosen': '-164.84', 'loss/train': '0.63816', 'examples_per_second': '31.643', 'grad_norm': '35.75', 'counters/examples': 138528, 'counters/updates': 4329}
train stats after 138560 examples: {'rewards_train/chosen': '0.098914', 'rewards_train/rejected': '0.15942', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.060506', 'logps_train/rejected': '-145.27', 'logps_train/chosen': '-146.35', 'loss/train': '0.75078', 'examples_per_second': '31.463', 'grad_norm': '33', 'counters/examples': 138560, 'counters/updates': 4330}
train stats after 138592 examples: {'rewards_train/chosen': '0.12503', 'rewards_train/rejected': '0.17626', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.051233', 'logps_train/rejected': '-130.68', 'logps_train/chosen': '-148.85', 'loss/train': '0.74248', 'examples_per_second': '30.712', 'grad_norm': '39.5', 'counters/examples': 138592, 'counters/updates': 4331}
train stats after 138624 examples: {'rewards_train/chosen': '0.095404', 'rewards_train/rejected': '0.094633', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0007704', 'logps_train/rejected': '-137.77', 'logps_train/chosen': '-114.84', 'loss/train': '0.70222', 'examples_per_second': '33.053', 'grad_norm': '49.5', 'counters/examples': 138624, 'counters/updates': 4332}
train stats after 138656 examples: {'rewards_train/chosen': '0.18077', 'rewards_train/rejected': '0.016567', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1642', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-141.08', 'loss/train': '0.62091', 'examples_per_second': '32.721', 'grad_norm': '28.25', 'counters/examples': 138656, 'counters/updates': 4333}
train stats after 138688 examples: {'rewards_train/chosen': '0.12971', 'rewards_train/rejected': '0.024593', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10512', 'logps_train/rejected': '-99.238', 'logps_train/chosen': '-120.94', 'loss/train': '0.64835', 'examples_per_second': '30.346', 'grad_norm': '32.25', 'counters/examples': 138688, 'counters/updates': 4334}
train stats after 138720 examples: {'rewards_train/chosen': '0.14964', 'rewards_train/rejected': '0.079495', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070148', 'logps_train/rejected': '-110.76', 'logps_train/chosen': '-154.65', 'loss/train': '0.67952', 'examples_per_second': '30.952', 'grad_norm': '60.5', 'counters/examples': 138720, 'counters/updates': 4335}
train stats after 138752 examples: {'rewards_train/chosen': '0.072402', 'rewards_train/rejected': '0.032819', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039582', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-148.1', 'loss/train': '0.68105', 'examples_per_second': '31.498', 'grad_norm': '26.875', 'counters/examples': 138752, 'counters/updates': 4336}
skipping logging after 138784 examples to avoid logging too frequently
train stats after 138816 examples: {'rewards_train/chosen': '0.0906', 'rewards_train/rejected': '0.041629', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048972', 'logps_train/rejected': '-87.181', 'logps_train/chosen': '-107.86', 'loss/train': '0.67389', 'examples_per_second': '35.525', 'grad_norm': '33.75', 'counters/examples': 138816, 'counters/updates': 4338}
train stats after 138848 examples: {'rewards_train/chosen': '0.17111', 'rewards_train/rejected': '0.1124', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058715', 'logps_train/rejected': '-140.62', 'logps_train/chosen': '-152.66', 'loss/train': '0.67482', 'examples_per_second': '31.443', 'grad_norm': '26.375', 'counters/examples': 138848, 'counters/updates': 4339}
train stats after 138880 examples: {'rewards_train/chosen': '0.1625', 'rewards_train/rejected': '0.10314', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059365', 'logps_train/rejected': '-128.54', 'logps_train/chosen': '-135.35', 'loss/train': '0.67674', 'examples_per_second': '32.416', 'grad_norm': '36', 'counters/examples': 138880, 'counters/updates': 4340}
train stats after 138912 examples: {'rewards_train/chosen': '0.12023', 'rewards_train/rejected': '0.06466', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055567', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-161.11', 'loss/train': '0.67327', 'examples_per_second': '30.655', 'grad_norm': '28.5', 'counters/examples': 138912, 'counters/updates': 4341}
skipping logging after 138944 examples to avoid logging too frequently
train stats after 138976 examples: {'rewards_train/chosen': '0.063609', 'rewards_train/rejected': '0.046946', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.016663', 'logps_train/rejected': '-141.28', 'logps_train/chosen': '-141.43', 'loss/train': '0.69384', 'examples_per_second': '30.873', 'grad_norm': '28.625', 'counters/examples': 138976, 'counters/updates': 4343}
train stats after 139008 examples: {'rewards_train/chosen': '0.12983', 'rewards_train/rejected': '0.13033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00049782', 'logps_train/rejected': '-121.06', 'logps_train/chosen': '-153.68', 'loss/train': '0.71755', 'examples_per_second': '32.534', 'grad_norm': '45.75', 'counters/examples': 139008, 'counters/updates': 4344}
train stats after 139040 examples: {'rewards_train/chosen': '0.19574', 'rewards_train/rejected': '0.066279', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12946', 'logps_train/rejected': '-84.162', 'logps_train/chosen': '-150.94', 'loss/train': '0.63865', 'examples_per_second': '29.849', 'grad_norm': '24.25', 'counters/examples': 139040, 'counters/updates': 4345}
train stats after 139072 examples: {'rewards_train/chosen': '0.080074', 'rewards_train/rejected': '-0.011399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091473', 'logps_train/rejected': '-168.78', 'logps_train/chosen': '-182.54', 'loss/train': '0.67068', 'examples_per_second': '31.479', 'grad_norm': '50.5', 'counters/examples': 139072, 'counters/updates': 4346}
train stats after 139104 examples: {'rewards_train/chosen': '0.14559', 'rewards_train/rejected': '0.03021', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11538', 'logps_train/rejected': '-110', 'logps_train/chosen': '-121.04', 'loss/train': '0.64512', 'examples_per_second': '30.424', 'grad_norm': '25.5', 'counters/examples': 139104, 'counters/updates': 4347}
train stats after 139136 examples: {'rewards_train/chosen': '0.18924', 'rewards_train/rejected': '0.065866', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12337', 'logps_train/rejected': '-119.59', 'logps_train/chosen': '-145.66', 'loss/train': '0.64551', 'examples_per_second': '32.238', 'grad_norm': '33.25', 'counters/examples': 139136, 'counters/updates': 4348}
train stats after 139168 examples: {'rewards_train/chosen': '0.15245', 'rewards_train/rejected': '0.014038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13841', 'logps_train/rejected': '-163.68', 'logps_train/chosen': '-151.48', 'loss/train': '0.63379', 'examples_per_second': '31.982', 'grad_norm': '37.5', 'counters/examples': 139168, 'counters/updates': 4349}
train stats after 139200 examples: {'rewards_train/chosen': '0.11785', 'rewards_train/rejected': '0.035176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082676', 'logps_train/rejected': '-151.76', 'logps_train/chosen': '-174.87', 'loss/train': '0.66701', 'examples_per_second': '32.414', 'grad_norm': '34', 'counters/examples': 139200, 'counters/updates': 4350}
train stats after 139232 examples: {'rewards_train/chosen': '0.087086', 'rewards_train/rejected': '0.085019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0020673', 'logps_train/rejected': '-167.63', 'logps_train/chosen': '-141.82', 'loss/train': '0.70059', 'examples_per_second': '31.435', 'grad_norm': '42.25', 'counters/examples': 139232, 'counters/updates': 4351}
train stats after 139264 examples: {'rewards_train/chosen': '0.017173', 'rewards_train/rejected': '0.014377', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0027969', 'logps_train/rejected': '-159.17', 'logps_train/chosen': '-141.23', 'loss/train': '0.69827', 'examples_per_second': '31.312', 'grad_norm': '34', 'counters/examples': 139264, 'counters/updates': 4352}
train stats after 139296 examples: {'rewards_train/chosen': '0.16394', 'rewards_train/rejected': '0.050349', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11359', 'logps_train/rejected': '-148.84', 'logps_train/chosen': '-130.49', 'loss/train': '0.65338', 'examples_per_second': '31.43', 'grad_norm': '31.375', 'counters/examples': 139296, 'counters/updates': 4353}
skipping logging after 139328 examples to avoid logging too frequently
train stats after 139360 examples: {'rewards_train/chosen': '0.079587', 'rewards_train/rejected': '0.037145', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042441', 'logps_train/rejected': '-131.4', 'logps_train/chosen': '-142.44', 'loss/train': '0.68421', 'examples_per_second': '31.985', 'grad_norm': '30.5', 'counters/examples': 139360, 'counters/updates': 4355}
train stats after 139392 examples: {'rewards_train/chosen': '0.12853', 'rewards_train/rejected': '0.051332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077201', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-160.11', 'loss/train': '0.66519', 'examples_per_second': '32.392', 'grad_norm': '32.5', 'counters/examples': 139392, 'counters/updates': 4356}
skipping logging after 139424 examples to avoid logging too frequently
train stats after 139456 examples: {'rewards_train/chosen': '0.13876', 'rewards_train/rejected': '-0.012689', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15145', 'logps_train/rejected': '-157.5', 'logps_train/chosen': '-151.92', 'loss/train': '0.63738', 'examples_per_second': '31.199', 'grad_norm': '36.75', 'counters/examples': 139456, 'counters/updates': 4358}
train stats after 139488 examples: {'rewards_train/chosen': '0.071621', 'rewards_train/rejected': '0.039046', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.032576', 'logps_train/rejected': '-108.99', 'logps_train/chosen': '-129.85', 'loss/train': '0.68537', 'examples_per_second': '30.74', 'grad_norm': '29.75', 'counters/examples': 139488, 'counters/updates': 4359}
train stats after 139520 examples: {'rewards_train/chosen': '0.17989', 'rewards_train/rejected': '0.06331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11658', 'logps_train/rejected': '-130.13', 'logps_train/chosen': '-148.01', 'loss/train': '0.64494', 'examples_per_second': '31.47', 'grad_norm': '28.5', 'counters/examples': 139520, 'counters/updates': 4360}
train stats after 139552 examples: {'rewards_train/chosen': '0.13103', 'rewards_train/rejected': '0.032146', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098887', 'logps_train/rejected': '-99.477', 'logps_train/chosen': '-130.46', 'loss/train': '0.65651', 'examples_per_second': '31.552', 'grad_norm': '24.875', 'counters/examples': 139552, 'counters/updates': 4361}
train stats after 139584 examples: {'rewards_train/chosen': '0.14137', 'rewards_train/rejected': '0.10523', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036145', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-129.95', 'loss/train': '0.68151', 'examples_per_second': '31.435', 'grad_norm': '29.625', 'counters/examples': 139584, 'counters/updates': 4362}
train stats after 139616 examples: {'rewards_train/chosen': '0.085786', 'rewards_train/rejected': '0.04574', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040047', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-179.73', 'loss/train': '0.6842', 'examples_per_second': '31.996', 'grad_norm': '38.25', 'counters/examples': 139616, 'counters/updates': 4363}
skipping logging after 139648 examples to avoid logging too frequently
train stats after 139680 examples: {'rewards_train/chosen': '0.090631', 'rewards_train/rejected': '0.054797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035834', 'logps_train/rejected': '-108.5', 'logps_train/chosen': '-139.38', 'loss/train': '0.69642', 'examples_per_second': '35.936', 'grad_norm': '28.75', 'counters/examples': 139680, 'counters/updates': 4365}
train stats after 139712 examples: {'rewards_train/chosen': '0.10837', 'rewards_train/rejected': '0.080962', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027411', 'logps_train/rejected': '-142.83', 'logps_train/chosen': '-157.74', 'loss/train': '0.68874', 'examples_per_second': '29.758', 'grad_norm': '30', 'counters/examples': 139712, 'counters/updates': 4366}
train stats after 139744 examples: {'rewards_train/chosen': '0.11893', 'rewards_train/rejected': '0.035777', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08315', 'logps_train/rejected': '-119.06', 'logps_train/chosen': '-136.21', 'loss/train': '0.65647', 'examples_per_second': '31.379', 'grad_norm': '29.75', 'counters/examples': 139744, 'counters/updates': 4367}
train stats after 139776 examples: {'rewards_train/chosen': '0.11374', 'rewards_train/rejected': '0.092471', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.021266', 'logps_train/rejected': '-120.28', 'logps_train/chosen': '-126.62', 'loss/train': '0.69318', 'examples_per_second': '32.922', 'grad_norm': '29', 'counters/examples': 139776, 'counters/updates': 4368}
skipping logging after 139808 examples to avoid logging too frequently
train stats after 139840 examples: {'rewards_train/chosen': '0.10336', 'rewards_train/rejected': '0.13077', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.027411', 'logps_train/rejected': '-160.77', 'logps_train/chosen': '-143.98', 'loss/train': '0.71331', 'examples_per_second': '30.25', 'grad_norm': '34.25', 'counters/examples': 139840, 'counters/updates': 4370}
train stats after 139872 examples: {'rewards_train/chosen': '0.14117', 'rewards_train/rejected': '0.01654', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12463', 'logps_train/rejected': '-128.39', 'logps_train/chosen': '-147.26', 'loss/train': '0.64797', 'examples_per_second': '31.495', 'grad_norm': '28.375', 'counters/examples': 139872, 'counters/updates': 4371}
train stats after 139904 examples: {'rewards_train/chosen': '0.13088', 'rewards_train/rejected': '0.074141', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056737', 'logps_train/rejected': '-109.21', 'logps_train/chosen': '-140.39', 'loss/train': '0.67658', 'examples_per_second': '30.294', 'grad_norm': '26.5', 'counters/examples': 139904, 'counters/updates': 4372}
train stats after 139936 examples: {'rewards_train/chosen': '0.060633', 'rewards_train/rejected': '0.11447', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.053837', 'logps_train/rejected': '-171.24', 'logps_train/chosen': '-161.92', 'loss/train': '0.75644', 'examples_per_second': '31.339', 'grad_norm': '74.5', 'counters/examples': 139936, 'counters/updates': 4373}
train stats after 139968 examples: {'rewards_train/chosen': '0.14864', 'rewards_train/rejected': '0.11805', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030588', 'logps_train/rejected': '-137.5', 'logps_train/chosen': '-137.31', 'loss/train': '0.69476', 'examples_per_second': '31.089', 'grad_norm': '46.75', 'counters/examples': 139968, 'counters/updates': 4374}
train stats after 140000 examples: {'rewards_train/chosen': '0.11998', 'rewards_train/rejected': '0.11799', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0019936', 'logps_train/rejected': '-116.13', 'logps_train/chosen': '-133.98', 'loss/train': '0.70214', 'examples_per_second': '31.426', 'grad_norm': '35.5', 'counters/examples': 140000, 'counters/updates': 4375}
train stats after 140032 examples: {'rewards_train/chosen': '0.11117', 'rewards_train/rejected': '0.15473', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.043561', 'logps_train/rejected': '-164.1', 'logps_train/chosen': '-126.26', 'loss/train': '0.72867', 'examples_per_second': '30.619', 'grad_norm': '50.5', 'counters/examples': 140032, 'counters/updates': 4376}
train stats after 140064 examples: {'rewards_train/chosen': '0.074571', 'rewards_train/rejected': '0.038939', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.035632', 'logps_train/rejected': '-98.175', 'logps_train/chosen': '-128.27', 'loss/train': '0.68774', 'examples_per_second': '31.481', 'grad_norm': '44', 'counters/examples': 140064, 'counters/updates': 4377}
train stats after 140096 examples: {'rewards_train/chosen': '0.1295', 'rewards_train/rejected': '0.0065567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12295', 'logps_train/rejected': '-89.379', 'logps_train/chosen': '-152.55', 'loss/train': '0.64425', 'examples_per_second': '31.718', 'grad_norm': '25.375', 'counters/examples': 140096, 'counters/updates': 4378}
train stats after 140128 examples: {'rewards_train/chosen': '0.11079', 'rewards_train/rejected': '0.07727', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033521', 'logps_train/rejected': '-130.45', 'logps_train/chosen': '-144.69', 'loss/train': '0.68711', 'examples_per_second': '30.031', 'grad_norm': '43.5', 'counters/examples': 140128, 'counters/updates': 4379}
train stats after 140160 examples: {'rewards_train/chosen': '0.14764', 'rewards_train/rejected': '0.054868', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092769', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-150.49', 'loss/train': '0.65627', 'examples_per_second': '30.391', 'grad_norm': '36', 'counters/examples': 140160, 'counters/updates': 4380}
train stats after 140192 examples: {'rewards_train/chosen': '0.11786', 'rewards_train/rejected': '0.043883', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073974', 'logps_train/rejected': '-108.7', 'logps_train/chosen': '-127.13', 'loss/train': '0.66827', 'examples_per_second': '31.392', 'grad_norm': '28.375', 'counters/examples': 140192, 'counters/updates': 4381}
train stats after 140224 examples: {'rewards_train/chosen': '0.13358', 'rewards_train/rejected': '0.01766', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11592', 'logps_train/rejected': '-111.97', 'logps_train/chosen': '-195.1', 'loss/train': '0.6534', 'examples_per_second': '31.432', 'grad_norm': '44.75', 'counters/examples': 140224, 'counters/updates': 4382}
train stats after 140256 examples: {'rewards_train/chosen': '0.12767', 'rewards_train/rejected': '0.071401', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05627', 'logps_train/rejected': '-180.82', 'logps_train/chosen': '-175.99', 'loss/train': '0.6823', 'examples_per_second': '32.937', 'grad_norm': '38.25', 'counters/examples': 140256, 'counters/updates': 4383}
train stats after 140288 examples: {'rewards_train/chosen': '0.092863', 'rewards_train/rejected': '0.010452', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082412', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-154.73', 'loss/train': '0.66252', 'examples_per_second': '31.462', 'grad_norm': '28.25', 'counters/examples': 140288, 'counters/updates': 4384}
train stats after 140320 examples: {'rewards_train/chosen': '0.10654', 'rewards_train/rejected': '0.087869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018669', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-140.72', 'loss/train': '0.69044', 'examples_per_second': '30.404', 'grad_norm': '28', 'counters/examples': 140320, 'counters/updates': 4385}
skipping logging after 140352 examples to avoid logging too frequently
train stats after 140384 examples: {'rewards_train/chosen': '0.12363', 'rewards_train/rejected': '0.09734', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026289', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-172.45', 'loss/train': '0.69613', 'examples_per_second': '30.456', 'grad_norm': '31.5', 'counters/examples': 140384, 'counters/updates': 4387}
train stats after 140416 examples: {'rewards_train/chosen': '0.13593', 'rewards_train/rejected': '0.073704', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.062227', 'logps_train/rejected': '-149.76', 'logps_train/chosen': '-111.94', 'loss/train': '0.67647', 'examples_per_second': '30.913', 'grad_norm': '41.5', 'counters/examples': 140416, 'counters/updates': 4388}
train stats after 140448 examples: {'rewards_train/chosen': '0.11297', 'rewards_train/rejected': '0.03134', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08163', 'logps_train/rejected': '-142.14', 'logps_train/chosen': '-154.86', 'loss/train': '0.65995', 'examples_per_second': '32.29', 'grad_norm': '42.25', 'counters/examples': 140448, 'counters/updates': 4389}
skipping logging after 140480 examples to avoid logging too frequently
train stats after 140512 examples: {'rewards_train/chosen': '0.095387', 'rewards_train/rejected': '0.041416', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053971', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-133.11', 'loss/train': '0.67308', 'examples_per_second': '32.567', 'grad_norm': '27.375', 'counters/examples': 140512, 'counters/updates': 4391}
train stats after 140544 examples: {'rewards_train/chosen': '0.13918', 'rewards_train/rejected': '0.013628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12555', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-149.47', 'loss/train': '0.65079', 'examples_per_second': '32.112', 'grad_norm': '32.75', 'counters/examples': 140544, 'counters/updates': 4392}
train stats after 140576 examples: {'rewards_train/chosen': '0.18181', 'rewards_train/rejected': '0.063621', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11819', 'logps_train/rejected': '-150.9', 'logps_train/chosen': '-150.98', 'loss/train': '0.65428', 'examples_per_second': '30.682', 'grad_norm': '56', 'counters/examples': 140576, 'counters/updates': 4393}
train stats after 140608 examples: {'rewards_train/chosen': '0.14527', 'rewards_train/rejected': '0.12015', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025122', 'logps_train/rejected': '-119.54', 'logps_train/chosen': '-120.85', 'loss/train': '0.6971', 'examples_per_second': '31.553', 'grad_norm': '40.25', 'counters/examples': 140608, 'counters/updates': 4394}
train stats after 140640 examples: {'rewards_train/chosen': '0.11129', 'rewards_train/rejected': '0.049016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062274', 'logps_train/rejected': '-119.14', 'logps_train/chosen': '-125.68', 'loss/train': '0.67533', 'examples_per_second': '29.724', 'grad_norm': '38', 'counters/examples': 140640, 'counters/updates': 4395}
train stats after 140672 examples: {'rewards_train/chosen': '0.12776', 'rewards_train/rejected': '0.15639', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.028634', 'logps_train/rejected': '-178.18', 'logps_train/chosen': '-160.58', 'loss/train': '0.71942', 'examples_per_second': '32.343', 'grad_norm': '42.25', 'counters/examples': 140672, 'counters/updates': 4396}
train stats after 140704 examples: {'rewards_train/chosen': '0.16685', 'rewards_train/rejected': '0.05109', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11576', 'logps_train/rejected': '-146.93', 'logps_train/chosen': '-187.05', 'loss/train': '0.64655', 'examples_per_second': '30.669', 'grad_norm': '43.25', 'counters/examples': 140704, 'counters/updates': 4397}
train stats after 140736 examples: {'rewards_train/chosen': '0.13589', 'rewards_train/rejected': '0.04611', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089777', 'logps_train/rejected': '-105.52', 'logps_train/chosen': '-139.61', 'loss/train': '0.65571', 'examples_per_second': '31.221', 'grad_norm': '32.25', 'counters/examples': 140736, 'counters/updates': 4398}
train stats after 140768 examples: {'rewards_train/chosen': '0.05985', 'rewards_train/rejected': '0.057738', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0021121', 'logps_train/rejected': '-129.93', 'logps_train/chosen': '-138.2', 'loss/train': '0.6977', 'examples_per_second': '30.752', 'grad_norm': '28.625', 'counters/examples': 140768, 'counters/updates': 4399}
train stats after 140800 examples: {'rewards_train/chosen': '0.12215', 'rewards_train/rejected': '-0.014886', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13704', 'logps_train/rejected': '-108.14', 'logps_train/chosen': '-152.1', 'loss/train': '0.63297', 'examples_per_second': '31.766', 'grad_norm': '24.625', 'counters/examples': 140800, 'counters/updates': 4400}
train stats after 140832 examples: {'rewards_train/chosen': '0.23537', 'rewards_train/rejected': '0.078682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15669', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-171.1', 'loss/train': '0.62889', 'examples_per_second': '30.57', 'grad_norm': '28.25', 'counters/examples': 140832, 'counters/updates': 4401}
train stats after 140864 examples: {'rewards_train/chosen': '0.14893', 'rewards_train/rejected': '0.043488', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10544', 'logps_train/rejected': '-92.965', 'logps_train/chosen': '-140.7', 'loss/train': '0.64654', 'examples_per_second': '30.49', 'grad_norm': '44.25', 'counters/examples': 140864, 'counters/updates': 4402}
train stats after 140896 examples: {'rewards_train/chosen': '0.090679', 'rewards_train/rejected': '0.011881', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078798', 'logps_train/rejected': '-82.585', 'logps_train/chosen': '-121.03', 'loss/train': '0.6592', 'examples_per_second': '31.948', 'grad_norm': '23.625', 'counters/examples': 140896, 'counters/updates': 4403}
train stats after 140928 examples: {'rewards_train/chosen': '0.11676', 'rewards_train/rejected': '0.067857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048902', 'logps_train/rejected': '-142.57', 'logps_train/chosen': '-129.17', 'loss/train': '0.68532', 'examples_per_second': '31.984', 'grad_norm': '32.25', 'counters/examples': 140928, 'counters/updates': 4404}
train stats after 140960 examples: {'rewards_train/chosen': '0.084972', 'rewards_train/rejected': '0.078429', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0065434', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-136.1', 'loss/train': '0.70045', 'examples_per_second': '30.787', 'grad_norm': '30.375', 'counters/examples': 140960, 'counters/updates': 4405}
train stats after 140992 examples: {'rewards_train/chosen': '0.19448', 'rewards_train/rejected': '0.036795', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15769', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-158.33', 'loss/train': '0.63765', 'examples_per_second': '25.822', 'grad_norm': '33.5', 'counters/examples': 140992, 'counters/updates': 4406}
train stats after 141024 examples: {'rewards_train/chosen': '0.098408', 'rewards_train/rejected': '0.097067', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0013409', 'logps_train/rejected': '-200.65', 'logps_train/chosen': '-132.27', 'loss/train': '0.70494', 'examples_per_second': '31.358', 'grad_norm': '62', 'counters/examples': 141024, 'counters/updates': 4407}
train stats after 141056 examples: {'rewards_train/chosen': '0.082953', 'rewards_train/rejected': '0.00048766', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.082465', 'logps_train/rejected': '-101.12', 'logps_train/chosen': '-151.37', 'loss/train': '0.65881', 'examples_per_second': '30.101', 'grad_norm': '42.25', 'counters/examples': 141056, 'counters/updates': 4408}
train stats after 141088 examples: {'rewards_train/chosen': '0.082696', 'rewards_train/rejected': '0.048886', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.033809', 'logps_train/rejected': '-124.52', 'logps_train/chosen': '-144.14', 'loss/train': '0.68415', 'examples_per_second': '25.102', 'grad_norm': '40', 'counters/examples': 141088, 'counters/updates': 4409}
skipping logging after 141120 examples to avoid logging too frequently
train stats after 141152 examples: {'rewards_train/chosen': '0.1295', 'rewards_train/rejected': '0.068971', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060528', 'logps_train/rejected': '-119.43', 'logps_train/chosen': '-140.56', 'loss/train': '0.67317', 'examples_per_second': '31.187', 'grad_norm': '26.25', 'counters/examples': 141152, 'counters/updates': 4411}
train stats after 141184 examples: {'rewards_train/chosen': '0.083631', 'rewards_train/rejected': '0.011654', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071977', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-127.79', 'loss/train': '0.67157', 'examples_per_second': '31.968', 'grad_norm': '33.25', 'counters/examples': 141184, 'counters/updates': 4412}
train stats after 141216 examples: {'rewards_train/chosen': '0.15974', 'rewards_train/rejected': '0.12721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.032528', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-165.07', 'loss/train': '0.68098', 'examples_per_second': '31.54', 'grad_norm': '32', 'counters/examples': 141216, 'counters/updates': 4413}
train stats after 141248 examples: {'rewards_train/chosen': '0.11657', 'rewards_train/rejected': '0.062367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054206', 'logps_train/rejected': '-107.27', 'logps_train/chosen': '-141.28', 'loss/train': '0.68008', 'examples_per_second': '31.316', 'grad_norm': '42', 'counters/examples': 141248, 'counters/updates': 4414}
train stats after 141280 examples: {'rewards_train/chosen': '0.076157', 'rewards_train/rejected': '0.03671', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039448', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-121.71', 'loss/train': '0.68292', 'examples_per_second': '31.165', 'grad_norm': '29', 'counters/examples': 141280, 'counters/updates': 4415}
train stats after 141312 examples: {'rewards_train/chosen': '0.040075', 'rewards_train/rejected': '0.022438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.017637', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-167.52', 'loss/train': '0.69232', 'examples_per_second': '31.33', 'grad_norm': '32.5', 'counters/examples': 141312, 'counters/updates': 4416}
train stats after 141344 examples: {'rewards_train/chosen': '0.094523', 'rewards_train/rejected': '0.0051104', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089412', 'logps_train/rejected': '-107.32', 'logps_train/chosen': '-136.17', 'loss/train': '0.65536', 'examples_per_second': '31.47', 'grad_norm': '25.875', 'counters/examples': 141344, 'counters/updates': 4417}
train stats after 141376 examples: {'rewards_train/chosen': '0.19083', 'rewards_train/rejected': '0.055214', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13561', 'logps_train/rejected': '-107.48', 'logps_train/chosen': '-126.08', 'loss/train': '0.64207', 'examples_per_second': '29.869', 'grad_norm': '25.75', 'counters/examples': 141376, 'counters/updates': 4418}
train stats after 141408 examples: {'rewards_train/chosen': '0.12166', 'rewards_train/rejected': '0.0010553', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1206', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-149.36', 'loss/train': '0.6426', 'examples_per_second': '31.425', 'grad_norm': '26.625', 'counters/examples': 141408, 'counters/updates': 4419}
train stats after 141440 examples: {'rewards_train/chosen': '0.18025', 'rewards_train/rejected': '-0.00088779', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18114', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-164.63', 'loss/train': '0.638', 'examples_per_second': '32.554', 'grad_norm': '28.875', 'counters/examples': 141440, 'counters/updates': 4420}
train stats after 141472 examples: {'rewards_train/chosen': '0.11966', 'rewards_train/rejected': '0.10201', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.017651', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-121.34', 'loss/train': '0.69312', 'examples_per_second': '31.206', 'grad_norm': '33', 'counters/examples': 141472, 'counters/updates': 4421}
train stats after 141504 examples: {'rewards_train/chosen': '0.10018', 'rewards_train/rejected': '0.037289', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062894', 'logps_train/rejected': '-145.96', 'logps_train/chosen': '-137.9', 'loss/train': '0.66684', 'examples_per_second': '31.509', 'grad_norm': '33.75', 'counters/examples': 141504, 'counters/updates': 4422}
train stats after 141536 examples: {'rewards_train/chosen': '0.10759', 'rewards_train/rejected': '0.058207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.049378', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-130.23', 'loss/train': '0.68375', 'examples_per_second': '32.281', 'grad_norm': '30.5', 'counters/examples': 141536, 'counters/updates': 4423}
skipping logging after 141568 examples to avoid logging too frequently
train stats after 141600 examples: {'rewards_train/chosen': '0.17451', 'rewards_train/rejected': '0.025302', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14921', 'logps_train/rejected': '-127.14', 'logps_train/chosen': '-151.25', 'loss/train': '0.63948', 'examples_per_second': '30.377', 'grad_norm': '30.875', 'counters/examples': 141600, 'counters/updates': 4425}
skipping logging after 141632 examples to avoid logging too frequently
train stats after 141664 examples: {'rewards_train/chosen': '0.14102', 'rewards_train/rejected': '0.015033', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12599', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-120.82', 'loss/train': '0.63867', 'examples_per_second': '33.432', 'grad_norm': '32.5', 'counters/examples': 141664, 'counters/updates': 4427}
train stats after 141696 examples: {'rewards_train/chosen': '0.18801', 'rewards_train/rejected': '0.061957', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12605', 'logps_train/rejected': '-141.21', 'logps_train/chosen': '-124.81', 'loss/train': '0.64414', 'examples_per_second': '32.268', 'grad_norm': '26.875', 'counters/examples': 141696, 'counters/updates': 4428}
train stats after 141728 examples: {'rewards_train/chosen': '0.14197', 'rewards_train/rejected': '0.087845', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054123', 'logps_train/rejected': '-104.77', 'logps_train/chosen': '-162.91', 'loss/train': '0.68136', 'examples_per_second': '31.911', 'grad_norm': '27.375', 'counters/examples': 141728, 'counters/updates': 4429}
train stats after 141760 examples: {'rewards_train/chosen': '0.12231', 'rewards_train/rejected': '-0.019264', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14157', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-141.05', 'loss/train': '0.63556', 'examples_per_second': '31.561', 'grad_norm': '37.25', 'counters/examples': 141760, 'counters/updates': 4430}
train stats after 141792 examples: {'rewards_train/chosen': '0.12264', 'rewards_train/rejected': '0.074162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048475', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-136.2', 'loss/train': '0.67482', 'examples_per_second': '31.595', 'grad_norm': '26.5', 'counters/examples': 141792, 'counters/updates': 4431}
skipping logging after 141824 examples to avoid logging too frequently
train stats after 141856 examples: {'rewards_train/chosen': '0.20679', 'rewards_train/rejected': '0.065512', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14128', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-146.24', 'loss/train': '0.64515', 'examples_per_second': '31.414', 'grad_norm': '31.75', 'counters/examples': 141856, 'counters/updates': 4433}
train stats after 141888 examples: {'rewards_train/chosen': '0.13749', 'rewards_train/rejected': '0.087275', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050212', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-192', 'loss/train': '0.68044', 'examples_per_second': '31.467', 'grad_norm': '60.25', 'counters/examples': 141888, 'counters/updates': 4434}
train stats after 141920 examples: {'rewards_train/chosen': '0.063245', 'rewards_train/rejected': '0.12409', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.060844', 'logps_train/rejected': '-159.56', 'logps_train/chosen': '-129.52', 'loss/train': '0.73887', 'examples_per_second': '30.502', 'grad_norm': '37.75', 'counters/examples': 141920, 'counters/updates': 4435}
train stats after 141952 examples: {'rewards_train/chosen': '0.12335', 'rewards_train/rejected': '0.044908', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078445', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-109.05', 'loss/train': '0.66655', 'examples_per_second': '31.552', 'grad_norm': '26.125', 'counters/examples': 141952, 'counters/updates': 4436}
train stats after 141984 examples: {'rewards_train/chosen': '0.10572', 'rewards_train/rejected': '0.022611', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083108', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-162.25', 'loss/train': '0.66163', 'examples_per_second': '31.42', 'grad_norm': '30.375', 'counters/examples': 141984, 'counters/updates': 4437}
train stats after 142016 examples: {'rewards_train/chosen': '0.18594', 'rewards_train/rejected': '0.071351', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11459', 'logps_train/rejected': '-126.15', 'logps_train/chosen': '-153.15', 'loss/train': '0.64944', 'examples_per_second': '31.143', 'grad_norm': '28.625', 'counters/examples': 142016, 'counters/updates': 4438}
skipping logging after 142048 examples to avoid logging too frequently
train stats after 142080 examples: {'rewards_train/chosen': '0.057706', 'rewards_train/rejected': '0.062811', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0051051', 'logps_train/rejected': '-125.99', 'logps_train/chosen': '-110.61', 'loss/train': '0.71085', 'examples_per_second': '31.316', 'grad_norm': '35.25', 'counters/examples': 142080, 'counters/updates': 4440}
train stats after 142112 examples: {'rewards_train/chosen': '0.041712', 'rewards_train/rejected': '0.03851', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0032019', 'logps_train/rejected': '-160.35', 'logps_train/chosen': '-174.58', 'loss/train': '0.70603', 'examples_per_second': '31.986', 'grad_norm': '36.75', 'counters/examples': 142112, 'counters/updates': 4441}
skipping logging after 142144 examples to avoid logging too frequently
train stats after 142176 examples: {'rewards_train/chosen': '0.20613', 'rewards_train/rejected': '0.21166', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0055339', 'logps_train/rejected': '-140.04', 'logps_train/chosen': '-121.59', 'loss/train': '0.73198', 'examples_per_second': '33.833', 'grad_norm': '49', 'counters/examples': 142176, 'counters/updates': 4443}
train stats after 142208 examples: {'rewards_train/chosen': '0.31548', 'rewards_train/rejected': '0.22351', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091974', 'logps_train/rejected': '-126.02', 'logps_train/chosen': '-161.91', 'loss/train': '0.66343', 'examples_per_second': '30.503', 'grad_norm': '35.25', 'counters/examples': 142208, 'counters/updates': 4444}
train stats after 142240 examples: {'rewards_train/chosen': '0.17905', 'rewards_train/rejected': '0.017117', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.16193', 'logps_train/rejected': '-93.584', 'logps_train/chosen': '-158.24', 'loss/train': '0.62371', 'examples_per_second': '31.002', 'grad_norm': '36', 'counters/examples': 142240, 'counters/updates': 4445}
skipping logging after 142272 examples to avoid logging too frequently
train stats after 142304 examples: {'rewards_train/chosen': '0.29396', 'rewards_train/rejected': '0.13235', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16161', 'logps_train/rejected': '-117.98', 'logps_train/chosen': '-164.49', 'loss/train': '0.63995', 'examples_per_second': '31.198', 'grad_norm': '42.25', 'counters/examples': 142304, 'counters/updates': 4447}
skipping logging after 142336 examples to avoid logging too frequently
train stats after 142368 examples: {'rewards_train/chosen': '0.12833', 'rewards_train/rejected': '0.013307', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11503', 'logps_train/rejected': '-98.482', 'logps_train/chosen': '-108.42', 'loss/train': '0.65106', 'examples_per_second': '30.578', 'grad_norm': '43.75', 'counters/examples': 142368, 'counters/updates': 4449}
train stats after 142400 examples: {'rewards_train/chosen': '0.15452', 'rewards_train/rejected': '0.056642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097876', 'logps_train/rejected': '-156.74', 'logps_train/chosen': '-162.92', 'loss/train': '0.65229', 'examples_per_second': '32.019', 'grad_norm': '35.75', 'counters/examples': 142400, 'counters/updates': 4450}
train stats after 142432 examples: {'rewards_train/chosen': '0.095619', 'rewards_train/rejected': '0.11806', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022438', 'logps_train/rejected': '-133.33', 'logps_train/chosen': '-125.9', 'loss/train': '0.71644', 'examples_per_second': '31.613', 'grad_norm': '37.75', 'counters/examples': 142432, 'counters/updates': 4451}
train stats after 142464 examples: {'rewards_train/chosen': '0.10502', 'rewards_train/rejected': '0.034025', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070996', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-135.4', 'loss/train': '0.6704', 'examples_per_second': '32.063', 'grad_norm': '37', 'counters/examples': 142464, 'counters/updates': 4452}
train stats after 142496 examples: {'rewards_train/chosen': '0.16068', 'rewards_train/rejected': '-0.040318', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.201', 'logps_train/rejected': '-140.98', 'logps_train/chosen': '-156.56', 'loss/train': '0.60615', 'examples_per_second': '33.144', 'grad_norm': '27.125', 'counters/examples': 142496, 'counters/updates': 4453}
train stats after 142528 examples: {'rewards_train/chosen': '0.10316', 'rewards_train/rejected': '0.028346', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074816', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-144.97', 'loss/train': '0.66475', 'examples_per_second': '32.808', 'grad_norm': '33.25', 'counters/examples': 142528, 'counters/updates': 4454}
train stats after 142560 examples: {'rewards_train/chosen': '0.12518', 'rewards_train/rejected': '0.012922', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11226', 'logps_train/rejected': '-125.89', 'logps_train/chosen': '-145.1', 'loss/train': '0.64846', 'examples_per_second': '31.252', 'grad_norm': '28.125', 'counters/examples': 142560, 'counters/updates': 4455}
train stats after 142592 examples: {'rewards_train/chosen': '0.24414', 'rewards_train/rejected': '0.03099', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21315', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-180.81', 'loss/train': '0.63182', 'examples_per_second': '30.456', 'grad_norm': '34.5', 'counters/examples': 142592, 'counters/updates': 4456}
train stats after 142624 examples: {'rewards_train/chosen': '0.17721', 'rewards_train/rejected': '0.13638', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040833', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-182.2', 'loss/train': '0.68321', 'examples_per_second': '31.354', 'grad_norm': '36.75', 'counters/examples': 142624, 'counters/updates': 4457}
train stats after 142656 examples: {'rewards_train/chosen': '0.1504', 'rewards_train/rejected': '0.024125', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12628', 'logps_train/rejected': '-85.756', 'logps_train/chosen': '-142.07', 'loss/train': '0.64732', 'examples_per_second': '31.468', 'grad_norm': '23.75', 'counters/examples': 142656, 'counters/updates': 4458}
train stats after 142688 examples: {'rewards_train/chosen': '0.18974', 'rewards_train/rejected': '0.06725', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12249', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-157.73', 'loss/train': '0.64528', 'examples_per_second': '31.229', 'grad_norm': '30.875', 'counters/examples': 142688, 'counters/updates': 4459}
skipping logging after 142720 examples to avoid logging too frequently
train stats after 142752 examples: {'rewards_train/chosen': '0.062346', 'rewards_train/rejected': '0.028934', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033412', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-115.85', 'loss/train': '0.68395', 'examples_per_second': '33.117', 'grad_norm': '29.125', 'counters/examples': 142752, 'counters/updates': 4461}
train stats after 142784 examples: {'rewards_train/chosen': '0.11775', 'rewards_train/rejected': '-0.001972', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11972', 'logps_train/rejected': '-153.78', 'logps_train/chosen': '-126.2', 'loss/train': '0.65331', 'examples_per_second': '32.46', 'grad_norm': '34.5', 'counters/examples': 142784, 'counters/updates': 4462}
train stats after 142816 examples: {'rewards_train/chosen': '0.15609', 'rewards_train/rejected': '0.039183', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11691', 'logps_train/rejected': '-121.27', 'logps_train/chosen': '-132.22', 'loss/train': '0.6439', 'examples_per_second': '31.262', 'grad_norm': '33.25', 'counters/examples': 142816, 'counters/updates': 4463}
train stats after 142848 examples: {'rewards_train/chosen': '0.11236', 'rewards_train/rejected': '0.10327', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0090852', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-132.05', 'loss/train': '0.70207', 'examples_per_second': '31.549', 'grad_norm': '42', 'counters/examples': 142848, 'counters/updates': 4464}
train stats after 142880 examples: {'rewards_train/chosen': '0.10172', 'rewards_train/rejected': '0.037219', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064498', 'logps_train/rejected': '-119.67', 'logps_train/chosen': '-130.73', 'loss/train': '0.67487', 'examples_per_second': '30.512', 'grad_norm': '30.25', 'counters/examples': 142880, 'counters/updates': 4465}
train stats after 142912 examples: {'rewards_train/chosen': '0.16395', 'rewards_train/rejected': '0.05917', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10478', 'logps_train/rejected': '-109.05', 'logps_train/chosen': '-133.04', 'loss/train': '0.65134', 'examples_per_second': '30.025', 'grad_norm': '26.125', 'counters/examples': 142912, 'counters/updates': 4466}
train stats after 142944 examples: {'rewards_train/chosen': '0.11253', 'rewards_train/rejected': '0.032373', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080162', 'logps_train/rejected': '-144.53', 'logps_train/chosen': '-152.85', 'loss/train': '0.66465', 'examples_per_second': '31.018', 'grad_norm': '51.5', 'counters/examples': 142944, 'counters/updates': 4467}
train stats after 142976 examples: {'rewards_train/chosen': '0.078064', 'rewards_train/rejected': '0.015244', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06282', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-155.61', 'loss/train': '0.67005', 'examples_per_second': '31.397', 'grad_norm': '38', 'counters/examples': 142976, 'counters/updates': 4468}
train stats after 143008 examples: {'rewards_train/chosen': '0.074726', 'rewards_train/rejected': '0.10886', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034136', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-158.04', 'loss/train': '0.72403', 'examples_per_second': '32.277', 'grad_norm': '51.25', 'counters/examples': 143008, 'counters/updates': 4469}
train stats after 143040 examples: {'rewards_train/chosen': '0.20611', 'rewards_train/rejected': '0.0073376', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19877', 'logps_train/rejected': '-98.234', 'logps_train/chosen': '-182.22', 'loss/train': '0.61046', 'examples_per_second': '29.891', 'grad_norm': '33', 'counters/examples': 143040, 'counters/updates': 4470}
skipping logging after 143072 examples to avoid logging too frequently
train stats after 143104 examples: {'rewards_train/chosen': '0.15163', 'rewards_train/rejected': '0.17311', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.021488', 'logps_train/rejected': '-184.37', 'logps_train/chosen': '-152.15', 'loss/train': '0.71348', 'examples_per_second': '32.315', 'grad_norm': '37.5', 'counters/examples': 143104, 'counters/updates': 4472}
train stats after 143136 examples: {'rewards_train/chosen': '0.18346', 'rewards_train/rejected': '0.032841', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15062', 'logps_train/rejected': '-141.9', 'logps_train/chosen': '-150.11', 'loss/train': '0.63563', 'examples_per_second': '31.479', 'grad_norm': '34.25', 'counters/examples': 143136, 'counters/updates': 4473}
train stats after 143168 examples: {'rewards_train/chosen': '0.1737', 'rewards_train/rejected': '0.063648', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11005', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-134.03', 'loss/train': '0.65186', 'examples_per_second': '30.092', 'grad_norm': '23.25', 'counters/examples': 143168, 'counters/updates': 4474}
train stats after 143200 examples: {'rewards_train/chosen': '0.14883', 'rewards_train/rejected': '0.069585', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07925', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-134.13', 'loss/train': '0.66191', 'examples_per_second': '32.374', 'grad_norm': '25.625', 'counters/examples': 143200, 'counters/updates': 4475}
train stats after 143232 examples: {'rewards_train/chosen': '0.13564', 'rewards_train/rejected': '0.037735', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097904', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-138.86', 'loss/train': '0.65416', 'examples_per_second': '31.399', 'grad_norm': '26.875', 'counters/examples': 143232, 'counters/updates': 4476}
train stats after 143264 examples: {'rewards_train/chosen': '0.21018', 'rewards_train/rejected': '0.11956', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090623', 'logps_train/rejected': '-141.6', 'logps_train/chosen': '-164.47', 'loss/train': '0.65854', 'examples_per_second': '31.51', 'grad_norm': '43.25', 'counters/examples': 143264, 'counters/updates': 4477}
train stats after 143296 examples: {'rewards_train/chosen': '0.13398', 'rewards_train/rejected': '0.067091', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.066889', 'logps_train/rejected': '-130.08', 'logps_train/chosen': '-145.24', 'loss/train': '0.66872', 'examples_per_second': '30.005', 'grad_norm': '32.75', 'counters/examples': 143296, 'counters/updates': 4478}
train stats after 143328 examples: {'rewards_train/chosen': '0.1534', 'rewards_train/rejected': '0.0044588', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14894', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-156.51', 'loss/train': '0.63749', 'examples_per_second': '31.343', 'grad_norm': '33', 'counters/examples': 143328, 'counters/updates': 4479}
train stats after 143360 examples: {'rewards_train/chosen': '0.16816', 'rewards_train/rejected': '0.1023', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065858', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-181.42', 'loss/train': '0.67511', 'examples_per_second': '30.884', 'grad_norm': '28.25', 'counters/examples': 143360, 'counters/updates': 4480}
train stats after 143392 examples: {'rewards_train/chosen': '0.1416', 'rewards_train/rejected': '0.032222', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10938', 'logps_train/rejected': '-118.22', 'logps_train/chosen': '-140.47', 'loss/train': '0.64872', 'examples_per_second': '31.492', 'grad_norm': '32.25', 'counters/examples': 143392, 'counters/updates': 4481}
train stats after 143424 examples: {'rewards_train/chosen': '0.089501', 'rewards_train/rejected': '0.028975', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060526', 'logps_train/rejected': '-122.08', 'logps_train/chosen': '-153', 'loss/train': '0.67098', 'examples_per_second': '31.178', 'grad_norm': '29.875', 'counters/examples': 143424, 'counters/updates': 4482}
train stats after 143456 examples: {'rewards_train/chosen': '0.072098', 'rewards_train/rejected': '0.092868', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.02077', 'logps_train/rejected': '-149.91', 'logps_train/chosen': '-154.72', 'loss/train': '0.71428', 'examples_per_second': '30.01', 'grad_norm': '29.5', 'counters/examples': 143456, 'counters/updates': 4483}
skipping logging after 143488 examples to avoid logging too frequently
train stats after 143520 examples: {'rewards_train/chosen': '0.18379', 'rewards_train/rejected': '0.064059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11973', 'logps_train/rejected': '-129.87', 'logps_train/chosen': '-147.25', 'loss/train': '0.64948', 'examples_per_second': '34.529', 'grad_norm': '48.5', 'counters/examples': 143520, 'counters/updates': 4485}
train stats after 143552 examples: {'rewards_train/chosen': '0.16462', 'rewards_train/rejected': '0.032055', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13257', 'logps_train/rejected': '-166.36', 'logps_train/chosen': '-179.65', 'loss/train': '0.63846', 'examples_per_second': '30.799', 'grad_norm': '28.25', 'counters/examples': 143552, 'counters/updates': 4486}
train stats after 143584 examples: {'rewards_train/chosen': '0.17973', 'rewards_train/rejected': '0.056792', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12294', 'logps_train/rejected': '-101.01', 'logps_train/chosen': '-136.96', 'loss/train': '0.6471', 'examples_per_second': '30.43', 'grad_norm': '27.75', 'counters/examples': 143584, 'counters/updates': 4487}
train stats after 143616 examples: {'rewards_train/chosen': '0.11859', 'rewards_train/rejected': '0.047259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071331', 'logps_train/rejected': '-100.04', 'logps_train/chosen': '-115.01', 'loss/train': '0.66394', 'examples_per_second': '31.963', 'grad_norm': '30.625', 'counters/examples': 143616, 'counters/updates': 4488}
train stats after 143648 examples: {'rewards_train/chosen': '0.1369', 'rewards_train/rejected': '0.029975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10693', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-170.69', 'loss/train': '0.64679', 'examples_per_second': '23.53', 'grad_norm': '31', 'counters/examples': 143648, 'counters/updates': 4489}
train stats after 143680 examples: {'rewards_train/chosen': '0.18769', 'rewards_train/rejected': '0.028639', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15905', 'logps_train/rejected': '-100.03', 'logps_train/chosen': '-131', 'loss/train': '0.63293', 'examples_per_second': '31.592', 'grad_norm': '24.25', 'counters/examples': 143680, 'counters/updates': 4490}
train stats after 143712 examples: {'rewards_train/chosen': '0.17339', 'rewards_train/rejected': '0.092304', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081084', 'logps_train/rejected': '-121.14', 'logps_train/chosen': '-139.62', 'loss/train': '0.66245', 'examples_per_second': '32.486', 'grad_norm': '25.5', 'counters/examples': 143712, 'counters/updates': 4491}
train stats after 143744 examples: {'rewards_train/chosen': '0.12023', 'rewards_train/rejected': '0.053418', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066807', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-158.72', 'loss/train': '0.66865', 'examples_per_second': '30.93', 'grad_norm': '27.625', 'counters/examples': 143744, 'counters/updates': 4492}
train stats after 143776 examples: {'rewards_train/chosen': '0.12637', 'rewards_train/rejected': '0.13415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0077783', 'logps_train/rejected': '-156.9', 'logps_train/chosen': '-145.81', 'loss/train': '0.71039', 'examples_per_second': '31.165', 'grad_norm': '34.75', 'counters/examples': 143776, 'counters/updates': 4493}
skipping logging after 143808 examples to avoid logging too frequently
train stats after 143840 examples: {'rewards_train/chosen': '0.098', 'rewards_train/rejected': '-0.03795', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13595', 'logps_train/rejected': '-99.734', 'logps_train/chosen': '-160.71', 'loss/train': '0.64114', 'examples_per_second': '32.08', 'grad_norm': '27.125', 'counters/examples': 143840, 'counters/updates': 4495}
train stats after 143872 examples: {'rewards_train/chosen': '0.13095', 'rewards_train/rejected': '0.029487', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10146', 'logps_train/rejected': '-117.83', 'logps_train/chosen': '-149.58', 'loss/train': '0.66462', 'examples_per_second': '30.872', 'grad_norm': '31.625', 'counters/examples': 143872, 'counters/updates': 4496}
skipping logging after 143904 examples to avoid logging too frequently
train stats after 143936 examples: {'rewards_train/chosen': '0.10555', 'rewards_train/rejected': '0.056083', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049466', 'logps_train/rejected': '-130.36', 'logps_train/chosen': '-140.59', 'loss/train': '0.6749', 'examples_per_second': '29.583', 'grad_norm': '29.75', 'counters/examples': 143936, 'counters/updates': 4498}
train stats after 143968 examples: {'rewards_train/chosen': '0.067574', 'rewards_train/rejected': '0.087867', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020293', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-141.66', 'loss/train': '0.71152', 'examples_per_second': '31.377', 'grad_norm': '56', 'counters/examples': 143968, 'counters/updates': 4499}
skipping logging after 144000 examples to avoid logging too frequently
Running evaluation after 144000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.19it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.83it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 144000: {'rewards_eval/chosen': '0.15914', 'rewards_eval/rejected': '0.058216', 'rewards_eval/accuracies': '0.62891', 'rewards_eval/margins': '0.10093', 'logps_eval/rejected': '-121.55', 'logps_eval/chosen': '-142.51', 'loss/eval': '0.65532'}
skipping save for non epoch
train stats after 144032 examples: {'rewards_train/chosen': '0.10687', 'rewards_train/rejected': '0.059515', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04736', 'logps_train/rejected': '-149.77', 'logps_train/chosen': '-157.12', 'loss/train': '0.68183', 'examples_per_second': '31.5', 'grad_norm': '33.75', 'counters/examples': 144032, 'counters/updates': 4501}
skipping logging after 144064 examples to avoid logging too frequently
train stats after 144096 examples: {'rewards_train/chosen': '0.0662', 'rewards_train/rejected': '0.039921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026279', 'logps_train/rejected': '-121.28', 'logps_train/chosen': '-145.19', 'loss/train': '0.69217', 'examples_per_second': '31.761', 'grad_norm': '37.25', 'counters/examples': 144096, 'counters/updates': 4503}
skipping logging after 144128 examples to avoid logging too frequently
train stats after 144160 examples: {'rewards_train/chosen': '0.15397', 'rewards_train/rejected': '0.063931', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090041', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-170.84', 'loss/train': '0.66612', 'examples_per_second': '31.045', 'grad_norm': '36', 'counters/examples': 144160, 'counters/updates': 4505}
skipping logging after 144192 examples to avoid logging too frequently
train stats after 144224 examples: {'rewards_train/chosen': '0.16392', 'rewards_train/rejected': '0.028876', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13505', 'logps_train/rejected': '-95.09', 'logps_train/chosen': '-130.93', 'loss/train': '0.6341', 'examples_per_second': '37.237', 'grad_norm': '25.125', 'counters/examples': 144224, 'counters/updates': 4507}
skipping logging after 144256 examples to avoid logging too frequently
train stats after 144288 examples: {'rewards_train/chosen': '0.14424', 'rewards_train/rejected': '0.098278', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045958', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-155.22', 'loss/train': '0.68431', 'examples_per_second': '35.709', 'grad_norm': '51.5', 'counters/examples': 144288, 'counters/updates': 4509}
train stats after 144320 examples: {'rewards_train/chosen': '0.13134', 'rewards_train/rejected': '0.050198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081146', 'logps_train/rejected': '-145.09', 'logps_train/chosen': '-166.7', 'loss/train': '0.66108', 'examples_per_second': '30.166', 'grad_norm': '29', 'counters/examples': 144320, 'counters/updates': 4510}
train stats after 144352 examples: {'rewards_train/chosen': '0.12545', 'rewards_train/rejected': '0.093449', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.032001', 'logps_train/rejected': '-98.814', 'logps_train/chosen': '-165.68', 'loss/train': '0.68465', 'examples_per_second': '31.619', 'grad_norm': '26.75', 'counters/examples': 144352, 'counters/updates': 4511}
train stats after 144384 examples: {'rewards_train/chosen': '0.20838', 'rewards_train/rejected': '0.12944', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078936', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-162.25', 'loss/train': '0.66088', 'examples_per_second': '31.466', 'grad_norm': '35.25', 'counters/examples': 144384, 'counters/updates': 4512}
train stats after 144416 examples: {'rewards_train/chosen': '0.15039', 'rewards_train/rejected': '0.10547', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044922', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-154.13', 'loss/train': '0.67764', 'examples_per_second': '30.398', 'grad_norm': '32.75', 'counters/examples': 144416, 'counters/updates': 4513}
train stats after 144448 examples: {'rewards_train/chosen': '0.099173', 'rewards_train/rejected': '0.13355', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.034375', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-121.48', 'loss/train': '0.71683', 'examples_per_second': '31.19', 'grad_norm': '27.875', 'counters/examples': 144448, 'counters/updates': 4514}
train stats after 144480 examples: {'rewards_train/chosen': '0.19845', 'rewards_train/rejected': '0.15111', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047346', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-132.01', 'loss/train': '0.69306', 'examples_per_second': '30.421', 'grad_norm': '44.75', 'counters/examples': 144480, 'counters/updates': 4515}
train stats after 144512 examples: {'rewards_train/chosen': '0.16585', 'rewards_train/rejected': '0.070935', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094919', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-147.22', 'loss/train': '0.65885', 'examples_per_second': '30.726', 'grad_norm': '60.5', 'counters/examples': 144512, 'counters/updates': 4516}
train stats after 144544 examples: {'rewards_train/chosen': '0.069921', 'rewards_train/rejected': '0.093109', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.023188', 'logps_train/rejected': '-141.65', 'logps_train/chosen': '-146.79', 'loss/train': '0.71113', 'examples_per_second': '31.931', 'grad_norm': '31', 'counters/examples': 144544, 'counters/updates': 4517}
train stats after 144576 examples: {'rewards_train/chosen': '0.079899', 'rewards_train/rejected': '0.025439', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05446', 'logps_train/rejected': '-108.35', 'logps_train/chosen': '-163.48', 'loss/train': '0.67255', 'examples_per_second': '31.802', 'grad_norm': '40.5', 'counters/examples': 144576, 'counters/updates': 4518}
train stats after 144608 examples: {'rewards_train/chosen': '0.114', 'rewards_train/rejected': '0.1085', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0055019', 'logps_train/rejected': '-141.19', 'logps_train/chosen': '-179.85', 'loss/train': '0.70479', 'examples_per_second': '31.429', 'grad_norm': '32.75', 'counters/examples': 144608, 'counters/updates': 4519}
train stats after 144640 examples: {'rewards_train/chosen': '0.13503', 'rewards_train/rejected': '0.070537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06449', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-129.81', 'loss/train': '0.66658', 'examples_per_second': '31.134', 'grad_norm': '26.5', 'counters/examples': 144640, 'counters/updates': 4520}
skipping logging after 144672 examples to avoid logging too frequently
train stats after 144704 examples: {'rewards_train/chosen': '0.069303', 'rewards_train/rejected': '0.021565', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047738', 'logps_train/rejected': '-142.42', 'logps_train/chosen': '-165.43', 'loss/train': '0.68004', 'examples_per_second': '31.467', 'grad_norm': '45.75', 'counters/examples': 144704, 'counters/updates': 4522}
train stats after 144736 examples: {'rewards_train/chosen': '0.11667', 'rewards_train/rejected': '0.11606', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00060736', 'logps_train/rejected': '-109.91', 'logps_train/chosen': '-133.52', 'loss/train': '0.69917', 'examples_per_second': '31.987', 'grad_norm': '30.375', 'counters/examples': 144736, 'counters/updates': 4523}
train stats after 144768 examples: {'rewards_train/chosen': '0.093456', 'rewards_train/rejected': '0.059504', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033953', 'logps_train/rejected': '-152.61', 'logps_train/chosen': '-112.74', 'loss/train': '0.68445', 'examples_per_second': '31.453', 'grad_norm': '31.125', 'counters/examples': 144768, 'counters/updates': 4524}
train stats after 144800 examples: {'rewards_train/chosen': '0.078462', 'rewards_train/rejected': '0.017698', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060764', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-131.55', 'loss/train': '0.67511', 'examples_per_second': '31.25', 'grad_norm': '29.5', 'counters/examples': 144800, 'counters/updates': 4525}
train stats after 144832 examples: {'rewards_train/chosen': '0.094073', 'rewards_train/rejected': '-0.0022474', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09632', 'logps_train/rejected': '-133.03', 'logps_train/chosen': '-187.73', 'loss/train': '0.65476', 'examples_per_second': '30.432', 'grad_norm': '34.75', 'counters/examples': 144832, 'counters/updates': 4526}
train stats after 144864 examples: {'rewards_train/chosen': '0.13861', 'rewards_train/rejected': '0.057219', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081392', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-158.82', 'loss/train': '0.66347', 'examples_per_second': '29.968', 'grad_norm': '31.5', 'counters/examples': 144864, 'counters/updates': 4527}
train stats after 144896 examples: {'rewards_train/chosen': '0.087579', 'rewards_train/rejected': '0.22769', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.14011', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-142.37', 'loss/train': '0.80762', 'examples_per_second': '29.911', 'grad_norm': '58.25', 'counters/examples': 144896, 'counters/updates': 4528}
train stats after 144928 examples: {'rewards_train/chosen': '0.075475', 'rewards_train/rejected': '0.0076553', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06782', 'logps_train/rejected': '-116.51', 'logps_train/chosen': '-141.3', 'loss/train': '0.66758', 'examples_per_second': '31.151', 'grad_norm': '26.875', 'counters/examples': 144928, 'counters/updates': 4529}
train stats after 144960 examples: {'rewards_train/chosen': '0.16891', 'rewards_train/rejected': '0.033008', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1359', 'logps_train/rejected': '-110.2', 'logps_train/chosen': '-169.58', 'loss/train': '0.63921', 'examples_per_second': '31.42', 'grad_norm': '46.25', 'counters/examples': 144960, 'counters/updates': 4530}
train stats after 144992 examples: {'rewards_train/chosen': '0.10653', 'rewards_train/rejected': '0.074288', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.032237', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-159.38', 'loss/train': '0.70027', 'examples_per_second': '31.486', 'grad_norm': '45', 'counters/examples': 144992, 'counters/updates': 4531}
train stats after 145024 examples: {'rewards_train/chosen': '0.090733', 'rewards_train/rejected': '0.03809', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.052643', 'logps_train/rejected': '-132.61', 'logps_train/chosen': '-145.12', 'loss/train': '0.67758', 'examples_per_second': '31.359', 'grad_norm': '28.875', 'counters/examples': 145024, 'counters/updates': 4532}
train stats after 145056 examples: {'rewards_train/chosen': '0.10926', 'rewards_train/rejected': '0.059853', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04941', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-131.66', 'loss/train': '0.67564', 'examples_per_second': '30.421', 'grad_norm': '37.75', 'counters/examples': 145056, 'counters/updates': 4533}
skipping logging after 145088 examples to avoid logging too frequently
train stats after 145120 examples: {'rewards_train/chosen': '0.13046', 'rewards_train/rejected': '0.068855', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061601', 'logps_train/rejected': '-120.43', 'logps_train/chosen': '-130.63', 'loss/train': '0.67068', 'examples_per_second': '31.324', 'grad_norm': '34.25', 'counters/examples': 145120, 'counters/updates': 4535}
train stats after 145152 examples: {'rewards_train/chosen': '0.19853', 'rewards_train/rejected': '0.10801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090519', 'logps_train/rejected': '-131.66', 'logps_train/chosen': '-152.38', 'loss/train': '0.66839', 'examples_per_second': '32.374', 'grad_norm': '34', 'counters/examples': 145152, 'counters/updates': 4536}
skipping logging after 145184 examples to avoid logging too frequently
train stats after 145216 examples: {'rewards_train/chosen': '0.16808', 'rewards_train/rejected': '0.031851', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13622', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-146.77', 'loss/train': '0.6378', 'examples_per_second': '33.301', 'grad_norm': '32.25', 'counters/examples': 145216, 'counters/updates': 4538}
skipping logging after 145248 examples to avoid logging too frequently
train stats after 145280 examples: {'rewards_train/chosen': '0.033933', 'rewards_train/rejected': '0.055222', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.021289', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-135.61', 'loss/train': '0.71628', 'examples_per_second': '32.34', 'grad_norm': '27', 'counters/examples': 145280, 'counters/updates': 4540}
train stats after 145312 examples: {'rewards_train/chosen': '0.11068', 'rewards_train/rejected': '0.036494', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074188', 'logps_train/rejected': '-126', 'logps_train/chosen': '-146.29', 'loss/train': '0.66945', 'examples_per_second': '31.395', 'grad_norm': '30.75', 'counters/examples': 145312, 'counters/updates': 4541}
skipping logging after 145344 examples to avoid logging too frequently
train stats after 145376 examples: {'rewards_train/chosen': '0.063723', 'rewards_train/rejected': '0.027236', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036486', 'logps_train/rejected': '-146.5', 'logps_train/chosen': '-126.54', 'loss/train': '0.68368', 'examples_per_second': '31.536', 'grad_norm': '31.875', 'counters/examples': 145376, 'counters/updates': 4543}
train stats after 145408 examples: {'rewards_train/chosen': '0.099132', 'rewards_train/rejected': '-0.021155', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12029', 'logps_train/rejected': '-146.86', 'logps_train/chosen': '-170.15', 'loss/train': '0.64456', 'examples_per_second': '31.444', 'grad_norm': '26', 'counters/examples': 145408, 'counters/updates': 4544}
train stats after 145440 examples: {'rewards_train/chosen': '0.19371', 'rewards_train/rejected': '-0.0068087', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20052', 'logps_train/rejected': '-141.66', 'logps_train/chosen': '-136.62', 'loss/train': '0.61417', 'examples_per_second': '32.73', 'grad_norm': '35.75', 'counters/examples': 145440, 'counters/updates': 4545}
train stats after 145472 examples: {'rewards_train/chosen': '0.070314', 'rewards_train/rejected': '0.086365', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.016051', 'logps_train/rejected': '-118.1', 'logps_train/chosen': '-141.21', 'loss/train': '0.70417', 'examples_per_second': '30.764', 'grad_norm': '41.25', 'counters/examples': 145472, 'counters/updates': 4546}
train stats after 145504 examples: {'rewards_train/chosen': '0.08595', 'rewards_train/rejected': '-0.065472', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15142', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-159.66', 'loss/train': '0.63028', 'examples_per_second': '30.77', 'grad_norm': '26.875', 'counters/examples': 145504, 'counters/updates': 4547}
train stats after 145536 examples: {'rewards_train/chosen': '0.13144', 'rewards_train/rejected': '0.075449', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055996', 'logps_train/rejected': '-125.41', 'logps_train/chosen': '-168.07', 'loss/train': '0.67697', 'examples_per_second': '31.392', 'grad_norm': '47.25', 'counters/examples': 145536, 'counters/updates': 4548}
train stats after 145568 examples: {'rewards_train/chosen': '0.11883', 'rewards_train/rejected': '0.13902', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.020186', 'logps_train/rejected': '-116.16', 'logps_train/chosen': '-136.25', 'loss/train': '0.71586', 'examples_per_second': '31.296', 'grad_norm': '34.75', 'counters/examples': 145568, 'counters/updates': 4549}
train stats after 145600 examples: {'rewards_train/chosen': '-0.018117', 'rewards_train/rejected': '-0.10383', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085715', 'logps_train/rejected': '-126.5', 'logps_train/chosen': '-143.09', 'loss/train': '0.66134', 'examples_per_second': '31.079', 'grad_norm': '52.75', 'counters/examples': 145600, 'counters/updates': 4550}
train stats after 145632 examples: {'rewards_train/chosen': '0.15282', 'rewards_train/rejected': '0.087661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065156', 'logps_train/rejected': '-150.78', 'logps_train/chosen': '-131.69', 'loss/train': '0.67621', 'examples_per_second': '31.435', 'grad_norm': '29.75', 'counters/examples': 145632, 'counters/updates': 4551}
skipping logging after 145664 examples to avoid logging too frequently
train stats after 145696 examples: {'rewards_train/chosen': '0.12007', 'rewards_train/rejected': '0.080641', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039432', 'logps_train/rejected': '-135.8', 'logps_train/chosen': '-142.72', 'loss/train': '0.68834', 'examples_per_second': '32.366', 'grad_norm': '34', 'counters/examples': 145696, 'counters/updates': 4553}
train stats after 145728 examples: {'rewards_train/chosen': '0.28709', 'rewards_train/rejected': '0.087723', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19937', 'logps_train/rejected': '-103.69', 'logps_train/chosen': '-145.84', 'loss/train': '0.61358', 'examples_per_second': '29.732', 'grad_norm': '33.25', 'counters/examples': 145728, 'counters/updates': 4554}
skipping logging after 145760 examples to avoid logging too frequently
train stats after 145792 examples: {'rewards_train/chosen': '0.17424', 'rewards_train/rejected': '0.073633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10061', 'logps_train/rejected': '-148.09', 'logps_train/chosen': '-144.92', 'loss/train': '0.65348', 'examples_per_second': '32.94', 'grad_norm': '28.375', 'counters/examples': 145792, 'counters/updates': 4556}
train stats after 145824 examples: {'rewards_train/chosen': '0.087981', 'rewards_train/rejected': '0.033234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054748', 'logps_train/rejected': '-98.077', 'logps_train/chosen': '-120.92', 'loss/train': '0.66949', 'examples_per_second': '31.365', 'grad_norm': '25.125', 'counters/examples': 145824, 'counters/updates': 4557}
train stats after 145856 examples: {'rewards_train/chosen': '0.067014', 'rewards_train/rejected': '0.029381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.037633', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-115.7', 'loss/train': '0.68491', 'examples_per_second': '30.407', 'grad_norm': '35.75', 'counters/examples': 145856, 'counters/updates': 4558}
train stats after 145888 examples: {'rewards_train/chosen': '0.08715', 'rewards_train/rejected': '0.012969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.074181', 'logps_train/rejected': '-89.991', 'logps_train/chosen': '-143.23', 'loss/train': '0.66404', 'examples_per_second': '30.977', 'grad_norm': '30.375', 'counters/examples': 145888, 'counters/updates': 4559}
train stats after 145920 examples: {'rewards_train/chosen': '0.10213', 'rewards_train/rejected': '0.10181', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.00031294', 'logps_train/rejected': '-97.705', 'logps_train/chosen': '-147.22', 'loss/train': '0.70269', 'examples_per_second': '31.453', 'grad_norm': '37', 'counters/examples': 145920, 'counters/updates': 4560}
train stats after 145952 examples: {'rewards_train/chosen': '0.21072', 'rewards_train/rejected': '0.10562', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10511', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-155.14', 'loss/train': '0.65418', 'examples_per_second': '30.513', 'grad_norm': '34.5', 'counters/examples': 145952, 'counters/updates': 4561}
skipping logging after 145984 examples to avoid logging too frequently
train stats after 146016 examples: {'rewards_train/chosen': '0.14947', 'rewards_train/rejected': '0.025782', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12368', 'logps_train/rejected': '-81.565', 'logps_train/chosen': '-132.06', 'loss/train': '0.64228', 'examples_per_second': '34.059', 'grad_norm': '24.875', 'counters/examples': 146016, 'counters/updates': 4563}
train stats after 146048 examples: {'rewards_train/chosen': '0.19089', 'rewards_train/rejected': '0.080492', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1104', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-157.65', 'loss/train': '0.65312', 'examples_per_second': '29.842', 'grad_norm': '28.25', 'counters/examples': 146048, 'counters/updates': 4564}
train stats after 146080 examples: {'rewards_train/chosen': '0.023248', 'rewards_train/rejected': '-0.023623', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046871', 'logps_train/rejected': '-102.25', 'logps_train/chosen': '-88.209', 'loss/train': '0.67408', 'examples_per_second': '31.717', 'grad_norm': '46.5', 'counters/examples': 146080, 'counters/updates': 4565}
train stats after 146112 examples: {'rewards_train/chosen': '0.18924', 'rewards_train/rejected': '0.074969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11428', 'logps_train/rejected': '-128.86', 'logps_train/chosen': '-158.74', 'loss/train': '0.65304', 'examples_per_second': '32.056', 'grad_norm': '34', 'counters/examples': 146112, 'counters/updates': 4566}
train stats after 146144 examples: {'rewards_train/chosen': '0.12825', 'rewards_train/rejected': '0.017898', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11035', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-197.13', 'loss/train': '0.65076', 'examples_per_second': '31.205', 'grad_norm': '35.25', 'counters/examples': 146144, 'counters/updates': 4567}
train stats after 146176 examples: {'rewards_train/chosen': '0.087912', 'rewards_train/rejected': '-0.018109', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.10602', 'logps_train/rejected': '-140.41', 'logps_train/chosen': '-165.86', 'loss/train': '0.64543', 'examples_per_second': '31.311', 'grad_norm': '30.75', 'counters/examples': 146176, 'counters/updates': 4568}
train stats after 146208 examples: {'rewards_train/chosen': '0.11401', 'rewards_train/rejected': '-0.02382', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13783', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-140.79', 'loss/train': '0.63497', 'examples_per_second': '31.123', 'grad_norm': '26.5', 'counters/examples': 146208, 'counters/updates': 4569}
skipping logging after 146240 examples to avoid logging too frequently
train stats after 146272 examples: {'rewards_train/chosen': '0.19524', 'rewards_train/rejected': '0.029039', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.1662', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-143.24', 'loss/train': '0.62148', 'examples_per_second': '31.431', 'grad_norm': '29.875', 'counters/examples': 146272, 'counters/updates': 4571}
train stats after 146304 examples: {'rewards_train/chosen': '0.16437', 'rewards_train/rejected': '0.10696', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057405', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-159.77', 'loss/train': '0.68385', 'examples_per_second': '32.899', 'grad_norm': '33.5', 'counters/examples': 146304, 'counters/updates': 4572}
skipping logging after 146336 examples to avoid logging too frequently
train stats after 146368 examples: {'rewards_train/chosen': '0.080335', 'rewards_train/rejected': '0.046355', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03398', 'logps_train/rejected': '-129.44', 'logps_train/chosen': '-161.46', 'loss/train': '0.68919', 'examples_per_second': '30.061', 'grad_norm': '40', 'counters/examples': 146368, 'counters/updates': 4574}
train stats after 146400 examples: {'rewards_train/chosen': '0.10897', 'rewards_train/rejected': '0.16884', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059865', 'logps_train/rejected': '-124.01', 'logps_train/chosen': '-114.08', 'loss/train': '0.74555', 'examples_per_second': '30.901', 'grad_norm': '45', 'counters/examples': 146400, 'counters/updates': 4575}
skipping logging after 146432 examples to avoid logging too frequently
train stats after 146464 examples: {'rewards_train/chosen': '0.13025', 'rewards_train/rejected': '0.064536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065714', 'logps_train/rejected': '-105.73', 'logps_train/chosen': '-164.45', 'loss/train': '0.69052', 'examples_per_second': '24.698', 'grad_norm': '39.25', 'counters/examples': 146464, 'counters/updates': 4577}
train stats after 146496 examples: {'rewards_train/chosen': '0.17115', 'rewards_train/rejected': '0.043318', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12784', 'logps_train/rejected': '-150.21', 'logps_train/chosen': '-173.67', 'loss/train': '0.64073', 'examples_per_second': '32.767', 'grad_norm': '31.875', 'counters/examples': 146496, 'counters/updates': 4578}
train stats after 146528 examples: {'rewards_train/chosen': '0.03472', 'rewards_train/rejected': '0.026534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0081856', 'logps_train/rejected': '-130.59', 'logps_train/chosen': '-151.03', 'loss/train': '0.71045', 'examples_per_second': '33.131', 'grad_norm': '57.25', 'counters/examples': 146528, 'counters/updates': 4579}
train stats after 146560 examples: {'rewards_train/chosen': '0.1927', 'rewards_train/rejected': '0.080142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11256', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-151.73', 'loss/train': '0.651', 'examples_per_second': '23.359', 'grad_norm': '36', 'counters/examples': 146560, 'counters/updates': 4580}
train stats after 146592 examples: {'rewards_train/chosen': '0.016962', 'rewards_train/rejected': '-0.0062573', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023219', 'logps_train/rejected': '-112.02', 'logps_train/chosen': '-100.74', 'loss/train': '0.68579', 'examples_per_second': '29.6', 'grad_norm': '25.25', 'counters/examples': 146592, 'counters/updates': 4581}
skipping logging after 146624 examples to avoid logging too frequently
train stats after 146656 examples: {'rewards_train/chosen': '0.126', 'rewards_train/rejected': '0.061599', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064397', 'logps_train/rejected': '-110.51', 'logps_train/chosen': '-114.93', 'loss/train': '0.66687', 'examples_per_second': '32.584', 'grad_norm': '21.75', 'counters/examples': 146656, 'counters/updates': 4583}
train stats after 146688 examples: {'rewards_train/chosen': '0.22545', 'rewards_train/rejected': '0.096795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12865', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-135.41', 'loss/train': '0.65598', 'examples_per_second': '30.871', 'grad_norm': '29', 'counters/examples': 146688, 'counters/updates': 4584}
train stats after 146720 examples: {'rewards_train/chosen': '0.095253', 'rewards_train/rejected': '0.15638', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.06113', 'logps_train/rejected': '-172.28', 'logps_train/chosen': '-154.06', 'loss/train': '0.74097', 'examples_per_second': '31.487', 'grad_norm': '44.5', 'counters/examples': 146720, 'counters/updates': 4585}
train stats after 146752 examples: {'rewards_train/chosen': '0.13585', 'rewards_train/rejected': '0.081596', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054251', 'logps_train/rejected': '-130.69', 'logps_train/chosen': '-144.57', 'loss/train': '0.68103', 'examples_per_second': '30.534', 'grad_norm': '35.25', 'counters/examples': 146752, 'counters/updates': 4586}
skipping logging after 146784 examples to avoid logging too frequently
train stats after 146816 examples: {'rewards_train/chosen': '0.1226', 'rewards_train/rejected': '0.096138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.026458', 'logps_train/rejected': '-124.43', 'logps_train/chosen': '-129.45', 'loss/train': '0.69162', 'examples_per_second': '31.597', 'grad_norm': '33.5', 'counters/examples': 146816, 'counters/updates': 4588}
skipping logging after 146848 examples to avoid logging too frequently
train stats after 146880 examples: {'rewards_train/chosen': '0.07083', 'rewards_train/rejected': '0.14828', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.077455', 'logps_train/rejected': '-165.67', 'logps_train/chosen': '-148.85', 'loss/train': '0.7517', 'examples_per_second': '30.949', 'grad_norm': '35.5', 'counters/examples': 146880, 'counters/updates': 4590}
train stats after 146912 examples: {'rewards_train/chosen': '0.067536', 'rewards_train/rejected': '0.13692', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.069381', 'logps_train/rejected': '-170.9', 'logps_train/chosen': '-148.55', 'loss/train': '0.74362', 'examples_per_second': '31.446', 'grad_norm': '38.75', 'counters/examples': 146912, 'counters/updates': 4591}
skipping logging after 146944 examples to avoid logging too frequently
train stats after 146976 examples: {'rewards_train/chosen': '0.1021', 'rewards_train/rejected': '0.037251', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064851', 'logps_train/rejected': '-77.58', 'logps_train/chosen': '-93.819', 'loss/train': '0.66762', 'examples_per_second': '33.323', 'grad_norm': '27.625', 'counters/examples': 146976, 'counters/updates': 4593}
train stats after 147008 examples: {'rewards_train/chosen': '0.13959', 'rewards_train/rejected': '0.048423', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091166', 'logps_train/rejected': '-164.62', 'logps_train/chosen': '-177.69', 'loss/train': '0.66686', 'examples_per_second': '31.341', 'grad_norm': '43', 'counters/examples': 147008, 'counters/updates': 4594}
train stats after 147040 examples: {'rewards_train/chosen': '0.10624', 'rewards_train/rejected': '0.081824', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.024419', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-131.95', 'loss/train': '0.69119', 'examples_per_second': '29.986', 'grad_norm': '33.75', 'counters/examples': 147040, 'counters/updates': 4595}
train stats after 147072 examples: {'rewards_train/chosen': '0.24714', 'rewards_train/rejected': '0.012502', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23464', 'logps_train/rejected': '-142.75', 'logps_train/chosen': '-150.94', 'loss/train': '0.59769', 'examples_per_second': '30.735', 'grad_norm': '53.75', 'counters/examples': 147072, 'counters/updates': 4596}
train stats after 147104 examples: {'rewards_train/chosen': '0.14476', 'rewards_train/rejected': '0.10994', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.034817', 'logps_train/rejected': '-120.22', 'logps_train/chosen': '-135.93', 'loss/train': '0.68626', 'examples_per_second': '31.059', 'grad_norm': '27.25', 'counters/examples': 147104, 'counters/updates': 4597}
train stats after 147136 examples: {'rewards_train/chosen': '0.17413', 'rewards_train/rejected': '0.075609', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098517', 'logps_train/rejected': '-138.24', 'logps_train/chosen': '-154.66', 'loss/train': '0.66299', 'examples_per_second': '32.194', 'grad_norm': '38.25', 'counters/examples': 147136, 'counters/updates': 4598}
train stats after 147168 examples: {'rewards_train/chosen': '0.10864', 'rewards_train/rejected': '0.086728', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.021909', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-125.96', 'loss/train': '0.6905', 'examples_per_second': '30.599', 'grad_norm': '32', 'counters/examples': 147168, 'counters/updates': 4599}
train stats after 147200 examples: {'rewards_train/chosen': '0.11209', 'rewards_train/rejected': '0.00073001', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11136', 'logps_train/rejected': '-137.2', 'logps_train/chosen': '-143.3', 'loss/train': '0.64941', 'examples_per_second': '29.661', 'grad_norm': '27.625', 'counters/examples': 147200, 'counters/updates': 4600}
train stats after 147232 examples: {'rewards_train/chosen': '0.19879', 'rewards_train/rejected': '0.059292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1395', 'logps_train/rejected': '-116.83', 'logps_train/chosen': '-173.91', 'loss/train': '0.63374', 'examples_per_second': '31.181', 'grad_norm': '28.5', 'counters/examples': 147232, 'counters/updates': 4601}
train stats after 147264 examples: {'rewards_train/chosen': '0.15718', 'rewards_train/rejected': '0.010229', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14695', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-141.75', 'loss/train': '0.63154', 'examples_per_second': '32.368', 'grad_norm': '28.625', 'counters/examples': 147264, 'counters/updates': 4602}
train stats after 147296 examples: {'rewards_train/chosen': '0.1554', 'rewards_train/rejected': '0.0052316', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15017', 'logps_train/rejected': '-86.101', 'logps_train/chosen': '-149.22', 'loss/train': '0.63579', 'examples_per_second': '31.742', 'grad_norm': '56.25', 'counters/examples': 147296, 'counters/updates': 4603}
train stats after 147328 examples: {'rewards_train/chosen': '0.11759', 'rewards_train/rejected': '0.029949', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087642', 'logps_train/rejected': '-110.66', 'logps_train/chosen': '-129.22', 'loss/train': '0.67883', 'examples_per_second': '31.104', 'grad_norm': '30.375', 'counters/examples': 147328, 'counters/updates': 4604}
train stats after 147360 examples: {'rewards_train/chosen': '0.18225', 'rewards_train/rejected': '0.10176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080498', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-160.16', 'loss/train': '0.66765', 'examples_per_second': '31.02', 'grad_norm': '38', 'counters/examples': 147360, 'counters/updates': 4605}
skipping logging after 147392 examples to avoid logging too frequently
train stats after 147424 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '0.058724', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10106', 'logps_train/rejected': '-218.76', 'logps_train/chosen': '-180.79', 'loss/train': '0.65363', 'examples_per_second': '31.119', 'grad_norm': '36.5', 'counters/examples': 147424, 'counters/updates': 4607}
train stats after 147456 examples: {'rewards_train/chosen': '0.21665', 'rewards_train/rejected': '0.13949', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077156', 'logps_train/rejected': '-185.9', 'logps_train/chosen': '-189.51', 'loss/train': '0.66743', 'examples_per_second': '31.06', 'grad_norm': '40.5', 'counters/examples': 147456, 'counters/updates': 4608}
train stats after 147488 examples: {'rewards_train/chosen': '0.11243', 'rewards_train/rejected': '-0.037293', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14972', 'logps_train/rejected': '-84.387', 'logps_train/chosen': '-97.127', 'loss/train': '0.64103', 'examples_per_second': '30.362', 'grad_norm': '24.375', 'counters/examples': 147488, 'counters/updates': 4609}
train stats after 147520 examples: {'rewards_train/chosen': '0.14441', 'rewards_train/rejected': '0.0023678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14205', 'logps_train/rejected': '-128.43', 'logps_train/chosen': '-138.35', 'loss/train': '0.63744', 'examples_per_second': '30.057', 'grad_norm': '29', 'counters/examples': 147520, 'counters/updates': 4610}
train stats after 147552 examples: {'rewards_train/chosen': '0.11736', 'rewards_train/rejected': '0.11804', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '-0.00067935', 'logps_train/rejected': '-106.02', 'logps_train/chosen': '-179.04', 'loss/train': '0.74656', 'examples_per_second': '32.782', 'grad_norm': '72.5', 'counters/examples': 147552, 'counters/updates': 4611}
train stats after 147584 examples: {'rewards_train/chosen': '0.17722', 'rewards_train/rejected': '0.10211', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075111', 'logps_train/rejected': '-133.32', 'logps_train/chosen': '-170.89', 'loss/train': '0.67104', 'examples_per_second': '31.04', 'grad_norm': '33.25', 'counters/examples': 147584, 'counters/updates': 4612}
train stats after 147616 examples: {'rewards_train/chosen': '0.13978', 'rewards_train/rejected': '0.10881', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030972', 'logps_train/rejected': '-193.17', 'logps_train/chosen': '-163.99', 'loss/train': '0.68515', 'examples_per_second': '31.133', 'grad_norm': '38', 'counters/examples': 147616, 'counters/updates': 4613}
train stats after 147648 examples: {'rewards_train/chosen': '0.14349', 'rewards_train/rejected': '0.038768', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10472', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-140.96', 'loss/train': '0.65327', 'examples_per_second': '32.925', 'grad_norm': '28.5', 'counters/examples': 147648, 'counters/updates': 4614}
skipping logging after 147680 examples to avoid logging too frequently
train stats after 147712 examples: {'rewards_train/chosen': '0.00061991', 'rewards_train/rejected': '0.013378', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.012758', 'logps_train/rejected': '-109.28', 'logps_train/chosen': '-205.21', 'loss/train': '0.75025', 'examples_per_second': '30.562', 'grad_norm': '191', 'counters/examples': 147712, 'counters/updates': 4616}
train stats after 147744 examples: {'rewards_train/chosen': '0.18164', 'rewards_train/rejected': '0.11537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066278', 'logps_train/rejected': '-169.95', 'logps_train/chosen': '-193.47', 'loss/train': '0.69143', 'examples_per_second': '31.038', 'grad_norm': '44.5', 'counters/examples': 147744, 'counters/updates': 4617}
train stats after 147776 examples: {'rewards_train/chosen': '0.054139', 'rewards_train/rejected': '0.14247', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.088327', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-140.52', 'loss/train': '0.7488', 'examples_per_second': '30.69', 'grad_norm': '41', 'counters/examples': 147776, 'counters/updates': 4618}
train stats after 147808 examples: {'rewards_train/chosen': '0.22757', 'rewards_train/rejected': '0.09798', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.12959', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-159.74', 'loss/train': '0.64999', 'examples_per_second': '32.03', 'grad_norm': '43', 'counters/examples': 147808, 'counters/updates': 4619}
train stats after 147840 examples: {'rewards_train/chosen': '0.18441', 'rewards_train/rejected': '0.080397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10402', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-159.84', 'loss/train': '0.65245', 'examples_per_second': '30.997', 'grad_norm': '54.75', 'counters/examples': 147840, 'counters/updates': 4620}
train stats after 147872 examples: {'rewards_train/chosen': '0.13445', 'rewards_train/rejected': '0.042246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0922', 'logps_train/rejected': '-145.33', 'logps_train/chosen': '-144.53', 'loss/train': '0.66159', 'examples_per_second': '29.659', 'grad_norm': '29.5', 'counters/examples': 147872, 'counters/updates': 4621}
train stats after 147904 examples: {'rewards_train/chosen': '0.082834', 'rewards_train/rejected': '-0.0015795', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084414', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-125.18', 'loss/train': '0.66095', 'examples_per_second': '31.136', 'grad_norm': '29.5', 'counters/examples': 147904, 'counters/updates': 4622}
train stats after 147936 examples: {'rewards_train/chosen': '0.1054', 'rewards_train/rejected': '0.038088', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.067309', 'logps_train/rejected': '-121.07', 'logps_train/chosen': '-137.81', 'loss/train': '0.66807', 'examples_per_second': '31.236', 'grad_norm': '32.5', 'counters/examples': 147936, 'counters/updates': 4623}
train stats after 147968 examples: {'rewards_train/chosen': '0.097094', 'rewards_train/rejected': '-0.0057316', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10283', 'logps_train/rejected': '-123.51', 'logps_train/chosen': '-123.19', 'loss/train': '0.64922', 'examples_per_second': '31.703', 'grad_norm': '40.25', 'counters/examples': 147968, 'counters/updates': 4624}
train stats after 148000 examples: {'rewards_train/chosen': '0.2015', 'rewards_train/rejected': '0.089479', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11202', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-176.93', 'loss/train': '0.65203', 'examples_per_second': '31.071', 'grad_norm': '45.5', 'counters/examples': 148000, 'counters/updates': 4625}
train stats after 148032 examples: {'rewards_train/chosen': '0.24846', 'rewards_train/rejected': '0.045508', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20295', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-149.05', 'loss/train': '0.60499', 'examples_per_second': '31.899', 'grad_norm': '36', 'counters/examples': 148032, 'counters/updates': 4626}
train stats after 148064 examples: {'rewards_train/chosen': '0.20833', 'rewards_train/rejected': '0.11815', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090186', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-132.75', 'loss/train': '0.67314', 'examples_per_second': '31.166', 'grad_norm': '33.25', 'counters/examples': 148064, 'counters/updates': 4627}
skipping logging after 148096 examples to avoid logging too frequently
train stats after 148128 examples: {'rewards_train/chosen': '0.16384', 'rewards_train/rejected': '0.088878', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074965', 'logps_train/rejected': '-136.18', 'logps_train/chosen': '-159.36', 'loss/train': '0.67168', 'examples_per_second': '31.298', 'grad_norm': '38.25', 'counters/examples': 148128, 'counters/updates': 4629}
train stats after 148160 examples: {'rewards_train/chosen': '0.099531', 'rewards_train/rejected': '0.039058', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060473', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-141.21', 'loss/train': '0.68017', 'examples_per_second': '31.744', 'grad_norm': '31.375', 'counters/examples': 148160, 'counters/updates': 4630}
skipping logging after 148192 examples to avoid logging too frequently
train stats after 148224 examples: {'rewards_train/chosen': '0.13109', 'rewards_train/rejected': '0.061556', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069531', 'logps_train/rejected': '-113.09', 'logps_train/chosen': '-116.67', 'loss/train': '0.66719', 'examples_per_second': '32.681', 'grad_norm': '33.25', 'counters/examples': 148224, 'counters/updates': 4632}
train stats after 148256 examples: {'rewards_train/chosen': '0.11147', 'rewards_train/rejected': '0.026199', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085267', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-126.7', 'loss/train': '0.66322', 'examples_per_second': '29.604', 'grad_norm': '28.875', 'counters/examples': 148256, 'counters/updates': 4633}
skipping logging after 148288 examples to avoid logging too frequently
train stats after 148320 examples: {'rewards_train/chosen': '0.16276', 'rewards_train/rejected': '0.058276', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10448', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-135.36', 'loss/train': '0.65079', 'examples_per_second': '30.817', 'grad_norm': '25.375', 'counters/examples': 148320, 'counters/updates': 4635}
skipping logging after 148352 examples to avoid logging too frequently
train stats after 148384 examples: {'rewards_train/chosen': '0.10078', 'rewards_train/rejected': '0.097534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0032436', 'logps_train/rejected': '-148.76', 'logps_train/chosen': '-131.1', 'loss/train': '0.69943', 'examples_per_second': '31.1', 'grad_norm': '41.25', 'counters/examples': 148384, 'counters/updates': 4637}
train stats after 148416 examples: {'rewards_train/chosen': '0.11532', 'rewards_train/rejected': '0.073056', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.042268', 'logps_train/rejected': '-137.58', 'logps_train/chosen': '-148.82', 'loss/train': '0.68069', 'examples_per_second': '30.219', 'grad_norm': '28.625', 'counters/examples': 148416, 'counters/updates': 4638}
train stats after 148448 examples: {'rewards_train/chosen': '0.08562', 'rewards_train/rejected': '0.008115', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.077505', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-116.63', 'loss/train': '0.67337', 'examples_per_second': '32.543', 'grad_norm': '47.25', 'counters/examples': 148448, 'counters/updates': 4639}
train stats after 148480 examples: {'rewards_train/chosen': '0.024858', 'rewards_train/rejected': '0.044795', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.019938', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-104.09', 'loss/train': '0.70944', 'examples_per_second': '32.815', 'grad_norm': '29.875', 'counters/examples': 148480, 'counters/updates': 4640}
train stats after 148512 examples: {'rewards_train/chosen': '0.11962', 'rewards_train/rejected': '0.044354', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075271', 'logps_train/rejected': '-148.27', 'logps_train/chosen': '-184.6', 'loss/train': '0.66363', 'examples_per_second': '30.775', 'grad_norm': '33.75', 'counters/examples': 148512, 'counters/updates': 4641}
train stats after 148544 examples: {'rewards_train/chosen': '0.077828', 'rewards_train/rejected': '0.078849', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.001021', 'logps_train/rejected': '-125.44', 'logps_train/chosen': '-140.6', 'loss/train': '0.71159', 'examples_per_second': '32.096', 'grad_norm': '36', 'counters/examples': 148544, 'counters/updates': 4642}
train stats after 148576 examples: {'rewards_train/chosen': '0.14872', 'rewards_train/rejected': '0.031092', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11763', 'logps_train/rejected': '-109.82', 'logps_train/chosen': '-144.32', 'loss/train': '0.64253', 'examples_per_second': '30.056', 'grad_norm': '29.25', 'counters/examples': 148576, 'counters/updates': 4643}
train stats after 148608 examples: {'rewards_train/chosen': '0.082848', 'rewards_train/rejected': '-0.0089404', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091788', 'logps_train/rejected': '-126.23', 'logps_train/chosen': '-158.09', 'loss/train': '0.65945', 'examples_per_second': '31.174', 'grad_norm': '32.25', 'counters/examples': 148608, 'counters/updates': 4644}
train stats after 148640 examples: {'rewards_train/chosen': '0.15899', 'rewards_train/rejected': '-0.0068518', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16584', 'logps_train/rejected': '-103.26', 'logps_train/chosen': '-139.98', 'loss/train': '0.62329', 'examples_per_second': '31.298', 'grad_norm': '38.75', 'counters/examples': 148640, 'counters/updates': 4645}
train stats after 148672 examples: {'rewards_train/chosen': '0.1754', 'rewards_train/rejected': '0.040673', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13472', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-145.55', 'loss/train': '0.63877', 'examples_per_second': '31.884', 'grad_norm': '37.25', 'counters/examples': 148672, 'counters/updates': 4646}
skipping logging after 148704 examples to avoid logging too frequently
train stats after 148736 examples: {'rewards_train/chosen': '0.14374', 'rewards_train/rejected': '0.044288', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099456', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-147.95', 'loss/train': '0.66069', 'examples_per_second': '33.803', 'grad_norm': '29.375', 'counters/examples': 148736, 'counters/updates': 4648}
train stats after 148768 examples: {'rewards_train/chosen': '0.076381', 'rewards_train/rejected': '0.05996', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01642', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-139.34', 'loss/train': '0.69429', 'examples_per_second': '32.165', 'grad_norm': '30.25', 'counters/examples': 148768, 'counters/updates': 4649}
train stats after 148800 examples: {'rewards_train/chosen': '0.1115', 'rewards_train/rejected': '0.01761', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093894', 'logps_train/rejected': '-100.38', 'logps_train/chosen': '-108.53', 'loss/train': '0.65393', 'examples_per_second': '32.013', 'grad_norm': '21.875', 'counters/examples': 148800, 'counters/updates': 4650}
train stats after 148832 examples: {'rewards_train/chosen': '0.093788', 'rewards_train/rejected': '0.0094158', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084373', 'logps_train/rejected': '-137.48', 'logps_train/chosen': '-138.97', 'loss/train': '0.67286', 'examples_per_second': '31.59', 'grad_norm': '55.75', 'counters/examples': 148832, 'counters/updates': 4651}
train stats after 148864 examples: {'rewards_train/chosen': '0.13506', 'rewards_train/rejected': '0.051625', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083439', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-125.64', 'loss/train': '0.66103', 'examples_per_second': '30.184', 'grad_norm': '35.75', 'counters/examples': 148864, 'counters/updates': 4652}
train stats after 148896 examples: {'rewards_train/chosen': '0.092327', 'rewards_train/rejected': '0.05184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040486', 'logps_train/rejected': '-138.07', 'logps_train/chosen': '-134.04', 'loss/train': '0.67926', 'examples_per_second': '30.987', 'grad_norm': '36.5', 'counters/examples': 148896, 'counters/updates': 4653}
train stats after 148928 examples: {'rewards_train/chosen': '0.12842', 'rewards_train/rejected': '0.086949', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041469', 'logps_train/rejected': '-107.85', 'logps_train/chosen': '-121.98', 'loss/train': '0.6838', 'examples_per_second': '33.015', 'grad_norm': '28.375', 'counters/examples': 148928, 'counters/updates': 4654}
train stats after 148960 examples: {'rewards_train/chosen': '0.10364', 'rewards_train/rejected': '0.036841', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066794', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-147.9', 'loss/train': '0.66922', 'examples_per_second': '30.246', 'grad_norm': '33.5', 'counters/examples': 148960, 'counters/updates': 4655}
train stats after 148992 examples: {'rewards_train/chosen': '0.058', 'rewards_train/rejected': '0.03533', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.02267', 'logps_train/rejected': '-100.25', 'logps_train/chosen': '-160.67', 'loss/train': '0.69849', 'examples_per_second': '30.291', 'grad_norm': '33', 'counters/examples': 148992, 'counters/updates': 4656}
train stats after 149024 examples: {'rewards_train/chosen': '0.15674', 'rewards_train/rejected': '0.059425', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097312', 'logps_train/rejected': '-136.71', 'logps_train/chosen': '-155.45', 'loss/train': '0.65428', 'examples_per_second': '32.675', 'grad_norm': '30.75', 'counters/examples': 149024, 'counters/updates': 4657}
train stats after 149056 examples: {'rewards_train/chosen': '0.098962', 'rewards_train/rejected': '0.099957', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00099524', 'logps_train/rejected': '-126.71', 'logps_train/chosen': '-172.99', 'loss/train': '0.70958', 'examples_per_second': '31.086', 'grad_norm': '39.75', 'counters/examples': 149056, 'counters/updates': 4658}
train stats after 149088 examples: {'rewards_train/chosen': '0.15936', 'rewards_train/rejected': '0.019989', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13937', 'logps_train/rejected': '-134.37', 'logps_train/chosen': '-144.63', 'loss/train': '0.63307', 'examples_per_second': '31.056', 'grad_norm': '29.875', 'counters/examples': 149088, 'counters/updates': 4659}
train stats after 149120 examples: {'rewards_train/chosen': '0.26236', 'rewards_train/rejected': '0.081369', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18099', 'logps_train/rejected': '-122', 'logps_train/chosen': '-149.72', 'loss/train': '0.65462', 'examples_per_second': '30.989', 'grad_norm': '38.75', 'counters/examples': 149120, 'counters/updates': 4660}
train stats after 149152 examples: {'rewards_train/chosen': '0.1851', 'rewards_train/rejected': '0.063441', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12166', 'logps_train/rejected': '-123.46', 'logps_train/chosen': '-160.3', 'loss/train': '0.64575', 'examples_per_second': '31.998', 'grad_norm': '27.25', 'counters/examples': 149152, 'counters/updates': 4661}
skipping logging after 149184 examples to avoid logging too frequently
train stats after 149216 examples: {'rewards_train/chosen': '0.00098171', 'rewards_train/rejected': '-0.011609', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012591', 'logps_train/rejected': '-108.72', 'logps_train/chosen': '-124.89', 'loss/train': '0.69897', 'examples_per_second': '23.579', 'grad_norm': '33', 'counters/examples': 149216, 'counters/updates': 4663}
train stats after 149248 examples: {'rewards_train/chosen': '0.18677', 'rewards_train/rejected': '0.079854', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10692', 'logps_train/rejected': '-115.53', 'logps_train/chosen': '-154.57', 'loss/train': '0.65959', 'examples_per_second': '31.697', 'grad_norm': '46.5', 'counters/examples': 149248, 'counters/updates': 4664}
train stats after 149280 examples: {'rewards_train/chosen': '0.07', 'rewards_train/rejected': '-0.045602', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1156', 'logps_train/rejected': '-92.134', 'logps_train/chosen': '-121.45', 'loss/train': '0.64331', 'examples_per_second': '31.341', 'grad_norm': '24.875', 'counters/examples': 149280, 'counters/updates': 4665}
train stats after 149312 examples: {'rewards_train/chosen': '0.15088', 'rewards_train/rejected': '0.079478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071405', 'logps_train/rejected': '-110.75', 'logps_train/chosen': '-130.58', 'loss/train': '0.67638', 'examples_per_second': '31.177', 'grad_norm': '39.5', 'counters/examples': 149312, 'counters/updates': 4666}
train stats after 149344 examples: {'rewards_train/chosen': '0.083895', 'rewards_train/rejected': '0.043712', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.040183', 'logps_train/rejected': '-110.17', 'logps_train/chosen': '-123.91', 'loss/train': '0.68268', 'examples_per_second': '32.544', 'grad_norm': '27.375', 'counters/examples': 149344, 'counters/updates': 4667}
skipping logging after 149376 examples to avoid logging too frequently
train stats after 149408 examples: {'rewards_train/chosen': '0.14646', 'rewards_train/rejected': '0.011825', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13463', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-142.61', 'loss/train': '0.63931', 'examples_per_second': '30.215', 'grad_norm': '32.25', 'counters/examples': 149408, 'counters/updates': 4669}
train stats after 149440 examples: {'rewards_train/chosen': '0.14525', 'rewards_train/rejected': '0.11176', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033485', 'logps_train/rejected': '-133.53', 'logps_train/chosen': '-126.78', 'loss/train': '0.68865', 'examples_per_second': '31.767', 'grad_norm': '29.5', 'counters/examples': 149440, 'counters/updates': 4670}
train stats after 149472 examples: {'rewards_train/chosen': '0.069833', 'rewards_train/rejected': '0.076102', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.006269', 'logps_train/rejected': '-116.12', 'logps_train/chosen': '-128.33', 'loss/train': '0.70458', 'examples_per_second': '29.845', 'grad_norm': '35.5', 'counters/examples': 149472, 'counters/updates': 4671}
train stats after 149504 examples: {'rewards_train/chosen': '0.085237', 'rewards_train/rejected': '0.11861', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.033374', 'logps_train/rejected': '-128.44', 'logps_train/chosen': '-135.31', 'loss/train': '0.71754', 'examples_per_second': '30.916', 'grad_norm': '35.75', 'counters/examples': 149504, 'counters/updates': 4672}
train stats after 149536 examples: {'rewards_train/chosen': '0.068485', 'rewards_train/rejected': '0.028811', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039674', 'logps_train/rejected': '-122.74', 'logps_train/chosen': '-133.61', 'loss/train': '0.67932', 'examples_per_second': '31.128', 'grad_norm': '31.875', 'counters/examples': 149536, 'counters/updates': 4673}
train stats after 149568 examples: {'rewards_train/chosen': '0.12974', 'rewards_train/rejected': '0.049998', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079742', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-163.47', 'loss/train': '0.66196', 'examples_per_second': '31.166', 'grad_norm': '50.5', 'counters/examples': 149568, 'counters/updates': 4674}
train stats after 149600 examples: {'rewards_train/chosen': '0.1695', 'rewards_train/rejected': '0.14084', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028659', 'logps_train/rejected': '-126.14', 'logps_train/chosen': '-139.1', 'loss/train': '0.68538', 'examples_per_second': '31.113', 'grad_norm': '30.25', 'counters/examples': 149600, 'counters/updates': 4675}
skipping logging after 149632 examples to avoid logging too frequently
train stats after 149664 examples: {'rewards_train/chosen': '0.056333', 'rewards_train/rejected': '0.075895', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.019562', 'logps_train/rejected': '-165.9', 'logps_train/chosen': '-136', 'loss/train': '0.71324', 'examples_per_second': '36.501', 'grad_norm': '57', 'counters/examples': 149664, 'counters/updates': 4677}
train stats after 149696 examples: {'rewards_train/chosen': '0.12803', 'rewards_train/rejected': '0.06574', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062288', 'logps_train/rejected': '-149.49', 'logps_train/chosen': '-134.25', 'loss/train': '0.67087', 'examples_per_second': '32.762', 'grad_norm': '33.75', 'counters/examples': 149696, 'counters/updates': 4678}
train stats after 149728 examples: {'rewards_train/chosen': '0.21828', 'rewards_train/rejected': '0.088012', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13027', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-140.92', 'loss/train': '0.66086', 'examples_per_second': '31.42', 'grad_norm': '25', 'counters/examples': 149728, 'counters/updates': 4679}
train stats after 149760 examples: {'rewards_train/chosen': '0.060843', 'rewards_train/rejected': '0.022639', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038204', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-155.24', 'loss/train': '0.68314', 'examples_per_second': '29.764', 'grad_norm': '56', 'counters/examples': 149760, 'counters/updates': 4680}
train stats after 149792 examples: {'rewards_train/chosen': '0.2317', 'rewards_train/rejected': '0.087629', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14407', 'logps_train/rejected': '-103.27', 'logps_train/chosen': '-133.97', 'loss/train': '0.63628', 'examples_per_second': '30.127', 'grad_norm': '27.125', 'counters/examples': 149792, 'counters/updates': 4681}
train stats after 149824 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '0.13999', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.037729', 'logps_train/rejected': '-165.41', 'logps_train/chosen': '-169.48', 'loss/train': '0.73827', 'examples_per_second': '31.617', 'grad_norm': '36.5', 'counters/examples': 149824, 'counters/updates': 4682}
train stats after 149856 examples: {'rewards_train/chosen': '0.14879', 'rewards_train/rejected': '0.10628', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042509', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-171.58', 'loss/train': '0.68454', 'examples_per_second': '30.826', 'grad_norm': '38.25', 'counters/examples': 149856, 'counters/updates': 4683}
train stats after 149888 examples: {'rewards_train/chosen': '0.16032', 'rewards_train/rejected': '0.091644', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068678', 'logps_train/rejected': '-138.88', 'logps_train/chosen': '-154.07', 'loss/train': '0.67213', 'examples_per_second': '31.67', 'grad_norm': '30.75', 'counters/examples': 149888, 'counters/updates': 4684}
skipping logging after 149920 examples to avoid logging too frequently
train stats after 149952 examples: {'rewards_train/chosen': '0.13904', 'rewards_train/rejected': '0.10373', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035313', 'logps_train/rejected': '-122.6', 'logps_train/chosen': '-125.35', 'loss/train': '0.68642', 'examples_per_second': '32.982', 'grad_norm': '32.5', 'counters/examples': 149952, 'counters/updates': 4686}
train stats after 149984 examples: {'rewards_train/chosen': '0.12231', 'rewards_train/rejected': '0.0059742', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11633', 'logps_train/rejected': '-82.366', 'logps_train/chosen': '-171.89', 'loss/train': '0.64111', 'examples_per_second': '30.84', 'grad_norm': '58.25', 'counters/examples': 149984, 'counters/updates': 4687}
train stats after 150016 examples: {'rewards_train/chosen': '0.11489', 'rewards_train/rejected': '0.050347', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.064543', 'logps_train/rejected': '-143.67', 'logps_train/chosen': '-142.57', 'loss/train': '0.66895', 'examples_per_second': '30.272', 'grad_norm': '31.25', 'counters/examples': 150016, 'counters/updates': 4688}
train stats after 150048 examples: {'rewards_train/chosen': '0.12513', 'rewards_train/rejected': '0.09057', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034561', 'logps_train/rejected': '-122.54', 'logps_train/chosen': '-140.07', 'loss/train': '0.68353', 'examples_per_second': '30.898', 'grad_norm': '32', 'counters/examples': 150048, 'counters/updates': 4689}
skipping logging after 150080 examples to avoid logging too frequently
train stats after 150112 examples: {'rewards_train/chosen': '0.12686', 'rewards_train/rejected': '0.064189', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062673', 'logps_train/rejected': '-94.187', 'logps_train/chosen': '-161.65', 'loss/train': '0.67633', 'examples_per_second': '32.646', 'grad_norm': '29', 'counters/examples': 150112, 'counters/updates': 4691}
train stats after 150144 examples: {'rewards_train/chosen': '0.1205', 'rewards_train/rejected': '0.025303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095198', 'logps_train/rejected': '-125.85', 'logps_train/chosen': '-146.34', 'loss/train': '0.6634', 'examples_per_second': '32.23', 'grad_norm': '38', 'counters/examples': 150144, 'counters/updates': 4692}
train stats after 150176 examples: {'rewards_train/chosen': '0.04247', 'rewards_train/rejected': '0.037747', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0047229', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-121.29', 'loss/train': '0.69791', 'examples_per_second': '32.748', 'grad_norm': '25.125', 'counters/examples': 150176, 'counters/updates': 4693}
train stats after 150208 examples: {'rewards_train/chosen': '0.079043', 'rewards_train/rejected': '0.068873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.01017', 'logps_train/rejected': '-104.39', 'logps_train/chosen': '-124.43', 'loss/train': '0.70051', 'examples_per_second': '32.833', 'grad_norm': '31.375', 'counters/examples': 150208, 'counters/updates': 4694}
train stats after 150240 examples: {'rewards_train/chosen': '0.20157', 'rewards_train/rejected': '0.061909', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13966', 'logps_train/rejected': '-159.56', 'logps_train/chosen': '-154.77', 'loss/train': '0.64213', 'examples_per_second': '31.366', 'grad_norm': '32.5', 'counters/examples': 150240, 'counters/updates': 4695}
train stats after 150272 examples: {'rewards_train/chosen': '0.20409', 'rewards_train/rejected': '0.091274', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11282', 'logps_train/rejected': '-112.23', 'logps_train/chosen': '-128.73', 'loss/train': '0.6518', 'examples_per_second': '30.091', 'grad_norm': '26.625', 'counters/examples': 150272, 'counters/updates': 4696}
train stats after 150304 examples: {'rewards_train/chosen': '0.20152', 'rewards_train/rejected': '-0.0082684', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20979', 'logps_train/rejected': '-106.67', 'logps_train/chosen': '-152.58', 'loss/train': '0.6027', 'examples_per_second': '31.056', 'grad_norm': '32.25', 'counters/examples': 150304, 'counters/updates': 4697}
train stats after 150336 examples: {'rewards_train/chosen': '0.11856', 'rewards_train/rejected': '0.042291', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076267', 'logps_train/rejected': '-111.79', 'logps_train/chosen': '-116.31', 'loss/train': '0.66433', 'examples_per_second': '31.165', 'grad_norm': '31', 'counters/examples': 150336, 'counters/updates': 4698}
skipping logging after 150368 examples to avoid logging too frequently
train stats after 150400 examples: {'rewards_train/chosen': '0.13876', 'rewards_train/rejected': '0.074659', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.064097', 'logps_train/rejected': '-112.5', 'logps_train/chosen': '-123.98', 'loss/train': '0.67108', 'examples_per_second': '38.224', 'grad_norm': '24.25', 'counters/examples': 150400, 'counters/updates': 4700}
train stats after 150432 examples: {'rewards_train/chosen': '0.061606', 'rewards_train/rejected': '0.042889', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018717', 'logps_train/rejected': '-131.2', 'logps_train/chosen': '-122.93', 'loss/train': '0.70096', 'examples_per_second': '30.779', 'grad_norm': '32.25', 'counters/examples': 150432, 'counters/updates': 4701}
train stats after 150464 examples: {'rewards_train/chosen': '0.15179', 'rewards_train/rejected': '0.019331', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13246', 'logps_train/rejected': '-101.97', 'logps_train/chosen': '-154.51', 'loss/train': '0.64804', 'examples_per_second': '29.89', 'grad_norm': '27.125', 'counters/examples': 150464, 'counters/updates': 4702}
train stats after 150496 examples: {'rewards_train/chosen': '0.10427', 'rewards_train/rejected': '0.077388', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.026882', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-133.45', 'loss/train': '0.68969', 'examples_per_second': '31.141', 'grad_norm': '31.875', 'counters/examples': 150496, 'counters/updates': 4703}
skipping logging after 150528 examples to avoid logging too frequently
train stats after 150560 examples: {'rewards_train/chosen': '0.14194', 'rewards_train/rejected': '0.02098', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12096', 'logps_train/rejected': '-150.3', 'logps_train/chosen': '-170.95', 'loss/train': '0.64866', 'examples_per_second': '31.038', 'grad_norm': '37.75', 'counters/examples': 150560, 'counters/updates': 4705}
skipping logging after 150592 examples to avoid logging too frequently
train stats after 150624 examples: {'rewards_train/chosen': '0.066054', 'rewards_train/rejected': '0.021835', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.044219', 'logps_train/rejected': '-153.66', 'logps_train/chosen': '-136.9', 'loss/train': '0.68426', 'examples_per_second': '31.019', 'grad_norm': '35', 'counters/examples': 150624, 'counters/updates': 4707}
train stats after 150656 examples: {'rewards_train/chosen': '0.11909', 'rewards_train/rejected': '0.11641', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0026853', 'logps_train/rejected': '-139.37', 'logps_train/chosen': '-171.78', 'loss/train': '0.70394', 'examples_per_second': '30.943', 'grad_norm': '70.5', 'counters/examples': 150656, 'counters/updates': 4708}
train stats after 150688 examples: {'rewards_train/chosen': '0.13625', 'rewards_train/rejected': '0.039403', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096849', 'logps_train/rejected': '-98.973', 'logps_train/chosen': '-142.85', 'loss/train': '0.65731', 'examples_per_second': '30.407', 'grad_norm': '33.75', 'counters/examples': 150688, 'counters/updates': 4709}
train stats after 150720 examples: {'rewards_train/chosen': '0.20071', 'rewards_train/rejected': '0.035779', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16493', 'logps_train/rejected': '-147.65', 'logps_train/chosen': '-162.79', 'loss/train': '0.62701', 'examples_per_second': '30.432', 'grad_norm': '34', 'counters/examples': 150720, 'counters/updates': 4710}
train stats after 150752 examples: {'rewards_train/chosen': '0.13461', 'rewards_train/rejected': '0.12034', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014276', 'logps_train/rejected': '-123.08', 'logps_train/chosen': '-155.59', 'loss/train': '0.72004', 'examples_per_second': '30.74', 'grad_norm': '53.25', 'counters/examples': 150752, 'counters/updates': 4711}
train stats after 150784 examples: {'rewards_train/chosen': '0.1699', 'rewards_train/rejected': '0.030055', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13984', 'logps_train/rejected': '-158.99', 'logps_train/chosen': '-147.11', 'loss/train': '0.64904', 'examples_per_second': '31.116', 'grad_norm': '33.5', 'counters/examples': 150784, 'counters/updates': 4712}
train stats after 150816 examples: {'rewards_train/chosen': '0.085492', 'rewards_train/rejected': '0.10632', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.020826', 'logps_train/rejected': '-146.84', 'logps_train/chosen': '-154.27', 'loss/train': '0.71116', 'examples_per_second': '32.493', 'grad_norm': '29.125', 'counters/examples': 150816, 'counters/updates': 4713}
train stats after 150848 examples: {'rewards_train/chosen': '0.095341', 'rewards_train/rejected': '0.044443', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050899', 'logps_train/rejected': '-90.877', 'logps_train/chosen': '-147.74', 'loss/train': '0.68269', 'examples_per_second': '31.134', 'grad_norm': '26.875', 'counters/examples': 150848, 'counters/updates': 4714}
train stats after 150880 examples: {'rewards_train/chosen': '0.16145', 'rewards_train/rejected': '0.036328', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12512', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-143.67', 'loss/train': '0.64785', 'examples_per_second': '31.923', 'grad_norm': '27.875', 'counters/examples': 150880, 'counters/updates': 4715}
train stats after 150912 examples: {'rewards_train/chosen': '0.089298', 'rewards_train/rejected': '0.046618', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.04268', 'logps_train/rejected': '-110.79', 'logps_train/chosen': '-137.05', 'loss/train': '0.68254', 'examples_per_second': '30.424', 'grad_norm': '30', 'counters/examples': 150912, 'counters/updates': 4716}
train stats after 150944 examples: {'rewards_train/chosen': '0.1783', 'rewards_train/rejected': '0.051749', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12655', 'logps_train/rejected': '-143.6', 'logps_train/chosen': '-139.66', 'loss/train': '0.65093', 'examples_per_second': '32.577', 'grad_norm': '30.25', 'counters/examples': 150944, 'counters/updates': 4717}
train stats after 150976 examples: {'rewards_train/chosen': '0.18118', 'rewards_train/rejected': '0.17847', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0027032', 'logps_train/rejected': '-165.83', 'logps_train/chosen': '-131.09', 'loss/train': '0.71063', 'examples_per_second': '30.215', 'grad_norm': '56.75', 'counters/examples': 150976, 'counters/updates': 4718}
train stats after 151008 examples: {'rewards_train/chosen': '0.0657', 'rewards_train/rejected': '0.03328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032421', 'logps_train/rejected': '-125.77', 'logps_train/chosen': '-118.33', 'loss/train': '0.68342', 'examples_per_second': '33.028', 'grad_norm': '30.75', 'counters/examples': 151008, 'counters/updates': 4719}
train stats after 151040 examples: {'rewards_train/chosen': '0.098533', 'rewards_train/rejected': '0.021456', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077077', 'logps_train/rejected': '-102.17', 'logps_train/chosen': '-151.11', 'loss/train': '0.66988', 'examples_per_second': '30.723', 'grad_norm': '30', 'counters/examples': 151040, 'counters/updates': 4720}
train stats after 151072 examples: {'rewards_train/chosen': '0.13374', 'rewards_train/rejected': '0.17383', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.040095', 'logps_train/rejected': '-147.99', 'logps_train/chosen': '-121.61', 'loss/train': '0.72413', 'examples_per_second': '31.573', 'grad_norm': '38', 'counters/examples': 151072, 'counters/updates': 4721}
train stats after 151104 examples: {'rewards_train/chosen': '0.13569', 'rewards_train/rejected': '0.05552', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080169', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-121.79', 'loss/train': '0.66567', 'examples_per_second': '30.887', 'grad_norm': '38', 'counters/examples': 151104, 'counters/updates': 4722}
train stats after 151136 examples: {'rewards_train/chosen': '0.18941', 'rewards_train/rejected': '0.09282', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096592', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-132.26', 'loss/train': '0.66091', 'examples_per_second': '30.269', 'grad_norm': '45.75', 'counters/examples': 151136, 'counters/updates': 4723}
train stats after 151168 examples: {'rewards_train/chosen': '0.16649', 'rewards_train/rejected': '0.060097', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1064', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-156.04', 'loss/train': '0.6549', 'examples_per_second': '31.06', 'grad_norm': '30.875', 'counters/examples': 151168, 'counters/updates': 4724}
train stats after 151200 examples: {'rewards_train/chosen': '0.11287', 'rewards_train/rejected': '0.0017384', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11113', 'logps_train/rejected': '-122.46', 'logps_train/chosen': '-141.74', 'loss/train': '0.65737', 'examples_per_second': '31.201', 'grad_norm': '26.5', 'counters/examples': 151200, 'counters/updates': 4725}
train stats after 151232 examples: {'rewards_train/chosen': '0.14147', 'rewards_train/rejected': '0.017867', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12361', 'logps_train/rejected': '-113.58', 'logps_train/chosen': '-158.15', 'loss/train': '0.65104', 'examples_per_second': '31.514', 'grad_norm': '29', 'counters/examples': 151232, 'counters/updates': 4726}
train stats after 151264 examples: {'rewards_train/chosen': '0.05184', 'rewards_train/rejected': '0.024477', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027363', 'logps_train/rejected': '-97.43', 'logps_train/chosen': '-122.68', 'loss/train': '0.68515', 'examples_per_second': '31.61', 'grad_norm': '28.75', 'counters/examples': 151264, 'counters/updates': 4727}
train stats after 151296 examples: {'rewards_train/chosen': '0.14704', 'rewards_train/rejected': '0.082788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064248', 'logps_train/rejected': '-134.84', 'logps_train/chosen': '-159.25', 'loss/train': '0.6717', 'examples_per_second': '30.598', 'grad_norm': '30.25', 'counters/examples': 151296, 'counters/updates': 4728}
train stats after 151328 examples: {'rewards_train/chosen': '0.12621', 'rewards_train/rejected': '0.042355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083856', 'logps_train/rejected': '-137.53', 'logps_train/chosen': '-142.16', 'loss/train': '0.66553', 'examples_per_second': '32.47', 'grad_norm': '26.25', 'counters/examples': 151328, 'counters/updates': 4729}
train stats after 151360 examples: {'rewards_train/chosen': '0.13859', 'rewards_train/rejected': '0.097012', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.04158', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-129.89', 'loss/train': '0.68619', 'examples_per_second': '31.179', 'grad_norm': '34.25', 'counters/examples': 151360, 'counters/updates': 4730}
train stats after 151392 examples: {'rewards_train/chosen': '0.13093', 'rewards_train/rejected': '0.085', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045932', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-185.16', 'loss/train': '0.68593', 'examples_per_second': '29.739', 'grad_norm': '40.75', 'counters/examples': 151392, 'counters/updates': 4731}
train stats after 151424 examples: {'rewards_train/chosen': '0.1253', 'rewards_train/rejected': '0.12235', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.0029469', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-173.19', 'loss/train': '0.71429', 'examples_per_second': '30.57', 'grad_norm': '54', 'counters/examples': 151424, 'counters/updates': 4732}
train stats after 151456 examples: {'rewards_train/chosen': '0.091958', 'rewards_train/rejected': '0.096333', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0043748', 'logps_train/rejected': '-173.03', 'logps_train/chosen': '-134.4', 'loss/train': '0.70337', 'examples_per_second': '31.112', 'grad_norm': '32.5', 'counters/examples': 151456, 'counters/updates': 4733}
train stats after 151488 examples: {'rewards_train/chosen': '0.12776', 'rewards_train/rejected': '0.10652', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021241', 'logps_train/rejected': '-136.4', 'logps_train/chosen': '-139.1', 'loss/train': '0.69374', 'examples_per_second': '31.132', 'grad_norm': '36.25', 'counters/examples': 151488, 'counters/updates': 4734}
train stats after 151520 examples: {'rewards_train/chosen': '0.22733', 'rewards_train/rejected': '0.081452', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14588', 'logps_train/rejected': '-106.62', 'logps_train/chosen': '-145.68', 'loss/train': '0.63109', 'examples_per_second': '32.904', 'grad_norm': '25', 'counters/examples': 151520, 'counters/updates': 4735}
train stats after 151552 examples: {'rewards_train/chosen': '0.068774', 'rewards_train/rejected': '0.013879', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054895', 'logps_train/rejected': '-104.72', 'logps_train/chosen': '-145.94', 'loss/train': '0.67321', 'examples_per_second': '31.502', 'grad_norm': '27', 'counters/examples': 151552, 'counters/updates': 4736}
train stats after 151584 examples: {'rewards_train/chosen': '0.071514', 'rewards_train/rejected': '0.095546', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.024032', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-136.24', 'loss/train': '0.71315', 'examples_per_second': '30.032', 'grad_norm': '34', 'counters/examples': 151584, 'counters/updates': 4737}
train stats after 151616 examples: {'rewards_train/chosen': '0.1262', 'rewards_train/rejected': '0.10977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016436', 'logps_train/rejected': '-176.18', 'logps_train/chosen': '-137.96', 'loss/train': '0.70021', 'examples_per_second': '29.683', 'grad_norm': '62.5', 'counters/examples': 151616, 'counters/updates': 4738}
train stats after 151648 examples: {'rewards_train/chosen': '0.1396', 'rewards_train/rejected': '0.069933', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069663', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-146.9', 'loss/train': '0.67413', 'examples_per_second': '32.021', 'grad_norm': '33.25', 'counters/examples': 151648, 'counters/updates': 4739}
train stats after 151680 examples: {'rewards_train/chosen': '0.21845', 'rewards_train/rejected': '0.066332', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15212', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-158.47', 'loss/train': '0.63794', 'examples_per_second': '32.066', 'grad_norm': '35.5', 'counters/examples': 151680, 'counters/updates': 4740}
train stats after 151712 examples: {'rewards_train/chosen': '0.098765', 'rewards_train/rejected': '0.018476', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080289', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-124.18', 'loss/train': '0.66403', 'examples_per_second': '32.719', 'grad_norm': '38.25', 'counters/examples': 151712, 'counters/updates': 4741}
train stats after 151744 examples: {'rewards_train/chosen': '0.089871', 'rewards_train/rejected': '0.019306', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070565', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-144.7', 'loss/train': '0.66474', 'examples_per_second': '31.571', 'grad_norm': '32.25', 'counters/examples': 151744, 'counters/updates': 4742}
train stats after 151776 examples: {'rewards_train/chosen': '0.081235', 'rewards_train/rejected': '0.061812', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019423', 'logps_train/rejected': '-123.92', 'logps_train/chosen': '-115.51', 'loss/train': '0.69197', 'examples_per_second': '31.439', 'grad_norm': '25.625', 'counters/examples': 151776, 'counters/updates': 4743}
train stats after 151808 examples: {'rewards_train/chosen': '0.167', 'rewards_train/rejected': '0.064845', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10215', 'logps_train/rejected': '-106.1', 'logps_train/chosen': '-144.83', 'loss/train': '0.64896', 'examples_per_second': '31.126', 'grad_norm': '27.625', 'counters/examples': 151808, 'counters/updates': 4744}
train stats after 151840 examples: {'rewards_train/chosen': '0.064395', 'rewards_train/rejected': '0.12391', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.059511', 'logps_train/rejected': '-172.83', 'logps_train/chosen': '-181.07', 'loss/train': '0.73991', 'examples_per_second': '31.623', 'grad_norm': '55', 'counters/examples': 151840, 'counters/updates': 4745}
train stats after 151872 examples: {'rewards_train/chosen': '0.055098', 'rewards_train/rejected': '0.055603', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00050534', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-133.15', 'loss/train': '0.70118', 'examples_per_second': '30.089', 'grad_norm': '33.5', 'counters/examples': 151872, 'counters/updates': 4746}
train stats after 151904 examples: {'rewards_train/chosen': '0.15648', 'rewards_train/rejected': '0.16496', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0084809', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-154', 'loss/train': '0.72047', 'examples_per_second': '29.74', 'grad_norm': '49.5', 'counters/examples': 151904, 'counters/updates': 4747}
train stats after 151936 examples: {'rewards_train/chosen': '0.15928', 'rewards_train/rejected': '0.037428', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12185', 'logps_train/rejected': '-119.88', 'logps_train/chosen': '-133.57', 'loss/train': '0.65317', 'examples_per_second': '26.542', 'grad_norm': '27.25', 'counters/examples': 151936, 'counters/updates': 4748}
skipping logging after 151968 examples to avoid logging too frequently
train stats after 152000 examples: {'rewards_train/chosen': '0.23396', 'rewards_train/rejected': '0.063913', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17005', 'logps_train/rejected': '-131.65', 'logps_train/chosen': '-177.8', 'loss/train': '0.62674', 'examples_per_second': '31.256', 'grad_norm': '28.625', 'counters/examples': 152000, 'counters/updates': 4750}
train stats after 152032 examples: {'rewards_train/chosen': '0.18341', 'rewards_train/rejected': '0.083543', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.099866', 'logps_train/rejected': '-127.27', 'logps_train/chosen': '-141.63', 'loss/train': '0.65913', 'examples_per_second': '24.266', 'grad_norm': '40.75', 'counters/examples': 152032, 'counters/updates': 4751}
train stats after 152064 examples: {'rewards_train/chosen': '0.11962', 'rewards_train/rejected': '0.037068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082551', 'logps_train/rejected': '-125.01', 'logps_train/chosen': '-106.25', 'loss/train': '0.66366', 'examples_per_second': '31.218', 'grad_norm': '33', 'counters/examples': 152064, 'counters/updates': 4752}
train stats after 152096 examples: {'rewards_train/chosen': '0.13043', 'rewards_train/rejected': '0.070029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060398', 'logps_train/rejected': '-128.89', 'logps_train/chosen': '-144.94', 'loss/train': '0.6697', 'examples_per_second': '30.483', 'grad_norm': '34', 'counters/examples': 152096, 'counters/updates': 4753}
train stats after 152128 examples: {'rewards_train/chosen': '0.1624', 'rewards_train/rejected': '0.095579', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066817', 'logps_train/rejected': '-139.12', 'logps_train/chosen': '-174.02', 'loss/train': '0.6669', 'examples_per_second': '31.447', 'grad_norm': '39.75', 'counters/examples': 152128, 'counters/updates': 4754}
skipping logging after 152160 examples to avoid logging too frequently
train stats after 152192 examples: {'rewards_train/chosen': '0.18382', 'rewards_train/rejected': '0.085148', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.098675', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-203.48', 'loss/train': '0.66168', 'examples_per_second': '31.439', 'grad_norm': '31.25', 'counters/examples': 152192, 'counters/updates': 4756}
train stats after 152224 examples: {'rewards_train/chosen': '0.083944', 'rewards_train/rejected': '0.12052', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '-0.036573', 'logps_train/rejected': '-151.35', 'logps_train/chosen': '-150.89', 'loss/train': '0.74997', 'examples_per_second': '31.285', 'grad_norm': '54', 'counters/examples': 152224, 'counters/updates': 4757}
train stats after 152256 examples: {'rewards_train/chosen': '0.081078', 'rewards_train/rejected': '0.086974', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0058964', 'logps_train/rejected': '-133.5', 'logps_train/chosen': '-147.7', 'loss/train': '0.70535', 'examples_per_second': '31.501', 'grad_norm': '34', 'counters/examples': 152256, 'counters/updates': 4758}
train stats after 152288 examples: {'rewards_train/chosen': '0.21122', 'rewards_train/rejected': '0.03712', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1741', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-164.85', 'loss/train': '0.6204', 'examples_per_second': '31.673', 'grad_norm': '29', 'counters/examples': 152288, 'counters/updates': 4759}
train stats after 152320 examples: {'rewards_train/chosen': '0.22516', 'rewards_train/rejected': '0.083858', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14131', 'logps_train/rejected': '-152.93', 'logps_train/chosen': '-148.11', 'loss/train': '0.64092', 'examples_per_second': '31.442', 'grad_norm': '41.5', 'counters/examples': 152320, 'counters/updates': 4760}
train stats after 152352 examples: {'rewards_train/chosen': '0.13626', 'rewards_train/rejected': '0.045605', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090655', 'logps_train/rejected': '-128.82', 'logps_train/chosen': '-128.4', 'loss/train': '0.65713', 'examples_per_second': '30.637', 'grad_norm': '27.75', 'counters/examples': 152352, 'counters/updates': 4761}
train stats after 152384 examples: {'rewards_train/chosen': '0.11096', 'rewards_train/rejected': '0.06871', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042247', 'logps_train/rejected': '-141.56', 'logps_train/chosen': '-132.48', 'loss/train': '0.68204', 'examples_per_second': '31.326', 'grad_norm': '34.25', 'counters/examples': 152384, 'counters/updates': 4762}
train stats after 152416 examples: {'rewards_train/chosen': '0.16525', 'rewards_train/rejected': '0.069785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095465', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-151.73', 'loss/train': '0.66306', 'examples_per_second': '31.389', 'grad_norm': '36.75', 'counters/examples': 152416, 'counters/updates': 4763}
train stats after 152448 examples: {'rewards_train/chosen': '0.04953', 'rewards_train/rejected': '-0.029706', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079236', 'logps_train/rejected': '-125.57', 'logps_train/chosen': '-140.79', 'loss/train': '0.66352', 'examples_per_second': '32.077', 'grad_norm': '27.125', 'counters/examples': 152448, 'counters/updates': 4764}
train stats after 152480 examples: {'rewards_train/chosen': '0.16914', 'rewards_train/rejected': '0.095711', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073427', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-151.5', 'loss/train': '0.66808', 'examples_per_second': '32.961', 'grad_norm': '30', 'counters/examples': 152480, 'counters/updates': 4765}
train stats after 152512 examples: {'rewards_train/chosen': '0.15611', 'rewards_train/rejected': '0.026027', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13009', 'logps_train/rejected': '-100.4', 'logps_train/chosen': '-150.55', 'loss/train': '0.64461', 'examples_per_second': '30.842', 'grad_norm': '28.875', 'counters/examples': 152512, 'counters/updates': 4766}
skipping logging after 152544 examples to avoid logging too frequently
train stats after 152576 examples: {'rewards_train/chosen': '0.14497', 'rewards_train/rejected': '0.083683', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061283', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-146.01', 'loss/train': '0.67048', 'examples_per_second': '33.417', 'grad_norm': '26.625', 'counters/examples': 152576, 'counters/updates': 4768}
train stats after 152608 examples: {'rewards_train/chosen': '0.19269', 'rewards_train/rejected': '0.13885', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053839', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-172.66', 'loss/train': '0.6874', 'examples_per_second': '31.412', 'grad_norm': '84', 'counters/examples': 152608, 'counters/updates': 4769}
train stats after 152640 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.016619', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11681', 'logps_train/rejected': '-127.68', 'logps_train/chosen': '-153.91', 'loss/train': '0.64805', 'examples_per_second': '32.646', 'grad_norm': '31.875', 'counters/examples': 152640, 'counters/updates': 4770}
train stats after 152672 examples: {'rewards_train/chosen': '0.24321', 'rewards_train/rejected': '0.022181', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22103', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-169.45', 'loss/train': '0.61408', 'examples_per_second': '32.004', 'grad_norm': '29.5', 'counters/examples': 152672, 'counters/updates': 4771}
skipping logging after 152704 examples to avoid logging too frequently
train stats after 152736 examples: {'rewards_train/chosen': '0.10221', 'rewards_train/rejected': '0.12657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.024361', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-141.17', 'loss/train': '0.72573', 'examples_per_second': '33.21', 'grad_norm': '33.75', 'counters/examples': 152736, 'counters/updates': 4773}
train stats after 152768 examples: {'rewards_train/chosen': '0.22474', 'rewards_train/rejected': '0.044498', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18024', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-129.22', 'loss/train': '0.62864', 'examples_per_second': '32.369', 'grad_norm': '30.5', 'counters/examples': 152768, 'counters/updates': 4774}
train stats after 152800 examples: {'rewards_train/chosen': '0.15177', 'rewards_train/rejected': '0.088655', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063119', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-115.79', 'loss/train': '0.67226', 'examples_per_second': '30.115', 'grad_norm': '30', 'counters/examples': 152800, 'counters/updates': 4775}
train stats after 152832 examples: {'rewards_train/chosen': '0.086093', 'rewards_train/rejected': '0.052412', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033681', 'logps_train/rejected': '-163.32', 'logps_train/chosen': '-150.14', 'loss/train': '0.69227', 'examples_per_second': '29.963', 'grad_norm': '44.25', 'counters/examples': 152832, 'counters/updates': 4776}
train stats after 152864 examples: {'rewards_train/chosen': '0.14202', 'rewards_train/rejected': '0.016877', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12514', 'logps_train/rejected': '-154.99', 'logps_train/chosen': '-159.96', 'loss/train': '0.64463', 'examples_per_second': '32.3', 'grad_norm': '31.25', 'counters/examples': 152864, 'counters/updates': 4777}
train stats after 152896 examples: {'rewards_train/chosen': '0.068459', 'rewards_train/rejected': '0.079404', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.010946', 'logps_train/rejected': '-148.64', 'logps_train/chosen': '-162.53', 'loss/train': '0.70903', 'examples_per_second': '31.251', 'grad_norm': '41.75', 'counters/examples': 152896, 'counters/updates': 4778}
train stats after 152928 examples: {'rewards_train/chosen': '0.19437', 'rewards_train/rejected': '0.1221', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072279', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-128.9', 'loss/train': '0.66658', 'examples_per_second': '31.426', 'grad_norm': '30.375', 'counters/examples': 152928, 'counters/updates': 4779}
skipping logging after 152960 examples to avoid logging too frequently
train stats after 152992 examples: {'rewards_train/chosen': '0.064323', 'rewards_train/rejected': '0.050218', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.014105', 'logps_train/rejected': '-121.09', 'logps_train/chosen': '-138.18', 'loss/train': '0.69397', 'examples_per_second': '32.559', 'grad_norm': '61.5', 'counters/examples': 152992, 'counters/updates': 4781}
skipping logging after 153024 examples to avoid logging too frequently
train stats after 153056 examples: {'rewards_train/chosen': '0.15075', 'rewards_train/rejected': '0.096359', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054387', 'logps_train/rejected': '-130.35', 'logps_train/chosen': '-149.2', 'loss/train': '0.6846', 'examples_per_second': '30.491', 'grad_norm': '33.25', 'counters/examples': 153056, 'counters/updates': 4783}
skipping logging after 153088 examples to avoid logging too frequently
train stats after 153120 examples: {'rewards_train/chosen': '0.22981', 'rewards_train/rejected': '0.080432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14938', 'logps_train/rejected': '-104.65', 'logps_train/chosen': '-155.53', 'loss/train': '0.6407', 'examples_per_second': '31.305', 'grad_norm': '29.75', 'counters/examples': 153120, 'counters/updates': 4785}
train stats after 153152 examples: {'rewards_train/chosen': '0.10912', 'rewards_train/rejected': '0.048156', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.060962', 'logps_train/rejected': '-108.08', 'logps_train/chosen': '-117.53', 'loss/train': '0.67242', 'examples_per_second': '32.551', 'grad_norm': '30.875', 'counters/examples': 153152, 'counters/updates': 4786}
train stats after 153184 examples: {'rewards_train/chosen': '0.14876', 'rewards_train/rejected': '-0.014994', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16375', 'logps_train/rejected': '-130.98', 'logps_train/chosen': '-125.08', 'loss/train': '0.6389', 'examples_per_second': '33.101', 'grad_norm': '38.25', 'counters/examples': 153184, 'counters/updates': 4787}
skipping logging after 153216 examples to avoid logging too frequently
train stats after 153248 examples: {'rewards_train/chosen': '0.12495', 'rewards_train/rejected': '0.090981', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033971', 'logps_train/rejected': '-150.06', 'logps_train/chosen': '-172.19', 'loss/train': '0.68399', 'examples_per_second': '31.423', 'grad_norm': '39.5', 'counters/examples': 153248, 'counters/updates': 4789}
train stats after 153280 examples: {'rewards_train/chosen': '0.013401', 'rewards_train/rejected': '0.02156', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0081591', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-94.723', 'loss/train': '0.70398', 'examples_per_second': '31.43', 'grad_norm': '24.375', 'counters/examples': 153280, 'counters/updates': 4790}
train stats after 153312 examples: {'rewards_train/chosen': '0.20322', 'rewards_train/rejected': '0.13169', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071537', 'logps_train/rejected': '-125.92', 'logps_train/chosen': '-150.79', 'loss/train': '0.66596', 'examples_per_second': '31.474', 'grad_norm': '41.5', 'counters/examples': 153312, 'counters/updates': 4791}
train stats after 153344 examples: {'rewards_train/chosen': '0.15625', 'rewards_train/rejected': '0.13489', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.021361', 'logps_train/rejected': '-130.25', 'logps_train/chosen': '-171.93', 'loss/train': '0.69601', 'examples_per_second': '30.663', 'grad_norm': '40.5', 'counters/examples': 153344, 'counters/updates': 4792}
skipping logging after 153376 examples to avoid logging too frequently
train stats after 153408 examples: {'rewards_train/chosen': '0.07342', 'rewards_train/rejected': '0.043576', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029844', 'logps_train/rejected': '-104.84', 'logps_train/chosen': '-106.52', 'loss/train': '0.68467', 'examples_per_second': '35.607', 'grad_norm': '26.125', 'counters/examples': 153408, 'counters/updates': 4794}
train stats after 153440 examples: {'rewards_train/chosen': '0.11514', 'rewards_train/rejected': '0.024519', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090624', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-155.9', 'loss/train': '0.67163', 'examples_per_second': '30.811', 'grad_norm': '28.625', 'counters/examples': 153440, 'counters/updates': 4795}
train stats after 153472 examples: {'rewards_train/chosen': '0.13457', 'rewards_train/rejected': '0.092071', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042497', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-159.9', 'loss/train': '0.68215', 'examples_per_second': '31.408', 'grad_norm': '53.75', 'counters/examples': 153472, 'counters/updates': 4796}
skipping logging after 153504 examples to avoid logging too frequently
train stats after 153536 examples: {'rewards_train/chosen': '0.095685', 'rewards_train/rejected': '0.067055', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.02863', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-155.09', 'loss/train': '0.69092', 'examples_per_second': '34.22', 'grad_norm': '34', 'counters/examples': 153536, 'counters/updates': 4798}
skipping logging after 153568 examples to avoid logging too frequently
train stats after 153600 examples: {'rewards_train/chosen': '0.10263', 'rewards_train/rejected': '0.040759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061868', 'logps_train/rejected': '-89.233', 'logps_train/chosen': '-94.435', 'loss/train': '0.67066', 'examples_per_second': '33.068', 'grad_norm': '27.25', 'counters/examples': 153600, 'counters/updates': 4800}
train stats after 153632 examples: {'rewards_train/chosen': '0.17267', 'rewards_train/rejected': '0.0777', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.094973', 'logps_train/rejected': '-96.157', 'logps_train/chosen': '-150.6', 'loss/train': '0.65464', 'examples_per_second': '31.041', 'grad_norm': '26.5', 'counters/examples': 153632, 'counters/updates': 4801}
train stats after 153664 examples: {'rewards_train/chosen': '0.13532', 'rewards_train/rejected': '-0.021988', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15731', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-183.01', 'loss/train': '0.62999', 'examples_per_second': '29.899', 'grad_norm': '34.75', 'counters/examples': 153664, 'counters/updates': 4802}
train stats after 153696 examples: {'rewards_train/chosen': '0.27762', 'rewards_train/rejected': '0.076506', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20111', 'logps_train/rejected': '-160.31', 'logps_train/chosen': '-216.16', 'loss/train': '0.61001', 'examples_per_second': '31.339', 'grad_norm': '46.25', 'counters/examples': 153696, 'counters/updates': 4803}
train stats after 153728 examples: {'rewards_train/chosen': '0.17551', 'rewards_train/rejected': '0.064937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11057', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-138.79', 'loss/train': '0.64789', 'examples_per_second': '30.416', 'grad_norm': '29.75', 'counters/examples': 153728, 'counters/updates': 4804}
skipping logging after 153760 examples to avoid logging too frequently
train stats after 153792 examples: {'rewards_train/chosen': '0.085636', 'rewards_train/rejected': '0.077632', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0080041', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-171.66', 'loss/train': '0.69852', 'examples_per_second': '32.505', 'grad_norm': '33', 'counters/examples': 153792, 'counters/updates': 4806}
train stats after 153824 examples: {'rewards_train/chosen': '0.12884', 'rewards_train/rejected': '0.037762', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091078', 'logps_train/rejected': '-156.09', 'logps_train/chosen': '-142.12', 'loss/train': '0.66005', 'examples_per_second': '29.755', 'grad_norm': '43.75', 'counters/examples': 153824, 'counters/updates': 4807}
train stats after 153856 examples: {'rewards_train/chosen': '0.20966', 'rewards_train/rejected': '0.066998', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14266', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-172.34', 'loss/train': '0.63876', 'examples_per_second': '31.472', 'grad_norm': '33.75', 'counters/examples': 153856, 'counters/updates': 4808}
train stats after 153888 examples: {'rewards_train/chosen': '0.13788', 'rewards_train/rejected': '0.081207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.056672', 'logps_train/rejected': '-131.27', 'logps_train/chosen': '-165.09', 'loss/train': '0.67286', 'examples_per_second': '30.486', 'grad_norm': '34.75', 'counters/examples': 153888, 'counters/updates': 4809}
train stats after 153920 examples: {'rewards_train/chosen': '0.14341', 'rewards_train/rejected': '0.039769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10364', 'logps_train/rejected': '-128.92', 'logps_train/chosen': '-144.4', 'loss/train': '0.6519', 'examples_per_second': '29.821', 'grad_norm': '32.5', 'counters/examples': 153920, 'counters/updates': 4810}
train stats after 153952 examples: {'rewards_train/chosen': '0.1354', 'rewards_train/rejected': '0.0018466', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13356', 'logps_train/rejected': '-97.795', 'logps_train/chosen': '-117.13', 'loss/train': '0.6372', 'examples_per_second': '31.771', 'grad_norm': '31.5', 'counters/examples': 153952, 'counters/updates': 4811}
train stats after 153984 examples: {'rewards_train/chosen': '0.11564', 'rewards_train/rejected': '-0.02714', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14278', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-163.89', 'loss/train': '0.64349', 'examples_per_second': '30.904', 'grad_norm': '30.375', 'counters/examples': 153984, 'counters/updates': 4812}
train stats after 154016 examples: {'rewards_train/chosen': '0.049322', 'rewards_train/rejected': '-0.0067873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056109', 'logps_train/rejected': '-145.85', 'logps_train/chosen': '-125.12', 'loss/train': '0.67399', 'examples_per_second': '29.921', 'grad_norm': '46.75', 'counters/examples': 154016, 'counters/updates': 4813}
train stats after 154048 examples: {'rewards_train/chosen': '0.052827', 'rewards_train/rejected': '0.05235', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00047744', 'logps_train/rejected': '-152.85', 'logps_train/chosen': '-151.83', 'loss/train': '0.70622', 'examples_per_second': '29.908', 'grad_norm': '31', 'counters/examples': 154048, 'counters/updates': 4814}
train stats after 154080 examples: {'rewards_train/chosen': '0.058406', 'rewards_train/rejected': '0.07678', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.018374', 'logps_train/rejected': '-147.69', 'logps_train/chosen': '-151.32', 'loss/train': '0.71753', 'examples_per_second': '29.384', 'grad_norm': '65', 'counters/examples': 154080, 'counters/updates': 4815}
skipping logging after 154112 examples to avoid logging too frequently
train stats after 154144 examples: {'rewards_train/chosen': '0.18718', 'rewards_train/rejected': '0.01946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16772', 'logps_train/rejected': '-151.03', 'logps_train/chosen': '-130.69', 'loss/train': '0.62315', 'examples_per_second': '31.341', 'grad_norm': '30.625', 'counters/examples': 154144, 'counters/updates': 4817}
skipping logging after 154176 examples to avoid logging too frequently
train stats after 154208 examples: {'rewards_train/chosen': '0.12462', 'rewards_train/rejected': '0.046096', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.07852', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-130.56', 'loss/train': '0.66296', 'examples_per_second': '30.015', 'grad_norm': '30.875', 'counters/examples': 154208, 'counters/updates': 4819}
train stats after 154240 examples: {'rewards_train/chosen': '0.095211', 'rewards_train/rejected': '0.038344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056867', 'logps_train/rejected': '-146.64', 'logps_train/chosen': '-141.28', 'loss/train': '0.67559', 'examples_per_second': '29.934', 'grad_norm': '43.75', 'counters/examples': 154240, 'counters/updates': 4820}
skipping logging after 154272 examples to avoid logging too frequently
train stats after 154304 examples: {'rewards_train/chosen': '0.089677', 'rewards_train/rejected': '0.025897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063781', 'logps_train/rejected': '-111.85', 'logps_train/chosen': '-113.04', 'loss/train': '0.67888', 'examples_per_second': '34.03', 'grad_norm': '45.5', 'counters/examples': 154304, 'counters/updates': 4822}
train stats after 154336 examples: {'rewards_train/chosen': '0.099704', 'rewards_train/rejected': '-0.044026', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14373', 'logps_train/rejected': '-113.29', 'logps_train/chosen': '-177.85', 'loss/train': '0.63859', 'examples_per_second': '30.023', 'grad_norm': '32', 'counters/examples': 154336, 'counters/updates': 4823}
train stats after 154368 examples: {'rewards_train/chosen': '0.16282', 'rewards_train/rejected': '0.16752', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0047036', 'logps_train/rejected': '-178.53', 'logps_train/chosen': '-143.03', 'loss/train': '0.71909', 'examples_per_second': '31.495', 'grad_norm': '47', 'counters/examples': 154368, 'counters/updates': 4824}
train stats after 154400 examples: {'rewards_train/chosen': '0.12831', 'rewards_train/rejected': '0.034998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093312', 'logps_train/rejected': '-156.67', 'logps_train/chosen': '-164.71', 'loss/train': '0.65892', 'examples_per_second': '33.061', 'grad_norm': '31.25', 'counters/examples': 154400, 'counters/updates': 4825}
train stats after 154432 examples: {'rewards_train/chosen': '0.11303', 'rewards_train/rejected': '0.047549', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.065482', 'logps_train/rejected': '-154.59', 'logps_train/chosen': '-140.27', 'loss/train': '0.66941', 'examples_per_second': '31.61', 'grad_norm': '39.75', 'counters/examples': 154432, 'counters/updates': 4826}
train stats after 154464 examples: {'rewards_train/chosen': '0.053609', 'rewards_train/rejected': '0.029437', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024172', 'logps_train/rejected': '-93.209', 'logps_train/chosen': '-134.79', 'loss/train': '0.69392', 'examples_per_second': '31.775', 'grad_norm': '31.625', 'counters/examples': 154464, 'counters/updates': 4827}
train stats after 154496 examples: {'rewards_train/chosen': '0.096389', 'rewards_train/rejected': '0.054248', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042141', 'logps_train/rejected': '-149.75', 'logps_train/chosen': '-164.53', 'loss/train': '0.68561', 'examples_per_second': '31.434', 'grad_norm': '33.75', 'counters/examples': 154496, 'counters/updates': 4828}
train stats after 154528 examples: {'rewards_train/chosen': '0.10945', 'rewards_train/rejected': '0.11498', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0055261', 'logps_train/rejected': '-124.82', 'logps_train/chosen': '-148.45', 'loss/train': '0.71315', 'examples_per_second': '30.924', 'grad_norm': '44.25', 'counters/examples': 154528, 'counters/updates': 4829}
train stats after 154560 examples: {'rewards_train/chosen': '0.20528', 'rewards_train/rejected': '0.06848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1368', 'logps_train/rejected': '-99.842', 'logps_train/chosen': '-144.61', 'loss/train': '0.63605', 'examples_per_second': '31.742', 'grad_norm': '49.5', 'counters/examples': 154560, 'counters/updates': 4830}
train stats after 154592 examples: {'rewards_train/chosen': '0.19837', 'rewards_train/rejected': '0.094668', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1037', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-134.83', 'loss/train': '0.66146', 'examples_per_second': '30.512', 'grad_norm': '41.75', 'counters/examples': 154592, 'counters/updates': 4831}
train stats after 154624 examples: {'rewards_train/chosen': '0.12539', 'rewards_train/rejected': '0.035893', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089495', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-144.57', 'loss/train': '0.66203', 'examples_per_second': '31.498', 'grad_norm': '29.125', 'counters/examples': 154624, 'counters/updates': 4832}
skipping logging after 154656 examples to avoid logging too frequently
train stats after 154688 examples: {'rewards_train/chosen': '0.14096', 'rewards_train/rejected': '0.08508', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05588', 'logps_train/rejected': '-150.16', 'logps_train/chosen': '-165.73', 'loss/train': '0.6752', 'examples_per_second': '30.649', 'grad_norm': '26.75', 'counters/examples': 154688, 'counters/updates': 4834}
train stats after 154720 examples: {'rewards_train/chosen': '0.042855', 'rewards_train/rejected': '0.10187', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.059014', 'logps_train/rejected': '-139.39', 'logps_train/chosen': '-135.11', 'loss/train': '0.78546', 'examples_per_second': '32.305', 'grad_norm': '51.5', 'counters/examples': 154720, 'counters/updates': 4835}
train stats after 154752 examples: {'rewards_train/chosen': '0.10191', 'rewards_train/rejected': '-0.020159', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12207', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-139.09', 'loss/train': '0.64093', 'examples_per_second': '31.488', 'grad_norm': '32.5', 'counters/examples': 154752, 'counters/updates': 4836}
train stats after 154784 examples: {'rewards_train/chosen': '0.040453', 'rewards_train/rejected': '-0.0010775', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04153', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-168.26', 'loss/train': '0.68321', 'examples_per_second': '23.446', 'grad_norm': '33.25', 'counters/examples': 154784, 'counters/updates': 4837}
train stats after 154816 examples: {'rewards_train/chosen': '0.19981', 'rewards_train/rejected': '0.058035', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14178', 'logps_train/rejected': '-139.57', 'logps_train/chosen': '-156.8', 'loss/train': '0.63786', 'examples_per_second': '32.61', 'grad_norm': '32.5', 'counters/examples': 154816, 'counters/updates': 4838}
train stats after 154848 examples: {'rewards_train/chosen': '0.12157', 'rewards_train/rejected': '0.12982', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0082502', 'logps_train/rejected': '-163.88', 'logps_train/chosen': '-145.7', 'loss/train': '0.70483', 'examples_per_second': '31.272', 'grad_norm': '34', 'counters/examples': 154848, 'counters/updates': 4839}
train stats after 154880 examples: {'rewards_train/chosen': '0.151', 'rewards_train/rejected': '0.0090122', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14199', 'logps_train/rejected': '-131.69', 'logps_train/chosen': '-170.94', 'loss/train': '0.63244', 'examples_per_second': '31.326', 'grad_norm': '32', 'counters/examples': 154880, 'counters/updates': 4840}
train stats after 154912 examples: {'rewards_train/chosen': '0.090117', 'rewards_train/rejected': '0.036511', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053606', 'logps_train/rejected': '-129.6', 'logps_train/chosen': '-138.2', 'loss/train': '0.67378', 'examples_per_second': '31.925', 'grad_norm': '30.5', 'counters/examples': 154912, 'counters/updates': 4841}
train stats after 154944 examples: {'rewards_train/chosen': '0.16968', 'rewards_train/rejected': '0.1204', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.049281', 'logps_train/rejected': '-113.09', 'logps_train/chosen': '-167.21', 'loss/train': '0.67677', 'examples_per_second': '31.413', 'grad_norm': '27.875', 'counters/examples': 154944, 'counters/updates': 4842}
train stats after 154976 examples: {'rewards_train/chosen': '0.23643', 'rewards_train/rejected': '0.16117', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075254', 'logps_train/rejected': '-113.91', 'logps_train/chosen': '-120.4', 'loss/train': '0.6837', 'examples_per_second': '32.574', 'grad_norm': '37.75', 'counters/examples': 154976, 'counters/updates': 4843}
train stats after 155008 examples: {'rewards_train/chosen': '0.20299', 'rewards_train/rejected': '0.018112', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18488', 'logps_train/rejected': '-121.55', 'logps_train/chosen': '-142.65', 'loss/train': '0.61771', 'examples_per_second': '30.74', 'grad_norm': '38.75', 'counters/examples': 155008, 'counters/updates': 4844}
skipping logging after 155040 examples to avoid logging too frequently
train stats after 155072 examples: {'rewards_train/chosen': '0.27821', 'rewards_train/rejected': '0.082136', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19608', 'logps_train/rejected': '-160.16', 'logps_train/chosen': '-156.76', 'loss/train': '0.61132', 'examples_per_second': '32.199', 'grad_norm': '28.625', 'counters/examples': 155072, 'counters/updates': 4846}
skipping logging after 155104 examples to avoid logging too frequently
train stats after 155136 examples: {'rewards_train/chosen': '0.11498', 'rewards_train/rejected': '0.022819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092163', 'logps_train/rejected': '-116.75', 'logps_train/chosen': '-132.71', 'loss/train': '0.65895', 'examples_per_second': '32.115', 'grad_norm': '65', 'counters/examples': 155136, 'counters/updates': 4848}
train stats after 155168 examples: {'rewards_train/chosen': '0.25445', 'rewards_train/rejected': '0.095446', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15901', 'logps_train/rejected': '-143.08', 'logps_train/chosen': '-181.33', 'loss/train': '0.62931', 'examples_per_second': '31.37', 'grad_norm': '44', 'counters/examples': 155168, 'counters/updates': 4849}
train stats after 155200 examples: {'rewards_train/chosen': '0.20743', 'rewards_train/rejected': '0.06433', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1431', 'logps_train/rejected': '-136.72', 'logps_train/chosen': '-151.14', 'loss/train': '0.63924', 'examples_per_second': '29.883', 'grad_norm': '37.25', 'counters/examples': 155200, 'counters/updates': 4850}
skipping logging after 155232 examples to avoid logging too frequently
train stats after 155264 examples: {'rewards_train/chosen': '0.12943', 'rewards_train/rejected': '0.054334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075096', 'logps_train/rejected': '-108.01', 'logps_train/chosen': '-128.9', 'loss/train': '0.66727', 'examples_per_second': '31.463', 'grad_norm': '27.5', 'counters/examples': 155264, 'counters/updates': 4852}
skipping logging after 155296 examples to avoid logging too frequently
train stats after 155328 examples: {'rewards_train/chosen': '0.087563', 'rewards_train/rejected': '0.0019677', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085595', 'logps_train/rejected': '-83.813', 'logps_train/chosen': '-127.27', 'loss/train': '0.65475', 'examples_per_second': '30.982', 'grad_norm': '28.5', 'counters/examples': 155328, 'counters/updates': 4854}
skipping logging after 155360 examples to avoid logging too frequently
train stats after 155392 examples: {'rewards_train/chosen': '0.20506', 'rewards_train/rejected': '0.022972', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18209', 'logps_train/rejected': '-133.47', 'logps_train/chosen': '-179.76', 'loss/train': '0.61821', 'examples_per_second': '31.299', 'grad_norm': '29.5', 'counters/examples': 155392, 'counters/updates': 4856}
train stats after 155424 examples: {'rewards_train/chosen': '0.13818', 'rewards_train/rejected': '0.078322', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059862', 'logps_train/rejected': '-131.31', 'logps_train/chosen': '-137.76', 'loss/train': '0.67392', 'examples_per_second': '30.464', 'grad_norm': '38.25', 'counters/examples': 155424, 'counters/updates': 4857}
train stats after 155456 examples: {'rewards_train/chosen': '0.10784', 'rewards_train/rejected': '0.038861', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.068984', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-104.75', 'loss/train': '0.6698', 'examples_per_second': '30.594', 'grad_norm': '25', 'counters/examples': 155456, 'counters/updates': 4858}
train stats after 155488 examples: {'rewards_train/chosen': '0.14735', 'rewards_train/rejected': '0.067581', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079772', 'logps_train/rejected': '-147.69', 'logps_train/chosen': '-153.11', 'loss/train': '0.66647', 'examples_per_second': '30.476', 'grad_norm': '35.5', 'counters/examples': 155488, 'counters/updates': 4859}
train stats after 155520 examples: {'rewards_train/chosen': '0.16221', 'rewards_train/rejected': '0.11302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049192', 'logps_train/rejected': '-132.51', 'logps_train/chosen': '-148.99', 'loss/train': '0.67894', 'examples_per_second': '30.065', 'grad_norm': '36.75', 'counters/examples': 155520, 'counters/updates': 4860}
train stats after 155552 examples: {'rewards_train/chosen': '0.19122', 'rewards_train/rejected': '-0.0081444', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19936', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-132.59', 'loss/train': '0.6078', 'examples_per_second': '31.985', 'grad_norm': '28.625', 'counters/examples': 155552, 'counters/updates': 4861}
train stats after 155584 examples: {'rewards_train/chosen': '0.26621', 'rewards_train/rejected': '0.021567', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24464', 'logps_train/rejected': '-108.12', 'logps_train/chosen': '-151.37', 'loss/train': '0.6037', 'examples_per_second': '32.255', 'grad_norm': '27', 'counters/examples': 155584, 'counters/updates': 4862}
train stats after 155616 examples: {'rewards_train/chosen': '0.094691', 'rewards_train/rejected': '0.071529', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023161', 'logps_train/rejected': '-152.71', 'logps_train/chosen': '-137.39', 'loss/train': '0.68835', 'examples_per_second': '31.354', 'grad_norm': '38.5', 'counters/examples': 155616, 'counters/updates': 4863}
train stats after 155648 examples: {'rewards_train/chosen': '0.12177', 'rewards_train/rejected': '0.073156', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.048613', 'logps_train/rejected': '-166.7', 'logps_train/chosen': '-146.21', 'loss/train': '0.68714', 'examples_per_second': '31.353', 'grad_norm': '44.5', 'counters/examples': 155648, 'counters/updates': 4864}
train stats after 155680 examples: {'rewards_train/chosen': '0.18161', 'rewards_train/rejected': '0.13213', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.049482', 'logps_train/rejected': '-143.29', 'logps_train/chosen': '-164.34', 'loss/train': '0.67614', 'examples_per_second': '32.193', 'grad_norm': '48.25', 'counters/examples': 155680, 'counters/updates': 4865}
skipping logging after 155712 examples to avoid logging too frequently
train stats after 155744 examples: {'rewards_train/chosen': '0.19042', 'rewards_train/rejected': '0.068658', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12176', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-149.17', 'loss/train': '0.64049', 'examples_per_second': '33.756', 'grad_norm': '36.25', 'counters/examples': 155744, 'counters/updates': 4867}
train stats after 155776 examples: {'rewards_train/chosen': '0.19105', 'rewards_train/rejected': '0.19018', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.00086903', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-167.46', 'loss/train': '0.73371', 'examples_per_second': '32.227', 'grad_norm': '76.5', 'counters/examples': 155776, 'counters/updates': 4868}
train stats after 155808 examples: {'rewards_train/chosen': '0.15271', 'rewards_train/rejected': '0.011368', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14134', 'logps_train/rejected': '-123.95', 'logps_train/chosen': '-148.17', 'loss/train': '0.6294', 'examples_per_second': '30.903', 'grad_norm': '27.875', 'counters/examples': 155808, 'counters/updates': 4869}
train stats after 155840 examples: {'rewards_train/chosen': '0.11588', 'rewards_train/rejected': '0.071196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044683', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-133.55', 'loss/train': '0.68021', 'examples_per_second': '32.052', 'grad_norm': '40.75', 'counters/examples': 155840, 'counters/updates': 4870}
train stats after 155872 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.019506', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15036', 'logps_train/rejected': '-132.26', 'logps_train/chosen': '-154.39', 'loss/train': '0.63149', 'examples_per_second': '31.416', 'grad_norm': '37.75', 'counters/examples': 155872, 'counters/updates': 4871}
train stats after 155904 examples: {'rewards_train/chosen': '0.072548', 'rewards_train/rejected': '0.0018008', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070747', 'logps_train/rejected': '-88.199', 'logps_train/chosen': '-111.02', 'loss/train': '0.66803', 'examples_per_second': '33.12', 'grad_norm': '39', 'counters/examples': 155904, 'counters/updates': 4872}
train stats after 155936 examples: {'rewards_train/chosen': '0.14747', 'rewards_train/rejected': '0.070124', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077346', 'logps_train/rejected': '-121.91', 'logps_train/chosen': '-142.97', 'loss/train': '0.66585', 'examples_per_second': '30.669', 'grad_norm': '33.5', 'counters/examples': 155936, 'counters/updates': 4873}
train stats after 155968 examples: {'rewards_train/chosen': '0.1794', 'rewards_train/rejected': '0.094929', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084466', 'logps_train/rejected': '-135.55', 'logps_train/chosen': '-187.95', 'loss/train': '0.67121', 'examples_per_second': '31.22', 'grad_norm': '67', 'counters/examples': 155968, 'counters/updates': 4874}
train stats after 156000 examples: {'rewards_train/chosen': '0.16295', 'rewards_train/rejected': '0.092298', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070649', 'logps_train/rejected': '-184.32', 'logps_train/chosen': '-166.13', 'loss/train': '0.66721', 'examples_per_second': '30.478', 'grad_norm': '38.25', 'counters/examples': 156000, 'counters/updates': 4875}
Running evaluation after 156000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.16it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.82it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 156000: {'rewards_eval/chosen': '0.14074', 'rewards_eval/rejected': '0.058669', 'rewards_eval/accuracies': '0.58203', 'rewards_eval/margins': '0.082073', 'logps_eval/rejected': '-121.55', 'logps_eval/chosen': '-142.7', 'loss/eval': '0.6655'}
skipping save for non epoch
train stats after 156032 examples: {'rewards_train/chosen': '0.095165', 'rewards_train/rejected': '0.048077', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047088', 'logps_train/rejected': '-115.73', 'logps_train/chosen': '-134.39', 'loss/train': '0.67495', 'examples_per_second': '33.823', 'grad_norm': '29.625', 'counters/examples': 156032, 'counters/updates': 4876}
train stats after 156064 examples: {'rewards_train/chosen': '0.11651', 'rewards_train/rejected': '0.0694', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047106', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-131.12', 'loss/train': '0.67735', 'examples_per_second': '30.792', 'grad_norm': '37.75', 'counters/examples': 156064, 'counters/updates': 4877}
skipping logging after 156096 examples to avoid logging too frequently
train stats after 156128 examples: {'rewards_train/chosen': '0.11383', 'rewards_train/rejected': '0.056028', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057799', 'logps_train/rejected': '-132.13', 'logps_train/chosen': '-126.44', 'loss/train': '0.67591', 'examples_per_second': '32.621', 'grad_norm': '29.5', 'counters/examples': 156128, 'counters/updates': 4879}
skipping logging after 156160 examples to avoid logging too frequently
train stats after 156192 examples: {'rewards_train/chosen': '0.065928', 'rewards_train/rejected': '0.11338', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.047456', 'logps_train/rejected': '-151.11', 'logps_train/chosen': '-119.51', 'loss/train': '0.73751', 'examples_per_second': '31.756', 'grad_norm': '52.25', 'counters/examples': 156192, 'counters/updates': 4881}
train stats after 156224 examples: {'rewards_train/chosen': '0.2032', 'rewards_train/rejected': '0.10804', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095155', 'logps_train/rejected': '-111.39', 'logps_train/chosen': '-122.78', 'loss/train': '0.65532', 'examples_per_second': '31.185', 'grad_norm': '25.625', 'counters/examples': 156224, 'counters/updates': 4882}
skipping logging after 156256 examples to avoid logging too frequently
train stats after 156288 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.036704', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085902', 'logps_train/rejected': '-102.85', 'logps_train/chosen': '-108.53', 'loss/train': '0.65973', 'examples_per_second': '39.828', 'grad_norm': '22.5', 'counters/examples': 156288, 'counters/updates': 4884}
train stats after 156320 examples: {'rewards_train/chosen': '0.13218', 'rewards_train/rejected': '0.052245', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079939', 'logps_train/rejected': '-136.16', 'logps_train/chosen': '-158.31', 'loss/train': '0.6679', 'examples_per_second': '31.456', 'grad_norm': '33', 'counters/examples': 156320, 'counters/updates': 4885}
skipping logging after 156352 examples to avoid logging too frequently
train stats after 156384 examples: {'rewards_train/chosen': '0.20578', 'rewards_train/rejected': '0.073254', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13253', 'logps_train/rejected': '-122.51', 'logps_train/chosen': '-145.38', 'loss/train': '0.6448', 'examples_per_second': '31.205', 'grad_norm': '32.75', 'counters/examples': 156384, 'counters/updates': 4887}
train stats after 156416 examples: {'rewards_train/chosen': '0.20975', 'rewards_train/rejected': '-0.03027', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24002', 'logps_train/rejected': '-87.221', 'logps_train/chosen': '-149.85', 'loss/train': '0.59827', 'examples_per_second': '32.173', 'grad_norm': '35', 'counters/examples': 156416, 'counters/updates': 4888}
train stats after 156448 examples: {'rewards_train/chosen': '0.089029', 'rewards_train/rejected': '0.034628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0544', 'logps_train/rejected': '-152.69', 'logps_train/chosen': '-163.2', 'loss/train': '0.67629', 'examples_per_second': '29.972', 'grad_norm': '30', 'counters/examples': 156448, 'counters/updates': 4889}
skipping logging after 156480 examples to avoid logging too frequently
train stats after 156512 examples: {'rewards_train/chosen': '0.19396', 'rewards_train/rejected': '0.086283', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10767', 'logps_train/rejected': '-130.37', 'logps_train/chosen': '-170.26', 'loss/train': '0.65376', 'examples_per_second': '31.463', 'grad_norm': '31.125', 'counters/examples': 156512, 'counters/updates': 4891}
train stats after 156544 examples: {'rewards_train/chosen': '0.13032', 'rewards_train/rejected': '0.063693', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.066622', 'logps_train/rejected': '-93.781', 'logps_train/chosen': '-182.05', 'loss/train': '0.66731', 'examples_per_second': '31.989', 'grad_norm': '35.75', 'counters/examples': 156544, 'counters/updates': 4892}
train stats after 156576 examples: {'rewards_train/chosen': '0.091637', 'rewards_train/rejected': '0.030794', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060843', 'logps_train/rejected': '-136.38', 'logps_train/chosen': '-172.99', 'loss/train': '0.67456', 'examples_per_second': '30.183', 'grad_norm': '31.625', 'counters/examples': 156576, 'counters/updates': 4893}
train stats after 156608 examples: {'rewards_train/chosen': '0.10846', 'rewards_train/rejected': '-0.010328', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11879', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-162.21', 'loss/train': '0.64695', 'examples_per_second': '31.464', 'grad_norm': '27.125', 'counters/examples': 156608, 'counters/updates': 4894}
train stats after 156640 examples: {'rewards_train/chosen': '0.033823', 'rewards_train/rejected': '-0.0033006', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037124', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-141.25', 'loss/train': '0.68933', 'examples_per_second': '30.522', 'grad_norm': '30.125', 'counters/examples': 156640, 'counters/updates': 4895}
skipping logging after 156672 examples to avoid logging too frequently
train stats after 156704 examples: {'rewards_train/chosen': '0.17228', 'rewards_train/rejected': '0.01052', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16176', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-179.16', 'loss/train': '0.62546', 'examples_per_second': '29.788', 'grad_norm': '31.625', 'counters/examples': 156704, 'counters/updates': 4897}
train stats after 156736 examples: {'rewards_train/chosen': '0.13902', 'rewards_train/rejected': '0.28305', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.14403', 'logps_train/rejected': '-158.32', 'logps_train/chosen': '-151.37', 'loss/train': '0.83242', 'examples_per_second': '30.046', 'grad_norm': '110.5', 'counters/examples': 156736, 'counters/updates': 4898}
train stats after 156768 examples: {'rewards_train/chosen': '0.14599', 'rewards_train/rejected': '0.049823', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096169', 'logps_train/rejected': '-122.05', 'logps_train/chosen': '-126.8', 'loss/train': '0.65055', 'examples_per_second': '31.52', 'grad_norm': '29.125', 'counters/examples': 156768, 'counters/updates': 4899}
train stats after 156800 examples: {'rewards_train/chosen': '0.23518', 'rewards_train/rejected': '0.1725', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062672', 'logps_train/rejected': '-120.24', 'logps_train/chosen': '-196.86', 'loss/train': '0.69024', 'examples_per_second': '30.43', 'grad_norm': '34', 'counters/examples': 156800, 'counters/updates': 4900}
skipping logging after 156832 examples to avoid logging too frequently
train stats after 156864 examples: {'rewards_train/chosen': '0.13029', 'rewards_train/rejected': '0.013152', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11714', 'logps_train/rejected': '-109.43', 'logps_train/chosen': '-138.62', 'loss/train': '0.64638', 'examples_per_second': '31.968', 'grad_norm': '27.5', 'counters/examples': 156864, 'counters/updates': 4902}
train stats after 156896 examples: {'rewards_train/chosen': '0.12405', 'rewards_train/rejected': '0.045622', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078432', 'logps_train/rejected': '-132.73', 'logps_train/chosen': '-153.35', 'loss/train': '0.66476', 'examples_per_second': '31.467', 'grad_norm': '37.5', 'counters/examples': 156896, 'counters/updates': 4903}
train stats after 156928 examples: {'rewards_train/chosen': '0.1761', 'rewards_train/rejected': '0.043124', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13298', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-120.54', 'loss/train': '0.64226', 'examples_per_second': '32.532', 'grad_norm': '28.25', 'counters/examples': 156928, 'counters/updates': 4904}
skipping logging after 156960 examples to avoid logging too frequently
train stats after 156992 examples: {'rewards_train/chosen': '0.089943', 'rewards_train/rejected': '-0.022319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11226', 'logps_train/rejected': '-81.039', 'logps_train/chosen': '-124.24', 'loss/train': '0.65094', 'examples_per_second': '34.004', 'grad_norm': '25.625', 'counters/examples': 156992, 'counters/updates': 4906}
skipping logging after 157024 examples to avoid logging too frequently
train stats after 157056 examples: {'rewards_train/chosen': '0.19782', 'rewards_train/rejected': '0.024274', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17355', 'logps_train/rejected': '-129.18', 'logps_train/chosen': '-145.64', 'loss/train': '0.63675', 'examples_per_second': '31.463', 'grad_norm': '37.5', 'counters/examples': 157056, 'counters/updates': 4908}
train stats after 157088 examples: {'rewards_train/chosen': '0.17177', 'rewards_train/rejected': '0.1018', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06997', 'logps_train/rejected': '-174.96', 'logps_train/chosen': '-186.58', 'loss/train': '0.67302', 'examples_per_second': '30.475', 'grad_norm': '50.25', 'counters/examples': 157088, 'counters/updates': 4909}
train stats after 157120 examples: {'rewards_train/chosen': '0.099842', 'rewards_train/rejected': '0.053608', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046234', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-123.01', 'loss/train': '0.67536', 'examples_per_second': '31.207', 'grad_norm': '47.5', 'counters/examples': 157120, 'counters/updates': 4910}
train stats after 157152 examples: {'rewards_train/chosen': '0.18249', 'rewards_train/rejected': '0.064707', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11779', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-153.68', 'loss/train': '0.64853', 'examples_per_second': '31.304', 'grad_norm': '29.75', 'counters/examples': 157152, 'counters/updates': 4911}
train stats after 157184 examples: {'rewards_train/chosen': '0.15431', 'rewards_train/rejected': '0.11029', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044021', 'logps_train/rejected': '-120.67', 'logps_train/chosen': '-143.3', 'loss/train': '0.6782', 'examples_per_second': '31.621', 'grad_norm': '25.875', 'counters/examples': 157184, 'counters/updates': 4912}
train stats after 157216 examples: {'rewards_train/chosen': '0.17658', 'rewards_train/rejected': '0.0010607', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17552', 'logps_train/rejected': '-151.83', 'logps_train/chosen': '-162.52', 'loss/train': '0.62556', 'examples_per_second': '30.067', 'grad_norm': '27.5', 'counters/examples': 157216, 'counters/updates': 4913}
train stats after 157248 examples: {'rewards_train/chosen': '0.081868', 'rewards_train/rejected': '0.053813', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028054', 'logps_train/rejected': '-109.81', 'logps_train/chosen': '-119.84', 'loss/train': '0.68942', 'examples_per_second': '30.958', 'grad_norm': '27.125', 'counters/examples': 157248, 'counters/updates': 4914}
train stats after 157280 examples: {'rewards_train/chosen': '0.16515', 'rewards_train/rejected': '0.0062465', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15891', 'logps_train/rejected': '-105.74', 'logps_train/chosen': '-121.01', 'loss/train': '0.62673', 'examples_per_second': '30.655', 'grad_norm': '23.625', 'counters/examples': 157280, 'counters/updates': 4915}
train stats after 157312 examples: {'rewards_train/chosen': '0.17134', 'rewards_train/rejected': '0.12406', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047286', 'logps_train/rejected': '-134.18', 'logps_train/chosen': '-141.52', 'loss/train': '0.68274', 'examples_per_second': '31.436', 'grad_norm': '46.25', 'counters/examples': 157312, 'counters/updates': 4916}
train stats after 157344 examples: {'rewards_train/chosen': '0.13669', 'rewards_train/rejected': '0.057391', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079297', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-154.14', 'loss/train': '0.66776', 'examples_per_second': '31.494', 'grad_norm': '28.125', 'counters/examples': 157344, 'counters/updates': 4917}
skipping logging after 157376 examples to avoid logging too frequently
train stats after 157408 examples: {'rewards_train/chosen': '0.096604', 'rewards_train/rejected': '0.02145', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075154', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-143.64', 'loss/train': '0.66597', 'examples_per_second': '23.238', 'grad_norm': '31.125', 'counters/examples': 157408, 'counters/updates': 4919}
train stats after 157440 examples: {'rewards_train/chosen': '0.1414', 'rewards_train/rejected': '0.058268', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.083134', 'logps_train/rejected': '-87.078', 'logps_train/chosen': '-127.59', 'loss/train': '0.66788', 'examples_per_second': '32.096', 'grad_norm': '26.75', 'counters/examples': 157440, 'counters/updates': 4920}
train stats after 157472 examples: {'rewards_train/chosen': '0.094273', 'rewards_train/rejected': '0.014465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079808', 'logps_train/rejected': '-91.993', 'logps_train/chosen': '-118.86', 'loss/train': '0.66203', 'examples_per_second': '32.72', 'grad_norm': '26.875', 'counters/examples': 157472, 'counters/updates': 4921}
train stats after 157504 examples: {'rewards_train/chosen': '0.14801', 'rewards_train/rejected': '0.063009', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085002', 'logps_train/rejected': '-95.249', 'logps_train/chosen': '-169.38', 'loss/train': '0.6584', 'examples_per_second': '23.889', 'grad_norm': '29.125', 'counters/examples': 157504, 'counters/updates': 4922}
train stats after 157536 examples: {'rewards_train/chosen': '0.12752', 'rewards_train/rejected': '0.014904', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11261', 'logps_train/rejected': '-92.243', 'logps_train/chosen': '-137.91', 'loss/train': '0.64407', 'examples_per_second': '31.162', 'grad_norm': '24', 'counters/examples': 157536, 'counters/updates': 4923}
train stats after 157568 examples: {'rewards_train/chosen': '0.17438', 'rewards_train/rejected': '0.049192', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12519', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-145.21', 'loss/train': '0.64675', 'examples_per_second': '31.398', 'grad_norm': '33', 'counters/examples': 157568, 'counters/updates': 4924}
train stats after 157600 examples: {'rewards_train/chosen': '0.22087', 'rewards_train/rejected': '0.076259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14461', 'logps_train/rejected': '-143.55', 'logps_train/chosen': '-175.63', 'loss/train': '0.64691', 'examples_per_second': '31.339', 'grad_norm': '31.75', 'counters/examples': 157600, 'counters/updates': 4925}
train stats after 157632 examples: {'rewards_train/chosen': '0.090976', 'rewards_train/rejected': '0.037043', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053933', 'logps_train/rejected': '-113.46', 'logps_train/chosen': '-114.28', 'loss/train': '0.68346', 'examples_per_second': '30.033', 'grad_norm': '32.75', 'counters/examples': 157632, 'counters/updates': 4926}
train stats after 157664 examples: {'rewards_train/chosen': '0.1671', 'rewards_train/rejected': '0.076192', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090906', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-136.29', 'loss/train': '0.66217', 'examples_per_second': '30.383', 'grad_norm': '28.75', 'counters/examples': 157664, 'counters/updates': 4927}
train stats after 157696 examples: {'rewards_train/chosen': '0.11308', 'rewards_train/rejected': '0.11518', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0020996', 'logps_train/rejected': '-160.22', 'logps_train/chosen': '-147.26', 'loss/train': '0.70746', 'examples_per_second': '31.95', 'grad_norm': '36.25', 'counters/examples': 157696, 'counters/updates': 4928}
train stats after 157728 examples: {'rewards_train/chosen': '0.21381', 'rewards_train/rejected': '0.093959', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11985', 'logps_train/rejected': '-106.25', 'logps_train/chosen': '-156.77', 'loss/train': '0.6526', 'examples_per_second': '30.358', 'grad_norm': '39', 'counters/examples': 157728, 'counters/updates': 4929}
train stats after 157760 examples: {'rewards_train/chosen': '0.11532', 'rewards_train/rejected': '-0.048475', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16379', 'logps_train/rejected': '-134.54', 'logps_train/chosen': '-166.4', 'loss/train': '0.63124', 'examples_per_second': '31.419', 'grad_norm': '33.25', 'counters/examples': 157760, 'counters/updates': 4930}
train stats after 157792 examples: {'rewards_train/chosen': '0.064001', 'rewards_train/rejected': '-0.019209', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08321', 'logps_train/rejected': '-108.42', 'logps_train/chosen': '-163.97', 'loss/train': '0.66912', 'examples_per_second': '30.319', 'grad_norm': '49.25', 'counters/examples': 157792, 'counters/updates': 4931}
train stats after 157824 examples: {'rewards_train/chosen': '0.12066', 'rewards_train/rejected': '0.14408', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.023422', 'logps_train/rejected': '-146.84', 'logps_train/chosen': '-168.49', 'loss/train': '0.71508', 'examples_per_second': '31.382', 'grad_norm': '34', 'counters/examples': 157824, 'counters/updates': 4932}
train stats after 157856 examples: {'rewards_train/chosen': '0.097435', 'rewards_train/rejected': '0.077185', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02025', 'logps_train/rejected': '-117.67', 'logps_train/chosen': '-117.82', 'loss/train': '0.69499', 'examples_per_second': '31.955', 'grad_norm': '43.25', 'counters/examples': 157856, 'counters/updates': 4933}
train stats after 157888 examples: {'rewards_train/chosen': '0.12294', 'rewards_train/rejected': '0.034534', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088402', 'logps_train/rejected': '-148.62', 'logps_train/chosen': '-146.33', 'loss/train': '0.6622', 'examples_per_second': '31.525', 'grad_norm': '31.5', 'counters/examples': 157888, 'counters/updates': 4934}
skipping logging after 157920 examples to avoid logging too frequently
train stats after 157952 examples: {'rewards_train/chosen': '0.13523', 'rewards_train/rejected': '-0.0082699', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1435', 'logps_train/rejected': '-111.24', 'logps_train/chosen': '-138.87', 'loss/train': '0.63447', 'examples_per_second': '31.796', 'grad_norm': '28.875', 'counters/examples': 157952, 'counters/updates': 4936}
train stats after 157984 examples: {'rewards_train/chosen': '0.12726', 'rewards_train/rejected': '0.075496', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051767', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-145.51', 'loss/train': '0.68671', 'examples_per_second': '30.487', 'grad_norm': '33.25', 'counters/examples': 157984, 'counters/updates': 4937}
train stats after 158016 examples: {'rewards_train/chosen': '0.10508', 'rewards_train/rejected': '0.010297', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094778', 'logps_train/rejected': '-142.41', 'logps_train/chosen': '-188.47', 'loss/train': '0.65597', 'examples_per_second': '30.034', 'grad_norm': '39.25', 'counters/examples': 158016, 'counters/updates': 4938}
skipping logging after 158048 examples to avoid logging too frequently
train stats after 158080 examples: {'rewards_train/chosen': '0.12951', 'rewards_train/rejected': '0.038367', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.09114', 'logps_train/rejected': '-110.88', 'logps_train/chosen': '-154.22', 'loss/train': '0.6805', 'examples_per_second': '33.301', 'grad_norm': '35', 'counters/examples': 158080, 'counters/updates': 4940}
train stats after 158112 examples: {'rewards_train/chosen': '0.11995', 'rewards_train/rejected': '-0.037154', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1571', 'logps_train/rejected': '-123.32', 'logps_train/chosen': '-126.15', 'loss/train': '0.62895', 'examples_per_second': '30.936', 'grad_norm': '38.5', 'counters/examples': 158112, 'counters/updates': 4941}
skipping logging after 158144 examples to avoid logging too frequently
train stats after 158176 examples: {'rewards_train/chosen': '0.12106', 'rewards_train/rejected': '0.13751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.016451', 'logps_train/rejected': '-135.94', 'logps_train/chosen': '-121.19', 'loss/train': '0.70741', 'examples_per_second': '32.952', 'grad_norm': '41.75', 'counters/examples': 158176, 'counters/updates': 4943}
train stats after 158208 examples: {'rewards_train/chosen': '0.21665', 'rewards_train/rejected': '0.10958', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10708', 'logps_train/rejected': '-165.14', 'logps_train/chosen': '-176.62', 'loss/train': '0.65531', 'examples_per_second': '30.991', 'grad_norm': '40', 'counters/examples': 158208, 'counters/updates': 4944}
train stats after 158240 examples: {'rewards_train/chosen': '0.19814', 'rewards_train/rejected': '0.0090494', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1891', 'logps_train/rejected': '-140.58', 'logps_train/chosen': '-185.19', 'loss/train': '0.61235', 'examples_per_second': '30.625', 'grad_norm': '33.75', 'counters/examples': 158240, 'counters/updates': 4945}
skipping logging after 158272 examples to avoid logging too frequently
train stats after 158304 examples: {'rewards_train/chosen': '0.15021', 'rewards_train/rejected': '0.0001078', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1501', 'logps_train/rejected': '-107.71', 'logps_train/chosen': '-134.47', 'loss/train': '0.63363', 'examples_per_second': '30.422', 'grad_norm': '29', 'counters/examples': 158304, 'counters/updates': 4947}
train stats after 158336 examples: {'rewards_train/chosen': '0.20764', 'rewards_train/rejected': '0.118', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089633', 'logps_train/rejected': '-96.127', 'logps_train/chosen': '-135.22', 'loss/train': '0.65841', 'examples_per_second': '32.059', 'grad_norm': '26.75', 'counters/examples': 158336, 'counters/updates': 4948}
train stats after 158368 examples: {'rewards_train/chosen': '0.17385', 'rewards_train/rejected': '0.060831', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11302', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-162.63', 'loss/train': '0.64971', 'examples_per_second': '30.635', 'grad_norm': '41.5', 'counters/examples': 158368, 'counters/updates': 4949}
train stats after 158400 examples: {'rewards_train/chosen': '0.20881', 'rewards_train/rejected': '0.073791', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13502', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-153.04', 'loss/train': '0.64119', 'examples_per_second': '30.497', 'grad_norm': '33.75', 'counters/examples': 158400, 'counters/updates': 4950}
train stats after 158432 examples: {'rewards_train/chosen': '0.14634', 'rewards_train/rejected': '0.056056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.090281', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-153.33', 'loss/train': '0.65665', 'examples_per_second': '31.464', 'grad_norm': '32.75', 'counters/examples': 158432, 'counters/updates': 4951}
train stats after 158464 examples: {'rewards_train/chosen': '0.11932', 'rewards_train/rejected': '0.13144', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012125', 'logps_train/rejected': '-158.83', 'logps_train/chosen': '-176.64', 'loss/train': '0.71142', 'examples_per_second': '30.723', 'grad_norm': '38', 'counters/examples': 158464, 'counters/updates': 4952}
train stats after 158496 examples: {'rewards_train/chosen': '0.10672', 'rewards_train/rejected': '0.043478', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06324', 'logps_train/rejected': '-149.85', 'logps_train/chosen': '-168.33', 'loss/train': '0.67242', 'examples_per_second': '30.497', 'grad_norm': '40.5', 'counters/examples': 158496, 'counters/updates': 4953}
train stats after 158528 examples: {'rewards_train/chosen': '0.14941', 'rewards_train/rejected': '0.060478', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088932', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-168.65', 'loss/train': '0.66203', 'examples_per_second': '31.621', 'grad_norm': '31.25', 'counters/examples': 158528, 'counters/updates': 4954}
train stats after 158560 examples: {'rewards_train/chosen': '0.14552', 'rewards_train/rejected': '0.058271', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087244', 'logps_train/rejected': '-96.3', 'logps_train/chosen': '-136.74', 'loss/train': '0.65804', 'examples_per_second': '31.085', 'grad_norm': '29.25', 'counters/examples': 158560, 'counters/updates': 4955}
skipping logging after 158592 examples to avoid logging too frequently
train stats after 158624 examples: {'rewards_train/chosen': '0.17073', 'rewards_train/rejected': '0.079206', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.091527', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-139.24', 'loss/train': '0.65924', 'examples_per_second': '31.177', 'grad_norm': '44.5', 'counters/examples': 158624, 'counters/updates': 4957}
train stats after 158656 examples: {'rewards_train/chosen': '0.098341', 'rewards_train/rejected': '0.045112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053229', 'logps_train/rejected': '-110.05', 'logps_train/chosen': '-109.39', 'loss/train': '0.6782', 'examples_per_second': '30.616', 'grad_norm': '23.125', 'counters/examples': 158656, 'counters/updates': 4958}
train stats after 158688 examples: {'rewards_train/chosen': '0.16153', 'rewards_train/rejected': '-0.019481', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18101', 'logps_train/rejected': '-112.17', 'logps_train/chosen': '-131.88', 'loss/train': '0.62339', 'examples_per_second': '30.811', 'grad_norm': '24.25', 'counters/examples': 158688, 'counters/updates': 4959}
train stats after 158720 examples: {'rewards_train/chosen': '0.13101', 'rewards_train/rejected': '0.045361', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08565', 'logps_train/rejected': '-162.71', 'logps_train/chosen': '-193.91', 'loss/train': '0.66724', 'examples_per_second': '30.37', 'grad_norm': '52', 'counters/examples': 158720, 'counters/updates': 4960}
train stats after 158752 examples: {'rewards_train/chosen': '0.20264', 'rewards_train/rejected': '0.098436', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10421', 'logps_train/rejected': '-136.87', 'logps_train/chosen': '-176.87', 'loss/train': '0.65176', 'examples_per_second': '31.229', 'grad_norm': '40.25', 'counters/examples': 158752, 'counters/updates': 4961}
train stats after 158784 examples: {'rewards_train/chosen': '0.24392', 'rewards_train/rejected': '0.25269', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0087726', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-147.08', 'loss/train': '0.72179', 'examples_per_second': '30.13', 'grad_norm': '43.75', 'counters/examples': 158784, 'counters/updates': 4962}
train stats after 158816 examples: {'rewards_train/chosen': '0.10862', 'rewards_train/rejected': '0.11523', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0066119', 'logps_train/rejected': '-122.98', 'logps_train/chosen': '-160.49', 'loss/train': '0.70815', 'examples_per_second': '31.576', 'grad_norm': '38.75', 'counters/examples': 158816, 'counters/updates': 4963}
skipping logging after 158848 examples to avoid logging too frequently
train stats after 158880 examples: {'rewards_train/chosen': '0.026926', 'rewards_train/rejected': '-0.023547', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050473', 'logps_train/rejected': '-135.4', 'logps_train/chosen': '-146.84', 'loss/train': '0.67706', 'examples_per_second': '31.396', 'grad_norm': '31.125', 'counters/examples': 158880, 'counters/updates': 4965}
train stats after 158912 examples: {'rewards_train/chosen': '0.19087', 'rewards_train/rejected': '0.023072', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16779', 'logps_train/rejected': '-122.31', 'logps_train/chosen': '-143.87', 'loss/train': '0.62651', 'examples_per_second': '30.673', 'grad_norm': '26.25', 'counters/examples': 158912, 'counters/updates': 4966}
train stats after 158944 examples: {'rewards_train/chosen': '0.13637', 'rewards_train/rejected': '0.057604', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078767', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-148.09', 'loss/train': '0.66209', 'examples_per_second': '31.325', 'grad_norm': '30.625', 'counters/examples': 158944, 'counters/updates': 4967}
train stats after 158976 examples: {'rewards_train/chosen': '0.029102', 'rewards_train/rejected': '0.07362', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.044517', 'logps_train/rejected': '-148.77', 'logps_train/chosen': '-134.4', 'loss/train': '0.72252', 'examples_per_second': '31.681', 'grad_norm': '30.875', 'counters/examples': 158976, 'counters/updates': 4968}
train stats after 159008 examples: {'rewards_train/chosen': '0.14328', 'rewards_train/rejected': '0.08717', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056114', 'logps_train/rejected': '-143.06', 'logps_train/chosen': '-177.38', 'loss/train': '0.68143', 'examples_per_second': '30.319', 'grad_norm': '31.75', 'counters/examples': 159008, 'counters/updates': 4969}
skipping logging after 159040 examples to avoid logging too frequently
train stats after 159072 examples: {'rewards_train/chosen': '0.04001', 'rewards_train/rejected': '0.065693', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.025683', 'logps_train/rejected': '-103.96', 'logps_train/chosen': '-127.93', 'loss/train': '0.71783', 'examples_per_second': '31.33', 'grad_norm': '36', 'counters/examples': 159072, 'counters/updates': 4971}
train stats after 159104 examples: {'rewards_train/chosen': '0.099477', 'rewards_train/rejected': '0.04697', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052507', 'logps_train/rejected': '-135.65', 'logps_train/chosen': '-145.28', 'loss/train': '0.67715', 'examples_per_second': '31.415', 'grad_norm': '28.875', 'counters/examples': 159104, 'counters/updates': 4972}
train stats after 159136 examples: {'rewards_train/chosen': '0.14471', 'rewards_train/rejected': '0.043194', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10152', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-157.47', 'loss/train': '0.65501', 'examples_per_second': '29.714', 'grad_norm': '38.5', 'counters/examples': 159136, 'counters/updates': 4973}
skipping logging after 159168 examples to avoid logging too frequently
train stats after 159200 examples: {'rewards_train/chosen': '0.22379', 'rewards_train/rejected': '0.131', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092787', 'logps_train/rejected': '-149.62', 'logps_train/chosen': '-160.59', 'loss/train': '0.65849', 'examples_per_second': '31.248', 'grad_norm': '29', 'counters/examples': 159200, 'counters/updates': 4975}
train stats after 159232 examples: {'rewards_train/chosen': '0.14489', 'rewards_train/rejected': '0.049885', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095009', 'logps_train/rejected': '-152.45', 'logps_train/chosen': '-194.41', 'loss/train': '0.65745', 'examples_per_second': '33.381', 'grad_norm': '37.5', 'counters/examples': 159232, 'counters/updates': 4976}
skipping logging after 159264 examples to avoid logging too frequently
train stats after 159296 examples: {'rewards_train/chosen': '0.24193', 'rewards_train/rejected': '0.12849', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11343', 'logps_train/rejected': '-104.82', 'logps_train/chosen': '-150.54', 'loss/train': '0.64947', 'examples_per_second': '31.553', 'grad_norm': '31.75', 'counters/examples': 159296, 'counters/updates': 4978}
skipping logging after 159328 examples to avoid logging too frequently
train stats after 159360 examples: {'rewards_train/chosen': '0.11802', 'rewards_train/rejected': '0.19217', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.074157', 'logps_train/rejected': '-142.6', 'logps_train/chosen': '-140.29', 'loss/train': '0.74528', 'examples_per_second': '33.117', 'grad_norm': '75', 'counters/examples': 159360, 'counters/updates': 4980}
train stats after 159392 examples: {'rewards_train/chosen': '0.098694', 'rewards_train/rejected': '0.013115', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085579', 'logps_train/rejected': '-112.59', 'logps_train/chosen': '-123.81', 'loss/train': '0.65649', 'examples_per_second': '31.171', 'grad_norm': '26.125', 'counters/examples': 159392, 'counters/updates': 4981}
train stats after 159424 examples: {'rewards_train/chosen': '0.045583', 'rewards_train/rejected': '0.072393', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.02681', 'logps_train/rejected': '-123.93', 'logps_train/chosen': '-136.68', 'loss/train': '0.72935', 'examples_per_second': '30.776', 'grad_norm': '51.75', 'counters/examples': 159424, 'counters/updates': 4982}
train stats after 159456 examples: {'rewards_train/chosen': '0.097231', 'rewards_train/rejected': '0.032974', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064256', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-148.85', 'loss/train': '0.66892', 'examples_per_second': '30.219', 'grad_norm': '26.75', 'counters/examples': 159456, 'counters/updates': 4983}
train stats after 159488 examples: {'rewards_train/chosen': '0.083367', 'rewards_train/rejected': '0.098753', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.015386', 'logps_train/rejected': '-122.14', 'logps_train/chosen': '-105.55', 'loss/train': '0.70735', 'examples_per_second': '31.777', 'grad_norm': '37.5', 'counters/examples': 159488, 'counters/updates': 4984}
skipping logging after 159520 examples to avoid logging too frequently
train stats after 159552 examples: {'rewards_train/chosen': '0.16229', 'rewards_train/rejected': '0.079479', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082811', 'logps_train/rejected': '-152.4', 'logps_train/chosen': '-175.25', 'loss/train': '0.66098', 'examples_per_second': '30.337', 'grad_norm': '40.25', 'counters/examples': 159552, 'counters/updates': 4986}
train stats after 159584 examples: {'rewards_train/chosen': '0.12622', 'rewards_train/rejected': '0.015594', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11063', 'logps_train/rejected': '-117.02', 'logps_train/chosen': '-137.65', 'loss/train': '0.65098', 'examples_per_second': '31.125', 'grad_norm': '34', 'counters/examples': 159584, 'counters/updates': 4987}
train stats after 159616 examples: {'rewards_train/chosen': '0.099469', 'rewards_train/rejected': '-0.0059341', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1054', 'logps_train/rejected': '-135.29', 'logps_train/chosen': '-151.98', 'loss/train': '0.64968', 'examples_per_second': '30.27', 'grad_norm': '30.25', 'counters/examples': 159616, 'counters/updates': 4988}
train stats after 159648 examples: {'rewards_train/chosen': '0.17056', 'rewards_train/rejected': '0.13054', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040026', 'logps_train/rejected': '-147.95', 'logps_train/chosen': '-137.68', 'loss/train': '0.68212', 'examples_per_second': '29.81', 'grad_norm': '41.75', 'counters/examples': 159648, 'counters/updates': 4989}
train stats after 159680 examples: {'rewards_train/chosen': '0.10221', 'rewards_train/rejected': '0.018406', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.083809', 'logps_train/rejected': '-127.99', 'logps_train/chosen': '-133.26', 'loss/train': '0.66203', 'examples_per_second': '31.354', 'grad_norm': '32.25', 'counters/examples': 159680, 'counters/updates': 4990}
skipping logging after 159712 examples to avoid logging too frequently
train stats after 159744 examples: {'rewards_train/chosen': '0.12364', 'rewards_train/rejected': '0.14411', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.020476', 'logps_train/rejected': '-109.62', 'logps_train/chosen': '-142.12', 'loss/train': '0.70871', 'examples_per_second': '31.601', 'grad_norm': '28', 'counters/examples': 159744, 'counters/updates': 4992}
train stats after 159776 examples: {'rewards_train/chosen': '0.12873', 'rewards_train/rejected': '0.11693', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.011801', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-155.81', 'loss/train': '0.69925', 'examples_per_second': '32.299', 'grad_norm': '34', 'counters/examples': 159776, 'counters/updates': 4993}
train stats after 159808 examples: {'rewards_train/chosen': '0.13292', 'rewards_train/rejected': '-0.0067978', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13972', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-136.72', 'loss/train': '0.64804', 'examples_per_second': '31.382', 'grad_norm': '32', 'counters/examples': 159808, 'counters/updates': 4994}
train stats after 159840 examples: {'rewards_train/chosen': '0.11596', 'rewards_train/rejected': '-0.026493', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14246', 'logps_train/rejected': '-118.59', 'logps_train/chosen': '-167.28', 'loss/train': '0.6369', 'examples_per_second': '31.345', 'grad_norm': '31', 'counters/examples': 159840, 'counters/updates': 4995}
train stats after 159872 examples: {'rewards_train/chosen': '0.070969', 'rewards_train/rejected': '0.025002', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.045967', 'logps_train/rejected': '-150.54', 'logps_train/chosen': '-149.45', 'loss/train': '0.69256', 'examples_per_second': '31.462', 'grad_norm': '37', 'counters/examples': 159872, 'counters/updates': 4996}
train stats after 159904 examples: {'rewards_train/chosen': '0.097038', 'rewards_train/rejected': '0.03239', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064649', 'logps_train/rejected': '-141.39', 'logps_train/chosen': '-108.05', 'loss/train': '0.66994', 'examples_per_second': '32.221', 'grad_norm': '27.125', 'counters/examples': 159904, 'counters/updates': 4997}
train stats after 159936 examples: {'rewards_train/chosen': '0.14308', 'rewards_train/rejected': '0.10477', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038314', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-135.2', 'loss/train': '0.68589', 'examples_per_second': '29.875', 'grad_norm': '27.5', 'counters/examples': 159936, 'counters/updates': 4998}
train stats after 159968 examples: {'rewards_train/chosen': '0.095727', 'rewards_train/rejected': '0.036549', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059178', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-118.22', 'loss/train': '0.66934', 'examples_per_second': '30.709', 'grad_norm': '25.625', 'counters/examples': 159968, 'counters/updates': 4999}
train stats after 160000 examples: {'rewards_train/chosen': '0.13709', 'rewards_train/rejected': '0.10408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.033018', 'logps_train/rejected': '-165.52', 'logps_train/chosen': '-168.26', 'loss/train': '0.68957', 'examples_per_second': '32.976', 'grad_norm': '34.5', 'counters/examples': 160000, 'counters/updates': 5000}
train stats after 160032 examples: {'rewards_train/chosen': '0.077329', 'rewards_train/rejected': '-0.014125', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091454', 'logps_train/rejected': '-145.13', 'logps_train/chosen': '-149.02', 'loss/train': '0.65979', 'examples_per_second': '31.369', 'grad_norm': '41', 'counters/examples': 160032, 'counters/updates': 5001}
train stats after 160064 examples: {'rewards_train/chosen': '0.17678', 'rewards_train/rejected': '0.064443', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11234', 'logps_train/rejected': '-143.95', 'logps_train/chosen': '-169.79', 'loss/train': '0.65397', 'examples_per_second': '31.342', 'grad_norm': '42.75', 'counters/examples': 160064, 'counters/updates': 5002}
skipping logging after 160096 examples to avoid logging too frequently
train stats after 160128 examples: {'rewards_train/chosen': '0.085651', 'rewards_train/rejected': '0.0030471', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082604', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-115.93', 'loss/train': '0.66116', 'examples_per_second': '29.887', 'grad_norm': '30.875', 'counters/examples': 160128, 'counters/updates': 5004}
train stats after 160160 examples: {'rewards_train/chosen': '0.1045', 'rewards_train/rejected': '0.10547', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00096558', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-191.06', 'loss/train': '0.71242', 'examples_per_second': '31.381', 'grad_norm': '36.75', 'counters/examples': 160160, 'counters/updates': 5005}
train stats after 160192 examples: {'rewards_train/chosen': '0.16165', 'rewards_train/rejected': '0.074566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087084', 'logps_train/rejected': '-138.42', 'logps_train/chosen': '-162.08', 'loss/train': '0.66466', 'examples_per_second': '31.103', 'grad_norm': '34.5', 'counters/examples': 160192, 'counters/updates': 5006}
train stats after 160224 examples: {'rewards_train/chosen': '0.15192', 'rewards_train/rejected': '0.096502', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05542', 'logps_train/rejected': '-140.55', 'logps_train/chosen': '-162.75', 'loss/train': '0.68688', 'examples_per_second': '32.088', 'grad_norm': '52', 'counters/examples': 160224, 'counters/updates': 5007}
train stats after 160256 examples: {'rewards_train/chosen': '0.16627', 'rewards_train/rejected': '0.057545', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10872', 'logps_train/rejected': '-140.11', 'logps_train/chosen': '-195.22', 'loss/train': '0.65459', 'examples_per_second': '31.911', 'grad_norm': '31.875', 'counters/examples': 160256, 'counters/updates': 5008}
train stats after 160288 examples: {'rewards_train/chosen': '0.16843', 'rewards_train/rejected': '0.039249', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12918', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-144.12', 'loss/train': '0.64409', 'examples_per_second': '32.257', 'grad_norm': '39', 'counters/examples': 160288, 'counters/updates': 5009}
train stats after 160320 examples: {'rewards_train/chosen': '0.16697', 'rewards_train/rejected': '0.015171', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1518', 'logps_train/rejected': '-117.6', 'logps_train/chosen': '-156.99', 'loss/train': '0.63312', 'examples_per_second': '31.079', 'grad_norm': '28.625', 'counters/examples': 160320, 'counters/updates': 5010}
train stats after 160352 examples: {'rewards_train/chosen': '0.06378', 'rewards_train/rejected': '0.16134', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.097557', 'logps_train/rejected': '-158.78', 'logps_train/chosen': '-169.05', 'loss/train': '0.77713', 'examples_per_second': '24.243', 'grad_norm': '69.5', 'counters/examples': 160352, 'counters/updates': 5011}
train stats after 160384 examples: {'rewards_train/chosen': '0.13297', 'rewards_train/rejected': '0.016617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11635', 'logps_train/rejected': '-102.83', 'logps_train/chosen': '-155.42', 'loss/train': '0.65048', 'examples_per_second': '29.789', 'grad_norm': '32.5', 'counters/examples': 160384, 'counters/updates': 5012}
train stats after 160416 examples: {'rewards_train/chosen': '0.13326', 'rewards_train/rejected': '0.046698', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086563', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-146.11', 'loss/train': '0.65922', 'examples_per_second': '32.233', 'grad_norm': '41', 'counters/examples': 160416, 'counters/updates': 5013}
skipping logging after 160448 examples to avoid logging too frequently
train stats after 160480 examples: {'rewards_train/chosen': '0.12405', 'rewards_train/rejected': '0.062952', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.0611', 'logps_train/rejected': '-120.78', 'logps_train/chosen': '-157.47', 'loss/train': '0.68119', 'examples_per_second': '30.709', 'grad_norm': '38', 'counters/examples': 160480, 'counters/updates': 5015}
train stats after 160512 examples: {'rewards_train/chosen': '0.075773', 'rewards_train/rejected': '0.053211', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022562', 'logps_train/rejected': '-87.935', 'logps_train/chosen': '-128.92', 'loss/train': '0.68843', 'examples_per_second': '31.259', 'grad_norm': '27.125', 'counters/examples': 160512, 'counters/updates': 5016}
train stats after 160544 examples: {'rewards_train/chosen': '0.17888', 'rewards_train/rejected': '0.066954', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11193', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-141.39', 'loss/train': '0.64847', 'examples_per_second': '31.326', 'grad_norm': '25.75', 'counters/examples': 160544, 'counters/updates': 5017}
skipping logging after 160576 examples to avoid logging too frequently
train stats after 160608 examples: {'rewards_train/chosen': '0.13452', 'rewards_train/rejected': '0.068174', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.066341', 'logps_train/rejected': '-128.36', 'logps_train/chosen': '-123.99', 'loss/train': '0.66977', 'examples_per_second': '31.335', 'grad_norm': '34.75', 'counters/examples': 160608, 'counters/updates': 5019}
train stats after 160640 examples: {'rewards_train/chosen': '0.17966', 'rewards_train/rejected': '0.10814', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071519', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-149.05', 'loss/train': '0.66866', 'examples_per_second': '31.057', 'grad_norm': '42.75', 'counters/examples': 160640, 'counters/updates': 5020}
train stats after 160672 examples: {'rewards_train/chosen': '0.1136', 'rewards_train/rejected': '0.036748', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076856', 'logps_train/rejected': '-137.14', 'logps_train/chosen': '-120.19', 'loss/train': '0.66642', 'examples_per_second': '31.385', 'grad_norm': '38.75', 'counters/examples': 160672, 'counters/updates': 5021}
train stats after 160704 examples: {'rewards_train/chosen': '0.13796', 'rewards_train/rejected': '0.026523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11144', 'logps_train/rejected': '-158.2', 'logps_train/chosen': '-148.98', 'loss/train': '0.6547', 'examples_per_second': '30.973', 'grad_norm': '29.25', 'counters/examples': 160704, 'counters/updates': 5022}
train stats after 160736 examples: {'rewards_train/chosen': '0.14407', 'rewards_train/rejected': '0.03328', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11079', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-117.76', 'loss/train': '0.66271', 'examples_per_second': '30.726', 'grad_norm': '31.875', 'counters/examples': 160736, 'counters/updates': 5023}
train stats after 160768 examples: {'rewards_train/chosen': '0.030597', 'rewards_train/rejected': '-0.047113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07771', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-119.14', 'loss/train': '0.66502', 'examples_per_second': '29.954', 'grad_norm': '25.5', 'counters/examples': 160768, 'counters/updates': 5024}
train stats after 160800 examples: {'rewards_train/chosen': '0.15165', 'rewards_train/rejected': '0.046055', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1056', 'logps_train/rejected': '-122.19', 'logps_train/chosen': '-115.67', 'loss/train': '0.65132', 'examples_per_second': '32.576', 'grad_norm': '31', 'counters/examples': 160800, 'counters/updates': 5025}
skipping logging after 160832 examples to avoid logging too frequently
train stats after 160864 examples: {'rewards_train/chosen': '0.10094', 'rewards_train/rejected': '0.0015316', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09941', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-175.3', 'loss/train': '0.66099', 'examples_per_second': '34.002', 'grad_norm': '31.875', 'counters/examples': 160864, 'counters/updates': 5027}
train stats after 160896 examples: {'rewards_train/chosen': '0.17851', 'rewards_train/rejected': '0.12382', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.054697', 'logps_train/rejected': '-148.1', 'logps_train/chosen': '-154.35', 'loss/train': '0.70158', 'examples_per_second': '31.18', 'grad_norm': '32.5', 'counters/examples': 160896, 'counters/updates': 5028}
train stats after 160928 examples: {'rewards_train/chosen': '0.07661', 'rewards_train/rejected': '0.09171', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '-0.015101', 'logps_train/rejected': '-128.31', 'logps_train/chosen': '-122.4', 'loss/train': '0.72387', 'examples_per_second': '31.691', 'grad_norm': '66.5', 'counters/examples': 160928, 'counters/updates': 5029}
skipping logging after 160960 examples to avoid logging too frequently
train stats after 160992 examples: {'rewards_train/chosen': '0.15763', 'rewards_train/rejected': '0.068926', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088706', 'logps_train/rejected': '-148.86', 'logps_train/chosen': '-169.6', 'loss/train': '0.66568', 'examples_per_second': '32.275', 'grad_norm': '34', 'counters/examples': 160992, 'counters/updates': 5031}
train stats after 161024 examples: {'rewards_train/chosen': '0.090113', 'rewards_train/rejected': '0.002135', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.087978', 'logps_train/rejected': '-141.33', 'logps_train/chosen': '-171.08', 'loss/train': '0.66071', 'examples_per_second': '31.328', 'grad_norm': '30.375', 'counters/examples': 161024, 'counters/updates': 5032}
train stats after 161056 examples: {'rewards_train/chosen': '0.10269', 'rewards_train/rejected': '-0.00492', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10761', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-142.01', 'loss/train': '0.65293', 'examples_per_second': '30.489', 'grad_norm': '29', 'counters/examples': 161056, 'counters/updates': 5033}
train stats after 161088 examples: {'rewards_train/chosen': '0.16562', 'rewards_train/rejected': '0.036682', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.12894', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-141.98', 'loss/train': '0.6371', 'examples_per_second': '32.264', 'grad_norm': '34.75', 'counters/examples': 161088, 'counters/updates': 5034}
train stats after 161120 examples: {'rewards_train/chosen': '0.11067', 'rewards_train/rejected': '0.074837', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035828', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-155.5', 'loss/train': '0.68055', 'examples_per_second': '30.445', 'grad_norm': '28.25', 'counters/examples': 161120, 'counters/updates': 5035}
train stats after 161152 examples: {'rewards_train/chosen': '0.085996', 'rewards_train/rejected': '-0.00040748', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086404', 'logps_train/rejected': '-130.85', 'logps_train/chosen': '-166.37', 'loss/train': '0.66198', 'examples_per_second': '31.537', 'grad_norm': '27.25', 'counters/examples': 161152, 'counters/updates': 5036}
skipping logging after 161184 examples to avoid logging too frequently
train stats after 161216 examples: {'rewards_train/chosen': '0.19245', 'rewards_train/rejected': '0.035279', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15717', 'logps_train/rejected': '-110.09', 'logps_train/chosen': '-125.73', 'loss/train': '0.63262', 'examples_per_second': '32.339', 'grad_norm': '32.5', 'counters/examples': 161216, 'counters/updates': 5038}
train stats after 161248 examples: {'rewards_train/chosen': '0.11818', 'rewards_train/rejected': '0.08589', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032289', 'logps_train/rejected': '-130.19', 'logps_train/chosen': '-128.68', 'loss/train': '0.68173', 'examples_per_second': '31.317', 'grad_norm': '27.25', 'counters/examples': 161248, 'counters/updates': 5039}
train stats after 161280 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '0.049689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097141', 'logps_train/rejected': '-111.88', 'logps_train/chosen': '-105.34', 'loss/train': '0.65349', 'examples_per_second': '31.675', 'grad_norm': '26.875', 'counters/examples': 161280, 'counters/updates': 5040}
train stats after 161312 examples: {'rewards_train/chosen': '0.11808', 'rewards_train/rejected': '0.10008', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017999', 'logps_train/rejected': '-105.98', 'logps_train/chosen': '-120.86', 'loss/train': '0.69423', 'examples_per_second': '30.345', 'grad_norm': '33.75', 'counters/examples': 161312, 'counters/updates': 5041}
train stats after 161344 examples: {'rewards_train/chosen': '0.056007', 'rewards_train/rejected': '-0.014914', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070921', 'logps_train/rejected': '-105.17', 'logps_train/chosen': '-145.73', 'loss/train': '0.66627', 'examples_per_second': '30.575', 'grad_norm': '32', 'counters/examples': 161344, 'counters/updates': 5042}
train stats after 161376 examples: {'rewards_train/chosen': '0.10168', 'rewards_train/rejected': '0.064512', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037172', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-156.81', 'loss/train': '0.68635', 'examples_per_second': '32.706', 'grad_norm': '44.5', 'counters/examples': 161376, 'counters/updates': 5043}
train stats after 161408 examples: {'rewards_train/chosen': '0.08676', 'rewards_train/rejected': '-0.025998', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11276', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-126.9', 'loss/train': '0.65658', 'examples_per_second': '32.181', 'grad_norm': '37.5', 'counters/examples': 161408, 'counters/updates': 5044}
train stats after 161440 examples: {'rewards_train/chosen': '0.12558', 'rewards_train/rejected': '0.10176', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023824', 'logps_train/rejected': '-141.79', 'logps_train/chosen': '-168.49', 'loss/train': '0.69298', 'examples_per_second': '31.165', 'grad_norm': '28.25', 'counters/examples': 161440, 'counters/updates': 5045}
train stats after 161472 examples: {'rewards_train/chosen': '0.14162', 'rewards_train/rejected': '0.010303', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13131', 'logps_train/rejected': '-150.94', 'logps_train/chosen': '-161.46', 'loss/train': '0.64544', 'examples_per_second': '31.391', 'grad_norm': '29.75', 'counters/examples': 161472, 'counters/updates': 5046}
train stats after 161504 examples: {'rewards_train/chosen': '0.22548', 'rewards_train/rejected': '0.045241', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18023', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-140.2', 'loss/train': '0.6295', 'examples_per_second': '33.008', 'grad_norm': '25.625', 'counters/examples': 161504, 'counters/updates': 5047}
train stats after 161536 examples: {'rewards_train/chosen': '0.15996', 'rewards_train/rejected': '0.13574', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024225', 'logps_train/rejected': '-116.9', 'logps_train/chosen': '-133.76', 'loss/train': '0.69291', 'examples_per_second': '31.767', 'grad_norm': '33', 'counters/examples': 161536, 'counters/updates': 5048}
train stats after 161568 examples: {'rewards_train/chosen': '0.083386', 'rewards_train/rejected': '0.012261', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.071126', 'logps_train/rejected': '-141.99', 'logps_train/chosen': '-152.63', 'loss/train': '0.66678', 'examples_per_second': '31.387', 'grad_norm': '55.75', 'counters/examples': 161568, 'counters/updates': 5049}
train stats after 161600 examples: {'rewards_train/chosen': '0.14979', 'rewards_train/rejected': '0.0196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13019', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-115.12', 'loss/train': '0.64008', 'examples_per_second': '30.436', 'grad_norm': '37.25', 'counters/examples': 161600, 'counters/updates': 5050}
train stats after 161632 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.044071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070785', 'logps_train/rejected': '-129.57', 'logps_train/chosen': '-156.89', 'loss/train': '0.66849', 'examples_per_second': '30.957', 'grad_norm': '29.25', 'counters/examples': 161632, 'counters/updates': 5051}
train stats after 161664 examples: {'rewards_train/chosen': '0.1025', 'rewards_train/rejected': '0.036244', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066256', 'logps_train/rejected': '-110.67', 'logps_train/chosen': '-145.06', 'loss/train': '0.66909', 'examples_per_second': '30.865', 'grad_norm': '29.625', 'counters/examples': 161664, 'counters/updates': 5052}
train stats after 161696 examples: {'rewards_train/chosen': '0.26272', 'rewards_train/rejected': '0.086931', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17579', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-162.76', 'loss/train': '0.63148', 'examples_per_second': '29.746', 'grad_norm': '40', 'counters/examples': 161696, 'counters/updates': 5053}
train stats after 161728 examples: {'rewards_train/chosen': '0.14321', 'rewards_train/rejected': '0.071461', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.07175', 'logps_train/rejected': '-108.1', 'logps_train/chosen': '-159.04', 'loss/train': '0.66822', 'examples_per_second': '31.309', 'grad_norm': '26.625', 'counters/examples': 161728, 'counters/updates': 5054}
train stats after 161760 examples: {'rewards_train/chosen': '0.096542', 'rewards_train/rejected': '-0.013249', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10979', 'logps_train/rejected': '-154.55', 'logps_train/chosen': '-177.86', 'loss/train': '0.66369', 'examples_per_second': '31.399', 'grad_norm': '37.25', 'counters/examples': 161760, 'counters/updates': 5055}
skipping logging after 161792 examples to avoid logging too frequently
train stats after 161824 examples: {'rewards_train/chosen': '0.2426', 'rewards_train/rejected': '0.058922', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18367', 'logps_train/rejected': '-137.04', 'logps_train/chosen': '-139.82', 'loss/train': '0.6201', 'examples_per_second': '35.322', 'grad_norm': '30.75', 'counters/examples': 161824, 'counters/updates': 5057}
train stats after 161856 examples: {'rewards_train/chosen': '0.14534', 'rewards_train/rejected': '0.11407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031271', 'logps_train/rejected': '-146.71', 'logps_train/chosen': '-134.03', 'loss/train': '0.69148', 'examples_per_second': '29.905', 'grad_norm': '44.25', 'counters/examples': 161856, 'counters/updates': 5058}
train stats after 161888 examples: {'rewards_train/chosen': '0.14712', 'rewards_train/rejected': '0.038339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10878', 'logps_train/rejected': '-115.96', 'logps_train/chosen': '-148.69', 'loss/train': '0.65376', 'examples_per_second': '31.473', 'grad_norm': '29.5', 'counters/examples': 161888, 'counters/updates': 5059}
skipping logging after 161920 examples to avoid logging too frequently
train stats after 161952 examples: {'rewards_train/chosen': '0.1341', 'rewards_train/rejected': '0.058803', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075294', 'logps_train/rejected': '-132.89', 'logps_train/chosen': '-127.68', 'loss/train': '0.66155', 'examples_per_second': '31.373', 'grad_norm': '35.5', 'counters/examples': 161952, 'counters/updates': 5061}
train stats after 161984 examples: {'rewards_train/chosen': '0.17635', 'rewards_train/rejected': '0.084216', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092137', 'logps_train/rejected': '-134.53', 'logps_train/chosen': '-167.91', 'loss/train': '0.65303', 'examples_per_second': '32.103', 'grad_norm': '37.5', 'counters/examples': 161984, 'counters/updates': 5062}
train stats after 162016 examples: {'rewards_train/chosen': '0.068261', 'rewards_train/rejected': '0.036713', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031548', 'logps_train/rejected': '-119.73', 'logps_train/chosen': '-134.42', 'loss/train': '0.70156', 'examples_per_second': '31.209', 'grad_norm': '76.5', 'counters/examples': 162016, 'counters/updates': 5063}
train stats after 162048 examples: {'rewards_train/chosen': '0.055395', 'rewards_train/rejected': '0.013966', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041429', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-172.33', 'loss/train': '0.68074', 'examples_per_second': '31.139', 'grad_norm': '31.25', 'counters/examples': 162048, 'counters/updates': 5064}
train stats after 162080 examples: {'rewards_train/chosen': '0.13815', 'rewards_train/rejected': '0.022337', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11581', 'logps_train/rejected': '-132.06', 'logps_train/chosen': '-172.92', 'loss/train': '0.65004', 'examples_per_second': '31.386', 'grad_norm': '29.125', 'counters/examples': 162080, 'counters/updates': 5065}
train stats after 162112 examples: {'rewards_train/chosen': '0.12977', 'rewards_train/rejected': '0.065976', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063797', 'logps_train/rejected': '-107.05', 'logps_train/chosen': '-153.09', 'loss/train': '0.6758', 'examples_per_second': '30.348', 'grad_norm': '32.25', 'counters/examples': 162112, 'counters/updates': 5066}
train stats after 162144 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.045956', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055645', 'logps_train/rejected': '-107.69', 'logps_train/chosen': '-128.01', 'loss/train': '0.67665', 'examples_per_second': '32.233', 'grad_norm': '25.875', 'counters/examples': 162144, 'counters/updates': 5067}
train stats after 162176 examples: {'rewards_train/chosen': '0.11715', 'rewards_train/rejected': '0.0070645', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11009', 'logps_train/rejected': '-109.35', 'logps_train/chosen': '-134.83', 'loss/train': '0.64707', 'examples_per_second': '31.404', 'grad_norm': '30.625', 'counters/examples': 162176, 'counters/updates': 5068}
train stats after 162208 examples: {'rewards_train/chosen': '0.037691', 'rewards_train/rejected': '0.072129', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.034438', 'logps_train/rejected': '-137.63', 'logps_train/chosen': '-107.64', 'loss/train': '0.72149', 'examples_per_second': '29.964', 'grad_norm': '35', 'counters/examples': 162208, 'counters/updates': 5069}
skipping logging after 162240 examples to avoid logging too frequently
train stats after 162272 examples: {'rewards_train/chosen': '0.10902', 'rewards_train/rejected': '0.021961', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087056', 'logps_train/rejected': '-108.74', 'logps_train/chosen': '-157.21', 'loss/train': '0.66565', 'examples_per_second': '32.486', 'grad_norm': '32.75', 'counters/examples': 162272, 'counters/updates': 5071}
train stats after 162304 examples: {'rewards_train/chosen': '0.058413', 'rewards_train/rejected': '0.011381', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.047032', 'logps_train/rejected': '-93.207', 'logps_train/chosen': '-130.35', 'loss/train': '0.68121', 'examples_per_second': '30.224', 'grad_norm': '30.5', 'counters/examples': 162304, 'counters/updates': 5072}
train stats after 162336 examples: {'rewards_train/chosen': '0.18402', 'rewards_train/rejected': '0.037096', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14692', 'logps_train/rejected': '-100.98', 'logps_train/chosen': '-153.31', 'loss/train': '0.63445', 'examples_per_second': '32.517', 'grad_norm': '28.75', 'counters/examples': 162336, 'counters/updates': 5073}
skipping logging after 162368 examples to avoid logging too frequently
train stats after 162400 examples: {'rewards_train/chosen': '0.097822', 'rewards_train/rejected': '-0.0022792', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1001', 'logps_train/rejected': '-96.733', 'logps_train/chosen': '-131.46', 'loss/train': '0.65205', 'examples_per_second': '34.293', 'grad_norm': '24.25', 'counters/examples': 162400, 'counters/updates': 5075}
train stats after 162432 examples: {'rewards_train/chosen': '0.11114', 'rewards_train/rejected': '0.086842', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.024297', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-123.11', 'loss/train': '0.69218', 'examples_per_second': '31.888', 'grad_norm': '27.625', 'counters/examples': 162432, 'counters/updates': 5076}
skipping logging after 162464 examples to avoid logging too frequently
train stats after 162496 examples: {'rewards_train/chosen': '0.097869', 'rewards_train/rejected': '0.011544', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086326', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-141.66', 'loss/train': '0.65965', 'examples_per_second': '31.501', 'grad_norm': '27.625', 'counters/examples': 162496, 'counters/updates': 5078}
train stats after 162528 examples: {'rewards_train/chosen': '0.084827', 'rewards_train/rejected': '-0.0007773', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.085604', 'logps_train/rejected': '-132.56', 'logps_train/chosen': '-140.25', 'loss/train': '0.66002', 'examples_per_second': '31.424', 'grad_norm': '34.25', 'counters/examples': 162528, 'counters/updates': 5079}
train stats after 162560 examples: {'rewards_train/chosen': '0.10468', 'rewards_train/rejected': '0.025879', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078798', 'logps_train/rejected': '-145.39', 'logps_train/chosen': '-146.35', 'loss/train': '0.66111', 'examples_per_second': '31.445', 'grad_norm': '28.75', 'counters/examples': 162560, 'counters/updates': 5080}
train stats after 162592 examples: {'rewards_train/chosen': '0.12967', 'rewards_train/rejected': '0.10557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024105', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-157.6', 'loss/train': '0.68821', 'examples_per_second': '31.606', 'grad_norm': '31.25', 'counters/examples': 162592, 'counters/updates': 5081}
skipping logging after 162624 examples to avoid logging too frequently
train stats after 162656 examples: {'rewards_train/chosen': '0.11916', 'rewards_train/rejected': '0.075015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.044146', 'logps_train/rejected': '-147.34', 'logps_train/chosen': '-186.22', 'loss/train': '0.69259', 'examples_per_second': '31.109', 'grad_norm': '41', 'counters/examples': 162656, 'counters/updates': 5083}
train stats after 162688 examples: {'rewards_train/chosen': '0.15586', 'rewards_train/rejected': '0.098101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057762', 'logps_train/rejected': '-142.35', 'logps_train/chosen': '-122.65', 'loss/train': '0.67633', 'examples_per_second': '29.841', 'grad_norm': '38.5', 'counters/examples': 162688, 'counters/updates': 5084}
train stats after 162720 examples: {'rewards_train/chosen': '0.17752', 'rewards_train/rejected': '0.10439', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073132', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-153.44', 'loss/train': '0.7038', 'examples_per_second': '31.153', 'grad_norm': '35.75', 'counters/examples': 162720, 'counters/updates': 5085}
train stats after 162752 examples: {'rewards_train/chosen': '0.056685', 'rewards_train/rejected': '0.10578', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.049093', 'logps_train/rejected': '-116.39', 'logps_train/chosen': '-105.76', 'loss/train': '0.73289', 'examples_per_second': '32.181', 'grad_norm': '53', 'counters/examples': 162752, 'counters/updates': 5086}
train stats after 162784 examples: {'rewards_train/chosen': '0.14677', 'rewards_train/rejected': '0.057148', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089624', 'logps_train/rejected': '-132.27', 'logps_train/chosen': '-198.26', 'loss/train': '0.65921', 'examples_per_second': '30.902', 'grad_norm': '33.5', 'counters/examples': 162784, 'counters/updates': 5087}
train stats after 162816 examples: {'rewards_train/chosen': '0.080304', 'rewards_train/rejected': '0.048819', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031485', 'logps_train/rejected': '-130.55', 'logps_train/chosen': '-130.35', 'loss/train': '0.69247', 'examples_per_second': '31.41', 'grad_norm': '54.5', 'counters/examples': 162816, 'counters/updates': 5088}
train stats after 162848 examples: {'rewards_train/chosen': '0.14205', 'rewards_train/rejected': '0.097581', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.044472', 'logps_train/rejected': '-126.82', 'logps_train/chosen': '-124.74', 'loss/train': '0.68259', 'examples_per_second': '30.51', 'grad_norm': '35', 'counters/examples': 162848, 'counters/updates': 5089}
train stats after 162880 examples: {'rewards_train/chosen': '0.13428', 'rewards_train/rejected': '0.042947', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091328', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-151.88', 'loss/train': '0.66513', 'examples_per_second': '23.3', 'grad_norm': '33.5', 'counters/examples': 162880, 'counters/updates': 5090}
train stats after 162912 examples: {'rewards_train/chosen': '0.085156', 'rewards_train/rejected': '0.037883', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.047273', 'logps_train/rejected': '-137.1', 'logps_train/chosen': '-144.9', 'loss/train': '0.6832', 'examples_per_second': '31.339', 'grad_norm': '34', 'counters/examples': 162912, 'counters/updates': 5091}
skipping logging after 162944 examples to avoid logging too frequently
train stats after 162976 examples: {'rewards_train/chosen': '0.17692', 'rewards_train/rejected': '0.036533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14039', 'logps_train/rejected': '-117.43', 'logps_train/chosen': '-153.12', 'loss/train': '0.63282', 'examples_per_second': '23.408', 'grad_norm': '35.5', 'counters/examples': 162976, 'counters/updates': 5093}
train stats after 163008 examples: {'rewards_train/chosen': '0.19866', 'rewards_train/rejected': '0.097455', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1012', 'logps_train/rejected': '-115.56', 'logps_train/chosen': '-159.84', 'loss/train': '0.65765', 'examples_per_second': '32.379', 'grad_norm': '30.125', 'counters/examples': 163008, 'counters/updates': 5094}
train stats after 163040 examples: {'rewards_train/chosen': '0.12402', 'rewards_train/rejected': '0.0246', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099424', 'logps_train/rejected': '-104.76', 'logps_train/chosen': '-152.52', 'loss/train': '0.66203', 'examples_per_second': '30.93', 'grad_norm': '40.5', 'counters/examples': 163040, 'counters/updates': 5095}
train stats after 163072 examples: {'rewards_train/chosen': '0.12745', 'rewards_train/rejected': '0.03312', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.094331', 'logps_train/rejected': '-125.71', 'logps_train/chosen': '-129.56', 'loss/train': '0.66574', 'examples_per_second': '30.92', 'grad_norm': '32.5', 'counters/examples': 163072, 'counters/updates': 5096}
skipping logging after 163104 examples to avoid logging too frequently
train stats after 163136 examples: {'rewards_train/chosen': '0.10673', 'rewards_train/rejected': '0.10423', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0025052', 'logps_train/rejected': '-161.42', 'logps_train/chosen': '-171.01', 'loss/train': '0.70736', 'examples_per_second': '31.29', 'grad_norm': '41.5', 'counters/examples': 163136, 'counters/updates': 5098}
skipping logging after 163168 examples to avoid logging too frequently
train stats after 163200 examples: {'rewards_train/chosen': '0.15222', 'rewards_train/rejected': '0.063071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089151', 'logps_train/rejected': '-134.96', 'logps_train/chosen': '-147.16', 'loss/train': '0.6636', 'examples_per_second': '29.939', 'grad_norm': '32.75', 'counters/examples': 163200, 'counters/updates': 5100}
train stats after 163232 examples: {'rewards_train/chosen': '0.1503', 'rewards_train/rejected': '0.049572', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10073', 'logps_train/rejected': '-136.3', 'logps_train/chosen': '-174.14', 'loss/train': '0.66415', 'examples_per_second': '30.158', 'grad_norm': '37.25', 'counters/examples': 163232, 'counters/updates': 5101}
skipping logging after 163264 examples to avoid logging too frequently
train stats after 163296 examples: {'rewards_train/chosen': '0.074695', 'rewards_train/rejected': '0.057934', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016761', 'logps_train/rejected': '-98.616', 'logps_train/chosen': '-139.36', 'loss/train': '0.69053', 'examples_per_second': '31.403', 'grad_norm': '27', 'counters/examples': 163296, 'counters/updates': 5103}
train stats after 163328 examples: {'rewards_train/chosen': '0.081448', 'rewards_train/rejected': '0.055115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026332', 'logps_train/rejected': '-128.9', 'logps_train/chosen': '-136.68', 'loss/train': '0.68996', 'examples_per_second': '31.636', 'grad_norm': '35.25', 'counters/examples': 163328, 'counters/updates': 5104}
train stats after 163360 examples: {'rewards_train/chosen': '0.11385', 'rewards_train/rejected': '0.10591', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0079412', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-134.31', 'loss/train': '0.70135', 'examples_per_second': '31.405', 'grad_norm': '29', 'counters/examples': 163360, 'counters/updates': 5105}
train stats after 163392 examples: {'rewards_train/chosen': '0.11226', 'rewards_train/rejected': '-0.0045681', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11683', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-138.78', 'loss/train': '0.66503', 'examples_per_second': '31.443', 'grad_norm': '31.875', 'counters/examples': 163392, 'counters/updates': 5106}
train stats after 163424 examples: {'rewards_train/chosen': '0.16661', 'rewards_train/rejected': '0.10353', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063074', 'logps_train/rejected': '-122.35', 'logps_train/chosen': '-166', 'loss/train': '0.67192', 'examples_per_second': '31.499', 'grad_norm': '40', 'counters/examples': 163424, 'counters/updates': 5107}
train stats after 163456 examples: {'rewards_train/chosen': '0.069163', 'rewards_train/rejected': '0.036912', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032251', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-146.91', 'loss/train': '0.68285', 'examples_per_second': '31.468', 'grad_norm': '40.25', 'counters/examples': 163456, 'counters/updates': 5108}
train stats after 163488 examples: {'rewards_train/chosen': '0.14415', 'rewards_train/rejected': '0.11797', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026185', 'logps_train/rejected': '-143.03', 'logps_train/chosen': '-134.71', 'loss/train': '0.6866', 'examples_per_second': '31.062', 'grad_norm': '59.25', 'counters/examples': 163488, 'counters/updates': 5109}
skipping logging after 163520 examples to avoid logging too frequently
train stats after 163552 examples: {'rewards_train/chosen': '0.1682', 'rewards_train/rejected': '0.070839', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.097359', 'logps_train/rejected': '-133.88', 'logps_train/chosen': '-163.33', 'loss/train': '0.65914', 'examples_per_second': '31.972', 'grad_norm': '35.5', 'counters/examples': 163552, 'counters/updates': 5111}
train stats after 163584 examples: {'rewards_train/chosen': '0.15318', 'rewards_train/rejected': '0.038277', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1149', 'logps_train/rejected': '-127.51', 'logps_train/chosen': '-131.14', 'loss/train': '0.64677', 'examples_per_second': '31.52', 'grad_norm': '33.25', 'counters/examples': 163584, 'counters/updates': 5112}
train stats after 163616 examples: {'rewards_train/chosen': '0.1309', 'rewards_train/rejected': '0.059851', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.071048', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-135.61', 'loss/train': '0.66866', 'examples_per_second': '32.38', 'grad_norm': '27.625', 'counters/examples': 163616, 'counters/updates': 5113}
train stats after 163648 examples: {'rewards_train/chosen': '0.10895', 'rewards_train/rejected': '0.092595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016356', 'logps_train/rejected': '-103.94', 'logps_train/chosen': '-155.67', 'loss/train': '0.69917', 'examples_per_second': '31.464', 'grad_norm': '35.25', 'counters/examples': 163648, 'counters/updates': 5114}
train stats after 163680 examples: {'rewards_train/chosen': '0.1199', 'rewards_train/rejected': '0.08667', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033235', 'logps_train/rejected': '-123.74', 'logps_train/chosen': '-127.76', 'loss/train': '0.69343', 'examples_per_second': '31.498', 'grad_norm': '47.75', 'counters/examples': 163680, 'counters/updates': 5115}
train stats after 163712 examples: {'rewards_train/chosen': '0.14714', 'rewards_train/rejected': '0.049722', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097422', 'logps_train/rejected': '-135.51', 'logps_train/chosen': '-113.29', 'loss/train': '0.6626', 'examples_per_second': '30.3', 'grad_norm': '27.375', 'counters/examples': 163712, 'counters/updates': 5116}
train stats after 163744 examples: {'rewards_train/chosen': '0.11149', 'rewards_train/rejected': '0.072901', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.038586', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-133.04', 'loss/train': '0.68456', 'examples_per_second': '30.06', 'grad_norm': '41.75', 'counters/examples': 163744, 'counters/updates': 5117}
skipping logging after 163776 examples to avoid logging too frequently
train stats after 163808 examples: {'rewards_train/chosen': '0.16427', 'rewards_train/rejected': '0.020879', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14339', 'logps_train/rejected': '-114.84', 'logps_train/chosen': '-145.89', 'loss/train': '0.62897', 'examples_per_second': '34.788', 'grad_norm': '30.375', 'counters/examples': 163808, 'counters/updates': 5119}
train stats after 163840 examples: {'rewards_train/chosen': '0.14929', 'rewards_train/rejected': '0.0046151', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14468', 'logps_train/rejected': '-124.63', 'logps_train/chosen': '-137.75', 'loss/train': '0.63988', 'examples_per_second': '31.411', 'grad_norm': '33', 'counters/examples': 163840, 'counters/updates': 5120}
train stats after 163872 examples: {'rewards_train/chosen': '0.1382', 'rewards_train/rejected': '0.020426', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11778', 'logps_train/rejected': '-146.04', 'logps_train/chosen': '-145.72', 'loss/train': '0.64281', 'examples_per_second': '31.481', 'grad_norm': '28.625', 'counters/examples': 163872, 'counters/updates': 5121}
train stats after 163904 examples: {'rewards_train/chosen': '0.31505', 'rewards_train/rejected': '0.016466', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.29859', 'logps_train/rejected': '-109.73', 'logps_train/chosen': '-209.07', 'loss/train': '0.58527', 'examples_per_second': '29.961', 'grad_norm': '51.5', 'counters/examples': 163904, 'counters/updates': 5122}
train stats after 163936 examples: {'rewards_train/chosen': '0.057124', 'rewards_train/rejected': '0.089501', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032377', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-118.44', 'loss/train': '0.72038', 'examples_per_second': '31.511', 'grad_norm': '40', 'counters/examples': 163936, 'counters/updates': 5123}
train stats after 163968 examples: {'rewards_train/chosen': '0.22472', 'rewards_train/rejected': '0.12835', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096371', 'logps_train/rejected': '-164.38', 'logps_train/chosen': '-165.44', 'loss/train': '0.65744', 'examples_per_second': '29.803', 'grad_norm': '68.5', 'counters/examples': 163968, 'counters/updates': 5124}
train stats after 164000 examples: {'rewards_train/chosen': '0.09849', 'rewards_train/rejected': '0.11978', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.021289', 'logps_train/rejected': '-114', 'logps_train/chosen': '-129.42', 'loss/train': '0.70882', 'examples_per_second': '31.895', 'grad_norm': '28.75', 'counters/examples': 164000, 'counters/updates': 5125}
train stats after 164032 examples: {'rewards_train/chosen': '0.1399', 'rewards_train/rejected': '0.045126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09477', 'logps_train/rejected': '-108.02', 'logps_train/chosen': '-112.74', 'loss/train': '0.66559', 'examples_per_second': '31.424', 'grad_norm': '28.75', 'counters/examples': 164032, 'counters/updates': 5126}
skipping logging after 164064 examples to avoid logging too frequently
train stats after 164096 examples: {'rewards_train/chosen': '0.12877', 'rewards_train/rejected': '0.064989', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063784', 'logps_train/rejected': '-138.1', 'logps_train/chosen': '-192.32', 'loss/train': '0.67469', 'examples_per_second': '29.921', 'grad_norm': '41', 'counters/examples': 164096, 'counters/updates': 5128}
train stats after 164128 examples: {'rewards_train/chosen': '0.16901', 'rewards_train/rejected': '0.059486', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10953', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-152.88', 'loss/train': '0.64649', 'examples_per_second': '32.328', 'grad_norm': '34', 'counters/examples': 164128, 'counters/updates': 5129}
skipping logging after 164160 examples to avoid logging too frequently
train stats after 164192 examples: {'rewards_train/chosen': '0.16613', 'rewards_train/rejected': '0.053906', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11223', 'logps_train/rejected': '-136.18', 'logps_train/chosen': '-184.21', 'loss/train': '0.64468', 'examples_per_second': '30.56', 'grad_norm': '33.25', 'counters/examples': 164192, 'counters/updates': 5131}
train stats after 164224 examples: {'rewards_train/chosen': '0.16656', 'rewards_train/rejected': '-0.010532', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17709', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-156.3', 'loss/train': '0.61886', 'examples_per_second': '31.887', 'grad_norm': '26.5', 'counters/examples': 164224, 'counters/updates': 5132}
train stats after 164256 examples: {'rewards_train/chosen': '0.076872', 'rewards_train/rejected': '0.030166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046706', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-177.82', 'loss/train': '0.68738', 'examples_per_second': '32.355', 'grad_norm': '36', 'counters/examples': 164256, 'counters/updates': 5133}
skipping logging after 164288 examples to avoid logging too frequently
train stats after 164320 examples: {'rewards_train/chosen': '0.15309', 'rewards_train/rejected': '0.1256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02749', 'logps_train/rejected': '-107.98', 'logps_train/chosen': '-116.76', 'loss/train': '0.70019', 'examples_per_second': '32.465', 'grad_norm': '63', 'counters/examples': 164320, 'counters/updates': 5135}
skipping logging after 164352 examples to avoid logging too frequently
train stats after 164384 examples: {'rewards_train/chosen': '0.067327', 'rewards_train/rejected': '-0.0071945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074521', 'logps_train/rejected': '-113.23', 'logps_train/chosen': '-130.44', 'loss/train': '0.66322', 'examples_per_second': '29.969', 'grad_norm': '38.5', 'counters/examples': 164384, 'counters/updates': 5137}
train stats after 164416 examples: {'rewards_train/chosen': '0.13533', 'rewards_train/rejected': '0.063941', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071394', 'logps_train/rejected': '-119.97', 'logps_train/chosen': '-142.12', 'loss/train': '0.66775', 'examples_per_second': '31.464', 'grad_norm': '36.25', 'counters/examples': 164416, 'counters/updates': 5138}
train stats after 164448 examples: {'rewards_train/chosen': '0.15517', 'rewards_train/rejected': '0.14908', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0060817', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-157.94', 'loss/train': '0.71254', 'examples_per_second': '29.891', 'grad_norm': '39.5', 'counters/examples': 164448, 'counters/updates': 5139}
train stats after 164480 examples: {'rewards_train/chosen': '0.20746', 'rewards_train/rejected': '0.070948', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13651', 'logps_train/rejected': '-162.64', 'logps_train/chosen': '-163.01', 'loss/train': '0.64499', 'examples_per_second': '32.396', 'grad_norm': '30.25', 'counters/examples': 164480, 'counters/updates': 5140}
train stats after 164512 examples: {'rewards_train/chosen': '0.13249', 'rewards_train/rejected': '0.082153', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050334', 'logps_train/rejected': '-187.6', 'logps_train/chosen': '-192.93', 'loss/train': '0.67753', 'examples_per_second': '29.942', 'grad_norm': '38.25', 'counters/examples': 164512, 'counters/updates': 5141}
train stats after 164544 examples: {'rewards_train/chosen': '0.12347', 'rewards_train/rejected': '-0.0052092', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12868', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-145.09', 'loss/train': '0.6426', 'examples_per_second': '30.682', 'grad_norm': '26', 'counters/examples': 164544, 'counters/updates': 5142}
skipping logging after 164576 examples to avoid logging too frequently
train stats after 164608 examples: {'rewards_train/chosen': '0.10972', 'rewards_train/rejected': '0.0019794', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10774', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-135.2', 'loss/train': '0.65065', 'examples_per_second': '30.07', 'grad_norm': '29.75', 'counters/examples': 164608, 'counters/updates': 5144}
train stats after 164640 examples: {'rewards_train/chosen': '0.15094', 'rewards_train/rejected': '0.11302', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.037912', 'logps_train/rejected': '-127.3', 'logps_train/chosen': '-126.1', 'loss/train': '0.68917', 'examples_per_second': '30.774', 'grad_norm': '32.25', 'counters/examples': 164640, 'counters/updates': 5145}
skipping logging after 164672 examples to avoid logging too frequently
train stats after 164704 examples: {'rewards_train/chosen': '0.14297', 'rewards_train/rejected': '0.088862', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054107', 'logps_train/rejected': '-123.54', 'logps_train/chosen': '-129.24', 'loss/train': '0.67567', 'examples_per_second': '31.216', 'grad_norm': '34', 'counters/examples': 164704, 'counters/updates': 5147}
train stats after 164736 examples: {'rewards_train/chosen': '0.13831', 'rewards_train/rejected': '0.022821', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11548', 'logps_train/rejected': '-111.74', 'logps_train/chosen': '-137.57', 'loss/train': '0.64737', 'examples_per_second': '31.537', 'grad_norm': '27.125', 'counters/examples': 164736, 'counters/updates': 5148}
train stats after 164768 examples: {'rewards_train/chosen': '0.17273', 'rewards_train/rejected': '0.016119', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15662', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-148.81', 'loss/train': '0.62511', 'examples_per_second': '31.325', 'grad_norm': '23.75', 'counters/examples': 164768, 'counters/updates': 5149}
train stats after 164800 examples: {'rewards_train/chosen': '0.25783', 'rewards_train/rejected': '0.090779', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16705', 'logps_train/rejected': '-133.7', 'logps_train/chosen': '-145.59', 'loss/train': '0.63385', 'examples_per_second': '30.936', 'grad_norm': '28.5', 'counters/examples': 164800, 'counters/updates': 5150}
skipping logging after 164832 examples to avoid logging too frequently
train stats after 164864 examples: {'rewards_train/chosen': '0.21555', 'rewards_train/rejected': '0.12669', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088857', 'logps_train/rejected': '-156.44', 'logps_train/chosen': '-167.11', 'loss/train': '0.67299', 'examples_per_second': '31.448', 'grad_norm': '34.5', 'counters/examples': 164864, 'counters/updates': 5152}
train stats after 164896 examples: {'rewards_train/chosen': '0.1289', 'rewards_train/rejected': '0.048818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080086', 'logps_train/rejected': '-103.21', 'logps_train/chosen': '-133.67', 'loss/train': '0.66888', 'examples_per_second': '30.661', 'grad_norm': '33', 'counters/examples': 164896, 'counters/updates': 5153}
skipping logging after 164928 examples to avoid logging too frequently
train stats after 164960 examples: {'rewards_train/chosen': '0.14025', 'rewards_train/rejected': '0.086697', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053548', 'logps_train/rejected': '-121.23', 'logps_train/chosen': '-123.83', 'loss/train': '0.67579', 'examples_per_second': '30.337', 'grad_norm': '29.625', 'counters/examples': 164960, 'counters/updates': 5155}
train stats after 164992 examples: {'rewards_train/chosen': '0.10452', 'rewards_train/rejected': '0.086098', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018426', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-148.97', 'loss/train': '0.69495', 'examples_per_second': '31.23', 'grad_norm': '32.5', 'counters/examples': 164992, 'counters/updates': 5156}
train stats after 165024 examples: {'rewards_train/chosen': '0.037475', 'rewards_train/rejected': '0.019266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018209', 'logps_train/rejected': '-127.07', 'logps_train/chosen': '-120.54', 'loss/train': '0.69422', 'examples_per_second': '29.775', 'grad_norm': '42.25', 'counters/examples': 165024, 'counters/updates': 5157}
train stats after 165056 examples: {'rewards_train/chosen': '0.16719', 'rewards_train/rejected': '0.036749', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13044', 'logps_train/rejected': '-118.02', 'logps_train/chosen': '-127.82', 'loss/train': '0.65752', 'examples_per_second': '32.291', 'grad_norm': '33.5', 'counters/examples': 165056, 'counters/updates': 5158}
train stats after 165088 examples: {'rewards_train/chosen': '0.15532', 'rewards_train/rejected': '0.049664', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10565', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-167.32', 'loss/train': '0.65393', 'examples_per_second': '31.447', 'grad_norm': '39', 'counters/examples': 165088, 'counters/updates': 5159}
train stats after 165120 examples: {'rewards_train/chosen': '0.17132', 'rewards_train/rejected': '0.066616', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1047', 'logps_train/rejected': '-128.65', 'logps_train/chosen': '-128.81', 'loss/train': '0.65664', 'examples_per_second': '32.748', 'grad_norm': '31', 'counters/examples': 165120, 'counters/updates': 5160}
train stats after 165152 examples: {'rewards_train/chosen': '0.14993', 'rewards_train/rejected': '0.061372', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088555', 'logps_train/rejected': '-143.52', 'logps_train/chosen': '-139', 'loss/train': '0.66954', 'examples_per_second': '30.374', 'grad_norm': '31.75', 'counters/examples': 165152, 'counters/updates': 5161}
train stats after 165184 examples: {'rewards_train/chosen': '0.11415', 'rewards_train/rejected': '0.05408', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.060073', 'logps_train/rejected': '-144.9', 'logps_train/chosen': '-173.51', 'loss/train': '0.68183', 'examples_per_second': '31.457', 'grad_norm': '37.75', 'counters/examples': 165184, 'counters/updates': 5162}
train stats after 165216 examples: {'rewards_train/chosen': '0.13874', 'rewards_train/rejected': '0.074343', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0644', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-143.14', 'loss/train': '0.68838', 'examples_per_second': '31.184', 'grad_norm': '42.75', 'counters/examples': 165216, 'counters/updates': 5163}
skipping logging after 165248 examples to avoid logging too frequently
train stats after 165280 examples: {'rewards_train/chosen': '0.14948', 'rewards_train/rejected': '0.039658', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10982', 'logps_train/rejected': '-115.89', 'logps_train/chosen': '-126.83', 'loss/train': '0.64939', 'examples_per_second': '31.469', 'grad_norm': '39.25', 'counters/examples': 165280, 'counters/updates': 5165}
train stats after 165312 examples: {'rewards_train/chosen': '0.12304', 'rewards_train/rejected': '0.038625', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084415', 'logps_train/rejected': '-155.97', 'logps_train/chosen': '-148.46', 'loss/train': '0.66715', 'examples_per_second': '31.152', 'grad_norm': '30.25', 'counters/examples': 165312, 'counters/updates': 5166}
train stats after 165344 examples: {'rewards_train/chosen': '0.12921', 'rewards_train/rejected': '-0.023884', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15309', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-144.37', 'loss/train': '0.62752', 'examples_per_second': '30.466', 'grad_norm': '36.75', 'counters/examples': 165344, 'counters/updates': 5167}
skipping logging after 165376 examples to avoid logging too frequently
train stats after 165408 examples: {'rewards_train/chosen': '0.14436', 'rewards_train/rejected': '0.0069958', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13736', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-133.97', 'loss/train': '0.63596', 'examples_per_second': '30.954', 'grad_norm': '29', 'counters/examples': 165408, 'counters/updates': 5169}
train stats after 165440 examples: {'rewards_train/chosen': '0.11136', 'rewards_train/rejected': '0.058166', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053199', 'logps_train/rejected': '-116.45', 'logps_train/chosen': '-144.79', 'loss/train': '0.67597', 'examples_per_second': '31.464', 'grad_norm': '32.75', 'counters/examples': 165440, 'counters/updates': 5170}
train stats after 165472 examples: {'rewards_train/chosen': '0.083012', 'rewards_train/rejected': '0.0085282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074484', 'logps_train/rejected': '-98.946', 'logps_train/chosen': '-159.78', 'loss/train': '0.66444', 'examples_per_second': '30.806', 'grad_norm': '26.25', 'counters/examples': 165472, 'counters/updates': 5171}
train stats after 165504 examples: {'rewards_train/chosen': '0.1196', 'rewards_train/rejected': '0.0051256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11448', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-120.55', 'loss/train': '0.65215', 'examples_per_second': '30.766', 'grad_norm': '33', 'counters/examples': 165504, 'counters/updates': 5172}
train stats after 165536 examples: {'rewards_train/chosen': '0.14661', 'rewards_train/rejected': '0.085507', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061102', 'logps_train/rejected': '-154.26', 'logps_train/chosen': '-143.6', 'loss/train': '0.67288', 'examples_per_second': '30.625', 'grad_norm': '37.5', 'counters/examples': 165536, 'counters/updates': 5173}
train stats after 165568 examples: {'rewards_train/chosen': '0.11723', 'rewards_train/rejected': '-0.049204', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16644', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-119.19', 'loss/train': '0.62579', 'examples_per_second': '31.695', 'grad_norm': '27.375', 'counters/examples': 165568, 'counters/updates': 5174}
train stats after 165600 examples: {'rewards_train/chosen': '0.11602', 'rewards_train/rejected': '0.017448', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.098574', 'logps_train/rejected': '-129.38', 'logps_train/chosen': '-145.7', 'loss/train': '0.6493', 'examples_per_second': '30.07', 'grad_norm': '27.125', 'counters/examples': 165600, 'counters/updates': 5175}
skipping logging after 165632 examples to avoid logging too frequently
train stats after 165664 examples: {'rewards_train/chosen': '0.16986', 'rewards_train/rejected': '0.048868', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.121', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-137.99', 'loss/train': '0.64937', 'examples_per_second': '36.704', 'grad_norm': '38.25', 'counters/examples': 165664, 'counters/updates': 5177}
train stats after 165696 examples: {'rewards_train/chosen': '0.17429', 'rewards_train/rejected': '0.065867', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10842', 'logps_train/rejected': '-110.12', 'logps_train/chosen': '-136.82', 'loss/train': '0.64648', 'examples_per_second': '30.333', 'grad_norm': '31.25', 'counters/examples': 165696, 'counters/updates': 5178}
skipping logging after 165728 examples to avoid logging too frequently
train stats after 165760 examples: {'rewards_train/chosen': '0.089215', 'rewards_train/rejected': '0.080508', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0087065', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-112.24', 'loss/train': '0.70027', 'examples_per_second': '31.612', 'grad_norm': '41', 'counters/examples': 165760, 'counters/updates': 5180}
train stats after 165792 examples: {'rewards_train/chosen': '0.10805', 'rewards_train/rejected': '0.099138', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0089088', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-143.03', 'loss/train': '0.70805', 'examples_per_second': '31.624', 'grad_norm': '30.5', 'counters/examples': 165792, 'counters/updates': 5181}
train stats after 165824 examples: {'rewards_train/chosen': '0.10485', 'rewards_train/rejected': '0.066508', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038339', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-135.79', 'loss/train': '0.68491', 'examples_per_second': '30.602', 'grad_norm': '30.375', 'counters/examples': 165824, 'counters/updates': 5182}
train stats after 165856 examples: {'rewards_train/chosen': '0.137', 'rewards_train/rejected': '0.040526', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096471', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-172.72', 'loss/train': '0.65851', 'examples_per_second': '29.915', 'grad_norm': '36', 'counters/examples': 165856, 'counters/updates': 5183}
train stats after 165888 examples: {'rewards_train/chosen': '0.17574', 'rewards_train/rejected': '0.14208', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033658', 'logps_train/rejected': '-128.69', 'logps_train/chosen': '-174.41', 'loss/train': '0.70685', 'examples_per_second': '32.415', 'grad_norm': '54.75', 'counters/examples': 165888, 'counters/updates': 5184}
train stats after 165920 examples: {'rewards_train/chosen': '0.076739', 'rewards_train/rejected': '0.088751', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.012012', 'logps_train/rejected': '-111.18', 'logps_train/chosen': '-138.35', 'loss/train': '0.70886', 'examples_per_second': '27.375', 'grad_norm': '42.75', 'counters/examples': 165920, 'counters/updates': 5185}
train stats after 165952 examples: {'rewards_train/chosen': '0.12353', 'rewards_train/rejected': '0.022092', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10144', 'logps_train/rejected': '-133.65', 'logps_train/chosen': '-117.74', 'loss/train': '0.66001', 'examples_per_second': '31.406', 'grad_norm': '28.5', 'counters/examples': 165952, 'counters/updates': 5186}
skipping logging after 165984 examples to avoid logging too frequently
train stats after 166016 examples: {'rewards_train/chosen': '0.16895', 'rewards_train/rejected': '0.13544', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.033513', 'logps_train/rejected': '-166.03', 'logps_train/chosen': '-146.71', 'loss/train': '0.68874', 'examples_per_second': '30.065', 'grad_norm': '31.5', 'counters/examples': 166016, 'counters/updates': 5188}
train stats after 166048 examples: {'rewards_train/chosen': '0.1048', 'rewards_train/rejected': '0.010436', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.094363', 'logps_train/rejected': '-159.46', 'logps_train/chosen': '-147.77', 'loss/train': '0.66641', 'examples_per_second': '31.398', 'grad_norm': '35.5', 'counters/examples': 166048, 'counters/updates': 5189}
skipping logging after 166080 examples to avoid logging too frequently
train stats after 166112 examples: {'rewards_train/chosen': '0.17734', 'rewards_train/rejected': '0.011667', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16567', 'logps_train/rejected': '-115.99', 'logps_train/chosen': '-148.96', 'loss/train': '0.62312', 'examples_per_second': '33.398', 'grad_norm': '31.75', 'counters/examples': 166112, 'counters/updates': 5191}
train stats after 166144 examples: {'rewards_train/chosen': '0.15838', 'rewards_train/rejected': '0.12534', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033038', 'logps_train/rejected': '-151.01', 'logps_train/chosen': '-152.34', 'loss/train': '0.70605', 'examples_per_second': '31.351', 'grad_norm': '64.5', 'counters/examples': 166144, 'counters/updates': 5192}
train stats after 166176 examples: {'rewards_train/chosen': '0.08769', 'rewards_train/rejected': '0.026388', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061302', 'logps_train/rejected': '-131.99', 'logps_train/chosen': '-110.07', 'loss/train': '0.67224', 'examples_per_second': '31.573', 'grad_norm': '27.875', 'counters/examples': 166176, 'counters/updates': 5193}
train stats after 166208 examples: {'rewards_train/chosen': '0.13918', 'rewards_train/rejected': '0.073094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066087', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-189.21', 'loss/train': '0.66915', 'examples_per_second': '32.54', 'grad_norm': '31.375', 'counters/examples': 166208, 'counters/updates': 5194}
train stats after 166240 examples: {'rewards_train/chosen': '0.11229', 'rewards_train/rejected': '0.056327', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055962', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-143.11', 'loss/train': '0.68336', 'examples_per_second': '30.292', 'grad_norm': '33.75', 'counters/examples': 166240, 'counters/updates': 5195}
train stats after 166272 examples: {'rewards_train/chosen': '0.12917', 'rewards_train/rejected': '0.054541', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07463', 'logps_train/rejected': '-140.09', 'logps_train/chosen': '-159.13', 'loss/train': '0.66826', 'examples_per_second': '29.974', 'grad_norm': '30.75', 'counters/examples': 166272, 'counters/updates': 5196}
train stats after 166304 examples: {'rewards_train/chosen': '0.075524', 'rewards_train/rejected': '0.080547', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0050228', 'logps_train/rejected': '-122.05', 'logps_train/chosen': '-107.33', 'loss/train': '0.70574', 'examples_per_second': '31.423', 'grad_norm': '27', 'counters/examples': 166304, 'counters/updates': 5197}
train stats after 166336 examples: {'rewards_train/chosen': '0.12384', 'rewards_train/rejected': '0.083163', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040678', 'logps_train/rejected': '-138.36', 'logps_train/chosen': '-132.62', 'loss/train': '0.68774', 'examples_per_second': '31.438', 'grad_norm': '46', 'counters/examples': 166336, 'counters/updates': 5198}
train stats after 166368 examples: {'rewards_train/chosen': '0.13033', 'rewards_train/rejected': '0.065275', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.065055', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-126.63', 'loss/train': '0.66829', 'examples_per_second': '30.386', 'grad_norm': '35.75', 'counters/examples': 166368, 'counters/updates': 5199}
train stats after 166400 examples: {'rewards_train/chosen': '0.16579', 'rewards_train/rejected': '-0.0045922', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17038', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-139.54', 'loss/train': '0.62974', 'examples_per_second': '31.344', 'grad_norm': '40.25', 'counters/examples': 166400, 'counters/updates': 5200}
train stats after 166432 examples: {'rewards_train/chosen': '0.16365', 'rewards_train/rejected': '0.067722', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095929', 'logps_train/rejected': '-154.61', 'logps_train/chosen': '-155.19', 'loss/train': '0.65475', 'examples_per_second': '31.41', 'grad_norm': '39', 'counters/examples': 166432, 'counters/updates': 5201}
train stats after 166464 examples: {'rewards_train/chosen': '0.035792', 'rewards_train/rejected': '-0.0044274', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040219', 'logps_train/rejected': '-95.539', 'logps_train/chosen': '-120.11', 'loss/train': '0.68735', 'examples_per_second': '32.531', 'grad_norm': '29.375', 'counters/examples': 166464, 'counters/updates': 5202}
train stats after 166496 examples: {'rewards_train/chosen': '0.10005', 'rewards_train/rejected': '0.028913', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071139', 'logps_train/rejected': '-120.88', 'logps_train/chosen': '-165.81', 'loss/train': '0.67376', 'examples_per_second': '31.392', 'grad_norm': '35', 'counters/examples': 166496, 'counters/updates': 5203}
train stats after 166528 examples: {'rewards_train/chosen': '0.064592', 'rewards_train/rejected': '0.063557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0010352', 'logps_train/rejected': '-102.02', 'logps_train/chosen': '-129.65', 'loss/train': '0.70013', 'examples_per_second': '31.41', 'grad_norm': '29.75', 'counters/examples': 166528, 'counters/updates': 5204}
train stats after 166560 examples: {'rewards_train/chosen': '0.10296', 'rewards_train/rejected': '0.0025348', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10042', 'logps_train/rejected': '-122.49', 'logps_train/chosen': '-142.61', 'loss/train': '0.65182', 'examples_per_second': '31.434', 'grad_norm': '25.75', 'counters/examples': 166560, 'counters/updates': 5205}
train stats after 166592 examples: {'rewards_train/chosen': '0.1298', 'rewards_train/rejected': '0.052256', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.077544', 'logps_train/rejected': '-125.45', 'logps_train/chosen': '-150.91', 'loss/train': '0.6767', 'examples_per_second': '30.329', 'grad_norm': '46.25', 'counters/examples': 166592, 'counters/updates': 5206}
train stats after 166624 examples: {'rewards_train/chosen': '0.043298', 'rewards_train/rejected': '0.021219', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.02208', 'logps_train/rejected': '-95.133', 'logps_train/chosen': '-109.23', 'loss/train': '0.69017', 'examples_per_second': '31.067', 'grad_norm': '25.5', 'counters/examples': 166624, 'counters/updates': 5207}
train stats after 166656 examples: {'rewards_train/chosen': '0.051999', 'rewards_train/rejected': '0.081322', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.029323', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-138.27', 'loss/train': '0.72293', 'examples_per_second': '31.398', 'grad_norm': '56.75', 'counters/examples': 166656, 'counters/updates': 5208}
train stats after 166688 examples: {'rewards_train/chosen': '0.18002', 'rewards_train/rejected': '0.033544', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14648', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-164.85', 'loss/train': '0.64198', 'examples_per_second': '29.977', 'grad_norm': '29.75', 'counters/examples': 166688, 'counters/updates': 5209}
skipping logging after 166720 examples to avoid logging too frequently
train stats after 166752 examples: {'rewards_train/chosen': '0.13408', 'rewards_train/rejected': '0.016669', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11741', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-168.47', 'loss/train': '0.64726', 'examples_per_second': '31.385', 'grad_norm': '36.25', 'counters/examples': 166752, 'counters/updates': 5211}
skipping logging after 166784 examples to avoid logging too frequently
train stats after 166816 examples: {'rewards_train/chosen': '0.061037', 'rewards_train/rejected': '0.027501', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033536', 'logps_train/rejected': '-153.75', 'logps_train/chosen': '-155.76', 'loss/train': '0.69796', 'examples_per_second': '30.236', 'grad_norm': '52.25', 'counters/examples': 166816, 'counters/updates': 5213}
train stats after 166848 examples: {'rewards_train/chosen': '0.057593', 'rewards_train/rejected': '-0.053211', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1108', 'logps_train/rejected': '-108.55', 'logps_train/chosen': '-95.347', 'loss/train': '0.66244', 'examples_per_second': '31.05', 'grad_norm': '25.375', 'counters/examples': 166848, 'counters/updates': 5214}
train stats after 166880 examples: {'rewards_train/chosen': '0.067585', 'rewards_train/rejected': '0.02826', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.039325', 'logps_train/rejected': '-145.81', 'logps_train/chosen': '-174.47', 'loss/train': '0.68549', 'examples_per_second': '31.316', 'grad_norm': '31', 'counters/examples': 166880, 'counters/updates': 5215}
skipping logging after 166912 examples to avoid logging too frequently
train stats after 166944 examples: {'rewards_train/chosen': '0.12546', 'rewards_train/rejected': '0.046367', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07909', 'logps_train/rejected': '-112.67', 'logps_train/chosen': '-137.78', 'loss/train': '0.66258', 'examples_per_second': '30.86', 'grad_norm': '29.25', 'counters/examples': 166944, 'counters/updates': 5217}
train stats after 166976 examples: {'rewards_train/chosen': '0.17126', 'rewards_train/rejected': '-0.026377', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19763', 'logps_train/rejected': '-120.87', 'logps_train/chosen': '-180.15', 'loss/train': '0.6185', 'examples_per_second': '31.344', 'grad_norm': '29.25', 'counters/examples': 166976, 'counters/updates': 5218}
train stats after 167008 examples: {'rewards_train/chosen': '0.14812', 'rewards_train/rejected': '0.07234', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075781', 'logps_train/rejected': '-137.42', 'logps_train/chosen': '-139.32', 'loss/train': '0.67786', 'examples_per_second': '31.008', 'grad_norm': '33.75', 'counters/examples': 167008, 'counters/updates': 5219}
skipping logging after 167040 examples to avoid logging too frequently
train stats after 167072 examples: {'rewards_train/chosen': '0.10651', 'rewards_train/rejected': '0.069302', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037205', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-125.96', 'loss/train': '0.68671', 'examples_per_second': '30.256', 'grad_norm': '37.25', 'counters/examples': 167072, 'counters/updates': 5221}
skipping logging after 167104 examples to avoid logging too frequently
train stats after 167136 examples: {'rewards_train/chosen': '0.12568', 'rewards_train/rejected': '0.095247', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030428', 'logps_train/rejected': '-145.67', 'logps_train/chosen': '-142.18', 'loss/train': '0.69509', 'examples_per_second': '30.025', 'grad_norm': '38.5', 'counters/examples': 167136, 'counters/updates': 5223}
train stats after 167168 examples: {'rewards_train/chosen': '0.2002', 'rewards_train/rejected': '0.17169', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028511', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-164.5', 'loss/train': '0.69678', 'examples_per_second': '33.128', 'grad_norm': '61.5', 'counters/examples': 167168, 'counters/updates': 5224}
skipping logging after 167200 examples to avoid logging too frequently
train stats after 167232 examples: {'rewards_train/chosen': '0.11388', 'rewards_train/rejected': '0.018757', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095125', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-137.75', 'loss/train': '0.6667', 'examples_per_second': '33.532', 'grad_norm': '34', 'counters/examples': 167232, 'counters/updates': 5226}
train stats after 167264 examples: {'rewards_train/chosen': '0.16305', 'rewards_train/rejected': '0.12143', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041623', 'logps_train/rejected': '-134.17', 'logps_train/chosen': '-156.78', 'loss/train': '0.67907', 'examples_per_second': '31.136', 'grad_norm': '48', 'counters/examples': 167264, 'counters/updates': 5227}
train stats after 167296 examples: {'rewards_train/chosen': '0.072849', 'rewards_train/rejected': '0.053055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019794', 'logps_train/rejected': '-150.2', 'logps_train/chosen': '-172.42', 'loss/train': '0.6923', 'examples_per_second': '31.213', 'grad_norm': '42', 'counters/examples': 167296, 'counters/updates': 5228}
train stats after 167328 examples: {'rewards_train/chosen': '0.14441', 'rewards_train/rejected': '0.063987', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08042', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-146.54', 'loss/train': '0.66512', 'examples_per_second': '31.232', 'grad_norm': '27.75', 'counters/examples': 167328, 'counters/updates': 5229}
train stats after 167360 examples: {'rewards_train/chosen': '0.20288', 'rewards_train/rejected': '0.11609', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086795', 'logps_train/rejected': '-147.84', 'logps_train/chosen': '-175.66', 'loss/train': '0.65921', 'examples_per_second': '31.023', 'grad_norm': '32', 'counters/examples': 167360, 'counters/updates': 5230}
skipping logging after 167392 examples to avoid logging too frequently
train stats after 167424 examples: {'rewards_train/chosen': '0.22307', 'rewards_train/rejected': '0.15999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063086', 'logps_train/rejected': '-147.68', 'logps_train/chosen': '-154.57', 'loss/train': '0.68679', 'examples_per_second': '35.23', 'grad_norm': '32.5', 'counters/examples': 167424, 'counters/updates': 5232}
train stats after 167456 examples: {'rewards_train/chosen': '0.12736', 'rewards_train/rejected': '-0.039489', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16685', 'logps_train/rejected': '-150.89', 'logps_train/chosen': '-165.53', 'loss/train': '0.62036', 'examples_per_second': '30.972', 'grad_norm': '48', 'counters/examples': 167456, 'counters/updates': 5233}
skipping logging after 167488 examples to avoid logging too frequently
train stats after 167520 examples: {'rewards_train/chosen': '0.11436', 'rewards_train/rejected': '0.05202', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062337', 'logps_train/rejected': '-114.84', 'logps_train/chosen': '-144.15', 'loss/train': '0.67437', 'examples_per_second': '31.28', 'grad_norm': '33.25', 'counters/examples': 167520, 'counters/updates': 5235}
skipping logging after 167552 examples to avoid logging too frequently
train stats after 167584 examples: {'rewards_train/chosen': '0.17983', 'rewards_train/rejected': '0.06628', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11355', 'logps_train/rejected': '-141.44', 'logps_train/chosen': '-160.52', 'loss/train': '0.651', 'examples_per_second': '32.571', 'grad_norm': '30.625', 'counters/examples': 167584, 'counters/updates': 5237}
train stats after 167616 examples: {'rewards_train/chosen': '0.16425', 'rewards_train/rejected': '0.056428', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10783', 'logps_train/rejected': '-105.18', 'logps_train/chosen': '-152.81', 'loss/train': '0.65767', 'examples_per_second': '31.781', 'grad_norm': '28.375', 'counters/examples': 167616, 'counters/updates': 5238}
skipping logging after 167648 examples to avoid logging too frequently
train stats after 167680 examples: {'rewards_train/chosen': '0.1455', 'rewards_train/rejected': '0.083331', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062168', 'logps_train/rejected': '-149.26', 'logps_train/chosen': '-140.03', 'loss/train': '0.67633', 'examples_per_second': '37.751', 'grad_norm': '33.25', 'counters/examples': 167680, 'counters/updates': 5240}
train stats after 167712 examples: {'rewards_train/chosen': '0.14203', 'rewards_train/rejected': '0.054715', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087318', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-109.07', 'loss/train': '0.66446', 'examples_per_second': '31.157', 'grad_norm': '26.5', 'counters/examples': 167712, 'counters/updates': 5241}
train stats after 167744 examples: {'rewards_train/chosen': '0.22069', 'rewards_train/rejected': '0.056244', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16445', 'logps_train/rejected': '-116.84', 'logps_train/chosen': '-150.58', 'loss/train': '0.6719', 'examples_per_second': '30.324', 'grad_norm': '35.25', 'counters/examples': 167744, 'counters/updates': 5242}
train stats after 167776 examples: {'rewards_train/chosen': '0.089203', 'rewards_train/rejected': '0.063289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025915', 'logps_train/rejected': '-94.921', 'logps_train/chosen': '-85.532', 'loss/train': '0.68605', 'examples_per_second': '29.856', 'grad_norm': '22.25', 'counters/examples': 167776, 'counters/updates': 5243}
train stats after 167808 examples: {'rewards_train/chosen': '0.18617', 'rewards_train/rejected': '0.11526', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070909', 'logps_train/rejected': '-153.25', 'logps_train/chosen': '-156.65', 'loss/train': '0.68221', 'examples_per_second': '30.563', 'grad_norm': '42', 'counters/examples': 167808, 'counters/updates': 5244}
skipping logging after 167840 examples to avoid logging too frequently
train stats after 167872 examples: {'rewards_train/chosen': '0.20256', 'rewards_train/rejected': '0.050168', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15239', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-141.42', 'loss/train': '0.63378', 'examples_per_second': '31.127', 'grad_norm': '34', 'counters/examples': 167872, 'counters/updates': 5246}
train stats after 167904 examples: {'rewards_train/chosen': '0.16444', 'rewards_train/rejected': '0.017347', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14709', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-135.25', 'loss/train': '0.62802', 'examples_per_second': '31.439', 'grad_norm': '44', 'counters/examples': 167904, 'counters/updates': 5247}
train stats after 167936 examples: {'rewards_train/chosen': '0.14127', 'rewards_train/rejected': '0.10702', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034246', 'logps_train/rejected': '-130.06', 'logps_train/chosen': '-135.11', 'loss/train': '0.68718', 'examples_per_second': '32.504', 'grad_norm': '48.5', 'counters/examples': 167936, 'counters/updates': 5248}
train stats after 167968 examples: {'rewards_train/chosen': '0.15024', 'rewards_train/rejected': '0.073812', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076432', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-140.05', 'loss/train': '0.66565', 'examples_per_second': '32.32', 'grad_norm': '33.5', 'counters/examples': 167968, 'counters/updates': 5249}
train stats after 168000 examples: {'rewards_train/chosen': '0.12584', 'rewards_train/rejected': '0.05076', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07508', 'logps_train/rejected': '-115.36', 'logps_train/chosen': '-97.149', 'loss/train': '0.66463', 'examples_per_second': '31.561', 'grad_norm': '23.75', 'counters/examples': 168000, 'counters/updates': 5250}
Running evaluation after 168000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.93it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.98it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 168000: {'rewards_eval/chosen': '0.15659', 'rewards_eval/rejected': '0.064479', 'rewards_eval/accuracies': '0.60156', 'rewards_eval/margins': '0.092106', 'logps_eval/rejected': '-121.49', 'logps_eval/chosen': '-142.54', 'loss/eval': '0.6628'}
skipping save for non epoch
train stats after 168032 examples: {'rewards_train/chosen': '0.062281', 'rewards_train/rejected': '0.054972', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0073088', 'logps_train/rejected': '-100.97', 'logps_train/chosen': '-122.87', 'loss/train': '0.69993', 'examples_per_second': '32.252', 'grad_norm': '25.25', 'counters/examples': 168032, 'counters/updates': 5251}
train stats after 168064 examples: {'rewards_train/chosen': '0.12789', 'rewards_train/rejected': '0.085832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042063', 'logps_train/rejected': '-131.55', 'logps_train/chosen': '-154.8', 'loss/train': '0.68436', 'examples_per_second': '32.169', 'grad_norm': '35.5', 'counters/examples': 168064, 'counters/updates': 5252}
train stats after 168096 examples: {'rewards_train/chosen': '0.033732', 'rewards_train/rejected': '-0.0043475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038079', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-120.86', 'loss/train': '0.68416', 'examples_per_second': '31.478', 'grad_norm': '25.25', 'counters/examples': 168096, 'counters/updates': 5253}
train stats after 168128 examples: {'rewards_train/chosen': '0.21012', 'rewards_train/rejected': '0.017395', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.19272', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-136.23', 'loss/train': '0.60833', 'examples_per_second': '30.931', 'grad_norm': '24.75', 'counters/examples': 168128, 'counters/updates': 5254}
train stats after 168160 examples: {'rewards_train/chosen': '0.074699', 'rewards_train/rejected': '0.020396', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054302', 'logps_train/rejected': '-120.49', 'logps_train/chosen': '-151.92', 'loss/train': '0.66985', 'examples_per_second': '31.411', 'grad_norm': '26.875', 'counters/examples': 168160, 'counters/updates': 5255}
train stats after 168192 examples: {'rewards_train/chosen': '0.15114', 'rewards_train/rejected': '0.080605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070533', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-148.8', 'loss/train': '0.6722', 'examples_per_second': '31.644', 'grad_norm': '42.75', 'counters/examples': 168192, 'counters/updates': 5256}
train stats after 168224 examples: {'rewards_train/chosen': '0.11584', 'rewards_train/rejected': '-0.0064788', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12232', 'logps_train/rejected': '-153.19', 'logps_train/chosen': '-166.37', 'loss/train': '0.64603', 'examples_per_second': '31.236', 'grad_norm': '40.25', 'counters/examples': 168224, 'counters/updates': 5257}
train stats after 168256 examples: {'rewards_train/chosen': '0.1417', 'rewards_train/rejected': '0.053657', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088044', 'logps_train/rejected': '-132.75', 'logps_train/chosen': '-150.95', 'loss/train': '0.66523', 'examples_per_second': '31.441', 'grad_norm': '39', 'counters/examples': 168256, 'counters/updates': 5258}
train stats after 168288 examples: {'rewards_train/chosen': '0.2498', 'rewards_train/rejected': '0.10077', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14903', 'logps_train/rejected': '-104.61', 'logps_train/chosen': '-149.81', 'loss/train': '0.64299', 'examples_per_second': '31.541', 'grad_norm': '28.625', 'counters/examples': 168288, 'counters/updates': 5259}
train stats after 168320 examples: {'rewards_train/chosen': '0.060525', 'rewards_train/rejected': '0.0059238', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.054602', 'logps_train/rejected': '-126.27', 'logps_train/chosen': '-111.5', 'loss/train': '0.67109', 'examples_per_second': '33.019', 'grad_norm': '28.25', 'counters/examples': 168320, 'counters/updates': 5260}
train stats after 168352 examples: {'rewards_train/chosen': '0.078903', 'rewards_train/rejected': '0.049783', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02912', 'logps_train/rejected': '-125.79', 'logps_train/chosen': '-111.63', 'loss/train': '0.6834', 'examples_per_second': '24.443', 'grad_norm': '29.875', 'counters/examples': 168352, 'counters/updates': 5261}
train stats after 168384 examples: {'rewards_train/chosen': '0.076279', 'rewards_train/rejected': '0.016657', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059622', 'logps_train/rejected': '-123.75', 'logps_train/chosen': '-138.25', 'loss/train': '0.67297', 'examples_per_second': '32.814', 'grad_norm': '26.625', 'counters/examples': 168384, 'counters/updates': 5262}
train stats after 168416 examples: {'rewards_train/chosen': '0.13535', 'rewards_train/rejected': '-0.032582', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16793', 'logps_train/rejected': '-112.87', 'logps_train/chosen': '-159.27', 'loss/train': '0.62475', 'examples_per_second': '29.752', 'grad_norm': '26.375', 'counters/examples': 168416, 'counters/updates': 5263}
train stats after 168448 examples: {'rewards_train/chosen': '0.16445', 'rewards_train/rejected': '0.080871', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083581', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-150.97', 'loss/train': '0.66545', 'examples_per_second': '24.228', 'grad_norm': '32.25', 'counters/examples': 168448, 'counters/updates': 5264}
train stats after 168480 examples: {'rewards_train/chosen': '0.23008', 'rewards_train/rejected': '0.13983', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090251', 'logps_train/rejected': '-86.815', 'logps_train/chosen': '-110.66', 'loss/train': '0.66288', 'examples_per_second': '33.117', 'grad_norm': '30.5', 'counters/examples': 168480, 'counters/updates': 5265}
train stats after 168512 examples: {'rewards_train/chosen': '0.14151', 'rewards_train/rejected': '0.067027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07448', 'logps_train/rejected': '-123.92', 'logps_train/chosen': '-163', 'loss/train': '0.66532', 'examples_per_second': '29.986', 'grad_norm': '40.75', 'counters/examples': 168512, 'counters/updates': 5266}
train stats after 168544 examples: {'rewards_train/chosen': '0.10901', 'rewards_train/rejected': '0.032498', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076516', 'logps_train/rejected': '-166.51', 'logps_train/chosen': '-130.27', 'loss/train': '0.66696', 'examples_per_second': '31.404', 'grad_norm': '33.75', 'counters/examples': 168544, 'counters/updates': 5267}
train stats after 168576 examples: {'rewards_train/chosen': '0.17413', 'rewards_train/rejected': '-0.0011074', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17524', 'logps_train/rejected': '-108.2', 'logps_train/chosen': '-145.93', 'loss/train': '0.63276', 'examples_per_second': '31.258', 'grad_norm': '58.5', 'counters/examples': 168576, 'counters/updates': 5268}
train stats after 168608 examples: {'rewards_train/chosen': '0.10827', 'rewards_train/rejected': '0.0084161', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099855', 'logps_train/rejected': '-105.18', 'logps_train/chosen': '-129.74', 'loss/train': '0.65261', 'examples_per_second': '30.669', 'grad_norm': '30.25', 'counters/examples': 168608, 'counters/updates': 5269}
train stats after 168640 examples: {'rewards_train/chosen': '0.10775', 'rewards_train/rejected': '0.011912', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.09584', 'logps_train/rejected': '-100.23', 'logps_train/chosen': '-112.37', 'loss/train': '0.64973', 'examples_per_second': '30.577', 'grad_norm': '26.25', 'counters/examples': 168640, 'counters/updates': 5270}
train stats after 168672 examples: {'rewards_train/chosen': '0.10739', 'rewards_train/rejected': '0.092407', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014985', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-157.82', 'loss/train': '0.69527', 'examples_per_second': '31.446', 'grad_norm': '36', 'counters/examples': 168672, 'counters/updates': 5271}
train stats after 168704 examples: {'rewards_train/chosen': '0.083018', 'rewards_train/rejected': '0.02558', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057438', 'logps_train/rejected': '-135.75', 'logps_train/chosen': '-138.36', 'loss/train': '0.6789', 'examples_per_second': '31.502', 'grad_norm': '28.5', 'counters/examples': 168704, 'counters/updates': 5272}
train stats after 168736 examples: {'rewards_train/chosen': '0.23345', 'rewards_train/rejected': '0.061155', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1723', 'logps_train/rejected': '-167.71', 'logps_train/chosen': '-203.04', 'loss/train': '0.62343', 'examples_per_second': '31.037', 'grad_norm': '34.25', 'counters/examples': 168736, 'counters/updates': 5273}
train stats after 168768 examples: {'rewards_train/chosen': '0.16458', 'rewards_train/rejected': '-0.0051474', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16972', 'logps_train/rejected': '-111.65', 'logps_train/chosen': '-133.98', 'loss/train': '0.62165', 'examples_per_second': '31.534', 'grad_norm': '30.375', 'counters/examples': 168768, 'counters/updates': 5274}
skipping logging after 168800 examples to avoid logging too frequently
train stats after 168832 examples: {'rewards_train/chosen': '0.19195', 'rewards_train/rejected': '0.149', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042948', 'logps_train/rejected': '-184.25', 'logps_train/chosen': '-156.64', 'loss/train': '0.68271', 'examples_per_second': '30.339', 'grad_norm': '56.75', 'counters/examples': 168832, 'counters/updates': 5276}
train stats after 168864 examples: {'rewards_train/chosen': '0.15734', 'rewards_train/rejected': '0.10956', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047783', 'logps_train/rejected': '-149.57', 'logps_train/chosen': '-125.52', 'loss/train': '0.68372', 'examples_per_second': '32.086', 'grad_norm': '31.75', 'counters/examples': 168864, 'counters/updates': 5277}
train stats after 168896 examples: {'rewards_train/chosen': '0.065365', 'rewards_train/rejected': '0.12229', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.056928', 'logps_train/rejected': '-108.63', 'logps_train/chosen': '-131', 'loss/train': '0.72957', 'examples_per_second': '30.888', 'grad_norm': '31.75', 'counters/examples': 168896, 'counters/updates': 5278}
train stats after 168928 examples: {'rewards_train/chosen': '0.036077', 'rewards_train/rejected': '0.033321', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0027559', 'logps_train/rejected': '-79.954', 'logps_train/chosen': '-99.871', 'loss/train': '0.70639', 'examples_per_second': '31.5', 'grad_norm': '29.125', 'counters/examples': 168928, 'counters/updates': 5279}
train stats after 168960 examples: {'rewards_train/chosen': '0.21386', 'rewards_train/rejected': '0.070678', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14319', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-158.69', 'loss/train': '0.66337', 'examples_per_second': '31.482', 'grad_norm': '38.5', 'counters/examples': 168960, 'counters/updates': 5280}
train stats after 168992 examples: {'rewards_train/chosen': '0.17388', 'rewards_train/rejected': '0.12201', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051879', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-124.99', 'loss/train': '0.68634', 'examples_per_second': '31.753', 'grad_norm': '31.75', 'counters/examples': 168992, 'counters/updates': 5281}
train stats after 169024 examples: {'rewards_train/chosen': '0.15356', 'rewards_train/rejected': '0.038753', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11481', 'logps_train/rejected': '-109.88', 'logps_train/chosen': '-153.99', 'loss/train': '0.64806', 'examples_per_second': '31.752', 'grad_norm': '29.5', 'counters/examples': 169024, 'counters/updates': 5282}
train stats after 169056 examples: {'rewards_train/chosen': '0.076583', 'rewards_train/rejected': '0.044306', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032277', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-184.98', 'loss/train': '0.68652', 'examples_per_second': '32.163', 'grad_norm': '32', 'counters/examples': 169056, 'counters/updates': 5283}
skipping logging after 169088 examples to avoid logging too frequently
train stats after 169120 examples: {'rewards_train/chosen': '0.15812', 'rewards_train/rejected': '0.022015', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13611', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-143.76', 'loss/train': '0.6406', 'examples_per_second': '30.589', 'grad_norm': '40.75', 'counters/examples': 169120, 'counters/updates': 5285}
train stats after 169152 examples: {'rewards_train/chosen': '0.12027', 'rewards_train/rejected': '0.020139', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10013', 'logps_train/rejected': '-131', 'logps_train/chosen': '-140.94', 'loss/train': '0.65418', 'examples_per_second': '32.188', 'grad_norm': '31.5', 'counters/examples': 169152, 'counters/updates': 5286}
train stats after 169184 examples: {'rewards_train/chosen': '0.17143', 'rewards_train/rejected': '0.070268', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10116', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-167.21', 'loss/train': '0.66169', 'examples_per_second': '32.2', 'grad_norm': '32.5', 'counters/examples': 169184, 'counters/updates': 5287}
train stats after 169216 examples: {'rewards_train/chosen': '0.079616', 'rewards_train/rejected': '0.025789', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053827', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-119.83', 'loss/train': '0.67163', 'examples_per_second': '31.457', 'grad_norm': '26.5', 'counters/examples': 169216, 'counters/updates': 5288}
train stats after 169248 examples: {'rewards_train/chosen': '0.12337', 'rewards_train/rejected': '0.03866', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084715', 'logps_train/rejected': '-175.47', 'logps_train/chosen': '-160.4', 'loss/train': '0.66139', 'examples_per_second': '31.335', 'grad_norm': '36.5', 'counters/examples': 169248, 'counters/updates': 5289}
skipping logging after 169280 examples to avoid logging too frequently
train stats after 169312 examples: {'rewards_train/chosen': '0.12858', 'rewards_train/rejected': '0.086997', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04158', 'logps_train/rejected': '-107.04', 'logps_train/chosen': '-145.54', 'loss/train': '0.68669', 'examples_per_second': '31.458', 'grad_norm': '33.75', 'counters/examples': 169312, 'counters/updates': 5291}
train stats after 169344 examples: {'rewards_train/chosen': '0.29781', 'rewards_train/rejected': '0.093176', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20464', 'logps_train/rejected': '-162.31', 'logps_train/chosen': '-184.64', 'loss/train': '0.61296', 'examples_per_second': '32.169', 'grad_norm': '32.75', 'counters/examples': 169344, 'counters/updates': 5292}
train stats after 169376 examples: {'rewards_train/chosen': '0.12915', 'rewards_train/rejected': '0.093284', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035867', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-128.43', 'loss/train': '0.68365', 'examples_per_second': '30.966', 'grad_norm': '36.5', 'counters/examples': 169376, 'counters/updates': 5293}
train stats after 169408 examples: {'rewards_train/chosen': '0.12634', 'rewards_train/rejected': '0.096317', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030025', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-116.13', 'loss/train': '0.69254', 'examples_per_second': '30.477', 'grad_norm': '24.25', 'counters/examples': 169408, 'counters/updates': 5294}
train stats after 169440 examples: {'rewards_train/chosen': '0.09388', 'rewards_train/rejected': '0.0062177', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087662', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-154.76', 'loss/train': '0.66421', 'examples_per_second': '31.416', 'grad_norm': '35.75', 'counters/examples': 169440, 'counters/updates': 5295}
skipping logging after 169472 examples to avoid logging too frequently
train stats after 169504 examples: {'rewards_train/chosen': '0.066346', 'rewards_train/rejected': '0.10933', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.042984', 'logps_train/rejected': '-127.16', 'logps_train/chosen': '-148.46', 'loss/train': '0.72641', 'examples_per_second': '30.632', 'grad_norm': '43', 'counters/examples': 169504, 'counters/updates': 5297}
train stats after 169536 examples: {'rewards_train/chosen': '0.078657', 'rewards_train/rejected': '-0.014523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093181', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-113.95', 'loss/train': '0.66538', 'examples_per_second': '31.23', 'grad_norm': '35.5', 'counters/examples': 169536, 'counters/updates': 5298}
train stats after 169568 examples: {'rewards_train/chosen': '0.095659', 'rewards_train/rejected': '0.11831', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022646', 'logps_train/rejected': '-137.22', 'logps_train/chosen': '-156.13', 'loss/train': '0.71528', 'examples_per_second': '31.338', 'grad_norm': '58', 'counters/examples': 169568, 'counters/updates': 5299}
train stats after 169600 examples: {'rewards_train/chosen': '0.045382', 'rewards_train/rejected': '0.10485', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.059472', 'logps_train/rejected': '-155.86', 'logps_train/chosen': '-115.89', 'loss/train': '0.73164', 'examples_per_second': '32.767', 'grad_norm': '71.5', 'counters/examples': 169600, 'counters/updates': 5300}
skipping logging after 169632 examples to avoid logging too frequently
train stats after 169664 examples: {'rewards_train/chosen': '0.13629', 'rewards_train/rejected': '0.022408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11388', 'logps_train/rejected': '-123.09', 'logps_train/chosen': '-148.07', 'loss/train': '0.66245', 'examples_per_second': '36.881', 'grad_norm': '28', 'counters/examples': 169664, 'counters/updates': 5302}
train stats after 169696 examples: {'rewards_train/chosen': '0.1642', 'rewards_train/rejected': '-0.021635', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18584', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-140.47', 'loss/train': '0.6258', 'examples_per_second': '32.589', 'grad_norm': '27', 'counters/examples': 169696, 'counters/updates': 5303}
train stats after 169728 examples: {'rewards_train/chosen': '0.21328', 'rewards_train/rejected': '0.034629', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17865', 'logps_train/rejected': '-110.33', 'logps_train/chosen': '-150.41', 'loss/train': '0.62629', 'examples_per_second': '31.51', 'grad_norm': '34', 'counters/examples': 169728, 'counters/updates': 5304}
train stats after 169760 examples: {'rewards_train/chosen': '0.16351', 'rewards_train/rejected': '0.090956', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072552', 'logps_train/rejected': '-110.35', 'logps_train/chosen': '-127.26', 'loss/train': '0.66932', 'examples_per_second': '30.859', 'grad_norm': '42.75', 'counters/examples': 169760, 'counters/updates': 5305}
train stats after 169792 examples: {'rewards_train/chosen': '0.14298', 'rewards_train/rejected': '0.060661', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082315', 'logps_train/rejected': '-120.41', 'logps_train/chosen': '-138.69', 'loss/train': '0.66406', 'examples_per_second': '32.829', 'grad_norm': '35.5', 'counters/examples': 169792, 'counters/updates': 5306}
skipping logging after 169824 examples to avoid logging too frequently
train stats after 169856 examples: {'rewards_train/chosen': '0.21846', 'rewards_train/rejected': '0.077837', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14062', 'logps_train/rejected': '-119.17', 'logps_train/chosen': '-141.18', 'loss/train': '0.63443', 'examples_per_second': '31.476', 'grad_norm': '29.5', 'counters/examples': 169856, 'counters/updates': 5308}
skipping logging after 169888 examples to avoid logging too frequently
train stats after 169920 examples: {'rewards_train/chosen': '0.21881', 'rewards_train/rejected': '0.024195', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19462', 'logps_train/rejected': '-135.91', 'logps_train/chosen': '-147.46', 'loss/train': '0.61429', 'examples_per_second': '30.769', 'grad_norm': '42.25', 'counters/examples': 169920, 'counters/updates': 5310}
skipping logging after 169952 examples to avoid logging too frequently
train stats after 169984 examples: {'rewards_train/chosen': '0.19344', 'rewards_train/rejected': '0.048774', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14467', 'logps_train/rejected': '-119.57', 'logps_train/chosen': '-138.09', 'loss/train': '0.63585', 'examples_per_second': '32.727', 'grad_norm': '31.75', 'counters/examples': 169984, 'counters/updates': 5312}
train stats after 170016 examples: {'rewards_train/chosen': '0.12213', 'rewards_train/rejected': '-0.009903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13203', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-157.23', 'loss/train': '0.63872', 'examples_per_second': '30.261', 'grad_norm': '27.375', 'counters/examples': 170016, 'counters/updates': 5313}
skipping logging after 170048 examples to avoid logging too frequently
train stats after 170080 examples: {'rewards_train/chosen': '0.097569', 'rewards_train/rejected': '0.092782', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0047866', 'logps_train/rejected': '-136.37', 'logps_train/chosen': '-82.703', 'loss/train': '0.71635', 'examples_per_second': '34.798', 'grad_norm': '61.5', 'counters/examples': 170080, 'counters/updates': 5315}
train stats after 170112 examples: {'rewards_train/chosen': '0.074387', 'rewards_train/rejected': '-0.070344', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14473', 'logps_train/rejected': '-145.42', 'logps_train/chosen': '-141.08', 'loss/train': '0.63643', 'examples_per_second': '30.218', 'grad_norm': '29.375', 'counters/examples': 170112, 'counters/updates': 5316}
train stats after 170144 examples: {'rewards_train/chosen': '0.14383', 'rewards_train/rejected': '0.0039581', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13988', 'logps_train/rejected': '-119.94', 'logps_train/chosen': '-152.01', 'loss/train': '0.64199', 'examples_per_second': '30.646', 'grad_norm': '29.625', 'counters/examples': 170144, 'counters/updates': 5317}
train stats after 170176 examples: {'rewards_train/chosen': '0.24002', 'rewards_train/rejected': '0.12932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1107', 'logps_train/rejected': '-109.18', 'logps_train/chosen': '-166.77', 'loss/train': '0.66661', 'examples_per_second': '31.316', 'grad_norm': '46.75', 'counters/examples': 170176, 'counters/updates': 5318}
train stats after 170208 examples: {'rewards_train/chosen': '0.19995', 'rewards_train/rejected': '0.075009', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12495', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-135.43', 'loss/train': '0.64734', 'examples_per_second': '31.594', 'grad_norm': '26.625', 'counters/examples': 170208, 'counters/updates': 5319}
skipping logging after 170240 examples to avoid logging too frequently
train stats after 170272 examples: {'rewards_train/chosen': '0.14954', 'rewards_train/rejected': '-0.017168', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1667', 'logps_train/rejected': '-96.851', 'logps_train/chosen': '-142.8', 'loss/train': '0.62042', 'examples_per_second': '34.077', 'grad_norm': '25.5', 'counters/examples': 170272, 'counters/updates': 5321}
train stats after 170304 examples: {'rewards_train/chosen': '0.086009', 'rewards_train/rejected': '0.06325', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022759', 'logps_train/rejected': '-112.55', 'logps_train/chosen': '-154.01', 'loss/train': '0.70192', 'examples_per_second': '31.631', 'grad_norm': '45.5', 'counters/examples': 170304, 'counters/updates': 5322}
skipping logging after 170336 examples to avoid logging too frequently
train stats after 170368 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.026681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09708', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-134.73', 'loss/train': '0.65518', 'examples_per_second': '31.613', 'grad_norm': '28.25', 'counters/examples': 170368, 'counters/updates': 5324}
train stats after 170400 examples: {'rewards_train/chosen': '0.084707', 'rewards_train/rejected': '-0.013997', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098704', 'logps_train/rejected': '-127.77', 'logps_train/chosen': '-143.33', 'loss/train': '0.66715', 'examples_per_second': '31.503', 'grad_norm': '31.5', 'counters/examples': 170400, 'counters/updates': 5325}
train stats after 170432 examples: {'rewards_train/chosen': '0.21343', 'rewards_train/rejected': '0.1177', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.095727', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-138.28', 'loss/train': '0.65723', 'examples_per_second': '31.375', 'grad_norm': '39.75', 'counters/examples': 170432, 'counters/updates': 5326}
train stats after 170464 examples: {'rewards_train/chosen': '0.17616', 'rewards_train/rejected': '0.041501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13466', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-149.99', 'loss/train': '0.64156', 'examples_per_second': '30.377', 'grad_norm': '27.375', 'counters/examples': 170464, 'counters/updates': 5327}
train stats after 170496 examples: {'rewards_train/chosen': '0.08394', 'rewards_train/rejected': '0.0641', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.01984', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-121.96', 'loss/train': '0.69195', 'examples_per_second': '31.302', 'grad_norm': '35.25', 'counters/examples': 170496, 'counters/updates': 5328}
train stats after 170528 examples: {'rewards_train/chosen': '0.1002', 'rewards_train/rejected': '0.0089307', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.091271', 'logps_train/rejected': '-104', 'logps_train/chosen': '-144.51', 'loss/train': '0.65845', 'examples_per_second': '31.39', 'grad_norm': '32.25', 'counters/examples': 170528, 'counters/updates': 5329}
train stats after 170560 examples: {'rewards_train/chosen': '0.081526', 'rewards_train/rejected': '-0.015498', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097024', 'logps_train/rejected': '-95.402', 'logps_train/chosen': '-148.72', 'loss/train': '0.6578', 'examples_per_second': '31.814', 'grad_norm': '28.375', 'counters/examples': 170560, 'counters/updates': 5330}
train stats after 170592 examples: {'rewards_train/chosen': '0.17516', 'rewards_train/rejected': '0.10528', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069881', 'logps_train/rejected': '-128.33', 'logps_train/chosen': '-141.86', 'loss/train': '0.66875', 'examples_per_second': '31.514', 'grad_norm': '29.375', 'counters/examples': 170592, 'counters/updates': 5331}
train stats after 170624 examples: {'rewards_train/chosen': '0.14777', 'rewards_train/rejected': '0.08189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065883', 'logps_train/rejected': '-114.15', 'logps_train/chosen': '-124.29', 'loss/train': '0.67202', 'examples_per_second': '31.633', 'grad_norm': '26.875', 'counters/examples': 170624, 'counters/updates': 5332}
train stats after 170656 examples: {'rewards_train/chosen': '0.21795', 'rewards_train/rejected': '0.13957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078387', 'logps_train/rejected': '-151.55', 'logps_train/chosen': '-130.65', 'loss/train': '0.66766', 'examples_per_second': '31.868', 'grad_norm': '30.25', 'counters/examples': 170656, 'counters/updates': 5333}
train stats after 170688 examples: {'rewards_train/chosen': '0.11866', 'rewards_train/rejected': '0.049965', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068697', 'logps_train/rejected': '-102.12', 'logps_train/chosen': '-131.39', 'loss/train': '0.66557', 'examples_per_second': '32.347', 'grad_norm': '35', 'counters/examples': 170688, 'counters/updates': 5334}
train stats after 170720 examples: {'rewards_train/chosen': '0.10312', 'rewards_train/rejected': '0.039492', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063628', 'logps_train/rejected': '-149.89', 'logps_train/chosen': '-158.01', 'loss/train': '0.67942', 'examples_per_second': '31.131', 'grad_norm': '48.5', 'counters/examples': 170720, 'counters/updates': 5335}
train stats after 170752 examples: {'rewards_train/chosen': '0.091584', 'rewards_train/rejected': '0.13487', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.043284', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-142.54', 'loss/train': '0.72457', 'examples_per_second': '31.517', 'grad_norm': '28.875', 'counters/examples': 170752, 'counters/updates': 5336}
train stats after 170784 examples: {'rewards_train/chosen': '0.17408', 'rewards_train/rejected': '0.12581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048278', 'logps_train/rejected': '-167.65', 'logps_train/chosen': '-144.28', 'loss/train': '0.67719', 'examples_per_second': '31.21', 'grad_norm': '32.75', 'counters/examples': 170784, 'counters/updates': 5337}
train stats after 170816 examples: {'rewards_train/chosen': '0.10987', 'rewards_train/rejected': '0.036513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073358', 'logps_train/rejected': '-131.77', 'logps_train/chosen': '-117.1', 'loss/train': '0.66697', 'examples_per_second': '31.491', 'grad_norm': '29', 'counters/examples': 170816, 'counters/updates': 5338}
skipping logging after 170848 examples to avoid logging too frequently
train stats after 170880 examples: {'rewards_train/chosen': '0.24926', 'rewards_train/rejected': '0.028415', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22084', 'logps_train/rejected': '-101.32', 'logps_train/chosen': '-122.26', 'loss/train': '0.59859', 'examples_per_second': '43.208', 'grad_norm': '28.75', 'counters/examples': 170880, 'counters/updates': 5340}
train stats after 170912 examples: {'rewards_train/chosen': '0.15744', 'rewards_train/rejected': '0.058474', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098962', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-135.99', 'loss/train': '0.65505', 'examples_per_second': '32.341', 'grad_norm': '31.375', 'counters/examples': 170912, 'counters/updates': 5341}
train stats after 170944 examples: {'rewards_train/chosen': '0.12617', 'rewards_train/rejected': '0.075055', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051114', 'logps_train/rejected': '-105.23', 'logps_train/chosen': '-114.94', 'loss/train': '0.67528', 'examples_per_second': '31.885', 'grad_norm': '22.25', 'counters/examples': 170944, 'counters/updates': 5342}
train stats after 170976 examples: {'rewards_train/chosen': '0.10873', 'rewards_train/rejected': '-0.041466', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1502', 'logps_train/rejected': '-102.84', 'logps_train/chosen': '-123.26', 'loss/train': '0.62792', 'examples_per_second': '31.059', 'grad_norm': '31', 'counters/examples': 170976, 'counters/updates': 5343}
train stats after 171008 examples: {'rewards_train/chosen': '0.17208', 'rewards_train/rejected': '0.065816', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10627', 'logps_train/rejected': '-120.31', 'logps_train/chosen': '-127.06', 'loss/train': '0.65008', 'examples_per_second': '31.743', 'grad_norm': '29.25', 'counters/examples': 171008, 'counters/updates': 5344}
train stats after 171040 examples: {'rewards_train/chosen': '0.20328', 'rewards_train/rejected': '0.060481', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1428', 'logps_train/rejected': '-194.64', 'logps_train/chosen': '-179.22', 'loss/train': '0.6373', 'examples_per_second': '31.507', 'grad_norm': '37', 'counters/examples': 171040, 'counters/updates': 5345}
train stats after 171072 examples: {'rewards_train/chosen': '0.14844', 'rewards_train/rejected': '0.12152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02692', 'logps_train/rejected': '-115.03', 'logps_train/chosen': '-166.85', 'loss/train': '0.68599', 'examples_per_second': '31.65', 'grad_norm': '32.25', 'counters/examples': 171072, 'counters/updates': 5346}
skipping logging after 171104 examples to avoid logging too frequently
train stats after 171136 examples: {'rewards_train/chosen': '0.08154', 'rewards_train/rejected': '0.0060733', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.075467', 'logps_train/rejected': '-122.58', 'logps_train/chosen': '-148.32', 'loss/train': '0.6657', 'examples_per_second': '31.457', 'grad_norm': '31.25', 'counters/examples': 171136, 'counters/updates': 5348}
train stats after 171168 examples: {'rewards_train/chosen': '0.08422', 'rewards_train/rejected': '0.033867', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050353', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-106.33', 'loss/train': '0.674', 'examples_per_second': '33.151', 'grad_norm': '25.75', 'counters/examples': 171168, 'counters/updates': 5349}
train stats after 171200 examples: {'rewards_train/chosen': '0.12739', 'rewards_train/rejected': '0.054044', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.073345', 'logps_train/rejected': '-132.55', 'logps_train/chosen': '-158.94', 'loss/train': '0.67588', 'examples_per_second': '31.526', 'grad_norm': '30.625', 'counters/examples': 171200, 'counters/updates': 5350}
train stats after 171232 examples: {'rewards_train/chosen': '0.16341', 'rewards_train/rejected': '0.033609', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1298', 'logps_train/rejected': '-117.7', 'logps_train/chosen': '-158.89', 'loss/train': '0.64003', 'examples_per_second': '31.512', 'grad_norm': '28.25', 'counters/examples': 171232, 'counters/updates': 5351}
train stats after 171264 examples: {'rewards_train/chosen': '0.13819', 'rewards_train/rejected': '0.20407', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.065873', 'logps_train/rejected': '-173.28', 'logps_train/chosen': '-163.62', 'loss/train': '0.78173', 'examples_per_second': '32.035', 'grad_norm': '72.5', 'counters/examples': 171264, 'counters/updates': 5352}
train stats after 171296 examples: {'rewards_train/chosen': '0.16441', 'rewards_train/rejected': '0.05435', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11006', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-118.04', 'loss/train': '0.64997', 'examples_per_second': '31.947', 'grad_norm': '22.75', 'counters/examples': 171296, 'counters/updates': 5353}
train stats after 171328 examples: {'rewards_train/chosen': '0.13911', 'rewards_train/rejected': '0.062413', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.076693', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-137.11', 'loss/train': '0.6657', 'examples_per_second': '30.435', 'grad_norm': '34.25', 'counters/examples': 171328, 'counters/updates': 5354}
train stats after 171360 examples: {'rewards_train/chosen': '0.22952', 'rewards_train/rejected': '0.01437', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21515', 'logps_train/rejected': '-146.35', 'logps_train/chosen': '-162.72', 'loss/train': '0.60939', 'examples_per_second': '31.675', 'grad_norm': '36.25', 'counters/examples': 171360, 'counters/updates': 5355}
skipping logging after 171392 examples to avoid logging too frequently
train stats after 171424 examples: {'rewards_train/chosen': '0.19163', 'rewards_train/rejected': '0.053239', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13839', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-170.19', 'loss/train': '0.63628', 'examples_per_second': '31.411', 'grad_norm': '35.5', 'counters/examples': 171424, 'counters/updates': 5357}
train stats after 171456 examples: {'rewards_train/chosen': '0.077491', 'rewards_train/rejected': '0.033907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043584', 'logps_train/rejected': '-115.26', 'logps_train/chosen': '-145.14', 'loss/train': '0.67958', 'examples_per_second': '33.161', 'grad_norm': '27.625', 'counters/examples': 171456, 'counters/updates': 5358}
train stats after 171488 examples: {'rewards_train/chosen': '0.2659', 'rewards_train/rejected': '0.075456', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19044', 'logps_train/rejected': '-195.06', 'logps_train/chosen': '-199.06', 'loss/train': '0.61492', 'examples_per_second': '24.096', 'grad_norm': '36.5', 'counters/examples': 171488, 'counters/updates': 5359}
train stats after 171520 examples: {'rewards_train/chosen': '0.23413', 'rewards_train/rejected': '0.10724', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12689', 'logps_train/rejected': '-110.29', 'logps_train/chosen': '-140.8', 'loss/train': '0.66052', 'examples_per_second': '29.882', 'grad_norm': '27.375', 'counters/examples': 171520, 'counters/updates': 5360}
train stats after 171552 examples: {'rewards_train/chosen': '0.16564', 'rewards_train/rejected': '0.17517', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0095255', 'logps_train/rejected': '-152.27', 'logps_train/chosen': '-138.26', 'loss/train': '0.72095', 'examples_per_second': '30.491', 'grad_norm': '54', 'counters/examples': 171552, 'counters/updates': 5361}
train stats after 171584 examples: {'rewards_train/chosen': '0.090259', 'rewards_train/rejected': '0.097776', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0075175', 'logps_train/rejected': '-111.29', 'logps_train/chosen': '-171.17', 'loss/train': '0.70949', 'examples_per_second': '30.123', 'grad_norm': '43.5', 'counters/examples': 171584, 'counters/updates': 5362}
train stats after 171616 examples: {'rewards_train/chosen': '0.098243', 'rewards_train/rejected': '0.076896', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021347', 'logps_train/rejected': '-164.09', 'logps_train/chosen': '-125.88', 'loss/train': '0.69331', 'examples_per_second': '30.544', 'grad_norm': '30', 'counters/examples': 171616, 'counters/updates': 5363}
train stats after 171648 examples: {'rewards_train/chosen': '0.19659', 'rewards_train/rejected': '0.054921', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14167', 'logps_train/rejected': '-137.42', 'logps_train/chosen': '-170.44', 'loss/train': '0.63793', 'examples_per_second': '31.846', 'grad_norm': '30.375', 'counters/examples': 171648, 'counters/updates': 5364}
train stats after 171680 examples: {'rewards_train/chosen': '0.22585', 'rewards_train/rejected': '0.065678', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16017', 'logps_train/rejected': '-142.32', 'logps_train/chosen': '-149.05', 'loss/train': '0.62649', 'examples_per_second': '31.526', 'grad_norm': '34.5', 'counters/examples': 171680, 'counters/updates': 5365}
train stats after 171712 examples: {'rewards_train/chosen': '0.15802', 'rewards_train/rejected': '0.041339', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11668', 'logps_train/rejected': '-105.53', 'logps_train/chosen': '-135.54', 'loss/train': '0.6432', 'examples_per_second': '32.597', 'grad_norm': '26.25', 'counters/examples': 171712, 'counters/updates': 5366}
train stats after 171744 examples: {'rewards_train/chosen': '0.14248', 'rewards_train/rejected': '0.027908', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11457', 'logps_train/rejected': '-111.16', 'logps_train/chosen': '-130.05', 'loss/train': '0.64428', 'examples_per_second': '32.749', 'grad_norm': '31.875', 'counters/examples': 171744, 'counters/updates': 5367}
train stats after 171776 examples: {'rewards_train/chosen': '0.19774', 'rewards_train/rejected': '0.074412', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12333', 'logps_train/rejected': '-124.88', 'logps_train/chosen': '-135.8', 'loss/train': '0.64143', 'examples_per_second': '31.486', 'grad_norm': '30.375', 'counters/examples': 171776, 'counters/updates': 5368}
train stats after 171808 examples: {'rewards_train/chosen': '0.045423', 'rewards_train/rejected': '0.055013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.00959', 'logps_train/rejected': '-159.93', 'logps_train/chosen': '-160.48', 'loss/train': '0.70857', 'examples_per_second': '32.063', 'grad_norm': '44.75', 'counters/examples': 171808, 'counters/updates': 5369}
train stats after 171840 examples: {'rewards_train/chosen': '0.18869', 'rewards_train/rejected': '0.18242', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.006269', 'logps_train/rejected': '-173.4', 'logps_train/chosen': '-193.28', 'loss/train': '0.71958', 'examples_per_second': '32.017', 'grad_norm': '42.5', 'counters/examples': 171840, 'counters/updates': 5370}
train stats after 171872 examples: {'rewards_train/chosen': '0.12637', 'rewards_train/rejected': '-0.065134', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19151', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-169.84', 'loss/train': '0.60809', 'examples_per_second': '30.475', 'grad_norm': '33.25', 'counters/examples': 171872, 'counters/updates': 5371}
train stats after 171904 examples: {'rewards_train/chosen': '0.23545', 'rewards_train/rejected': '0.091214', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14423', 'logps_train/rejected': '-126.67', 'logps_train/chosen': '-155.57', 'loss/train': '0.64019', 'examples_per_second': '31.876', 'grad_norm': '27', 'counters/examples': 171904, 'counters/updates': 5372}
train stats after 171936 examples: {'rewards_train/chosen': '0.060641', 'rewards_train/rejected': '-0.0005407', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061182', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-195.03', 'loss/train': '0.67101', 'examples_per_second': '31.481', 'grad_norm': '39', 'counters/examples': 171936, 'counters/updates': 5373}
train stats after 171968 examples: {'rewards_train/chosen': '0.14056', 'rewards_train/rejected': '-0.023738', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1643', 'logps_train/rejected': '-95.356', 'logps_train/chosen': '-127.04', 'loss/train': '0.64334', 'examples_per_second': '31.531', 'grad_norm': '26.25', 'counters/examples': 171968, 'counters/updates': 5374}
train stats after 172000 examples: {'rewards_train/chosen': '0.1019', 'rewards_train/rejected': '0.06013', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.041774', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-147.98', 'loss/train': '0.68043', 'examples_per_second': '31.38', 'grad_norm': '29.75', 'counters/examples': 172000, 'counters/updates': 5375}
train stats after 172032 examples: {'rewards_train/chosen': '0.22971', 'rewards_train/rejected': '0.09959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13012', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-174.94', 'loss/train': '0.64138', 'examples_per_second': '32.204', 'grad_norm': '36', 'counters/examples': 172032, 'counters/updates': 5376}
train stats after 172064 examples: {'rewards_train/chosen': '0.22031', 'rewards_train/rejected': '0.073909', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1464', 'logps_train/rejected': '-159.24', 'logps_train/chosen': '-144.76', 'loss/train': '0.63813', 'examples_per_second': '31.475', 'grad_norm': '36.25', 'counters/examples': 172064, 'counters/updates': 5377}
train stats after 172096 examples: {'rewards_train/chosen': '0.027977', 'rewards_train/rejected': '-0.017595', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045572', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-137.52', 'loss/train': '0.67678', 'examples_per_second': '31.52', 'grad_norm': '27', 'counters/examples': 172096, 'counters/updates': 5378}
train stats after 172128 examples: {'rewards_train/chosen': '0.13861', 'rewards_train/rejected': '0.16259', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.023984', 'logps_train/rejected': '-143.75', 'logps_train/chosen': '-175.89', 'loss/train': '0.73683', 'examples_per_second': '32.373', 'grad_norm': '117.5', 'counters/examples': 172128, 'counters/updates': 5379}
train stats after 172160 examples: {'rewards_train/chosen': '0.14288', 'rewards_train/rejected': '0.022768', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12011', 'logps_train/rejected': '-142.67', 'logps_train/chosen': '-139.48', 'loss/train': '0.64427', 'examples_per_second': '29.934', 'grad_norm': '32.75', 'counters/examples': 172160, 'counters/updates': 5380}
train stats after 172192 examples: {'rewards_train/chosen': '0.16592', 'rewards_train/rejected': '0.039776', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12615', 'logps_train/rejected': '-118.89', 'logps_train/chosen': '-152.58', 'loss/train': '0.64289', 'examples_per_second': '31.579', 'grad_norm': '31.75', 'counters/examples': 172192, 'counters/updates': 5381}
train stats after 172224 examples: {'rewards_train/chosen': '0.17403', 'rewards_train/rejected': '0.082564', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091463', 'logps_train/rejected': '-135.14', 'logps_train/chosen': '-145.9', 'loss/train': '0.66653', 'examples_per_second': '32.792', 'grad_norm': '33.25', 'counters/examples': 172224, 'counters/updates': 5382}
train stats after 172256 examples: {'rewards_train/chosen': '0.12237', 'rewards_train/rejected': '0.011138', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11123', 'logps_train/rejected': '-148.06', 'logps_train/chosen': '-119.43', 'loss/train': '0.65705', 'examples_per_second': '31.263', 'grad_norm': '34', 'counters/examples': 172256, 'counters/updates': 5383}
train stats after 172288 examples: {'rewards_train/chosen': '0.099584', 'rewards_train/rejected': '0.031898', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.067685', 'logps_train/rejected': '-150.75', 'logps_train/chosen': '-146.87', 'loss/train': '0.67435', 'examples_per_second': '31.284', 'grad_norm': '38.75', 'counters/examples': 172288, 'counters/updates': 5384}
train stats after 172320 examples: {'rewards_train/chosen': '0.266', 'rewards_train/rejected': '0.075083', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19091', 'logps_train/rejected': '-140.42', 'logps_train/chosen': '-166.28', 'loss/train': '0.61007', 'examples_per_second': '32.051', 'grad_norm': '33.25', 'counters/examples': 172320, 'counters/updates': 5385}
skipping logging after 172352 examples to avoid logging too frequently
train stats after 172384 examples: {'rewards_train/chosen': '0.13326', 'rewards_train/rejected': '0.10414', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029124', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-118.84', 'loss/train': '0.6849', 'examples_per_second': '35.816', 'grad_norm': '29.125', 'counters/examples': 172384, 'counters/updates': 5387}
train stats after 172416 examples: {'rewards_train/chosen': '0.11473', 'rewards_train/rejected': '0.0010071', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11373', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-147.93', 'loss/train': '0.64396', 'examples_per_second': '30.904', 'grad_norm': '27.625', 'counters/examples': 172416, 'counters/updates': 5388}
skipping logging after 172448 examples to avoid logging too frequently
train stats after 172480 examples: {'rewards_train/chosen': '0.17324', 'rewards_train/rejected': '0.011637', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16161', 'logps_train/rejected': '-131.19', 'logps_train/chosen': '-151.3', 'loss/train': '0.63703', 'examples_per_second': '30.786', 'grad_norm': '34.25', 'counters/examples': 172480, 'counters/updates': 5390}
train stats after 172512 examples: {'rewards_train/chosen': '0.30113', 'rewards_train/rejected': '0.066853', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23428', 'logps_train/rejected': '-104.02', 'logps_train/chosen': '-191.17', 'loss/train': '0.60706', 'examples_per_second': '30.402', 'grad_norm': '27.5', 'counters/examples': 172512, 'counters/updates': 5391}
train stats after 172544 examples: {'rewards_train/chosen': '0.19595', 'rewards_train/rejected': '0.032758', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16319', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-128.73', 'loss/train': '0.64033', 'examples_per_second': '31.107', 'grad_norm': '28.625', 'counters/examples': 172544, 'counters/updates': 5392}
train stats after 172576 examples: {'rewards_train/chosen': '0.16465', 'rewards_train/rejected': '0.14372', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.020929', 'logps_train/rejected': '-173.06', 'logps_train/chosen': '-155.76', 'loss/train': '0.69492', 'examples_per_second': '29.857', 'grad_norm': '40.25', 'counters/examples': 172576, 'counters/updates': 5393}
train stats after 172608 examples: {'rewards_train/chosen': '0.091627', 'rewards_train/rejected': '0.010141', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.081486', 'logps_train/rejected': '-95.112', 'logps_train/chosen': '-137.41', 'loss/train': '0.66465', 'examples_per_second': '29.9', 'grad_norm': '25.125', 'counters/examples': 172608, 'counters/updates': 5394}
skipping logging after 172640 examples to avoid logging too frequently
train stats after 172672 examples: {'rewards_train/chosen': '0.20008', 'rewards_train/rejected': '-0.0011698', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20125', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-147.65', 'loss/train': '0.61624', 'examples_per_second': '32.748', 'grad_norm': '28.125', 'counters/examples': 172672, 'counters/updates': 5396}
train stats after 172704 examples: {'rewards_train/chosen': '0.18811', 'rewards_train/rejected': '0.035085', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15302', 'logps_train/rejected': '-132.36', 'logps_train/chosen': '-169.88', 'loss/train': '0.63998', 'examples_per_second': '31.491', 'grad_norm': '42.25', 'counters/examples': 172704, 'counters/updates': 5397}
train stats after 172736 examples: {'rewards_train/chosen': '0.10234', 'rewards_train/rejected': '0.03785', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064489', 'logps_train/rejected': '-128.81', 'logps_train/chosen': '-154.24', 'loss/train': '0.66945', 'examples_per_second': '31.511', 'grad_norm': '29.5', 'counters/examples': 172736, 'counters/updates': 5398}
train stats after 172768 examples: {'rewards_train/chosen': '0.10948', 'rewards_train/rejected': '0.018585', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090899', 'logps_train/rejected': '-104.63', 'logps_train/chosen': '-133.03', 'loss/train': '0.65649', 'examples_per_second': '31.434', 'grad_norm': '24.25', 'counters/examples': 172768, 'counters/updates': 5399}
train stats after 172800 examples: {'rewards_train/chosen': '0.20025', 'rewards_train/rejected': '0.068253', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.132', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-133.15', 'loss/train': '0.6371', 'examples_per_second': '30.276', 'grad_norm': '32.5', 'counters/examples': 172800, 'counters/updates': 5400}
train stats after 172832 examples: {'rewards_train/chosen': '0.17791', 'rewards_train/rejected': '0.082774', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.095131', 'logps_train/rejected': '-163.32', 'logps_train/chosen': '-155.06', 'loss/train': '0.65361', 'examples_per_second': '30.586', 'grad_norm': '37.75', 'counters/examples': 172832, 'counters/updates': 5401}
train stats after 172864 examples: {'rewards_train/chosen': '0.17197', 'rewards_train/rejected': '0.12177', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.050199', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-122.26', 'loss/train': '0.68281', 'examples_per_second': '32.599', 'grad_norm': '30.625', 'counters/examples': 172864, 'counters/updates': 5402}
train stats after 172896 examples: {'rewards_train/chosen': '0.17728', 'rewards_train/rejected': '0.062209', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11507', 'logps_train/rejected': '-122.22', 'logps_train/chosen': '-153.21', 'loss/train': '0.64844', 'examples_per_second': '31.501', 'grad_norm': '31.25', 'counters/examples': 172896, 'counters/updates': 5403}
train stats after 172928 examples: {'rewards_train/chosen': '0.14016', 'rewards_train/rejected': '0.050683', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089476', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-131.08', 'loss/train': '0.65987', 'examples_per_second': '31.251', 'grad_norm': '25', 'counters/examples': 172928, 'counters/updates': 5404}
train stats after 172960 examples: {'rewards_train/chosen': '0.1245', 'rewards_train/rejected': '0.0046543', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11985', 'logps_train/rejected': '-123.92', 'logps_train/chosen': '-140.65', 'loss/train': '0.64263', 'examples_per_second': '29.982', 'grad_norm': '34', 'counters/examples': 172960, 'counters/updates': 5405}
skipping logging after 172992 examples to avoid logging too frequently
train stats after 173024 examples: {'rewards_train/chosen': '0.10272', 'rewards_train/rejected': '-0.049858', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15258', 'logps_train/rejected': '-130.5', 'logps_train/chosen': '-186.83', 'loss/train': '0.63421', 'examples_per_second': '30.993', 'grad_norm': '33.25', 'counters/examples': 173024, 'counters/updates': 5407}
train stats after 173056 examples: {'rewards_train/chosen': '0.12963', 'rewards_train/rejected': '0.065727', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063899', 'logps_train/rejected': '-155.33', 'logps_train/chosen': '-145.19', 'loss/train': '0.67873', 'examples_per_second': '29.921', 'grad_norm': '66.5', 'counters/examples': 173056, 'counters/updates': 5408}
skipping logging after 173088 examples to avoid logging too frequently
train stats after 173120 examples: {'rewards_train/chosen': '0.17177', 'rewards_train/rejected': '0.063413', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10835', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-140.58', 'loss/train': '0.65437', 'examples_per_second': '33.177', 'grad_norm': '31.875', 'counters/examples': 173120, 'counters/updates': 5410}
train stats after 173152 examples: {'rewards_train/chosen': '0.086162', 'rewards_train/rejected': '-0.0057434', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091906', 'logps_train/rejected': '-130.18', 'logps_train/chosen': '-129.48', 'loss/train': '0.6608', 'examples_per_second': '32.208', 'grad_norm': '36.5', 'counters/examples': 173152, 'counters/updates': 5411}
train stats after 173184 examples: {'rewards_train/chosen': '0.15971', 'rewards_train/rejected': '0.0887', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.071009', 'logps_train/rejected': '-128.82', 'logps_train/chosen': '-139.23', 'loss/train': '0.66563', 'examples_per_second': '30.837', 'grad_norm': '35.5', 'counters/examples': 173184, 'counters/updates': 5412}
train stats after 173216 examples: {'rewards_train/chosen': '0.13178', 'rewards_train/rejected': '0.06798', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063804', 'logps_train/rejected': '-98.678', 'logps_train/chosen': '-118.2', 'loss/train': '0.67309', 'examples_per_second': '29.939', 'grad_norm': '25.25', 'counters/examples': 173216, 'counters/updates': 5413}
train stats after 173248 examples: {'rewards_train/chosen': '0.092596', 'rewards_train/rejected': '0.073087', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019509', 'logps_train/rejected': '-132.85', 'logps_train/chosen': '-138.66', 'loss/train': '0.69379', 'examples_per_second': '31.859', 'grad_norm': '37', 'counters/examples': 173248, 'counters/updates': 5414}
train stats after 173280 examples: {'rewards_train/chosen': '0.20797', 'rewards_train/rejected': '0.035149', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17282', 'logps_train/rejected': '-137.41', 'logps_train/chosen': '-164.49', 'loss/train': '0.62632', 'examples_per_second': '30.049', 'grad_norm': '30.5', 'counters/examples': 173280, 'counters/updates': 5415}
train stats after 173312 examples: {'rewards_train/chosen': '0.19265', 'rewards_train/rejected': '0.12324', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069406', 'logps_train/rejected': '-161.75', 'logps_train/chosen': '-181.27', 'loss/train': '0.67821', 'examples_per_second': '30.823', 'grad_norm': '49.25', 'counters/examples': 173312, 'counters/updates': 5416}
train stats after 173344 examples: {'rewards_train/chosen': '0.14096', 'rewards_train/rejected': '0.033365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1076', 'logps_train/rejected': '-119.56', 'logps_train/chosen': '-160.26', 'loss/train': '0.6553', 'examples_per_second': '33.102', 'grad_norm': '36', 'counters/examples': 173344, 'counters/updates': 5417}
train stats after 173376 examples: {'rewards_train/chosen': '0.21632', 'rewards_train/rejected': '0.09403', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12229', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-187.49', 'loss/train': '0.64676', 'examples_per_second': '31.426', 'grad_norm': '38', 'counters/examples': 173376, 'counters/updates': 5418}
train stats after 173408 examples: {'rewards_train/chosen': '0.21019', 'rewards_train/rejected': '0.097548', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11265', 'logps_train/rejected': '-139.47', 'logps_train/chosen': '-162.25', 'loss/train': '0.65549', 'examples_per_second': '31.935', 'grad_norm': '29.5', 'counters/examples': 173408, 'counters/updates': 5419}
skipping logging after 173440 examples to avoid logging too frequently
train stats after 173472 examples: {'rewards_train/chosen': '0.20478', 'rewards_train/rejected': '0.067652', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13712', 'logps_train/rejected': '-126.74', 'logps_train/chosen': '-162.98', 'loss/train': '0.64297', 'examples_per_second': '31.314', 'grad_norm': '34.25', 'counters/examples': 173472, 'counters/updates': 5421}
train stats after 173504 examples: {'rewards_train/chosen': '0.1434', 'rewards_train/rejected': '0.094869', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.048536', 'logps_train/rejected': '-141.48', 'logps_train/chosen': '-166.17', 'loss/train': '0.67721', 'examples_per_second': '31.921', 'grad_norm': '31', 'counters/examples': 173504, 'counters/updates': 5422}
skipping logging after 173536 examples to avoid logging too frequently
train stats after 173568 examples: {'rewards_train/chosen': '0.12949', 'rewards_train/rejected': '0.080167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049323', 'logps_train/rejected': '-141.55', 'logps_train/chosen': '-139.71', 'loss/train': '0.67874', 'examples_per_second': '31.606', 'grad_norm': '34.5', 'counters/examples': 173568, 'counters/updates': 5424}
train stats after 173600 examples: {'rewards_train/chosen': '0.25646', 'rewards_train/rejected': '0.084955', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1715', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-156.78', 'loss/train': '0.62235', 'examples_per_second': '32.516', 'grad_norm': '24.875', 'counters/examples': 173600, 'counters/updates': 5425}
skipping logging after 173632 examples to avoid logging too frequently
train stats after 173664 examples: {'rewards_train/chosen': '0.27044', 'rewards_train/rejected': '0.081837', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1886', 'logps_train/rejected': '-161.58', 'logps_train/chosen': '-179.81', 'loss/train': '0.63523', 'examples_per_second': '29.94', 'grad_norm': '36.5', 'counters/examples': 173664, 'counters/updates': 5427}
train stats after 173696 examples: {'rewards_train/chosen': '0.046888', 'rewards_train/rejected': '0.03121', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.015678', 'logps_train/rejected': '-114.32', 'logps_train/chosen': '-140.39', 'loss/train': '0.69623', 'examples_per_second': '30.718', 'grad_norm': '36.5', 'counters/examples': 173696, 'counters/updates': 5428}
train stats after 173728 examples: {'rewards_train/chosen': '0.078344', 'rewards_train/rejected': '0.028786', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.049558', 'logps_train/rejected': '-155.92', 'logps_train/chosen': '-135.95', 'loss/train': '0.70042', 'examples_per_second': '31.374', 'grad_norm': '56.5', 'counters/examples': 173728, 'counters/updates': 5429}
train stats after 173760 examples: {'rewards_train/chosen': '0.14629', 'rewards_train/rejected': '0.1517', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0054115', 'logps_train/rejected': '-146.25', 'logps_train/chosen': '-128.64', 'loss/train': '0.70578', 'examples_per_second': '30.998', 'grad_norm': '33', 'counters/examples': 173760, 'counters/updates': 5430}
train stats after 173792 examples: {'rewards_train/chosen': '0.13707', 'rewards_train/rejected': '0.087631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049443', 'logps_train/rejected': '-118.07', 'logps_train/chosen': '-119.36', 'loss/train': '0.67521', 'examples_per_second': '31.41', 'grad_norm': '32.25', 'counters/examples': 173792, 'counters/updates': 5431}
train stats after 173824 examples: {'rewards_train/chosen': '0.16034', 'rewards_train/rejected': '0.029929', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13041', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-152.65', 'loss/train': '0.64233', 'examples_per_second': '24.693', 'grad_norm': '30.25', 'counters/examples': 173824, 'counters/updates': 5432}
train stats after 173856 examples: {'rewards_train/chosen': '0.12443', 'rewards_train/rejected': '0.043178', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081251', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-139.3', 'loss/train': '0.66409', 'examples_per_second': '31.423', 'grad_norm': '43', 'counters/examples': 173856, 'counters/updates': 5433}
train stats after 173888 examples: {'rewards_train/chosen': '0.10624', 'rewards_train/rejected': '0.10721', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.00097106', 'logps_train/rejected': '-131.42', 'logps_train/chosen': '-156.56', 'loss/train': '0.71106', 'examples_per_second': '31.151', 'grad_norm': '36.5', 'counters/examples': 173888, 'counters/updates': 5434}
train stats after 173920 examples: {'rewards_train/chosen': '0.16215', 'rewards_train/rejected': '0.050781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11137', 'logps_train/rejected': '-155.95', 'logps_train/chosen': '-194.89', 'loss/train': '0.65192', 'examples_per_second': '24.732', 'grad_norm': '31.25', 'counters/examples': 173920, 'counters/updates': 5435}
train stats after 173952 examples: {'rewards_train/chosen': '0.00083546', 'rewards_train/rejected': '0.026807', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.025972', 'logps_train/rejected': '-123.29', 'logps_train/chosen': '-153.06', 'loss/train': '0.71746', 'examples_per_second': '31.969', 'grad_norm': '36.25', 'counters/examples': 173952, 'counters/updates': 5436}
train stats after 173984 examples: {'rewards_train/chosen': '0.1925', 'rewards_train/rejected': '0.0030451', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18945', 'logps_train/rejected': '-123.39', 'logps_train/chosen': '-147.48', 'loss/train': '0.62573', 'examples_per_second': '30.649', 'grad_norm': '27.125', 'counters/examples': 173984, 'counters/updates': 5437}
train stats after 174016 examples: {'rewards_train/chosen': '0.11471', 'rewards_train/rejected': '0.078586', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.036127', 'logps_train/rejected': '-163.7', 'logps_train/chosen': '-182.5', 'loss/train': '0.68981', 'examples_per_second': '32.339', 'grad_norm': '33.75', 'counters/examples': 174016, 'counters/updates': 5438}
train stats after 174048 examples: {'rewards_train/chosen': '0.10945', 'rewards_train/rejected': '-0.064537', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17399', 'logps_train/rejected': '-124', 'logps_train/chosen': '-130.67', 'loss/train': '0.62906', 'examples_per_second': '31.275', 'grad_norm': '25.5', 'counters/examples': 174048, 'counters/updates': 5439}
train stats after 174080 examples: {'rewards_train/chosen': '0.20593', 'rewards_train/rejected': '0.087814', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11812', 'logps_train/rejected': '-142.72', 'logps_train/chosen': '-159.91', 'loss/train': '0.6524', 'examples_per_second': '31.453', 'grad_norm': '30.625', 'counters/examples': 174080, 'counters/updates': 5440}
train stats after 174112 examples: {'rewards_train/chosen': '0.11774', 'rewards_train/rejected': '0.018891', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098845', 'logps_train/rejected': '-145.41', 'logps_train/chosen': '-118.47', 'loss/train': '0.65424', 'examples_per_second': '31.458', 'grad_norm': '36', 'counters/examples': 174112, 'counters/updates': 5441}
train stats after 174144 examples: {'rewards_train/chosen': '0.14385', 'rewards_train/rejected': '0.040838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10302', 'logps_train/rejected': '-115.41', 'logps_train/chosen': '-131.06', 'loss/train': '0.64986', 'examples_per_second': '29.955', 'grad_norm': '45.25', 'counters/examples': 174144, 'counters/updates': 5442}
train stats after 174176 examples: {'rewards_train/chosen': '0.17368', 'rewards_train/rejected': '0.086126', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087558', 'logps_train/rejected': '-119.21', 'logps_train/chosen': '-138.39', 'loss/train': '0.66484', 'examples_per_second': '31.928', 'grad_norm': '39', 'counters/examples': 174176, 'counters/updates': 5443}
train stats after 174208 examples: {'rewards_train/chosen': '0.13195', 'rewards_train/rejected': '0.070494', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061454', 'logps_train/rejected': '-160.77', 'logps_train/chosen': '-137.38', 'loss/train': '0.67461', 'examples_per_second': '32.967', 'grad_norm': '30.625', 'counters/examples': 174208, 'counters/updates': 5444}
train stats after 174240 examples: {'rewards_train/chosen': '0.15176', 'rewards_train/rejected': '0.035779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11598', 'logps_train/rejected': '-105.74', 'logps_train/chosen': '-154.34', 'loss/train': '0.65364', 'examples_per_second': '31.474', 'grad_norm': '40.5', 'counters/examples': 174240, 'counters/updates': 5445}
train stats after 174272 examples: {'rewards_train/chosen': '0.15309', 'rewards_train/rejected': '0.057934', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.095154', 'logps_train/rejected': '-106.03', 'logps_train/chosen': '-134.54', 'loss/train': '0.6526', 'examples_per_second': '31.275', 'grad_norm': '25', 'counters/examples': 174272, 'counters/updates': 5446}
train stats after 174304 examples: {'rewards_train/chosen': '0.20159', 'rewards_train/rejected': '0.12565', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.075936', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-193.84', 'loss/train': '0.67308', 'examples_per_second': '30.789', 'grad_norm': '31', 'counters/examples': 174304, 'counters/updates': 5447}
train stats after 174336 examples: {'rewards_train/chosen': '0.11331', 'rewards_train/rejected': '0.13533', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022017', 'logps_train/rejected': '-118.46', 'logps_train/chosen': '-144.42', 'loss/train': '0.71449', 'examples_per_second': '32.268', 'grad_norm': '38.5', 'counters/examples': 174336, 'counters/updates': 5448}
train stats after 174368 examples: {'rewards_train/chosen': '0.15693', 'rewards_train/rejected': '0.094617', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06231', 'logps_train/rejected': '-132.49', 'logps_train/chosen': '-175.87', 'loss/train': '0.66985', 'examples_per_second': '31.379', 'grad_norm': '29.125', 'counters/examples': 174368, 'counters/updates': 5449}
train stats after 174400 examples: {'rewards_train/chosen': '0.12878', 'rewards_train/rejected': '0.05613', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072648', 'logps_train/rejected': '-130.31', 'logps_train/chosen': '-134.07', 'loss/train': '0.67236', 'examples_per_second': '31.72', 'grad_norm': '31.625', 'counters/examples': 174400, 'counters/updates': 5450}
train stats after 174432 examples: {'rewards_train/chosen': '0.1928', 'rewards_train/rejected': '0.025065', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16774', 'logps_train/rejected': '-107.89', 'logps_train/chosen': '-157.68', 'loss/train': '0.63201', 'examples_per_second': '30.569', 'grad_norm': '43.25', 'counters/examples': 174432, 'counters/updates': 5451}
train stats after 174464 examples: {'rewards_train/chosen': '0.1197', 'rewards_train/rejected': '0.12503', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0053321', 'logps_train/rejected': '-111.96', 'logps_train/chosen': '-135.31', 'loss/train': '0.7087', 'examples_per_second': '31.403', 'grad_norm': '34.5', 'counters/examples': 174464, 'counters/updates': 5452}
train stats after 174496 examples: {'rewards_train/chosen': '0.028018', 'rewards_train/rejected': '0.02103', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0069877', 'logps_train/rejected': '-94.135', 'logps_train/chosen': '-140.09', 'loss/train': '0.69472', 'examples_per_second': '32.298', 'grad_norm': '158', 'counters/examples': 174496, 'counters/updates': 5453}
skipping logging after 174528 examples to avoid logging too frequently
train stats after 174560 examples: {'rewards_train/chosen': '0.071024', 'rewards_train/rejected': '0.020406', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050618', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-140.48', 'loss/train': '0.681', 'examples_per_second': '33.755', 'grad_norm': '38.5', 'counters/examples': 174560, 'counters/updates': 5455}
train stats after 174592 examples: {'rewards_train/chosen': '0.15106', 'rewards_train/rejected': '0.00802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14304', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-138.3', 'loss/train': '0.63333', 'examples_per_second': '31.292', 'grad_norm': '25.875', 'counters/examples': 174592, 'counters/updates': 5456}
train stats after 174624 examples: {'rewards_train/chosen': '0.061638', 'rewards_train/rejected': '0.0092226', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.052416', 'logps_train/rejected': '-116.72', 'logps_train/chosen': '-115.42', 'loss/train': '0.67252', 'examples_per_second': '30.886', 'grad_norm': '28.125', 'counters/examples': 174624, 'counters/updates': 5457}
train stats after 174656 examples: {'rewards_train/chosen': '0.22886', 'rewards_train/rejected': '0.11829', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11057', 'logps_train/rejected': '-139.06', 'logps_train/chosen': '-182.08', 'loss/train': '0.65854', 'examples_per_second': '30.021', 'grad_norm': '53', 'counters/examples': 174656, 'counters/updates': 5458}
train stats after 174688 examples: {'rewards_train/chosen': '0.16086', 'rewards_train/rejected': '0.033793', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12707', 'logps_train/rejected': '-88.6', 'logps_train/chosen': '-134.74', 'loss/train': '0.641', 'examples_per_second': '31.547', 'grad_norm': '26.875', 'counters/examples': 174688, 'counters/updates': 5459}
train stats after 174720 examples: {'rewards_train/chosen': '0.13367', 'rewards_train/rejected': '0.11564', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018031', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-118.88', 'loss/train': '0.69931', 'examples_per_second': '31.462', 'grad_norm': '34', 'counters/examples': 174720, 'counters/updates': 5460}
train stats after 174752 examples: {'rewards_train/chosen': '0.075903', 'rewards_train/rejected': '0.038698', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037205', 'logps_train/rejected': '-125.96', 'logps_train/chosen': '-114.61', 'loss/train': '0.68539', 'examples_per_second': '32.324', 'grad_norm': '28.25', 'counters/examples': 174752, 'counters/updates': 5461}
train stats after 174784 examples: {'rewards_train/chosen': '0.19025', 'rewards_train/rejected': '0.052075', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13817', 'logps_train/rejected': '-110.27', 'logps_train/chosen': '-152.22', 'loss/train': '0.6526', 'examples_per_second': '30.967', 'grad_norm': '31.5', 'counters/examples': 174784, 'counters/updates': 5462}
train stats after 174816 examples: {'rewards_train/chosen': '0.12752', 'rewards_train/rejected': '0.1147', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012816', 'logps_train/rejected': '-129.86', 'logps_train/chosen': '-147.06', 'loss/train': '0.69819', 'examples_per_second': '30.909', 'grad_norm': '31.75', 'counters/examples': 174816, 'counters/updates': 5463}
train stats after 174848 examples: {'rewards_train/chosen': '0.093402', 'rewards_train/rejected': '0.041813', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05159', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-122.74', 'loss/train': '0.67651', 'examples_per_second': '31.018', 'grad_norm': '30.375', 'counters/examples': 174848, 'counters/updates': 5464}
train stats after 174880 examples: {'rewards_train/chosen': '0.18294', 'rewards_train/rejected': '0.12905', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.053886', 'logps_train/rejected': '-113.55', 'logps_train/chosen': '-157.41', 'loss/train': '0.6949', 'examples_per_second': '31.103', 'grad_norm': '38.5', 'counters/examples': 174880, 'counters/updates': 5465}
train stats after 174912 examples: {'rewards_train/chosen': '0.073836', 'rewards_train/rejected': '0.058783', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015053', 'logps_train/rejected': '-107.49', 'logps_train/chosen': '-110.82', 'loss/train': '0.70009', 'examples_per_second': '30.247', 'grad_norm': '28.875', 'counters/examples': 174912, 'counters/updates': 5466}
train stats after 174944 examples: {'rewards_train/chosen': '0.22257', 'rewards_train/rejected': '-0.011431', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.234', 'logps_train/rejected': '-118.9', 'logps_train/chosen': '-161.35', 'loss/train': '0.60641', 'examples_per_second': '30.513', 'grad_norm': '26.75', 'counters/examples': 174944, 'counters/updates': 5467}
train stats after 174976 examples: {'rewards_train/chosen': '0.14056', 'rewards_train/rejected': '0.038798', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10176', 'logps_train/rejected': '-123.4', 'logps_train/chosen': '-152.73', 'loss/train': '0.65667', 'examples_per_second': '31.579', 'grad_norm': '27.875', 'counters/examples': 174976, 'counters/updates': 5468}
train stats after 175008 examples: {'rewards_train/chosen': '0.11789', 'rewards_train/rejected': '0.10237', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015514', 'logps_train/rejected': '-158.98', 'logps_train/chosen': '-169.04', 'loss/train': '0.69487', 'examples_per_second': '30.475', 'grad_norm': '34', 'counters/examples': 175008, 'counters/updates': 5469}
train stats after 175040 examples: {'rewards_train/chosen': '0.094082', 'rewards_train/rejected': '0.036914', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057168', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-150.75', 'loss/train': '0.67694', 'examples_per_second': '32.163', 'grad_norm': '38.75', 'counters/examples': 175040, 'counters/updates': 5470}
train stats after 175072 examples: {'rewards_train/chosen': '0.080752', 'rewards_train/rejected': '0.037816', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.042936', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-133.33', 'loss/train': '0.68462', 'examples_per_second': '30.316', 'grad_norm': '26.625', 'counters/examples': 175072, 'counters/updates': 5471}
train stats after 175104 examples: {'rewards_train/chosen': '0.13529', 'rewards_train/rejected': '0.061443', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073849', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-164.46', 'loss/train': '0.66655', 'examples_per_second': '30.783', 'grad_norm': '48.5', 'counters/examples': 175104, 'counters/updates': 5472}
train stats after 175136 examples: {'rewards_train/chosen': '0.079319', 'rewards_train/rejected': '0.027796', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.051523', 'logps_train/rejected': '-154.35', 'logps_train/chosen': '-157.5', 'loss/train': '0.67561', 'examples_per_second': '31.435', 'grad_norm': '36', 'counters/examples': 175136, 'counters/updates': 5473}
train stats after 175168 examples: {'rewards_train/chosen': '0.10461', 'rewards_train/rejected': '0.038569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066037', 'logps_train/rejected': '-126.62', 'logps_train/chosen': '-147.61', 'loss/train': '0.67116', 'examples_per_second': '31.164', 'grad_norm': '29.375', 'counters/examples': 175168, 'counters/updates': 5474}
train stats after 175200 examples: {'rewards_train/chosen': '0.15365', 'rewards_train/rejected': '-0.018766', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17241', 'logps_train/rejected': '-154.06', 'logps_train/chosen': '-142.02', 'loss/train': '0.62397', 'examples_per_second': '31.465', 'grad_norm': '28.25', 'counters/examples': 175200, 'counters/updates': 5475}
train stats after 175232 examples: {'rewards_train/chosen': '0.11772', 'rewards_train/rejected': '0.093527', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.024189', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-142.13', 'loss/train': '0.68985', 'examples_per_second': '31.599', 'grad_norm': '29.75', 'counters/examples': 175232, 'counters/updates': 5476}
skipping logging after 175264 examples to avoid logging too frequently
train stats after 175296 examples: {'rewards_train/chosen': '0.23634', 'rewards_train/rejected': '-0.0006318', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.23697', 'logps_train/rejected': '-104.04', 'logps_train/chosen': '-126.61', 'loss/train': '0.62959', 'examples_per_second': '31.187', 'grad_norm': '31.125', 'counters/examples': 175296, 'counters/updates': 5478}
train stats after 175328 examples: {'rewards_train/chosen': '0.20055', 'rewards_train/rejected': '0.11186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088685', 'logps_train/rejected': '-167.51', 'logps_train/chosen': '-144.56', 'loss/train': '0.66684', 'examples_per_second': '31.227', 'grad_norm': '41.5', 'counters/examples': 175328, 'counters/updates': 5479}
train stats after 175360 examples: {'rewards_train/chosen': '0.09591', 'rewards_train/rejected': '0.065438', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.030472', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-181.51', 'loss/train': '0.68554', 'examples_per_second': '30.545', 'grad_norm': '29.625', 'counters/examples': 175360, 'counters/updates': 5480}
train stats after 175392 examples: {'rewards_train/chosen': '0.069499', 'rewards_train/rejected': '0.1141', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.044602', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-137.63', 'loss/train': '0.73024', 'examples_per_second': '30.876', 'grad_norm': '36', 'counters/examples': 175392, 'counters/updates': 5481}
train stats after 175424 examples: {'rewards_train/chosen': '0.1728', 'rewards_train/rejected': '0.10927', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.063538', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-127.06', 'loss/train': '0.66865', 'examples_per_second': '31.507', 'grad_norm': '31.375', 'counters/examples': 175424, 'counters/updates': 5482}
train stats after 175456 examples: {'rewards_train/chosen': '0.17553', 'rewards_train/rejected': '0.021062', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15447', 'logps_train/rejected': '-104.6', 'logps_train/chosen': '-141.78', 'loss/train': '0.62536', 'examples_per_second': '31.498', 'grad_norm': '35.75', 'counters/examples': 175456, 'counters/updates': 5483}
train stats after 175488 examples: {'rewards_train/chosen': '0.15914', 'rewards_train/rejected': '0.08904', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070103', 'logps_train/rejected': '-103.39', 'logps_train/chosen': '-129.01', 'loss/train': '0.6706', 'examples_per_second': '29.969', 'grad_norm': '24', 'counters/examples': 175488, 'counters/updates': 5484}
train stats after 175520 examples: {'rewards_train/chosen': '0.073804', 'rewards_train/rejected': '0.13836', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.06456', 'logps_train/rejected': '-116.92', 'logps_train/chosen': '-146.94', 'loss/train': '0.73791', 'examples_per_second': '30.634', 'grad_norm': '41.25', 'counters/examples': 175520, 'counters/updates': 5485}
train stats after 175552 examples: {'rewards_train/chosen': '0.1369', 'rewards_train/rejected': '0.05334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083557', 'logps_train/rejected': '-136.96', 'logps_train/chosen': '-117.74', 'loss/train': '0.6681', 'examples_per_second': '30.93', 'grad_norm': '34.5', 'counters/examples': 175552, 'counters/updates': 5486}
train stats after 175584 examples: {'rewards_train/chosen': '0.12774', 'rewards_train/rejected': '0.060546', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067197', 'logps_train/rejected': '-127.4', 'logps_train/chosen': '-140.87', 'loss/train': '0.66881', 'examples_per_second': '29.997', 'grad_norm': '38', 'counters/examples': 175584, 'counters/updates': 5487}
skipping logging after 175616 examples to avoid logging too frequently
train stats after 175648 examples: {'rewards_train/chosen': '0.05197', 'rewards_train/rejected': '0.095171', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.043201', 'logps_train/rejected': '-117.97', 'logps_train/chosen': '-131.53', 'loss/train': '0.72028', 'examples_per_second': '30.493', 'grad_norm': '34.5', 'counters/examples': 175648, 'counters/updates': 5489}
train stats after 175680 examples: {'rewards_train/chosen': '0.17534', 'rewards_train/rejected': '0.13585', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039489', 'logps_train/rejected': '-161.96', 'logps_train/chosen': '-164.03', 'loss/train': '0.68928', 'examples_per_second': '31.371', 'grad_norm': '37', 'counters/examples': 175680, 'counters/updates': 5490}
train stats after 175712 examples: {'rewards_train/chosen': '0.25588', 'rewards_train/rejected': '0.23144', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.024439', 'logps_train/rejected': '-142.56', 'logps_train/chosen': '-161.25', 'loss/train': '0.69639', 'examples_per_second': '32.569', 'grad_norm': '31.5', 'counters/examples': 175712, 'counters/updates': 5491}
train stats after 175744 examples: {'rewards_train/chosen': '0.17124', 'rewards_train/rejected': '-0.023371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19462', 'logps_train/rejected': '-94.739', 'logps_train/chosen': '-163.41', 'loss/train': '0.61762', 'examples_per_second': '30.119', 'grad_norm': '25.25', 'counters/examples': 175744, 'counters/updates': 5492}
train stats after 175776 examples: {'rewards_train/chosen': '0.18749', 'rewards_train/rejected': '0.051237', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13625', 'logps_train/rejected': '-110.55', 'logps_train/chosen': '-156.31', 'loss/train': '0.63996', 'examples_per_second': '31.482', 'grad_norm': '27.875', 'counters/examples': 175776, 'counters/updates': 5493}
train stats after 175808 examples: {'rewards_train/chosen': '0.13189', 'rewards_train/rejected': '0.040756', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091136', 'logps_train/rejected': '-162.83', 'logps_train/chosen': '-151.22', 'loss/train': '0.66356', 'examples_per_second': '31.543', 'grad_norm': '41.75', 'counters/examples': 175808, 'counters/updates': 5494}
train stats after 175840 examples: {'rewards_train/chosen': '0.10441', 'rewards_train/rejected': '0.11849', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014079', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-126.79', 'loss/train': '0.71218', 'examples_per_second': '30.962', 'grad_norm': '45', 'counters/examples': 175840, 'counters/updates': 5495}
train stats after 175872 examples: {'rewards_train/chosen': '0.051677', 'rewards_train/rejected': '0.016508', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.035169', 'logps_train/rejected': '-153.29', 'logps_train/chosen': '-125.2', 'loss/train': '0.68747', 'examples_per_second': '32.023', 'grad_norm': '30.25', 'counters/examples': 175872, 'counters/updates': 5496}
train stats after 175904 examples: {'rewards_train/chosen': '0.12219', 'rewards_train/rejected': '0.0027012', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11949', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-128.71', 'loss/train': '0.64759', 'examples_per_second': '30.55', 'grad_norm': '27.5', 'counters/examples': 175904, 'counters/updates': 5497}
train stats after 175936 examples: {'rewards_train/chosen': '0.1493', 'rewards_train/rejected': '-0.0013015', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1506', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-151.22', 'loss/train': '0.63292', 'examples_per_second': '31.496', 'grad_norm': '29.375', 'counters/examples': 175936, 'counters/updates': 5498}
train stats after 175968 examples: {'rewards_train/chosen': '0.2329', 'rewards_train/rejected': '0.044719', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18818', 'logps_train/rejected': '-129.54', 'logps_train/chosen': '-147.51', 'loss/train': '0.6252', 'examples_per_second': '31.622', 'grad_norm': '35', 'counters/examples': 175968, 'counters/updates': 5499}
train stats after 176000 examples: {'rewards_train/chosen': '0.1312', 'rewards_train/rejected': '0.026429', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10477', 'logps_train/rejected': '-157.27', 'logps_train/chosen': '-128.77', 'loss/train': '0.6547', 'examples_per_second': '31.661', 'grad_norm': '35.25', 'counters/examples': 176000, 'counters/updates': 5500}
skipping logging after 176032 examples to avoid logging too frequently
train stats after 176064 examples: {'rewards_train/chosen': '0.060629', 'rewards_train/rejected': '0.011864', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048766', 'logps_train/rejected': '-134.87', 'logps_train/chosen': '-153.08', 'loss/train': '0.67987', 'examples_per_second': '34.354', 'grad_norm': '31.125', 'counters/examples': 176064, 'counters/updates': 5502}
train stats after 176096 examples: {'rewards_train/chosen': '0.10904', 'rewards_train/rejected': '-0.039406', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14844', 'logps_train/rejected': '-145.49', 'logps_train/chosen': '-168.15', 'loss/train': '0.63092', 'examples_per_second': '31.515', 'grad_norm': '30.5', 'counters/examples': 176096, 'counters/updates': 5503}
train stats after 176128 examples: {'rewards_train/chosen': '0.13644', 'rewards_train/rejected': '0.0098022', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12664', 'logps_train/rejected': '-100.92', 'logps_train/chosen': '-134.33', 'loss/train': '0.6493', 'examples_per_second': '31.504', 'grad_norm': '55.25', 'counters/examples': 176128, 'counters/updates': 5504}
train stats after 176160 examples: {'rewards_train/chosen': '0.17306', 'rewards_train/rejected': '0.086198', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086861', 'logps_train/rejected': '-145.07', 'logps_train/chosen': '-177.14', 'loss/train': '0.66415', 'examples_per_second': '31.224', 'grad_norm': '36.75', 'counters/examples': 176160, 'counters/updates': 5505}
train stats after 176192 examples: {'rewards_train/chosen': '0.12017', 'rewards_train/rejected': '0.054215', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.065955', 'logps_train/rejected': '-157.6', 'logps_train/chosen': '-154.72', 'loss/train': '0.66745', 'examples_per_second': '30.847', 'grad_norm': '33.25', 'counters/examples': 176192, 'counters/updates': 5506}
train stats after 176224 examples: {'rewards_train/chosen': '0.12749', 'rewards_train/rejected': '0.012779', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11471', 'logps_train/rejected': '-137.89', 'logps_train/chosen': '-130.88', 'loss/train': '0.64805', 'examples_per_second': '31.468', 'grad_norm': '34', 'counters/examples': 176224, 'counters/updates': 5507}
train stats after 176256 examples: {'rewards_train/chosen': '0.15035', 'rewards_train/rejected': '-0.0098843', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16023', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-128.58', 'loss/train': '0.6281', 'examples_per_second': '31.804', 'grad_norm': '28.25', 'counters/examples': 176256, 'counters/updates': 5508}
train stats after 176288 examples: {'rewards_train/chosen': '0.19916', 'rewards_train/rejected': '0.11806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081102', 'logps_train/rejected': '-149.59', 'logps_train/chosen': '-145.9', 'loss/train': '0.68046', 'examples_per_second': '32.023', 'grad_norm': '31.625', 'counters/examples': 176288, 'counters/updates': 5509}
train stats after 176320 examples: {'rewards_train/chosen': '0.12946', 'rewards_train/rejected': '0.023405', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10606', 'logps_train/rejected': '-113.36', 'logps_train/chosen': '-161.45', 'loss/train': '0.66763', 'examples_per_second': '30.087', 'grad_norm': '54.25', 'counters/examples': 176320, 'counters/updates': 5510}
train stats after 176352 examples: {'rewards_train/chosen': '0.25973', 'rewards_train/rejected': '0.064505', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19522', 'logps_train/rejected': '-127.94', 'logps_train/chosen': '-176.86', 'loss/train': '0.62349', 'examples_per_second': '32.233', 'grad_norm': '31.75', 'counters/examples': 176352, 'counters/updates': 5511}
train stats after 176384 examples: {'rewards_train/chosen': '0.20173', 'rewards_train/rejected': '0.027334', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1744', 'logps_train/rejected': '-99.15', 'logps_train/chosen': '-152.86', 'loss/train': '0.63157', 'examples_per_second': '31.447', 'grad_norm': '27.375', 'counters/examples': 176384, 'counters/updates': 5512}
skipping logging after 176416 examples to avoid logging too frequently
train stats after 176448 examples: {'rewards_train/chosen': '0.30121', 'rewards_train/rejected': '0.026857', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27435', 'logps_train/rejected': '-134.54', 'logps_train/chosen': '-171.54', 'loss/train': '0.58129', 'examples_per_second': '31.349', 'grad_norm': '26.25', 'counters/examples': 176448, 'counters/updates': 5514}
skipping logging after 176480 examples to avoid logging too frequently
train stats after 176512 examples: {'rewards_train/chosen': '0.18234', 'rewards_train/rejected': '0.079224', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10312', 'logps_train/rejected': '-135.85', 'logps_train/chosen': '-138.71', 'loss/train': '0.64932', 'examples_per_second': '32.351', 'grad_norm': '35', 'counters/examples': 176512, 'counters/updates': 5516}
train stats after 176544 examples: {'rewards_train/chosen': '0.17546', 'rewards_train/rejected': '0.025848', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14962', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-182.57', 'loss/train': '0.63339', 'examples_per_second': '31.643', 'grad_norm': '42', 'counters/examples': 176544, 'counters/updates': 5517}
skipping logging after 176576 examples to avoid logging too frequently
train stats after 176608 examples: {'rewards_train/chosen': '0.15584', 'rewards_train/rejected': '0.1223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033537', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-119.78', 'loss/train': '0.69725', 'examples_per_second': '32.339', 'grad_norm': '27.875', 'counters/examples': 176608, 'counters/updates': 5519}
train stats after 176640 examples: {'rewards_train/chosen': '0.17045', 'rewards_train/rejected': '0.020813', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14964', 'logps_train/rejected': '-93.42', 'logps_train/chosen': '-138.82', 'loss/train': '0.63409', 'examples_per_second': '31.518', 'grad_norm': '28', 'counters/examples': 176640, 'counters/updates': 5520}
train stats after 176672 examples: {'rewards_train/chosen': '0.1432', 'rewards_train/rejected': '0.070204', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072992', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-127.34', 'loss/train': '0.66654', 'examples_per_second': '29.83', 'grad_norm': '32', 'counters/examples': 176672, 'counters/updates': 5521}
train stats after 176704 examples: {'rewards_train/chosen': '0.21563', 'rewards_train/rejected': '0.057394', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15824', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-156.97', 'loss/train': '0.63222', 'examples_per_second': '30.299', 'grad_norm': '34', 'counters/examples': 176704, 'counters/updates': 5522}
skipping logging after 176736 examples to avoid logging too frequently
train stats after 176768 examples: {'rewards_train/chosen': '0.10335', 'rewards_train/rejected': '-0.0085899', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11194', 'logps_train/rejected': '-86.449', 'logps_train/chosen': '-122.52', 'loss/train': '0.64569', 'examples_per_second': '32.705', 'grad_norm': '26.375', 'counters/examples': 176768, 'counters/updates': 5524}
train stats after 176800 examples: {'rewards_train/chosen': '0.1431', 'rewards_train/rejected': '-0.0051065', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14821', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-161.04', 'loss/train': '0.63764', 'examples_per_second': '30.726', 'grad_norm': '34.75', 'counters/examples': 176800, 'counters/updates': 5525}
train stats after 176832 examples: {'rewards_train/chosen': '0.1412', 'rewards_train/rejected': '0.033125', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10807', 'logps_train/rejected': '-155.81', 'logps_train/chosen': '-165.16', 'loss/train': '0.64972', 'examples_per_second': '29.983', 'grad_norm': '49.5', 'counters/examples': 176832, 'counters/updates': 5526}
skipping logging after 176864 examples to avoid logging too frequently
train stats after 176896 examples: {'rewards_train/chosen': '0.083238', 'rewards_train/rejected': '-0.003213', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086451', 'logps_train/rejected': '-119.01', 'logps_train/chosen': '-129.03', 'loss/train': '0.65992', 'examples_per_second': '31.086', 'grad_norm': '25', 'counters/examples': 176896, 'counters/updates': 5528}
train stats after 176928 examples: {'rewards_train/chosen': '0.20721', 'rewards_train/rejected': '0.057158', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15005', 'logps_train/rejected': '-134.92', 'logps_train/chosen': '-151.61', 'loss/train': '0.62956', 'examples_per_second': '31.338', 'grad_norm': '28.5', 'counters/examples': 176928, 'counters/updates': 5529}
train stats after 176960 examples: {'rewards_train/chosen': '0.090432', 'rewards_train/rejected': '0.063279', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.027153', 'logps_train/rejected': '-100.33', 'logps_train/chosen': '-118.65', 'loss/train': '0.68738', 'examples_per_second': '32.666', 'grad_norm': '27.375', 'counters/examples': 176960, 'counters/updates': 5530}
train stats after 176992 examples: {'rewards_train/chosen': '0.14643', 'rewards_train/rejected': '0.099039', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047389', 'logps_train/rejected': '-130.07', 'logps_train/chosen': '-155.33', 'loss/train': '0.67981', 'examples_per_second': '31.339', 'grad_norm': '35.5', 'counters/examples': 176992, 'counters/updates': 5531}
skipping logging after 177024 examples to avoid logging too frequently
train stats after 177056 examples: {'rewards_train/chosen': '0.14198', 'rewards_train/rejected': '0.1257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016276', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-162.05', 'loss/train': '0.69672', 'examples_per_second': '23.731', 'grad_norm': '40.75', 'counters/examples': 177056, 'counters/updates': 5533}
train stats after 177088 examples: {'rewards_train/chosen': '0.083214', 'rewards_train/rejected': '0.071068', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012146', 'logps_train/rejected': '-156.85', 'logps_train/chosen': '-126.26', 'loss/train': '0.70179', 'examples_per_second': '29.9', 'grad_norm': '50.75', 'counters/examples': 177088, 'counters/updates': 5534}
train stats after 177120 examples: {'rewards_train/chosen': '0.16257', 'rewards_train/rejected': '0.15115', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011421', 'logps_train/rejected': '-159.21', 'logps_train/chosen': '-152.48', 'loss/train': '0.70083', 'examples_per_second': '31.465', 'grad_norm': '33.75', 'counters/examples': 177120, 'counters/updates': 5535}
train stats after 177152 examples: {'rewards_train/chosen': '0.069458', 'rewards_train/rejected': '0.01534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054118', 'logps_train/rejected': '-146.68', 'logps_train/chosen': '-139.15', 'loss/train': '0.67469', 'examples_per_second': '30.167', 'grad_norm': '26.25', 'counters/examples': 177152, 'counters/updates': 5536}
train stats after 177184 examples: {'rewards_train/chosen': '0.20075', 'rewards_train/rejected': '0.059531', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14122', 'logps_train/rejected': '-159.71', 'logps_train/chosen': '-199.35', 'loss/train': '0.63904', 'examples_per_second': '30.457', 'grad_norm': '34.75', 'counters/examples': 177184, 'counters/updates': 5537}
train stats after 177216 examples: {'rewards_train/chosen': '0.14857', 'rewards_train/rejected': '0.026588', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12199', 'logps_train/rejected': '-116.92', 'logps_train/chosen': '-153.98', 'loss/train': '0.64586', 'examples_per_second': '31.254', 'grad_norm': '30.25', 'counters/examples': 177216, 'counters/updates': 5538}
skipping logging after 177248 examples to avoid logging too frequently
train stats after 177280 examples: {'rewards_train/chosen': '0.24472', 'rewards_train/rejected': '0.13669', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10804', 'logps_train/rejected': '-117.96', 'logps_train/chosen': '-137.47', 'loss/train': '0.65346', 'examples_per_second': '31.619', 'grad_norm': '29', 'counters/examples': 177280, 'counters/updates': 5540}
train stats after 177312 examples: {'rewards_train/chosen': '0.091829', 'rewards_train/rejected': '0.042485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049344', 'logps_train/rejected': '-93.993', 'logps_train/chosen': '-138.62', 'loss/train': '0.67728', 'examples_per_second': '32.553', 'grad_norm': '23.625', 'counters/examples': 177312, 'counters/updates': 5541}
train stats after 177344 examples: {'rewards_train/chosen': '0.19353', 'rewards_train/rejected': '0.17191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021622', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-156.97', 'loss/train': '0.69837', 'examples_per_second': '30.398', 'grad_norm': '37.75', 'counters/examples': 177344, 'counters/updates': 5542}
train stats after 177376 examples: {'rewards_train/chosen': '0.2246', 'rewards_train/rejected': '0.084104', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1405', 'logps_train/rejected': '-148.99', 'logps_train/chosen': '-205.95', 'loss/train': '0.64397', 'examples_per_second': '30.352', 'grad_norm': '32.25', 'counters/examples': 177376, 'counters/updates': 5543}
skipping logging after 177408 examples to avoid logging too frequently
train stats after 177440 examples: {'rewards_train/chosen': '0.15098', 'rewards_train/rejected': '0.016156', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.13482', 'logps_train/rejected': '-94.738', 'logps_train/chosen': '-133.64', 'loss/train': '0.64191', 'examples_per_second': '30.647', 'grad_norm': '41.75', 'counters/examples': 177440, 'counters/updates': 5545}
train stats after 177472 examples: {'rewards_train/chosen': '0.12765', 'rewards_train/rejected': '0.041046', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086608', 'logps_train/rejected': '-115.58', 'logps_train/chosen': '-162.68', 'loss/train': '0.66635', 'examples_per_second': '31.526', 'grad_norm': '30.125', 'counters/examples': 177472, 'counters/updates': 5546}
train stats after 177504 examples: {'rewards_train/chosen': '0.21216', 'rewards_train/rejected': '0.0031743', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20899', 'logps_train/rejected': '-108.28', 'logps_train/chosen': '-152.79', 'loss/train': '0.62432', 'examples_per_second': '31.49', 'grad_norm': '29.375', 'counters/examples': 177504, 'counters/updates': 5547}
skipping logging after 177536 examples to avoid logging too frequently
train stats after 177568 examples: {'rewards_train/chosen': '0.088534', 'rewards_train/rejected': '0.059317', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029217', 'logps_train/rejected': '-128.95', 'logps_train/chosen': '-109.82', 'loss/train': '0.69425', 'examples_per_second': '33.339', 'grad_norm': '31', 'counters/examples': 177568, 'counters/updates': 5549}
train stats after 177600 examples: {'rewards_train/chosen': '0.054324', 'rewards_train/rejected': '0.0060835', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04824', 'logps_train/rejected': '-133.27', 'logps_train/chosen': '-139.12', 'loss/train': '0.67849', 'examples_per_second': '30.186', 'grad_norm': '28.375', 'counters/examples': 177600, 'counters/updates': 5550}
train stats after 177632 examples: {'rewards_train/chosen': '0.10304', 'rewards_train/rejected': '0.057562', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045482', 'logps_train/rejected': '-142.2', 'logps_train/chosen': '-137.08', 'loss/train': '0.67766', 'examples_per_second': '31.632', 'grad_norm': '27.875', 'counters/examples': 177632, 'counters/updates': 5551}
train stats after 177664 examples: {'rewards_train/chosen': '0.10536', 'rewards_train/rejected': '0.037293', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06807', 'logps_train/rejected': '-112.79', 'logps_train/chosen': '-134.58', 'loss/train': '0.66916', 'examples_per_second': '31.807', 'grad_norm': '30.125', 'counters/examples': 177664, 'counters/updates': 5552}
train stats after 177696 examples: {'rewards_train/chosen': '0.16154', 'rewards_train/rejected': '0.025464', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13607', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-173.39', 'loss/train': '0.65585', 'examples_per_second': '30.505', 'grad_norm': '34.5', 'counters/examples': 177696, 'counters/updates': 5553}
train stats after 177728 examples: {'rewards_train/chosen': '0.19102', 'rewards_train/rejected': '0.059896', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13112', 'logps_train/rejected': '-150.2', 'logps_train/chosen': '-163.13', 'loss/train': '0.64501', 'examples_per_second': '30.584', 'grad_norm': '37.5', 'counters/examples': 177728, 'counters/updates': 5554}
train stats after 177760 examples: {'rewards_train/chosen': '0.13073', 'rewards_train/rejected': '0.024985', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10574', 'logps_train/rejected': '-124.61', 'logps_train/chosen': '-148.8', 'loss/train': '0.65108', 'examples_per_second': '32.075', 'grad_norm': '26.5', 'counters/examples': 177760, 'counters/updates': 5555}
train stats after 177792 examples: {'rewards_train/chosen': '0.17292', 'rewards_train/rejected': '0.050054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12286', 'logps_train/rejected': '-142.7', 'logps_train/chosen': '-163.94', 'loss/train': '0.65778', 'examples_per_second': '31.487', 'grad_norm': '40.5', 'counters/examples': 177792, 'counters/updates': 5556}
train stats after 177824 examples: {'rewards_train/chosen': '0.099943', 'rewards_train/rejected': '0.10273', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0027835', 'logps_train/rejected': '-129.05', 'logps_train/chosen': '-136.88', 'loss/train': '0.70757', 'examples_per_second': '31.423', 'grad_norm': '54', 'counters/examples': 177824, 'counters/updates': 5557}
train stats after 177856 examples: {'rewards_train/chosen': '0.11089', 'rewards_train/rejected': '-0.001955', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11284', 'logps_train/rejected': '-170.76', 'logps_train/chosen': '-152.65', 'loss/train': '0.65366', 'examples_per_second': '31.5', 'grad_norm': '48.5', 'counters/examples': 177856, 'counters/updates': 5558}
train stats after 177888 examples: {'rewards_train/chosen': '0.23817', 'rewards_train/rejected': '0.14874', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089428', 'logps_train/rejected': '-168.43', 'logps_train/chosen': '-159.14', 'loss/train': '0.66756', 'examples_per_second': '30.852', 'grad_norm': '45', 'counters/examples': 177888, 'counters/updates': 5559}
train stats after 177920 examples: {'rewards_train/chosen': '0.096673', 'rewards_train/rejected': '0.069384', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027289', 'logps_train/rejected': '-113.13', 'logps_train/chosen': '-136', 'loss/train': '0.69301', 'examples_per_second': '31.529', 'grad_norm': '26.25', 'counters/examples': 177920, 'counters/updates': 5560}
train stats after 177952 examples: {'rewards_train/chosen': '0.13344', 'rewards_train/rejected': '0.022254', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11118', 'logps_train/rejected': '-91.382', 'logps_train/chosen': '-128.29', 'loss/train': '0.64628', 'examples_per_second': '31.376', 'grad_norm': '21.875', 'counters/examples': 177952, 'counters/updates': 5561}
train stats after 177984 examples: {'rewards_train/chosen': '0.24429', 'rewards_train/rejected': '0.09981', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14448', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-155', 'loss/train': '0.6391', 'examples_per_second': '32.954', 'grad_norm': '42.5', 'counters/examples': 177984, 'counters/updates': 5562}
skipping logging after 178016 examples to avoid logging too frequently
train stats after 178048 examples: {'rewards_train/chosen': '0.31869', 'rewards_train/rejected': '0.089419', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22927', 'logps_train/rejected': '-107.57', 'logps_train/chosen': '-175.42', 'loss/train': '0.61344', 'examples_per_second': '30.351', 'grad_norm': '26.875', 'counters/examples': 178048, 'counters/updates': 5564}
train stats after 178080 examples: {'rewards_train/chosen': '0.10942', 'rewards_train/rejected': '0.078225', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031192', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-134.43', 'loss/train': '0.68869', 'examples_per_second': '31.207', 'grad_norm': '28.875', 'counters/examples': 178080, 'counters/updates': 5565}
train stats after 178112 examples: {'rewards_train/chosen': '0.13802', 'rewards_train/rejected': '0.076001', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.062021', 'logps_train/rejected': '-128.79', 'logps_train/chosen': '-140.34', 'loss/train': '0.67378', 'examples_per_second': '30.269', 'grad_norm': '34.5', 'counters/examples': 178112, 'counters/updates': 5566}
train stats after 178144 examples: {'rewards_train/chosen': '0.11153', 'rewards_train/rejected': '0.1619', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.05037', 'logps_train/rejected': '-143.39', 'logps_train/chosen': '-143.89', 'loss/train': '0.72742', 'examples_per_second': '33.155', 'grad_norm': '30.375', 'counters/examples': 178144, 'counters/updates': 5567}
train stats after 178176 examples: {'rewards_train/chosen': '0.12989', 'rewards_train/rejected': '-0.031473', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16136', 'logps_train/rejected': '-105.01', 'logps_train/chosen': '-109.69', 'loss/train': '0.62986', 'examples_per_second': '32.109', 'grad_norm': '24.5', 'counters/examples': 178176, 'counters/updates': 5568}
train stats after 178208 examples: {'rewards_train/chosen': '0.15359', 'rewards_train/rejected': '0.02778', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12581', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-159.94', 'loss/train': '0.64665', 'examples_per_second': '31.564', 'grad_norm': '31.25', 'counters/examples': 178208, 'counters/updates': 5569}
train stats after 178240 examples: {'rewards_train/chosen': '0.17152', 'rewards_train/rejected': '0.089801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081724', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-152.27', 'loss/train': '0.66342', 'examples_per_second': '30.08', 'grad_norm': '36.75', 'counters/examples': 178240, 'counters/updates': 5570}
train stats after 178272 examples: {'rewards_train/chosen': '0.043069', 'rewards_train/rejected': '-0.044049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087117', 'logps_train/rejected': '-118.67', 'logps_train/chosen': '-126.97', 'loss/train': '0.65697', 'examples_per_second': '33.094', 'grad_norm': '35', 'counters/examples': 178272, 'counters/updates': 5571}
train stats after 178304 examples: {'rewards_train/chosen': '0.11661', 'rewards_train/rejected': '-0.038571', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15518', 'logps_train/rejected': '-104.48', 'logps_train/chosen': '-129.85', 'loss/train': '0.6318', 'examples_per_second': '31.429', 'grad_norm': '29', 'counters/examples': 178304, 'counters/updates': 5572}
skipping logging after 178336 examples to avoid logging too frequently
train stats after 178368 examples: {'rewards_train/chosen': '0.11203', 'rewards_train/rejected': '0.015241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096787', 'logps_train/rejected': '-88.637', 'logps_train/chosen': '-138.27', 'loss/train': '0.65395', 'examples_per_second': '30.857', 'grad_norm': '28.5', 'counters/examples': 178368, 'counters/updates': 5574}
train stats after 178400 examples: {'rewards_train/chosen': '0.095678', 'rewards_train/rejected': '0.079211', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.016467', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-120.76', 'loss/train': '0.68958', 'examples_per_second': '32.808', 'grad_norm': '24.125', 'counters/examples': 178400, 'counters/updates': 5575}
skipping logging after 178432 examples to avoid logging too frequently
train stats after 178464 examples: {'rewards_train/chosen': '0.19897', 'rewards_train/rejected': '0.076795', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12218', 'logps_train/rejected': '-89.853', 'logps_train/chosen': '-150.38', 'loss/train': '0.64423', 'examples_per_second': '30.443', 'grad_norm': '27.75', 'counters/examples': 178464, 'counters/updates': 5577}
train stats after 178496 examples: {'rewards_train/chosen': '0.1098', 'rewards_train/rejected': '0.081491', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028311', 'logps_train/rejected': '-109.83', 'logps_train/chosen': '-157.82', 'loss/train': '0.68591', 'examples_per_second': '31.112', 'grad_norm': '65.5', 'counters/examples': 178496, 'counters/updates': 5578}
train stats after 178528 examples: {'rewards_train/chosen': '0.11279', 'rewards_train/rejected': '0.063772', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049015', 'logps_train/rejected': '-137.25', 'logps_train/chosen': '-137.92', 'loss/train': '0.678', 'examples_per_second': '31.254', 'grad_norm': '55.5', 'counters/examples': 178528, 'counters/updates': 5579}
skipping logging after 178560 examples to avoid logging too frequently
train stats after 178592 examples: {'rewards_train/chosen': '0.045425', 'rewards_train/rejected': '0.033849', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.011576', 'logps_train/rejected': '-85.02', 'logps_train/chosen': '-127.68', 'loss/train': '0.69877', 'examples_per_second': '30.167', 'grad_norm': '25.5', 'counters/examples': 178592, 'counters/updates': 5581}
train stats after 178624 examples: {'rewards_train/chosen': '0.086747', 'rewards_train/rejected': '0.016977', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.06977', 'logps_train/rejected': '-95.02', 'logps_train/chosen': '-100.03', 'loss/train': '0.6671', 'examples_per_second': '32.933', 'grad_norm': '26.875', 'counters/examples': 178624, 'counters/updates': 5582}
train stats after 178656 examples: {'rewards_train/chosen': '0.2126', 'rewards_train/rejected': '0.11845', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094143', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-136.98', 'loss/train': '0.65808', 'examples_per_second': '30.953', 'grad_norm': '28.5', 'counters/examples': 178656, 'counters/updates': 5583}
skipping logging after 178688 examples to avoid logging too frequently
train stats after 178720 examples: {'rewards_train/chosen': '0.12489', 'rewards_train/rejected': '0.067915', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.056972', 'logps_train/rejected': '-91.028', 'logps_train/chosen': '-125.29', 'loss/train': '0.67377', 'examples_per_second': '32.962', 'grad_norm': '23.25', 'counters/examples': 178720, 'counters/updates': 5585}
skipping logging after 178752 examples to avoid logging too frequently
train stats after 178784 examples: {'rewards_train/chosen': '0.18253', 'rewards_train/rejected': '0.061669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12086', 'logps_train/rejected': '-147.21', 'logps_train/chosen': '-192.5', 'loss/train': '0.64882', 'examples_per_second': '31.483', 'grad_norm': '30.125', 'counters/examples': 178784, 'counters/updates': 5587}
skipping logging after 178816 examples to avoid logging too frequently
train stats after 178848 examples: {'rewards_train/chosen': '0.13953', 'rewards_train/rejected': '0.022319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11721', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-133.82', 'loss/train': '0.66067', 'examples_per_second': '31.134', 'grad_norm': '30.625', 'counters/examples': 178848, 'counters/updates': 5589}
train stats after 178880 examples: {'rewards_train/chosen': '0.17237', 'rewards_train/rejected': '0.10182', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070548', 'logps_train/rejected': '-177.34', 'logps_train/chosen': '-136.93', 'loss/train': '0.67139', 'examples_per_second': '31.106', 'grad_norm': '47.5', 'counters/examples': 178880, 'counters/updates': 5590}
train stats after 178912 examples: {'rewards_train/chosen': '0.1187', 'rewards_train/rejected': '0.074919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043784', 'logps_train/rejected': '-128.22', 'logps_train/chosen': '-150.64', 'loss/train': '0.68329', 'examples_per_second': '31.516', 'grad_norm': '32.5', 'counters/examples': 178912, 'counters/updates': 5591}
train stats after 178944 examples: {'rewards_train/chosen': '0.16909', 'rewards_train/rejected': '0.18481', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.015716', 'logps_train/rejected': '-143.02', 'logps_train/chosen': '-141.76', 'loss/train': '0.70753', 'examples_per_second': '30.872', 'grad_norm': '30.125', 'counters/examples': 178944, 'counters/updates': 5592}
train stats after 178976 examples: {'rewards_train/chosen': '0.20385', 'rewards_train/rejected': '0.11445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0894', 'logps_train/rejected': '-114.33', 'logps_train/chosen': '-136.26', 'loss/train': '0.6787', 'examples_per_second': '30.775', 'grad_norm': '37', 'counters/examples': 178976, 'counters/updates': 5593}
train stats after 179008 examples: {'rewards_train/chosen': '0.054819', 'rewards_train/rejected': '0.013273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.041546', 'logps_train/rejected': '-143.07', 'logps_train/chosen': '-128.41', 'loss/train': '0.68444', 'examples_per_second': '30.086', 'grad_norm': '29.875', 'counters/examples': 179008, 'counters/updates': 5594}
train stats after 179040 examples: {'rewards_train/chosen': '0.12539', 'rewards_train/rejected': '0.07156', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053834', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-122.26', 'loss/train': '0.67682', 'examples_per_second': '30.899', 'grad_norm': '26.375', 'counters/examples': 179040, 'counters/updates': 5595}
train stats after 179072 examples: {'rewards_train/chosen': '0.15499', 'rewards_train/rejected': '0.069533', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085459', 'logps_train/rejected': '-142.86', 'logps_train/chosen': '-140.65', 'loss/train': '0.66184', 'examples_per_second': '32.352', 'grad_norm': '37', 'counters/examples': 179072, 'counters/updates': 5596}
train stats after 179104 examples: {'rewards_train/chosen': '0.11073', 'rewards_train/rejected': '0.0082603', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.10247', 'logps_train/rejected': '-100.11', 'logps_train/chosen': '-153.49', 'loss/train': '0.65918', 'examples_per_second': '32.915', 'grad_norm': '30.375', 'counters/examples': 179104, 'counters/updates': 5597}
skipping logging after 179136 examples to avoid logging too frequently
train stats after 179168 examples: {'rewards_train/chosen': '0.1163', 'rewards_train/rejected': '0.053677', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062625', 'logps_train/rejected': '-108.77', 'logps_train/chosen': '-136.64', 'loss/train': '0.67018', 'examples_per_second': '32.939', 'grad_norm': '26.625', 'counters/examples': 179168, 'counters/updates': 5599}
train stats after 179200 examples: {'rewards_train/chosen': '0.09111', 'rewards_train/rejected': '0.034596', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056514', 'logps_train/rejected': '-145.24', 'logps_train/chosen': '-159.19', 'loss/train': '0.68289', 'examples_per_second': '31.228', 'grad_norm': '36.5', 'counters/examples': 179200, 'counters/updates': 5600}
skipping logging after 179232 examples to avoid logging too frequently
train stats after 179264 examples: {'rewards_train/chosen': '0.13929', 'rewards_train/rejected': '-0.044902', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18419', 'logps_train/rejected': '-167.1', 'logps_train/chosen': '-163.03', 'loss/train': '0.61915', 'examples_per_second': '30.34', 'grad_norm': '39.5', 'counters/examples': 179264, 'counters/updates': 5602}
train stats after 179296 examples: {'rewards_train/chosen': '0.17215', 'rewards_train/rejected': '-0.011901', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18405', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-156.3', 'loss/train': '0.61781', 'examples_per_second': '25.507', 'grad_norm': '25.75', 'counters/examples': 179296, 'counters/updates': 5603}
train stats after 179328 examples: {'rewards_train/chosen': '0.086719', 'rewards_train/rejected': '0.02041', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066309', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-137.61', 'loss/train': '0.66832', 'examples_per_second': '30.409', 'grad_norm': '40', 'counters/examples': 179328, 'counters/updates': 5604}
train stats after 179360 examples: {'rewards_train/chosen': '0.16596', 'rewards_train/rejected': '0.12156', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044404', 'logps_train/rejected': '-144.75', 'logps_train/chosen': '-120.75', 'loss/train': '0.6804', 'examples_per_second': '32.31', 'grad_norm': '32.25', 'counters/examples': 179360, 'counters/updates': 5605}
train stats after 179392 examples: {'rewards_train/chosen': '0.12284', 'rewards_train/rejected': '0.043693', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079147', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-139.53', 'loss/train': '0.66874', 'examples_per_second': '25.882', 'grad_norm': '26.25', 'counters/examples': 179392, 'counters/updates': 5606}
skipping logging after 179424 examples to avoid logging too frequently
train stats after 179456 examples: {'rewards_train/chosen': '0.022665', 'rewards_train/rejected': '0.065785', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.04312', 'logps_train/rejected': '-104.7', 'logps_train/chosen': '-117.48', 'loss/train': '0.72029', 'examples_per_second': '41.478', 'grad_norm': '25.5', 'counters/examples': 179456, 'counters/updates': 5608}
train stats after 179488 examples: {'rewards_train/chosen': '0.12638', 'rewards_train/rejected': '0.13605', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0096675', 'logps_train/rejected': '-141.08', 'logps_train/chosen': '-129.8', 'loss/train': '0.70682', 'examples_per_second': '31.41', 'grad_norm': '50.5', 'counters/examples': 179488, 'counters/updates': 5609}
train stats after 179520 examples: {'rewards_train/chosen': '0.19022', 'rewards_train/rejected': '-0.0062602', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19648', 'logps_train/rejected': '-120.48', 'logps_train/chosen': '-159.41', 'loss/train': '0.61276', 'examples_per_second': '31.152', 'grad_norm': '26.125', 'counters/examples': 179520, 'counters/updates': 5610}
train stats after 179552 examples: {'rewards_train/chosen': '0.060729', 'rewards_train/rejected': '-0.0095992', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070328', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-169.78', 'loss/train': '0.66581', 'examples_per_second': '29.999', 'grad_norm': '28.375', 'counters/examples': 179552, 'counters/updates': 5611}
train stats after 179584 examples: {'rewards_train/chosen': '0.22779', 'rewards_train/rejected': '0.051805', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17599', 'logps_train/rejected': '-106.89', 'logps_train/chosen': '-154.01', 'loss/train': '0.62665', 'examples_per_second': '31.969', 'grad_norm': '30.625', 'counters/examples': 179584, 'counters/updates': 5612}
skipping logging after 179616 examples to avoid logging too frequently
train stats after 179648 examples: {'rewards_train/chosen': '0.27744', 'rewards_train/rejected': '0.061872', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21557', 'logps_train/rejected': '-171.2', 'logps_train/chosen': '-184.33', 'loss/train': '0.61099', 'examples_per_second': '33.33', 'grad_norm': '34', 'counters/examples': 179648, 'counters/updates': 5614}
train stats after 179680 examples: {'rewards_train/chosen': '0.1656', 'rewards_train/rejected': '0.12046', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045144', 'logps_train/rejected': '-116.87', 'logps_train/chosen': '-116.18', 'loss/train': '0.68443', 'examples_per_second': '30.546', 'grad_norm': '30.75', 'counters/examples': 179680, 'counters/updates': 5615}
train stats after 179712 examples: {'rewards_train/chosen': '0.29835', 'rewards_train/rejected': '0.1645', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.13385', 'logps_train/rejected': '-133.33', 'logps_train/chosen': '-157.56', 'loss/train': '0.65833', 'examples_per_second': '30.174', 'grad_norm': '42.75', 'counters/examples': 179712, 'counters/updates': 5616}
skipping logging after 179744 examples to avoid logging too frequently
train stats after 179776 examples: {'rewards_train/chosen': '0.17883', 'rewards_train/rejected': '0.10959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069245', 'logps_train/rejected': '-142.29', 'logps_train/chosen': '-174.94', 'loss/train': '0.67087', 'examples_per_second': '31.436', 'grad_norm': '31', 'counters/examples': 179776, 'counters/updates': 5618}
train stats after 179808 examples: {'rewards_train/chosen': '0.16569', 'rewards_train/rejected': '0.18381', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018119', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-139.52', 'loss/train': '0.72019', 'examples_per_second': '31.503', 'grad_norm': '38.5', 'counters/examples': 179808, 'counters/updates': 5619}
train stats after 179840 examples: {'rewards_train/chosen': '0.16031', 'rewards_train/rejected': '0.080644', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07967', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-135.59', 'loss/train': '0.68844', 'examples_per_second': '31.523', 'grad_norm': '46.75', 'counters/examples': 179840, 'counters/updates': 5620}
skipping logging after 179872 examples to avoid logging too frequently
train stats after 179904 examples: {'rewards_train/chosen': '0.091385', 'rewards_train/rejected': '-0.0065787', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097964', 'logps_train/rejected': '-103.46', 'logps_train/chosen': '-113.67', 'loss/train': '0.6526', 'examples_per_second': '32.575', 'grad_norm': '28', 'counters/examples': 179904, 'counters/updates': 5622}
train stats after 179936 examples: {'rewards_train/chosen': '0.089356', 'rewards_train/rejected': '0.0086284', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.080728', 'logps_train/rejected': '-125.18', 'logps_train/chosen': '-140.67', 'loss/train': '0.66852', 'examples_per_second': '31.502', 'grad_norm': '40', 'counters/examples': 179936, 'counters/updates': 5623}
skipping logging after 179968 examples to avoid logging too frequently
train stats after 180000 examples: {'rewards_train/chosen': '0.16226', 'rewards_train/rejected': '0.037604', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12465', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-142.45', 'loss/train': '0.64549', 'examples_per_second': '31.403', 'grad_norm': '28.5', 'counters/examples': 180000, 'counters/updates': 5625}
Running evaluation after 180000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.97it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.89it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.90it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.91it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.82it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.79it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 180000: {'rewards_eval/chosen': '0.18383', 'rewards_eval/rejected': '0.069709', 'rewards_eval/accuracies': '0.625', 'rewards_eval/margins': '0.11412', 'logps_eval/rejected': '-121.44', 'logps_eval/chosen': '-142.27', 'loss/eval': '0.65158'}
skipping save for non epoch
train stats after 180032 examples: {'rewards_train/chosen': '0.16331', 'rewards_train/rejected': '0.078914', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.084392', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-151.23', 'loss/train': '0.67111', 'examples_per_second': '31.994', 'grad_norm': '44', 'counters/examples': 180032, 'counters/updates': 5626}
skipping logging after 180064 examples to avoid logging too frequently
train stats after 180096 examples: {'rewards_train/chosen': '0.14864', 'rewards_train/rejected': '-0.016051', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16469', 'logps_train/rejected': '-122.68', 'logps_train/chosen': '-158.93', 'loss/train': '0.62968', 'examples_per_second': '32.641', 'grad_norm': '37', 'counters/examples': 180096, 'counters/updates': 5628}
train stats after 180128 examples: {'rewards_train/chosen': '0.13303', 'rewards_train/rejected': '0.089483', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043551', 'logps_train/rejected': '-104.73', 'logps_train/chosen': '-129.31', 'loss/train': '0.67919', 'examples_per_second': '29.776', 'grad_norm': '36.75', 'counters/examples': 180128, 'counters/updates': 5629}
skipping logging after 180160 examples to avoid logging too frequently
train stats after 180192 examples: {'rewards_train/chosen': '0.16327', 'rewards_train/rejected': '0.058847', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10442', 'logps_train/rejected': '-124.69', 'logps_train/chosen': '-173.61', 'loss/train': '0.65389', 'examples_per_second': '31.44', 'grad_norm': '30.75', 'counters/examples': 180192, 'counters/updates': 5631}
train stats after 180224 examples: {'rewards_train/chosen': '0.11769', 'rewards_train/rejected': '0.10199', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.015697', 'logps_train/rejected': '-174.89', 'logps_train/chosen': '-138.38', 'loss/train': '0.69303', 'examples_per_second': '30.814', 'grad_norm': '34.5', 'counters/examples': 180224, 'counters/updates': 5632}
train stats after 180256 examples: {'rewards_train/chosen': '0.15941', 'rewards_train/rejected': '0.019888', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13952', 'logps_train/rejected': '-100.45', 'logps_train/chosen': '-183.28', 'loss/train': '0.63309', 'examples_per_second': '30.065', 'grad_norm': '26.125', 'counters/examples': 180256, 'counters/updates': 5633}
train stats after 180288 examples: {'rewards_train/chosen': '0.077854', 'rewards_train/rejected': '-6.9329e-05', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077923', 'logps_train/rejected': '-105.56', 'logps_train/chosen': '-140.35', 'loss/train': '0.66425', 'examples_per_second': '30.794', 'grad_norm': '30.625', 'counters/examples': 180288, 'counters/updates': 5634}
train stats after 180320 examples: {'rewards_train/chosen': '0.18328', 'rewards_train/rejected': '0.032929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15035', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-168.29', 'loss/train': '0.64049', 'examples_per_second': '31.98', 'grad_norm': '37.25', 'counters/examples': 180320, 'counters/updates': 5635}
train stats after 180352 examples: {'rewards_train/chosen': '0.068825', 'rewards_train/rejected': '0.17945', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.11063', 'logps_train/rejected': '-154.4', 'logps_train/chosen': '-146.42', 'loss/train': '0.76651', 'examples_per_second': '31.886', 'grad_norm': '49.5', 'counters/examples': 180352, 'counters/updates': 5636}
skipping logging after 180384 examples to avoid logging too frequently
train stats after 180416 examples: {'rewards_train/chosen': '0.17238', 'rewards_train/rejected': '0.042226', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13015', 'logps_train/rejected': '-100.03', 'logps_train/chosen': '-117.44', 'loss/train': '0.65266', 'examples_per_second': '31.509', 'grad_norm': '26.125', 'counters/examples': 180416, 'counters/updates': 5638}
train stats after 180448 examples: {'rewards_train/chosen': '0.12174', 'rewards_train/rejected': '0.068238', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.053504', 'logps_train/rejected': '-126.51', 'logps_train/chosen': '-111.78', 'loss/train': '0.67961', 'examples_per_second': '31.777', 'grad_norm': '29.125', 'counters/examples': 180448, 'counters/updates': 5639}
train stats after 180480 examples: {'rewards_train/chosen': '0.12072', 'rewards_train/rejected': '0.053334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06739', 'logps_train/rejected': '-144.55', 'logps_train/chosen': '-121.58', 'loss/train': '0.67614', 'examples_per_second': '31.276', 'grad_norm': '38.25', 'counters/examples': 180480, 'counters/updates': 5640}
skipping logging after 180512 examples to avoid logging too frequently
train stats after 180544 examples: {'rewards_train/chosen': '0.12206', 'rewards_train/rejected': '0.065757', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056306', 'logps_train/rejected': '-98.903', 'logps_train/chosen': '-106.9', 'loss/train': '0.67304', 'examples_per_second': '34.72', 'grad_norm': '24.125', 'counters/examples': 180544, 'counters/updates': 5642}
train stats after 180576 examples: {'rewards_train/chosen': '0.096919', 'rewards_train/rejected': '-0.012595', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10951', 'logps_train/rejected': '-105.07', 'logps_train/chosen': '-168.86', 'loss/train': '0.64709', 'examples_per_second': '29.876', 'grad_norm': '48', 'counters/examples': 180576, 'counters/updates': 5643}
train stats after 180608 examples: {'rewards_train/chosen': '0.21927', 'rewards_train/rejected': '0.083278', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.136', 'logps_train/rejected': '-133.11', 'logps_train/chosen': '-176.13', 'loss/train': '0.64976', 'examples_per_second': '30.032', 'grad_norm': '46.5', 'counters/examples': 180608, 'counters/updates': 5644}
train stats after 180640 examples: {'rewards_train/chosen': '0.094887', 'rewards_train/rejected': '0.084515', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010372', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-125.43', 'loss/train': '0.69916', 'examples_per_second': '31.238', 'grad_norm': '31.625', 'counters/examples': 180640, 'counters/updates': 5645}
train stats after 180672 examples: {'rewards_train/chosen': '0.18554', 'rewards_train/rejected': '0.032354', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15318', 'logps_train/rejected': '-96.636', 'logps_train/chosen': '-142.25', 'loss/train': '0.62685', 'examples_per_second': '32.648', 'grad_norm': '29.75', 'counters/examples': 180672, 'counters/updates': 5646}
train stats after 180704 examples: {'rewards_train/chosen': '-0.05875', 'rewards_train/rejected': '-0.068433', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0096824', 'logps_train/rejected': '-117.77', 'logps_train/chosen': '-128.54', 'loss/train': '0.69565', 'examples_per_second': '31.549', 'grad_norm': '28.375', 'counters/examples': 180704, 'counters/updates': 5647}
train stats after 180736 examples: {'rewards_train/chosen': '0.15875', 'rewards_train/rejected': '0.14877', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0099803', 'logps_train/rejected': '-134.1', 'logps_train/chosen': '-134.99', 'loss/train': '0.71418', 'examples_per_second': '31.118', 'grad_norm': '34', 'counters/examples': 180736, 'counters/updates': 5648}
train stats after 180768 examples: {'rewards_train/chosen': '0.1588', 'rewards_train/rejected': '0.061733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097064', 'logps_train/rejected': '-110.32', 'logps_train/chosen': '-138.39', 'loss/train': '0.65618', 'examples_per_second': '30.74', 'grad_norm': '42.25', 'counters/examples': 180768, 'counters/updates': 5649}
skipping logging after 180800 examples to avoid logging too frequently
train stats after 180832 examples: {'rewards_train/chosen': '0.14643', 'rewards_train/rejected': '0.01892', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12751', 'logps_train/rejected': '-145.71', 'logps_train/chosen': '-165.76', 'loss/train': '0.6508', 'examples_per_second': '31.883', 'grad_norm': '44.5', 'counters/examples': 180832, 'counters/updates': 5651}
train stats after 180864 examples: {'rewards_train/chosen': '0.073215', 'rewards_train/rejected': '0.026799', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046416', 'logps_train/rejected': '-116.64', 'logps_train/chosen': '-171.61', 'loss/train': '0.68446', 'examples_per_second': '31.258', 'grad_norm': '29', 'counters/examples': 180864, 'counters/updates': 5652}
train stats after 180896 examples: {'rewards_train/chosen': '0.18214', 'rewards_train/rejected': '0.11389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068245', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-161.77', 'loss/train': '0.68039', 'examples_per_second': '30.629', 'grad_norm': '41.25', 'counters/examples': 180896, 'counters/updates': 5653}
train stats after 180928 examples: {'rewards_train/chosen': '0.19597', 'rewards_train/rejected': '0.047655', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14832', 'logps_train/rejected': '-154.64', 'logps_train/chosen': '-157.92', 'loss/train': '0.64015', 'examples_per_second': '30.389', 'grad_norm': '35.75', 'counters/examples': 180928, 'counters/updates': 5654}
train stats after 180960 examples: {'rewards_train/chosen': '0.19302', 'rewards_train/rejected': '0.059767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13325', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-125.09', 'loss/train': '0.65013', 'examples_per_second': '32.65', 'grad_norm': '36.75', 'counters/examples': 180960, 'counters/updates': 5655}
train stats after 180992 examples: {'rewards_train/chosen': '0.086706', 'rewards_train/rejected': '-0.017034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10374', 'logps_train/rejected': '-149.44', 'logps_train/chosen': '-168.66', 'loss/train': '0.68272', 'examples_per_second': '31.525', 'grad_norm': '34.25', 'counters/examples': 180992, 'counters/updates': 5656}
train stats after 181024 examples: {'rewards_train/chosen': '0.13008', 'rewards_train/rejected': '0.078139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051939', 'logps_train/rejected': '-108.99', 'logps_train/chosen': '-144.51', 'loss/train': '0.67775', 'examples_per_second': '29.812', 'grad_norm': '27.375', 'counters/examples': 181024, 'counters/updates': 5657}
train stats after 181056 examples: {'rewards_train/chosen': '0.17388', 'rewards_train/rejected': '0.079334', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094549', 'logps_train/rejected': '-122.15', 'logps_train/chosen': '-156.88', 'loss/train': '0.66509', 'examples_per_second': '32.875', 'grad_norm': '27.375', 'counters/examples': 181056, 'counters/updates': 5658}
train stats after 181088 examples: {'rewards_train/chosen': '0.15841', 'rewards_train/rejected': '0.084074', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074337', 'logps_train/rejected': '-118.44', 'logps_train/chosen': '-179.18', 'loss/train': '0.66725', 'examples_per_second': '30.64', 'grad_norm': '27.125', 'counters/examples': 181088, 'counters/updates': 5659}
train stats after 181120 examples: {'rewards_train/chosen': '0.23584', 'rewards_train/rejected': '0.059499', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17634', 'logps_train/rejected': '-128', 'logps_train/chosen': '-160.17', 'loss/train': '0.63713', 'examples_per_second': '30.909', 'grad_norm': '29.75', 'counters/examples': 181120, 'counters/updates': 5660}
train stats after 181152 examples: {'rewards_train/chosen': '0.079915', 'rewards_train/rejected': '0.076984', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0029304', 'logps_train/rejected': '-140.11', 'logps_train/chosen': '-174.74', 'loss/train': '0.70537', 'examples_per_second': '31.525', 'grad_norm': '53.75', 'counters/examples': 181152, 'counters/updates': 5661}
train stats after 181184 examples: {'rewards_train/chosen': '0.15097', 'rewards_train/rejected': '-0.0085916', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15956', 'logps_train/rejected': '-129.2', 'logps_train/chosen': '-139.97', 'loss/train': '0.62967', 'examples_per_second': '30.099', 'grad_norm': '27.875', 'counters/examples': 181184, 'counters/updates': 5662}
train stats after 181216 examples: {'rewards_train/chosen': '0.25027', 'rewards_train/rejected': '0.035602', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21467', 'logps_train/rejected': '-115.86', 'logps_train/chosen': '-172.97', 'loss/train': '0.60816', 'examples_per_second': '30.894', 'grad_norm': '33', 'counters/examples': 181216, 'counters/updates': 5663}
train stats after 181248 examples: {'rewards_train/chosen': '0.19882', 'rewards_train/rejected': '0.11877', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080058', 'logps_train/rejected': '-145.72', 'logps_train/chosen': '-169.68', 'loss/train': '0.67735', 'examples_per_second': '30.038', 'grad_norm': '36.75', 'counters/examples': 181248, 'counters/updates': 5664}
train stats after 181280 examples: {'rewards_train/chosen': '0.24182', 'rewards_train/rejected': '0.063916', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17791', 'logps_train/rejected': '-140.95', 'logps_train/chosen': '-200.86', 'loss/train': '0.63173', 'examples_per_second': '32.714', 'grad_norm': '29.375', 'counters/examples': 181280, 'counters/updates': 5665}
train stats after 181312 examples: {'rewards_train/chosen': '0.090582', 'rewards_train/rejected': '0.013292', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077289', 'logps_train/rejected': '-116.96', 'logps_train/chosen': '-124.27', 'loss/train': '0.66101', 'examples_per_second': '33.201', 'grad_norm': '25.875', 'counters/examples': 181312, 'counters/updates': 5666}
skipping logging after 181344 examples to avoid logging too frequently
train stats after 181376 examples: {'rewards_train/chosen': '0.10483', 'rewards_train/rejected': '0.06587', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.038956', 'logps_train/rejected': '-157.14', 'logps_train/chosen': '-120.73', 'loss/train': '0.69568', 'examples_per_second': '30.321', 'grad_norm': '73', 'counters/examples': 181376, 'counters/updates': 5668}
skipping logging after 181408 examples to avoid logging too frequently
train stats after 181440 examples: {'rewards_train/chosen': '0.20411', 'rewards_train/rejected': '-0.0084229', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21253', 'logps_train/rejected': '-141.43', 'logps_train/chosen': '-173.71', 'loss/train': '0.60217', 'examples_per_second': '34.669', 'grad_norm': '37.5', 'counters/examples': 181440, 'counters/updates': 5670}
train stats after 181472 examples: {'rewards_train/chosen': '0.19067', 'rewards_train/rejected': '0.093389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097282', 'logps_train/rejected': '-135.53', 'logps_train/chosen': '-172.55', 'loss/train': '0.65086', 'examples_per_second': '30.334', 'grad_norm': '29', 'counters/examples': 181472, 'counters/updates': 5671}
train stats after 181504 examples: {'rewards_train/chosen': '0.26388', 'rewards_train/rejected': '0.14917', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1147', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-134.82', 'loss/train': '0.65285', 'examples_per_second': '32.139', 'grad_norm': '36.25', 'counters/examples': 181504, 'counters/updates': 5672}
train stats after 181536 examples: {'rewards_train/chosen': '0.28984', 'rewards_train/rejected': '-0.00051357', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.29036', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-175.32', 'loss/train': '0.57989', 'examples_per_second': '31.666', 'grad_norm': '26', 'counters/examples': 181536, 'counters/updates': 5673}
train stats after 181568 examples: {'rewards_train/chosen': '0.066339', 'rewards_train/rejected': '0.0047003', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061639', 'logps_train/rejected': '-127.06', 'logps_train/chosen': '-162.27', 'loss/train': '0.67271', 'examples_per_second': '31.082', 'grad_norm': '35.25', 'counters/examples': 181568, 'counters/updates': 5674}
train stats after 181600 examples: {'rewards_train/chosen': '0.23967', 'rewards_train/rejected': '-0.0018833', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.24155', 'logps_train/rejected': '-158.44', 'logps_train/chosen': '-135.88', 'loss/train': '0.6053', 'examples_per_second': '30.604', 'grad_norm': '31', 'counters/examples': 181600, 'counters/updates': 5675}
train stats after 181632 examples: {'rewards_train/chosen': '0.17679', 'rewards_train/rejected': '0.081612', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095178', 'logps_train/rejected': '-97.094', 'logps_train/chosen': '-116.5', 'loss/train': '0.6578', 'examples_per_second': '32.597', 'grad_norm': '25.25', 'counters/examples': 181632, 'counters/updates': 5676}
train stats after 181664 examples: {'rewards_train/chosen': '0.21872', 'rewards_train/rejected': '0.050603', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16812', 'logps_train/rejected': '-114.54', 'logps_train/chosen': '-160.13', 'loss/train': '0.6421', 'examples_per_second': '31.407', 'grad_norm': '101', 'counters/examples': 181664, 'counters/updates': 5677}
train stats after 181696 examples: {'rewards_train/chosen': '0.12491', 'rewards_train/rejected': '0.03114', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093767', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-150.9', 'loss/train': '0.66223', 'examples_per_second': '31.068', 'grad_norm': '32.75', 'counters/examples': 181696, 'counters/updates': 5678}
train stats after 181728 examples: {'rewards_train/chosen': '0.2254', 'rewards_train/rejected': '0.12732', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098077', 'logps_train/rejected': '-170.32', 'logps_train/chosen': '-182.54', 'loss/train': '0.67267', 'examples_per_second': '31.538', 'grad_norm': '37.25', 'counters/examples': 181728, 'counters/updates': 5679}
train stats after 181760 examples: {'rewards_train/chosen': '0.16942', 'rewards_train/rejected': '0.054227', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11519', 'logps_train/rejected': '-126.85', 'logps_train/chosen': '-157.5', 'loss/train': '0.64734', 'examples_per_second': '31.54', 'grad_norm': '31.125', 'counters/examples': 181760, 'counters/updates': 5680}
skipping logging after 181792 examples to avoid logging too frequently
train stats after 181824 examples: {'rewards_train/chosen': '0.10952', 'rewards_train/rejected': '0.058256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051264', 'logps_train/rejected': '-91.784', 'logps_train/chosen': '-111.44', 'loss/train': '0.67709', 'examples_per_second': '31.655', 'grad_norm': '29.75', 'counters/examples': 181824, 'counters/updates': 5682}
train stats after 181856 examples: {'rewards_train/chosen': '0.37868', 'rewards_train/rejected': '0.1151', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26358', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-155.14', 'loss/train': '0.59756', 'examples_per_second': '32.468', 'grad_norm': '29.625', 'counters/examples': 181856, 'counters/updates': 5683}
train stats after 181888 examples: {'rewards_train/chosen': '0.16745', 'rewards_train/rejected': '0.035782', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13167', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-163.69', 'loss/train': '0.64173', 'examples_per_second': '32.013', 'grad_norm': '30.75', 'counters/examples': 181888, 'counters/updates': 5684}
skipping logging after 181920 examples to avoid logging too frequently
train stats after 181952 examples: {'rewards_train/chosen': '0.14185', 'rewards_train/rejected': '0.076449', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065398', 'logps_train/rejected': '-110.47', 'logps_train/chosen': '-104.01', 'loss/train': '0.66883', 'examples_per_second': '35.071', 'grad_norm': '25.25', 'counters/examples': 181952, 'counters/updates': 5686}
train stats after 181984 examples: {'rewards_train/chosen': '0.096357', 'rewards_train/rejected': '0.11847', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022114', 'logps_train/rejected': '-137.37', 'logps_train/chosen': '-135.78', 'loss/train': '0.72068', 'examples_per_second': '31.515', 'grad_norm': '33.5', 'counters/examples': 181984, 'counters/updates': 5687}
skipping logging after 182016 examples to avoid logging too frequently
train stats after 182048 examples: {'rewards_train/chosen': '0.083354', 'rewards_train/rejected': '0.048289', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035066', 'logps_train/rejected': '-125.9', 'logps_train/chosen': '-133.97', 'loss/train': '0.68205', 'examples_per_second': '33.421', 'grad_norm': '28.875', 'counters/examples': 182048, 'counters/updates': 5689}
train stats after 182080 examples: {'rewards_train/chosen': '0.098769', 'rewards_train/rejected': '0.006613', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.092156', 'logps_train/rejected': '-169.52', 'logps_train/chosen': '-161.46', 'loss/train': '0.66072', 'examples_per_second': '31.534', 'grad_norm': '35.25', 'counters/examples': 182080, 'counters/updates': 5690}
skipping logging after 182112 examples to avoid logging too frequently
train stats after 182144 examples: {'rewards_train/chosen': '0.18979', 'rewards_train/rejected': '0.083091', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10669', 'logps_train/rejected': '-106.91', 'logps_train/chosen': '-142.45', 'loss/train': '0.66598', 'examples_per_second': '33.086', 'grad_norm': '31.75', 'counters/examples': 182144, 'counters/updates': 5692}
skipping logging after 182176 examples to avoid logging too frequently
train stats after 182208 examples: {'rewards_train/chosen': '0.17856', 'rewards_train/rejected': '0.13444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044122', 'logps_train/rejected': '-153.56', 'logps_train/chosen': '-158.02', 'loss/train': '0.6849', 'examples_per_second': '31.207', 'grad_norm': '29', 'counters/examples': 182208, 'counters/updates': 5694}
skipping logging after 182240 examples to avoid logging too frequently
train stats after 182272 examples: {'rewards_train/chosen': '0.18433', 'rewards_train/rejected': '0.082991', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10134', 'logps_train/rejected': '-125.33', 'logps_train/chosen': '-156.57', 'loss/train': '0.65276', 'examples_per_second': '31.018', 'grad_norm': '33', 'counters/examples': 182272, 'counters/updates': 5696}
train stats after 182304 examples: {'rewards_train/chosen': '0.14354', 'rewards_train/rejected': '0.05588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087658', 'logps_train/rejected': '-98.237', 'logps_train/chosen': '-135.75', 'loss/train': '0.65865', 'examples_per_second': '31.448', 'grad_norm': '29.25', 'counters/examples': 182304, 'counters/updates': 5697}
train stats after 182336 examples: {'rewards_train/chosen': '0.17365', 'rewards_train/rejected': '0.026276', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14738', 'logps_train/rejected': '-126.39', 'logps_train/chosen': '-160.56', 'loss/train': '0.6383', 'examples_per_second': '31.642', 'grad_norm': '37.75', 'counters/examples': 182336, 'counters/updates': 5698}
train stats after 182368 examples: {'rewards_train/chosen': '0.14344', 'rewards_train/rejected': '0.074479', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068964', 'logps_train/rejected': '-133.95', 'logps_train/chosen': '-135.99', 'loss/train': '0.66686', 'examples_per_second': '30.559', 'grad_norm': '26.25', 'counters/examples': 182368, 'counters/updates': 5699}
train stats after 182400 examples: {'rewards_train/chosen': '0.26652', 'rewards_train/rejected': '-0.034901', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.30142', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-169.09', 'loss/train': '0.56598', 'examples_per_second': '31.253', 'grad_norm': '27.875', 'counters/examples': 182400, 'counters/updates': 5700}
train stats after 182432 examples: {'rewards_train/chosen': '0.17105', 'rewards_train/rejected': '0.059793', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11126', 'logps_train/rejected': '-105.25', 'logps_train/chosen': '-113.25', 'loss/train': '0.65628', 'examples_per_second': '32.612', 'grad_norm': '36.5', 'counters/examples': 182432, 'counters/updates': 5701}
train stats after 182464 examples: {'rewards_train/chosen': '0.15124', 'rewards_train/rejected': '0.13085', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02039', 'logps_train/rejected': '-110.56', 'logps_train/chosen': '-149.93', 'loss/train': '0.69705', 'examples_per_second': '31.049', 'grad_norm': '33', 'counters/examples': 182464, 'counters/updates': 5702}
train stats after 182496 examples: {'rewards_train/chosen': '0.14475', 'rewards_train/rejected': '0.089265', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055483', 'logps_train/rejected': '-123.8', 'logps_train/chosen': '-125.84', 'loss/train': '0.6884', 'examples_per_second': '31.493', 'grad_norm': '29.375', 'counters/examples': 182496, 'counters/updates': 5703}
train stats after 182528 examples: {'rewards_train/chosen': '0.1671', 'rewards_train/rejected': '0.050169', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11693', 'logps_train/rejected': '-140.84', 'logps_train/chosen': '-116.08', 'loss/train': '0.64838', 'examples_per_second': '31.273', 'grad_norm': '31.125', 'counters/examples': 182528, 'counters/updates': 5704}
skipping logging after 182560 examples to avoid logging too frequently
train stats after 182592 examples: {'rewards_train/chosen': '0.19327', 'rewards_train/rejected': '0.068605', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12466', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-130.07', 'loss/train': '0.64339', 'examples_per_second': '32.732', 'grad_norm': '30.875', 'counters/examples': 182592, 'counters/updates': 5706}
train stats after 182624 examples: {'rewards_train/chosen': '0.21549', 'rewards_train/rejected': '0.12763', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087856', 'logps_train/rejected': '-137.9', 'logps_train/chosen': '-127.43', 'loss/train': '0.66497', 'examples_per_second': '24.61', 'grad_norm': '35.5', 'counters/examples': 182624, 'counters/updates': 5707}
skipping logging after 182656 examples to avoid logging too frequently
train stats after 182688 examples: {'rewards_train/chosen': '0.12131', 'rewards_train/rejected': '0.026405', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094908', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-135.76', 'loss/train': '0.66665', 'examples_per_second': '31.094', 'grad_norm': '26.5', 'counters/examples': 182688, 'counters/updates': 5709}
train stats after 182720 examples: {'rewards_train/chosen': '0.21988', 'rewards_train/rejected': '0.21994', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-6.2031e-05', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-174.72', 'loss/train': '0.70418', 'examples_per_second': '30.402', 'grad_norm': '39.75', 'counters/examples': 182720, 'counters/updates': 5710}
train stats after 182752 examples: {'rewards_train/chosen': '0.10831', 'rewards_train/rejected': '0.041899', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06641', 'logps_train/rejected': '-131.28', 'logps_train/chosen': '-125.89', 'loss/train': '0.66867', 'examples_per_second': '31.471', 'grad_norm': '32', 'counters/examples': 182752, 'counters/updates': 5711}
train stats after 182784 examples: {'rewards_train/chosen': '0.1406', 'rewards_train/rejected': '0.049667', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090928', 'logps_train/rejected': '-92.115', 'logps_train/chosen': '-145.01', 'loss/train': '0.66845', 'examples_per_second': '31.498', 'grad_norm': '53.25', 'counters/examples': 182784, 'counters/updates': 5712}
train stats after 182816 examples: {'rewards_train/chosen': '0.1153', 'rewards_train/rejected': '0.090749', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.024552', 'logps_train/rejected': '-125.98', 'logps_train/chosen': '-154.76', 'loss/train': '0.69691', 'examples_per_second': '30.292', 'grad_norm': '45.25', 'counters/examples': 182816, 'counters/updates': 5713}
train stats after 182848 examples: {'rewards_train/chosen': '0.097573', 'rewards_train/rejected': '0.010346', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087227', 'logps_train/rejected': '-136.28', 'logps_train/chosen': '-118.43', 'loss/train': '0.66212', 'examples_per_second': '31.379', 'grad_norm': '32.25', 'counters/examples': 182848, 'counters/updates': 5714}
train stats after 182880 examples: {'rewards_train/chosen': '0.20809', 'rewards_train/rejected': '0.12207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086026', 'logps_train/rejected': '-120.3', 'logps_train/chosen': '-132.07', 'loss/train': '0.67261', 'examples_per_second': '29.947', 'grad_norm': '36.75', 'counters/examples': 182880, 'counters/updates': 5715}
train stats after 182912 examples: {'rewards_train/chosen': '0.085652', 'rewards_train/rejected': '-0.069578', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15523', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-149.22', 'loss/train': '0.62812', 'examples_per_second': '32.79', 'grad_norm': '25.25', 'counters/examples': 182912, 'counters/updates': 5716}
train stats after 182944 examples: {'rewards_train/chosen': '0.24414', 'rewards_train/rejected': '0.047369', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19678', 'logps_train/rejected': '-138.43', 'logps_train/chosen': '-171.67', 'loss/train': '0.61736', 'examples_per_second': '29.956', 'grad_norm': '29.375', 'counters/examples': 182944, 'counters/updates': 5717}
train stats after 182976 examples: {'rewards_train/chosen': '0.090497', 'rewards_train/rejected': '0.09282', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0023229', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-142.97', 'loss/train': '0.7042', 'examples_per_second': '31.441', 'grad_norm': '30.25', 'counters/examples': 182976, 'counters/updates': 5718}
skipping logging after 183008 examples to avoid logging too frequently
train stats after 183040 examples: {'rewards_train/chosen': '0.20219', 'rewards_train/rejected': '0.074217', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12797', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-172.15', 'loss/train': '0.64334', 'examples_per_second': '34.275', 'grad_norm': '66.5', 'counters/examples': 183040, 'counters/updates': 5720}
skipping logging after 183072 examples to avoid logging too frequently
train stats after 183104 examples: {'rewards_train/chosen': '0.15853', 'rewards_train/rejected': '0.079059', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07947', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-131.74', 'loss/train': '0.66263', 'examples_per_second': '32.036', 'grad_norm': '33', 'counters/examples': 183104, 'counters/updates': 5722}
train stats after 183136 examples: {'rewards_train/chosen': '0.10672', 'rewards_train/rejected': '0.049354', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.057364', 'logps_train/rejected': '-91.927', 'logps_train/chosen': '-121.66', 'loss/train': '0.6715', 'examples_per_second': '29.927', 'grad_norm': '23.125', 'counters/examples': 183136, 'counters/updates': 5723}
train stats after 183168 examples: {'rewards_train/chosen': '0.12243', 'rewards_train/rejected': '0.14464', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.022211', 'logps_train/rejected': '-126.64', 'logps_train/chosen': '-173.72', 'loss/train': '0.72523', 'examples_per_second': '29.938', 'grad_norm': '55.75', 'counters/examples': 183168, 'counters/updates': 5724}
train stats after 183200 examples: {'rewards_train/chosen': '0.31094', 'rewards_train/rejected': '0.080967', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22997', 'logps_train/rejected': '-139.3', 'logps_train/chosen': '-157.59', 'loss/train': '0.61747', 'examples_per_second': '30.949', 'grad_norm': '32.5', 'counters/examples': 183200, 'counters/updates': 5725}
train stats after 183232 examples: {'rewards_train/chosen': '0.12106', 'rewards_train/rejected': '0.090757', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.030303', 'logps_train/rejected': '-129.12', 'logps_train/chosen': '-157.68', 'loss/train': '0.69086', 'examples_per_second': '31.453', 'grad_norm': '36.5', 'counters/examples': 183232, 'counters/updates': 5726}
train stats after 183264 examples: {'rewards_train/chosen': '0.20126', 'rewards_train/rejected': '0.15241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.048851', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-142.72', 'loss/train': '0.7009', 'examples_per_second': '31.222', 'grad_norm': '37', 'counters/examples': 183264, 'counters/updates': 5727}
train stats after 183296 examples: {'rewards_train/chosen': '0.18592', 'rewards_train/rejected': '0.035762', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15016', 'logps_train/rejected': '-127.8', 'logps_train/chosen': '-148.66', 'loss/train': '0.62986', 'examples_per_second': '31.555', 'grad_norm': '37.75', 'counters/examples': 183296, 'counters/updates': 5728}
skipping logging after 183328 examples to avoid logging too frequently
train stats after 183360 examples: {'rewards_train/chosen': '0.1696', 'rewards_train/rejected': '0.075013', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094586', 'logps_train/rejected': '-137.48', 'logps_train/chosen': '-156.41', 'loss/train': '0.65669', 'examples_per_second': '31.95', 'grad_norm': '60.25', 'counters/examples': 183360, 'counters/updates': 5730}
train stats after 183392 examples: {'rewards_train/chosen': '0.17744', 'rewards_train/rejected': '0.015372', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16207', 'logps_train/rejected': '-122.88', 'logps_train/chosen': '-132.91', 'loss/train': '0.63548', 'examples_per_second': '31.593', 'grad_norm': '33', 'counters/examples': 183392, 'counters/updates': 5731}
train stats after 183424 examples: {'rewards_train/chosen': '0.088252', 'rewards_train/rejected': '-0.035699', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12395', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-178.51', 'loss/train': '0.64217', 'examples_per_second': '31.122', 'grad_norm': '32.5', 'counters/examples': 183424, 'counters/updates': 5732}
train stats after 183456 examples: {'rewards_train/chosen': '0.25167', 'rewards_train/rejected': '0.018203', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23347', 'logps_train/rejected': '-114.9', 'logps_train/chosen': '-174.98', 'loss/train': '0.62108', 'examples_per_second': '32.471', 'grad_norm': '33.25', 'counters/examples': 183456, 'counters/updates': 5733}
train stats after 183488 examples: {'rewards_train/chosen': '0.17031', 'rewards_train/rejected': '0.03322', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13709', 'logps_train/rejected': '-139.42', 'logps_train/chosen': '-167.57', 'loss/train': '0.64134', 'examples_per_second': '31.664', 'grad_norm': '28.875', 'counters/examples': 183488, 'counters/updates': 5734}
train stats after 183520 examples: {'rewards_train/chosen': '0.2824', 'rewards_train/rejected': '0.10795', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17445', 'logps_train/rejected': '-142.47', 'logps_train/chosen': '-179.45', 'loss/train': '0.61767', 'examples_per_second': '31.247', 'grad_norm': '31.875', 'counters/examples': 183520, 'counters/updates': 5735}
skipping logging after 183552 examples to avoid logging too frequently
train stats after 183584 examples: {'rewards_train/chosen': '0.1764', 'rewards_train/rejected': '-0.015673', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19207', 'logps_train/rejected': '-130.04', 'logps_train/chosen': '-158.88', 'loss/train': '0.61525', 'examples_per_second': '31.465', 'grad_norm': '30.875', 'counters/examples': 183584, 'counters/updates': 5737}
train stats after 183616 examples: {'rewards_train/chosen': '0.1919', 'rewards_train/rejected': '0.15419', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03771', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-161.08', 'loss/train': '0.70527', 'examples_per_second': '31.463', 'grad_norm': '43', 'counters/examples': 183616, 'counters/updates': 5738}
train stats after 183648 examples: {'rewards_train/chosen': '0.23701', 'rewards_train/rejected': '0.091166', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14585', 'logps_train/rejected': '-113.15', 'logps_train/chosen': '-150.34', 'loss/train': '0.63732', 'examples_per_second': '31.274', 'grad_norm': '46.25', 'counters/examples': 183648, 'counters/updates': 5739}
train stats after 183680 examples: {'rewards_train/chosen': '0.15066', 'rewards_train/rejected': '-0.030521', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18118', 'logps_train/rejected': '-126', 'logps_train/chosen': '-142.28', 'loss/train': '0.61961', 'examples_per_second': '31.414', 'grad_norm': '43', 'counters/examples': 183680, 'counters/updates': 5740}
train stats after 183712 examples: {'rewards_train/chosen': '0.10274', 'rewards_train/rejected': '0.081897', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.020846', 'logps_train/rejected': '-135.45', 'logps_train/chosen': '-120.56', 'loss/train': '0.69841', 'examples_per_second': '29.923', 'grad_norm': '30.25', 'counters/examples': 183712, 'counters/updates': 5741}
train stats after 183744 examples: {'rewards_train/chosen': '0.18402', 'rewards_train/rejected': '0.10677', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.077251', 'logps_train/rejected': '-146.24', 'logps_train/chosen': '-122.04', 'loss/train': '0.67582', 'examples_per_second': '31.524', 'grad_norm': '36', 'counters/examples': 183744, 'counters/updates': 5742}
skipping logging after 183776 examples to avoid logging too frequently
train stats after 183808 examples: {'rewards_train/chosen': '0.11215', 'rewards_train/rejected': '0.020479', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.091671', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-114.96', 'loss/train': '0.6573', 'examples_per_second': '32.492', 'grad_norm': '28.75', 'counters/examples': 183808, 'counters/updates': 5744}
train stats after 183840 examples: {'rewards_train/chosen': '0.18701', 'rewards_train/rejected': '0.021979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16503', 'logps_train/rejected': '-158.74', 'logps_train/chosen': '-168.28', 'loss/train': '0.63002', 'examples_per_second': '31.109', 'grad_norm': '31.75', 'counters/examples': 183840, 'counters/updates': 5745}
train stats after 183872 examples: {'rewards_train/chosen': '0.14403', 'rewards_train/rejected': '-0.0037475', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14778', 'logps_train/rejected': '-124.59', 'logps_train/chosen': '-118.53', 'loss/train': '0.63841', 'examples_per_second': '31.805', 'grad_norm': '27.5', 'counters/examples': 183872, 'counters/updates': 5746}
train stats after 183904 examples: {'rewards_train/chosen': '0.16179', 'rewards_train/rejected': '0.025714', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13608', 'logps_train/rejected': '-117.79', 'logps_train/chosen': '-165.15', 'loss/train': '0.64227', 'examples_per_second': '30.507', 'grad_norm': '33', 'counters/examples': 183904, 'counters/updates': 5747}
train stats after 183936 examples: {'rewards_train/chosen': '0.15038', 'rewards_train/rejected': '0.13087', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019514', 'logps_train/rejected': '-140.94', 'logps_train/chosen': '-162.95', 'loss/train': '0.6926', 'examples_per_second': '31.851', 'grad_norm': '42.75', 'counters/examples': 183936, 'counters/updates': 5748}
skipping logging after 183968 examples to avoid logging too frequently
train stats after 184000 examples: {'rewards_train/chosen': '0.19552', 'rewards_train/rejected': '0.048518', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.147', 'logps_train/rejected': '-136.58', 'logps_train/chosen': '-127.99', 'loss/train': '0.64066', 'examples_per_second': '30.122', 'grad_norm': '38.25', 'counters/examples': 184000, 'counters/updates': 5750}
train stats after 184032 examples: {'rewards_train/chosen': '0.28505', 'rewards_train/rejected': '0.066689', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21837', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-180.38', 'loss/train': '0.6332', 'examples_per_second': '30.445', 'grad_norm': '27.875', 'counters/examples': 184032, 'counters/updates': 5751}
train stats after 184064 examples: {'rewards_train/chosen': '0.09373', 'rewards_train/rejected': '0.027393', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066337', 'logps_train/rejected': '-147.35', 'logps_train/chosen': '-148.22', 'loss/train': '0.6745', 'examples_per_second': '29.974', 'grad_norm': '32.5', 'counters/examples': 184064, 'counters/updates': 5752}
train stats after 184096 examples: {'rewards_train/chosen': '0.25807', 'rewards_train/rejected': '-0.032388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.29045', 'logps_train/rejected': '-154.54', 'logps_train/chosen': '-196.14', 'loss/train': '0.59', 'examples_per_second': '31.522', 'grad_norm': '31.625', 'counters/examples': 184096, 'counters/updates': 5753}
train stats after 184128 examples: {'rewards_train/chosen': '0.21463', 'rewards_train/rejected': '0.062306', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15232', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-132.54', 'loss/train': '0.63167', 'examples_per_second': '30.865', 'grad_norm': '29.75', 'counters/examples': 184128, 'counters/updates': 5754}
train stats after 184160 examples: {'rewards_train/chosen': '0.17984', 'rewards_train/rejected': '0.10025', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.079583', 'logps_train/rejected': '-143.26', 'logps_train/chosen': '-123.65', 'loss/train': '0.66639', 'examples_per_second': '30.554', 'grad_norm': '35', 'counters/examples': 184160, 'counters/updates': 5755}
train stats after 184192 examples: {'rewards_train/chosen': '0.17809', 'rewards_train/rejected': '-0.009025', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18711', 'logps_train/rejected': '-143.3', 'logps_train/chosen': '-165.09', 'loss/train': '0.61466', 'examples_per_second': '31.593', 'grad_norm': '30.875', 'counters/examples': 184192, 'counters/updates': 5756}
skipping logging after 184224 examples to avoid logging too frequently
train stats after 184256 examples: {'rewards_train/chosen': '0.1849', 'rewards_train/rejected': '0.18325', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.0016564', 'logps_train/rejected': '-170.72', 'logps_train/chosen': '-144.71', 'loss/train': '0.71023', 'examples_per_second': '30.765', 'grad_norm': '43', 'counters/examples': 184256, 'counters/updates': 5758}
train stats after 184288 examples: {'rewards_train/chosen': '0.19352', 'rewards_train/rejected': '0.16366', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.029859', 'logps_train/rejected': '-174.75', 'logps_train/chosen': '-135', 'loss/train': '0.69542', 'examples_per_second': '31.965', 'grad_norm': '34.25', 'counters/examples': 184288, 'counters/updates': 5759}
skipping logging after 184320 examples to avoid logging too frequently
train stats after 184352 examples: {'rewards_train/chosen': '0.1616', 'rewards_train/rejected': '0.04057', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12103', 'logps_train/rejected': '-118', 'logps_train/chosen': '-125.1', 'loss/train': '0.64189', 'examples_per_second': '29.671', 'grad_norm': '30.375', 'counters/examples': 184352, 'counters/updates': 5761}
skipping logging after 184384 examples to avoid logging too frequently
train stats after 184416 examples: {'rewards_train/chosen': '0.090418', 'rewards_train/rejected': '-0.035322', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12574', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-124.63', 'loss/train': '0.63816', 'examples_per_second': '35.962', 'grad_norm': '24.625', 'counters/examples': 184416, 'counters/updates': 5763}
train stats after 184448 examples: {'rewards_train/chosen': '0.083874', 'rewards_train/rejected': '0.013929', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069945', 'logps_train/rejected': '-117.69', 'logps_train/chosen': '-134.13', 'loss/train': '0.66569', 'examples_per_second': '31.738', 'grad_norm': '27.625', 'counters/examples': 184448, 'counters/updates': 5764}
skipping logging after 184480 examples to avoid logging too frequently
train stats after 184512 examples: {'rewards_train/chosen': '0.19862', 'rewards_train/rejected': '0.078228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12039', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-147.86', 'loss/train': '0.6457', 'examples_per_second': '30.531', 'grad_norm': '29.375', 'counters/examples': 184512, 'counters/updates': 5766}
train stats after 184544 examples: {'rewards_train/chosen': '0.14974', 'rewards_train/rejected': '0.03767', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11207', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-126.18', 'loss/train': '0.6505', 'examples_per_second': '31.499', 'grad_norm': '30.75', 'counters/examples': 184544, 'counters/updates': 5767}
train stats after 184576 examples: {'rewards_train/chosen': '0.1304', 'rewards_train/rejected': '-0.022818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15322', 'logps_train/rejected': '-109.43', 'logps_train/chosen': '-151.83', 'loss/train': '0.63624', 'examples_per_second': '30.375', 'grad_norm': '31.5', 'counters/examples': 184576, 'counters/updates': 5768}
train stats after 184608 examples: {'rewards_train/chosen': '0.26141', 'rewards_train/rejected': '0.17701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084401', 'logps_train/rejected': '-145.77', 'logps_train/chosen': '-162.26', 'loss/train': '0.68006', 'examples_per_second': '31.491', 'grad_norm': '56.5', 'counters/examples': 184608, 'counters/updates': 5769}
skipping logging after 184640 examples to avoid logging too frequently
train stats after 184672 examples: {'rewards_train/chosen': '0.14704', 'rewards_train/rejected': '0.048521', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098515', 'logps_train/rejected': '-161.44', 'logps_train/chosen': '-150.39', 'loss/train': '0.65108', 'examples_per_second': '30.048', 'grad_norm': '31.25', 'counters/examples': 184672, 'counters/updates': 5771}
train stats after 184704 examples: {'rewards_train/chosen': '0.13429', 'rewards_train/rejected': '0.0012026', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13309', 'logps_train/rejected': '-121.2', 'logps_train/chosen': '-113.71', 'loss/train': '0.63463', 'examples_per_second': '30.198', 'grad_norm': '25.25', 'counters/examples': 184704, 'counters/updates': 5772}
skipping logging after 184736 examples to avoid logging too frequently
train stats after 184768 examples: {'rewards_train/chosen': '0.22391', 'rewards_train/rejected': '0.011909', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21201', 'logps_train/rejected': '-119.96', 'logps_train/chosen': '-138.33', 'loss/train': '0.60615', 'examples_per_second': '25.181', 'grad_norm': '27.25', 'counters/examples': 184768, 'counters/updates': 5774}
train stats after 184800 examples: {'rewards_train/chosen': '0.091666', 'rewards_train/rejected': '0.083703', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.007963', 'logps_train/rejected': '-132.09', 'logps_train/chosen': '-130.31', 'loss/train': '0.70981', 'examples_per_second': '32.356', 'grad_norm': '56', 'counters/examples': 184800, 'counters/updates': 5775}
train stats after 184832 examples: {'rewards_train/chosen': '0.11509', 'rewards_train/rejected': '0.10243', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.012657', 'logps_train/rejected': '-148.06', 'logps_train/chosen': '-145.64', 'loss/train': '0.69625', 'examples_per_second': '29.943', 'grad_norm': '34.25', 'counters/examples': 184832, 'counters/updates': 5776}
train stats after 184864 examples: {'rewards_train/chosen': '0.16517', 'rewards_train/rejected': '-0.019393', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18456', 'logps_train/rejected': '-151.67', 'logps_train/chosen': '-145.25', 'loss/train': '0.62415', 'examples_per_second': '26.141', 'grad_norm': '27.625', 'counters/examples': 184864, 'counters/updates': 5777}
train stats after 184896 examples: {'rewards_train/chosen': '0.1592', 'rewards_train/rejected': '0.074329', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.084869', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-167.99', 'loss/train': '0.66564', 'examples_per_second': '30.926', 'grad_norm': '31.875', 'counters/examples': 184896, 'counters/updates': 5778}
train stats after 184928 examples: {'rewards_train/chosen': '0.13881', 'rewards_train/rejected': '0.036558', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10225', 'logps_train/rejected': '-112', 'logps_train/chosen': '-125.55', 'loss/train': '0.65516', 'examples_per_second': '31.097', 'grad_norm': '29.625', 'counters/examples': 184928, 'counters/updates': 5779}
skipping logging after 184960 examples to avoid logging too frequently
train stats after 184992 examples: {'rewards_train/chosen': '0.20646', 'rewards_train/rejected': '0.070844', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13562', 'logps_train/rejected': '-138.55', 'logps_train/chosen': '-167.89', 'loss/train': '0.64452', 'examples_per_second': '31.86', 'grad_norm': '29', 'counters/examples': 184992, 'counters/updates': 5781}
skipping logging after 185024 examples to avoid logging too frequently
train stats after 185056 examples: {'rewards_train/chosen': '0.20776', 'rewards_train/rejected': '0.06263', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14513', 'logps_train/rejected': '-144.05', 'logps_train/chosen': '-133.81', 'loss/train': '0.63691', 'examples_per_second': '31.423', 'grad_norm': '29.125', 'counters/examples': 185056, 'counters/updates': 5783}
skipping logging after 185088 examples to avoid logging too frequently
train stats after 185120 examples: {'rewards_train/chosen': '0.14207', 'rewards_train/rejected': '-0.019752', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16183', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-158.98', 'loss/train': '0.62471', 'examples_per_second': '30.309', 'grad_norm': '26.125', 'counters/examples': 185120, 'counters/updates': 5785}
skipping logging after 185152 examples to avoid logging too frequently
train stats after 185184 examples: {'rewards_train/chosen': '0.26748', 'rewards_train/rejected': '0.058744', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20873', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-127.18', 'loss/train': '0.63312', 'examples_per_second': '30.574', 'grad_norm': '30.625', 'counters/examples': 185184, 'counters/updates': 5787}
train stats after 185216 examples: {'rewards_train/chosen': '0.074839', 'rewards_train/rejected': '0.056316', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018523', 'logps_train/rejected': '-129.48', 'logps_train/chosen': '-145.27', 'loss/train': '0.69863', 'examples_per_second': '31.073', 'grad_norm': '33.25', 'counters/examples': 185216, 'counters/updates': 5788}
train stats after 185248 examples: {'rewards_train/chosen': '0.17409', 'rewards_train/rejected': '0.055568', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11852', 'logps_train/rejected': '-120.7', 'logps_train/chosen': '-138.14', 'loss/train': '0.64808', 'examples_per_second': '30.83', 'grad_norm': '29.625', 'counters/examples': 185248, 'counters/updates': 5789}
skipping logging after 185280 examples to avoid logging too frequently
train stats after 185312 examples: {'rewards_train/chosen': '0.24219', 'rewards_train/rejected': '0.070642', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17154', 'logps_train/rejected': '-143.87', 'logps_train/chosen': '-149.92', 'loss/train': '0.62141', 'examples_per_second': '32.065', 'grad_norm': '33.25', 'counters/examples': 185312, 'counters/updates': 5791}
train stats after 185344 examples: {'rewards_train/chosen': '0.15244', 'rewards_train/rejected': '0.083873', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.06857', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-131.93', 'loss/train': '0.6666', 'examples_per_second': '30.179', 'grad_norm': '27.875', 'counters/examples': 185344, 'counters/updates': 5792}
skipping logging after 185376 examples to avoid logging too frequently
train stats after 185408 examples: {'rewards_train/chosen': '0.17345', 'rewards_train/rejected': '0.093238', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080216', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-132.78', 'loss/train': '0.66366', 'examples_per_second': '31.101', 'grad_norm': '27.5', 'counters/examples': 185408, 'counters/updates': 5794}
train stats after 185440 examples: {'rewards_train/chosen': '0.32029', 'rewards_train/rejected': '0.091954', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22833', 'logps_train/rejected': '-119.45', 'logps_train/chosen': '-170.35', 'loss/train': '0.63862', 'examples_per_second': '30.029', 'grad_norm': '34', 'counters/examples': 185440, 'counters/updates': 5795}
skipping logging after 185472 examples to avoid logging too frequently
train stats after 185504 examples: {'rewards_train/chosen': '0.070559', 'rewards_train/rejected': '0.068573', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.001986', 'logps_train/rejected': '-128.99', 'logps_train/chosen': '-167.5', 'loss/train': '0.70921', 'examples_per_second': '31.638', 'grad_norm': '30.25', 'counters/examples': 185504, 'counters/updates': 5797}
train stats after 185536 examples: {'rewards_train/chosen': '0.14035', 'rewards_train/rejected': '0.084297', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05605', 'logps_train/rejected': '-152.57', 'logps_train/chosen': '-153.25', 'loss/train': '0.67434', 'examples_per_second': '31.967', 'grad_norm': '36.5', 'counters/examples': 185536, 'counters/updates': 5798}
train stats after 185568 examples: {'rewards_train/chosen': '0.12582', 'rewards_train/rejected': '0.049234', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076587', 'logps_train/rejected': '-120.34', 'logps_train/chosen': '-143.41', 'loss/train': '0.66975', 'examples_per_second': '30.774', 'grad_norm': '28', 'counters/examples': 185568, 'counters/updates': 5799}
skipping logging after 185600 examples to avoid logging too frequently
train stats after 185632 examples: {'rewards_train/chosen': '0.11083', 'rewards_train/rejected': '0.059273', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051553', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-144.93', 'loss/train': '0.68112', 'examples_per_second': '33.319', 'grad_norm': '36', 'counters/examples': 185632, 'counters/updates': 5801}
skipping logging after 185664 examples to avoid logging too frequently
train stats after 185696 examples: {'rewards_train/chosen': '0.12597', 'rewards_train/rejected': '0.010939', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11503', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-166.6', 'loss/train': '0.64963', 'examples_per_second': '34.526', 'grad_norm': '28.875', 'counters/examples': 185696, 'counters/updates': 5803}
train stats after 185728 examples: {'rewards_train/chosen': '0.1948', 'rewards_train/rejected': '0.054607', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14019', 'logps_train/rejected': '-111.22', 'logps_train/chosen': '-142.78', 'loss/train': '0.63347', 'examples_per_second': '31.456', 'grad_norm': '28.5', 'counters/examples': 185728, 'counters/updates': 5804}
train stats after 185760 examples: {'rewards_train/chosen': '0.10079', 'rewards_train/rejected': '0.056999', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043793', 'logps_train/rejected': '-141.64', 'logps_train/chosen': '-168.59', 'loss/train': '0.68507', 'examples_per_second': '31.399', 'grad_norm': '39', 'counters/examples': 185760, 'counters/updates': 5805}
train stats after 185792 examples: {'rewards_train/chosen': '0.14107', 'rewards_train/rejected': '0.06176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079309', 'logps_train/rejected': '-134.59', 'logps_train/chosen': '-140.72', 'loss/train': '0.66103', 'examples_per_second': '31.376', 'grad_norm': '36.5', 'counters/examples': 185792, 'counters/updates': 5806}
train stats after 185824 examples: {'rewards_train/chosen': '0.090427', 'rewards_train/rejected': '0.1087', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018271', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-114.01', 'loss/train': '0.70823', 'examples_per_second': '30.462', 'grad_norm': '34', 'counters/examples': 185824, 'counters/updates': 5807}
train stats after 185856 examples: {'rewards_train/chosen': '0.10738', 'rewards_train/rejected': '0.025672', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081709', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-175.15', 'loss/train': '0.6647', 'examples_per_second': '29.881', 'grad_norm': '35.75', 'counters/examples': 185856, 'counters/updates': 5808}
train stats after 185888 examples: {'rewards_train/chosen': '0.25359', 'rewards_train/rejected': '0.020803', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23279', 'logps_train/rejected': '-105.43', 'logps_train/chosen': '-154.7', 'loss/train': '0.61165', 'examples_per_second': '31.188', 'grad_norm': '24.625', 'counters/examples': 185888, 'counters/updates': 5809}
train stats after 185920 examples: {'rewards_train/chosen': '0.16494', 'rewards_train/rejected': '0.16489', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '5.4835e-05', 'logps_train/rejected': '-146.01', 'logps_train/chosen': '-114.56', 'loss/train': '0.70979', 'examples_per_second': '31.433', 'grad_norm': '33', 'counters/examples': 185920, 'counters/updates': 5810}
train stats after 185952 examples: {'rewards_train/chosen': '0.24755', 'rewards_train/rejected': '-0.045632', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.29318', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-161.83', 'loss/train': '0.57246', 'examples_per_second': '30.187', 'grad_norm': '24', 'counters/examples': 185952, 'counters/updates': 5811}
train stats after 185984 examples: {'rewards_train/chosen': '0.15624', 'rewards_train/rejected': '0.058593', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097644', 'logps_train/rejected': '-122.03', 'logps_train/chosen': '-157.69', 'loss/train': '0.65035', 'examples_per_second': '32.884', 'grad_norm': '31', 'counters/examples': 185984, 'counters/updates': 5812}
train stats after 186016 examples: {'rewards_train/chosen': '0.17714', 'rewards_train/rejected': '0.023339', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1538', 'logps_train/rejected': '-112.96', 'logps_train/chosen': '-204.07', 'loss/train': '0.63496', 'examples_per_second': '31.478', 'grad_norm': '46', 'counters/examples': 186016, 'counters/updates': 5813}
train stats after 186048 examples: {'rewards_train/chosen': '0.15869', 'rewards_train/rejected': '0.12473', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033961', 'logps_train/rejected': '-144.28', 'logps_train/chosen': '-160.29', 'loss/train': '0.68648', 'examples_per_second': '31.199', 'grad_norm': '39', 'counters/examples': 186048, 'counters/updates': 5814}
train stats after 186080 examples: {'rewards_train/chosen': '0.14511', 'rewards_train/rejected': '-0.00027137', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14538', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-169.93', 'loss/train': '0.62958', 'examples_per_second': '32.213', 'grad_norm': '42.5', 'counters/examples': 186080, 'counters/updates': 5815}
skipping logging after 186112 examples to avoid logging too frequently
train stats after 186144 examples: {'rewards_train/chosen': '0.14215', 'rewards_train/rejected': '0.026872', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11527', 'logps_train/rejected': '-125.92', 'logps_train/chosen': '-102.19', 'loss/train': '0.6459', 'examples_per_second': '37.905', 'grad_norm': '25.25', 'counters/examples': 186144, 'counters/updates': 5817}
train stats after 186176 examples: {'rewards_train/chosen': '0.11756', 'rewards_train/rejected': '0.077545', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04001', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-122.6', 'loss/train': '0.68714', 'examples_per_second': '31.375', 'grad_norm': '34.75', 'counters/examples': 186176, 'counters/updates': 5818}
train stats after 186208 examples: {'rewards_train/chosen': '0.14081', 'rewards_train/rejected': '0.062521', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.078288', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-138.42', 'loss/train': '0.66869', 'examples_per_second': '31.928', 'grad_norm': '30.25', 'counters/examples': 186208, 'counters/updates': 5819}
train stats after 186240 examples: {'rewards_train/chosen': '0.15708', 'rewards_train/rejected': '0.038553', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11853', 'logps_train/rejected': '-122', 'logps_train/chosen': '-122.85', 'loss/train': '0.64341', 'examples_per_second': '31.495', 'grad_norm': '31.875', 'counters/examples': 186240, 'counters/updates': 5820}
train stats after 186272 examples: {'rewards_train/chosen': '0.19317', 'rewards_train/rejected': '0.085851', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10731', 'logps_train/rejected': '-143.13', 'logps_train/chosen': '-157.38', 'loss/train': '0.65649', 'examples_per_second': '31.866', 'grad_norm': '32.75', 'counters/examples': 186272, 'counters/updates': 5821}
skipping logging after 186304 examples to avoid logging too frequently
train stats after 186336 examples: {'rewards_train/chosen': '0.12468', 'rewards_train/rejected': '0.097961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026723', 'logps_train/rejected': '-108.15', 'logps_train/chosen': '-164.33', 'loss/train': '0.6878', 'examples_per_second': '32.847', 'grad_norm': '27.25', 'counters/examples': 186336, 'counters/updates': 5823}
train stats after 186368 examples: {'rewards_train/chosen': '0.21442', 'rewards_train/rejected': '0.064111', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15031', 'logps_train/rejected': '-102.5', 'logps_train/chosen': '-168.74', 'loss/train': '0.64627', 'examples_per_second': '31.243', 'grad_norm': '65', 'counters/examples': 186368, 'counters/updates': 5824}
train stats after 186400 examples: {'rewards_train/chosen': '0.12762', 'rewards_train/rejected': '-0.015745', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14337', 'logps_train/rejected': '-86.809', 'logps_train/chosen': '-98.796', 'loss/train': '0.629', 'examples_per_second': '31.924', 'grad_norm': '27.875', 'counters/examples': 186400, 'counters/updates': 5825}
train stats after 186432 examples: {'rewards_train/chosen': '0.26688', 'rewards_train/rejected': '0.10664', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16025', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-166.81', 'loss/train': '0.64926', 'examples_per_second': '31.788', 'grad_norm': '28.875', 'counters/examples': 186432, 'counters/updates': 5826}
train stats after 186464 examples: {'rewards_train/chosen': '0.17899', 'rewards_train/rejected': '0.058013', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12098', 'logps_train/rejected': '-126.59', 'logps_train/chosen': '-141.87', 'loss/train': '0.64639', 'examples_per_second': '31.118', 'grad_norm': '35', 'counters/examples': 186464, 'counters/updates': 5827}
train stats after 186496 examples: {'rewards_train/chosen': '0.14926', 'rewards_train/rejected': '0.074442', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074818', 'logps_train/rejected': '-108.38', 'logps_train/chosen': '-202.25', 'loss/train': '0.6747', 'examples_per_second': '30.594', 'grad_norm': '39', 'counters/examples': 186496, 'counters/updates': 5828}
train stats after 186528 examples: {'rewards_train/chosen': '0.26043', 'rewards_train/rejected': '0.036194', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22424', 'logps_train/rejected': '-149.26', 'logps_train/chosen': '-154.8', 'loss/train': '0.61992', 'examples_per_second': '31.413', 'grad_norm': '33', 'counters/examples': 186528, 'counters/updates': 5829}
train stats after 186560 examples: {'rewards_train/chosen': '0.10101', 'rewards_train/rejected': '0.054998', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046008', 'logps_train/rejected': '-103.04', 'logps_train/chosen': '-127.67', 'loss/train': '0.68024', 'examples_per_second': '32.453', 'grad_norm': '27.375', 'counters/examples': 186560, 'counters/updates': 5830}
train stats after 186592 examples: {'rewards_train/chosen': '0.1697', 'rewards_train/rejected': '0.068356', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10135', 'logps_train/rejected': '-121.66', 'logps_train/chosen': '-155.01', 'loss/train': '0.65692', 'examples_per_second': '30.005', 'grad_norm': '28.875', 'counters/examples': 186592, 'counters/updates': 5831}
train stats after 186624 examples: {'rewards_train/chosen': '0.21652', 'rewards_train/rejected': '0.067718', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1488', 'logps_train/rejected': '-116.48', 'logps_train/chosen': '-158.17', 'loss/train': '0.64606', 'examples_per_second': '31.435', 'grad_norm': '33.5', 'counters/examples': 186624, 'counters/updates': 5832}
train stats after 186656 examples: {'rewards_train/chosen': '0.24071', 'rewards_train/rejected': '0.13537', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10533', 'logps_train/rejected': '-129.53', 'logps_train/chosen': '-143.76', 'loss/train': '0.67283', 'examples_per_second': '31.427', 'grad_norm': '30.25', 'counters/examples': 186656, 'counters/updates': 5833}
train stats after 186688 examples: {'rewards_train/chosen': '0.12542', 'rewards_train/rejected': '0.066569', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058851', 'logps_train/rejected': '-133.47', 'logps_train/chosen': '-149.46', 'loss/train': '0.67112', 'examples_per_second': '30.069', 'grad_norm': '39', 'counters/examples': 186688, 'counters/updates': 5834}
train stats after 186720 examples: {'rewards_train/chosen': '0.14982', 'rewards_train/rejected': '0.042335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10748', 'logps_train/rejected': '-112.1', 'logps_train/chosen': '-116.35', 'loss/train': '0.65357', 'examples_per_second': '31.478', 'grad_norm': '25', 'counters/examples': 186720, 'counters/updates': 5835}
train stats after 186752 examples: {'rewards_train/chosen': '0.21178', 'rewards_train/rejected': '0.058864', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15292', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-164.2', 'loss/train': '0.64474', 'examples_per_second': '32.015', 'grad_norm': '30.125', 'counters/examples': 186752, 'counters/updates': 5836}
train stats after 186784 examples: {'rewards_train/chosen': '0.19607', 'rewards_train/rejected': '0.04959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14648', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-186.68', 'loss/train': '0.64447', 'examples_per_second': '31.453', 'grad_norm': '33', 'counters/examples': 186784, 'counters/updates': 5837}
train stats after 186816 examples: {'rewards_train/chosen': '0.26601', 'rewards_train/rejected': '0.1208', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14521', 'logps_train/rejected': '-138.13', 'logps_train/chosen': '-156.43', 'loss/train': '0.63659', 'examples_per_second': '31.985', 'grad_norm': '42.5', 'counters/examples': 186816, 'counters/updates': 5838}
train stats after 186848 examples: {'rewards_train/chosen': '0.26937', 'rewards_train/rejected': '0.10304', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16633', 'logps_train/rejected': '-109.44', 'logps_train/chosen': '-121.95', 'loss/train': '0.6345', 'examples_per_second': '30.489', 'grad_norm': '25.125', 'counters/examples': 186848, 'counters/updates': 5839}
train stats after 186880 examples: {'rewards_train/chosen': '0.12901', 'rewards_train/rejected': '0.12719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0018196', 'logps_train/rejected': '-138.3', 'logps_train/chosen': '-121.15', 'loss/train': '0.71349', 'examples_per_second': '32.921', 'grad_norm': '41.75', 'counters/examples': 186880, 'counters/updates': 5840}
train stats after 186912 examples: {'rewards_train/chosen': '0.19543', 'rewards_train/rejected': '0.15586', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.03957', 'logps_train/rejected': '-136.32', 'logps_train/chosen': '-168.24', 'loss/train': '0.69073', 'examples_per_second': '30.687', 'grad_norm': '39.25', 'counters/examples': 186912, 'counters/updates': 5841}
skipping logging after 186944 examples to avoid logging too frequently
train stats after 186976 examples: {'rewards_train/chosen': '0.2351', 'rewards_train/rejected': '0.095205', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13989', 'logps_train/rejected': '-110.59', 'logps_train/chosen': '-129.24', 'loss/train': '0.65542', 'examples_per_second': '34.137', 'grad_norm': '28', 'counters/examples': 186976, 'counters/updates': 5843}
train stats after 187008 examples: {'rewards_train/chosen': '0.13168', 'rewards_train/rejected': '0.10567', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026012', 'logps_train/rejected': '-132.35', 'logps_train/chosen': '-126.72', 'loss/train': '0.69509', 'examples_per_second': '30.373', 'grad_norm': '37', 'counters/examples': 187008, 'counters/updates': 5844}
train stats after 187040 examples: {'rewards_train/chosen': '0.16355', 'rewards_train/rejected': '0.068767', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094784', 'logps_train/rejected': '-112.75', 'logps_train/chosen': '-143.62', 'loss/train': '0.66287', 'examples_per_second': '31.51', 'grad_norm': '26.875', 'counters/examples': 187040, 'counters/updates': 5845}
train stats after 187072 examples: {'rewards_train/chosen': '0.17734', 'rewards_train/rejected': '0.070879', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10646', 'logps_train/rejected': '-87.787', 'logps_train/chosen': '-126.59', 'loss/train': '0.65465', 'examples_per_second': '32.446', 'grad_norm': '23.875', 'counters/examples': 187072, 'counters/updates': 5846}
train stats after 187104 examples: {'rewards_train/chosen': '0.13032', 'rewards_train/rejected': '0.059851', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070471', 'logps_train/rejected': '-95.562', 'logps_train/chosen': '-141.91', 'loss/train': '0.66872', 'examples_per_second': '29.899', 'grad_norm': '24', 'counters/examples': 187104, 'counters/updates': 5847}
train stats after 187136 examples: {'rewards_train/chosen': '0.1826', 'rewards_train/rejected': '0.086369', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096233', 'logps_train/rejected': '-127.74', 'logps_train/chosen': '-139.72', 'loss/train': '0.6665', 'examples_per_second': '30.55', 'grad_norm': '26.125', 'counters/examples': 187136, 'counters/updates': 5848}
train stats after 187168 examples: {'rewards_train/chosen': '0.18846', 'rewards_train/rejected': '-0.011384', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19984', 'logps_train/rejected': '-122.56', 'logps_train/chosen': '-132.12', 'loss/train': '0.61063', 'examples_per_second': '31.475', 'grad_norm': '30.125', 'counters/examples': 187168, 'counters/updates': 5849}
train stats after 187200 examples: {'rewards_train/chosen': '0.10049', 'rewards_train/rejected': '0.022664', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077829', 'logps_train/rejected': '-126.01', 'logps_train/chosen': '-146.08', 'loss/train': '0.66105', 'examples_per_second': '31.878', 'grad_norm': '26.125', 'counters/examples': 187200, 'counters/updates': 5850}
train stats after 187232 examples: {'rewards_train/chosen': '0.16242', 'rewards_train/rejected': '0.047019', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1154', 'logps_train/rejected': '-102.38', 'logps_train/chosen': '-129.01', 'loss/train': '0.65418', 'examples_per_second': '31.547', 'grad_norm': '26.625', 'counters/examples': 187232, 'counters/updates': 5851}
skipping logging after 187264 examples to avoid logging too frequently
train stats after 187296 examples: {'rewards_train/chosen': '0.23401', 'rewards_train/rejected': '0.085895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14811', 'logps_train/rejected': '-109.14', 'logps_train/chosen': '-147.16', 'loss/train': '0.64512', 'examples_per_second': '35.437', 'grad_norm': '29.25', 'counters/examples': 187296, 'counters/updates': 5853}
train stats after 187328 examples: {'rewards_train/chosen': '0.19567', 'rewards_train/rejected': '0.060234', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13543', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-139.23', 'loss/train': '0.63836', 'examples_per_second': '31.348', 'grad_norm': '26.5', 'counters/examples': 187328, 'counters/updates': 5854}
train stats after 187360 examples: {'rewards_train/chosen': '0.0012105', 'rewards_train/rejected': '-0.0079257', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0091362', 'logps_train/rejected': '-113.33', 'logps_train/chosen': '-145.94', 'loss/train': '0.71459', 'examples_per_second': '31.4', 'grad_norm': '70', 'counters/examples': 187360, 'counters/updates': 5855}
skipping logging after 187392 examples to avoid logging too frequently
train stats after 187424 examples: {'rewards_train/chosen': '0.20604', 'rewards_train/rejected': '0.057236', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1488', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-153.99', 'loss/train': '0.63134', 'examples_per_second': '32.215', 'grad_norm': '31', 'counters/examples': 187424, 'counters/updates': 5857}
train stats after 187456 examples: {'rewards_train/chosen': '0.18096', 'rewards_train/rejected': '0.05694', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12402', 'logps_train/rejected': '-106.49', 'logps_train/chosen': '-140.62', 'loss/train': '0.64736', 'examples_per_second': '30.011', 'grad_norm': '31.5', 'counters/examples': 187456, 'counters/updates': 5858}
train stats after 187488 examples: {'rewards_train/chosen': '0.14011', 'rewards_train/rejected': '0.099377', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040735', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-136.51', 'loss/train': '0.68322', 'examples_per_second': '30.372', 'grad_norm': '30.25', 'counters/examples': 187488, 'counters/updates': 5859}
train stats after 187520 examples: {'rewards_train/chosen': '0.18678', 'rewards_train/rejected': '0.23117', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.044388', 'logps_train/rejected': '-203.14', 'logps_train/chosen': '-167.53', 'loss/train': '0.75561', 'examples_per_second': '31.299', 'grad_norm': '55.25', 'counters/examples': 187520, 'counters/updates': 5860}
train stats after 187552 examples: {'rewards_train/chosen': '0.3464', 'rewards_train/rejected': '0.13917', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20723', 'logps_train/rejected': '-120.62', 'logps_train/chosen': '-145.72', 'loss/train': '0.62878', 'examples_per_second': '30.036', 'grad_norm': '33', 'counters/examples': 187552, 'counters/updates': 5861}
train stats after 187584 examples: {'rewards_train/chosen': '0.16571', 'rewards_train/rejected': '0.11101', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054701', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-128.22', 'loss/train': '0.6819', 'examples_per_second': '32.237', 'grad_norm': '29.125', 'counters/examples': 187584, 'counters/updates': 5862}
train stats after 187616 examples: {'rewards_train/chosen': '0.076682', 'rewards_train/rejected': '0.086009', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0093271', 'logps_train/rejected': '-155.32', 'logps_train/chosen': '-140.69', 'loss/train': '0.70981', 'examples_per_second': '31.359', 'grad_norm': '38.5', 'counters/examples': 187616, 'counters/updates': 5863}
skipping logging after 187648 examples to avoid logging too frequently
train stats after 187680 examples: {'rewards_train/chosen': '0.12911', 'rewards_train/rejected': '-0.011658', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14077', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-154.99', 'loss/train': '0.63393', 'examples_per_second': '32.545', 'grad_norm': '26.625', 'counters/examples': 187680, 'counters/updates': 5865}
train stats after 187712 examples: {'rewards_train/chosen': '0.15377', 'rewards_train/rejected': '0.25255', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.098781', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-128.18', 'loss/train': '0.7914', 'examples_per_second': '31.493', 'grad_norm': '125.5', 'counters/examples': 187712, 'counters/updates': 5866}
train stats after 187744 examples: {'rewards_train/chosen': '0.13764', 'rewards_train/rejected': '0.089024', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048617', 'logps_train/rejected': '-122.45', 'logps_train/chosen': '-140.41', 'loss/train': '0.683', 'examples_per_second': '30.454', 'grad_norm': '37', 'counters/examples': 187744, 'counters/updates': 5867}
train stats after 187776 examples: {'rewards_train/chosen': '0.15892', 'rewards_train/rejected': '0.1043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054618', 'logps_train/rejected': '-140.4', 'logps_train/chosen': '-140.9', 'loss/train': '0.68031', 'examples_per_second': '30.387', 'grad_norm': '39', 'counters/examples': 187776, 'counters/updates': 5868}
train stats after 187808 examples: {'rewards_train/chosen': '0.1518', 'rewards_train/rejected': '0.049044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10276', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-134.36', 'loss/train': '0.65212', 'examples_per_second': '31.487', 'grad_norm': '26.375', 'counters/examples': 187808, 'counters/updates': 5869}
train stats after 187840 examples: {'rewards_train/chosen': '0.14527', 'rewards_train/rejected': '0.065549', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07972', 'logps_train/rejected': '-102', 'logps_train/chosen': '-134.51', 'loss/train': '0.66022', 'examples_per_second': '29.839', 'grad_norm': '27', 'counters/examples': 187840, 'counters/updates': 5870}
train stats after 187872 examples: {'rewards_train/chosen': '0.15468', 'rewards_train/rejected': '0.098339', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05634', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-176.07', 'loss/train': '0.67654', 'examples_per_second': '31.428', 'grad_norm': '30.75', 'counters/examples': 187872, 'counters/updates': 5871}
train stats after 187904 examples: {'rewards_train/chosen': '0.11248', 'rewards_train/rejected': '0.082628', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029856', 'logps_train/rejected': '-154.07', 'logps_train/chosen': '-147.54', 'loss/train': '0.70012', 'examples_per_second': '31.024', 'grad_norm': '44.5', 'counters/examples': 187904, 'counters/updates': 5872}
train stats after 187936 examples: {'rewards_train/chosen': '0.12607', 'rewards_train/rejected': '0.024774', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1013', 'logps_train/rejected': '-125.68', 'logps_train/chosen': '-139.68', 'loss/train': '0.65515', 'examples_per_second': '29.929', 'grad_norm': '25', 'counters/examples': 187936, 'counters/updates': 5873}
train stats after 187968 examples: {'rewards_train/chosen': '0.14771', 'rewards_train/rejected': '0.036893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11082', 'logps_train/rejected': '-148.4', 'logps_train/chosen': '-143.27', 'loss/train': '0.65485', 'examples_per_second': '32.546', 'grad_norm': '31', 'counters/examples': 187968, 'counters/updates': 5874}
train stats after 188000 examples: {'rewards_train/chosen': '0.28863', 'rewards_train/rejected': '0.062811', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22582', 'logps_train/rejected': '-122.38', 'logps_train/chosen': '-139.14', 'loss/train': '0.6114', 'examples_per_second': '31.352', 'grad_norm': '32.5', 'counters/examples': 188000, 'counters/updates': 5875}
train stats after 188032 examples: {'rewards_train/chosen': '0.27601', 'rewards_train/rejected': '0.079384', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19663', 'logps_train/rejected': '-159.03', 'logps_train/chosen': '-182.29', 'loss/train': '0.63195', 'examples_per_second': '30.445', 'grad_norm': '34.75', 'counters/examples': 188032, 'counters/updates': 5876}
skipping logging after 188064 examples to avoid logging too frequently
train stats after 188096 examples: {'rewards_train/chosen': '0.12841', 'rewards_train/rejected': '0.073049', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055359', 'logps_train/rejected': '-132.04', 'logps_train/chosen': '-113.28', 'loss/train': '0.67425', 'examples_per_second': '37.846', 'grad_norm': '27.625', 'counters/examples': 188096, 'counters/updates': 5878}
train stats after 188128 examples: {'rewards_train/chosen': '0.13156', 'rewards_train/rejected': '0.08101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050547', 'logps_train/rejected': '-155.12', 'logps_train/chosen': '-176.04', 'loss/train': '0.67341', 'examples_per_second': '31.504', 'grad_norm': '30.125', 'counters/examples': 188128, 'counters/updates': 5879}
train stats after 188160 examples: {'rewards_train/chosen': '0.06739', 'rewards_train/rejected': '-0.017016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084406', 'logps_train/rejected': '-105.1', 'logps_train/chosen': '-135.7', 'loss/train': '0.65797', 'examples_per_second': '30.146', 'grad_norm': '27.5', 'counters/examples': 188160, 'counters/updates': 5880}
train stats after 188192 examples: {'rewards_train/chosen': '0.25263', 'rewards_train/rejected': '0.073763', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17887', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-148.03', 'loss/train': '0.63347', 'examples_per_second': '24.693', 'grad_norm': '27.75', 'counters/examples': 188192, 'counters/updates': 5881}
skipping logging after 188224 examples to avoid logging too frequently
train stats after 188256 examples: {'rewards_train/chosen': '0.21593', 'rewards_train/rejected': '0.072767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14316', 'logps_train/rejected': '-116', 'logps_train/chosen': '-120.29', 'loss/train': '0.63755', 'examples_per_second': '31.216', 'grad_norm': '29.875', 'counters/examples': 188256, 'counters/updates': 5883}
skipping logging after 188288 examples to avoid logging too frequently
train stats after 188320 examples: {'rewards_train/chosen': '0.15329', 'rewards_train/rejected': '0.17435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '-0.021066', 'logps_train/rejected': '-142.21', 'logps_train/chosen': '-155.67', 'loss/train': '0.72609', 'examples_per_second': '31.428', 'grad_norm': '46.75', 'counters/examples': 188320, 'counters/updates': 5885}
skipping logging after 188352 examples to avoid logging too frequently
train stats after 188384 examples: {'rewards_train/chosen': '0.28388', 'rewards_train/rejected': '0.18212', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10176', 'logps_train/rejected': '-134.19', 'logps_train/chosen': '-127.28', 'loss/train': '0.66839', 'examples_per_second': '31.53', 'grad_norm': '26.75', 'counters/examples': 188384, 'counters/updates': 5887}
train stats after 188416 examples: {'rewards_train/chosen': '0.10644', 'rewards_train/rejected': '0.048221', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058216', 'logps_train/rejected': '-119.38', 'logps_train/chosen': '-107.44', 'loss/train': '0.67247', 'examples_per_second': '31.513', 'grad_norm': '26.375', 'counters/examples': 188416, 'counters/updates': 5888}
train stats after 188448 examples: {'rewards_train/chosen': '0.1046', 'rewards_train/rejected': '0.13878', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.034173', 'logps_train/rejected': '-113.28', 'logps_train/chosen': '-104.14', 'loss/train': '0.7243', 'examples_per_second': '31.475', 'grad_norm': '28.625', 'counters/examples': 188448, 'counters/updates': 5889}
train stats after 188480 examples: {'rewards_train/chosen': '0.14616', 'rewards_train/rejected': '0.068352', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077808', 'logps_train/rejected': '-130.96', 'logps_train/chosen': '-151.05', 'loss/train': '0.67046', 'examples_per_second': '30.47', 'grad_norm': '36.5', 'counters/examples': 188480, 'counters/updates': 5890}
train stats after 188512 examples: {'rewards_train/chosen': '0.18191', 'rewards_train/rejected': '0.022457', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15946', 'logps_train/rejected': '-154.53', 'logps_train/chosen': '-171.43', 'loss/train': '0.62929', 'examples_per_second': '31.451', 'grad_norm': '31', 'counters/examples': 188512, 'counters/updates': 5891}
train stats after 188544 examples: {'rewards_train/chosen': '0.11366', 'rewards_train/rejected': '0.086852', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02681', 'logps_train/rejected': '-144.11', 'logps_train/chosen': '-161.24', 'loss/train': '0.70336', 'examples_per_second': '31.511', 'grad_norm': '133', 'counters/examples': 188544, 'counters/updates': 5892}
skipping logging after 188576 examples to avoid logging too frequently
train stats after 188608 examples: {'rewards_train/chosen': '0.18187', 'rewards_train/rejected': '0.15792', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023955', 'logps_train/rejected': '-132.87', 'logps_train/chosen': '-143.84', 'loss/train': '0.69998', 'examples_per_second': '30.973', 'grad_norm': '88', 'counters/examples': 188608, 'counters/updates': 5894}
train stats after 188640 examples: {'rewards_train/chosen': '0.24022', 'rewards_train/rejected': '0.13536', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10485', 'logps_train/rejected': '-120.96', 'logps_train/chosen': '-132.8', 'loss/train': '0.66338', 'examples_per_second': '31.536', 'grad_norm': '27.75', 'counters/examples': 188640, 'counters/updates': 5895}
train stats after 188672 examples: {'rewards_train/chosen': '0.12284', 'rewards_train/rejected': '0.039566', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.08327', 'logps_train/rejected': '-107.12', 'logps_train/chosen': '-152.2', 'loss/train': '0.67736', 'examples_per_second': '32.216', 'grad_norm': '40', 'counters/examples': 188672, 'counters/updates': 5896}
train stats after 188704 examples: {'rewards_train/chosen': '0.18236', 'rewards_train/rejected': '0.024331', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15803', 'logps_train/rejected': '-108.3', 'logps_train/chosen': '-141.55', 'loss/train': '0.62497', 'examples_per_second': '30.051', 'grad_norm': '27.375', 'counters/examples': 188704, 'counters/updates': 5897}
train stats after 188736 examples: {'rewards_train/chosen': '0.24673', 'rewards_train/rejected': '0.098751', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14798', 'logps_train/rejected': '-115.06', 'logps_train/chosen': '-117.16', 'loss/train': '0.63859', 'examples_per_second': '30.736', 'grad_norm': '29', 'counters/examples': 188736, 'counters/updates': 5898}
skipping logging after 188768 examples to avoid logging too frequently
train stats after 188800 examples: {'rewards_train/chosen': '0.2381', 'rewards_train/rejected': '0.12456', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11354', 'logps_train/rejected': '-100.05', 'logps_train/chosen': '-146.3', 'loss/train': '0.65589', 'examples_per_second': '34.232', 'grad_norm': '42.25', 'counters/examples': 188800, 'counters/updates': 5900}
skipping logging after 188832 examples to avoid logging too frequently
train stats after 188864 examples: {'rewards_train/chosen': '0.20396', 'rewards_train/rejected': '0.076541', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12742', 'logps_train/rejected': '-93.668', 'logps_train/chosen': '-148.8', 'loss/train': '0.65492', 'examples_per_second': '32.449', 'grad_norm': '46.25', 'counters/examples': 188864, 'counters/updates': 5902}
train stats after 188896 examples: {'rewards_train/chosen': '0.14391', 'rewards_train/rejected': '0.025245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11867', 'logps_train/rejected': '-92.734', 'logps_train/chosen': '-121.48', 'loss/train': '0.64647', 'examples_per_second': '31.866', 'grad_norm': '31.75', 'counters/examples': 188896, 'counters/updates': 5903}
train stats after 188928 examples: {'rewards_train/chosen': '0.19685', 'rewards_train/rejected': '0.1265', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070356', 'logps_train/rejected': '-147.68', 'logps_train/chosen': '-177.67', 'loss/train': '0.67829', 'examples_per_second': '32.428', 'grad_norm': '35.25', 'counters/examples': 188928, 'counters/updates': 5904}
train stats after 188960 examples: {'rewards_train/chosen': '0.15864', 'rewards_train/rejected': '0.042924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11571', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-126.03', 'loss/train': '0.64669', 'examples_per_second': '30.419', 'grad_norm': '26.625', 'counters/examples': 188960, 'counters/updates': 5905}
train stats after 188992 examples: {'rewards_train/chosen': '0.19422', 'rewards_train/rejected': '0.015564', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17865', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-184.6', 'loss/train': '0.61495', 'examples_per_second': '29.732', 'grad_norm': '31.375', 'counters/examples': 188992, 'counters/updates': 5906}
train stats after 189024 examples: {'rewards_train/chosen': '0.21496', 'rewards_train/rejected': '0.13932', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075643', 'logps_train/rejected': '-155.19', 'logps_train/chosen': '-162.99', 'loss/train': '0.67468', 'examples_per_second': '30.066', 'grad_norm': '48.25', 'counters/examples': 189024, 'counters/updates': 5907}
train stats after 189056 examples: {'rewards_train/chosen': '0.18867', 'rewards_train/rejected': '0.088826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09984', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-131.16', 'loss/train': '0.66265', 'examples_per_second': '29.972', 'grad_norm': '30.375', 'counters/examples': 189056, 'counters/updates': 5908}
train stats after 189088 examples: {'rewards_train/chosen': '0.13427', 'rewards_train/rejected': '0.022853', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11141', 'logps_train/rejected': '-124.65', 'logps_train/chosen': '-154.51', 'loss/train': '0.654', 'examples_per_second': '32.549', 'grad_norm': '44.75', 'counters/examples': 189088, 'counters/updates': 5909}
train stats after 189120 examples: {'rewards_train/chosen': '0.093354', 'rewards_train/rejected': '0.097595', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0042409', 'logps_train/rejected': '-92.869', 'logps_train/chosen': '-89.844', 'loss/train': '0.7083', 'examples_per_second': '32.024', 'grad_norm': '41.75', 'counters/examples': 189120, 'counters/updates': 5910}
train stats after 189152 examples: {'rewards_train/chosen': '0.16704', 'rewards_train/rejected': '0.048629', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.11842', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-137.46', 'loss/train': '0.64266', 'examples_per_second': '31.526', 'grad_norm': '24.125', 'counters/examples': 189152, 'counters/updates': 5911}
train stats after 189184 examples: {'rewards_train/chosen': '0.17867', 'rewards_train/rejected': '0.086911', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091763', 'logps_train/rejected': '-120.09', 'logps_train/chosen': '-125.11', 'loss/train': '0.67258', 'examples_per_second': '31.089', 'grad_norm': '48.25', 'counters/examples': 189184, 'counters/updates': 5912}
train stats after 189216 examples: {'rewards_train/chosen': '0.20621', 'rewards_train/rejected': '0.050954', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15526', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-163.33', 'loss/train': '0.62973', 'examples_per_second': '31.422', 'grad_norm': '26.875', 'counters/examples': 189216, 'counters/updates': 5913}
train stats after 189248 examples: {'rewards_train/chosen': '0.088463', 'rewards_train/rejected': '0.1051', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.016641', 'logps_train/rejected': '-139.72', 'logps_train/chosen': '-134.93', 'loss/train': '0.71623', 'examples_per_second': '32.222', 'grad_norm': '33.75', 'counters/examples': 189248, 'counters/updates': 5914}
train stats after 189280 examples: {'rewards_train/chosen': '0.14495', 'rewards_train/rejected': '0.059431', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085523', 'logps_train/rejected': '-142.63', 'logps_train/chosen': '-144.62', 'loss/train': '0.66262', 'examples_per_second': '31.485', 'grad_norm': '36.25', 'counters/examples': 189280, 'counters/updates': 5915}
train stats after 189312 examples: {'rewards_train/chosen': '0.16944', 'rewards_train/rejected': '0.075173', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094266', 'logps_train/rejected': '-181.21', 'logps_train/chosen': '-125.14', 'loss/train': '0.6656', 'examples_per_second': '31.482', 'grad_norm': '48.25', 'counters/examples': 189312, 'counters/updates': 5916}
skipping logging after 189344 examples to avoid logging too frequently
train stats after 189376 examples: {'rewards_train/chosen': '0.16039', 'rewards_train/rejected': '0.053207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10718', 'logps_train/rejected': '-106.52', 'logps_train/chosen': '-133.81', 'loss/train': '0.65694', 'examples_per_second': '35.458', 'grad_norm': '24', 'counters/examples': 189376, 'counters/updates': 5918}
skipping logging after 189408 examples to avoid logging too frequently
train stats after 189440 examples: {'rewards_train/chosen': '0.039436', 'rewards_train/rejected': '0.011168', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028268', 'logps_train/rejected': '-98.902', 'logps_train/chosen': '-130.11', 'loss/train': '0.6917', 'examples_per_second': '35.817', 'grad_norm': '23', 'counters/examples': 189440, 'counters/updates': 5920}
train stats after 189472 examples: {'rewards_train/chosen': '0.24089', 'rewards_train/rejected': '0.051499', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18939', 'logps_train/rejected': '-132.93', 'logps_train/chosen': '-170.1', 'loss/train': '0.62023', 'examples_per_second': '31.458', 'grad_norm': '37.25', 'counters/examples': 189472, 'counters/updates': 5921}
train stats after 189504 examples: {'rewards_train/chosen': '0.14318', 'rewards_train/rejected': '0.032025', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11116', 'logps_train/rejected': '-147.55', 'logps_train/chosen': '-132.08', 'loss/train': '0.65268', 'examples_per_second': '31.52', 'grad_norm': '29.125', 'counters/examples': 189504, 'counters/updates': 5922}
skipping logging after 189536 examples to avoid logging too frequently
train stats after 189568 examples: {'rewards_train/chosen': '0.15286', 'rewards_train/rejected': '0.053531', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099329', 'logps_train/rejected': '-134.24', 'logps_train/chosen': '-160.16', 'loss/train': '0.65175', 'examples_per_second': '33.349', 'grad_norm': '28.125', 'counters/examples': 189568, 'counters/updates': 5924}
train stats after 189600 examples: {'rewards_train/chosen': '0.32756', 'rewards_train/rejected': '0.13671', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19085', 'logps_train/rejected': '-129', 'logps_train/chosen': '-206.92', 'loss/train': '0.62436', 'examples_per_second': '30.182', 'grad_norm': '30.125', 'counters/examples': 189600, 'counters/updates': 5925}
train stats after 189632 examples: {'rewards_train/chosen': '0.26912', 'rewards_train/rejected': '0.024763', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24435', 'logps_train/rejected': '-117.12', 'logps_train/chosen': '-156.27', 'loss/train': '0.60793', 'examples_per_second': '31.917', 'grad_norm': '44.5', 'counters/examples': 189632, 'counters/updates': 5926}
train stats after 189664 examples: {'rewards_train/chosen': '0.19324', 'rewards_train/rejected': '0.035306', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15794', 'logps_train/rejected': '-130.95', 'logps_train/chosen': '-137.68', 'loss/train': '0.6261', 'examples_per_second': '30.876', 'grad_norm': '45.25', 'counters/examples': 189664, 'counters/updates': 5927}
train stats after 189696 examples: {'rewards_train/chosen': '0.16289', 'rewards_train/rejected': '0.093284', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06961', 'logps_train/rejected': '-151.34', 'logps_train/chosen': '-159.45', 'loss/train': '0.66989', 'examples_per_second': '30.811', 'grad_norm': '45.25', 'counters/examples': 189696, 'counters/updates': 5928}
train stats after 189728 examples: {'rewards_train/chosen': '0.12323', 'rewards_train/rejected': '0.024242', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098991', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-129.43', 'loss/train': '0.65588', 'examples_per_second': '29.916', 'grad_norm': '27.875', 'counters/examples': 189728, 'counters/updates': 5929}
train stats after 189760 examples: {'rewards_train/chosen': '0.082631', 'rewards_train/rejected': '-0.013342', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095973', 'logps_train/rejected': '-112.28', 'logps_train/chosen': '-119.32', 'loss/train': '0.65548', 'examples_per_second': '30.491', 'grad_norm': '24.125', 'counters/examples': 189760, 'counters/updates': 5930}
train stats after 189792 examples: {'rewards_train/chosen': '0.089004', 'rewards_train/rejected': '0.10261', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.013605', 'logps_train/rejected': '-121.03', 'logps_train/chosen': '-109.17', 'loss/train': '0.7117', 'examples_per_second': '30.91', 'grad_norm': '27.125', 'counters/examples': 189792, 'counters/updates': 5931}
train stats after 189824 examples: {'rewards_train/chosen': '0.20243', 'rewards_train/rejected': '0.022149', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18028', 'logps_train/rejected': '-115.39', 'logps_train/chosen': '-163.76', 'loss/train': '0.62032', 'examples_per_second': '32.012', 'grad_norm': '31.125', 'counters/examples': 189824, 'counters/updates': 5932}
train stats after 189856 examples: {'rewards_train/chosen': '0.12543', 'rewards_train/rejected': '0.12523', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.00020724', 'logps_train/rejected': '-154.71', 'logps_train/chosen': '-118.29', 'loss/train': '0.70535', 'examples_per_second': '30.364', 'grad_norm': '37.5', 'counters/examples': 189856, 'counters/updates': 5933}
train stats after 189888 examples: {'rewards_train/chosen': '0.19458', 'rewards_train/rejected': '0.066344', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12823', 'logps_train/rejected': '-121.82', 'logps_train/chosen': '-141.03', 'loss/train': '0.64202', 'examples_per_second': '31.415', 'grad_norm': '30.75', 'counters/examples': 189888, 'counters/updates': 5934}
train stats after 189920 examples: {'rewards_train/chosen': '0.16283', 'rewards_train/rejected': '0.091031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071801', 'logps_train/rejected': '-161.74', 'logps_train/chosen': '-154.87', 'loss/train': '0.6793', 'examples_per_second': '31.921', 'grad_norm': '39.5', 'counters/examples': 189920, 'counters/updates': 5935}
train stats after 189952 examples: {'rewards_train/chosen': '0.1003', 'rewards_train/rejected': '0.065663', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.034636', 'logps_train/rejected': '-136.32', 'logps_train/chosen': '-169.23', 'loss/train': '0.6868', 'examples_per_second': '31.093', 'grad_norm': '32', 'counters/examples': 189952, 'counters/updates': 5936}
train stats after 189984 examples: {'rewards_train/chosen': '0.097962', 'rewards_train/rejected': '0.098419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00045694', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-149.54', 'loss/train': '0.70761', 'examples_per_second': '31.461', 'grad_norm': '31.5', 'counters/examples': 189984, 'counters/updates': 5937}
train stats after 190016 examples: {'rewards_train/chosen': '0.21919', 'rewards_train/rejected': '0.025638', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19355', 'logps_train/rejected': '-108.32', 'logps_train/chosen': '-168.46', 'loss/train': '0.6163', 'examples_per_second': '31.532', 'grad_norm': '29.75', 'counters/examples': 190016, 'counters/updates': 5938}
train stats after 190048 examples: {'rewards_train/chosen': '0.13713', 'rewards_train/rejected': '0.02486', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11227', 'logps_train/rejected': '-124.51', 'logps_train/chosen': '-116.55', 'loss/train': '0.64383', 'examples_per_second': '30.946', 'grad_norm': '24.125', 'counters/examples': 190048, 'counters/updates': 5939}
train stats after 190080 examples: {'rewards_train/chosen': '0.18883', 'rewards_train/rejected': '0.099045', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.089785', 'logps_train/rejected': '-171.15', 'logps_train/chosen': '-183.74', 'loss/train': '0.65622', 'examples_per_second': '31.82', 'grad_norm': '38', 'counters/examples': 190080, 'counters/updates': 5940}
train stats after 190112 examples: {'rewards_train/chosen': '0.1205', 'rewards_train/rejected': '0.1014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.019093', 'logps_train/rejected': '-142.44', 'logps_train/chosen': '-127.18', 'loss/train': '0.69958', 'examples_per_second': '30.483', 'grad_norm': '35.5', 'counters/examples': 190112, 'counters/updates': 5941}
train stats after 190144 examples: {'rewards_train/chosen': '0.10392', 'rewards_train/rejected': '0.042808', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061116', 'logps_train/rejected': '-106.5', 'logps_train/chosen': '-136.84', 'loss/train': '0.67592', 'examples_per_second': '32.854', 'grad_norm': '40.5', 'counters/examples': 190144, 'counters/updates': 5942}
skipping logging after 190176 examples to avoid logging too frequently
train stats after 190208 examples: {'rewards_train/chosen': '0.13165', 'rewards_train/rejected': '0.079508', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.052146', 'logps_train/rejected': '-131.12', 'logps_train/chosen': '-133.24', 'loss/train': '0.67744', 'examples_per_second': '30.082', 'grad_norm': '34.5', 'counters/examples': 190208, 'counters/updates': 5944}
train stats after 190240 examples: {'rewards_train/chosen': '0.19626', 'rewards_train/rejected': '0.11895', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077309', 'logps_train/rejected': '-125.23', 'logps_train/chosen': '-139.68', 'loss/train': '0.66958', 'examples_per_second': '23.644', 'grad_norm': '34.75', 'counters/examples': 190240, 'counters/updates': 5945}
train stats after 190272 examples: {'rewards_train/chosen': '0.16294', 'rewards_train/rejected': '-0.011183', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17412', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-122.37', 'loss/train': '0.61769', 'examples_per_second': '30.859', 'grad_norm': '26.25', 'counters/examples': 190272, 'counters/updates': 5946}
train stats after 190304 examples: {'rewards_train/chosen': '0.17022', 'rewards_train/rejected': '0.13858', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031641', 'logps_train/rejected': '-154.97', 'logps_train/chosen': '-156.24', 'loss/train': '0.69909', 'examples_per_second': '31.152', 'grad_norm': '33.5', 'counters/examples': 190304, 'counters/updates': 5947}
train stats after 190336 examples: {'rewards_train/chosen': '0.19334', 'rewards_train/rejected': '0.11332', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.080021', 'logps_train/rejected': '-140.32', 'logps_train/chosen': '-142.22', 'loss/train': '0.67492', 'examples_per_second': '23.918', 'grad_norm': '42.75', 'counters/examples': 190336, 'counters/updates': 5948}
train stats after 190368 examples: {'rewards_train/chosen': '0.12605', 'rewards_train/rejected': '0.067575', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058477', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-129.05', 'loss/train': '0.67304', 'examples_per_second': '32.43', 'grad_norm': '25.875', 'counters/examples': 190368, 'counters/updates': 5949}
train stats after 190400 examples: {'rewards_train/chosen': '0.10185', 'rewards_train/rejected': '0.093215', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0086302', 'logps_train/rejected': '-140.03', 'logps_train/chosen': '-177.48', 'loss/train': '0.6985', 'examples_per_second': '31.13', 'grad_norm': '48.75', 'counters/examples': 190400, 'counters/updates': 5950}
skipping logging after 190432 examples to avoid logging too frequently
train stats after 190464 examples: {'rewards_train/chosen': '0.21875', 'rewards_train/rejected': '0.016441', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20231', 'logps_train/rejected': '-100.55', 'logps_train/chosen': '-177.02', 'loss/train': '0.61442', 'examples_per_second': '30.261', 'grad_norm': '28.875', 'counters/examples': 190464, 'counters/updates': 5952}
train stats after 190496 examples: {'rewards_train/chosen': '0.13599', 'rewards_train/rejected': '0.12371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.012277', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-128.28', 'loss/train': '0.70148', 'examples_per_second': '29.95', 'grad_norm': '37.5', 'counters/examples': 190496, 'counters/updates': 5953}
train stats after 190528 examples: {'rewards_train/chosen': '0.14326', 'rewards_train/rejected': '0.062164', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081094', 'logps_train/rejected': '-118.75', 'logps_train/chosen': '-113.1', 'loss/train': '0.66386', 'examples_per_second': '32.393', 'grad_norm': '27.375', 'counters/examples': 190528, 'counters/updates': 5954}
train stats after 190560 examples: {'rewards_train/chosen': '0.098264', 'rewards_train/rejected': '0.057359', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.040905', 'logps_train/rejected': '-179.13', 'logps_train/chosen': '-151.75', 'loss/train': '0.6833', 'examples_per_second': '30.228', 'grad_norm': '32.25', 'counters/examples': 190560, 'counters/updates': 5955}
train stats after 190592 examples: {'rewards_train/chosen': '0.11974', 'rewards_train/rejected': '-0.034513', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15425', 'logps_train/rejected': '-99.196', 'logps_train/chosen': '-136.13', 'loss/train': '0.62809', 'examples_per_second': '31.892', 'grad_norm': '24.5', 'counters/examples': 190592, 'counters/updates': 5956}
train stats after 190624 examples: {'rewards_train/chosen': '0.25004', 'rewards_train/rejected': '0.067367', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18268', 'logps_train/rejected': '-135.84', 'logps_train/chosen': '-175.5', 'loss/train': '0.6305', 'examples_per_second': '31.742', 'grad_norm': '29.125', 'counters/examples': 190624, 'counters/updates': 5957}
train stats after 190656 examples: {'rewards_train/chosen': '0.16118', 'rewards_train/rejected': '0.060541', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10064', 'logps_train/rejected': '-121.86', 'logps_train/chosen': '-167.71', 'loss/train': '0.6685', 'examples_per_second': '32.36', 'grad_norm': '40.25', 'counters/examples': 190656, 'counters/updates': 5958}
train stats after 190688 examples: {'rewards_train/chosen': '0.15323', 'rewards_train/rejected': '0.05923', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094005', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-136.22', 'loss/train': '0.6621', 'examples_per_second': '31.503', 'grad_norm': '34', 'counters/examples': 190688, 'counters/updates': 5959}
train stats after 190720 examples: {'rewards_train/chosen': '0.14434', 'rewards_train/rejected': '-0.0097587', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1541', 'logps_train/rejected': '-100.47', 'logps_train/chosen': '-156.94', 'loss/train': '0.63337', 'examples_per_second': '30.361', 'grad_norm': '30', 'counters/examples': 190720, 'counters/updates': 5960}
train stats after 190752 examples: {'rewards_train/chosen': '0.15522', 'rewards_train/rejected': '0.080437', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07478', 'logps_train/rejected': '-152.03', 'logps_train/chosen': '-186.05', 'loss/train': '0.66573', 'examples_per_second': '31.452', 'grad_norm': '34.5', 'counters/examples': 190752, 'counters/updates': 5961}
train stats after 190784 examples: {'rewards_train/chosen': '0.22182', 'rewards_train/rejected': '0.10299', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11883', 'logps_train/rejected': '-143.15', 'logps_train/chosen': '-183.32', 'loss/train': '0.65801', 'examples_per_second': '30.907', 'grad_norm': '39.75', 'counters/examples': 190784, 'counters/updates': 5962}
train stats after 190816 examples: {'rewards_train/chosen': '0.078229', 'rewards_train/rejected': '0.1049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.026667', 'logps_train/rejected': '-145.39', 'logps_train/chosen': '-156.63', 'loss/train': '0.72302', 'examples_per_second': '31.613', 'grad_norm': '67', 'counters/examples': 190816, 'counters/updates': 5963}
train stats after 190848 examples: {'rewards_train/chosen': '0.19654', 'rewards_train/rejected': '0.1925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0040401', 'logps_train/rejected': '-142.84', 'logps_train/chosen': '-152.16', 'loss/train': '0.70257', 'examples_per_second': '32.01', 'grad_norm': '54.25', 'counters/examples': 190848, 'counters/updates': 5964}
train stats after 190880 examples: {'rewards_train/chosen': '0.17164', 'rewards_train/rejected': '0.032468', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13917', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-169.54', 'loss/train': '0.64074', 'examples_per_second': '32.788', 'grad_norm': '30.25', 'counters/examples': 190880, 'counters/updates': 5965}
train stats after 190912 examples: {'rewards_train/chosen': '0.16441', 'rewards_train/rejected': '0.1184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.046007', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-122.23', 'loss/train': '0.67878', 'examples_per_second': '30.491', 'grad_norm': '30.75', 'counters/examples': 190912, 'counters/updates': 5966}
skipping logging after 190944 examples to avoid logging too frequently
train stats after 190976 examples: {'rewards_train/chosen': '0.12933', 'rewards_train/rejected': '0.070874', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058454', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-133.67', 'loss/train': '0.67753', 'examples_per_second': '31.297', 'grad_norm': '27.875', 'counters/examples': 190976, 'counters/updates': 5968}
train stats after 191008 examples: {'rewards_train/chosen': '0.079906', 'rewards_train/rejected': '0.034925', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.04498', 'logps_train/rejected': '-132.98', 'logps_train/chosen': '-200.83', 'loss/train': '0.70036', 'examples_per_second': '30.624', 'grad_norm': '35.75', 'counters/examples': 191008, 'counters/updates': 5969}
train stats after 191040 examples: {'rewards_train/chosen': '0.13372', 'rewards_train/rejected': '0.055969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077751', 'logps_train/rejected': '-119.19', 'logps_train/chosen': '-159.5', 'loss/train': '0.67136', 'examples_per_second': '31.361', 'grad_norm': '28.25', 'counters/examples': 191040, 'counters/updates': 5970}
train stats after 191072 examples: {'rewards_train/chosen': '0.096045', 'rewards_train/rejected': '0.091046', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.0049992', 'logps_train/rejected': '-118.82', 'logps_train/chosen': '-95.822', 'loss/train': '0.69753', 'examples_per_second': '31.565', 'grad_norm': '25.625', 'counters/examples': 191072, 'counters/updates': 5971}
train stats after 191104 examples: {'rewards_train/chosen': '0.21541', 'rewards_train/rejected': '0.00062967', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21478', 'logps_train/rejected': '-135.07', 'logps_train/chosen': '-155.2', 'loss/train': '0.62945', 'examples_per_second': '30.051', 'grad_norm': '28.75', 'counters/examples': 191104, 'counters/updates': 5972}
train stats after 191136 examples: {'rewards_train/chosen': '0.15273', 'rewards_train/rejected': '0.074792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077938', 'logps_train/rejected': '-137.68', 'logps_train/chosen': '-148.47', 'loss/train': '0.67332', 'examples_per_second': '31.556', 'grad_norm': '51.25', 'counters/examples': 191136, 'counters/updates': 5973}
skipping logging after 191168 examples to avoid logging too frequently
train stats after 191200 examples: {'rewards_train/chosen': '0.20411', 'rewards_train/rejected': '0.18223', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021876', 'logps_train/rejected': '-154.26', 'logps_train/chosen': '-145.04', 'loss/train': '0.69691', 'examples_per_second': '31.441', 'grad_norm': '35.25', 'counters/examples': 191200, 'counters/updates': 5975}
train stats after 191232 examples: {'rewards_train/chosen': '0.2381', 'rewards_train/rejected': '0.16686', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.071242', 'logps_train/rejected': '-131.51', 'logps_train/chosen': '-178.61', 'loss/train': '0.67469', 'examples_per_second': '30.041', 'grad_norm': '40.5', 'counters/examples': 191232, 'counters/updates': 5976}
train stats after 191264 examples: {'rewards_train/chosen': '0.13662', 'rewards_train/rejected': '0.047086', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.08953', 'logps_train/rejected': '-139.52', 'logps_train/chosen': '-156.33', 'loss/train': '0.66221', 'examples_per_second': '30.175', 'grad_norm': '28.25', 'counters/examples': 191264, 'counters/updates': 5977}
train stats after 191296 examples: {'rewards_train/chosen': '0.18326', 'rewards_train/rejected': '0.019942', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16332', 'logps_train/rejected': '-124.26', 'logps_train/chosen': '-145.63', 'loss/train': '0.62598', 'examples_per_second': '31.088', 'grad_norm': '35.5', 'counters/examples': 191296, 'counters/updates': 5978}
train stats after 191328 examples: {'rewards_train/chosen': '0.14803', 'rewards_train/rejected': '0.046356', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10167', 'logps_train/rejected': '-150.77', 'logps_train/chosen': '-136.99', 'loss/train': '0.66109', 'examples_per_second': '30.466', 'grad_norm': '45.75', 'counters/examples': 191328, 'counters/updates': 5979}
train stats after 191360 examples: {'rewards_train/chosen': '0.10365', 'rewards_train/rejected': '0.091796', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011857', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-161.63', 'loss/train': '0.70992', 'examples_per_second': '31.537', 'grad_norm': '40', 'counters/examples': 191360, 'counters/updates': 5980}
train stats after 191392 examples: {'rewards_train/chosen': '0.19381', 'rewards_train/rejected': '0.039301', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15451', 'logps_train/rejected': '-109.57', 'logps_train/chosen': '-151.14', 'loss/train': '0.62975', 'examples_per_second': '31.58', 'grad_norm': '27', 'counters/examples': 191392, 'counters/updates': 5981}
train stats after 191424 examples: {'rewards_train/chosen': '0.13594', 'rewards_train/rejected': '0.04141', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094529', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-141.84', 'loss/train': '0.657', 'examples_per_second': '30.515', 'grad_norm': '30.75', 'counters/examples': 191424, 'counters/updates': 5982}
train stats after 191456 examples: {'rewards_train/chosen': '0.12382', 'rewards_train/rejected': '-0.00053561', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12436', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-143.5', 'loss/train': '0.64156', 'examples_per_second': '29.611', 'grad_norm': '25.375', 'counters/examples': 191456, 'counters/updates': 5983}
train stats after 191488 examples: {'rewards_train/chosen': '0.095077', 'rewards_train/rejected': '0.061718', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033359', 'logps_train/rejected': '-123.68', 'logps_train/chosen': '-132.35', 'loss/train': '0.69213', 'examples_per_second': '29.965', 'grad_norm': '33.5', 'counters/examples': 191488, 'counters/updates': 5984}
train stats after 191520 examples: {'rewards_train/chosen': '0.13773', 'rewards_train/rejected': '0.063747', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073979', 'logps_train/rejected': '-149.08', 'logps_train/chosen': '-162.62', 'loss/train': '0.67336', 'examples_per_second': '31.49', 'grad_norm': '30.25', 'counters/examples': 191520, 'counters/updates': 5985}
train stats after 191552 examples: {'rewards_train/chosen': '0.15454', 'rewards_train/rejected': '-0.10294', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25748', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-135.61', 'loss/train': '0.61244', 'examples_per_second': '31.078', 'grad_norm': '24.25', 'counters/examples': 191552, 'counters/updates': 5986}
train stats after 191584 examples: {'rewards_train/chosen': '0.16551', 'rewards_train/rejected': '0.14034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.025167', 'logps_train/rejected': '-150.83', 'logps_train/chosen': '-158.67', 'loss/train': '0.69772', 'examples_per_second': '31.23', 'grad_norm': '49.5', 'counters/examples': 191584, 'counters/updates': 5987}
train stats after 191616 examples: {'rewards_train/chosen': '0.26937', 'rewards_train/rejected': '0.083687', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18568', 'logps_train/rejected': '-148.06', 'logps_train/chosen': '-157.62', 'loss/train': '0.62913', 'examples_per_second': '30.579', 'grad_norm': '39', 'counters/examples': 191616, 'counters/updates': 5988}
train stats after 191648 examples: {'rewards_train/chosen': '0.16885', 'rewards_train/rejected': '0.0682', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10066', 'logps_train/rejected': '-110.53', 'logps_train/chosen': '-127.38', 'loss/train': '0.65316', 'examples_per_second': '30.788', 'grad_norm': '27.25', 'counters/examples': 191648, 'counters/updates': 5989}
skipping logging after 191680 examples to avoid logging too frequently
train stats after 191712 examples: {'rewards_train/chosen': '0.16647', 'rewards_train/rejected': '0.10145', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.065024', 'logps_train/rejected': '-165.02', 'logps_train/chosen': '-176.62', 'loss/train': '0.67511', 'examples_per_second': '31.248', 'grad_norm': '36.25', 'counters/examples': 191712, 'counters/updates': 5991}
train stats after 191744 examples: {'rewards_train/chosen': '0.23488', 'rewards_train/rejected': '0.053699', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18118', 'logps_train/rejected': '-137.05', 'logps_train/chosen': '-163.19', 'loss/train': '0.62615', 'examples_per_second': '30.523', 'grad_norm': '32.25', 'counters/examples': 191744, 'counters/updates': 5992}
skipping logging after 191776 examples to avoid logging too frequently
train stats after 191808 examples: {'rewards_train/chosen': '0.12055', 'rewards_train/rejected': '0.031698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088848', 'logps_train/rejected': '-83.956', 'logps_train/chosen': '-113.22', 'loss/train': '0.66168', 'examples_per_second': '31.459', 'grad_norm': '23.875', 'counters/examples': 191808, 'counters/updates': 5994}
skipping logging after 191840 examples to avoid logging too frequently
train stats after 191872 examples: {'rewards_train/chosen': '0.19541', 'rewards_train/rejected': '0.13435', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06106', 'logps_train/rejected': '-160.08', 'logps_train/chosen': '-162.02', 'loss/train': '0.70444', 'examples_per_second': '29.958', 'grad_norm': '40.5', 'counters/examples': 191872, 'counters/updates': 5996}
skipping logging after 191904 examples to avoid logging too frequently
train stats after 191936 examples: {'rewards_train/chosen': '0.18187', 'rewards_train/rejected': '0.039019', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14285', 'logps_train/rejected': '-136.21', 'logps_train/chosen': '-167.47', 'loss/train': '0.64037', 'examples_per_second': '31.543', 'grad_norm': '58', 'counters/examples': 191936, 'counters/updates': 5998}
train stats after 191968 examples: {'rewards_train/chosen': '0.20932', 'rewards_train/rejected': '0.090642', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11868', 'logps_train/rejected': '-139.58', 'logps_train/chosen': '-147.15', 'loss/train': '0.64718', 'examples_per_second': '30.208', 'grad_norm': '40.75', 'counters/examples': 191968, 'counters/updates': 5999}
train stats after 192000 examples: {'rewards_train/chosen': '0.16883', 'rewards_train/rejected': '0.041929', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1269', 'logps_train/rejected': '-125.22', 'logps_train/chosen': '-150.24', 'loss/train': '0.64982', 'examples_per_second': '31.462', 'grad_norm': '28.25', 'counters/examples': 192000, 'counters/updates': 6000}
Running evaluation after 192000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.22it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 192000: {'rewards_eval/chosen': '0.18935', 'rewards_eval/rejected': '0.081009', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.10834', 'logps_eval/rejected': '-121.32', 'logps_eval/chosen': '-142.21', 'loss/eval': '0.6572'}
creating checkpoint to write to .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349/step-192000...
writing checkpoint to .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349/step-192000/policy.pt...
train stats after 192032 examples: {'rewards_train/chosen': '0.12171', 'rewards_train/rejected': '0.016972', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10474', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-131.31', 'loss/train': '0.65142', 'examples_per_second': '20.92', 'grad_norm': '27.375', 'counters/examples': 192032, 'counters/updates': 6001}
train stats after 192064 examples: {'rewards_train/chosen': '0.38263', 'rewards_train/rejected': '0.074671', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.30796', 'logps_train/rejected': '-134.89', 'logps_train/chosen': '-164.02', 'loss/train': '0.59652', 'examples_per_second': '29.525', 'grad_norm': '27.125', 'counters/examples': 192064, 'counters/updates': 6002}
train stats after 192096 examples: {'rewards_train/chosen': '0.22348', 'rewards_train/rejected': '0.08687', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13661', 'logps_train/rejected': '-144.01', 'logps_train/chosen': '-190.09', 'loss/train': '0.6422', 'examples_per_second': '31.828', 'grad_norm': '41', 'counters/examples': 192096, 'counters/updates': 6003}
skipping logging after 192128 examples to avoid logging too frequently
train stats after 192160 examples: {'rewards_train/chosen': '0.10611', 'rewards_train/rejected': '0.018582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087524', 'logps_train/rejected': '-159.13', 'logps_train/chosen': '-153.1', 'loss/train': '0.66433', 'examples_per_second': '31.283', 'grad_norm': '35.25', 'counters/examples': 192160, 'counters/updates': 6005}
train stats after 192192 examples: {'rewards_train/chosen': '0.25286', 'rewards_train/rejected': '0.058435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19442', 'logps_train/rejected': '-101.11', 'logps_train/chosen': '-175.71', 'loss/train': '0.61903', 'examples_per_second': '31.382', 'grad_norm': '27.875', 'counters/examples': 192192, 'counters/updates': 6006}
skipping logging after 192224 examples to avoid logging too frequently
train stats after 192256 examples: {'rewards_train/chosen': '0.11563', 'rewards_train/rejected': '0.13792', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022289', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-132.16', 'loss/train': '0.71962', 'examples_per_second': '31.515', 'grad_norm': '54.75', 'counters/examples': 192256, 'counters/updates': 6008}
train stats after 192288 examples: {'rewards_train/chosen': '0.22684', 'rewards_train/rejected': '0.096206', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13064', 'logps_train/rejected': '-133.97', 'logps_train/chosen': '-156.08', 'loss/train': '0.64078', 'examples_per_second': '31.056', 'grad_norm': '34.5', 'counters/examples': 192288, 'counters/updates': 6009}
train stats after 192320 examples: {'rewards_train/chosen': '0.12926', 'rewards_train/rejected': '0.056022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073242', 'logps_train/rejected': '-152.51', 'logps_train/chosen': '-119.37', 'loss/train': '0.6729', 'examples_per_second': '31.361', 'grad_norm': '27.75', 'counters/examples': 192320, 'counters/updates': 6010}
skipping logging after 192352 examples to avoid logging too frequently
train stats after 192384 examples: {'rewards_train/chosen': '0.093579', 'rewards_train/rejected': '0.094754', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0011749', 'logps_train/rejected': '-96.572', 'logps_train/chosen': '-127.45', 'loss/train': '0.70687', 'examples_per_second': '30.39', 'grad_norm': '85.5', 'counters/examples': 192384, 'counters/updates': 6012}
train stats after 192416 examples: {'rewards_train/chosen': '0.13991', 'rewards_train/rejected': '0.002193', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13772', 'logps_train/rejected': '-151.47', 'logps_train/chosen': '-141.95', 'loss/train': '0.63372', 'examples_per_second': '31.034', 'grad_norm': '42.5', 'counters/examples': 192416, 'counters/updates': 6013}
train stats after 192448 examples: {'rewards_train/chosen': '0.21563', 'rewards_train/rejected': '0.099285', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11635', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-128.13', 'loss/train': '0.65553', 'examples_per_second': '31.821', 'grad_norm': '28.125', 'counters/examples': 192448, 'counters/updates': 6014}
train stats after 192480 examples: {'rewards_train/chosen': '0.2399', 'rewards_train/rejected': '0.15982', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080084', 'logps_train/rejected': '-161.56', 'logps_train/chosen': '-164.73', 'loss/train': '0.67287', 'examples_per_second': '30.078', 'grad_norm': '50', 'counters/examples': 192480, 'counters/updates': 6015}
train stats after 192512 examples: {'rewards_train/chosen': '0.12586', 'rewards_train/rejected': '0.016197', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10966', 'logps_train/rejected': '-163.93', 'logps_train/chosen': '-132.02', 'loss/train': '0.65591', 'examples_per_second': '30.685', 'grad_norm': '35.5', 'counters/examples': 192512, 'counters/updates': 6016}
train stats after 192544 examples: {'rewards_train/chosen': '0.18446', 'rewards_train/rejected': '0.01545', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16901', 'logps_train/rejected': '-147.67', 'logps_train/chosen': '-169.92', 'loss/train': '0.6306', 'examples_per_second': '32.051', 'grad_norm': '31.125', 'counters/examples': 192544, 'counters/updates': 6017}
train stats after 192576 examples: {'rewards_train/chosen': '0.19381', 'rewards_train/rejected': '0.052074', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14173', 'logps_train/rejected': '-102', 'logps_train/chosen': '-132.62', 'loss/train': '0.64022', 'examples_per_second': '30.682', 'grad_norm': '55', 'counters/examples': 192576, 'counters/updates': 6018}
train stats after 192608 examples: {'rewards_train/chosen': '0.20952', 'rewards_train/rejected': '-0.041476', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.251', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-164.09', 'loss/train': '0.60226', 'examples_per_second': '31.557', 'grad_norm': '25', 'counters/examples': 192608, 'counters/updates': 6019}
train stats after 192640 examples: {'rewards_train/chosen': '0.18271', 'rewards_train/rejected': '0.069873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11284', 'logps_train/rejected': '-170.37', 'logps_train/chosen': '-187.16', 'loss/train': '0.67578', 'examples_per_second': '30.202', 'grad_norm': '35.75', 'counters/examples': 192640, 'counters/updates': 6020}
train stats after 192672 examples: {'rewards_train/chosen': '0.28687', 'rewards_train/rejected': '0.12241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16447', 'logps_train/rejected': '-153.98', 'logps_train/chosen': '-166.33', 'loss/train': '0.64019', 'examples_per_second': '30.048', 'grad_norm': '31.125', 'counters/examples': 192672, 'counters/updates': 6021}
train stats after 192704 examples: {'rewards_train/chosen': '0.16014', 'rewards_train/rejected': '-0.013138', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17327', 'logps_train/rejected': '-92.146', 'logps_train/chosen': '-148.07', 'loss/train': '0.61987', 'examples_per_second': '32.581', 'grad_norm': '22.5', 'counters/examples': 192704, 'counters/updates': 6022}
train stats after 192736 examples: {'rewards_train/chosen': '0.14315', 'rewards_train/rejected': '-0.0095449', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15269', 'logps_train/rejected': '-114.89', 'logps_train/chosen': '-137.18', 'loss/train': '0.63745', 'examples_per_second': '31.287', 'grad_norm': '24', 'counters/examples': 192736, 'counters/updates': 6023}
train stats after 192768 examples: {'rewards_train/chosen': '0.22844', 'rewards_train/rejected': '0.058015', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17042', 'logps_train/rejected': '-107.76', 'logps_train/chosen': '-159.49', 'loss/train': '0.62834', 'examples_per_second': '31.491', 'grad_norm': '31.25', 'counters/examples': 192768, 'counters/updates': 6024}
train stats after 192800 examples: {'rewards_train/chosen': '0.30575', 'rewards_train/rejected': '0.10229', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20346', 'logps_train/rejected': '-102.76', 'logps_train/chosen': '-139.7', 'loss/train': '0.61774', 'examples_per_second': '31.477', 'grad_norm': '25.5', 'counters/examples': 192800, 'counters/updates': 6025}
train stats after 192832 examples: {'rewards_train/chosen': '0.16025', 'rewards_train/rejected': '0.052578', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10767', 'logps_train/rejected': '-163.95', 'logps_train/chosen': '-157.27', 'loss/train': '0.65424', 'examples_per_second': '30.891', 'grad_norm': '30.625', 'counters/examples': 192832, 'counters/updates': 6026}
train stats after 192864 examples: {'rewards_train/chosen': '0.1378', 'rewards_train/rejected': '0.024126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11367', 'logps_train/rejected': '-90.746', 'logps_train/chosen': '-152.89', 'loss/train': '0.65377', 'examples_per_second': '32.041', 'grad_norm': '56.75', 'counters/examples': 192864, 'counters/updates': 6027}
train stats after 192896 examples: {'rewards_train/chosen': '0.34778', 'rewards_train/rejected': '0.08517', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26261', 'logps_train/rejected': '-155.26', 'logps_train/chosen': '-200.04', 'loss/train': '0.59373', 'examples_per_second': '30.479', 'grad_norm': '30.625', 'counters/examples': 192896, 'counters/updates': 6028}
train stats after 192928 examples: {'rewards_train/chosen': '0.071915', 'rewards_train/rejected': '0.0080167', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063898', 'logps_train/rejected': '-105.58', 'logps_train/chosen': '-113.94', 'loss/train': '0.67326', 'examples_per_second': '32.073', 'grad_norm': '25.375', 'counters/examples': 192928, 'counters/updates': 6029}
train stats after 192960 examples: {'rewards_train/chosen': '0.11648', 'rewards_train/rejected': '-0.024999', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14148', 'logps_train/rejected': '-85.529', 'logps_train/chosen': '-109.75', 'loss/train': '0.64272', 'examples_per_second': '31.588', 'grad_norm': '23.375', 'counters/examples': 192960, 'counters/updates': 6030}
train stats after 192992 examples: {'rewards_train/chosen': '0.159', 'rewards_train/rejected': '0.15804', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00095332', 'logps_train/rejected': '-170.97', 'logps_train/chosen': '-162.47', 'loss/train': '0.74502', 'examples_per_second': '31.512', 'grad_norm': '54.75', 'counters/examples': 192992, 'counters/updates': 6031}
train stats after 193024 examples: {'rewards_train/chosen': '0.11258', 'rewards_train/rejected': '0.02074', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091845', 'logps_train/rejected': '-144.46', 'logps_train/chosen': '-132.51', 'loss/train': '0.6581', 'examples_per_second': '31.246', 'grad_norm': '28.625', 'counters/examples': 193024, 'counters/updates': 6032}
skipping logging after 193056 examples to avoid logging too frequently
train stats after 193088 examples: {'rewards_train/chosen': '0.13986', 'rewards_train/rejected': '0.025838', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11402', 'logps_train/rejected': '-104.09', 'logps_train/chosen': '-138.13', 'loss/train': '0.68481', 'examples_per_second': '31.423', 'grad_norm': '37.25', 'counters/examples': 193088, 'counters/updates': 6034}
train stats after 193120 examples: {'rewards_train/chosen': '0.1206', 'rewards_train/rejected': '0.1285', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0079017', 'logps_train/rejected': '-129.75', 'logps_train/chosen': '-112.05', 'loss/train': '0.71051', 'examples_per_second': '31.501', 'grad_norm': '44.75', 'counters/examples': 193120, 'counters/updates': 6035}
skipping logging after 193152 examples to avoid logging too frequently
train stats after 193184 examples: {'rewards_train/chosen': '0.16986', 'rewards_train/rejected': '0.1275', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042359', 'logps_train/rejected': '-99.214', 'logps_train/chosen': '-144.57', 'loss/train': '0.69333', 'examples_per_second': '32.278', 'grad_norm': '32.5', 'counters/examples': 193184, 'counters/updates': 6037}
train stats after 193216 examples: {'rewards_train/chosen': '0.20324', 'rewards_train/rejected': '0.07519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12805', 'logps_train/rejected': '-129.81', 'logps_train/chosen': '-125.61', 'loss/train': '0.64674', 'examples_per_second': '32.723', 'grad_norm': '32.5', 'counters/examples': 193216, 'counters/updates': 6038}
train stats after 193248 examples: {'rewards_train/chosen': '0.11971', 'rewards_train/rejected': '0.039903', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079803', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-128.6', 'loss/train': '0.66726', 'examples_per_second': '31.562', 'grad_norm': '44.5', 'counters/examples': 193248, 'counters/updates': 6039}
train stats after 193280 examples: {'rewards_train/chosen': '0.13249', 'rewards_train/rejected': '0.073969', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.058517', 'logps_train/rejected': '-175.66', 'logps_train/chosen': '-148.24', 'loss/train': '0.67867', 'examples_per_second': '30.452', 'grad_norm': '37.75', 'counters/examples': 193280, 'counters/updates': 6040}
train stats after 193312 examples: {'rewards_train/chosen': '0.13195', 'rewards_train/rejected': '0.11956', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.012391', 'logps_train/rejected': '-160.94', 'logps_train/chosen': '-137.23', 'loss/train': '0.69934', 'examples_per_second': '31.366', 'grad_norm': '34.5', 'counters/examples': 193312, 'counters/updates': 6041}
train stats after 193344 examples: {'rewards_train/chosen': '0.19034', 'rewards_train/rejected': '0.047325', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14302', 'logps_train/rejected': '-159.58', 'logps_train/chosen': '-155.36', 'loss/train': '0.65095', 'examples_per_second': '29.794', 'grad_norm': '40.5', 'counters/examples': 193344, 'counters/updates': 6042}
train stats after 193376 examples: {'rewards_train/chosen': '0.17799', 'rewards_train/rejected': '0.064593', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1134', 'logps_train/rejected': '-148.83', 'logps_train/chosen': '-146.34', 'loss/train': '0.64844', 'examples_per_second': '31.454', 'grad_norm': '29.25', 'counters/examples': 193376, 'counters/updates': 6043}
skipping logging after 193408 examples to avoid logging too frequently
train stats after 193440 examples: {'rewards_train/chosen': '0.17042', 'rewards_train/rejected': '0.090771', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07965', 'logps_train/rejected': '-140', 'logps_train/chosen': '-151.45', 'loss/train': '0.66562', 'examples_per_second': '31.175', 'grad_norm': '33.75', 'counters/examples': 193440, 'counters/updates': 6045}
train stats after 193472 examples: {'rewards_train/chosen': '0.11493', 'rewards_train/rejected': '0.045863', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069071', 'logps_train/rejected': '-152.65', 'logps_train/chosen': '-143.69', 'loss/train': '0.67243', 'examples_per_second': '31.317', 'grad_norm': '30.375', 'counters/examples': 193472, 'counters/updates': 6046}
train stats after 193504 examples: {'rewards_train/chosen': '0.20772', 'rewards_train/rejected': '0.044161', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16356', 'logps_train/rejected': '-99.189', 'logps_train/chosen': '-109.51', 'loss/train': '0.62268', 'examples_per_second': '31.529', 'grad_norm': '33', 'counters/examples': 193504, 'counters/updates': 6047}
skipping logging after 193536 examples to avoid logging too frequently
train stats after 193568 examples: {'rewards_train/chosen': '0.17245', 'rewards_train/rejected': '-0.005044', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17749', 'logps_train/rejected': '-124.17', 'logps_train/chosen': '-120.4', 'loss/train': '0.62098', 'examples_per_second': '31.448', 'grad_norm': '24.875', 'counters/examples': 193568, 'counters/updates': 6049}
train stats after 193600 examples: {'rewards_train/chosen': '0.2103', 'rewards_train/rejected': '0.048989', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16131', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-149.47', 'loss/train': '0.63528', 'examples_per_second': '30.518', 'grad_norm': '31.375', 'counters/examples': 193600, 'counters/updates': 6050}
train stats after 193632 examples: {'rewards_train/chosen': '0.26371', 'rewards_train/rejected': '0.061862', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20185', 'logps_train/rejected': '-143.81', 'logps_train/chosen': '-156.43', 'loss/train': '0.62999', 'examples_per_second': '32.585', 'grad_norm': '36', 'counters/examples': 193632, 'counters/updates': 6051}
train stats after 193664 examples: {'rewards_train/chosen': '0.17756', 'rewards_train/rejected': '0.092525', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085032', 'logps_train/rejected': '-99.908', 'logps_train/chosen': '-103.68', 'loss/train': '0.66141', 'examples_per_second': '30.697', 'grad_norm': '29.125', 'counters/examples': 193664, 'counters/updates': 6052}
train stats after 193696 examples: {'rewards_train/chosen': '0.10499', 'rewards_train/rejected': '0.059094', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045898', 'logps_train/rejected': '-104.53', 'logps_train/chosen': '-106.05', 'loss/train': '0.67706', 'examples_per_second': '32.337', 'grad_norm': '32.5', 'counters/examples': 193696, 'counters/updates': 6053}
train stats after 193728 examples: {'rewards_train/chosen': '0.16995', 'rewards_train/rejected': '0.028895', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14106', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-186.27', 'loss/train': '0.64206', 'examples_per_second': '31.377', 'grad_norm': '31.125', 'counters/examples': 193728, 'counters/updates': 6054}
train stats after 193760 examples: {'rewards_train/chosen': '0.19259', 'rewards_train/rejected': '0.028074', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16451', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-138.9', 'loss/train': '0.63202', 'examples_per_second': '31.49', 'grad_norm': '36.25', 'counters/examples': 193760, 'counters/updates': 6055}
train stats after 193792 examples: {'rewards_train/chosen': '0.10423', 'rewards_train/rejected': '0.0076779', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096554', 'logps_train/rejected': '-126.86', 'logps_train/chosen': '-153.34', 'loss/train': '0.65443', 'examples_per_second': '32.276', 'grad_norm': '28.875', 'counters/examples': 193792, 'counters/updates': 6056}
train stats after 193824 examples: {'rewards_train/chosen': '0.1405', 'rewards_train/rejected': '0.024446', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11605', 'logps_train/rejected': '-123.35', 'logps_train/chosen': '-142.32', 'loss/train': '0.64525', 'examples_per_second': '30.176', 'grad_norm': '25.375', 'counters/examples': 193824, 'counters/updates': 6057}
train stats after 193856 examples: {'rewards_train/chosen': '0.33244', 'rewards_train/rejected': '0.17963', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15281', 'logps_train/rejected': '-151.37', 'logps_train/chosen': '-168.98', 'loss/train': '0.65077', 'examples_per_second': '31.478', 'grad_norm': '35', 'counters/examples': 193856, 'counters/updates': 6058}
train stats after 193888 examples: {'rewards_train/chosen': '0.16378', 'rewards_train/rejected': '0.04019', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12359', 'logps_train/rejected': '-102.58', 'logps_train/chosen': '-144.47', 'loss/train': '0.64317', 'examples_per_second': '26.319', 'grad_norm': '30.125', 'counters/examples': 193888, 'counters/updates': 6059}
skipping logging after 193920 examples to avoid logging too frequently
train stats after 193952 examples: {'rewards_train/chosen': '0.21276', 'rewards_train/rejected': '0.077228', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13553', 'logps_train/rejected': '-150.33', 'logps_train/chosen': '-129.46', 'loss/train': '0.64329', 'examples_per_second': '31.45', 'grad_norm': '28.375', 'counters/examples': 193952, 'counters/updates': 6061}
train stats after 193984 examples: {'rewards_train/chosen': '0.089583', 'rewards_train/rejected': '-0.016705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10629', 'logps_train/rejected': '-108.18', 'logps_train/chosen': '-148.49', 'loss/train': '0.65134', 'examples_per_second': '32.352', 'grad_norm': '27', 'counters/examples': 193984, 'counters/updates': 6062}
skipping logging after 194016 examples to avoid logging too frequently
train stats after 194048 examples: {'rewards_train/chosen': '0.19138', 'rewards_train/rejected': '0.07971', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11167', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-156.78', 'loss/train': '0.64701', 'examples_per_second': '34.12', 'grad_norm': '25.5', 'counters/examples': 194048, 'counters/updates': 6064}
train stats after 194080 examples: {'rewards_train/chosen': '0.24759', 'rewards_train/rejected': '0.12228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12531', 'logps_train/rejected': '-127.66', 'logps_train/chosen': '-128.97', 'loss/train': '0.6482', 'examples_per_second': '30.195', 'grad_norm': '26.625', 'counters/examples': 194080, 'counters/updates': 6065}
train stats after 194112 examples: {'rewards_train/chosen': '0.21185', 'rewards_train/rejected': '0.085656', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1262', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-144.17', 'loss/train': '0.64113', 'examples_per_second': '30.617', 'grad_norm': '33.75', 'counters/examples': 194112, 'counters/updates': 6066}
train stats after 194144 examples: {'rewards_train/chosen': '0.24813', 'rewards_train/rejected': '0.010381', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.23774', 'logps_train/rejected': '-96.277', 'logps_train/chosen': '-127.87', 'loss/train': '0.58785', 'examples_per_second': '31.537', 'grad_norm': '20.75', 'counters/examples': 194144, 'counters/updates': 6067}
train stats after 194176 examples: {'rewards_train/chosen': '0.18999', 'rewards_train/rejected': '0.083854', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10614', 'logps_train/rejected': '-135.92', 'logps_train/chosen': '-170.97', 'loss/train': '0.66851', 'examples_per_second': '31.463', 'grad_norm': '35.75', 'counters/examples': 194176, 'counters/updates': 6068}
skipping logging after 194208 examples to avoid logging too frequently
train stats after 194240 examples: {'rewards_train/chosen': '0.028407', 'rewards_train/rejected': '-0.0038635', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03227', 'logps_train/rejected': '-98.831', 'logps_train/chosen': '-132.59', 'loss/train': '0.68831', 'examples_per_second': '30.207', 'grad_norm': '25.875', 'counters/examples': 194240, 'counters/updates': 6070}
train stats after 194272 examples: {'rewards_train/chosen': '0.22623', 'rewards_train/rejected': '0.023008', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20322', 'logps_train/rejected': '-123.28', 'logps_train/chosen': '-164.7', 'loss/train': '0.61808', 'examples_per_second': '32.462', 'grad_norm': '30', 'counters/examples': 194272, 'counters/updates': 6071}
skipping logging after 194304 examples to avoid logging too frequently
train stats after 194336 examples: {'rewards_train/chosen': '0.17518', 'rewards_train/rejected': '0.02966', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14552', 'logps_train/rejected': '-136.48', 'logps_train/chosen': '-140.13', 'loss/train': '0.64084', 'examples_per_second': '29.894', 'grad_norm': '32', 'counters/examples': 194336, 'counters/updates': 6073}
skipping logging after 194368 examples to avoid logging too frequently
train stats after 194400 examples: {'rewards_train/chosen': '0.24442', 'rewards_train/rejected': '0.090808', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15361', 'logps_train/rejected': '-154.31', 'logps_train/chosen': '-176.22', 'loss/train': '0.64163', 'examples_per_second': '29.957', 'grad_norm': '29.75', 'counters/examples': 194400, 'counters/updates': 6075}
train stats after 194432 examples: {'rewards_train/chosen': '0.18143', 'rewards_train/rejected': '0.11101', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.070414', 'logps_train/rejected': '-131.56', 'logps_train/chosen': '-160.79', 'loss/train': '0.67556', 'examples_per_second': '31.311', 'grad_norm': '44.5', 'counters/examples': 194432, 'counters/updates': 6076}
skipping logging after 194464 examples to avoid logging too frequently
train stats after 194496 examples: {'rewards_train/chosen': '0.24548', 'rewards_train/rejected': '0.06735', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17813', 'logps_train/rejected': '-91.507', 'logps_train/chosen': '-113.52', 'loss/train': '0.62314', 'examples_per_second': '32.936', 'grad_norm': '52', 'counters/examples': 194496, 'counters/updates': 6078}
train stats after 194528 examples: {'rewards_train/chosen': '0.11746', 'rewards_train/rejected': '0.10415', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.013306', 'logps_train/rejected': '-139.34', 'logps_train/chosen': '-138.42', 'loss/train': '0.69585', 'examples_per_second': '31.942', 'grad_norm': '31.875', 'counters/examples': 194528, 'counters/updates': 6079}
train stats after 194560 examples: {'rewards_train/chosen': '0.1791', 'rewards_train/rejected': '0.075698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1034', 'logps_train/rejected': '-140.52', 'logps_train/chosen': '-163.31', 'loss/train': '0.6602', 'examples_per_second': '31.12', 'grad_norm': '36.25', 'counters/examples': 194560, 'counters/updates': 6080}
train stats after 194592 examples: {'rewards_train/chosen': '0.15365', 'rewards_train/rejected': '0.051654', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.102', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-161.02', 'loss/train': '0.65546', 'examples_per_second': '30.53', 'grad_norm': '38.75', 'counters/examples': 194592, 'counters/updates': 6081}
train stats after 194624 examples: {'rewards_train/chosen': '0.10187', 'rewards_train/rejected': '0.032474', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069396', 'logps_train/rejected': '-114.98', 'logps_train/chosen': '-132.15', 'loss/train': '0.67023', 'examples_per_second': '30.988', 'grad_norm': '26.625', 'counters/examples': 194624, 'counters/updates': 6082}
train stats after 194656 examples: {'rewards_train/chosen': '0.1491', 'rewards_train/rejected': '0.023758', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12534', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-110.65', 'loss/train': '0.64119', 'examples_per_second': '30.571', 'grad_norm': '28.125', 'counters/examples': 194656, 'counters/updates': 6083}
train stats after 194688 examples: {'rewards_train/chosen': '0.24342', 'rewards_train/rejected': '0.11718', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12624', 'logps_train/rejected': '-140.57', 'logps_train/chosen': '-179.22', 'loss/train': '0.65106', 'examples_per_second': '30.119', 'grad_norm': '30.25', 'counters/examples': 194688, 'counters/updates': 6084}
train stats after 194720 examples: {'rewards_train/chosen': '0.30878', 'rewards_train/rejected': '0.11908', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1897', 'logps_train/rejected': '-137.42', 'logps_train/chosen': '-151.93', 'loss/train': '0.6285', 'examples_per_second': '31.177', 'grad_norm': '42.25', 'counters/examples': 194720, 'counters/updates': 6085}
train stats after 194752 examples: {'rewards_train/chosen': '0.25723', 'rewards_train/rejected': '0.1443', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11292', 'logps_train/rejected': '-154.8', 'logps_train/chosen': '-158.28', 'loss/train': '0.64895', 'examples_per_second': '30.184', 'grad_norm': '41', 'counters/examples': 194752, 'counters/updates': 6086}
train stats after 194784 examples: {'rewards_train/chosen': '0.1162', 'rewards_train/rejected': '0.079242', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.036961', 'logps_train/rejected': '-142.69', 'logps_train/chosen': '-120.29', 'loss/train': '0.68568', 'examples_per_second': '31.174', 'grad_norm': '31.375', 'counters/examples': 194784, 'counters/updates': 6087}
train stats after 194816 examples: {'rewards_train/chosen': '0.16605', 'rewards_train/rejected': '0.02912', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13693', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-163.55', 'loss/train': '0.6412', 'examples_per_second': '31.451', 'grad_norm': '27.625', 'counters/examples': 194816, 'counters/updates': 6088}
train stats after 194848 examples: {'rewards_train/chosen': '0.22397', 'rewards_train/rejected': '0.10322', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12074', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-187.26', 'loss/train': '0.68896', 'examples_per_second': '31.487', 'grad_norm': '51.75', 'counters/examples': 194848, 'counters/updates': 6089}
train stats after 194880 examples: {'rewards_train/chosen': '0.21431', 'rewards_train/rejected': '0.06386', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15045', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-167.37', 'loss/train': '0.62819', 'examples_per_second': '30.121', 'grad_norm': '33.25', 'counters/examples': 194880, 'counters/updates': 6090}
train stats after 194912 examples: {'rewards_train/chosen': '0.10495', 'rewards_train/rejected': '0.067339', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037607', 'logps_train/rejected': '-137.52', 'logps_train/chosen': '-151.72', 'loss/train': '0.68129', 'examples_per_second': '31.337', 'grad_norm': '29.875', 'counters/examples': 194912, 'counters/updates': 6091}
skipping logging after 194944 examples to avoid logging too frequently
train stats after 194976 examples: {'rewards_train/chosen': '0.12256', 'rewards_train/rejected': '0.020155', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1024', 'logps_train/rejected': '-97.574', 'logps_train/chosen': '-136.96', 'loss/train': '0.65895', 'examples_per_second': '35.426', 'grad_norm': '31.125', 'counters/examples': 194976, 'counters/updates': 6093}
train stats after 195008 examples: {'rewards_train/chosen': '0.077764', 'rewards_train/rejected': '0.03034', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.047424', 'logps_train/rejected': '-107', 'logps_train/chosen': '-129.76', 'loss/train': '0.68599', 'examples_per_second': '30.429', 'grad_norm': '28.625', 'counters/examples': 195008, 'counters/updates': 6094}
skipping logging after 195040 examples to avoid logging too frequently
train stats after 195072 examples: {'rewards_train/chosen': '0.23542', 'rewards_train/rejected': '0.20767', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.027749', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-154.45', 'loss/train': '0.70217', 'examples_per_second': '32.13', 'grad_norm': '47.75', 'counters/examples': 195072, 'counters/updates': 6096}
train stats after 195104 examples: {'rewards_train/chosen': '0.25757', 'rewards_train/rejected': '0.14424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11333', 'logps_train/rejected': '-138.73', 'logps_train/chosen': '-175.79', 'loss/train': '0.65596', 'examples_per_second': '31.42', 'grad_norm': '31.75', 'counters/examples': 195104, 'counters/updates': 6097}
train stats after 195136 examples: {'rewards_train/chosen': '0.21376', 'rewards_train/rejected': '0.15795', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.055812', 'logps_train/rejected': '-116.31', 'logps_train/chosen': '-125.61', 'loss/train': '0.67869', 'examples_per_second': '30.709', 'grad_norm': '32.25', 'counters/examples': 195136, 'counters/updates': 6098}
skipping logging after 195168 examples to avoid logging too frequently
train stats after 195200 examples: {'rewards_train/chosen': '0.20703', 'rewards_train/rejected': '0.030599', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.17643', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-154.33', 'loss/train': '0.62769', 'examples_per_second': '31.408', 'grad_norm': '29.625', 'counters/examples': 195200, 'counters/updates': 6100}
train stats after 195232 examples: {'rewards_train/chosen': '0.14329', 'rewards_train/rejected': '0.102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.041296', 'logps_train/rejected': '-136.41', 'logps_train/chosen': '-121.56', 'loss/train': '0.6873', 'examples_per_second': '31.383', 'grad_norm': '55', 'counters/examples': 195232, 'counters/updates': 6101}
train stats after 195264 examples: {'rewards_train/chosen': '0.18555', 'rewards_train/rejected': '0.0719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11365', 'logps_train/rejected': '-140.91', 'logps_train/chosen': '-151.88', 'loss/train': '0.64937', 'examples_per_second': '30.64', 'grad_norm': '33', 'counters/examples': 195264, 'counters/updates': 6102}
train stats after 195296 examples: {'rewards_train/chosen': '0.057179', 'rewards_train/rejected': '0.04945', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0077288', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-132.51', 'loss/train': '0.6968', 'examples_per_second': '30.47', 'grad_norm': '39.5', 'counters/examples': 195296, 'counters/updates': 6103}
train stats after 195328 examples: {'rewards_train/chosen': '0.16111', 'rewards_train/rejected': '0.11551', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.045596', 'logps_train/rejected': '-108.16', 'logps_train/chosen': '-117.88', 'loss/train': '0.68789', 'examples_per_second': '30.092', 'grad_norm': '29.875', 'counters/examples': 195328, 'counters/updates': 6104}
train stats after 195360 examples: {'rewards_train/chosen': '0.12767', 'rewards_train/rejected': '0.051404', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.076268', 'logps_train/rejected': '-122.8', 'logps_train/chosen': '-142.02', 'loss/train': '0.66264', 'examples_per_second': '32.304', 'grad_norm': '33.5', 'counters/examples': 195360, 'counters/updates': 6105}
train stats after 195392 examples: {'rewards_train/chosen': '0.18014', 'rewards_train/rejected': '0.19804', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.017895', 'logps_train/rejected': '-114.95', 'logps_train/chosen': '-117.35', 'loss/train': '0.73157', 'examples_per_second': '31.262', 'grad_norm': '60.25', 'counters/examples': 195392, 'counters/updates': 6106}
train stats after 195424 examples: {'rewards_train/chosen': '0.14586', 'rewards_train/rejected': '0.093698', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052166', 'logps_train/rejected': '-101.17', 'logps_train/chosen': '-132.63', 'loss/train': '0.67921', 'examples_per_second': '31.526', 'grad_norm': '29.5', 'counters/examples': 195424, 'counters/updates': 6107}
train stats after 195456 examples: {'rewards_train/chosen': '0.16631', 'rewards_train/rejected': '0.020639', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14567', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-162.34', 'loss/train': '0.63169', 'examples_per_second': '30.291', 'grad_norm': '24.875', 'counters/examples': 195456, 'counters/updates': 6108}
train stats after 195488 examples: {'rewards_train/chosen': '0.1338', 'rewards_train/rejected': '0.078866', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054934', 'logps_train/rejected': '-153.5', 'logps_train/chosen': '-186.07', 'loss/train': '0.67773', 'examples_per_second': '29.742', 'grad_norm': '33.25', 'counters/examples': 195488, 'counters/updates': 6109}
train stats after 195520 examples: {'rewards_train/chosen': '0.21531', 'rewards_train/rejected': '0.16153', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053779', 'logps_train/rejected': '-165.64', 'logps_train/chosen': '-152.86', 'loss/train': '0.68334', 'examples_per_second': '31.447', 'grad_norm': '79', 'counters/examples': 195520, 'counters/updates': 6110}
train stats after 195552 examples: {'rewards_train/chosen': '0.11702', 'rewards_train/rejected': '0.039646', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077377', 'logps_train/rejected': '-120.66', 'logps_train/chosen': '-153.7', 'loss/train': '0.66578', 'examples_per_second': '31.026', 'grad_norm': '26.875', 'counters/examples': 195552, 'counters/updates': 6111}
train stats after 195584 examples: {'rewards_train/chosen': '0.15578', 'rewards_train/rejected': '0.11164', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.044141', 'logps_train/rejected': '-162.06', 'logps_train/chosen': '-165.68', 'loss/train': '0.68759', 'examples_per_second': '32.568', 'grad_norm': '39.25', 'counters/examples': 195584, 'counters/updates': 6112}
train stats after 195616 examples: {'rewards_train/chosen': '0.087261', 'rewards_train/rejected': '0.03857', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.048691', 'logps_train/rejected': '-141.56', 'logps_train/chosen': '-150.98', 'loss/train': '0.68014', 'examples_per_second': '32.742', 'grad_norm': '29.875', 'counters/examples': 195616, 'counters/updates': 6113}
train stats after 195648 examples: {'rewards_train/chosen': '0.20081', 'rewards_train/rejected': '0.12817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072642', 'logps_train/rejected': '-111.39', 'logps_train/chosen': '-145.67', 'loss/train': '0.66584', 'examples_per_second': '30.801', 'grad_norm': '27.25', 'counters/examples': 195648, 'counters/updates': 6114}
train stats after 195680 examples: {'rewards_train/chosen': '0.19964', 'rewards_train/rejected': '0.17584', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.023798', 'logps_train/rejected': '-155.17', 'logps_train/chosen': '-148.27', 'loss/train': '0.69312', 'examples_per_second': '31.449', 'grad_norm': '30.375', 'counters/examples': 195680, 'counters/updates': 6115}
train stats after 195712 examples: {'rewards_train/chosen': '0.26048', 'rewards_train/rejected': '0.19837', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062104', 'logps_train/rejected': '-139.13', 'logps_train/chosen': '-177.39', 'loss/train': '0.68884', 'examples_per_second': '29.98', 'grad_norm': '38.25', 'counters/examples': 195712, 'counters/updates': 6116}
train stats after 195744 examples: {'rewards_train/chosen': '0.11238', 'rewards_train/rejected': '0.025113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.087268', 'logps_train/rejected': '-101.84', 'logps_train/chosen': '-134.1', 'loss/train': '0.65842', 'examples_per_second': '30.463', 'grad_norm': '29.875', 'counters/examples': 195744, 'counters/updates': 6117}
train stats after 195776 examples: {'rewards_train/chosen': '0.16425', 'rewards_train/rejected': '0.028759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13549', 'logps_train/rejected': '-147.41', 'logps_train/chosen': '-143.56', 'loss/train': '0.64462', 'examples_per_second': '30.307', 'grad_norm': '38.75', 'counters/examples': 195776, 'counters/updates': 6118}
train stats after 195808 examples: {'rewards_train/chosen': '0.17879', 'rewards_train/rejected': '0.19112', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.01233', 'logps_train/rejected': '-152.23', 'logps_train/chosen': '-139.87', 'loss/train': '0.71768', 'examples_per_second': '30.104', 'grad_norm': '44', 'counters/examples': 195808, 'counters/updates': 6119}
train stats after 195840 examples: {'rewards_train/chosen': '0.12697', 'rewards_train/rejected': '0.063914', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063054', 'logps_train/rejected': '-96.333', 'logps_train/chosen': '-112.13', 'loss/train': '0.66621', 'examples_per_second': '31.161', 'grad_norm': '23.5', 'counters/examples': 195840, 'counters/updates': 6120}
train stats after 195872 examples: {'rewards_train/chosen': '0.22491', 'rewards_train/rejected': '0.11002', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11489', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-174.01', 'loss/train': '0.64749', 'examples_per_second': '31.475', 'grad_norm': '36', 'counters/examples': 195872, 'counters/updates': 6121}
train stats after 195904 examples: {'rewards_train/chosen': '0.1558', 'rewards_train/rejected': '0.051372', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10442', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-153.96', 'loss/train': '0.65856', 'examples_per_second': '31.451', 'grad_norm': '26.375', 'counters/examples': 195904, 'counters/updates': 6122}
train stats after 195936 examples: {'rewards_train/chosen': '0.080182', 'rewards_train/rejected': '0.011115', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069067', 'logps_train/rejected': '-132.76', 'logps_train/chosen': '-147.9', 'loss/train': '0.67438', 'examples_per_second': '31.294', 'grad_norm': '32.25', 'counters/examples': 195936, 'counters/updates': 6123}
skipping logging after 195968 examples to avoid logging too frequently
train stats after 196000 examples: {'rewards_train/chosen': '0.17399', 'rewards_train/rejected': '0.026088', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1479', 'logps_train/rejected': '-97.616', 'logps_train/chosen': '-94.851', 'loss/train': '0.63606', 'examples_per_second': '34.168', 'grad_norm': '32.75', 'counters/examples': 196000, 'counters/updates': 6125}
skipping logging after 196032 examples to avoid logging too frequently
train stats after 196064 examples: {'rewards_train/chosen': '0.16236', 'rewards_train/rejected': '0.11741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044955', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-126.81', 'loss/train': '0.68198', 'examples_per_second': '31.226', 'grad_norm': '31', 'counters/examples': 196064, 'counters/updates': 6127}
train stats after 196096 examples: {'rewards_train/chosen': '0.093223', 'rewards_train/rejected': '0.028633', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06459', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-143.67', 'loss/train': '0.67045', 'examples_per_second': '30.966', 'grad_norm': '28.75', 'counters/examples': 196096, 'counters/updates': 6128}
skipping logging after 196128 examples to avoid logging too frequently
train stats after 196160 examples: {'rewards_train/chosen': '0.3484', 'rewards_train/rejected': '0.015902', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.3325', 'logps_train/rejected': '-134.02', 'logps_train/chosen': '-172.44', 'loss/train': '0.56976', 'examples_per_second': '33.397', 'grad_norm': '25.75', 'counters/examples': 196160, 'counters/updates': 6130}
train stats after 196192 examples: {'rewards_train/chosen': '0.14041', 'rewards_train/rejected': '0.017296', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12311', 'logps_train/rejected': '-149.36', 'logps_train/chosen': '-165.04', 'loss/train': '0.64344', 'examples_per_second': '31.266', 'grad_norm': '52', 'counters/examples': 196192, 'counters/updates': 6131}
train stats after 196224 examples: {'rewards_train/chosen': '0.097081', 'rewards_train/rejected': '0.065318', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031763', 'logps_train/rejected': '-151.38', 'logps_train/chosen': '-133.04', 'loss/train': '0.68948', 'examples_per_second': '32.281', 'grad_norm': '30.25', 'counters/examples': 196224, 'counters/updates': 6132}
train stats after 196256 examples: {'rewards_train/chosen': '0.16779', 'rewards_train/rejected': '0.045801', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12199', 'logps_train/rejected': '-132.48', 'logps_train/chosen': '-141.36', 'loss/train': '0.64638', 'examples_per_second': '32.726', 'grad_norm': '25', 'counters/examples': 196256, 'counters/updates': 6133}
train stats after 196288 examples: {'rewards_train/chosen': '0.23478', 'rewards_train/rejected': '0.16968', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065098', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-148.08', 'loss/train': '0.69626', 'examples_per_second': '30.141', 'grad_norm': '53.75', 'counters/examples': 196288, 'counters/updates': 6134}
train stats after 196320 examples: {'rewards_train/chosen': '0.13621', 'rewards_train/rejected': '0.094096', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042112', 'logps_train/rejected': '-106.11', 'logps_train/chosen': '-99.563', 'loss/train': '0.68532', 'examples_per_second': '32.183', 'grad_norm': '26.25', 'counters/examples': 196320, 'counters/updates': 6135}
train stats after 196352 examples: {'rewards_train/chosen': '0.10417', 'rewards_train/rejected': '-0.011975', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11615', 'logps_train/rejected': '-75.798', 'logps_train/chosen': '-132.28', 'loss/train': '0.64442', 'examples_per_second': '32.424', 'grad_norm': '24.875', 'counters/examples': 196352, 'counters/updates': 6136}
skipping logging after 196384 examples to avoid logging too frequently
train stats after 196416 examples: {'rewards_train/chosen': '0.20016', 'rewards_train/rejected': '0.11111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089047', 'logps_train/rejected': '-141.62', 'logps_train/chosen': '-123.45', 'loss/train': '0.65962', 'examples_per_second': '31.23', 'grad_norm': '26.75', 'counters/examples': 196416, 'counters/updates': 6138}
train stats after 196448 examples: {'rewards_train/chosen': '0.16434', 'rewards_train/rejected': '0.17279', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0084417', 'logps_train/rejected': '-160.35', 'logps_train/chosen': '-151.46', 'loss/train': '0.70858', 'examples_per_second': '30.799', 'grad_norm': '36.5', 'counters/examples': 196448, 'counters/updates': 6139}
train stats after 196480 examples: {'rewards_train/chosen': '0.14504', 'rewards_train/rejected': '0.15443', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0093897', 'logps_train/rejected': '-135.76', 'logps_train/chosen': '-125.97', 'loss/train': '0.71442', 'examples_per_second': '30.672', 'grad_norm': '35.25', 'counters/examples': 196480, 'counters/updates': 6140}
train stats after 196512 examples: {'rewards_train/chosen': '0.17404', 'rewards_train/rejected': '0.074139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099897', 'logps_train/rejected': '-160.51', 'logps_train/chosen': '-140.61', 'loss/train': '0.66197', 'examples_per_second': '32.643', 'grad_norm': '36.25', 'counters/examples': 196512, 'counters/updates': 6141}
train stats after 196544 examples: {'rewards_train/chosen': '0.16058', 'rewards_train/rejected': '0.042891', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11769', 'logps_train/rejected': '-161.58', 'logps_train/chosen': '-132.46', 'loss/train': '0.65018', 'examples_per_second': '31.489', 'grad_norm': '31', 'counters/examples': 196544, 'counters/updates': 6142}
train stats after 196576 examples: {'rewards_train/chosen': '0.11925', 'rewards_train/rejected': '0.087342', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031908', 'logps_train/rejected': '-144.82', 'logps_train/chosen': '-155.31', 'loss/train': '0.68827', 'examples_per_second': '31.434', 'grad_norm': '44', 'counters/examples': 196576, 'counters/updates': 6143}
train stats after 196608 examples: {'rewards_train/chosen': '0.21884', 'rewards_train/rejected': '0.10742', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11141', 'logps_train/rejected': '-142.05', 'logps_train/chosen': '-159.57', 'loss/train': '0.65372', 'examples_per_second': '31.474', 'grad_norm': '35.75', 'counters/examples': 196608, 'counters/updates': 6144}
train stats after 196640 examples: {'rewards_train/chosen': '0.18189', 'rewards_train/rejected': '0.15161', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030281', 'logps_train/rejected': '-151.22', 'logps_train/chosen': '-165.67', 'loss/train': '0.70503', 'examples_per_second': '31.276', 'grad_norm': '39', 'counters/examples': 196640, 'counters/updates': 6145}
train stats after 196672 examples: {'rewards_train/chosen': '0.19933', 'rewards_train/rejected': '0.070911', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12842', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-161.3', 'loss/train': '0.64589', 'examples_per_second': '31.579', 'grad_norm': '29.875', 'counters/examples': 196672, 'counters/updates': 6146}
train stats after 196704 examples: {'rewards_train/chosen': '0.21946', 'rewards_train/rejected': '0.11344', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10602', 'logps_train/rejected': '-121.71', 'logps_train/chosen': '-131.89', 'loss/train': '0.6673', 'examples_per_second': '32.531', 'grad_norm': '27.875', 'counters/examples': 196704, 'counters/updates': 6147}
train stats after 196736 examples: {'rewards_train/chosen': '0.21991', 'rewards_train/rejected': '0.19992', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.019998', 'logps_train/rejected': '-159.37', 'logps_train/chosen': '-167.67', 'loss/train': '0.69534', 'examples_per_second': '23.601', 'grad_norm': '47.5', 'counters/examples': 196736, 'counters/updates': 6148}
train stats after 196768 examples: {'rewards_train/chosen': '0.17669', 'rewards_train/rejected': '0.07344', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10325', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-140.91', 'loss/train': '0.66917', 'examples_per_second': '29.953', 'grad_norm': '34.75', 'counters/examples': 196768, 'counters/updates': 6149}
skipping logging after 196800 examples to avoid logging too frequently
train stats after 196832 examples: {'rewards_train/chosen': '0.17424', 'rewards_train/rejected': '0.20664', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.032399', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-137.35', 'loss/train': '0.72967', 'examples_per_second': '24.151', 'grad_norm': '40.5', 'counters/examples': 196832, 'counters/updates': 6151}
train stats after 196864 examples: {'rewards_train/chosen': '0.13986', 'rewards_train/rejected': '0.060315', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.079547', 'logps_train/rejected': '-144.81', 'logps_train/chosen': '-159.81', 'loss/train': '0.68191', 'examples_per_second': '30.392', 'grad_norm': '39.75', 'counters/examples': 196864, 'counters/updates': 6152}
skipping logging after 196896 examples to avoid logging too frequently
train stats after 196928 examples: {'rewards_train/chosen': '0.034264', 'rewards_train/rejected': '0.030745', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0035189', 'logps_train/rejected': '-118.96', 'logps_train/chosen': '-137.9', 'loss/train': '0.70415', 'examples_per_second': '31.494', 'grad_norm': '46.5', 'counters/examples': 196928, 'counters/updates': 6154}
train stats after 196960 examples: {'rewards_train/chosen': '0.26216', 'rewards_train/rejected': '-0.0055812', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26774', 'logps_train/rejected': '-129.79', 'logps_train/chosen': '-168.93', 'loss/train': '0.60275', 'examples_per_second': '31.301', 'grad_norm': '27.875', 'counters/examples': 196960, 'counters/updates': 6155}
train stats after 196992 examples: {'rewards_train/chosen': '0.2397', 'rewards_train/rejected': '0.073317', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16638', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-141.59', 'loss/train': '0.62641', 'examples_per_second': '31.88', 'grad_norm': '33.75', 'counters/examples': 196992, 'counters/updates': 6156}
train stats after 197024 examples: {'rewards_train/chosen': '0.14351', 'rewards_train/rejected': '0.14371', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.00020294', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-160.24', 'loss/train': '0.71503', 'examples_per_second': '30.295', 'grad_norm': '38.75', 'counters/examples': 197024, 'counters/updates': 6157}
train stats after 197056 examples: {'rewards_train/chosen': '0.31382', 'rewards_train/rejected': '0.059282', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25453', 'logps_train/rejected': '-120.01', 'logps_train/chosen': '-159.02', 'loss/train': '0.58896', 'examples_per_second': '30.509', 'grad_norm': '27', 'counters/examples': 197056, 'counters/updates': 6158}
train stats after 197088 examples: {'rewards_train/chosen': '0.12928', 'rewards_train/rejected': '-0.0023973', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13168', 'logps_train/rejected': '-77.873', 'logps_train/chosen': '-142.73', 'loss/train': '0.63911', 'examples_per_second': '31.512', 'grad_norm': '24.125', 'counters/examples': 197088, 'counters/updates': 6159}
train stats after 197120 examples: {'rewards_train/chosen': '0.10625', 'rewards_train/rejected': '0.099267', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0069822', 'logps_train/rejected': '-165.32', 'logps_train/chosen': '-127.03', 'loss/train': '0.70312', 'examples_per_second': '31.158', 'grad_norm': '34.5', 'counters/examples': 197120, 'counters/updates': 6160}
train stats after 197152 examples: {'rewards_train/chosen': '0.16407', 'rewards_train/rejected': '0.089604', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07447', 'logps_train/rejected': '-97.83', 'logps_train/chosen': '-106.99', 'loss/train': '0.66573', 'examples_per_second': '31.091', 'grad_norm': '25.5', 'counters/examples': 197152, 'counters/updates': 6161}
train stats after 197184 examples: {'rewards_train/chosen': '0.094376', 'rewards_train/rejected': '0.0017872', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.092589', 'logps_train/rejected': '-116.19', 'logps_train/chosen': '-106.07', 'loss/train': '0.65304', 'examples_per_second': '30.217', 'grad_norm': '23', 'counters/examples': 197184, 'counters/updates': 6162}
train stats after 197216 examples: {'rewards_train/chosen': '0.1851', 'rewards_train/rejected': '0.061719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12339', 'logps_train/rejected': '-132.47', 'logps_train/chosen': '-142.89', 'loss/train': '0.64162', 'examples_per_second': '31.372', 'grad_norm': '30.625', 'counters/examples': 197216, 'counters/updates': 6163}
train stats after 197248 examples: {'rewards_train/chosen': '0.18518', 'rewards_train/rejected': '0.15244', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.032741', 'logps_train/rejected': '-118.66', 'logps_train/chosen': '-169.45', 'loss/train': '0.69158', 'examples_per_second': '29.861', 'grad_norm': '43', 'counters/examples': 197248, 'counters/updates': 6164}
train stats after 197280 examples: {'rewards_train/chosen': '0.17703', 'rewards_train/rejected': '0.098532', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078502', 'logps_train/rejected': '-110.68', 'logps_train/chosen': '-146.04', 'loss/train': '0.66751', 'examples_per_second': '30.964', 'grad_norm': '31.75', 'counters/examples': 197280, 'counters/updates': 6165}
train stats after 197312 examples: {'rewards_train/chosen': '0.12011', 'rewards_train/rejected': '0.082222', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.037886', 'logps_train/rejected': '-120.02', 'logps_train/chosen': '-151.47', 'loss/train': '0.68168', 'examples_per_second': '29.937', 'grad_norm': '35.5', 'counters/examples': 197312, 'counters/updates': 6166}
train stats after 197344 examples: {'rewards_train/chosen': '0.1067', 'rewards_train/rejected': '0.02681', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079893', 'logps_train/rejected': '-111.15', 'logps_train/chosen': '-133.62', 'loss/train': '0.66405', 'examples_per_second': '31.457', 'grad_norm': '25.5', 'counters/examples': 197344, 'counters/updates': 6167}
train stats after 197376 examples: {'rewards_train/chosen': '0.1614', 'rewards_train/rejected': '0.092656', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068741', 'logps_train/rejected': '-132.93', 'logps_train/chosen': '-140.61', 'loss/train': '0.67282', 'examples_per_second': '31.471', 'grad_norm': '28.625', 'counters/examples': 197376, 'counters/updates': 6168}
train stats after 197408 examples: {'rewards_train/chosen': '0.12398', 'rewards_train/rejected': '0.067399', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056582', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-138.02', 'loss/train': '0.67759', 'examples_per_second': '29.983', 'grad_norm': '25.25', 'counters/examples': 197408, 'counters/updates': 6169}
train stats after 197440 examples: {'rewards_train/chosen': '0.19587', 'rewards_train/rejected': '0.10202', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093854', 'logps_train/rejected': '-127.73', 'logps_train/chosen': '-132.53', 'loss/train': '0.66274', 'examples_per_second': '31.168', 'grad_norm': '26.625', 'counters/examples': 197440, 'counters/updates': 6170}
train stats after 197472 examples: {'rewards_train/chosen': '0.18759', 'rewards_train/rejected': '0.033668', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15392', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-133.43', 'loss/train': '0.62743', 'examples_per_second': '33.076', 'grad_norm': '37', 'counters/examples': 197472, 'counters/updates': 6171}
skipping logging after 197504 examples to avoid logging too frequently
train stats after 197536 examples: {'rewards_train/chosen': '0.080478', 'rewards_train/rejected': '-0.018319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098797', 'logps_train/rejected': '-98.146', 'logps_train/chosen': '-124.87', 'loss/train': '0.6552', 'examples_per_second': '37.74', 'grad_norm': '24.25', 'counters/examples': 197536, 'counters/updates': 6173}
train stats after 197568 examples: {'rewards_train/chosen': '0.16755', 'rewards_train/rejected': '-0.011342', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17889', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-166.15', 'loss/train': '0.62126', 'examples_per_second': '31.573', 'grad_norm': '34.25', 'counters/examples': 197568, 'counters/updates': 6174}
train stats after 197600 examples: {'rewards_train/chosen': '0.13017', 'rewards_train/rejected': '0.16007', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029896', 'logps_train/rejected': '-128.98', 'logps_train/chosen': '-151.61', 'loss/train': '0.72017', 'examples_per_second': '32.314', 'grad_norm': '35', 'counters/examples': 197600, 'counters/updates': 6175}
train stats after 197632 examples: {'rewards_train/chosen': '0.16742', 'rewards_train/rejected': '-0.023105', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19053', 'logps_train/rejected': '-150.59', 'logps_train/chosen': '-142.49', 'loss/train': '0.62306', 'examples_per_second': '29.909', 'grad_norm': '28.375', 'counters/examples': 197632, 'counters/updates': 6176}
train stats after 197664 examples: {'rewards_train/chosen': '0.048795', 'rewards_train/rejected': '0.052538', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.003743', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-131.79', 'loss/train': '0.71466', 'examples_per_second': '30.593', 'grad_norm': '38', 'counters/examples': 197664, 'counters/updates': 6177}
train stats after 197696 examples: {'rewards_train/chosen': '0.20907', 'rewards_train/rejected': '0.071522', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13754', 'logps_train/rejected': '-128', 'logps_train/chosen': '-139.82', 'loss/train': '0.63801', 'examples_per_second': '31.377', 'grad_norm': '27', 'counters/examples': 197696, 'counters/updates': 6178}
train stats after 197728 examples: {'rewards_train/chosen': '0.16406', 'rewards_train/rejected': '0.080066', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083996', 'logps_train/rejected': '-149.92', 'logps_train/chosen': '-124.35', 'loss/train': '0.67292', 'examples_per_second': '31.314', 'grad_norm': '33.75', 'counters/examples': 197728, 'counters/updates': 6179}
train stats after 197760 examples: {'rewards_train/chosen': '0.11344', 'rewards_train/rejected': '0.064358', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049077', 'logps_train/rejected': '-139.29', 'logps_train/chosen': '-147.9', 'loss/train': '0.67875', 'examples_per_second': '31.422', 'grad_norm': '34', 'counters/examples': 197760, 'counters/updates': 6180}
train stats after 197792 examples: {'rewards_train/chosen': '0.14822', 'rewards_train/rejected': '-0.014469', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16269', 'logps_train/rejected': '-145.86', 'logps_train/chosen': '-132.76', 'loss/train': '0.62076', 'examples_per_second': '31.487', 'grad_norm': '30.375', 'counters/examples': 197792, 'counters/updates': 6181}
train stats after 197824 examples: {'rewards_train/chosen': '0.17971', 'rewards_train/rejected': '0.14788', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.031832', 'logps_train/rejected': '-124.95', 'logps_train/chosen': '-116.45', 'loss/train': '0.68266', 'examples_per_second': '30.969', 'grad_norm': '38', 'counters/examples': 197824, 'counters/updates': 6182}
train stats after 197856 examples: {'rewards_train/chosen': '0.12956', 'rewards_train/rejected': '0.065978', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.063578', 'logps_train/rejected': '-152.54', 'logps_train/chosen': '-156.01', 'loss/train': '0.67842', 'examples_per_second': '32.107', 'grad_norm': '55', 'counters/examples': 197856, 'counters/updates': 6183}
train stats after 197888 examples: {'rewards_train/chosen': '0.23265', 'rewards_train/rejected': '0.1928', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039851', 'logps_train/rejected': '-156.59', 'logps_train/chosen': '-152.72', 'loss/train': '0.701', 'examples_per_second': '30.739', 'grad_norm': '35.25', 'counters/examples': 197888, 'counters/updates': 6184}
skipping logging after 197920 examples to avoid logging too frequently
train stats after 197952 examples: {'rewards_train/chosen': '0.21578', 'rewards_train/rejected': '0.076122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13966', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-168.96', 'loss/train': '0.63779', 'examples_per_second': '33.461', 'grad_norm': '29.625', 'counters/examples': 197952, 'counters/updates': 6186}
train stats after 197984 examples: {'rewards_train/chosen': '0.19136', 'rewards_train/rejected': '0.0070012', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18435', 'logps_train/rejected': '-100.29', 'logps_train/chosen': '-125.8', 'loss/train': '0.61451', 'examples_per_second': '29.951', 'grad_norm': '26.75', 'counters/examples': 197984, 'counters/updates': 6187}
train stats after 198016 examples: {'rewards_train/chosen': '0.17453', 'rewards_train/rejected': '-0.086033', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26056', 'logps_train/rejected': '-112.84', 'logps_train/chosen': '-182.91', 'loss/train': '0.58963', 'examples_per_second': '31.53', 'grad_norm': '40', 'counters/examples': 198016, 'counters/updates': 6188}
skipping logging after 198048 examples to avoid logging too frequently
train stats after 198080 examples: {'rewards_train/chosen': '0.063619', 'rewards_train/rejected': '0.05419', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0094286', 'logps_train/rejected': '-138.61', 'logps_train/chosen': '-129.34', 'loss/train': '0.69727', 'examples_per_second': '31.044', 'grad_norm': '55.5', 'counters/examples': 198080, 'counters/updates': 6190}
train stats after 198112 examples: {'rewards_train/chosen': '0.14164', 'rewards_train/rejected': '0.068591', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073051', 'logps_train/rejected': '-140.75', 'logps_train/chosen': '-161.45', 'loss/train': '0.67472', 'examples_per_second': '31.442', 'grad_norm': '58.25', 'counters/examples': 198112, 'counters/updates': 6191}
train stats after 198144 examples: {'rewards_train/chosen': '0.21677', 'rewards_train/rejected': '0.064067', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15271', 'logps_train/rejected': '-93.985', 'logps_train/chosen': '-134.21', 'loss/train': '0.63506', 'examples_per_second': '31.301', 'grad_norm': '24', 'counters/examples': 198144, 'counters/updates': 6192}
train stats after 198176 examples: {'rewards_train/chosen': '0.24648', 'rewards_train/rejected': '0.081673', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16481', 'logps_train/rejected': '-118.33', 'logps_train/chosen': '-182.71', 'loss/train': '0.63284', 'examples_per_second': '31.392', 'grad_norm': '29.125', 'counters/examples': 198176, 'counters/updates': 6193}
train stats after 198208 examples: {'rewards_train/chosen': '0.10028', 'rewards_train/rejected': '0.020305', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079977', 'logps_train/rejected': '-94.296', 'logps_train/chosen': '-165.38', 'loss/train': '0.66161', 'examples_per_second': '32.201', 'grad_norm': '30.75', 'counters/examples': 198208, 'counters/updates': 6194}
train stats after 198240 examples: {'rewards_train/chosen': '0.13359', 'rewards_train/rejected': '0.0011327', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13246', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-138.58', 'loss/train': '0.64225', 'examples_per_second': '32.939', 'grad_norm': '26', 'counters/examples': 198240, 'counters/updates': 6195}
train stats after 198272 examples: {'rewards_train/chosen': '0.045904', 'rewards_train/rejected': '0.082711', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.036807', 'logps_train/rejected': '-126.2', 'logps_train/chosen': '-149.7', 'loss/train': '0.71832', 'examples_per_second': '33.101', 'grad_norm': '29.25', 'counters/examples': 198272, 'counters/updates': 6196}
train stats after 198304 examples: {'rewards_train/chosen': '0.10921', 'rewards_train/rejected': '0.023626', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085589', 'logps_train/rejected': '-102.08', 'logps_train/chosen': '-115.05', 'loss/train': '0.66924', 'examples_per_second': '31.347', 'grad_norm': '28.5', 'counters/examples': 198304, 'counters/updates': 6197}
train stats after 198336 examples: {'rewards_train/chosen': '0.12175', 'rewards_train/rejected': '0.063272', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058477', 'logps_train/rejected': '-123.86', 'logps_train/chosen': '-126.43', 'loss/train': '0.67362', 'examples_per_second': '30.448', 'grad_norm': '26.25', 'counters/examples': 198336, 'counters/updates': 6198}
train stats after 198368 examples: {'rewards_train/chosen': '0.24663', 'rewards_train/rejected': '0.069246', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17738', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-145.18', 'loss/train': '0.62642', 'examples_per_second': '31.357', 'grad_norm': '33', 'counters/examples': 198368, 'counters/updates': 6199}
train stats after 198400 examples: {'rewards_train/chosen': '0.16721', 'rewards_train/rejected': '0.064931', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10227', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-157.14', 'loss/train': '0.66135', 'examples_per_second': '30.572', 'grad_norm': '28.75', 'counters/examples': 198400, 'counters/updates': 6200}
train stats after 198432 examples: {'rewards_train/chosen': '0.31601', 'rewards_train/rejected': '0.055337', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.26067', 'logps_train/rejected': '-129.94', 'logps_train/chosen': '-147.74', 'loss/train': '0.5875', 'examples_per_second': '31.064', 'grad_norm': '27.625', 'counters/examples': 198432, 'counters/updates': 6201}
train stats after 198464 examples: {'rewards_train/chosen': '0.14909', 'rewards_train/rejected': '-0.027259', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17635', 'logps_train/rejected': '-98.111', 'logps_train/chosen': '-126.65', 'loss/train': '0.6194', 'examples_per_second': '32.329', 'grad_norm': '31.875', 'counters/examples': 198464, 'counters/updates': 6202}
train stats after 198496 examples: {'rewards_train/chosen': '0.1559', 'rewards_train/rejected': '0.099377', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056522', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-145.55', 'loss/train': '0.6803', 'examples_per_second': '30.83', 'grad_norm': '42', 'counters/examples': 198496, 'counters/updates': 6203}
train stats after 198528 examples: {'rewards_train/chosen': '0.13323', 'rewards_train/rejected': '0.049587', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083647', 'logps_train/rejected': '-112.51', 'logps_train/chosen': '-117.31', 'loss/train': '0.66725', 'examples_per_second': '31.28', 'grad_norm': '24.125', 'counters/examples': 198528, 'counters/updates': 6204}
train stats after 198560 examples: {'rewards_train/chosen': '0.096072', 'rewards_train/rejected': '0.11911', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023037', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-125.33', 'loss/train': '0.71759', 'examples_per_second': '31.624', 'grad_norm': '44.5', 'counters/examples': 198560, 'counters/updates': 6205}
train stats after 198592 examples: {'rewards_train/chosen': '0.19926', 'rewards_train/rejected': '0.20203', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0027701', 'logps_train/rejected': '-124.77', 'logps_train/chosen': '-166.9', 'loss/train': '0.70257', 'examples_per_second': '30.491', 'grad_norm': '44.5', 'counters/examples': 198592, 'counters/updates': 6206}
train stats after 198624 examples: {'rewards_train/chosen': '0.14768', 'rewards_train/rejected': '-0.055083', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20276', 'logps_train/rejected': '-122.96', 'logps_train/chosen': '-128.24', 'loss/train': '0.6153', 'examples_per_second': '32.17', 'grad_norm': '23.625', 'counters/examples': 198624, 'counters/updates': 6207}
train stats after 198656 examples: {'rewards_train/chosen': '0.14052', 'rewards_train/rejected': '0.086318', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054197', 'logps_train/rejected': '-107.9', 'logps_train/chosen': '-154.04', 'loss/train': '0.68347', 'examples_per_second': '31.512', 'grad_norm': '35', 'counters/examples': 198656, 'counters/updates': 6208}
train stats after 198688 examples: {'rewards_train/chosen': '0.20399', 'rewards_train/rejected': '0.10147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10251', 'logps_train/rejected': '-143.46', 'logps_train/chosen': '-139.48', 'loss/train': '0.65838', 'examples_per_second': '30.142', 'grad_norm': '32.25', 'counters/examples': 198688, 'counters/updates': 6209}
train stats after 198720 examples: {'rewards_train/chosen': '0.16796', 'rewards_train/rejected': '0.14334', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02462', 'logps_train/rejected': '-138.5', 'logps_train/chosen': '-175.83', 'loss/train': '0.7036', 'examples_per_second': '31.414', 'grad_norm': '39', 'counters/examples': 198720, 'counters/updates': 6210}
train stats after 198752 examples: {'rewards_train/chosen': '0.14424', 'rewards_train/rejected': '0.12904', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.015194', 'logps_train/rejected': '-136.75', 'logps_train/chosen': '-144.39', 'loss/train': '0.69428', 'examples_per_second': '31.478', 'grad_norm': '35.25', 'counters/examples': 198752, 'counters/updates': 6211}
skipping logging after 198784 examples to avoid logging too frequently
train stats after 198816 examples: {'rewards_train/chosen': '0.15264', 'rewards_train/rejected': '0.066345', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086294', 'logps_train/rejected': '-116.42', 'logps_train/chosen': '-175.99', 'loss/train': '0.67402', 'examples_per_second': '32.673', 'grad_norm': '30.5', 'counters/examples': 198816, 'counters/updates': 6213}
train stats after 198848 examples: {'rewards_train/chosen': '0.21267', 'rewards_train/rejected': '0.061035', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15163', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-135.78', 'loss/train': '0.63113', 'examples_per_second': '31.499', 'grad_norm': '26.25', 'counters/examples': 198848, 'counters/updates': 6214}
train stats after 198880 examples: {'rewards_train/chosen': '0.16877', 'rewards_train/rejected': '0.063511', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10526', 'logps_train/rejected': '-138.54', 'logps_train/chosen': '-144.05', 'loss/train': '0.65203', 'examples_per_second': '30.544', 'grad_norm': '31.5', 'counters/examples': 198880, 'counters/updates': 6215}
train stats after 198912 examples: {'rewards_train/chosen': '0.14527', 'rewards_train/rejected': '0.066402', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.078866', 'logps_train/rejected': '-103.02', 'logps_train/chosen': '-138.05', 'loss/train': '0.66443', 'examples_per_second': '31.756', 'grad_norm': '31.125', 'counters/examples': 198912, 'counters/updates': 6216}
skipping logging after 198944 examples to avoid logging too frequently
train stats after 198976 examples: {'rewards_train/chosen': '0.19914', 'rewards_train/rejected': '0.069622', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12951', 'logps_train/rejected': '-109.16', 'logps_train/chosen': '-118.08', 'loss/train': '0.64273', 'examples_per_second': '31.643', 'grad_norm': '25.5', 'counters/examples': 198976, 'counters/updates': 6218}
train stats after 199008 examples: {'rewards_train/chosen': '0.086159', 'rewards_train/rejected': '0.039372', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046787', 'logps_train/rejected': '-118.52', 'logps_train/chosen': '-141.65', 'loss/train': '0.68019', 'examples_per_second': '32.635', 'grad_norm': '27.875', 'counters/examples': 199008, 'counters/updates': 6219}
train stats after 199040 examples: {'rewards_train/chosen': '0.17858', 'rewards_train/rejected': '-0.0042548', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18283', 'logps_train/rejected': '-138.14', 'logps_train/chosen': '-187.05', 'loss/train': '0.62116', 'examples_per_second': '31.52', 'grad_norm': '30.25', 'counters/examples': 199040, 'counters/updates': 6220}
train stats after 199072 examples: {'rewards_train/chosen': '0.21094', 'rewards_train/rejected': '0.00095457', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20998', 'logps_train/rejected': '-135.22', 'logps_train/chosen': '-198.42', 'loss/train': '0.612', 'examples_per_second': '31.469', 'grad_norm': '30.75', 'counters/examples': 199072, 'counters/updates': 6221}
train stats after 199104 examples: {'rewards_train/chosen': '0.17103', 'rewards_train/rejected': '0.096803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074231', 'logps_train/rejected': '-130.3', 'logps_train/chosen': '-179.14', 'loss/train': '0.67114', 'examples_per_second': '31.48', 'grad_norm': '28.875', 'counters/examples': 199104, 'counters/updates': 6222}
skipping logging after 199136 examples to avoid logging too frequently
train stats after 199168 examples: {'rewards_train/chosen': '0.18683', 'rewards_train/rejected': '0.010083', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17675', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-140.06', 'loss/train': '0.62407', 'examples_per_second': '35.189', 'grad_norm': '31', 'counters/examples': 199168, 'counters/updates': 6224}
train stats after 199200 examples: {'rewards_train/chosen': '0.060016', 'rewards_train/rejected': '-0.064466', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12448', 'logps_train/rejected': '-106.08', 'logps_train/chosen': '-120.23', 'loss/train': '0.64817', 'examples_per_second': '31.555', 'grad_norm': '28.875', 'counters/examples': 199200, 'counters/updates': 6225}
skipping logging after 199232 examples to avoid logging too frequently
train stats after 199264 examples: {'rewards_train/chosen': '0.18944', 'rewards_train/rejected': '0.086401', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10304', 'logps_train/rejected': '-110.38', 'logps_train/chosen': '-149.63', 'loss/train': '0.65646', 'examples_per_second': '30.869', 'grad_norm': '25.375', 'counters/examples': 199264, 'counters/updates': 6227}
skipping logging after 199296 examples to avoid logging too frequently
train stats after 199328 examples: {'rewards_train/chosen': '0.14437', 'rewards_train/rejected': '0.099696', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.044677', 'logps_train/rejected': '-167.55', 'logps_train/chosen': '-174.23', 'loss/train': '0.68233', 'examples_per_second': '31.474', 'grad_norm': '35.75', 'counters/examples': 199328, 'counters/updates': 6229}
train stats after 199360 examples: {'rewards_train/chosen': '0.14488', 'rewards_train/rejected': '0.10447', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.040418', 'logps_train/rejected': '-113.44', 'logps_train/chosen': '-136.66', 'loss/train': '0.68334', 'examples_per_second': '30.251', 'grad_norm': '36.5', 'counters/examples': 199360, 'counters/updates': 6230}
train stats after 199392 examples: {'rewards_train/chosen': '0.13811', 'rewards_train/rejected': '0.021298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11681', 'logps_train/rejected': '-113.26', 'logps_train/chosen': '-124.47', 'loss/train': '0.6467', 'examples_per_second': '30.053', 'grad_norm': '25.875', 'counters/examples': 199392, 'counters/updates': 6231}
skipping logging after 199424 examples to avoid logging too frequently
train stats after 199456 examples: {'rewards_train/chosen': '0.15117', 'rewards_train/rejected': '0.035515', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11565', 'logps_train/rejected': '-92.511', 'logps_train/chosen': '-115.19', 'loss/train': '0.64914', 'examples_per_second': '34.208', 'grad_norm': '25', 'counters/examples': 199456, 'counters/updates': 6233}
train stats after 199488 examples: {'rewards_train/chosen': '0.20869', 'rewards_train/rejected': '0.064014', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14468', 'logps_train/rejected': '-139.9', 'logps_train/chosen': '-125.82', 'loss/train': '0.63935', 'examples_per_second': '32.797', 'grad_norm': '46.25', 'counters/examples': 199488, 'counters/updates': 6234}
train stats after 199520 examples: {'rewards_train/chosen': '0.10829', 'rewards_train/rejected': '0.07967', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.02862', 'logps_train/rejected': '-127.96', 'logps_train/chosen': '-161.46', 'loss/train': '0.69688', 'examples_per_second': '31.555', 'grad_norm': '34.75', 'counters/examples': 199520, 'counters/updates': 6235}
train stats after 199552 examples: {'rewards_train/chosen': '0.23648', 'rewards_train/rejected': '-0.0018403', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23832', 'logps_train/rejected': '-143.65', 'logps_train/chosen': '-167.03', 'loss/train': '0.60112', 'examples_per_second': '30.576', 'grad_norm': '46.25', 'counters/examples': 199552, 'counters/updates': 6236}
train stats after 199584 examples: {'rewards_train/chosen': '0.12334', 'rewards_train/rejected': '0.057835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0655', 'logps_train/rejected': '-130.57', 'logps_train/chosen': '-129.09', 'loss/train': '0.66979', 'examples_per_second': '31.294', 'grad_norm': '31.875', 'counters/examples': 199584, 'counters/updates': 6237}
train stats after 199616 examples: {'rewards_train/chosen': '0.21', 'rewards_train/rejected': '0.11468', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095316', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-143.46', 'loss/train': '0.65762', 'examples_per_second': '32.865', 'grad_norm': '27.875', 'counters/examples': 199616, 'counters/updates': 6238}
skipping logging after 199648 examples to avoid logging too frequently
train stats after 199680 examples: {'rewards_train/chosen': '0.18261', 'rewards_train/rejected': '0.043542', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13907', 'logps_train/rejected': '-110.24', 'logps_train/chosen': '-137.35', 'loss/train': '0.6354', 'examples_per_second': '31.874', 'grad_norm': '24.375', 'counters/examples': 199680, 'counters/updates': 6240}
train stats after 199712 examples: {'rewards_train/chosen': '0.18749', 'rewards_train/rejected': '0.045011', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14248', 'logps_train/rejected': '-123.84', 'logps_train/chosen': '-152.46', 'loss/train': '0.63958', 'examples_per_second': '31.31', 'grad_norm': '30.125', 'counters/examples': 199712, 'counters/updates': 6241}
train stats after 199744 examples: {'rewards_train/chosen': '0.17607', 'rewards_train/rejected': '0.095303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080763', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-147.36', 'loss/train': '0.66602', 'examples_per_second': '31.377', 'grad_norm': '26.75', 'counters/examples': 199744, 'counters/updates': 6242}
train stats after 199776 examples: {'rewards_train/chosen': '0.27354', 'rewards_train/rejected': '0.11708', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15646', 'logps_train/rejected': '-141.88', 'logps_train/chosen': '-166.68', 'loss/train': '0.62985', 'examples_per_second': '31.513', 'grad_norm': '33.75', 'counters/examples': 199776, 'counters/updates': 6243}
train stats after 199808 examples: {'rewards_train/chosen': '0.31498', 'rewards_train/rejected': '0.056819', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25816', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-141.1', 'loss/train': '0.60576', 'examples_per_second': '31.532', 'grad_norm': '25.625', 'counters/examples': 199808, 'counters/updates': 6244}
train stats after 199840 examples: {'rewards_train/chosen': '0.21629', 'rewards_train/rejected': '0.079242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13705', 'logps_train/rejected': '-133.36', 'logps_train/chosen': '-123.96', 'loss/train': '0.63902', 'examples_per_second': '31.244', 'grad_norm': '35.25', 'counters/examples': 199840, 'counters/updates': 6245}
skipping logging after 199872 examples to avoid logging too frequently
train stats after 199904 examples: {'rewards_train/chosen': '0.19077', 'rewards_train/rejected': '-0.029882', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22065', 'logps_train/rejected': '-112.21', 'logps_train/chosen': '-141.84', 'loss/train': '0.59935', 'examples_per_second': '30.006', 'grad_norm': '24.375', 'counters/examples': 199904, 'counters/updates': 6247}
train stats after 199936 examples: {'rewards_train/chosen': '0.20826', 'rewards_train/rejected': '0.098865', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1094', 'logps_train/rejected': '-132.06', 'logps_train/chosen': '-147.21', 'loss/train': '0.65599', 'examples_per_second': '31.554', 'grad_norm': '29.375', 'counters/examples': 199936, 'counters/updates': 6248}
train stats after 199968 examples: {'rewards_train/chosen': '0.15593', 'rewards_train/rejected': '0.12654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029387', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-129.36', 'loss/train': '0.68677', 'examples_per_second': '30.4', 'grad_norm': '24.875', 'counters/examples': 199968, 'counters/updates': 6249}
train stats after 200000 examples: {'rewards_train/chosen': '0.079501', 'rewards_train/rejected': '-0.021637', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10114', 'logps_train/rejected': '-104.8', 'logps_train/chosen': '-146.07', 'loss/train': '0.66157', 'examples_per_second': '30.026', 'grad_norm': '27.25', 'counters/examples': 200000, 'counters/updates': 6250}
train stats after 200032 examples: {'rewards_train/chosen': '0.18903', 'rewards_train/rejected': '0.051981', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13705', 'logps_train/rejected': '-145.84', 'logps_train/chosen': '-132.32', 'loss/train': '0.65016', 'examples_per_second': '31.525', 'grad_norm': '37.5', 'counters/examples': 200032, 'counters/updates': 6251}
train stats after 200064 examples: {'rewards_train/chosen': '0.18177', 'rewards_train/rejected': '0.12149', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060282', 'logps_train/rejected': '-150.26', 'logps_train/chosen': '-140.7', 'loss/train': '0.6746', 'examples_per_second': '33.232', 'grad_norm': '38', 'counters/examples': 200064, 'counters/updates': 6252}
train stats after 200096 examples: {'rewards_train/chosen': '0.15872', 'rewards_train/rejected': '0.0037325', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15499', 'logps_train/rejected': '-113.03', 'logps_train/chosen': '-150.33', 'loss/train': '0.62705', 'examples_per_second': '31.541', 'grad_norm': '28.75', 'counters/examples': 200096, 'counters/updates': 6253}
train stats after 200128 examples: {'rewards_train/chosen': '0.24039', 'rewards_train/rejected': '0.18259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.057796', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-142.8', 'loss/train': '0.68279', 'examples_per_second': '31.529', 'grad_norm': '46.5', 'counters/examples': 200128, 'counters/updates': 6254}
train stats after 200160 examples: {'rewards_train/chosen': '0.18937', 'rewards_train/rejected': '0.048559', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14081', 'logps_train/rejected': '-155.74', 'logps_train/chosen': '-189.32', 'loss/train': '0.65152', 'examples_per_second': '31.186', 'grad_norm': '44', 'counters/examples': 200160, 'counters/updates': 6255}
train stats after 200192 examples: {'rewards_train/chosen': '0.17685', 'rewards_train/rejected': '0.11758', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.059263', 'logps_train/rejected': '-136.88', 'logps_train/chosen': '-170.66', 'loss/train': '0.6819', 'examples_per_second': '29.998', 'grad_norm': '33.5', 'counters/examples': 200192, 'counters/updates': 6256}
train stats after 200224 examples: {'rewards_train/chosen': '0.28772', 'rewards_train/rejected': '0.10309', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18464', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-141.19', 'loss/train': '0.64376', 'examples_per_second': '31.255', 'grad_norm': '33', 'counters/examples': 200224, 'counters/updates': 6257}
train stats after 200256 examples: {'rewards_train/chosen': '0.23565', 'rewards_train/rejected': '0.10381', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13185', 'logps_train/rejected': '-169.53', 'logps_train/chosen': '-239.45', 'loss/train': '0.6426', 'examples_per_second': '29.948', 'grad_norm': '36.5', 'counters/examples': 200256, 'counters/updates': 6258}
train stats after 200288 examples: {'rewards_train/chosen': '0.19477', 'rewards_train/rejected': '0.0086609', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18611', 'logps_train/rejected': '-104.54', 'logps_train/chosen': '-145.71', 'loss/train': '0.63114', 'examples_per_second': '30.01', 'grad_norm': '28.25', 'counters/examples': 200288, 'counters/updates': 6259}
train stats after 200320 examples: {'rewards_train/chosen': '0.19736', 'rewards_train/rejected': '0.05161', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14575', 'logps_train/rejected': '-135.29', 'logps_train/chosen': '-137.65', 'loss/train': '0.64161', 'examples_per_second': '30.884', 'grad_norm': '38.75', 'counters/examples': 200320, 'counters/updates': 6260}
skipping logging after 200352 examples to avoid logging too frequently
train stats after 200384 examples: {'rewards_train/chosen': '0.15577', 'rewards_train/rejected': '-0.028569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18434', 'logps_train/rejected': '-105.29', 'logps_train/chosen': '-117.05', 'loss/train': '0.62095', 'examples_per_second': '31.43', 'grad_norm': '25.125', 'counters/examples': 200384, 'counters/updates': 6262}
train stats after 200416 examples: {'rewards_train/chosen': '0.23971', 'rewards_train/rejected': '0.1218', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11792', 'logps_train/rejected': '-120.13', 'logps_train/chosen': '-137.67', 'loss/train': '0.65196', 'examples_per_second': '31.583', 'grad_norm': '42', 'counters/examples': 200416, 'counters/updates': 6263}
train stats after 200448 examples: {'rewards_train/chosen': '0.15629', 'rewards_train/rejected': '0.090012', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066273', 'logps_train/rejected': '-147.5', 'logps_train/chosen': '-131.2', 'loss/train': '0.68584', 'examples_per_second': '30.063', 'grad_norm': '43.5', 'counters/examples': 200448, 'counters/updates': 6264}
skipping logging after 200480 examples to avoid logging too frequently
train stats after 200512 examples: {'rewards_train/chosen': '0.21546', 'rewards_train/rejected': '0.05101', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16445', 'logps_train/rejected': '-145.08', 'logps_train/chosen': '-162.17', 'loss/train': '0.62528', 'examples_per_second': '29.953', 'grad_norm': '28.5', 'counters/examples': 200512, 'counters/updates': 6266}
train stats after 200544 examples: {'rewards_train/chosen': '0.24646', 'rewards_train/rejected': '0.13405', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11242', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-155.29', 'loss/train': '0.6523', 'examples_per_second': '30.517', 'grad_norm': '42.25', 'counters/examples': 200544, 'counters/updates': 6267}
train stats after 200576 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.002196', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12157', 'logps_train/rejected': '-139.44', 'logps_train/chosen': '-144.61', 'loss/train': '0.64616', 'examples_per_second': '32.186', 'grad_norm': '41.25', 'counters/examples': 200576, 'counters/updates': 6268}
skipping logging after 200608 examples to avoid logging too frequently
train stats after 200640 examples: {'rewards_train/chosen': '0.091696', 'rewards_train/rejected': '-0.0051753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096871', 'logps_train/rejected': '-97.654', 'logps_train/chosen': '-116.29', 'loss/train': '0.65817', 'examples_per_second': '32.128', 'grad_norm': '27', 'counters/examples': 200640, 'counters/updates': 6270}
train stats after 200672 examples: {'rewards_train/chosen': '0.18665', 'rewards_train/rejected': '0.054111', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13254', 'logps_train/rejected': '-155.11', 'logps_train/chosen': '-141.86', 'loss/train': '0.65685', 'examples_per_second': '31.448', 'grad_norm': '39', 'counters/examples': 200672, 'counters/updates': 6271}
train stats after 200704 examples: {'rewards_train/chosen': '0.22678', 'rewards_train/rejected': '0.12801', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098767', 'logps_train/rejected': '-120.07', 'logps_train/chosen': '-133.73', 'loss/train': '0.65714', 'examples_per_second': '30.537', 'grad_norm': '32.75', 'counters/examples': 200704, 'counters/updates': 6272}
train stats after 200736 examples: {'rewards_train/chosen': '0.086358', 'rewards_train/rejected': '0.088155', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0017971', 'logps_train/rejected': '-138.73', 'logps_train/chosen': '-143.63', 'loss/train': '0.69836', 'examples_per_second': '32.463', 'grad_norm': '31.125', 'counters/examples': 200736, 'counters/updates': 6273}
train stats after 200768 examples: {'rewards_train/chosen': '0.1862', 'rewards_train/rejected': '0.12406', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062142', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-165', 'loss/train': '0.67713', 'examples_per_second': '31.824', 'grad_norm': '30.5', 'counters/examples': 200768, 'counters/updates': 6274}
train stats after 200800 examples: {'rewards_train/chosen': '0.15947', 'rewards_train/rejected': '0.087436', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07203', 'logps_train/rejected': '-138.59', 'logps_train/chosen': '-161.66', 'loss/train': '0.66911', 'examples_per_second': '30.455', 'grad_norm': '39.75', 'counters/examples': 200800, 'counters/updates': 6275}
train stats after 200832 examples: {'rewards_train/chosen': '0.17327', 'rewards_train/rejected': '0.099197', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074073', 'logps_train/rejected': '-123.91', 'logps_train/chosen': '-124.13', 'loss/train': '0.66397', 'examples_per_second': '32.298', 'grad_norm': '28.25', 'counters/examples': 200832, 'counters/updates': 6276}
train stats after 200864 examples: {'rewards_train/chosen': '0.22165', 'rewards_train/rejected': '0.052866', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16879', 'logps_train/rejected': '-113.56', 'logps_train/chosen': '-134.76', 'loss/train': '0.62448', 'examples_per_second': '30.259', 'grad_norm': '34', 'counters/examples': 200864, 'counters/updates': 6277}
train stats after 200896 examples: {'rewards_train/chosen': '0.20232', 'rewards_train/rejected': '0.069444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13288', 'logps_train/rejected': '-95.925', 'logps_train/chosen': '-156.46', 'loss/train': '0.64467', 'examples_per_second': '28.148', 'grad_norm': '26.375', 'counters/examples': 200896, 'counters/updates': 6278}
train stats after 200928 examples: {'rewards_train/chosen': '0.20669', 'rewards_train/rejected': '0.03162', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17507', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-161.52', 'loss/train': '0.6266', 'examples_per_second': '31.458', 'grad_norm': '34.75', 'counters/examples': 200928, 'counters/updates': 6279}
train stats after 200960 examples: {'rewards_train/chosen': '0.2188', 'rewards_train/rejected': '0.11129', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10751', 'logps_train/rejected': '-130.82', 'logps_train/chosen': '-148.35', 'loss/train': '0.65033', 'examples_per_second': '31.326', 'grad_norm': '30.875', 'counters/examples': 200960, 'counters/updates': 6280}
skipping logging after 200992 examples to avoid logging too frequently
train stats after 201024 examples: {'rewards_train/chosen': '0.20432', 'rewards_train/rejected': '0.10057', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10375', 'logps_train/rejected': '-100', 'logps_train/chosen': '-134.49', 'loss/train': '0.65725', 'examples_per_second': '31.527', 'grad_norm': '38.5', 'counters/examples': 201024, 'counters/updates': 6282}
skipping logging after 201056 examples to avoid logging too frequently
train stats after 201088 examples: {'rewards_train/chosen': '0.10363', 'rewards_train/rejected': '0.092428', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.011204', 'logps_train/rejected': '-136.26', 'logps_train/chosen': '-165.16', 'loss/train': '0.70168', 'examples_per_second': '31.631', 'grad_norm': '29.25', 'counters/examples': 201088, 'counters/updates': 6284}
train stats after 201120 examples: {'rewards_train/chosen': '0.20883', 'rewards_train/rejected': '0.089087', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11974', 'logps_train/rejected': '-136.86', 'logps_train/chosen': '-130.91', 'loss/train': '0.64296', 'examples_per_second': '29.972', 'grad_norm': '28.875', 'counters/examples': 201120, 'counters/updates': 6285}
train stats after 201152 examples: {'rewards_train/chosen': '0.15748', 'rewards_train/rejected': '-0.030824', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18831', 'logps_train/rejected': '-90.277', 'logps_train/chosen': '-118.39', 'loss/train': '0.62088', 'examples_per_second': '32.03', 'grad_norm': '29.125', 'counters/examples': 201152, 'counters/updates': 6286}
train stats after 201184 examples: {'rewards_train/chosen': '0.17207', 'rewards_train/rejected': '0.035316', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13676', 'logps_train/rejected': '-130.58', 'logps_train/chosen': '-141.59', 'loss/train': '0.64085', 'examples_per_second': '31.429', 'grad_norm': '37.5', 'counters/examples': 201184, 'counters/updates': 6287}
train stats after 201216 examples: {'rewards_train/chosen': '0.21404', 'rewards_train/rejected': '0.080274', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13376', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-195.01', 'loss/train': '0.64441', 'examples_per_second': '30.486', 'grad_norm': '35', 'counters/examples': 201216, 'counters/updates': 6288}
train stats after 201248 examples: {'rewards_train/chosen': '0.15057', 'rewards_train/rejected': '0.081142', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.069427', 'logps_train/rejected': '-141.33', 'logps_train/chosen': '-137.92', 'loss/train': '0.66548', 'examples_per_second': '31.783', 'grad_norm': '29.25', 'counters/examples': 201248, 'counters/updates': 6289}
train stats after 201280 examples: {'rewards_train/chosen': '0.2435', 'rewards_train/rejected': '0.025545', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.21795', 'logps_train/rejected': '-113.05', 'logps_train/chosen': '-120.74', 'loss/train': '0.59973', 'examples_per_second': '31.478', 'grad_norm': '22.375', 'counters/examples': 201280, 'counters/updates': 6290}
train stats after 201312 examples: {'rewards_train/chosen': '0.13146', 'rewards_train/rejected': '0.17782', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.046352', 'logps_train/rejected': '-144.17', 'logps_train/chosen': '-132.88', 'loss/train': '0.72682', 'examples_per_second': '30.287', 'grad_norm': '29.625', 'counters/examples': 201312, 'counters/updates': 6291}
train stats after 201344 examples: {'rewards_train/chosen': '0.20537', 'rewards_train/rejected': '0.036517', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16885', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-155.01', 'loss/train': '0.62541', 'examples_per_second': '32.884', 'grad_norm': '31.625', 'counters/examples': 201344, 'counters/updates': 6292}
train stats after 201376 examples: {'rewards_train/chosen': '0.21469', 'rewards_train/rejected': '0.071605', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14309', 'logps_train/rejected': '-161.01', 'logps_train/chosen': '-158.62', 'loss/train': '0.65187', 'examples_per_second': '29.963', 'grad_norm': '46', 'counters/examples': 201376, 'counters/updates': 6293}
train stats after 201408 examples: {'rewards_train/chosen': '0.11008', 'rewards_train/rejected': '0.097781', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.012297', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-163.05', 'loss/train': '0.69719', 'examples_per_second': '31.36', 'grad_norm': '33.5', 'counters/examples': 201408, 'counters/updates': 6294}
train stats after 201440 examples: {'rewards_train/chosen': '0.21645', 'rewards_train/rejected': '0.063684', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15276', 'logps_train/rejected': '-129.51', 'logps_train/chosen': '-156.62', 'loss/train': '0.64053', 'examples_per_second': '30.927', 'grad_norm': '31.25', 'counters/examples': 201440, 'counters/updates': 6295}
train stats after 201472 examples: {'rewards_train/chosen': '0.14092', 'rewards_train/rejected': '-0.017657', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15857', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-161.5', 'loss/train': '0.62339', 'examples_per_second': '31.061', 'grad_norm': '25.25', 'counters/examples': 201472, 'counters/updates': 6296}
train stats after 201504 examples: {'rewards_train/chosen': '0.15881', 'rewards_train/rejected': '0.1323', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.026512', 'logps_train/rejected': '-100.29', 'logps_train/chosen': '-126.73', 'loss/train': '0.69238', 'examples_per_second': '30.136', 'grad_norm': '29.25', 'counters/examples': 201504, 'counters/updates': 6297}
train stats after 201536 examples: {'rewards_train/chosen': '0.19277', 'rewards_train/rejected': '0.028831', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16394', 'logps_train/rejected': '-104.36', 'logps_train/chosen': '-155.85', 'loss/train': '0.62243', 'examples_per_second': '29.853', 'grad_norm': '25.625', 'counters/examples': 201536, 'counters/updates': 6298}
train stats after 201568 examples: {'rewards_train/chosen': '0.2215', 'rewards_train/rejected': '0.15664', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.064856', 'logps_train/rejected': '-160.15', 'logps_train/chosen': '-155.97', 'loss/train': '0.67773', 'examples_per_second': '31.532', 'grad_norm': '36.75', 'counters/examples': 201568, 'counters/updates': 6299}
train stats after 201600 examples: {'rewards_train/chosen': '0.19171', 'rewards_train/rejected': '0.14698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04473', 'logps_train/rejected': '-148.93', 'logps_train/chosen': '-129.11', 'loss/train': '0.67964', 'examples_per_second': '31.503', 'grad_norm': '31.5', 'counters/examples': 201600, 'counters/updates': 6300}
train stats after 201632 examples: {'rewards_train/chosen': '0.18258', 'rewards_train/rejected': '0.094615', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087969', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-166.73', 'loss/train': '0.65872', 'examples_per_second': '30.608', 'grad_norm': '59.75', 'counters/examples': 201632, 'counters/updates': 6301}
train stats after 201664 examples: {'rewards_train/chosen': '0.21304', 'rewards_train/rejected': '0.068043', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.145', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-140.78', 'loss/train': '0.65097', 'examples_per_second': '31.181', 'grad_norm': '30.125', 'counters/examples': 201664, 'counters/updates': 6302}
train stats after 201696 examples: {'rewards_train/chosen': '0.13961', 'rewards_train/rejected': '0.035753', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10385', 'logps_train/rejected': '-139.26', 'logps_train/chosen': '-109.47', 'loss/train': '0.65468', 'examples_per_second': '31.456', 'grad_norm': '26.25', 'counters/examples': 201696, 'counters/updates': 6303}
train stats after 201728 examples: {'rewards_train/chosen': '0.15971', 'rewards_train/rejected': '0.070825', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.088887', 'logps_train/rejected': '-110.94', 'logps_train/chosen': '-139.74', 'loss/train': '0.66543', 'examples_per_second': '32.051', 'grad_norm': '27.875', 'counters/examples': 201728, 'counters/updates': 6304}
train stats after 201760 examples: {'rewards_train/chosen': '0.052709', 'rewards_train/rejected': '0.061758', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0090489', 'logps_train/rejected': '-98.477', 'logps_train/chosen': '-117.37', 'loss/train': '0.70261', 'examples_per_second': '31.472', 'grad_norm': '24.625', 'counters/examples': 201760, 'counters/updates': 6305}
skipping logging after 201792 examples to avoid logging too frequently
train stats after 201824 examples: {'rewards_train/chosen': '0.16259', 'rewards_train/rejected': '0.10936', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.053235', 'logps_train/rejected': '-117.95', 'logps_train/chosen': '-154.7', 'loss/train': '0.68371', 'examples_per_second': '32.419', 'grad_norm': '28.25', 'counters/examples': 201824, 'counters/updates': 6307}
train stats after 201856 examples: {'rewards_train/chosen': '0.14654', 'rewards_train/rejected': '0.012285', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13425', 'logps_train/rejected': '-107.63', 'logps_train/chosen': '-138', 'loss/train': '0.64553', 'examples_per_second': '29.892', 'grad_norm': '26.5', 'counters/examples': 201856, 'counters/updates': 6308}
train stats after 201888 examples: {'rewards_train/chosen': '0.14459', 'rewards_train/rejected': '0.050127', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094463', 'logps_train/rejected': '-96.373', 'logps_train/chosen': '-140.8', 'loss/train': '0.65595', 'examples_per_second': '30.713', 'grad_norm': '30.375', 'counters/examples': 201888, 'counters/updates': 6309}
train stats after 201920 examples: {'rewards_train/chosen': '0.20373', 'rewards_train/rejected': '0.10435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099379', 'logps_train/rejected': '-143.89', 'logps_train/chosen': '-157.4', 'loss/train': '0.66167', 'examples_per_second': '32.325', 'grad_norm': '51.5', 'counters/examples': 201920, 'counters/updates': 6310}
train stats after 201952 examples: {'rewards_train/chosen': '0.23136', 'rewards_train/rejected': '0.081084', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15027', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-154.77', 'loss/train': '0.64161', 'examples_per_second': '32.407', 'grad_norm': '31.875', 'counters/examples': 201952, 'counters/updates': 6311}
train stats after 201984 examples: {'rewards_train/chosen': '0.17186', 'rewards_train/rejected': '0.13847', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033387', 'logps_train/rejected': '-121.2', 'logps_train/chosen': '-135.09', 'loss/train': '0.69188', 'examples_per_second': '32.161', 'grad_norm': '32', 'counters/examples': 201984, 'counters/updates': 6312}
train stats after 202016 examples: {'rewards_train/chosen': '0.18863', 'rewards_train/rejected': '0.073751', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11488', 'logps_train/rejected': '-154.07', 'logps_train/chosen': '-149.28', 'loss/train': '0.6529', 'examples_per_second': '33.076', 'grad_norm': '39.25', 'counters/examples': 202016, 'counters/updates': 6313}
train stats after 202048 examples: {'rewards_train/chosen': '0.13224', 'rewards_train/rejected': '-0.022038', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15428', 'logps_train/rejected': '-101.82', 'logps_train/chosen': '-106.67', 'loss/train': '0.62658', 'examples_per_second': '32.231', 'grad_norm': '24.375', 'counters/examples': 202048, 'counters/updates': 6314}
train stats after 202080 examples: {'rewards_train/chosen': '0.087596', 'rewards_train/rejected': '0.23869', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.15109', 'logps_train/rejected': '-189.85', 'logps_train/chosen': '-147.86', 'loss/train': '0.84169', 'examples_per_second': '31.516', 'grad_norm': '109.5', 'counters/examples': 202080, 'counters/updates': 6315}
train stats after 202112 examples: {'rewards_train/chosen': '0.023721', 'rewards_train/rejected': '0.045607', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.021886', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-136.16', 'loss/train': '0.71638', 'examples_per_second': '31.552', 'grad_norm': '51', 'counters/examples': 202112, 'counters/updates': 6316}
skipping logging after 202144 examples to avoid logging too frequently
train stats after 202176 examples: {'rewards_train/chosen': '0.13912', 'rewards_train/rejected': '0.024411', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11471', 'logps_train/rejected': '-101.29', 'logps_train/chosen': '-107.35', 'loss/train': '0.64526', 'examples_per_second': '31.701', 'grad_norm': '54.5', 'counters/examples': 202176, 'counters/updates': 6318}
train stats after 202208 examples: {'rewards_train/chosen': '0.27691', 'rewards_train/rejected': '0.10012', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1768', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-136.02', 'loss/train': '0.62061', 'examples_per_second': '30.008', 'grad_norm': '29.75', 'counters/examples': 202208, 'counters/updates': 6319}
train stats after 202240 examples: {'rewards_train/chosen': '0.16132', 'rewards_train/rejected': '0.09618', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065137', 'logps_train/rejected': '-126.07', 'logps_train/chosen': '-131.68', 'loss/train': '0.67698', 'examples_per_second': '31.459', 'grad_norm': '30.375', 'counters/examples': 202240, 'counters/updates': 6320}
train stats after 202272 examples: {'rewards_train/chosen': '0.14871', 'rewards_train/rejected': '0.031755', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11696', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-159.09', 'loss/train': '0.64579', 'examples_per_second': '31.751', 'grad_norm': '29.25', 'counters/examples': 202272, 'counters/updates': 6321}
train stats after 202304 examples: {'rewards_train/chosen': '0.23413', 'rewards_train/rejected': '0.17325', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.060874', 'logps_train/rejected': '-182.14', 'logps_train/chosen': '-191.07', 'loss/train': '0.68511', 'examples_per_second': '31.495', 'grad_norm': '59.25', 'counters/examples': 202304, 'counters/updates': 6322}
train stats after 202336 examples: {'rewards_train/chosen': '0.21177', 'rewards_train/rejected': '0.094491', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11728', 'logps_train/rejected': '-145', 'logps_train/chosen': '-169.95', 'loss/train': '0.66517', 'examples_per_second': '24.194', 'grad_norm': '30.25', 'counters/examples': 202336, 'counters/updates': 6323}
train stats after 202368 examples: {'rewards_train/chosen': '0.14263', 'rewards_train/rejected': '0.14889', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0062636', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-149.37', 'loss/train': '0.71165', 'examples_per_second': '32.614', 'grad_norm': '50.5', 'counters/examples': 202368, 'counters/updates': 6324}
skipping logging after 202400 examples to avoid logging too frequently
train stats after 202432 examples: {'rewards_train/chosen': '0.22828', 'rewards_train/rejected': '-0.0047912', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23307', 'logps_train/rejected': '-101.7', 'logps_train/chosen': '-144.34', 'loss/train': '0.59752', 'examples_per_second': '23.901', 'grad_norm': '29.625', 'counters/examples': 202432, 'counters/updates': 6326}
skipping logging after 202464 examples to avoid logging too frequently
train stats after 202496 examples: {'rewards_train/chosen': '0.083045', 'rewards_train/rejected': '0.033848', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049197', 'logps_train/rejected': '-141.52', 'logps_train/chosen': '-128.71', 'loss/train': '0.67555', 'examples_per_second': '32.906', 'grad_norm': '29.75', 'counters/examples': 202496, 'counters/updates': 6328}
train stats after 202528 examples: {'rewards_train/chosen': '0.15316', 'rewards_train/rejected': '0.082268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070887', 'logps_train/rejected': '-104.75', 'logps_train/chosen': '-134.33', 'loss/train': '0.66915', 'examples_per_second': '32.28', 'grad_norm': '33.25', 'counters/examples': 202528, 'counters/updates': 6329}
train stats after 202560 examples: {'rewards_train/chosen': '0.09805', 'rewards_train/rejected': '0.012612', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085439', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-139.44', 'loss/train': '0.662', 'examples_per_second': '31.588', 'grad_norm': '29.875', 'counters/examples': 202560, 'counters/updates': 6330}
skipping logging after 202592 examples to avoid logging too frequently
train stats after 202624 examples: {'rewards_train/chosen': '0.19774', 'rewards_train/rejected': '0.13266', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.065083', 'logps_train/rejected': '-163.9', 'logps_train/chosen': '-131.02', 'loss/train': '0.67708', 'examples_per_second': '32.026', 'grad_norm': '29.625', 'counters/examples': 202624, 'counters/updates': 6332}
train stats after 202656 examples: {'rewards_train/chosen': '0.18611', 'rewards_train/rejected': '0.084202', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10191', 'logps_train/rejected': '-109.37', 'logps_train/chosen': '-109.67', 'loss/train': '0.65551', 'examples_per_second': '32.243', 'grad_norm': '29', 'counters/examples': 202656, 'counters/updates': 6333}
train stats after 202688 examples: {'rewards_train/chosen': '0.14943', 'rewards_train/rejected': '0.073185', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.076248', 'logps_train/rejected': '-122.83', 'logps_train/chosen': '-142.95', 'loss/train': '0.6701', 'examples_per_second': '31.657', 'grad_norm': '29.625', 'counters/examples': 202688, 'counters/updates': 6334}
train stats after 202720 examples: {'rewards_train/chosen': '0.11199', 'rewards_train/rejected': '0.065878', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04611', 'logps_train/rejected': '-101.37', 'logps_train/chosen': '-125.02', 'loss/train': '0.6775', 'examples_per_second': '31.792', 'grad_norm': '23', 'counters/examples': 202720, 'counters/updates': 6335}
train stats after 202752 examples: {'rewards_train/chosen': '0.16224', 'rewards_train/rejected': '0.06984', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092401', 'logps_train/rejected': '-139.77', 'logps_train/chosen': '-148.83', 'loss/train': '0.67042', 'examples_per_second': '31.488', 'grad_norm': '33.25', 'counters/examples': 202752, 'counters/updates': 6336}
train stats after 202784 examples: {'rewards_train/chosen': '0.15372', 'rewards_train/rejected': '0.14599', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0077212', 'logps_train/rejected': '-160.32', 'logps_train/chosen': '-145.49', 'loss/train': '0.7016', 'examples_per_second': '31.462', 'grad_norm': '34.5', 'counters/examples': 202784, 'counters/updates': 6337}
train stats after 202816 examples: {'rewards_train/chosen': '0.1118', 'rewards_train/rejected': '0.015453', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.096348', 'logps_train/rejected': '-125.86', 'logps_train/chosen': '-157.58', 'loss/train': '0.65724', 'examples_per_second': '32.652', 'grad_norm': '29', 'counters/examples': 202816, 'counters/updates': 6338}
skipping logging after 202848 examples to avoid logging too frequently
train stats after 202880 examples: {'rewards_train/chosen': '0.099854', 'rewards_train/rejected': '0.12461', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.02476', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-132.78', 'loss/train': '0.7163', 'examples_per_second': '33.761', 'grad_norm': '44', 'counters/examples': 202880, 'counters/updates': 6340}
skipping logging after 202912 examples to avoid logging too frequently
train stats after 202944 examples: {'rewards_train/chosen': '0.12943', 'rewards_train/rejected': '-0.030278', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1597', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-149.18', 'loss/train': '0.63738', 'examples_per_second': '30.802', 'grad_norm': '35.75', 'counters/examples': 202944, 'counters/updates': 6342}
train stats after 202976 examples: {'rewards_train/chosen': '0.19463', 'rewards_train/rejected': '0.12617', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068461', 'logps_train/rejected': '-113.08', 'logps_train/chosen': '-141.69', 'loss/train': '0.67816', 'examples_per_second': '31.522', 'grad_norm': '43', 'counters/examples': 202976, 'counters/updates': 6343}
train stats after 203008 examples: {'rewards_train/chosen': '0.11862', 'rewards_train/rejected': '0.057043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061572', 'logps_train/rejected': '-116.25', 'logps_train/chosen': '-141.67', 'loss/train': '0.68923', 'examples_per_second': '32.622', 'grad_norm': '30', 'counters/examples': 203008, 'counters/updates': 6344}
train stats after 203040 examples: {'rewards_train/chosen': '0.28387', 'rewards_train/rejected': '0.11231', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17156', 'logps_train/rejected': '-131.07', 'logps_train/chosen': '-168.7', 'loss/train': '0.6329', 'examples_per_second': '29.821', 'grad_norm': '32.75', 'counters/examples': 203040, 'counters/updates': 6345}
train stats after 203072 examples: {'rewards_train/chosen': '0.13517', 'rewards_train/rejected': '0.044344', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090824', 'logps_train/rejected': '-106.23', 'logps_train/chosen': '-134.25', 'loss/train': '0.66161', 'examples_per_second': '30.511', 'grad_norm': '25.625', 'counters/examples': 203072, 'counters/updates': 6346}
train stats after 203104 examples: {'rewards_train/chosen': '0.13401', 'rewards_train/rejected': '0.015477', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11853', 'logps_train/rejected': '-148.05', 'logps_train/chosen': '-120.45', 'loss/train': '0.64946', 'examples_per_second': '30.696', 'grad_norm': '31.75', 'counters/examples': 203104, 'counters/updates': 6347}
train stats after 203136 examples: {'rewards_train/chosen': '0.12681', 'rewards_train/rejected': '0.13009', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0032751', 'logps_train/rejected': '-144.02', 'logps_train/chosen': '-168.67', 'loss/train': '0.72169', 'examples_per_second': '32.489', 'grad_norm': '43.5', 'counters/examples': 203136, 'counters/updates': 6348}
train stats after 203168 examples: {'rewards_train/chosen': '0.095196', 'rewards_train/rejected': '-0.0072229', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10242', 'logps_train/rejected': '-80.307', 'logps_train/chosen': '-106.63', 'loss/train': '0.64801', 'examples_per_second': '31.025', 'grad_norm': '22', 'counters/examples': 203168, 'counters/updates': 6349}
train stats after 203200 examples: {'rewards_train/chosen': '0.14731', 'rewards_train/rejected': '0.042162', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10515', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-133.89', 'loss/train': '0.65239', 'examples_per_second': '31.647', 'grad_norm': '26.625', 'counters/examples': 203200, 'counters/updates': 6350}
train stats after 203232 examples: {'rewards_train/chosen': '0.15497', 'rewards_train/rejected': '0.063467', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091507', 'logps_train/rejected': '-182.35', 'logps_train/chosen': '-169.87', 'loss/train': '0.66101', 'examples_per_second': '31.496', 'grad_norm': '35.75', 'counters/examples': 203232, 'counters/updates': 6351}
train stats after 203264 examples: {'rewards_train/chosen': '0.18096', 'rewards_train/rejected': '0.049188', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13177', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-152.61', 'loss/train': '0.64416', 'examples_per_second': '30.434', 'grad_norm': '30.875', 'counters/examples': 203264, 'counters/updates': 6352}
skipping logging after 203296 examples to avoid logging too frequently
train stats after 203328 examples: {'rewards_train/chosen': '0.10448', 'rewards_train/rejected': '0.019065', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085419', 'logps_train/rejected': '-113.69', 'logps_train/chosen': '-130.16', 'loss/train': '0.65842', 'examples_per_second': '30.647', 'grad_norm': '27.5', 'counters/examples': 203328, 'counters/updates': 6354}
train stats after 203360 examples: {'rewards_train/chosen': '0.12077', 'rewards_train/rejected': '0.081947', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.038827', 'logps_train/rejected': '-122.91', 'logps_train/chosen': '-128.98', 'loss/train': '0.68094', 'examples_per_second': '31.401', 'grad_norm': '39.5', 'counters/examples': 203360, 'counters/updates': 6355}
train stats after 203392 examples: {'rewards_train/chosen': '0.16311', 'rewards_train/rejected': '0.063761', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.099353', 'logps_train/rejected': '-108.34', 'logps_train/chosen': '-120.21', 'loss/train': '0.66072', 'examples_per_second': '31.135', 'grad_norm': '25.375', 'counters/examples': 203392, 'counters/updates': 6356}
train stats after 203424 examples: {'rewards_train/chosen': '0.149', 'rewards_train/rejected': '0.12074', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028265', 'logps_train/rejected': '-147.21', 'logps_train/chosen': '-161.31', 'loss/train': '0.68597', 'examples_per_second': '31.837', 'grad_norm': '31.375', 'counters/examples': 203424, 'counters/updates': 6357}
skipping logging after 203456 examples to avoid logging too frequently
train stats after 203488 examples: {'rewards_train/chosen': '0.14495', 'rewards_train/rejected': '0.048789', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096158', 'logps_train/rejected': '-117.35', 'logps_train/chosen': '-169.57', 'loss/train': '0.66324', 'examples_per_second': '31.516', 'grad_norm': '40.75', 'counters/examples': 203488, 'counters/updates': 6359}
train stats after 203520 examples: {'rewards_train/chosen': '0.039017', 'rewards_train/rejected': '0.045708', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0066903', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-123.35', 'loss/train': '0.71041', 'examples_per_second': '29.921', 'grad_norm': '45.75', 'counters/examples': 203520, 'counters/updates': 6360}
train stats after 203552 examples: {'rewards_train/chosen': '0.17907', 'rewards_train/rejected': '0.024128', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15495', 'logps_train/rejected': '-111.17', 'logps_train/chosen': '-138.18', 'loss/train': '0.62913', 'examples_per_second': '29.869', 'grad_norm': '34.75', 'counters/examples': 203552, 'counters/updates': 6361}
train stats after 203584 examples: {'rewards_train/chosen': '0.19869', 'rewards_train/rejected': '0.1503', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048388', 'logps_train/rejected': '-150.3', 'logps_train/chosen': '-182.19', 'loss/train': '0.68599', 'examples_per_second': '30.464', 'grad_norm': '34', 'counters/examples': 203584, 'counters/updates': 6362}
train stats after 203616 examples: {'rewards_train/chosen': '0.099782', 'rewards_train/rejected': '0.092376', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0074061', 'logps_train/rejected': '-161.89', 'logps_train/chosen': '-156.43', 'loss/train': '0.70097', 'examples_per_second': '31.191', 'grad_norm': '32.25', 'counters/examples': 203616, 'counters/updates': 6363}
train stats after 203648 examples: {'rewards_train/chosen': '0.13928', 'rewards_train/rejected': '0.016386', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12289', 'logps_train/rejected': '-114.55', 'logps_train/chosen': '-143.38', 'loss/train': '0.64853', 'examples_per_second': '31.408', 'grad_norm': '25.875', 'counters/examples': 203648, 'counters/updates': 6364}
train stats after 203680 examples: {'rewards_train/chosen': '0.18713', 'rewards_train/rejected': '0.14589', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041237', 'logps_train/rejected': '-166.98', 'logps_train/chosen': '-130.57', 'loss/train': '0.70016', 'examples_per_second': '29.981', 'grad_norm': '44.25', 'counters/examples': 203680, 'counters/updates': 6365}
skipping logging after 203712 examples to avoid logging too frequently
train stats after 203744 examples: {'rewards_train/chosen': '0.077874', 'rewards_train/rejected': '0.07471', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0031645', 'logps_train/rejected': '-108.07', 'logps_train/chosen': '-128.68', 'loss/train': '0.7205', 'examples_per_second': '34.101', 'grad_norm': '31.375', 'counters/examples': 203744, 'counters/updates': 6367}
skipping logging after 203776 examples to avoid logging too frequently
train stats after 203808 examples: {'rewards_train/chosen': '0.16473', 'rewards_train/rejected': '0.039144', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12558', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-137.15', 'loss/train': '0.64669', 'examples_per_second': '38.934', 'grad_norm': '25', 'counters/examples': 203808, 'counters/updates': 6369}
train stats after 203840 examples: {'rewards_train/chosen': '0.18945', 'rewards_train/rejected': '0.036301', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15315', 'logps_train/rejected': '-123.87', 'logps_train/chosen': '-132.18', 'loss/train': '0.62824', 'examples_per_second': '30.627', 'grad_norm': '54', 'counters/examples': 203840, 'counters/updates': 6370}
train stats after 203872 examples: {'rewards_train/chosen': '0.23083', 'rewards_train/rejected': '0.060622', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1702', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-155.21', 'loss/train': '0.62784', 'examples_per_second': '32.272', 'grad_norm': '31.875', 'counters/examples': 203872, 'counters/updates': 6371}
train stats after 203904 examples: {'rewards_train/chosen': '0.21577', 'rewards_train/rejected': '0.12317', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0926', 'logps_train/rejected': '-134.85', 'logps_train/chosen': '-159.15', 'loss/train': '0.65944', 'examples_per_second': '31.503', 'grad_norm': '30.125', 'counters/examples': 203904, 'counters/updates': 6372}
train stats after 203936 examples: {'rewards_train/chosen': '0.14176', 'rewards_train/rejected': '0.099003', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042756', 'logps_train/rejected': '-110.82', 'logps_train/chosen': '-99.761', 'loss/train': '0.6815', 'examples_per_second': '32.404', 'grad_norm': '33.75', 'counters/examples': 203936, 'counters/updates': 6373}
skipping logging after 203968 examples to avoid logging too frequently
train stats after 204000 examples: {'rewards_train/chosen': '0.14329', 'rewards_train/rejected': '0.064746', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078541', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-130.41', 'loss/train': '0.67155', 'examples_per_second': '31.462', 'grad_norm': '29.625', 'counters/examples': 204000, 'counters/updates': 6375}
Running evaluation after 204000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.94it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.87it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 204000: {'rewards_eval/chosen': '0.18175', 'rewards_eval/rejected': '0.066107', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.11564', 'logps_eval/rejected': '-121.47', 'logps_eval/chosen': '-142.29', 'loss/eval': '0.65487'}
skipping save for non epoch
train stats after 204032 examples: {'rewards_train/chosen': '0.19122', 'rewards_train/rejected': '0.15607', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.035144', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-173.74', 'loss/train': '0.69515', 'examples_per_second': '34.308', 'grad_norm': '127.5', 'counters/examples': 204032, 'counters/updates': 6376}
train stats after 204064 examples: {'rewards_train/chosen': '0.14128', 'rewards_train/rejected': '0.13643', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0048471', 'logps_train/rejected': '-135.17', 'logps_train/chosen': '-127.2', 'loss/train': '0.71585', 'examples_per_second': '31.608', 'grad_norm': '37.75', 'counters/examples': 204064, 'counters/updates': 6377}
train stats after 204096 examples: {'rewards_train/chosen': '0.20199', 'rewards_train/rejected': '0.14853', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053453', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-135.95', 'loss/train': '0.67788', 'examples_per_second': '31.939', 'grad_norm': '29', 'counters/examples': 204096, 'counters/updates': 6378}
train stats after 204128 examples: {'rewards_train/chosen': '0.11076', 'rewards_train/rejected': '0.17284', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.062077', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-129.19', 'loss/train': '0.76431', 'examples_per_second': '32.642', 'grad_norm': '63', 'counters/examples': 204128, 'counters/updates': 6379}
skipping logging after 204160 examples to avoid logging too frequently
train stats after 204192 examples: {'rewards_train/chosen': '0.059124', 'rewards_train/rejected': '0.030074', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029051', 'logps_train/rejected': '-114.24', 'logps_train/chosen': '-118.48', 'loss/train': '0.69001', 'examples_per_second': '31.422', 'grad_norm': '26.25', 'counters/examples': 204192, 'counters/updates': 6381}
train stats after 204224 examples: {'rewards_train/chosen': '0.15344', 'rewards_train/rejected': '0.032191', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12125', 'logps_train/rejected': '-109.48', 'logps_train/chosen': '-129.58', 'loss/train': '0.6461', 'examples_per_second': '30.436', 'grad_norm': '32.75', 'counters/examples': 204224, 'counters/updates': 6382}
train stats after 204256 examples: {'rewards_train/chosen': '0.18233', 'rewards_train/rejected': '0.1134', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06893', 'logps_train/rejected': '-150.19', 'logps_train/chosen': '-167.1', 'loss/train': '0.67357', 'examples_per_second': '32.976', 'grad_norm': '46.75', 'counters/examples': 204256, 'counters/updates': 6383}
train stats after 204288 examples: {'rewards_train/chosen': '0.19058', 'rewards_train/rejected': '0.023417', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16716', 'logps_train/rejected': '-155.85', 'logps_train/chosen': '-145.98', 'loss/train': '0.65032', 'examples_per_second': '32.443', 'grad_norm': '36.25', 'counters/examples': 204288, 'counters/updates': 6384}
train stats after 204320 examples: {'rewards_train/chosen': '0.23001', 'rewards_train/rejected': '0.11944', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11057', 'logps_train/rejected': '-149.86', 'logps_train/chosen': '-163.43', 'loss/train': '0.65343', 'examples_per_second': '31.492', 'grad_norm': '39', 'counters/examples': 204320, 'counters/updates': 6385}
train stats after 204352 examples: {'rewards_train/chosen': '0.16909', 'rewards_train/rejected': '0.097067', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07202', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-166.62', 'loss/train': '0.67079', 'examples_per_second': '31.527', 'grad_norm': '32', 'counters/examples': 204352, 'counters/updates': 6386}
train stats after 204384 examples: {'rewards_train/chosen': '0.19352', 'rewards_train/rejected': '0.057778', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13574', 'logps_train/rejected': '-140.71', 'logps_train/chosen': '-168.51', 'loss/train': '0.6396', 'examples_per_second': '33.258', 'grad_norm': '40.25', 'counters/examples': 204384, 'counters/updates': 6387}
train stats after 204416 examples: {'rewards_train/chosen': '0.091083', 'rewards_train/rejected': '0.15935', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.068268', 'logps_train/rejected': '-151.89', 'logps_train/chosen': '-163.35', 'loss/train': '0.74333', 'examples_per_second': '30.031', 'grad_norm': '40.5', 'counters/examples': 204416, 'counters/updates': 6388}
train stats after 204448 examples: {'rewards_train/chosen': '0.26502', 'rewards_train/rejected': '0.067476', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19754', 'logps_train/rejected': '-178.51', 'logps_train/chosen': '-166.69', 'loss/train': '0.67858', 'examples_per_second': '30.488', 'grad_norm': '32', 'counters/examples': 204448, 'counters/updates': 6389}
train stats after 204480 examples: {'rewards_train/chosen': '0.15928', 'rewards_train/rejected': '0.10676', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052519', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-159.29', 'loss/train': '0.67821', 'examples_per_second': '31.857', 'grad_norm': '33.5', 'counters/examples': 204480, 'counters/updates': 6390}
train stats after 204512 examples: {'rewards_train/chosen': '0.12844', 'rewards_train/rejected': '0.096761', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031674', 'logps_train/rejected': '-156.65', 'logps_train/chosen': '-126.11', 'loss/train': '0.68503', 'examples_per_second': '30.989', 'grad_norm': '29.5', 'counters/examples': 204512, 'counters/updates': 6391}
train stats after 204544 examples: {'rewards_train/chosen': '0.15835', 'rewards_train/rejected': '0.10307', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.05528', 'logps_train/rejected': '-116.04', 'logps_train/chosen': '-119.61', 'loss/train': '0.68634', 'examples_per_second': '30.517', 'grad_norm': '85.5', 'counters/examples': 204544, 'counters/updates': 6392}
skipping logging after 204576 examples to avoid logging too frequently
train stats after 204608 examples: {'rewards_train/chosen': '0.15117', 'rewards_train/rejected': '0.10772', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.043443', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-155.52', 'loss/train': '0.68442', 'examples_per_second': '31.328', 'grad_norm': '29.375', 'counters/examples': 204608, 'counters/updates': 6394}
train stats after 204640 examples: {'rewards_train/chosen': '0.13045', 'rewards_train/rejected': '0.052387', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078065', 'logps_train/rejected': '-124.85', 'logps_train/chosen': '-133.55', 'loss/train': '0.66488', 'examples_per_second': '30.992', 'grad_norm': '28.875', 'counters/examples': 204640, 'counters/updates': 6395}
train stats after 204672 examples: {'rewards_train/chosen': '0.099625', 'rewards_train/rejected': '0.071635', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02799', 'logps_train/rejected': '-118.6', 'logps_train/chosen': '-110.25', 'loss/train': '0.6855', 'examples_per_second': '31.441', 'grad_norm': '56', 'counters/examples': 204672, 'counters/updates': 6396}
train stats after 204704 examples: {'rewards_train/chosen': '0.079274', 'rewards_train/rejected': '-0.0089415', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088216', 'logps_train/rejected': '-147.22', 'logps_train/chosen': '-172.8', 'loss/train': '0.66359', 'examples_per_second': '31.479', 'grad_norm': '53.5', 'counters/examples': 204704, 'counters/updates': 6397}
train stats after 204736 examples: {'rewards_train/chosen': '0.25377', 'rewards_train/rejected': '0.029612', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22416', 'logps_train/rejected': '-137.09', 'logps_train/chosen': '-147.12', 'loss/train': '0.60319', 'examples_per_second': '30.27', 'grad_norm': '37', 'counters/examples': 204736, 'counters/updates': 6398}
skipping logging after 204768 examples to avoid logging too frequently
train stats after 204800 examples: {'rewards_train/chosen': '0.17377', 'rewards_train/rejected': '0.10144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072338', 'logps_train/rejected': '-131.85', 'logps_train/chosen': '-136.6', 'loss/train': '0.68057', 'examples_per_second': '31.568', 'grad_norm': '30.375', 'counters/examples': 204800, 'counters/updates': 6400}
train stats after 204832 examples: {'rewards_train/chosen': '0.17005', 'rewards_train/rejected': '0.1189', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051147', 'logps_train/rejected': '-166.96', 'logps_train/chosen': '-172.23', 'loss/train': '0.68128', 'examples_per_second': '31.3', 'grad_norm': '38.5', 'counters/examples': 204832, 'counters/updates': 6401}
train stats after 204864 examples: {'rewards_train/chosen': '0.15588', 'rewards_train/rejected': '0.10523', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.05065', 'logps_train/rejected': '-127.9', 'logps_train/chosen': '-156.86', 'loss/train': '0.67795', 'examples_per_second': '31.453', 'grad_norm': '32', 'counters/examples': 204864, 'counters/updates': 6402}
train stats after 204896 examples: {'rewards_train/chosen': '0.084225', 'rewards_train/rejected': '0.050495', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03373', 'logps_train/rejected': '-101.9', 'logps_train/chosen': '-154.16', 'loss/train': '0.68949', 'examples_per_second': '30.339', 'grad_norm': '32.75', 'counters/examples': 204896, 'counters/updates': 6403}
train stats after 204928 examples: {'rewards_train/chosen': '0.23893', 'rewards_train/rejected': '0.071252', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16768', 'logps_train/rejected': '-103.07', 'logps_train/chosen': '-118.79', 'loss/train': '0.62141', 'examples_per_second': '30.914', 'grad_norm': '24.5', 'counters/examples': 204928, 'counters/updates': 6404}
train stats after 204960 examples: {'rewards_train/chosen': '0.1912', 'rewards_train/rejected': '0.028702', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16249', 'logps_train/rejected': '-122.13', 'logps_train/chosen': '-182.03', 'loss/train': '0.64993', 'examples_per_second': '31.578', 'grad_norm': '41.75', 'counters/examples': 204960, 'counters/updates': 6405}
train stats after 204992 examples: {'rewards_train/chosen': '0.15241', 'rewards_train/rejected': '0.022344', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13006', 'logps_train/rejected': '-135.87', 'logps_train/chosen': '-155.9', 'loss/train': '0.64123', 'examples_per_second': '31.29', 'grad_norm': '29.5', 'counters/examples': 204992, 'counters/updates': 6406}
train stats after 205024 examples: {'rewards_train/chosen': '0.15925', 'rewards_train/rejected': '0.058135', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10111', 'logps_train/rejected': '-162.77', 'logps_train/chosen': '-141.6', 'loss/train': '0.65639', 'examples_per_second': '30.025', 'grad_norm': '33.25', 'counters/examples': 205024, 'counters/updates': 6407}
train stats after 205056 examples: {'rewards_train/chosen': '0.13129', 'rewards_train/rejected': '0.024916', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10637', 'logps_train/rejected': '-127.37', 'logps_train/chosen': '-144.02', 'loss/train': '0.65515', 'examples_per_second': '31.993', 'grad_norm': '30.25', 'counters/examples': 205056, 'counters/updates': 6408}
train stats after 205088 examples: {'rewards_train/chosen': '0.17536', 'rewards_train/rejected': '0.12381', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.051555', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-144.12', 'loss/train': '0.69343', 'examples_per_second': '32.156', 'grad_norm': '32', 'counters/examples': 205088, 'counters/updates': 6409}
train stats after 205120 examples: {'rewards_train/chosen': '0.11856', 'rewards_train/rejected': '0.021011', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097551', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-134.97', 'loss/train': '0.65747', 'examples_per_second': '30.981', 'grad_norm': '27.125', 'counters/examples': 205120, 'counters/updates': 6410}
skipping logging after 205152 examples to avoid logging too frequently
train stats after 205184 examples: {'rewards_train/chosen': '0.21159', 'rewards_train/rejected': '0.034435', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17715', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-150.38', 'loss/train': '0.61913', 'examples_per_second': '31.345', 'grad_norm': '47', 'counters/examples': 205184, 'counters/updates': 6412}
train stats after 205216 examples: {'rewards_train/chosen': '0.12004', 'rewards_train/rejected': '0.054812', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065232', 'logps_train/rejected': '-126.38', 'logps_train/chosen': '-131.36', 'loss/train': '0.67109', 'examples_per_second': '30.59', 'grad_norm': '38.5', 'counters/examples': 205216, 'counters/updates': 6413}
train stats after 205248 examples: {'rewards_train/chosen': '0.18492', 'rewards_train/rejected': '0.098415', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086507', 'logps_train/rejected': '-128.21', 'logps_train/chosen': '-154.85', 'loss/train': '0.66122', 'examples_per_second': '31.467', 'grad_norm': '27.75', 'counters/examples': 205248, 'counters/updates': 6414}
train stats after 205280 examples: {'rewards_train/chosen': '0.19736', 'rewards_train/rejected': '0.12167', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.075692', 'logps_train/rejected': '-156.1', 'logps_train/chosen': '-153.15', 'loss/train': '0.66973', 'examples_per_second': '31.928', 'grad_norm': '31.375', 'counters/examples': 205280, 'counters/updates': 6415}
train stats after 205312 examples: {'rewards_train/chosen': '0.16218', 'rewards_train/rejected': '0.11803', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044149', 'logps_train/rejected': '-123.19', 'logps_train/chosen': '-156.07', 'loss/train': '0.68221', 'examples_per_second': '31.512', 'grad_norm': '33.75', 'counters/examples': 205312, 'counters/updates': 6416}
train stats after 205344 examples: {'rewards_train/chosen': '0.18816', 'rewards_train/rejected': '0.053813', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13435', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-170.49', 'loss/train': '0.64786', 'examples_per_second': '31.567', 'grad_norm': '27.875', 'counters/examples': 205344, 'counters/updates': 6417}
train stats after 205376 examples: {'rewards_train/chosen': '0.20794', 'rewards_train/rejected': '0.0078981', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20004', 'logps_train/rejected': '-110.19', 'logps_train/chosen': '-158.97', 'loss/train': '0.60949', 'examples_per_second': '31.845', 'grad_norm': '25.75', 'counters/examples': 205376, 'counters/updates': 6418}
train stats after 205408 examples: {'rewards_train/chosen': '0.10183', 'rewards_train/rejected': '0.086686', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.015147', 'logps_train/rejected': '-129.71', 'logps_train/chosen': '-150.07', 'loss/train': '0.69848', 'examples_per_second': '30.039', 'grad_norm': '30.625', 'counters/examples': 205408, 'counters/updates': 6419}
train stats after 205440 examples: {'rewards_train/chosen': '0.26086', 'rewards_train/rejected': '0.038299', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22256', 'logps_train/rejected': '-99.064', 'logps_train/chosen': '-142.78', 'loss/train': '0.62493', 'examples_per_second': '31.654', 'grad_norm': '25.625', 'counters/examples': 205440, 'counters/updates': 6420}
train stats after 205472 examples: {'rewards_train/chosen': '0.10641', 'rewards_train/rejected': '0.018611', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.087797', 'logps_train/rejected': '-110.31', 'logps_train/chosen': '-116.66', 'loss/train': '0.66368', 'examples_per_second': '31.4', 'grad_norm': '27.25', 'counters/examples': 205472, 'counters/updates': 6421}
train stats after 205504 examples: {'rewards_train/chosen': '0.20136', 'rewards_train/rejected': '0.16657', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034794', 'logps_train/rejected': '-156.79', 'logps_train/chosen': '-174.21', 'loss/train': '0.68796', 'examples_per_second': '30.281', 'grad_norm': '31.75', 'counters/examples': 205504, 'counters/updates': 6422}
skipping logging after 205536 examples to avoid logging too frequently
train stats after 205568 examples: {'rewards_train/chosen': '0.07828', 'rewards_train/rejected': '0.15665', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.078371', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-147.4', 'loss/train': '0.74569', 'examples_per_second': '31.49', 'grad_norm': '28.75', 'counters/examples': 205568, 'counters/updates': 6424}
train stats after 205600 examples: {'rewards_train/chosen': '0.27718', 'rewards_train/rejected': '0.12338', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1538', 'logps_train/rejected': '-139.71', 'logps_train/chosen': '-157.78', 'loss/train': '0.63231', 'examples_per_second': '30.116', 'grad_norm': '29.375', 'counters/examples': 205600, 'counters/updates': 6425}
train stats after 205632 examples: {'rewards_train/chosen': '0.088753', 'rewards_train/rejected': '0.074788', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013965', 'logps_train/rejected': '-150.22', 'logps_train/chosen': '-137.31', 'loss/train': '0.69702', 'examples_per_second': '30.544', 'grad_norm': '30.875', 'counters/examples': 205632, 'counters/updates': 6426}
train stats after 205664 examples: {'rewards_train/chosen': '0.13389', 'rewards_train/rejected': '0.062426', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071462', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-153.61', 'loss/train': '0.6797', 'examples_per_second': '31.51', 'grad_norm': '30.75', 'counters/examples': 205664, 'counters/updates': 6427}
train stats after 205696 examples: {'rewards_train/chosen': '0.14577', 'rewards_train/rejected': '0.033619', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11215', 'logps_train/rejected': '-89.166', 'logps_train/chosen': '-117.97', 'loss/train': '0.6479', 'examples_per_second': '30.025', 'grad_norm': '23.125', 'counters/examples': 205696, 'counters/updates': 6428}
train stats after 205728 examples: {'rewards_train/chosen': '0.1831', 'rewards_train/rejected': '0.10829', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.074804', 'logps_train/rejected': '-136.78', 'logps_train/chosen': '-139.26', 'loss/train': '0.6666', 'examples_per_second': '29.871', 'grad_norm': '33.25', 'counters/examples': 205728, 'counters/updates': 6429}
train stats after 205760 examples: {'rewards_train/chosen': '0.15232', 'rewards_train/rejected': '0.027758', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12456', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-148.62', 'loss/train': '0.64465', 'examples_per_second': '31.455', 'grad_norm': '36.75', 'counters/examples': 205760, 'counters/updates': 6430}
skipping logging after 205792 examples to avoid logging too frequently
train stats after 205824 examples: {'rewards_train/chosen': '0.17478', 'rewards_train/rejected': '0.071882', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1029', 'logps_train/rejected': '-151.52', 'logps_train/chosen': '-161.34', 'loss/train': '0.65887', 'examples_per_second': '30.617', 'grad_norm': '30.625', 'counters/examples': 205824, 'counters/updates': 6432}
train stats after 205856 examples: {'rewards_train/chosen': '0.096615', 'rewards_train/rejected': '0.010504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086112', 'logps_train/rejected': '-114.64', 'logps_train/chosen': '-133.57', 'loss/train': '0.66235', 'examples_per_second': '30.916', 'grad_norm': '27', 'counters/examples': 205856, 'counters/updates': 6433}
train stats after 205888 examples: {'rewards_train/chosen': '0.1959', 'rewards_train/rejected': '0.075499', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1204', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-144.72', 'loss/train': '0.65464', 'examples_per_second': '30.931', 'grad_norm': '26.5', 'counters/examples': 205888, 'counters/updates': 6434}
train stats after 205920 examples: {'rewards_train/chosen': '0.14564', 'rewards_train/rejected': '0.09532', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.050319', 'logps_train/rejected': '-140.2', 'logps_train/chosen': '-118.87', 'loss/train': '0.67732', 'examples_per_second': '32.047', 'grad_norm': '32.75', 'counters/examples': 205920, 'counters/updates': 6435}
train stats after 205952 examples: {'rewards_train/chosen': '0.24064', 'rewards_train/rejected': '0.014385', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.22626', 'logps_train/rejected': '-110.86', 'logps_train/chosen': '-160.77', 'loss/train': '0.60275', 'examples_per_second': '32.559', 'grad_norm': '28.625', 'counters/examples': 205952, 'counters/updates': 6436}
train stats after 205984 examples: {'rewards_train/chosen': '0.12966', 'rewards_train/rejected': '0.075205', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054455', 'logps_train/rejected': '-128.35', 'logps_train/chosen': '-170.25', 'loss/train': '0.68648', 'examples_per_second': '31.518', 'grad_norm': '35', 'counters/examples': 205984, 'counters/updates': 6437}
skipping logging after 206016 examples to avoid logging too frequently
train stats after 206048 examples: {'rewards_train/chosen': '0.16459', 'rewards_train/rejected': '0.12212', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04247', 'logps_train/rejected': '-158.74', 'logps_train/chosen': '-143.01', 'loss/train': '0.67966', 'examples_per_second': '31.31', 'grad_norm': '36', 'counters/examples': 206048, 'counters/updates': 6439}
train stats after 206080 examples: {'rewards_train/chosen': '0.25217', 'rewards_train/rejected': '0.046901', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20527', 'logps_train/rejected': '-146.3', 'logps_train/chosen': '-159.75', 'loss/train': '0.61405', 'examples_per_second': '31.372', 'grad_norm': '27.875', 'counters/examples': 206080, 'counters/updates': 6440}
train stats after 206112 examples: {'rewards_train/chosen': '0.19714', 'rewards_train/rejected': '0.15973', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037416', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-135.36', 'loss/train': '0.68982', 'examples_per_second': '32.404', 'grad_norm': '38.75', 'counters/examples': 206112, 'counters/updates': 6441}
train stats after 206144 examples: {'rewards_train/chosen': '0.23477', 'rewards_train/rejected': '0.12865', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10612', 'logps_train/rejected': '-128.7', 'logps_train/chosen': '-157.85', 'loss/train': '0.65548', 'examples_per_second': '29.992', 'grad_norm': '33.25', 'counters/examples': 206144, 'counters/updates': 6442}
train stats after 206176 examples: {'rewards_train/chosen': '0.16441', 'rewards_train/rejected': '0.039253', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12515', 'logps_train/rejected': '-99.796', 'logps_train/chosen': '-168.41', 'loss/train': '0.64284', 'examples_per_second': '31.844', 'grad_norm': '24.75', 'counters/examples': 206176, 'counters/updates': 6443}
train stats after 206208 examples: {'rewards_train/chosen': '0.15257', 'rewards_train/rejected': '0.09615', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056417', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-131.09', 'loss/train': '0.67922', 'examples_per_second': '30.656', 'grad_norm': '29.625', 'counters/examples': 206208, 'counters/updates': 6444}
train stats after 206240 examples: {'rewards_train/chosen': '0.14937', 'rewards_train/rejected': '0.070819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078551', 'logps_train/rejected': '-98.618', 'logps_train/chosen': '-101.23', 'loss/train': '0.66997', 'examples_per_second': '31.459', 'grad_norm': '29.5', 'counters/examples': 206240, 'counters/updates': 6445}
train stats after 206272 examples: {'rewards_train/chosen': '0.1402', 'rewards_train/rejected': '0.037937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10227', 'logps_train/rejected': '-97.19', 'logps_train/chosen': '-150.87', 'loss/train': '0.66296', 'examples_per_second': '30.023', 'grad_norm': '24.5', 'counters/examples': 206272, 'counters/updates': 6446}
skipping logging after 206304 examples to avoid logging too frequently
train stats after 206336 examples: {'rewards_train/chosen': '0.20072', 'rewards_train/rejected': '0.054669', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14605', 'logps_train/rejected': '-123.12', 'logps_train/chosen': '-152.78', 'loss/train': '0.64299', 'examples_per_second': '31.547', 'grad_norm': '90.5', 'counters/examples': 206336, 'counters/updates': 6448}
train stats after 206368 examples: {'rewards_train/chosen': '0.26479', 'rewards_train/rejected': '0.077501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18729', 'logps_train/rejected': '-104.19', 'logps_train/chosen': '-149.41', 'loss/train': '0.6315', 'examples_per_second': '32.101', 'grad_norm': '35', 'counters/examples': 206368, 'counters/updates': 6449}
train stats after 206400 examples: {'rewards_train/chosen': '0.22073', 'rewards_train/rejected': '0.020483', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20024', 'logps_train/rejected': '-137.32', 'logps_train/chosen': '-145.36', 'loss/train': '0.60768', 'examples_per_second': '31.559', 'grad_norm': '27.875', 'counters/examples': 206400, 'counters/updates': 6450}
train stats after 206432 examples: {'rewards_train/chosen': '0.20802', 'rewards_train/rejected': '0.20706', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.00096379', 'logps_train/rejected': '-187.72', 'logps_train/chosen': '-160.52', 'loss/train': '0.70571', 'examples_per_second': '30.594', 'grad_norm': '50.75', 'counters/examples': 206432, 'counters/updates': 6451}
skipping logging after 206464 examples to avoid logging too frequently
train stats after 206496 examples: {'rewards_train/chosen': '0.12654', 'rewards_train/rejected': '-0.018647', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14518', 'logps_train/rejected': '-139.41', 'logps_train/chosen': '-140.69', 'loss/train': '0.63749', 'examples_per_second': '25.63', 'grad_norm': '34.5', 'counters/examples': 206496, 'counters/updates': 6453}
train stats after 206528 examples: {'rewards_train/chosen': '0.12038', 'rewards_train/rejected': '0.034598', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085781', 'logps_train/rejected': '-108.91', 'logps_train/chosen': '-110.64', 'loss/train': '0.65925', 'examples_per_second': '32.173', 'grad_norm': '25.875', 'counters/examples': 206528, 'counters/updates': 6454}
train stats after 206560 examples: {'rewards_train/chosen': '0.22384', 'rewards_train/rejected': '0.040024', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18381', 'logps_train/rejected': '-147.09', 'logps_train/chosen': '-151.86', 'loss/train': '0.6255', 'examples_per_second': '32.906', 'grad_norm': '28.875', 'counters/examples': 206560, 'counters/updates': 6455}
train stats after 206592 examples: {'rewards_train/chosen': '0.19582', 'rewards_train/rejected': '0.0040197', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19181', 'logps_train/rejected': '-160.43', 'logps_train/chosen': '-177.88', 'loss/train': '0.61471', 'examples_per_second': '31.431', 'grad_norm': '41.75', 'counters/examples': 206592, 'counters/updates': 6456}
skipping logging after 206624 examples to avoid logging too frequently
train stats after 206656 examples: {'rewards_train/chosen': '0.17982', 'rewards_train/rejected': '0.024152', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15567', 'logps_train/rejected': '-109.81', 'logps_train/chosen': '-119.58', 'loss/train': '0.66024', 'examples_per_second': '39.079', 'grad_norm': '34.5', 'counters/examples': 206656, 'counters/updates': 6458}
train stats after 206688 examples: {'rewards_train/chosen': '0.19738', 'rewards_train/rejected': '0.029441', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16793', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-139.65', 'loss/train': '0.6233', 'examples_per_second': '30.404', 'grad_norm': '30', 'counters/examples': 206688, 'counters/updates': 6459}
train stats after 206720 examples: {'rewards_train/chosen': '0.1847', 'rewards_train/rejected': '0.046586', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13811', 'logps_train/rejected': '-94.964', 'logps_train/chosen': '-132.52', 'loss/train': '0.63715', 'examples_per_second': '32.516', 'grad_norm': '22.25', 'counters/examples': 206720, 'counters/updates': 6460}
train stats after 206752 examples: {'rewards_train/chosen': '0.29143', 'rewards_train/rejected': '0.07743', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.214', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-168.3', 'loss/train': '0.6132', 'examples_per_second': '32.012', 'grad_norm': '27.875', 'counters/examples': 206752, 'counters/updates': 6461}
skipping logging after 206784 examples to avoid logging too frequently
train stats after 206816 examples: {'rewards_train/chosen': '0.088577', 'rewards_train/rejected': '0.040159', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.048418', 'logps_train/rejected': '-117.99', 'logps_train/chosen': '-110.91', 'loss/train': '0.68417', 'examples_per_second': '34.084', 'grad_norm': '30.125', 'counters/examples': 206816, 'counters/updates': 6463}
train stats after 206848 examples: {'rewards_train/chosen': '0.18135', 'rewards_train/rejected': '0.02661', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15474', 'logps_train/rejected': '-124.02', 'logps_train/chosen': '-151.39', 'loss/train': '0.62653', 'examples_per_second': '30.955', 'grad_norm': '27', 'counters/examples': 206848, 'counters/updates': 6464}
train stats after 206880 examples: {'rewards_train/chosen': '0.22277', 'rewards_train/rejected': '0.05361', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16916', 'logps_train/rejected': '-136.47', 'logps_train/chosen': '-139.81', 'loss/train': '0.62691', 'examples_per_second': '30.934', 'grad_norm': '35', 'counters/examples': 206880, 'counters/updates': 6465}
train stats after 206912 examples: {'rewards_train/chosen': '0.12241', 'rewards_train/rejected': '0.018671', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10374', 'logps_train/rejected': '-146.26', 'logps_train/chosen': '-143.17', 'loss/train': '0.65547', 'examples_per_second': '31.98', 'grad_norm': '34.5', 'counters/examples': 206912, 'counters/updates': 6466}
train stats after 206944 examples: {'rewards_train/chosen': '0.21774', 'rewards_train/rejected': '0.0027513', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21499', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-134.56', 'loss/train': '0.60088', 'examples_per_second': '32.065', 'grad_norm': '31.25', 'counters/examples': 206944, 'counters/updates': 6467}
train stats after 206976 examples: {'rewards_train/chosen': '0.20498', 'rewards_train/rejected': '0.043471', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16151', 'logps_train/rejected': '-133.05', 'logps_train/chosen': '-147.63', 'loss/train': '0.63272', 'examples_per_second': '31.992', 'grad_norm': '26.75', 'counters/examples': 206976, 'counters/updates': 6468}
train stats after 207008 examples: {'rewards_train/chosen': '0.2328', 'rewards_train/rejected': '0.12239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11041', 'logps_train/rejected': '-146.31', 'logps_train/chosen': '-167.03', 'loss/train': '0.65758', 'examples_per_second': '31.294', 'grad_norm': '36', 'counters/examples': 207008, 'counters/updates': 6469}
train stats after 207040 examples: {'rewards_train/chosen': '0.20785', 'rewards_train/rejected': '0.11761', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090239', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-129.83', 'loss/train': '0.6643', 'examples_per_second': '31.448', 'grad_norm': '24.5', 'counters/examples': 207040, 'counters/updates': 6470}
train stats after 207072 examples: {'rewards_train/chosen': '0.15034', 'rewards_train/rejected': '0.040679', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10966', 'logps_train/rejected': '-109.86', 'logps_train/chosen': '-138.2', 'loss/train': '0.64602', 'examples_per_second': '32.153', 'grad_norm': '26.875', 'counters/examples': 207072, 'counters/updates': 6471}
train stats after 207104 examples: {'rewards_train/chosen': '0.29668', 'rewards_train/rejected': '0.14283', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15385', 'logps_train/rejected': '-175.79', 'logps_train/chosen': '-173.06', 'loss/train': '0.65817', 'examples_per_second': '31.415', 'grad_norm': '39', 'counters/examples': 207104, 'counters/updates': 6472}
train stats after 207136 examples: {'rewards_train/chosen': '0.12534', 'rewards_train/rejected': '-0.024137', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14947', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-114.39', 'loss/train': '0.62695', 'examples_per_second': '32.377', 'grad_norm': '23.875', 'counters/examples': 207136, 'counters/updates': 6473}
train stats after 207168 examples: {'rewards_train/chosen': '0.086994', 'rewards_train/rejected': '0.087828', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.00083339', 'logps_train/rejected': '-153.45', 'logps_train/chosen': '-143.63', 'loss/train': '0.71095', 'examples_per_second': '31.494', 'grad_norm': '46.75', 'counters/examples': 207168, 'counters/updates': 6474}
train stats after 207200 examples: {'rewards_train/chosen': '0.16436', 'rewards_train/rejected': '0.0324', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13196', 'logps_train/rejected': '-109.52', 'logps_train/chosen': '-153.04', 'loss/train': '0.63641', 'examples_per_second': '32.086', 'grad_norm': '25.75', 'counters/examples': 207200, 'counters/updates': 6475}
train stats after 207232 examples: {'rewards_train/chosen': '0.1162', 'rewards_train/rejected': '0.033501', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082697', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-131.15', 'loss/train': '0.66393', 'examples_per_second': '32.157', 'grad_norm': '30.625', 'counters/examples': 207232, 'counters/updates': 6476}
train stats after 207264 examples: {'rewards_train/chosen': '0.16532', 'rewards_train/rejected': '0.14181', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023507', 'logps_train/rejected': '-133.44', 'logps_train/chosen': '-146.73', 'loss/train': '0.69067', 'examples_per_second': '30.44', 'grad_norm': '39', 'counters/examples': 207264, 'counters/updates': 6477}
train stats after 207296 examples: {'rewards_train/chosen': '0.093191', 'rewards_train/rejected': '0.052122', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04107', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-132.37', 'loss/train': '0.67961', 'examples_per_second': '32.771', 'grad_norm': '29.375', 'counters/examples': 207296, 'counters/updates': 6478}
skipping logging after 207328 examples to avoid logging too frequently
train stats after 207360 examples: {'rewards_train/chosen': '0.20465', 'rewards_train/rejected': '0.10162', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10304', 'logps_train/rejected': '-132.25', 'logps_train/chosen': '-169.89', 'loss/train': '0.65524', 'examples_per_second': '31.264', 'grad_norm': '30.75', 'counters/examples': 207360, 'counters/updates': 6480}
train stats after 207392 examples: {'rewards_train/chosen': '0.21309', 'rewards_train/rejected': '0.079958', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13313', 'logps_train/rejected': '-146.51', 'logps_train/chosen': '-188.91', 'loss/train': '0.64522', 'examples_per_second': '30.533', 'grad_norm': '41.25', 'counters/examples': 207392, 'counters/updates': 6481}
train stats after 207424 examples: {'rewards_train/chosen': '0.20952', 'rewards_train/rejected': '0.022852', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18667', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-139.92', 'loss/train': '0.63074', 'examples_per_second': '33.127', 'grad_norm': '25.875', 'counters/examples': 207424, 'counters/updates': 6482}
train stats after 207456 examples: {'rewards_train/chosen': '0.2174', 'rewards_train/rejected': '0.096907', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12049', 'logps_train/rejected': '-126.65', 'logps_train/chosen': '-137.97', 'loss/train': '0.64814', 'examples_per_second': '30.646', 'grad_norm': '54.5', 'counters/examples': 207456, 'counters/updates': 6483}
train stats after 207488 examples: {'rewards_train/chosen': '0.16293', 'rewards_train/rejected': '0.10004', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.062895', 'logps_train/rejected': '-112', 'logps_train/chosen': '-100.34', 'loss/train': '0.66921', 'examples_per_second': '32.692', 'grad_norm': '31.375', 'counters/examples': 207488, 'counters/updates': 6484}
train stats after 207520 examples: {'rewards_train/chosen': '0.21792', 'rewards_train/rejected': '0.051713', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16621', 'logps_train/rejected': '-121.44', 'logps_train/chosen': '-147.04', 'loss/train': '0.62108', 'examples_per_second': '31.472', 'grad_norm': '35.5', 'counters/examples': 207520, 'counters/updates': 6485}
train stats after 207552 examples: {'rewards_train/chosen': '0.14528', 'rewards_train/rejected': '0.057428', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087852', 'logps_train/rejected': '-109.02', 'logps_train/chosen': '-147.67', 'loss/train': '0.66202', 'examples_per_second': '30.513', 'grad_norm': '30.875', 'counters/examples': 207552, 'counters/updates': 6486}
train stats after 207584 examples: {'rewards_train/chosen': '0.071646', 'rewards_train/rejected': '-0.0024375', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074083', 'logps_train/rejected': '-108.87', 'logps_train/chosen': '-139.11', 'loss/train': '0.66696', 'examples_per_second': '32.495', 'grad_norm': '30.625', 'counters/examples': 207584, 'counters/updates': 6487}
train stats after 207616 examples: {'rewards_train/chosen': '0.20332', 'rewards_train/rejected': '0.13738', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.065941', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-132.58', 'loss/train': '0.67231', 'examples_per_second': '30.511', 'grad_norm': '46.25', 'counters/examples': 207616, 'counters/updates': 6488}
train stats after 207648 examples: {'rewards_train/chosen': '0.14255', 'rewards_train/rejected': '0.042298', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10025', 'logps_train/rejected': '-129.55', 'logps_train/chosen': '-173.1', 'loss/train': '0.65801', 'examples_per_second': '32.382', 'grad_norm': '28.125', 'counters/examples': 207648, 'counters/updates': 6489}
train stats after 207680 examples: {'rewards_train/chosen': '0.085651', 'rewards_train/rejected': '0.10053', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.014882', 'logps_train/rejected': '-108.43', 'logps_train/chosen': '-115.12', 'loss/train': '0.7075', 'examples_per_second': '31.452', 'grad_norm': '31.25', 'counters/examples': 207680, 'counters/updates': 6490}
train stats after 207712 examples: {'rewards_train/chosen': '0.26008', 'rewards_train/rejected': '0.0022141', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25786', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-144.76', 'loss/train': '0.5904', 'examples_per_second': '30.311', 'grad_norm': '27.875', 'counters/examples': 207712, 'counters/updates': 6491}
train stats after 207744 examples: {'rewards_train/chosen': '0.27768', 'rewards_train/rejected': '0.13827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1394', 'logps_train/rejected': '-160.21', 'logps_train/chosen': '-173.94', 'loss/train': '0.63582', 'examples_per_second': '32.139', 'grad_norm': '32', 'counters/examples': 207744, 'counters/updates': 6492}
train stats after 207776 examples: {'rewards_train/chosen': '0.1502', 'rewards_train/rejected': '0.058226', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.091974', 'logps_train/rejected': '-122.23', 'logps_train/chosen': '-166.58', 'loss/train': '0.66125', 'examples_per_second': '30.437', 'grad_norm': '44', 'counters/examples': 207776, 'counters/updates': 6493}
train stats after 207808 examples: {'rewards_train/chosen': '0.17001', 'rewards_train/rejected': '0.081512', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0885', 'logps_train/rejected': '-124.45', 'logps_train/chosen': '-145.73', 'loss/train': '0.66946', 'examples_per_second': '24.046', 'grad_norm': '35.25', 'counters/examples': 207808, 'counters/updates': 6494}
skipping logging after 207840 examples to avoid logging too frequently
train stats after 207872 examples: {'rewards_train/chosen': '0.2044', 'rewards_train/rejected': '0.1232', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081199', 'logps_train/rejected': '-117.32', 'logps_train/chosen': '-151.43', 'loss/train': '0.67018', 'examples_per_second': '31.073', 'grad_norm': '39.75', 'counters/examples': 207872, 'counters/updates': 6496}
train stats after 207904 examples: {'rewards_train/chosen': '0.11354', 'rewards_train/rejected': '0.12216', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0086244', 'logps_train/rejected': '-148.16', 'logps_train/chosen': '-166.3', 'loss/train': '0.71731', 'examples_per_second': '24.278', 'grad_norm': '67.5', 'counters/examples': 207904, 'counters/updates': 6497}
train stats after 207936 examples: {'rewards_train/chosen': '0.30453', 'rewards_train/rejected': '0.12188', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18265', 'logps_train/rejected': '-148.55', 'logps_train/chosen': '-222.44', 'loss/train': '0.62568', 'examples_per_second': '31.44', 'grad_norm': '34.5', 'counters/examples': 207936, 'counters/updates': 6498}
train stats after 207968 examples: {'rewards_train/chosen': '0.25406', 'rewards_train/rejected': '0.11512', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13895', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-154.07', 'loss/train': '0.6428', 'examples_per_second': '31.428', 'grad_norm': '32.75', 'counters/examples': 207968, 'counters/updates': 6499}
skipping logging after 208000 examples to avoid logging too frequently
train stats after 208032 examples: {'rewards_train/chosen': '0.13378', 'rewards_train/rejected': '0.08317', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050608', 'logps_train/rejected': '-161.72', 'logps_train/chosen': '-138.28', 'loss/train': '0.6859', 'examples_per_second': '31.883', 'grad_norm': '30.75', 'counters/examples': 208032, 'counters/updates': 6501}
train stats after 208064 examples: {'rewards_train/chosen': '0.038346', 'rewards_train/rejected': '-0.098555', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1369', 'logps_train/rejected': '-152.62', 'logps_train/chosen': '-130.38', 'loss/train': '0.67548', 'examples_per_second': '31.517', 'grad_norm': '35.25', 'counters/examples': 208064, 'counters/updates': 6502}
train stats after 208096 examples: {'rewards_train/chosen': '0.22728', 'rewards_train/rejected': '0.038692', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18858', 'logps_train/rejected': '-153.35', 'logps_train/chosen': '-143.26', 'loss/train': '0.61306', 'examples_per_second': '31.541', 'grad_norm': '26', 'counters/examples': 208096, 'counters/updates': 6503}
train stats after 208128 examples: {'rewards_train/chosen': '0.15055', 'rewards_train/rejected': '0.040924', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10963', 'logps_train/rejected': '-111.03', 'logps_train/chosen': '-175.98', 'loss/train': '0.6553', 'examples_per_second': '30.874', 'grad_norm': '39.75', 'counters/examples': 208128, 'counters/updates': 6504}
train stats after 208160 examples: {'rewards_train/chosen': '0.12257', 'rewards_train/rejected': '0.095907', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.026664', 'logps_train/rejected': '-142.8', 'logps_train/chosen': '-144.15', 'loss/train': '0.69237', 'examples_per_second': '30.823', 'grad_norm': '33.25', 'counters/examples': 208160, 'counters/updates': 6505}
skipping logging after 208192 examples to avoid logging too frequently
train stats after 208224 examples: {'rewards_train/chosen': '0.16242', 'rewards_train/rejected': '0.060889', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10153', 'logps_train/rejected': '-148.57', 'logps_train/chosen': '-182.48', 'loss/train': '0.65874', 'examples_per_second': '34.533', 'grad_norm': '49.75', 'counters/examples': 208224, 'counters/updates': 6507}
skipping logging after 208256 examples to avoid logging too frequently
train stats after 208288 examples: {'rewards_train/chosen': '0.17876', 'rewards_train/rejected': '0.12647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052286', 'logps_train/rejected': '-124.17', 'logps_train/chosen': '-119.85', 'loss/train': '0.67842', 'examples_per_second': '30.384', 'grad_norm': '33', 'counters/examples': 208288, 'counters/updates': 6509}
skipping logging after 208320 examples to avoid logging too frequently
train stats after 208352 examples: {'rewards_train/chosen': '0.083726', 'rewards_train/rejected': '0.086061', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.002335', 'logps_train/rejected': '-144.64', 'logps_train/chosen': '-87.193', 'loss/train': '0.70066', 'examples_per_second': '30.909', 'grad_norm': '41.75', 'counters/examples': 208352, 'counters/updates': 6511}
train stats after 208384 examples: {'rewards_train/chosen': '0.18343', 'rewards_train/rejected': '0.062003', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12142', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-115.78', 'loss/train': '0.64484', 'examples_per_second': '31.492', 'grad_norm': '28.375', 'counters/examples': 208384, 'counters/updates': 6512}
train stats after 208416 examples: {'rewards_train/chosen': '0.13379', 'rewards_train/rejected': '0.031914', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10187', 'logps_train/rejected': '-145.69', 'logps_train/chosen': '-132.35', 'loss/train': '0.65363', 'examples_per_second': '31.543', 'grad_norm': '31.5', 'counters/examples': 208416, 'counters/updates': 6513}
train stats after 208448 examples: {'rewards_train/chosen': '0.14624', 'rewards_train/rejected': '0.063047', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.083197', 'logps_train/rejected': '-99.462', 'logps_train/chosen': '-174.02', 'loss/train': '0.66526', 'examples_per_second': '31.033', 'grad_norm': '30.875', 'counters/examples': 208448, 'counters/updates': 6514}
train stats after 208480 examples: {'rewards_train/chosen': '0.15399', 'rewards_train/rejected': '0.041133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11286', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-150.74', 'loss/train': '0.65284', 'examples_per_second': '30.077', 'grad_norm': '29.375', 'counters/examples': 208480, 'counters/updates': 6515}
train stats after 208512 examples: {'rewards_train/chosen': '0.19604', 'rewards_train/rejected': '0.044095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15194', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-145.03', 'loss/train': '0.63216', 'examples_per_second': '30.504', 'grad_norm': '25.75', 'counters/examples': 208512, 'counters/updates': 6516}
skipping logging after 208544 examples to avoid logging too frequently
train stats after 208576 examples: {'rewards_train/chosen': '0.18328', 'rewards_train/rejected': '0.0059666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17731', 'logps_train/rejected': '-104.46', 'logps_train/chosen': '-133.59', 'loss/train': '0.64515', 'examples_per_second': '30.996', 'grad_norm': '29', 'counters/examples': 208576, 'counters/updates': 6518}
train stats after 208608 examples: {'rewards_train/chosen': '0.32829', 'rewards_train/rejected': '0.22769', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1006', 'logps_train/rejected': '-167.89', 'logps_train/chosen': '-166.3', 'loss/train': '0.71818', 'examples_per_second': '32.309', 'grad_norm': '196', 'counters/examples': 208608, 'counters/updates': 6519}
skipping logging after 208640 examples to avoid logging too frequently
train stats after 208672 examples: {'rewards_train/chosen': '0.19006', 'rewards_train/rejected': '0.25622', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.066165', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-168.08', 'loss/train': '0.75811', 'examples_per_second': '32.579', 'grad_norm': '44', 'counters/examples': 208672, 'counters/updates': 6521}
skipping logging after 208704 examples to avoid logging too frequently
train stats after 208736 examples: {'rewards_train/chosen': '0.1517', 'rewards_train/rejected': '0.011162', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14053', 'logps_train/rejected': '-141.87', 'logps_train/chosen': '-138.03', 'loss/train': '0.64137', 'examples_per_second': '33.862', 'grad_norm': '28.5', 'counters/examples': 208736, 'counters/updates': 6523}
train stats after 208768 examples: {'rewards_train/chosen': '0.17105', 'rewards_train/rejected': '0.12075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.050293', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-117.22', 'loss/train': '0.69282', 'examples_per_second': '30.508', 'grad_norm': '35.5', 'counters/examples': 208768, 'counters/updates': 6524}
skipping logging after 208800 examples to avoid logging too frequently
train stats after 208832 examples: {'rewards_train/chosen': '0.20482', 'rewards_train/rejected': '0.027634', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17719', 'logps_train/rejected': '-103.79', 'logps_train/chosen': '-119.38', 'loss/train': '0.62953', 'examples_per_second': '35.785', 'grad_norm': '28.75', 'counters/examples': 208832, 'counters/updates': 6526}
train stats after 208864 examples: {'rewards_train/chosen': '0.062853', 'rewards_train/rejected': '-0.022352', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085206', 'logps_train/rejected': '-136.93', 'logps_train/chosen': '-149.3', 'loss/train': '0.66509', 'examples_per_second': '31.332', 'grad_norm': '32.5', 'counters/examples': 208864, 'counters/updates': 6527}
train stats after 208896 examples: {'rewards_train/chosen': '0.15788', 'rewards_train/rejected': '0.043114', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11476', 'logps_train/rejected': '-118.14', 'logps_train/chosen': '-125.71', 'loss/train': '0.65204', 'examples_per_second': '31.367', 'grad_norm': '36.25', 'counters/examples': 208896, 'counters/updates': 6528}
skipping logging after 208928 examples to avoid logging too frequently
train stats after 208960 examples: {'rewards_train/chosen': '0.092702', 'rewards_train/rejected': '0.043093', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04961', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-131.13', 'loss/train': '0.6781', 'examples_per_second': '31.408', 'grad_norm': '24.25', 'counters/examples': 208960, 'counters/updates': 6530}
train stats after 208992 examples: {'rewards_train/chosen': '0.11601', 'rewards_train/rejected': '0.025838', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090172', 'logps_train/rejected': '-141.2', 'logps_train/chosen': '-173.46', 'loss/train': '0.66422', 'examples_per_second': '30.001', 'grad_norm': '27.375', 'counters/examples': 208992, 'counters/updates': 6531}
train stats after 209024 examples: {'rewards_train/chosen': '0.22333', 'rewards_train/rejected': '0.023514', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19982', 'logps_train/rejected': '-114.71', 'logps_train/chosen': '-128.84', 'loss/train': '0.61845', 'examples_per_second': '31.543', 'grad_norm': '33', 'counters/examples': 209024, 'counters/updates': 6532}
train stats after 209056 examples: {'rewards_train/chosen': '0.19995', 'rewards_train/rejected': '0.032022', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16793', 'logps_train/rejected': '-110.08', 'logps_train/chosen': '-174', 'loss/train': '0.62445', 'examples_per_second': '32.535', 'grad_norm': '60.25', 'counters/examples': 209056, 'counters/updates': 6533}
train stats after 209088 examples: {'rewards_train/chosen': '0.25899', 'rewards_train/rejected': '0.043391', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.2156', 'logps_train/rejected': '-102.97', 'logps_train/chosen': '-155.39', 'loss/train': '0.60369', 'examples_per_second': '33.321', 'grad_norm': '24.625', 'counters/examples': 209088, 'counters/updates': 6534}
train stats after 209120 examples: {'rewards_train/chosen': '0.17371', 'rewards_train/rejected': '0.13175', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041957', 'logps_train/rejected': '-136.39', 'logps_train/chosen': '-144.18', 'loss/train': '0.69088', 'examples_per_second': '32.782', 'grad_norm': '39', 'counters/examples': 209120, 'counters/updates': 6535}
train stats after 209152 examples: {'rewards_train/chosen': '0.20737', 'rewards_train/rejected': '0.019359', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.18801', 'logps_train/rejected': '-131.59', 'logps_train/chosen': '-167.34', 'loss/train': '0.61534', 'examples_per_second': '31.228', 'grad_norm': '31', 'counters/examples': 209152, 'counters/updates': 6536}
train stats after 209184 examples: {'rewards_train/chosen': '0.071976', 'rewards_train/rejected': '0.010703', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061273', 'logps_train/rejected': '-146.15', 'logps_train/chosen': '-143.97', 'loss/train': '0.67154', 'examples_per_second': '30.866', 'grad_norm': '45.75', 'counters/examples': 209184, 'counters/updates': 6537}
train stats after 209216 examples: {'rewards_train/chosen': '0.11042', 'rewards_train/rejected': '0.0084242', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.102', 'logps_train/rejected': '-149.44', 'logps_train/chosen': '-122.03', 'loss/train': '0.64999', 'examples_per_second': '31.472', 'grad_norm': '24.25', 'counters/examples': 209216, 'counters/updates': 6538}
train stats after 209248 examples: {'rewards_train/chosen': '0.061926', 'rewards_train/rejected': '-0.040748', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10267', 'logps_train/rejected': '-154.21', 'logps_train/chosen': '-156.51', 'loss/train': '0.65082', 'examples_per_second': '31.492', 'grad_norm': '31.75', 'counters/examples': 209248, 'counters/updates': 6539}
train stats after 209280 examples: {'rewards_train/chosen': '0.21847', 'rewards_train/rejected': '0.083721', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13475', 'logps_train/rejected': '-114.91', 'logps_train/chosen': '-165.56', 'loss/train': '0.63999', 'examples_per_second': '31.469', 'grad_norm': '25.625', 'counters/examples': 209280, 'counters/updates': 6540}
skipping logging after 209312 examples to avoid logging too frequently
train stats after 209344 examples: {'rewards_train/chosen': '0.19825', 'rewards_train/rejected': '0.0036319', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19462', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-169.91', 'loss/train': '0.62133', 'examples_per_second': '33.199', 'grad_norm': '28', 'counters/examples': 209344, 'counters/updates': 6542}
skipping logging after 209376 examples to avoid logging too frequently
train stats after 209408 examples: {'rewards_train/chosen': '0.12697', 'rewards_train/rejected': '0.0386', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088368', 'logps_train/rejected': '-123.73', 'logps_train/chosen': '-119.36', 'loss/train': '0.65933', 'examples_per_second': '31.529', 'grad_norm': '27.25', 'counters/examples': 209408, 'counters/updates': 6544}
train stats after 209440 examples: {'rewards_train/chosen': '0.16414', 'rewards_train/rejected': '0.047036', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11711', 'logps_train/rejected': '-95.438', 'logps_train/chosen': '-115.49', 'loss/train': '0.64345', 'examples_per_second': '31.981', 'grad_norm': '27.75', 'counters/examples': 209440, 'counters/updates': 6545}
train stats after 209472 examples: {'rewards_train/chosen': '0.16291', 'rewards_train/rejected': '0.11971', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.043202', 'logps_train/rejected': '-107.5', 'logps_train/chosen': '-97.351', 'loss/train': '0.67958', 'examples_per_second': '29.959', 'grad_norm': '26.125', 'counters/examples': 209472, 'counters/updates': 6546}
skipping logging after 209504 examples to avoid logging too frequently
train stats after 209536 examples: {'rewards_train/chosen': '0.27138', 'rewards_train/rejected': '0.052805', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21858', 'logps_train/rejected': '-109.43', 'logps_train/chosen': '-145.65', 'loss/train': '0.60178', 'examples_per_second': '34.056', 'grad_norm': '26.875', 'counters/examples': 209536, 'counters/updates': 6548}
train stats after 209568 examples: {'rewards_train/chosen': '0.1787', 'rewards_train/rejected': '0.083938', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094764', 'logps_train/rejected': '-157.56', 'logps_train/chosen': '-194.61', 'loss/train': '0.66649', 'examples_per_second': '31.462', 'grad_norm': '38.75', 'counters/examples': 209568, 'counters/updates': 6549}
train stats after 209600 examples: {'rewards_train/chosen': '0.16704', 'rewards_train/rejected': '0.12334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.043702', 'logps_train/rejected': '-142.16', 'logps_train/chosen': '-136.45', 'loss/train': '0.68773', 'examples_per_second': '31.125', 'grad_norm': '31.5', 'counters/examples': 209600, 'counters/updates': 6550}
train stats after 209632 examples: {'rewards_train/chosen': '0.15031', 'rewards_train/rejected': '0.10687', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.043441', 'logps_train/rejected': '-101.22', 'logps_train/chosen': '-121.39', 'loss/train': '0.67793', 'examples_per_second': '30.004', 'grad_norm': '35.25', 'counters/examples': 209632, 'counters/updates': 6551}
skipping logging after 209664 examples to avoid logging too frequently
train stats after 209696 examples: {'rewards_train/chosen': '0.2159', 'rewards_train/rejected': '0.058533', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15737', 'logps_train/rejected': '-140.73', 'logps_train/chosen': '-162.84', 'loss/train': '0.63073', 'examples_per_second': '29.995', 'grad_norm': '27', 'counters/examples': 209696, 'counters/updates': 6553}
train stats after 209728 examples: {'rewards_train/chosen': '0.1395', 'rewards_train/rejected': '0.021026', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11847', 'logps_train/rejected': '-146.23', 'logps_train/chosen': '-118.66', 'loss/train': '0.64722', 'examples_per_second': '31.513', 'grad_norm': '36', 'counters/examples': 209728, 'counters/updates': 6554}
skipping logging after 209760 examples to avoid logging too frequently
train stats after 209792 examples: {'rewards_train/chosen': '0.1368', 'rewards_train/rejected': '0.013451', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12335', 'logps_train/rejected': '-142.17', 'logps_train/chosen': '-140.82', 'loss/train': '0.6476', 'examples_per_second': '31.391', 'grad_norm': '26.125', 'counters/examples': 209792, 'counters/updates': 6556}
train stats after 209824 examples: {'rewards_train/chosen': '0.13922', 'rewards_train/rejected': '0.044577', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.094645', 'logps_train/rejected': '-111.47', 'logps_train/chosen': '-145.45', 'loss/train': '0.66019', 'examples_per_second': '30.58', 'grad_norm': '29.625', 'counters/examples': 209824, 'counters/updates': 6557}
train stats after 209856 examples: {'rewards_train/chosen': '0.12945', 'rewards_train/rejected': '0.078991', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050463', 'logps_train/rejected': '-128.47', 'logps_train/chosen': '-141.74', 'loss/train': '0.68517', 'examples_per_second': '31.634', 'grad_norm': '34.75', 'counters/examples': 209856, 'counters/updates': 6558}
train stats after 209888 examples: {'rewards_train/chosen': '0.19058', 'rewards_train/rejected': '0.02271', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16787', 'logps_train/rejected': '-112.56', 'logps_train/chosen': '-176.69', 'loss/train': '0.62949', 'examples_per_second': '31.445', 'grad_norm': '32', 'counters/examples': 209888, 'counters/updates': 6559}
train stats after 209920 examples: {'rewards_train/chosen': '0.17374', 'rewards_train/rejected': '-0.03382', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20756', 'logps_train/rejected': '-157.21', 'logps_train/chosen': '-122.29', 'loss/train': '0.60873', 'examples_per_second': '32.275', 'grad_norm': '33.25', 'counters/examples': 209920, 'counters/updates': 6560}
train stats after 209952 examples: {'rewards_train/chosen': '0.1761', 'rewards_train/rejected': '0.1962', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.020103', 'logps_train/rejected': '-148.6', 'logps_train/chosen': '-134.47', 'loss/train': '0.71796', 'examples_per_second': '32.226', 'grad_norm': '34.5', 'counters/examples': 209952, 'counters/updates': 6561}
train stats after 209984 examples: {'rewards_train/chosen': '0.26377', 'rewards_train/rejected': '0.12845', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13532', 'logps_train/rejected': '-157.22', 'logps_train/chosen': '-153.25', 'loss/train': '0.6383', 'examples_per_second': '32.068', 'grad_norm': '31', 'counters/examples': 209984, 'counters/updates': 6562}
skipping logging after 210016 examples to avoid logging too frequently
train stats after 210048 examples: {'rewards_train/chosen': '0.18185', 'rewards_train/rejected': '0.0069974', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17486', 'logps_train/rejected': '-120.45', 'logps_train/chosen': '-181.55', 'loss/train': '0.62733', 'examples_per_second': '31.328', 'grad_norm': '29.625', 'counters/examples': 210048, 'counters/updates': 6564}
train stats after 210080 examples: {'rewards_train/chosen': '0.1804', 'rewards_train/rejected': '0.032702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1477', 'logps_train/rejected': '-116.94', 'logps_train/chosen': '-120.14', 'loss/train': '0.62794', 'examples_per_second': '31.98', 'grad_norm': '28.5', 'counters/examples': 210080, 'counters/updates': 6565}
train stats after 210112 examples: {'rewards_train/chosen': '0.26912', 'rewards_train/rejected': '0.088923', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1802', 'logps_train/rejected': '-152.83', 'logps_train/chosen': '-172.93', 'loss/train': '0.63405', 'examples_per_second': '31.487', 'grad_norm': '30', 'counters/examples': 210112, 'counters/updates': 6566}
train stats after 210144 examples: {'rewards_train/chosen': '0.16866', 'rewards_train/rejected': '0.083767', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084892', 'logps_train/rejected': '-115.56', 'logps_train/chosen': '-132.03', 'loss/train': '0.65968', 'examples_per_second': '31.869', 'grad_norm': '36.5', 'counters/examples': 210144, 'counters/updates': 6567}
train stats after 210176 examples: {'rewards_train/chosen': '0.19395', 'rewards_train/rejected': '-0.017736', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21168', 'logps_train/rejected': '-147.35', 'logps_train/chosen': '-156.66', 'loss/train': '0.60635', 'examples_per_second': '30.508', 'grad_norm': '27.5', 'counters/examples': 210176, 'counters/updates': 6568}
train stats after 210208 examples: {'rewards_train/chosen': '0.18456', 'rewards_train/rejected': '0.099948', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084614', 'logps_train/rejected': '-144.83', 'logps_train/chosen': '-181.55', 'loss/train': '0.65832', 'examples_per_second': '30.063', 'grad_norm': '37.25', 'counters/examples': 210208, 'counters/updates': 6569}
train stats after 210240 examples: {'rewards_train/chosen': '0.15886', 'rewards_train/rejected': '0.11047', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048393', 'logps_train/rejected': '-101', 'logps_train/chosen': '-147.74', 'loss/train': '0.67736', 'examples_per_second': '30.336', 'grad_norm': '41.5', 'counters/examples': 210240, 'counters/updates': 6570}
train stats after 210272 examples: {'rewards_train/chosen': '0.17743', 'rewards_train/rejected': '0.044115', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13331', 'logps_train/rejected': '-103.46', 'logps_train/chosen': '-122.83', 'loss/train': '0.64755', 'examples_per_second': '31.988', 'grad_norm': '30.625', 'counters/examples': 210272, 'counters/updates': 6571}
train stats after 210304 examples: {'rewards_train/chosen': '0.096887', 'rewards_train/rejected': '-0.03349', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13038', 'logps_train/rejected': '-93.252', 'logps_train/chosen': '-107.69', 'loss/train': '0.63603', 'examples_per_second': '30.081', 'grad_norm': '27.375', 'counters/examples': 210304, 'counters/updates': 6572}
train stats after 210336 examples: {'rewards_train/chosen': '0.095679', 'rewards_train/rejected': '0.028434', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.067246', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-127.04', 'loss/train': '0.67717', 'examples_per_second': '32.204', 'grad_norm': '30', 'counters/examples': 210336, 'counters/updates': 6573}
train stats after 210368 examples: {'rewards_train/chosen': '0.24601', 'rewards_train/rejected': '0.22232', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.023685', 'logps_train/rejected': '-150.79', 'logps_train/chosen': '-182.06', 'loss/train': '0.70705', 'examples_per_second': '29.971', 'grad_norm': '46.75', 'counters/examples': 210368, 'counters/updates': 6574}
train stats after 210400 examples: {'rewards_train/chosen': '0.071835', 'rewards_train/rejected': '0.055621', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016214', 'logps_train/rejected': '-134.78', 'logps_train/chosen': '-121.13', 'loss/train': '0.69243', 'examples_per_second': '30.213', 'grad_norm': '49.25', 'counters/examples': 210400, 'counters/updates': 6575}
train stats after 210432 examples: {'rewards_train/chosen': '0.16022', 'rewards_train/rejected': '0.1231', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037118', 'logps_train/rejected': '-109.38', 'logps_train/chosen': '-133.55', 'loss/train': '0.6838', 'examples_per_second': '30.757', 'grad_norm': '33.25', 'counters/examples': 210432, 'counters/updates': 6576}
skipping logging after 210464 examples to avoid logging too frequently
train stats after 210496 examples: {'rewards_train/chosen': '0.20219', 'rewards_train/rejected': '0.071703', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13048', 'logps_train/rejected': '-101.18', 'logps_train/chosen': '-166.42', 'loss/train': '0.64407', 'examples_per_second': '35.507', 'grad_norm': '26.625', 'counters/examples': 210496, 'counters/updates': 6578}
train stats after 210528 examples: {'rewards_train/chosen': '0.18229', 'rewards_train/rejected': '0.096879', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085413', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-138.78', 'loss/train': '0.68067', 'examples_per_second': '31.438', 'grad_norm': '75', 'counters/examples': 210528, 'counters/updates': 6579}
train stats after 210560 examples: {'rewards_train/chosen': '0.23993', 'rewards_train/rejected': '0.13289', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10703', 'logps_train/rejected': '-165.37', 'logps_train/chosen': '-141.17', 'loss/train': '0.65254', 'examples_per_second': '31.655', 'grad_norm': '37.25', 'counters/examples': 210560, 'counters/updates': 6580}
train stats after 210592 examples: {'rewards_train/chosen': '0.27699', 'rewards_train/rejected': '0.013894', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26309', 'logps_train/rejected': '-122.24', 'logps_train/chosen': '-150.09', 'loss/train': '0.58983', 'examples_per_second': '30.354', 'grad_norm': '31.5', 'counters/examples': 210592, 'counters/updates': 6581}
train stats after 210624 examples: {'rewards_train/chosen': '0.048175', 'rewards_train/rejected': '0.024981', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.023195', 'logps_train/rejected': '-143.1', 'logps_train/chosen': '-140.63', 'loss/train': '0.68949', 'examples_per_second': '31.488', 'grad_norm': '33.25', 'counters/examples': 210624, 'counters/updates': 6582}
train stats after 210656 examples: {'rewards_train/chosen': '0.085432', 'rewards_train/rejected': '-0.042815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12825', 'logps_train/rejected': '-118.78', 'logps_train/chosen': '-125.49', 'loss/train': '0.63869', 'examples_per_second': '32.039', 'grad_norm': '25.25', 'counters/examples': 210656, 'counters/updates': 6583}
train stats after 210688 examples: {'rewards_train/chosen': '0.12066', 'rewards_train/rejected': '0.028164', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.092494', 'logps_train/rejected': '-107.42', 'logps_train/chosen': '-150.2', 'loss/train': '0.65971', 'examples_per_second': '30.901', 'grad_norm': '31.5', 'counters/examples': 210688, 'counters/updates': 6584}
train stats after 210720 examples: {'rewards_train/chosen': '0.25829', 'rewards_train/rejected': '-0.031608', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.2899', 'logps_train/rejected': '-123.38', 'logps_train/chosen': '-146.52', 'loss/train': '0.5736', 'examples_per_second': '31.477', 'grad_norm': '26.875', 'counters/examples': 210720, 'counters/updates': 6585}
train stats after 210752 examples: {'rewards_train/chosen': '0.26425', 'rewards_train/rejected': '0.046224', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21803', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-156.51', 'loss/train': '0.61213', 'examples_per_second': '31.808', 'grad_norm': '25.5', 'counters/examples': 210752, 'counters/updates': 6586}
skipping logging after 210784 examples to avoid logging too frequently
train stats after 210816 examples: {'rewards_train/chosen': '0.13081', 'rewards_train/rejected': '0.0010353', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12977', 'logps_train/rejected': '-161.07', 'logps_train/chosen': '-120.57', 'loss/train': '0.64622', 'examples_per_second': '31.435', 'grad_norm': '31.75', 'counters/examples': 210816, 'counters/updates': 6588}
skipping logging after 210848 examples to avoid logging too frequently
train stats after 210880 examples: {'rewards_train/chosen': '0.17227', 'rewards_train/rejected': '0.041508', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13076', 'logps_train/rejected': '-162.29', 'logps_train/chosen': '-149.26', 'loss/train': '0.65528', 'examples_per_second': '32.768', 'grad_norm': '29', 'counters/examples': 210880, 'counters/updates': 6590}
train stats after 210912 examples: {'rewards_train/chosen': '0.17677', 'rewards_train/rejected': '0.041018', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13575', 'logps_train/rejected': '-150.96', 'logps_train/chosen': '-132.62', 'loss/train': '0.63958', 'examples_per_second': '30.729', 'grad_norm': '31.125', 'counters/examples': 210912, 'counters/updates': 6591}
train stats after 210944 examples: {'rewards_train/chosen': '0.18987', 'rewards_train/rejected': '0.059699', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13017', 'logps_train/rejected': '-112.57', 'logps_train/chosen': '-137.37', 'loss/train': '0.64275', 'examples_per_second': '31.201', 'grad_norm': '37.5', 'counters/examples': 210944, 'counters/updates': 6592}
train stats after 210976 examples: {'rewards_train/chosen': '0.16338', 'rewards_train/rejected': '0.017214', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14616', 'logps_train/rejected': '-161.98', 'logps_train/chosen': '-155.95', 'loss/train': '0.63369', 'examples_per_second': '31.482', 'grad_norm': '32', 'counters/examples': 210976, 'counters/updates': 6593}
train stats after 211008 examples: {'rewards_train/chosen': '0.22805', 'rewards_train/rejected': '0.090377', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.13768', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-152.52', 'loss/train': '0.65385', 'examples_per_second': '32.22', 'grad_norm': '34', 'counters/examples': 211008, 'counters/updates': 6594}
train stats after 211040 examples: {'rewards_train/chosen': '0.26587', 'rewards_train/rejected': '0.095014', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17085', 'logps_train/rejected': '-145.47', 'logps_train/chosen': '-154.93', 'loss/train': '0.64054', 'examples_per_second': '31.474', 'grad_norm': '43.5', 'counters/examples': 211040, 'counters/updates': 6595}
train stats after 211072 examples: {'rewards_train/chosen': '0.12207', 'rewards_train/rejected': '0.043113', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.078954', 'logps_train/rejected': '-153.31', 'logps_train/chosen': '-137.67', 'loss/train': '0.66349', 'examples_per_second': '30.279', 'grad_norm': '58.5', 'counters/examples': 211072, 'counters/updates': 6596}
train stats after 211104 examples: {'rewards_train/chosen': '0.2174', 'rewards_train/rejected': '0.13343', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.083971', 'logps_train/rejected': '-162.19', 'logps_train/chosen': '-135.34', 'loss/train': '0.67879', 'examples_per_second': '31.487', 'grad_norm': '58.75', 'counters/examples': 211104, 'counters/updates': 6597}
train stats after 211136 examples: {'rewards_train/chosen': '0.063139', 'rewards_train/rejected': '-0.049703', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11284', 'logps_train/rejected': '-97.263', 'logps_train/chosen': '-134.23', 'loss/train': '0.64926', 'examples_per_second': '30.728', 'grad_norm': '27', 'counters/examples': 211136, 'counters/updates': 6598}
train stats after 211168 examples: {'rewards_train/chosen': '0.22252', 'rewards_train/rejected': '0.067705', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15481', 'logps_train/rejected': '-104.3', 'logps_train/chosen': '-147.16', 'loss/train': '0.63976', 'examples_per_second': '30.376', 'grad_norm': '27.625', 'counters/examples': 211168, 'counters/updates': 6599}
skipping logging after 211200 examples to avoid logging too frequently
train stats after 211232 examples: {'rewards_train/chosen': '0.11243', 'rewards_train/rejected': '0.027353', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085075', 'logps_train/rejected': '-128.12', 'logps_train/chosen': '-130.94', 'loss/train': '0.66545', 'examples_per_second': '31.491', 'grad_norm': '26', 'counters/examples': 211232, 'counters/updates': 6601}
skipping logging after 211264 examples to avoid logging too frequently
train stats after 211296 examples: {'rewards_train/chosen': '0.12102', 'rewards_train/rejected': '0.027262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09376', 'logps_train/rejected': '-140.59', 'logps_train/chosen': '-108.86', 'loss/train': '0.65997', 'examples_per_second': '31.437', 'grad_norm': '30.75', 'counters/examples': 211296, 'counters/updates': 6603}
skipping logging after 211328 examples to avoid logging too frequently
train stats after 211360 examples: {'rewards_train/chosen': '0.18115', 'rewards_train/rejected': '0.068404', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11275', 'logps_train/rejected': '-121.24', 'logps_train/chosen': '-163.89', 'loss/train': '0.64994', 'examples_per_second': '31.67', 'grad_norm': '35.75', 'counters/examples': 211360, 'counters/updates': 6605}
train stats after 211392 examples: {'rewards_train/chosen': '0.13568', 'rewards_train/rejected': '0.017815', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11787', 'logps_train/rejected': '-121.31', 'logps_train/chosen': '-159.25', 'loss/train': '0.64882', 'examples_per_second': '31.52', 'grad_norm': '29.75', 'counters/examples': 211392, 'counters/updates': 6606}
train stats after 211424 examples: {'rewards_train/chosen': '0.15686', 'rewards_train/rejected': '0.083111', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073753', 'logps_train/rejected': '-115.23', 'logps_train/chosen': '-154.75', 'loss/train': '0.67076', 'examples_per_second': '29.683', 'grad_norm': '34.25', 'counters/examples': 211424, 'counters/updates': 6607}
train stats after 211456 examples: {'rewards_train/chosen': '0.16529', 'rewards_train/rejected': '0.053764', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11153', 'logps_train/rejected': '-139.78', 'logps_train/chosen': '-154.93', 'loss/train': '0.65581', 'examples_per_second': '31.511', 'grad_norm': '40', 'counters/examples': 211456, 'counters/updates': 6608}
train stats after 211488 examples: {'rewards_train/chosen': '0.15059', 'rewards_train/rejected': '0.086369', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.064219', 'logps_train/rejected': '-111.04', 'logps_train/chosen': '-138.73', 'loss/train': '0.67716', 'examples_per_second': '31.422', 'grad_norm': '23.5', 'counters/examples': 211488, 'counters/updates': 6609}
train stats after 211520 examples: {'rewards_train/chosen': '0.29466', 'rewards_train/rejected': '0.16273', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13192', 'logps_train/rejected': '-150.76', 'logps_train/chosen': '-185.67', 'loss/train': '0.65967', 'examples_per_second': '30.294', 'grad_norm': '44.25', 'counters/examples': 211520, 'counters/updates': 6610}
train stats after 211552 examples: {'rewards_train/chosen': '0.17646', 'rewards_train/rejected': '0.10241', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.074047', 'logps_train/rejected': '-130.27', 'logps_train/chosen': '-161.13', 'loss/train': '0.67227', 'examples_per_second': '30.018', 'grad_norm': '30.375', 'counters/examples': 211552, 'counters/updates': 6611}
train stats after 211584 examples: {'rewards_train/chosen': '0.21818', 'rewards_train/rejected': '0.065882', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1523', 'logps_train/rejected': '-124.84', 'logps_train/chosen': '-159.16', 'loss/train': '0.64853', 'examples_per_second': '30.375', 'grad_norm': '30.875', 'counters/examples': 211584, 'counters/updates': 6612}
train stats after 211616 examples: {'rewards_train/chosen': '0.063438', 'rewards_train/rejected': '0.098036', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.034597', 'logps_train/rejected': '-141.09', 'logps_train/chosen': '-143.09', 'loss/train': '0.72503', 'examples_per_second': '31.526', 'grad_norm': '33.25', 'counters/examples': 211616, 'counters/updates': 6613}
train stats after 211648 examples: {'rewards_train/chosen': '0.13167', 'rewards_train/rejected': '0.1192', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.012471', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-154.17', 'loss/train': '0.69244', 'examples_per_second': '31.512', 'grad_norm': '27.25', 'counters/examples': 211648, 'counters/updates': 6614}
train stats after 211680 examples: {'rewards_train/chosen': '0.13732', 'rewards_train/rejected': '0.010509', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12681', 'logps_train/rejected': '-124.19', 'logps_train/chosen': '-145.75', 'loss/train': '0.64729', 'examples_per_second': '30.518', 'grad_norm': '36', 'counters/examples': 211680, 'counters/updates': 6615}
train stats after 211712 examples: {'rewards_train/chosen': '0.15993', 'rewards_train/rejected': '0.022127', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1378', 'logps_train/rejected': '-133.15', 'logps_train/chosen': '-200.84', 'loss/train': '0.63792', 'examples_per_second': '30.028', 'grad_norm': '29.5', 'counters/examples': 211712, 'counters/updates': 6616}
train stats after 211744 examples: {'rewards_train/chosen': '0.12987', 'rewards_train/rejected': '0.052431', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077435', 'logps_train/rejected': '-155.56', 'logps_train/chosen': '-126.42', 'loss/train': '0.6672', 'examples_per_second': '30.989', 'grad_norm': '33', 'counters/examples': 211744, 'counters/updates': 6617}
train stats after 211776 examples: {'rewards_train/chosen': '0.090184', 'rewards_train/rejected': '0.14252', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.052339', 'logps_train/rejected': '-130.71', 'logps_train/chosen': '-152.69', 'loss/train': '0.7259', 'examples_per_second': '31.428', 'grad_norm': '33', 'counters/examples': 211776, 'counters/updates': 6618}
train stats after 211808 examples: {'rewards_train/chosen': '0.08561', 'rewards_train/rejected': '0.033969', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05164', 'logps_train/rejected': '-94', 'logps_train/chosen': '-130.9', 'loss/train': '0.67869', 'examples_per_second': '31.391', 'grad_norm': '25.125', 'counters/examples': 211808, 'counters/updates': 6619}
skipping logging after 211840 examples to avoid logging too frequently
train stats after 211872 examples: {'rewards_train/chosen': '0.21215', 'rewards_train/rejected': '0.15579', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056364', 'logps_train/rejected': '-148.27', 'logps_train/chosen': '-142.47', 'loss/train': '0.67771', 'examples_per_second': '33.609', 'grad_norm': '32.25', 'counters/examples': 211872, 'counters/updates': 6621}
train stats after 211904 examples: {'rewards_train/chosen': '0.11083', 'rewards_train/rejected': '0.1332', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.022363', 'logps_train/rejected': '-101.61', 'logps_train/chosen': '-104.84', 'loss/train': '0.71561', 'examples_per_second': '31.226', 'grad_norm': '26.625', 'counters/examples': 211904, 'counters/updates': 6622}
train stats after 211936 examples: {'rewards_train/chosen': '0.16163', 'rewards_train/rejected': '0.013908', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14772', 'logps_train/rejected': '-129.93', 'logps_train/chosen': '-150.58', 'loss/train': '0.63389', 'examples_per_second': '30.078', 'grad_norm': '31.75', 'counters/examples': 211936, 'counters/updates': 6623}
train stats after 211968 examples: {'rewards_train/chosen': '0.076485', 'rewards_train/rejected': '0.082959', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0064741', 'logps_train/rejected': '-131.71', 'logps_train/chosen': '-138.44', 'loss/train': '0.71556', 'examples_per_second': '32.732', 'grad_norm': '43.5', 'counters/examples': 211968, 'counters/updates': 6624}
train stats after 212000 examples: {'rewards_train/chosen': '0.28023', 'rewards_train/rejected': '0.050588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.22964', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-176.98', 'loss/train': '0.61214', 'examples_per_second': '30.181', 'grad_norm': '28.5', 'counters/examples': 212000, 'counters/updates': 6625}
train stats after 212032 examples: {'rewards_train/chosen': '0.16527', 'rewards_train/rejected': '0.02296', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14231', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-169.9', 'loss/train': '0.64578', 'examples_per_second': '31.535', 'grad_norm': '32.25', 'counters/examples': 212032, 'counters/updates': 6626}
train stats after 212064 examples: {'rewards_train/chosen': '0.12553', 'rewards_train/rejected': '0.021606', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10392', 'logps_train/rejected': '-90.406', 'logps_train/chosen': '-122.14', 'loss/train': '0.65296', 'examples_per_second': '27.114', 'grad_norm': '34', 'counters/examples': 212064, 'counters/updates': 6627}
skipping logging after 212096 examples to avoid logging too frequently
train stats after 212128 examples: {'rewards_train/chosen': '0.11338', 'rewards_train/rejected': '0.022188', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091191', 'logps_train/rejected': '-90.381', 'logps_train/chosen': '-120.05', 'loss/train': '0.65586', 'examples_per_second': '35.649', 'grad_norm': '29.5', 'counters/examples': 212128, 'counters/updates': 6629}
train stats after 212160 examples: {'rewards_train/chosen': '0.11848', 'rewards_train/rejected': '0.055046', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063438', 'logps_train/rejected': '-138.86', 'logps_train/chosen': '-181.18', 'loss/train': '0.67472', 'examples_per_second': '31.485', 'grad_norm': '32', 'counters/examples': 212160, 'counters/updates': 6630}
train stats after 212192 examples: {'rewards_train/chosen': '0.18857', 'rewards_train/rejected': '0.045016', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14355', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-185.1', 'loss/train': '0.63777', 'examples_per_second': '31.424', 'grad_norm': '38.5', 'counters/examples': 212192, 'counters/updates': 6631}
train stats after 212224 examples: {'rewards_train/chosen': '0.12793', 'rewards_train/rejected': '0.024985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10295', 'logps_train/rejected': '-119.74', 'logps_train/chosen': '-136.29', 'loss/train': '0.65107', 'examples_per_second': '30.181', 'grad_norm': '25.125', 'counters/examples': 212224, 'counters/updates': 6632}
train stats after 212256 examples: {'rewards_train/chosen': '0.11238', 'rewards_train/rejected': '0.085887', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026493', 'logps_train/rejected': '-126.24', 'logps_train/chosen': '-142.54', 'loss/train': '0.69283', 'examples_per_second': '30.871', 'grad_norm': '31.25', 'counters/examples': 212256, 'counters/updates': 6633}
train stats after 212288 examples: {'rewards_train/chosen': '0.21117', 'rewards_train/rejected': '0.050025', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16114', 'logps_train/rejected': '-139.1', 'logps_train/chosen': '-182.57', 'loss/train': '0.63608', 'examples_per_second': '32.279', 'grad_norm': '37.75', 'counters/examples': 212288, 'counters/updates': 6634}
train stats after 212320 examples: {'rewards_train/chosen': '0.10831', 'rewards_train/rejected': '0.0048107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1035', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-117.15', 'loss/train': '0.65152', 'examples_per_second': '30.663', 'grad_norm': '31.875', 'counters/examples': 212320, 'counters/updates': 6635}
train stats after 212352 examples: {'rewards_train/chosen': '0.16659', 'rewards_train/rejected': '0.11014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056449', 'logps_train/rejected': '-107.74', 'logps_train/chosen': '-127.95', 'loss/train': '0.69388', 'examples_per_second': '31.552', 'grad_norm': '29.375', 'counters/examples': 212352, 'counters/updates': 6636}
train stats after 212384 examples: {'rewards_train/chosen': '0.099743', 'rewards_train/rejected': '0.052448', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047295', 'logps_train/rejected': '-124.04', 'logps_train/chosen': '-131.9', 'loss/train': '0.68201', 'examples_per_second': '31.346', 'grad_norm': '34.25', 'counters/examples': 212384, 'counters/updates': 6637}
train stats after 212416 examples: {'rewards_train/chosen': '0.18004', 'rewards_train/rejected': '0.058678', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12136', 'logps_train/rejected': '-121.35', 'logps_train/chosen': '-156.38', 'loss/train': '0.64384', 'examples_per_second': '30.596', 'grad_norm': '29.125', 'counters/examples': 212416, 'counters/updates': 6638}
train stats after 212448 examples: {'rewards_train/chosen': '0.17027', 'rewards_train/rejected': '0.10511', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065158', 'logps_train/rejected': '-137.76', 'logps_train/chosen': '-159.64', 'loss/train': '0.67954', 'examples_per_second': '31.498', 'grad_norm': '35.75', 'counters/examples': 212448, 'counters/updates': 6639}
train stats after 212480 examples: {'rewards_train/chosen': '0.057694', 'rewards_train/rejected': '0.003997', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053697', 'logps_train/rejected': '-111.64', 'logps_train/chosen': '-128.28', 'loss/train': '0.67125', 'examples_per_second': '31.992', 'grad_norm': '36.5', 'counters/examples': 212480, 'counters/updates': 6640}
train stats after 212512 examples: {'rewards_train/chosen': '0.14109', 'rewards_train/rejected': '0.15844', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.017352', 'logps_train/rejected': '-112.28', 'logps_train/chosen': '-153.32', 'loss/train': '0.73265', 'examples_per_second': '31.34', 'grad_norm': '119.5', 'counters/examples': 212512, 'counters/updates': 6641}
train stats after 212544 examples: {'rewards_train/chosen': '0.19517', 'rewards_train/rejected': '0.089074', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1061', 'logps_train/rejected': '-113.21', 'logps_train/chosen': '-135.37', 'loss/train': '0.65668', 'examples_per_second': '29.924', 'grad_norm': '26.5', 'counters/examples': 212544, 'counters/updates': 6642}
train stats after 212576 examples: {'rewards_train/chosen': '0.23689', 'rewards_train/rejected': '0.087819', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14907', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-126.67', 'loss/train': '0.62811', 'examples_per_second': '30.265', 'grad_norm': '30.75', 'counters/examples': 212576, 'counters/updates': 6643}
train stats after 212608 examples: {'rewards_train/chosen': '0.065544', 'rewards_train/rejected': '0.024741', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.040803', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-121.29', 'loss/train': '0.68081', 'examples_per_second': '31.029', 'grad_norm': '28.375', 'counters/examples': 212608, 'counters/updates': 6644}
train stats after 212640 examples: {'rewards_train/chosen': '0.15515', 'rewards_train/rejected': '0.017919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13723', 'logps_train/rejected': '-146.45', 'logps_train/chosen': '-133.17', 'loss/train': '0.64611', 'examples_per_second': '30.704', 'grad_norm': '30.125', 'counters/examples': 212640, 'counters/updates': 6645}
train stats after 212672 examples: {'rewards_train/chosen': '0.19772', 'rewards_train/rejected': '0.01803', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17969', 'logps_train/rejected': '-129.58', 'logps_train/chosen': '-153.9', 'loss/train': '0.6223', 'examples_per_second': '32.547', 'grad_norm': '26.875', 'counters/examples': 212672, 'counters/updates': 6646}
train stats after 212704 examples: {'rewards_train/chosen': '0.047959', 'rewards_train/rejected': '0.081774', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.033815', 'logps_train/rejected': '-124.49', 'logps_train/chosen': '-167.21', 'loss/train': '0.72617', 'examples_per_second': '32.604', 'grad_norm': '37', 'counters/examples': 212704, 'counters/updates': 6647}
train stats after 212736 examples: {'rewards_train/chosen': '0.20166', 'rewards_train/rejected': '-0.0035327', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20519', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-154.28', 'loss/train': '0.62352', 'examples_per_second': '32.025', 'grad_norm': '28.5', 'counters/examples': 212736, 'counters/updates': 6648}
skipping logging after 212768 examples to avoid logging too frequently
train stats after 212800 examples: {'rewards_train/chosen': '0.19513', 'rewards_train/rejected': '0.086504', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10863', 'logps_train/rejected': '-125', 'logps_train/chosen': '-164.28', 'loss/train': '0.66878', 'examples_per_second': '31.085', 'grad_norm': '33.5', 'counters/examples': 212800, 'counters/updates': 6650}
skipping logging after 212832 examples to avoid logging too frequently
train stats after 212864 examples: {'rewards_train/chosen': '0.3018', 'rewards_train/rejected': '0.099847', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20195', 'logps_train/rejected': '-122.44', 'logps_train/chosen': '-137.61', 'loss/train': '0.61848', 'examples_per_second': '30.686', 'grad_norm': '30.625', 'counters/examples': 212864, 'counters/updates': 6652}
skipping logging after 212896 examples to avoid logging too frequently
train stats after 212928 examples: {'rewards_train/chosen': '0.20553', 'rewards_train/rejected': '0.022135', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18339', 'logps_train/rejected': '-124.71', 'logps_train/chosen': '-129.72', 'loss/train': '0.61696', 'examples_per_second': '31.065', 'grad_norm': '26.25', 'counters/examples': 212928, 'counters/updates': 6654}
train stats after 212960 examples: {'rewards_train/chosen': '0.13911', 'rewards_train/rejected': '0.08865', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.050457', 'logps_train/rejected': '-126.52', 'logps_train/chosen': '-137.81', 'loss/train': '0.68682', 'examples_per_second': '30.695', 'grad_norm': '40.5', 'counters/examples': 212960, 'counters/updates': 6655}
train stats after 212992 examples: {'rewards_train/chosen': '0.19472', 'rewards_train/rejected': '-0.029557', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22428', 'logps_train/rejected': '-78.669', 'logps_train/chosen': '-163.62', 'loss/train': '0.61845', 'examples_per_second': '31.132', 'grad_norm': '32', 'counters/examples': 212992, 'counters/updates': 6656}
skipping logging after 213024 examples to avoid logging too frequently
train stats after 213056 examples: {'rewards_train/chosen': '0.13556', 'rewards_train/rejected': '0.023802', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11175', 'logps_train/rejected': '-147.46', 'logps_train/chosen': '-123.61', 'loss/train': '0.64791', 'examples_per_second': '31.334', 'grad_norm': '28.625', 'counters/examples': 213056, 'counters/updates': 6658}
train stats after 213088 examples: {'rewards_train/chosen': '0.18734', 'rewards_train/rejected': '0.087754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.09959', 'logps_train/rejected': '-122.29', 'logps_train/chosen': '-139.54', 'loss/train': '0.65235', 'examples_per_second': '31.156', 'grad_norm': '26.125', 'counters/examples': 213088, 'counters/updates': 6659}
train stats after 213120 examples: {'rewards_train/chosen': '0.19877', 'rewards_train/rejected': '0.053635', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14514', 'logps_train/rejected': '-109.2', 'logps_train/chosen': '-144.13', 'loss/train': '0.63668', 'examples_per_second': '30.04', 'grad_norm': '24', 'counters/examples': 213120, 'counters/updates': 6660}
train stats after 213152 examples: {'rewards_train/chosen': '0.15436', 'rewards_train/rejected': '0.083625', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.070735', 'logps_train/rejected': '-99.421', 'logps_train/chosen': '-140.49', 'loss/train': '0.67989', 'examples_per_second': '31.118', 'grad_norm': '30.125', 'counters/examples': 213152, 'counters/updates': 6661}
train stats after 213184 examples: {'rewards_train/chosen': '0.10952', 'rewards_train/rejected': '0.039319', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070206', 'logps_train/rejected': '-142.02', 'logps_train/chosen': '-148.74', 'loss/train': '0.66899', 'examples_per_second': '30.483', 'grad_norm': '35', 'counters/examples': 213184, 'counters/updates': 6662}
train stats after 213216 examples: {'rewards_train/chosen': '0.25977', 'rewards_train/rejected': '0.067946', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19183', 'logps_train/rejected': '-129.89', 'logps_train/chosen': '-158.89', 'loss/train': '0.62034', 'examples_per_second': '32.498', 'grad_norm': '38.75', 'counters/examples': 213216, 'counters/updates': 6663}
train stats after 213248 examples: {'rewards_train/chosen': '0.28419', 'rewards_train/rejected': '0.12013', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16406', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-151.91', 'loss/train': '0.63058', 'examples_per_second': '31.671', 'grad_norm': '59.25', 'counters/examples': 213248, 'counters/updates': 6664}
train stats after 213280 examples: {'rewards_train/chosen': '0.11304', 'rewards_train/rejected': '0.047747', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.065295', 'logps_train/rejected': '-122.94', 'logps_train/chosen': '-130.58', 'loss/train': '0.67915', 'examples_per_second': '24.909', 'grad_norm': '98', 'counters/examples': 213280, 'counters/updates': 6665}
train stats after 213312 examples: {'rewards_train/chosen': '0.17456', 'rewards_train/rejected': '0.028286', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14628', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-138.31', 'loss/train': '0.63408', 'examples_per_second': '30.34', 'grad_norm': '25.125', 'counters/examples': 213312, 'counters/updates': 6666}
train stats after 213344 examples: {'rewards_train/chosen': '0.19464', 'rewards_train/rejected': '0.18527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0093697', 'logps_train/rejected': '-136.11', 'logps_train/chosen': '-130.94', 'loss/train': '0.71366', 'examples_per_second': '30.51', 'grad_norm': '39.25', 'counters/examples': 213344, 'counters/updates': 6667}
train stats after 213376 examples: {'rewards_train/chosen': '0.072805', 'rewards_train/rejected': '0.0035174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069288', 'logps_train/rejected': '-126.11', 'logps_train/chosen': '-170.44', 'loss/train': '0.66729', 'examples_per_second': '24.624', 'grad_norm': '28.75', 'counters/examples': 213376, 'counters/updates': 6668}
skipping logging after 213408 examples to avoid logging too frequently
train stats after 213440 examples: {'rewards_train/chosen': '0.18888', 'rewards_train/rejected': '0.082128', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10675', 'logps_train/rejected': '-144.4', 'logps_train/chosen': '-153.3', 'loss/train': '0.65225', 'examples_per_second': '30.554', 'grad_norm': '32.75', 'counters/examples': 213440, 'counters/updates': 6670}
skipping logging after 213472 examples to avoid logging too frequently
train stats after 213504 examples: {'rewards_train/chosen': '0.19687', 'rewards_train/rejected': '0.11134', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085528', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-150.84', 'loss/train': '0.6814', 'examples_per_second': '34.106', 'grad_norm': '28.125', 'counters/examples': 213504, 'counters/updates': 6672}
skipping logging after 213536 examples to avoid logging too frequently
train stats after 213568 examples: {'rewards_train/chosen': '0.22829', 'rewards_train/rejected': '0.065263', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16303', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-131.49', 'loss/train': '0.63903', 'examples_per_second': '33.64', 'grad_norm': '40.25', 'counters/examples': 213568, 'counters/updates': 6674}
skipping logging after 213600 examples to avoid logging too frequently
train stats after 213632 examples: {'rewards_train/chosen': '0.21091', 'rewards_train/rejected': '0.064501', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14641', 'logps_train/rejected': '-139.99', 'logps_train/chosen': '-159.34', 'loss/train': '0.64387', 'examples_per_second': '30.852', 'grad_norm': '40.25', 'counters/examples': 213632, 'counters/updates': 6676}
train stats after 213664 examples: {'rewards_train/chosen': '0.24676', 'rewards_train/rejected': '0.073279', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17348', 'logps_train/rejected': '-96.053', 'logps_train/chosen': '-185.95', 'loss/train': '0.63388', 'examples_per_second': '32.989', 'grad_norm': '32', 'counters/examples': 213664, 'counters/updates': 6677}
train stats after 213696 examples: {'rewards_train/chosen': '0.41927', 'rewards_train/rejected': '0.26808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1512', 'logps_train/rejected': '-158.22', 'logps_train/chosen': '-152.78', 'loss/train': '0.67189', 'examples_per_second': '30.096', 'grad_norm': '45.25', 'counters/examples': 213696, 'counters/updates': 6678}
train stats after 213728 examples: {'rewards_train/chosen': '0.21984', 'rewards_train/rejected': '0.0040336', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21581', 'logps_train/rejected': '-109.03', 'logps_train/chosen': '-145.53', 'loss/train': '0.60473', 'examples_per_second': '31.468', 'grad_norm': '24.75', 'counters/examples': 213728, 'counters/updates': 6679}
train stats after 213760 examples: {'rewards_train/chosen': '0.16158', 'rewards_train/rejected': '0.090608', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '0.070969', 'logps_train/rejected': '-157.24', 'logps_train/chosen': '-183.1', 'loss/train': '0.67711', 'examples_per_second': '29.581', 'grad_norm': '36.25', 'counters/examples': 213760, 'counters/updates': 6680}
train stats after 213792 examples: {'rewards_train/chosen': '0.080903', 'rewards_train/rejected': '0.066034', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.014869', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-124.43', 'loss/train': '0.70149', 'examples_per_second': '29.952', 'grad_norm': '28.625', 'counters/examples': 213792, 'counters/updates': 6681}
train stats after 213824 examples: {'rewards_train/chosen': '0.23603', 'rewards_train/rejected': '0.014874', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22116', 'logps_train/rejected': '-126.79', 'logps_train/chosen': '-175.08', 'loss/train': '0.60629', 'examples_per_second': '30.818', 'grad_norm': '26.5', 'counters/examples': 213824, 'counters/updates': 6682}
train stats after 213856 examples: {'rewards_train/chosen': '0.16938', 'rewards_train/rejected': '0.047063', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12231', 'logps_train/rejected': '-108.63', 'logps_train/chosen': '-125.33', 'loss/train': '0.64444', 'examples_per_second': '32.424', 'grad_norm': '27', 'counters/examples': 213856, 'counters/updates': 6683}
train stats after 213888 examples: {'rewards_train/chosen': '0.20407', 'rewards_train/rejected': '-0.086971', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.29104', 'logps_train/rejected': '-124.92', 'logps_train/chosen': '-153.65', 'loss/train': '0.60542', 'examples_per_second': '31.506', 'grad_norm': '25.125', 'counters/examples': 213888, 'counters/updates': 6684}
train stats after 213920 examples: {'rewards_train/chosen': '0.1433', 'rewards_train/rejected': '0.015006', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12829', 'logps_train/rejected': '-112.99', 'logps_train/chosen': '-168.15', 'loss/train': '0.64775', 'examples_per_second': '31.74', 'grad_norm': '30.625', 'counters/examples': 213920, 'counters/updates': 6685}
train stats after 213952 examples: {'rewards_train/chosen': '0.11222', 'rewards_train/rejected': '0.0503', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.061916', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-138.85', 'loss/train': '0.68689', 'examples_per_second': '31.563', 'grad_norm': '31.5', 'counters/examples': 213952, 'counters/updates': 6686}
train stats after 213984 examples: {'rewards_train/chosen': '0.15121', 'rewards_train/rejected': '0.024665', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12655', 'logps_train/rejected': '-113.4', 'logps_train/chosen': '-141.35', 'loss/train': '0.64575', 'examples_per_second': '31.302', 'grad_norm': '27.875', 'counters/examples': 213984, 'counters/updates': 6687}
train stats after 214016 examples: {'rewards_train/chosen': '0.1541', 'rewards_train/rejected': '0.055965', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098132', 'logps_train/rejected': '-117.1', 'logps_train/chosen': '-135.62', 'loss/train': '0.6643', 'examples_per_second': '31.402', 'grad_norm': '27.75', 'counters/examples': 214016, 'counters/updates': 6688}
train stats after 214048 examples: {'rewards_train/chosen': '0.1501', 'rewards_train/rejected': '0.10861', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041493', 'logps_train/rejected': '-128.18', 'logps_train/chosen': '-144.4', 'loss/train': '0.69269', 'examples_per_second': '31.023', 'grad_norm': '32', 'counters/examples': 214048, 'counters/updates': 6689}
train stats after 214080 examples: {'rewards_train/chosen': '0.10645', 'rewards_train/rejected': '-0.033053', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1395', 'logps_train/rejected': '-153.8', 'logps_train/chosen': '-145.19', 'loss/train': '0.63738', 'examples_per_second': '31.486', 'grad_norm': '30.75', 'counters/examples': 214080, 'counters/updates': 6690}
train stats after 214112 examples: {'rewards_train/chosen': '0.17994', 'rewards_train/rejected': '0.041969', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13797', 'logps_train/rejected': '-107.98', 'logps_train/chosen': '-127.51', 'loss/train': '0.65863', 'examples_per_second': '32.04', 'grad_norm': '32.75', 'counters/examples': 214112, 'counters/updates': 6691}
train stats after 214144 examples: {'rewards_train/chosen': '0.024162', 'rewards_train/rejected': '0.011866', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.012295', 'logps_train/rejected': '-89.606', 'logps_train/chosen': '-94.254', 'loss/train': '0.69678', 'examples_per_second': '31.471', 'grad_norm': '24.875', 'counters/examples': 214144, 'counters/updates': 6692}
train stats after 214176 examples: {'rewards_train/chosen': '0.11755', 'rewards_train/rejected': '0.011115', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10643', 'logps_train/rejected': '-122.36', 'logps_train/chosen': '-143.26', 'loss/train': '0.66104', 'examples_per_second': '30.93', 'grad_norm': '33.25', 'counters/examples': 214176, 'counters/updates': 6693}
train stats after 214208 examples: {'rewards_train/chosen': '0.19239', 'rewards_train/rejected': '0.022998', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16939', 'logps_train/rejected': '-88.221', 'logps_train/chosen': '-128.59', 'loss/train': '0.62284', 'examples_per_second': '32.152', 'grad_norm': '37.25', 'counters/examples': 214208, 'counters/updates': 6694}
train stats after 214240 examples: {'rewards_train/chosen': '0.065528', 'rewards_train/rejected': '0.067746', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0022176', 'logps_train/rejected': '-136.89', 'logps_train/chosen': '-141.8', 'loss/train': '0.70595', 'examples_per_second': '31.411', 'grad_norm': '39.25', 'counters/examples': 214240, 'counters/updates': 6695}
train stats after 214272 examples: {'rewards_train/chosen': '0.16048', 'rewards_train/rejected': '-0.017329', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1778', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-182.46', 'loss/train': '0.63258', 'examples_per_second': '29.88', 'grad_norm': '24.625', 'counters/examples': 214272, 'counters/updates': 6696}
train stats after 214304 examples: {'rewards_train/chosen': '0.15557', 'rewards_train/rejected': '0.06584', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089731', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-167.03', 'loss/train': '0.66185', 'examples_per_second': '31.771', 'grad_norm': '33.25', 'counters/examples': 214304, 'counters/updates': 6697}
train stats after 214336 examples: {'rewards_train/chosen': '0.24434', 'rewards_train/rejected': '0.039396', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20495', 'logps_train/rejected': '-131.6', 'logps_train/chosen': '-127.98', 'loss/train': '0.6163', 'examples_per_second': '32.386', 'grad_norm': '24.625', 'counters/examples': 214336, 'counters/updates': 6698}
train stats after 214368 examples: {'rewards_train/chosen': '0.089272', 'rewards_train/rejected': '-0.026106', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11538', 'logps_train/rejected': '-116.67', 'logps_train/chosen': '-139.27', 'loss/train': '0.64707', 'examples_per_second': '30.842', 'grad_norm': '26.625', 'counters/examples': 214368, 'counters/updates': 6699}
train stats after 214400 examples: {'rewards_train/chosen': '0.10866', 'rewards_train/rejected': '0.0077075', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10096', 'logps_train/rejected': '-114.04', 'logps_train/chosen': '-146.39', 'loss/train': '0.65684', 'examples_per_second': '31.785', 'grad_norm': '29', 'counters/examples': 214400, 'counters/updates': 6700}
train stats after 214432 examples: {'rewards_train/chosen': '0.18358', 'rewards_train/rejected': '0.12784', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055738', 'logps_train/rejected': '-106.2', 'logps_train/chosen': '-156', 'loss/train': '0.68859', 'examples_per_second': '30.463', 'grad_norm': '38.5', 'counters/examples': 214432, 'counters/updates': 6701}
train stats after 214464 examples: {'rewards_train/chosen': '0.3041', 'rewards_train/rejected': '0.12928', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17482', 'logps_train/rejected': '-146.97', 'logps_train/chosen': '-149.14', 'loss/train': '0.64443', 'examples_per_second': '31.408', 'grad_norm': '34.5', 'counters/examples': 214464, 'counters/updates': 6702}
skipping logging after 214496 examples to avoid logging too frequently
train stats after 214528 examples: {'rewards_train/chosen': '0.16656', 'rewards_train/rejected': '0.075434', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091129', 'logps_train/rejected': '-111.4', 'logps_train/chosen': '-154.53', 'loss/train': '0.66261', 'examples_per_second': '31.474', 'grad_norm': '25.875', 'counters/examples': 214528, 'counters/updates': 6704}
train stats after 214560 examples: {'rewards_train/chosen': '0.13999', 'rewards_train/rejected': '0.072536', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067451', 'logps_train/rejected': '-112.52', 'logps_train/chosen': '-123.21', 'loss/train': '0.67067', 'examples_per_second': '31.544', 'grad_norm': '24.125', 'counters/examples': 214560, 'counters/updates': 6705}
train stats after 214592 examples: {'rewards_train/chosen': '0.057989', 'rewards_train/rejected': '0.18425', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.12627', 'logps_train/rejected': '-133.14', 'logps_train/chosen': '-182.07', 'loss/train': '0.79329', 'examples_per_second': '31.511', 'grad_norm': '42', 'counters/examples': 214592, 'counters/updates': 6706}
skipping logging after 214624 examples to avoid logging too frequently
train stats after 214656 examples: {'rewards_train/chosen': '0.1534', 'rewards_train/rejected': '0.041984', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11141', 'logps_train/rejected': '-105.29', 'logps_train/chosen': '-115.44', 'loss/train': '0.65163', 'examples_per_second': '35.656', 'grad_norm': '24.375', 'counters/examples': 214656, 'counters/updates': 6708}
train stats after 214688 examples: {'rewards_train/chosen': '0.060852', 'rewards_train/rejected': '-0.015017', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07587', 'logps_train/rejected': '-84.079', 'logps_train/chosen': '-133.46', 'loss/train': '0.66844', 'examples_per_second': '32.497', 'grad_norm': '22.25', 'counters/examples': 214688, 'counters/updates': 6709}
train stats after 214720 examples: {'rewards_train/chosen': '0.15718', 'rewards_train/rejected': '0.073851', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083329', 'logps_train/rejected': '-132.52', 'logps_train/chosen': '-154.34', 'loss/train': '0.6707', 'examples_per_second': '29.862', 'grad_norm': '27.375', 'counters/examples': 214720, 'counters/updates': 6710}
train stats after 214752 examples: {'rewards_train/chosen': '0.1735', 'rewards_train/rejected': '0.051513', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12198', 'logps_train/rejected': '-137.96', 'logps_train/chosen': '-202.31', 'loss/train': '0.6561', 'examples_per_second': '31.319', 'grad_norm': '38.75', 'counters/examples': 214752, 'counters/updates': 6711}
skipping logging after 214784 examples to avoid logging too frequently
train stats after 214816 examples: {'rewards_train/chosen': '0.1135', 'rewards_train/rejected': '0.12969', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.016194', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-97.8', 'loss/train': '0.71361', 'examples_per_second': '31.592', 'grad_norm': '35', 'counters/examples': 214816, 'counters/updates': 6713}
train stats after 214848 examples: {'rewards_train/chosen': '0.17655', 'rewards_train/rejected': '0.0054241', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17112', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-129.9', 'loss/train': '0.63249', 'examples_per_second': '31.198', 'grad_norm': '30.875', 'counters/examples': 214848, 'counters/updates': 6714}
train stats after 214880 examples: {'rewards_train/chosen': '0.1625', 'rewards_train/rejected': '0.087192', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.075309', 'logps_train/rejected': '-124.08', 'logps_train/chosen': '-142.31', 'loss/train': '0.66479', 'examples_per_second': '32.908', 'grad_norm': '27.375', 'counters/examples': 214880, 'counters/updates': 6715}
train stats after 214912 examples: {'rewards_train/chosen': '0.092941', 'rewards_train/rejected': '0.029534', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063408', 'logps_train/rejected': '-132.81', 'logps_train/chosen': '-138.76', 'loss/train': '0.66792', 'examples_per_second': '32.728', 'grad_norm': '30.625', 'counters/examples': 214912, 'counters/updates': 6716}
train stats after 214944 examples: {'rewards_train/chosen': '0.14818', 'rewards_train/rejected': '0.065918', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082264', 'logps_train/rejected': '-109.6', 'logps_train/chosen': '-144.65', 'loss/train': '0.66767', 'examples_per_second': '30.432', 'grad_norm': '27.25', 'counters/examples': 214944, 'counters/updates': 6717}
train stats after 214976 examples: {'rewards_train/chosen': '0.13965', 'rewards_train/rejected': '0.055337', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.084311', 'logps_train/rejected': '-128.3', 'logps_train/chosen': '-139.55', 'loss/train': '0.65975', 'examples_per_second': '31.499', 'grad_norm': '25.75', 'counters/examples': 214976, 'counters/updates': 6718}
skipping logging after 215008 examples to avoid logging too frequently
train stats after 215040 examples: {'rewards_train/chosen': '0.1638', 'rewards_train/rejected': '0.082532', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081269', 'logps_train/rejected': '-140.85', 'logps_train/chosen': '-153.04', 'loss/train': '0.66875', 'examples_per_second': '30.101', 'grad_norm': '31.375', 'counters/examples': 215040, 'counters/updates': 6720}
skipping logging after 215072 examples to avoid logging too frequently
train stats after 215104 examples: {'rewards_train/chosen': '0.20701', 'rewards_train/rejected': '0.074643', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13237', 'logps_train/rejected': '-108.94', 'logps_train/chosen': '-186.31', 'loss/train': '0.6472', 'examples_per_second': '33.051', 'grad_norm': '30.125', 'counters/examples': 215104, 'counters/updates': 6722}
train stats after 215136 examples: {'rewards_train/chosen': '0.12726', 'rewards_train/rejected': '-0.026651', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15391', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-176.84', 'loss/train': '0.6321', 'examples_per_second': '31.499', 'grad_norm': '44', 'counters/examples': 215136, 'counters/updates': 6723}
train stats after 215168 examples: {'rewards_train/chosen': '0.22438', 'rewards_train/rejected': '0.15044', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.073945', 'logps_train/rejected': '-154.34', 'logps_train/chosen': '-117.8', 'loss/train': '0.68426', 'examples_per_second': '29.711', 'grad_norm': '40.25', 'counters/examples': 215168, 'counters/updates': 6724}
train stats after 215200 examples: {'rewards_train/chosen': '0.046503', 'rewards_train/rejected': '0.06903', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022527', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-122.59', 'loss/train': '0.7106', 'examples_per_second': '31.414', 'grad_norm': '26.75', 'counters/examples': 215200, 'counters/updates': 6725}
train stats after 215232 examples: {'rewards_train/chosen': '0.13662', 'rewards_train/rejected': '0.019392', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.11723', 'logps_train/rejected': '-109.64', 'logps_train/chosen': '-129.91', 'loss/train': '0.64477', 'examples_per_second': '31.455', 'grad_norm': '22.5', 'counters/examples': 215232, 'counters/updates': 6726}
train stats after 215264 examples: {'rewards_train/chosen': '0.12834', 'rewards_train/rejected': '0.10587', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022462', 'logps_train/rejected': '-193.52', 'logps_train/chosen': '-168.2', 'loss/train': '0.71205', 'examples_per_second': '30.216', 'grad_norm': '102', 'counters/examples': 215264, 'counters/updates': 6727}
train stats after 215296 examples: {'rewards_train/chosen': '0.23985', 'rewards_train/rejected': '-0.0030747', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24292', 'logps_train/rejected': '-109.1', 'logps_train/chosen': '-153.39', 'loss/train': '0.59856', 'examples_per_second': '30.613', 'grad_norm': '25.5', 'counters/examples': 215296, 'counters/updates': 6728}
train stats after 215328 examples: {'rewards_train/chosen': '0.11234', 'rewards_train/rejected': '-0.0065516', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11889', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-148.33', 'loss/train': '0.6462', 'examples_per_second': '30.81', 'grad_norm': '42.5', 'counters/examples': 215328, 'counters/updates': 6729}
skipping logging after 215360 examples to avoid logging too frequently
train stats after 215392 examples: {'rewards_train/chosen': '0.091068', 'rewards_train/rejected': '0.064668', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0264', 'logps_train/rejected': '-121.6', 'logps_train/chosen': '-104.8', 'loss/train': '0.68692', 'examples_per_second': '33.481', 'grad_norm': '29.5', 'counters/examples': 215392, 'counters/updates': 6731}
train stats after 215424 examples: {'rewards_train/chosen': '0.15261', 'rewards_train/rejected': '-0.00048904', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1531', 'logps_train/rejected': '-128.77', 'logps_train/chosen': '-156.14', 'loss/train': '0.63272', 'examples_per_second': '32.237', 'grad_norm': '30.875', 'counters/examples': 215424, 'counters/updates': 6732}
train stats after 215456 examples: {'rewards_train/chosen': '0.12223', 'rewards_train/rejected': '-0.031339', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15357', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-133.22', 'loss/train': '0.62856', 'examples_per_second': '30.664', 'grad_norm': '26.75', 'counters/examples': 215456, 'counters/updates': 6733}
train stats after 215488 examples: {'rewards_train/chosen': '0.112', 'rewards_train/rejected': '0.095614', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016387', 'logps_train/rejected': '-128.5', 'logps_train/chosen': '-136.46', 'loss/train': '0.69202', 'examples_per_second': '31.722', 'grad_norm': '29', 'counters/examples': 215488, 'counters/updates': 6734}
skipping logging after 215520 examples to avoid logging too frequently
train stats after 215552 examples: {'rewards_train/chosen': '0.12038', 'rewards_train/rejected': '0.029077', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091301', 'logps_train/rejected': '-124.97', 'logps_train/chosen': '-122.12', 'loss/train': '0.66425', 'examples_per_second': '33.297', 'grad_norm': '28', 'counters/examples': 215552, 'counters/updates': 6736}
train stats after 215584 examples: {'rewards_train/chosen': '0.12787', 'rewards_train/rejected': '0.056796', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071079', 'logps_train/rejected': '-114.83', 'logps_train/chosen': '-148.84', 'loss/train': '0.67066', 'examples_per_second': '31.516', 'grad_norm': '38.75', 'counters/examples': 215584, 'counters/updates': 6737}
train stats after 215616 examples: {'rewards_train/chosen': '0.21693', 'rewards_train/rejected': '0.075677', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14125', 'logps_train/rejected': '-150.52', 'logps_train/chosen': '-138.85', 'loss/train': '0.63457', 'examples_per_second': '30.513', 'grad_norm': '31.5', 'counters/examples': 215616, 'counters/updates': 6738}
train stats after 215648 examples: {'rewards_train/chosen': '0.24262', 'rewards_train/rejected': '0.068756', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17387', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-154.01', 'loss/train': '0.62514', 'examples_per_second': '30.659', 'grad_norm': '29.25', 'counters/examples': 215648, 'counters/updates': 6739}
skipping logging after 215680 examples to avoid logging too frequently
train stats after 215712 examples: {'rewards_train/chosen': '0.12864', 'rewards_train/rejected': '-0.053237', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18188', 'logps_train/rejected': '-126.96', 'logps_train/chosen': '-169.83', 'loss/train': '0.61904', 'examples_per_second': '33.254', 'grad_norm': '30', 'counters/examples': 215712, 'counters/updates': 6741}
train stats after 215744 examples: {'rewards_train/chosen': '0.19019', 'rewards_train/rejected': '0.10463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085557', 'logps_train/rejected': '-110.69', 'logps_train/chosen': '-147.99', 'loss/train': '0.66442', 'examples_per_second': '30.446', 'grad_norm': '28.5', 'counters/examples': 215744, 'counters/updates': 6742}
skipping logging after 215776 examples to avoid logging too frequently
train stats after 215808 examples: {'rewards_train/chosen': '0.10224', 'rewards_train/rejected': '0.068661', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.033579', 'logps_train/rejected': '-146.36', 'logps_train/chosen': '-159.53', 'loss/train': '0.70542', 'examples_per_second': '31.833', 'grad_norm': '33.25', 'counters/examples': 215808, 'counters/updates': 6744}
train stats after 215840 examples: {'rewards_train/chosen': '0.076003', 'rewards_train/rejected': '0.084984', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0089805', 'logps_train/rejected': '-143.56', 'logps_train/chosen': '-146.43', 'loss/train': '0.72808', 'examples_per_second': '30.208', 'grad_norm': '58.25', 'counters/examples': 215840, 'counters/updates': 6745}
train stats after 215872 examples: {'rewards_train/chosen': '0.17961', 'rewards_train/rejected': '0.078185', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10142', 'logps_train/rejected': '-116.43', 'logps_train/chosen': '-146.81', 'loss/train': '0.66199', 'examples_per_second': '31.444', 'grad_norm': '34.25', 'counters/examples': 215872, 'counters/updates': 6746}
train stats after 215904 examples: {'rewards_train/chosen': '0.20271', 'rewards_train/rejected': '0.1086', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094102', 'logps_train/rejected': '-157.61', 'logps_train/chosen': '-146.84', 'loss/train': '0.66403', 'examples_per_second': '31.296', 'grad_norm': '44.25', 'counters/examples': 215904, 'counters/updates': 6747}
skipping logging after 215936 examples to avoid logging too frequently
train stats after 215968 examples: {'rewards_train/chosen': '0.23368', 'rewards_train/rejected': '0.13928', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.094401', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-168.75', 'loss/train': '0.66566', 'examples_per_second': '35.675', 'grad_norm': '34.75', 'counters/examples': 215968, 'counters/updates': 6749}
train stats after 216000 examples: {'rewards_train/chosen': '0.22671', 'rewards_train/rejected': '0.10261', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1241', 'logps_train/rejected': '-155.94', 'logps_train/chosen': '-149.5', 'loss/train': '0.66446', 'examples_per_second': '30', 'grad_norm': '51.75', 'counters/examples': 216000, 'counters/updates': 6750}
Running evaluation after 216000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.70it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.89it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.81it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.06it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.80it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.89it/s]
eval after 216000: {'rewards_eval/chosen': '0.18366', 'rewards_eval/rejected': '0.07152', 'rewards_eval/accuracies': '0.58594', 'rewards_eval/margins': '0.11214', 'logps_eval/rejected': '-121.42', 'logps_eval/chosen': '-142.27', 'loss/eval': '0.65361'}
skipping save for non epoch
train stats after 216032 examples: {'rewards_train/chosen': '0.21369', 'rewards_train/rejected': '0.093656', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.12004', 'logps_train/rejected': '-131.78', 'logps_train/chosen': '-149.14', 'loss/train': '0.6709', 'examples_per_second': '34.211', 'grad_norm': '27.5', 'counters/examples': 216032, 'counters/updates': 6751}
train stats after 216064 examples: {'rewards_train/chosen': '0.24087', 'rewards_train/rejected': '-0.0019971', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24287', 'logps_train/rejected': '-142.88', 'logps_train/chosen': '-142.94', 'loss/train': '0.60453', 'examples_per_second': '32.28', 'grad_norm': '31.25', 'counters/examples': 216064, 'counters/updates': 6752}
skipping logging after 216096 examples to avoid logging too frequently
train stats after 216128 examples: {'rewards_train/chosen': '0.28067', 'rewards_train/rejected': '0.046933', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23374', 'logps_train/rejected': '-144.5', 'logps_train/chosen': '-207.23', 'loss/train': '0.60491', 'examples_per_second': '30.049', 'grad_norm': '36', 'counters/examples': 216128, 'counters/updates': 6754}
train stats after 216160 examples: {'rewards_train/chosen': '0.028187', 'rewards_train/rejected': '0.065687', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0375', 'logps_train/rejected': '-129.01', 'logps_train/chosen': '-135.93', 'loss/train': '0.72577', 'examples_per_second': '32.038', 'grad_norm': '55.5', 'counters/examples': 216160, 'counters/updates': 6755}
train stats after 216192 examples: {'rewards_train/chosen': '0.12047', 'rewards_train/rejected': '0.078399', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042075', 'logps_train/rejected': '-175.74', 'logps_train/chosen': '-157.52', 'loss/train': '0.68238', 'examples_per_second': '31.195', 'grad_norm': '43.75', 'counters/examples': 216192, 'counters/updates': 6756}
train stats after 216224 examples: {'rewards_train/chosen': '0.15855', 'rewards_train/rejected': '0.052586', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10597', 'logps_train/rejected': '-135.91', 'logps_train/chosen': '-122.37', 'loss/train': '0.65033', 'examples_per_second': '30.861', 'grad_norm': '30.875', 'counters/examples': 216224, 'counters/updates': 6757}
train stats after 216256 examples: {'rewards_train/chosen': '0.16326', 'rewards_train/rejected': '0.035524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12773', 'logps_train/rejected': '-133.67', 'logps_train/chosen': '-180.87', 'loss/train': '0.65009', 'examples_per_second': '31.842', 'grad_norm': '33.25', 'counters/examples': 216256, 'counters/updates': 6758}
train stats after 216288 examples: {'rewards_train/chosen': '0.1486', 'rewards_train/rejected': '0.027064', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12153', 'logps_train/rejected': '-164.03', 'logps_train/chosen': '-125.99', 'loss/train': '0.64588', 'examples_per_second': '31.136', 'grad_norm': '36.5', 'counters/examples': 216288, 'counters/updates': 6759}
train stats after 216320 examples: {'rewards_train/chosen': '0.20567', 'rewards_train/rejected': '0.068011', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13766', 'logps_train/rejected': '-149.99', 'logps_train/chosen': '-191.56', 'loss/train': '0.63899', 'examples_per_second': '31.256', 'grad_norm': '28', 'counters/examples': 216320, 'counters/updates': 6760}
train stats after 216352 examples: {'rewards_train/chosen': '0.041954', 'rewards_train/rejected': '0.077594', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.03564', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-166.54', 'loss/train': '0.72307', 'examples_per_second': '32.981', 'grad_norm': '34', 'counters/examples': 216352, 'counters/updates': 6761}
train stats after 216384 examples: {'rewards_train/chosen': '0.20667', 'rewards_train/rejected': '0.041325', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16535', 'logps_train/rejected': '-100.76', 'logps_train/chosen': '-163.38', 'loss/train': '0.62745', 'examples_per_second': '31.488', 'grad_norm': '32', 'counters/examples': 216384, 'counters/updates': 6762}
train stats after 216416 examples: {'rewards_train/chosen': '0.19693', 'rewards_train/rejected': '0.099771', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097155', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-134.7', 'loss/train': '0.65294', 'examples_per_second': '30.003', 'grad_norm': '30.125', 'counters/examples': 216416, 'counters/updates': 6763}
skipping logging after 216448 examples to avoid logging too frequently
train stats after 216480 examples: {'rewards_train/chosen': '0.16496', 'rewards_train/rejected': '0.041017', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12394', 'logps_train/rejected': '-132.88', 'logps_train/chosen': '-147.82', 'loss/train': '0.64228', 'examples_per_second': '33.539', 'grad_norm': '27.375', 'counters/examples': 216480, 'counters/updates': 6765}
train stats after 216512 examples: {'rewards_train/chosen': '0.2215', 'rewards_train/rejected': '-0.011474', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23298', 'logps_train/rejected': '-126.44', 'logps_train/chosen': '-140.17', 'loss/train': '0.59695', 'examples_per_second': '31.665', 'grad_norm': '34.25', 'counters/examples': 216512, 'counters/updates': 6766}
train stats after 216544 examples: {'rewards_train/chosen': '0.19987', 'rewards_train/rejected': '0.079684', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12018', 'logps_train/rejected': '-165.69', 'logps_train/chosen': '-177.67', 'loss/train': '0.6496', 'examples_per_second': '31.448', 'grad_norm': '28.875', 'counters/examples': 216544, 'counters/updates': 6767}
train stats after 216576 examples: {'rewards_train/chosen': '0.15942', 'rewards_train/rejected': '0.0121', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14732', 'logps_train/rejected': '-153.75', 'logps_train/chosen': '-142.17', 'loss/train': '0.64014', 'examples_per_second': '30.539', 'grad_norm': '29.75', 'counters/examples': 216576, 'counters/updates': 6768}
train stats after 216608 examples: {'rewards_train/chosen': '0.15455', 'rewards_train/rejected': '0.02557', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12898', 'logps_train/rejected': '-118.93', 'logps_train/chosen': '-118.49', 'loss/train': '0.6369', 'examples_per_second': '31.642', 'grad_norm': '32.75', 'counters/examples': 216608, 'counters/updates': 6769}
train stats after 216640 examples: {'rewards_train/chosen': '0.097538', 'rewards_train/rejected': '0.11465', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.017113', 'logps_train/rejected': '-149.02', 'logps_train/chosen': '-127.39', 'loss/train': '0.71324', 'examples_per_second': '29.997', 'grad_norm': '36.75', 'counters/examples': 216640, 'counters/updates': 6770}
skipping logging after 216672 examples to avoid logging too frequently
train stats after 216704 examples: {'rewards_train/chosen': '0.22616', 'rewards_train/rejected': '0.023498', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20266', 'logps_train/rejected': '-120.46', 'logps_train/chosen': '-166.26', 'loss/train': '0.61367', 'examples_per_second': '31.722', 'grad_norm': '28.25', 'counters/examples': 216704, 'counters/updates': 6772}
train stats after 216736 examples: {'rewards_train/chosen': '0.10345', 'rewards_train/rejected': '0.0046408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098813', 'logps_train/rejected': '-147.88', 'logps_train/chosen': '-158.4', 'loss/train': '0.65066', 'examples_per_second': '30.868', 'grad_norm': '37', 'counters/examples': 216736, 'counters/updates': 6773}
train stats after 216768 examples: {'rewards_train/chosen': '0.20538', 'rewards_train/rejected': '0.010314', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19506', 'logps_train/rejected': '-138.75', 'logps_train/chosen': '-119.52', 'loss/train': '0.61472', 'examples_per_second': '31.382', 'grad_norm': '46.25', 'counters/examples': 216768, 'counters/updates': 6774}
skipping logging after 216800 examples to avoid logging too frequently
train stats after 216832 examples: {'rewards_train/chosen': '0.19161', 'rewards_train/rejected': '-0.036392', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.228', 'logps_train/rejected': '-97.936', 'logps_train/chosen': '-137.67', 'loss/train': '0.61012', 'examples_per_second': '30.142', 'grad_norm': '31.875', 'counters/examples': 216832, 'counters/updates': 6776}
skipping logging after 216864 examples to avoid logging too frequently
train stats after 216896 examples: {'rewards_train/chosen': '0.1303', 'rewards_train/rejected': '-0.0081729', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13847', 'logps_train/rejected': '-91.528', 'logps_train/chosen': '-127.22', 'loss/train': '0.6341', 'examples_per_second': '35.364', 'grad_norm': '24.625', 'counters/examples': 216896, 'counters/updates': 6778}
train stats after 216928 examples: {'rewards_train/chosen': '0.16706', 'rewards_train/rejected': '0.14324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.023828', 'logps_train/rejected': '-109.77', 'logps_train/chosen': '-134.65', 'loss/train': '0.70733', 'examples_per_second': '32.923', 'grad_norm': '43.25', 'counters/examples': 216928, 'counters/updates': 6779}
train stats after 216960 examples: {'rewards_train/chosen': '0.19323', 'rewards_train/rejected': '0.069406', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12382', 'logps_train/rejected': '-105.72', 'logps_train/chosen': '-143.69', 'loss/train': '0.64124', 'examples_per_second': '31.493', 'grad_norm': '27', 'counters/examples': 216960, 'counters/updates': 6780}
train stats after 216992 examples: {'rewards_train/chosen': '0.13232', 'rewards_train/rejected': '0.060002', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072317', 'logps_train/rejected': '-138', 'logps_train/chosen': '-112.35', 'loss/train': '0.67332', 'examples_per_second': '32.811', 'grad_norm': '28.5', 'counters/examples': 216992, 'counters/updates': 6781}
train stats after 217024 examples: {'rewards_train/chosen': '0.29884', 'rewards_train/rejected': '0.15021', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14862', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-148.91', 'loss/train': '0.64732', 'examples_per_second': '30.861', 'grad_norm': '27.125', 'counters/examples': 217024, 'counters/updates': 6782}
train stats after 217056 examples: {'rewards_train/chosen': '0.14748', 'rewards_train/rejected': '0.022428', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12505', 'logps_train/rejected': '-124.21', 'logps_train/chosen': '-165.31', 'loss/train': '0.64258', 'examples_per_second': '31.543', 'grad_norm': '26.375', 'counters/examples': 217056, 'counters/updates': 6783}
skipping logging after 217088 examples to avoid logging too frequently
train stats after 217120 examples: {'rewards_train/chosen': '0.14214', 'rewards_train/rejected': '0.015919', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12622', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-104.81', 'loss/train': '0.65058', 'examples_per_second': '34.213', 'grad_norm': '24.125', 'counters/examples': 217120, 'counters/updates': 6785}
train stats after 217152 examples: {'rewards_train/chosen': '0.13567', 'rewards_train/rejected': '0.062351', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.073324', 'logps_train/rejected': '-123.31', 'logps_train/chosen': '-154.47', 'loss/train': '0.66885', 'examples_per_second': '30.662', 'grad_norm': '30.375', 'counters/examples': 217152, 'counters/updates': 6786}
train stats after 217184 examples: {'rewards_train/chosen': '0.23739', 'rewards_train/rejected': '0.15171', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.085678', 'logps_train/rejected': '-151.21', 'logps_train/chosen': '-155.1', 'loss/train': '0.66769', 'examples_per_second': '31.503', 'grad_norm': '37', 'counters/examples': 217184, 'counters/updates': 6787}
train stats after 217216 examples: {'rewards_train/chosen': '0.11767', 'rewards_train/rejected': '0.073069', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.044602', 'logps_train/rejected': '-135.48', 'logps_train/chosen': '-145.56', 'loss/train': '0.68748', 'examples_per_second': '31.274', 'grad_norm': '33.5', 'counters/examples': 217216, 'counters/updates': 6788}
train stats after 217248 examples: {'rewards_train/chosen': '0.20213', 'rewards_train/rejected': '0.057601', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14453', 'logps_train/rejected': '-177.92', 'logps_train/chosen': '-172.1', 'loss/train': '0.64223', 'examples_per_second': '32.257', 'grad_norm': '45', 'counters/examples': 217248, 'counters/updates': 6789}
train stats after 217280 examples: {'rewards_train/chosen': '0.16991', 'rewards_train/rejected': '-0.083351', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25326', 'logps_train/rejected': '-178.01', 'logps_train/chosen': '-177.29', 'loss/train': '0.60061', 'examples_per_second': '31.518', 'grad_norm': '27.25', 'counters/examples': 217280, 'counters/updates': 6790}
train stats after 217312 examples: {'rewards_train/chosen': '0.1034', 'rewards_train/rejected': '0.019', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084398', 'logps_train/rejected': '-113.87', 'logps_train/chosen': '-124.3', 'loss/train': '0.6604', 'examples_per_second': '31.517', 'grad_norm': '28.125', 'counters/examples': 217312, 'counters/updates': 6791}
train stats after 217344 examples: {'rewards_train/chosen': '0.19871', 'rewards_train/rejected': '0.046187', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15253', 'logps_train/rejected': '-119.32', 'logps_train/chosen': '-161.68', 'loss/train': '0.64135', 'examples_per_second': '31.035', 'grad_norm': '41', 'counters/examples': 217344, 'counters/updates': 6792}
train stats after 217376 examples: {'rewards_train/chosen': '0.17191', 'rewards_train/rejected': '0.02626', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14565', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-127.58', 'loss/train': '0.63495', 'examples_per_second': '30.374', 'grad_norm': '27.5', 'counters/examples': 217376, 'counters/updates': 6793}
train stats after 217408 examples: {'rewards_train/chosen': '0.15501', 'rewards_train/rejected': '0.021167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13385', 'logps_train/rejected': '-121.77', 'logps_train/chosen': '-157.6', 'loss/train': '0.63773', 'examples_per_second': '30.97', 'grad_norm': '32.5', 'counters/examples': 217408, 'counters/updates': 6794}
train stats after 217440 examples: {'rewards_train/chosen': '0.20929', 'rewards_train/rejected': '0.085812', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12348', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-135.72', 'loss/train': '0.65178', 'examples_per_second': '30.365', 'grad_norm': '28.75', 'counters/examples': 217440, 'counters/updates': 6795}
skipping logging after 217472 examples to avoid logging too frequently
train stats after 217504 examples: {'rewards_train/chosen': '0.14113', 'rewards_train/rejected': '0.050681', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090454', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-170.98', 'loss/train': '0.6662', 'examples_per_second': '33.102', 'grad_norm': '34.75', 'counters/examples': 217504, 'counters/updates': 6797}
train stats after 217536 examples: {'rewards_train/chosen': '0.13044', 'rewards_train/rejected': '0.017759', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11268', 'logps_train/rejected': '-119.25', 'logps_train/chosen': '-109.11', 'loss/train': '0.64884', 'examples_per_second': '30.892', 'grad_norm': '38', 'counters/examples': 217536, 'counters/updates': 6798}
train stats after 217568 examples: {'rewards_train/chosen': '0.1123', 'rewards_train/rejected': '0.13502', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.022722', 'logps_train/rejected': '-141.76', 'logps_train/chosen': '-115.59', 'loss/train': '0.73463', 'examples_per_second': '32.412', 'grad_norm': '52.75', 'counters/examples': 217568, 'counters/updates': 6799}
skipping logging after 217600 examples to avoid logging too frequently
train stats after 217632 examples: {'rewards_train/chosen': '0.064352', 'rewards_train/rejected': '0.026796', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.037556', 'logps_train/rejected': '-125.51', 'logps_train/chosen': '-120.14', 'loss/train': '0.68516', 'examples_per_second': '25.405', 'grad_norm': '28.125', 'counters/examples': 217632, 'counters/updates': 6801}
train stats after 217664 examples: {'rewards_train/chosen': '0.21185', 'rewards_train/rejected': '0.11725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094599', 'logps_train/rejected': '-124.84', 'logps_train/chosen': '-185.35', 'loss/train': '0.65998', 'examples_per_second': '29.934', 'grad_norm': '43.25', 'counters/examples': 217664, 'counters/updates': 6802}
train stats after 217696 examples: {'rewards_train/chosen': '0.10233', 'rewards_train/rejected': '0.033389', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.068942', 'logps_train/rejected': '-124.15', 'logps_train/chosen': '-178.78', 'loss/train': '0.6791', 'examples_per_second': '31.416', 'grad_norm': '32.5', 'counters/examples': 217696, 'counters/updates': 6803}
train stats after 217728 examples: {'rewards_train/chosen': '0.14068', 'rewards_train/rejected': '-0.062572', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20325', 'logps_train/rejected': '-107.45', 'logps_train/chosen': '-149.34', 'loss/train': '0.60927', 'examples_per_second': '30.069', 'grad_norm': '27.125', 'counters/examples': 217728, 'counters/updates': 6804}
skipping logging after 217760 examples to avoid logging too frequently
train stats after 217792 examples: {'rewards_train/chosen': '0.23162', 'rewards_train/rejected': '0.14736', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084251', 'logps_train/rejected': '-152.95', 'logps_train/chosen': '-160.04', 'loss/train': '0.66948', 'examples_per_second': '32.313', 'grad_norm': '35', 'counters/examples': 217792, 'counters/updates': 6806}
train stats after 217824 examples: {'rewards_train/chosen': '0.15324', 'rewards_train/rejected': '-0.067045', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22029', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-144', 'loss/train': '0.60651', 'examples_per_second': '30.643', 'grad_norm': '25', 'counters/examples': 217824, 'counters/updates': 6807}
train stats after 217856 examples: {'rewards_train/chosen': '0.12705', 'rewards_train/rejected': '0.042935', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.084114', 'logps_train/rejected': '-135.88', 'logps_train/chosen': '-166.79', 'loss/train': '0.66463', 'examples_per_second': '30.466', 'grad_norm': '31.5', 'counters/examples': 217856, 'counters/updates': 6808}
train stats after 217888 examples: {'rewards_train/chosen': '0.10929', 'rewards_train/rejected': '0.044843', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064451', 'logps_train/rejected': '-100.2', 'logps_train/chosen': '-129.63', 'loss/train': '0.67119', 'examples_per_second': '31.225', 'grad_norm': '31.25', 'counters/examples': 217888, 'counters/updates': 6809}
skipping logging after 217920 examples to avoid logging too frequently
train stats after 217952 examples: {'rewards_train/chosen': '0.10264', 'rewards_train/rejected': '0.063143', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039497', 'logps_train/rejected': '-114.56', 'logps_train/chosen': '-131.04', 'loss/train': '0.6875', 'examples_per_second': '32.095', 'grad_norm': '31.75', 'counters/examples': 217952, 'counters/updates': 6811}
train stats after 217984 examples: {'rewards_train/chosen': '0.10249', 'rewards_train/rejected': '0.080138', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.022357', 'logps_train/rejected': '-118.91', 'logps_train/chosen': '-127.81', 'loss/train': '0.69245', 'examples_per_second': '30.404', 'grad_norm': '39.5', 'counters/examples': 217984, 'counters/updates': 6812}
train stats after 218016 examples: {'rewards_train/chosen': '0.18881', 'rewards_train/rejected': '0.085126', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10368', 'logps_train/rejected': '-130.1', 'logps_train/chosen': '-163.69', 'loss/train': '0.65944', 'examples_per_second': '31.22', 'grad_norm': '33.5', 'counters/examples': 218016, 'counters/updates': 6813}
train stats after 218048 examples: {'rewards_train/chosen': '0.16101', 'rewards_train/rejected': '0.018491', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14252', 'logps_train/rejected': '-135.47', 'logps_train/chosen': '-144.59', 'loss/train': '0.64135', 'examples_per_second': '31.119', 'grad_norm': '41.25', 'counters/examples': 218048, 'counters/updates': 6814}
train stats after 218080 examples: {'rewards_train/chosen': '0.23387', 'rewards_train/rejected': '0.063276', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17059', 'logps_train/rejected': '-163.12', 'logps_train/chosen': '-216.33', 'loss/train': '0.63955', 'examples_per_second': '31.432', 'grad_norm': '48.75', 'counters/examples': 218080, 'counters/updates': 6815}
train stats after 218112 examples: {'rewards_train/chosen': '0.19517', 'rewards_train/rejected': '0.057782', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13739', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-129.58', 'loss/train': '0.64196', 'examples_per_second': '31.848', 'grad_norm': '36.75', 'counters/examples': 218112, 'counters/updates': 6816}
train stats after 218144 examples: {'rewards_train/chosen': '0.1647', 'rewards_train/rejected': '0.078529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.086169', 'logps_train/rejected': '-131', 'logps_train/chosen': '-131.93', 'loss/train': '0.66571', 'examples_per_second': '31.378', 'grad_norm': '28.375', 'counters/examples': 218144, 'counters/updates': 6817}
train stats after 218176 examples: {'rewards_train/chosen': '0.079998', 'rewards_train/rejected': '0.010809', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069189', 'logps_train/rejected': '-145.03', 'logps_train/chosen': '-130.51', 'loss/train': '0.67079', 'examples_per_second': '29.738', 'grad_norm': '25.5', 'counters/examples': 218176, 'counters/updates': 6818}
train stats after 218208 examples: {'rewards_train/chosen': '0.21738', 'rewards_train/rejected': '0.019624', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19776', 'logps_train/rejected': '-149.69', 'logps_train/chosen': '-160.14', 'loss/train': '0.62075', 'examples_per_second': '32.073', 'grad_norm': '35.25', 'counters/examples': 218208, 'counters/updates': 6819}
train stats after 218240 examples: {'rewards_train/chosen': '0.15342', 'rewards_train/rejected': '0.1063', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047119', 'logps_train/rejected': '-160.62', 'logps_train/chosen': '-148.84', 'loss/train': '0.68856', 'examples_per_second': '32.159', 'grad_norm': '49.5', 'counters/examples': 218240, 'counters/updates': 6820}
train stats after 218272 examples: {'rewards_train/chosen': '0.062531', 'rewards_train/rejected': '-0.01877', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.081301', 'logps_train/rejected': '-96.701', 'logps_train/chosen': '-109.1', 'loss/train': '0.67139', 'examples_per_second': '31.576', 'grad_norm': '25.5', 'counters/examples': 218272, 'counters/updates': 6821}
train stats after 218304 examples: {'rewards_train/chosen': '0.15013', 'rewards_train/rejected': '0.0565', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.093629', 'logps_train/rejected': '-95.514', 'logps_train/chosen': '-148.19', 'loss/train': '0.65841', 'examples_per_second': '31.513', 'grad_norm': '25.75', 'counters/examples': 218304, 'counters/updates': 6822}
train stats after 218336 examples: {'rewards_train/chosen': '0.20235', 'rewards_train/rejected': '0.043021', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15933', 'logps_train/rejected': '-113.02', 'logps_train/chosen': '-133.8', 'loss/train': '0.62403', 'examples_per_second': '30.8', 'grad_norm': '23.25', 'counters/examples': 218336, 'counters/updates': 6823}
train stats after 218368 examples: {'rewards_train/chosen': '0.17184', 'rewards_train/rejected': '0.032827', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13901', 'logps_train/rejected': '-116.99', 'logps_train/chosen': '-119.15', 'loss/train': '0.63426', 'examples_per_second': '31.389', 'grad_norm': '34.25', 'counters/examples': 218368, 'counters/updates': 6824}
train stats after 218400 examples: {'rewards_train/chosen': '0.13846', 'rewards_train/rejected': '0.091119', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047342', 'logps_train/rejected': '-142.07', 'logps_train/chosen': '-147.26', 'loss/train': '0.68', 'examples_per_second': '31.242', 'grad_norm': '31.375', 'counters/examples': 218400, 'counters/updates': 6825}
skipping logging after 218432 examples to avoid logging too frequently
train stats after 218464 examples: {'rewards_train/chosen': '0.10122', 'rewards_train/rejected': '-0.0092516', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11047', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-150.75', 'loss/train': '0.65235', 'examples_per_second': '31.24', 'grad_norm': '33.75', 'counters/examples': 218464, 'counters/updates': 6827}
train stats after 218496 examples: {'rewards_train/chosen': '0.20228', 'rewards_train/rejected': '-0.022602', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22488', 'logps_train/rejected': '-133.04', 'logps_train/chosen': '-157.16', 'loss/train': '0.6039', 'examples_per_second': '31.187', 'grad_norm': '36.75', 'counters/examples': 218496, 'counters/updates': 6828}
train stats after 218528 examples: {'rewards_train/chosen': '0.088228', 'rewards_train/rejected': '-0.0022233', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090452', 'logps_train/rejected': '-95.417', 'logps_train/chosen': '-158.76', 'loss/train': '0.65943', 'examples_per_second': '31.878', 'grad_norm': '32', 'counters/examples': 218528, 'counters/updates': 6829}
skipping logging after 218560 examples to avoid logging too frequently
train stats after 218592 examples: {'rewards_train/chosen': '0.15022', 'rewards_train/rejected': '0.012174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13805', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-141.5', 'loss/train': '0.64404', 'examples_per_second': '30.029', 'grad_norm': '31.125', 'counters/examples': 218592, 'counters/updates': 6831}
skipping logging after 218624 examples to avoid logging too frequently
train stats after 218656 examples: {'rewards_train/chosen': '0.17658', 'rewards_train/rejected': '-0.036618', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21319', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-149.06', 'loss/train': '0.60838', 'examples_per_second': '33.848', 'grad_norm': '24.125', 'counters/examples': 218656, 'counters/updates': 6833}
train stats after 218688 examples: {'rewards_train/chosen': '0.19354', 'rewards_train/rejected': '0.043841', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1497', 'logps_train/rejected': '-133.28', 'logps_train/chosen': '-165.85', 'loss/train': '0.64011', 'examples_per_second': '31.584', 'grad_norm': '26.75', 'counters/examples': 218688, 'counters/updates': 6834}
train stats after 218720 examples: {'rewards_train/chosen': '0.24162', 'rewards_train/rejected': '0.19402', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.047599', 'logps_train/rejected': '-145.85', 'logps_train/chosen': '-164.96', 'loss/train': '0.68569', 'examples_per_second': '31.368', 'grad_norm': '51', 'counters/examples': 218720, 'counters/updates': 6835}
train stats after 218752 examples: {'rewards_train/chosen': '0.20303', 'rewards_train/rejected': '0.041486', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16155', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-162.15', 'loss/train': '0.62414', 'examples_per_second': '24.185', 'grad_norm': '31.375', 'counters/examples': 218752, 'counters/updates': 6836}
train stats after 218784 examples: {'rewards_train/chosen': '0.19103', 'rewards_train/rejected': '0.07139', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.11964', 'logps_train/rejected': '-104.94', 'logps_train/chosen': '-130.16', 'loss/train': '0.67813', 'examples_per_second': '32.134', 'grad_norm': '32.75', 'counters/examples': 218784, 'counters/updates': 6837}
skipping logging after 218816 examples to avoid logging too frequently
train stats after 218848 examples: {'rewards_train/chosen': '0.34299', 'rewards_train/rejected': '0.076899', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26609', 'logps_train/rejected': '-107.02', 'logps_train/chosen': '-142.04', 'loss/train': '0.59477', 'examples_per_second': '24.243', 'grad_norm': '26.875', 'counters/examples': 218848, 'counters/updates': 6839}
skipping logging after 218880 examples to avoid logging too frequently
train stats after 218912 examples: {'rewards_train/chosen': '0.11088', 'rewards_train/rejected': '-0.00082647', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11171', 'logps_train/rejected': '-95.488', 'logps_train/chosen': '-120.16', 'loss/train': '0.65185', 'examples_per_second': '32.798', 'grad_norm': '28', 'counters/examples': 218912, 'counters/updates': 6841}
train stats after 218944 examples: {'rewards_train/chosen': '0.2301', 'rewards_train/rejected': '0.095104', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13499', 'logps_train/rejected': '-117.06', 'logps_train/chosen': '-145.78', 'loss/train': '0.63763', 'examples_per_second': '31.432', 'grad_norm': '28.75', 'counters/examples': 218944, 'counters/updates': 6842}
train stats after 218976 examples: {'rewards_train/chosen': '0.14402', 'rewards_train/rejected': '0.14205', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0019757', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-113.93', 'loss/train': '0.72466', 'examples_per_second': '29.889', 'grad_norm': '64', 'counters/examples': 218976, 'counters/updates': 6843}
train stats after 219008 examples: {'rewards_train/chosen': '0.17349', 'rewards_train/rejected': '0.11776', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.055731', 'logps_train/rejected': '-154.69', 'logps_train/chosen': '-155.72', 'loss/train': '0.67852', 'examples_per_second': '31.357', 'grad_norm': '31.125', 'counters/examples': 219008, 'counters/updates': 6844}
train stats after 219040 examples: {'rewards_train/chosen': '0.161', 'rewards_train/rejected': '-0.016233', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17723', 'logps_train/rejected': '-151.91', 'logps_train/chosen': '-183.15', 'loss/train': '0.62288', 'examples_per_second': '32.771', 'grad_norm': '38', 'counters/examples': 219040, 'counters/updates': 6845}
train stats after 219072 examples: {'rewards_train/chosen': '0.21471', 'rewards_train/rejected': '0.065055', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14966', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-143.83', 'loss/train': '0.64538', 'examples_per_second': '31.411', 'grad_norm': '27', 'counters/examples': 219072, 'counters/updates': 6846}
train stats after 219104 examples: {'rewards_train/chosen': '0.1282', 'rewards_train/rejected': '0.11413', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.014069', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-131.63', 'loss/train': '0.69863', 'examples_per_second': '32.967', 'grad_norm': '28.25', 'counters/examples': 219104, 'counters/updates': 6847}
train stats after 219136 examples: {'rewards_train/chosen': '0.12962', 'rewards_train/rejected': '0.075653', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.053971', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-110.43', 'loss/train': '0.67286', 'examples_per_second': '31.406', 'grad_norm': '29.125', 'counters/examples': 219136, 'counters/updates': 6848}
skipping logging after 219168 examples to avoid logging too frequently
train stats after 219200 examples: {'rewards_train/chosen': '0.18912', 'rewards_train/rejected': '0.043171', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14595', 'logps_train/rejected': '-133.43', 'logps_train/chosen': '-164.75', 'loss/train': '0.63516', 'examples_per_second': '30.88', 'grad_norm': '47.75', 'counters/examples': 219200, 'counters/updates': 6850}
train stats after 219232 examples: {'rewards_train/chosen': '0.091239', 'rewards_train/rejected': '-0.0040553', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.095295', 'logps_train/rejected': '-117.44', 'logps_train/chosen': '-111.02', 'loss/train': '0.65889', 'examples_per_second': '30.38', 'grad_norm': '24.375', 'counters/examples': 219232, 'counters/updates': 6851}
skipping logging after 219264 examples to avoid logging too frequently
train stats after 219296 examples: {'rewards_train/chosen': '0.23056', 'rewards_train/rejected': '0.0044465', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22611', 'logps_train/rejected': '-125.43', 'logps_train/chosen': '-141.07', 'loss/train': '0.60607', 'examples_per_second': '31.982', 'grad_norm': '28.5', 'counters/examples': 219296, 'counters/updates': 6853}
train stats after 219328 examples: {'rewards_train/chosen': '0.13905', 'rewards_train/rejected': '0.0025618', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13649', 'logps_train/rejected': '-131.31', 'logps_train/chosen': '-150.49', 'loss/train': '0.64355', 'examples_per_second': '31.252', 'grad_norm': '29.25', 'counters/examples': 219328, 'counters/updates': 6854}
train stats after 219360 examples: {'rewards_train/chosen': '0.15781', 'rewards_train/rejected': '-0.017215', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17503', 'logps_train/rejected': '-135.67', 'logps_train/chosen': '-125.88', 'loss/train': '0.62281', 'examples_per_second': '32.116', 'grad_norm': '30.75', 'counters/examples': 219360, 'counters/updates': 6855}
skipping logging after 219392 examples to avoid logging too frequently
train stats after 219424 examples: {'rewards_train/chosen': '0.20549', 'rewards_train/rejected': '0.017711', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18778', 'logps_train/rejected': '-105.59', 'logps_train/chosen': '-161.05', 'loss/train': '0.64648', 'examples_per_second': '32.357', 'grad_norm': '34.25', 'counters/examples': 219424, 'counters/updates': 6857}
train stats after 219456 examples: {'rewards_train/chosen': '0.1482', 'rewards_train/rejected': '0.0016323', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14657', 'logps_train/rejected': '-112.21', 'logps_train/chosen': '-150.83', 'loss/train': '0.63388', 'examples_per_second': '30.588', 'grad_norm': '26', 'counters/examples': 219456, 'counters/updates': 6858}
train stats after 219488 examples: {'rewards_train/chosen': '0.1291', 'rewards_train/rejected': '0.13325', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0041484', 'logps_train/rejected': '-154.73', 'logps_train/chosen': '-161.65', 'loss/train': '0.70833', 'examples_per_second': '31.447', 'grad_norm': '29.625', 'counters/examples': 219488, 'counters/updates': 6859}
skipping logging after 219520 examples to avoid logging too frequently
train stats after 219552 examples: {'rewards_train/chosen': '0.17671', 'rewards_train/rejected': '0.021121', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15559', 'logps_train/rejected': '-121.7', 'logps_train/chosen': '-137.51', 'loss/train': '0.63396', 'examples_per_second': '32.014', 'grad_norm': '25.5', 'counters/examples': 219552, 'counters/updates': 6861}
train stats after 219584 examples: {'rewards_train/chosen': '0.11337', 'rewards_train/rejected': '0.016668', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096703', 'logps_train/rejected': '-136.27', 'logps_train/chosen': '-173.29', 'loss/train': '0.65805', 'examples_per_second': '30.397', 'grad_norm': '38.75', 'counters/examples': 219584, 'counters/updates': 6862}
train stats after 219616 examples: {'rewards_train/chosen': '0.1884', 'rewards_train/rejected': '0.045024', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14337', 'logps_train/rejected': '-134.4', 'logps_train/chosen': '-170.66', 'loss/train': '0.64999', 'examples_per_second': '30.518', 'grad_norm': '31.75', 'counters/examples': 219616, 'counters/updates': 6863}
train stats after 219648 examples: {'rewards_train/chosen': '0.16965', 'rewards_train/rejected': '0.020835', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14881', 'logps_train/rejected': '-106.74', 'logps_train/chosen': '-134.37', 'loss/train': '0.63416', 'examples_per_second': '32.021', 'grad_norm': '30.25', 'counters/examples': 219648, 'counters/updates': 6864}
train stats after 219680 examples: {'rewards_train/chosen': '0.052869', 'rewards_train/rejected': '-0.044967', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097836', 'logps_train/rejected': '-125.39', 'logps_train/chosen': '-148.49', 'loss/train': '0.65837', 'examples_per_second': '31.467', 'grad_norm': '28.375', 'counters/examples': 219680, 'counters/updates': 6865}
train stats after 219712 examples: {'rewards_train/chosen': '0.12979', 'rewards_train/rejected': '0.042056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.087739', 'logps_train/rejected': '-134.63', 'logps_train/chosen': '-164.4', 'loss/train': '0.66401', 'examples_per_second': '30.569', 'grad_norm': '33', 'counters/examples': 219712, 'counters/updates': 6866}
train stats after 219744 examples: {'rewards_train/chosen': '0.20475', 'rewards_train/rejected': '0.062152', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14259', 'logps_train/rejected': '-104.16', 'logps_train/chosen': '-134.44', 'loss/train': '0.63415', 'examples_per_second': '31.081', 'grad_norm': '24.875', 'counters/examples': 219744, 'counters/updates': 6867}
train stats after 219776 examples: {'rewards_train/chosen': '0.15759', 'rewards_train/rejected': '0.03333', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12426', 'logps_train/rejected': '-144.4', 'logps_train/chosen': '-148.03', 'loss/train': '0.65343', 'examples_per_second': '31.353', 'grad_norm': '45.5', 'counters/examples': 219776, 'counters/updates': 6868}
train stats after 219808 examples: {'rewards_train/chosen': '0.076573', 'rewards_train/rejected': '0.054815', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.021758', 'logps_train/rejected': '-109.99', 'logps_train/chosen': '-148.29', 'loss/train': '0.68943', 'examples_per_second': '31.75', 'grad_norm': '33.75', 'counters/examples': 219808, 'counters/updates': 6869}
train stats after 219840 examples: {'rewards_train/chosen': '0.16855', 'rewards_train/rejected': '0.11567', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052877', 'logps_train/rejected': '-115.07', 'logps_train/chosen': '-125.21', 'loss/train': '0.67732', 'examples_per_second': '31.433', 'grad_norm': '28.625', 'counters/examples': 219840, 'counters/updates': 6870}
train stats after 219872 examples: {'rewards_train/chosen': '0.18775', 'rewards_train/rejected': '0.049552', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1382', 'logps_train/rejected': '-118.49', 'logps_train/chosen': '-150.99', 'loss/train': '0.63755', 'examples_per_second': '31.425', 'grad_norm': '26.5', 'counters/examples': 219872, 'counters/updates': 6871}
skipping logging after 219904 examples to avoid logging too frequently
train stats after 219936 examples: {'rewards_train/chosen': '0.079428', 'rewards_train/rejected': '-0.00041538', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079844', 'logps_train/rejected': '-157.97', 'logps_train/chosen': '-156.89', 'loss/train': '0.66095', 'examples_per_second': '29.829', 'grad_norm': '28.125', 'counters/examples': 219936, 'counters/updates': 6873}
skipping logging after 219968 examples to avoid logging too frequently
train stats after 220000 examples: {'rewards_train/chosen': '0.18948', 'rewards_train/rejected': '-0.035703', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22519', 'logps_train/rejected': '-151.4', 'logps_train/chosen': '-171.91', 'loss/train': '0.60948', 'examples_per_second': '30.065', 'grad_norm': '62', 'counters/examples': 220000, 'counters/updates': 6875}
train stats after 220032 examples: {'rewards_train/chosen': '0.11161', 'rewards_train/rejected': '0.012984', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.098631', 'logps_train/rejected': '-143.49', 'logps_train/chosen': '-148.79', 'loss/train': '0.65248', 'examples_per_second': '32.502', 'grad_norm': '30.5', 'counters/examples': 220032, 'counters/updates': 6876}
train stats after 220064 examples: {'rewards_train/chosen': '0.12579', 'rewards_train/rejected': '0.034632', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091161', 'logps_train/rejected': '-134.43', 'logps_train/chosen': '-96.295', 'loss/train': '0.66759', 'examples_per_second': '31.057', 'grad_norm': '29.5', 'counters/examples': 220064, 'counters/updates': 6877}
train stats after 220096 examples: {'rewards_train/chosen': '0.13005', 'rewards_train/rejected': '0.028007', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10205', 'logps_train/rejected': '-97.935', 'logps_train/chosen': '-108.89', 'loss/train': '0.65201', 'examples_per_second': '31.188', 'grad_norm': '25', 'counters/examples': 220096, 'counters/updates': 6878}
train stats after 220128 examples: {'rewards_train/chosen': '0.13745', 'rewards_train/rejected': '0.13181', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056366', 'logps_train/rejected': '-140.46', 'logps_train/chosen': '-138.86', 'loss/train': '0.71653', 'examples_per_second': '31.4', 'grad_norm': '43.5', 'counters/examples': 220128, 'counters/updates': 6879}
train stats after 220160 examples: {'rewards_train/chosen': '0.21823', 'rewards_train/rejected': '0.051482', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16675', 'logps_train/rejected': '-112.47', 'logps_train/chosen': '-132.28', 'loss/train': '0.61891', 'examples_per_second': '30.779', 'grad_norm': '25.5', 'counters/examples': 220160, 'counters/updates': 6880}
skipping logging after 220192 examples to avoid logging too frequently
train stats after 220224 examples: {'rewards_train/chosen': '0.088337', 'rewards_train/rejected': '0.04478', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.043557', 'logps_train/rejected': '-139.12', 'logps_train/chosen': '-139.26', 'loss/train': '0.68099', 'examples_per_second': '33.759', 'grad_norm': '44.5', 'counters/examples': 220224, 'counters/updates': 6882}
skipping logging after 220256 examples to avoid logging too frequently
train stats after 220288 examples: {'rewards_train/chosen': '0.15783', 'rewards_train/rejected': '0.093113', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064719', 'logps_train/rejected': '-140.38', 'logps_train/chosen': '-144.67', 'loss/train': '0.6788', 'examples_per_second': '30.953', 'grad_norm': '32.75', 'counters/examples': 220288, 'counters/updates': 6884}
skipping logging after 220320 examples to avoid logging too frequently
train stats after 220352 examples: {'rewards_train/chosen': '0.11539', 'rewards_train/rejected': '-0.012241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12763', 'logps_train/rejected': '-127.12', 'logps_train/chosen': '-132.88', 'loss/train': '0.64667', 'examples_per_second': '31.419', 'grad_norm': '26.75', 'counters/examples': 220352, 'counters/updates': 6886}
skipping logging after 220384 examples to avoid logging too frequently
train stats after 220416 examples: {'rewards_train/chosen': '0.15305', 'rewards_train/rejected': '0.042148', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1109', 'logps_train/rejected': '-120.94', 'logps_train/chosen': '-153.34', 'loss/train': '0.64423', 'examples_per_second': '29.99', 'grad_norm': '29.625', 'counters/examples': 220416, 'counters/updates': 6888}
train stats after 220448 examples: {'rewards_train/chosen': '0.18648', 'rewards_train/rejected': '0.040188', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14629', 'logps_train/rejected': '-146.22', 'logps_train/chosen': '-137.43', 'loss/train': '0.64911', 'examples_per_second': '31.373', 'grad_norm': '29.25', 'counters/examples': 220448, 'counters/updates': 6889}
train stats after 220480 examples: {'rewards_train/chosen': '0.14267', 'rewards_train/rejected': '0.069326', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073342', 'logps_train/rejected': '-139.43', 'logps_train/chosen': '-156.82', 'loss/train': '0.67435', 'examples_per_second': '31.229', 'grad_norm': '37.75', 'counters/examples': 220480, 'counters/updates': 6890}
train stats after 220512 examples: {'rewards_train/chosen': '0.17858', 'rewards_train/rejected': '0.096567', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.082016', 'logps_train/rejected': '-113.82', 'logps_train/chosen': '-106.49', 'loss/train': '0.67246', 'examples_per_second': '30.662', 'grad_norm': '28.75', 'counters/examples': 220512, 'counters/updates': 6891}
train stats after 220544 examples: {'rewards_train/chosen': '0.24194', 'rewards_train/rejected': '0.11733', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1246', 'logps_train/rejected': '-142.58', 'logps_train/chosen': '-155.98', 'loss/train': '0.6528', 'examples_per_second': '32.119', 'grad_norm': '36.75', 'counters/examples': 220544, 'counters/updates': 6892}
train stats after 220576 examples: {'rewards_train/chosen': '0.16959', 'rewards_train/rejected': '0.15123', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018362', 'logps_train/rejected': '-167.22', 'logps_train/chosen': '-173.19', 'loss/train': '0.70846', 'examples_per_second': '30.925', 'grad_norm': '37.75', 'counters/examples': 220576, 'counters/updates': 6893}
train stats after 220608 examples: {'rewards_train/chosen': '0.14092', 'rewards_train/rejected': '-0.033236', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17416', 'logps_train/rejected': '-106.24', 'logps_train/chosen': '-128.41', 'loss/train': '0.62344', 'examples_per_second': '31.466', 'grad_norm': '32.75', 'counters/examples': 220608, 'counters/updates': 6894}
train stats after 220640 examples: {'rewards_train/chosen': '0.06414', 'rewards_train/rejected': '0.042398', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021742', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-132.02', 'loss/train': '0.69285', 'examples_per_second': '32.008', 'grad_norm': '31.625', 'counters/examples': 220640, 'counters/updates': 6895}
train stats after 220672 examples: {'rewards_train/chosen': '0.26849', 'rewards_train/rejected': '0.051967', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21653', 'logps_train/rejected': '-190.54', 'logps_train/chosen': '-196.17', 'loss/train': '0.61902', 'examples_per_second': '32.204', 'grad_norm': '36', 'counters/examples': 220672, 'counters/updates': 6896}
train stats after 220704 examples: {'rewards_train/chosen': '0.1638', 'rewards_train/rejected': '-0.071745', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23555', 'logps_train/rejected': '-114.62', 'logps_train/chosen': '-135.33', 'loss/train': '0.59825', 'examples_per_second': '30.502', 'grad_norm': '25.75', 'counters/examples': 220704, 'counters/updates': 6897}
train stats after 220736 examples: {'rewards_train/chosen': '0.17359', 'rewards_train/rejected': '0.022975', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15061', 'logps_train/rejected': '-123.9', 'logps_train/chosen': '-156.53', 'loss/train': '0.63662', 'examples_per_second': '30.033', 'grad_norm': '35', 'counters/examples': 220736, 'counters/updates': 6898}
train stats after 220768 examples: {'rewards_train/chosen': '0.17653', 'rewards_train/rejected': '0.066479', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11005', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-146.15', 'loss/train': '0.65758', 'examples_per_second': '33.195', 'grad_norm': '45', 'counters/examples': 220768, 'counters/updates': 6899}
skipping logging after 220800 examples to avoid logging too frequently
train stats after 220832 examples: {'rewards_train/chosen': '0.1785', 'rewards_train/rejected': '0.011864', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16663', 'logps_train/rejected': '-100.31', 'logps_train/chosen': '-132.32', 'loss/train': '0.62693', 'examples_per_second': '32.625', 'grad_norm': '22.375', 'counters/examples': 220832, 'counters/updates': 6901}
train stats after 220864 examples: {'rewards_train/chosen': '0.10125', 'rewards_train/rejected': '-0.010846', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1121', 'logps_train/rejected': '-114', 'logps_train/chosen': '-127.65', 'loss/train': '0.64938', 'examples_per_second': '31.495', 'grad_norm': '23.75', 'counters/examples': 220864, 'counters/updates': 6902}
skipping logging after 220896 examples to avoid logging too frequently
train stats after 220928 examples: {'rewards_train/chosen': '0.12025', 'rewards_train/rejected': '0.10969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.010563', 'logps_train/rejected': '-145.48', 'logps_train/chosen': '-101.13', 'loss/train': '0.6951', 'examples_per_second': '30.937', 'grad_norm': '26.5', 'counters/examples': 220928, 'counters/updates': 6904}
train stats after 220960 examples: {'rewards_train/chosen': '0.14263', 'rewards_train/rejected': '-0.022959', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16558', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-133.94', 'loss/train': '0.63252', 'examples_per_second': '32.632', 'grad_norm': '40.75', 'counters/examples': 220960, 'counters/updates': 6905}
train stats after 220992 examples: {'rewards_train/chosen': '0.066374', 'rewards_train/rejected': '0.0089948', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.05738', 'logps_train/rejected': '-156.49', 'logps_train/chosen': '-149.71', 'loss/train': '0.67796', 'examples_per_second': '31.121', 'grad_norm': '43.75', 'counters/examples': 220992, 'counters/updates': 6906}
train stats after 221024 examples: {'rewards_train/chosen': '0.086635', 'rewards_train/rejected': '0.08909', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0024551', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-151.39', 'loss/train': '0.71487', 'examples_per_second': '31.433', 'grad_norm': '62.25', 'counters/examples': 221024, 'counters/updates': 6907}
train stats after 221056 examples: {'rewards_train/chosen': '0.187', 'rewards_train/rejected': '0.025734', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16127', 'logps_train/rejected': '-125.93', 'logps_train/chosen': '-138.26', 'loss/train': '0.62994', 'examples_per_second': '31.474', 'grad_norm': '31.75', 'counters/examples': 221056, 'counters/updates': 6908}
train stats after 221088 examples: {'rewards_train/chosen': '0.14475', 'rewards_train/rejected': '0.014283', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13047', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-145.47', 'loss/train': '0.65006', 'examples_per_second': '32.354', 'grad_norm': '30.75', 'counters/examples': 221088, 'counters/updates': 6909}
train stats after 221120 examples: {'rewards_train/chosen': '0.19302', 'rewards_train/rejected': '-0.0067169', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19973', 'logps_train/rejected': '-133.07', 'logps_train/chosen': '-139.38', 'loss/train': '0.61207', 'examples_per_second': '31.486', 'grad_norm': '34.25', 'counters/examples': 221120, 'counters/updates': 6910}
train stats after 221152 examples: {'rewards_train/chosen': '0.11257', 'rewards_train/rejected': '0.067298', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.045273', 'logps_train/rejected': '-131.19', 'logps_train/chosen': '-94.787', 'loss/train': '0.68326', 'examples_per_second': '30.503', 'grad_norm': '31.125', 'counters/examples': 221152, 'counters/updates': 6911}
train stats after 221184 examples: {'rewards_train/chosen': '0.086248', 'rewards_train/rejected': '-0.007535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.093783', 'logps_train/rejected': '-110.69', 'logps_train/chosen': '-150.41', 'loss/train': '0.65755', 'examples_per_second': '30.198', 'grad_norm': '27.375', 'counters/examples': 221184, 'counters/updates': 6912}
train stats after 221216 examples: {'rewards_train/chosen': '0.16304', 'rewards_train/rejected': '0.05959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10345', 'logps_train/rejected': '-123.08', 'logps_train/chosen': '-135.98', 'loss/train': '0.6584', 'examples_per_second': '32.023', 'grad_norm': '33.25', 'counters/examples': 221216, 'counters/updates': 6913}
train stats after 221248 examples: {'rewards_train/chosen': '0.11952', 'rewards_train/rejected': '-0.01319', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13271', 'logps_train/rejected': '-119.83', 'logps_train/chosen': '-136.4', 'loss/train': '0.63925', 'examples_per_second': '31.447', 'grad_norm': '26', 'counters/examples': 221248, 'counters/updates': 6914}
skipping logging after 221280 examples to avoid logging too frequently
train stats after 221312 examples: {'rewards_train/chosen': '0.14', 'rewards_train/rejected': '0.081132', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058865', 'logps_train/rejected': '-98.123', 'logps_train/chosen': '-136.64', 'loss/train': '0.66981', 'examples_per_second': '35.193', 'grad_norm': '31.25', 'counters/examples': 221312, 'counters/updates': 6916}
train stats after 221344 examples: {'rewards_train/chosen': '0.094114', 'rewards_train/rejected': '-0.0093638', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10348', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-115.95', 'loss/train': '0.65574', 'examples_per_second': '31.429', 'grad_norm': '33.75', 'counters/examples': 221344, 'counters/updates': 6917}
skipping logging after 221376 examples to avoid logging too frequently
train stats after 221408 examples: {'rewards_train/chosen': '0.1032', 'rewards_train/rejected': '-0.0064546', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10965', 'logps_train/rejected': '-99.252', 'logps_train/chosen': '-138.19', 'loss/train': '0.64453', 'examples_per_second': '33.242', 'grad_norm': '29', 'counters/examples': 221408, 'counters/updates': 6919}
skipping logging after 221440 examples to avoid logging too frequently
train stats after 221472 examples: {'rewards_train/chosen': '0.23303', 'rewards_train/rejected': '0.066259', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16677', 'logps_train/rejected': '-127.84', 'logps_train/chosen': '-132.45', 'loss/train': '0.63122', 'examples_per_second': '31.405', 'grad_norm': '47.75', 'counters/examples': 221472, 'counters/updates': 6921}
skipping logging after 221504 examples to avoid logging too frequently
train stats after 221536 examples: {'rewards_train/chosen': '0.18345', 'rewards_train/rejected': '0.15594', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.027511', 'logps_train/rejected': '-161.04', 'logps_train/chosen': '-159.15', 'loss/train': '0.69398', 'examples_per_second': '29.977', 'grad_norm': '39.75', 'counters/examples': 221536, 'counters/updates': 6923}
skipping logging after 221568 examples to avoid logging too frequently
train stats after 221600 examples: {'rewards_train/chosen': '0.18854', 'rewards_train/rejected': '0.044675', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14386', 'logps_train/rejected': '-115.85', 'logps_train/chosen': '-137.8', 'loss/train': '0.63591', 'examples_per_second': '34.556', 'grad_norm': '28.875', 'counters/examples': 221600, 'counters/updates': 6925}
train stats after 221632 examples: {'rewards_train/chosen': '0.1988', 'rewards_train/rejected': '0.048708', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15009', 'logps_train/rejected': '-139.48', 'logps_train/chosen': '-179.53', 'loss/train': '0.64821', 'examples_per_second': '30.683', 'grad_norm': '32.25', 'counters/examples': 221632, 'counters/updates': 6926}
skipping logging after 221664 examples to avoid logging too frequently
train stats after 221696 examples: {'rewards_train/chosen': '0.1533', 'rewards_train/rejected': '0.067105', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0862', 'logps_train/rejected': '-144.64', 'logps_train/chosen': '-112.39', 'loss/train': '0.66686', 'examples_per_second': '32.022', 'grad_norm': '26.375', 'counters/examples': 221696, 'counters/updates': 6928}
train stats after 221728 examples: {'rewards_train/chosen': '0.14612', 'rewards_train/rejected': '0.092088', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054035', 'logps_train/rejected': '-130.92', 'logps_train/chosen': '-141.37', 'loss/train': '0.68028', 'examples_per_second': '32.887', 'grad_norm': '29.125', 'counters/examples': 221728, 'counters/updates': 6929}
train stats after 221760 examples: {'rewards_train/chosen': '0.12064', 'rewards_train/rejected': '0.030357', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090281', 'logps_train/rejected': '-100.3', 'logps_train/chosen': '-122.49', 'loss/train': '0.66295', 'examples_per_second': '32.527', 'grad_norm': '25.125', 'counters/examples': 221760, 'counters/updates': 6930}
train stats after 221792 examples: {'rewards_train/chosen': '0.202', 'rewards_train/rejected': '-0.022717', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22472', 'logps_train/rejected': '-124.98', 'logps_train/chosen': '-133.71', 'loss/train': '0.60657', 'examples_per_second': '29.96', 'grad_norm': '32', 'counters/examples': 221792, 'counters/updates': 6931}
train stats after 221824 examples: {'rewards_train/chosen': '0.17452', 'rewards_train/rejected': '0.094059', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08046', 'logps_train/rejected': '-159.3', 'logps_train/chosen': '-161.87', 'loss/train': '0.66559', 'examples_per_second': '31.474', 'grad_norm': '29', 'counters/examples': 221824, 'counters/updates': 6932}
skipping logging after 221856 examples to avoid logging too frequently
train stats after 221888 examples: {'rewards_train/chosen': '0.15371', 'rewards_train/rejected': '0.097712', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.055994', 'logps_train/rejected': '-124.8', 'logps_train/chosen': '-140.65', 'loss/train': '0.6734', 'examples_per_second': '30.768', 'grad_norm': '32.25', 'counters/examples': 221888, 'counters/updates': 6934}
skipping logging after 221920 examples to avoid logging too frequently
train stats after 221952 examples: {'rewards_train/chosen': '0.22537', 'rewards_train/rejected': '0.057759', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16762', 'logps_train/rejected': '-144.71', 'logps_train/chosen': '-173.45', 'loss/train': '0.63933', 'examples_per_second': '30.244', 'grad_norm': '36.5', 'counters/examples': 221952, 'counters/updates': 6936}
train stats after 221984 examples: {'rewards_train/chosen': '0.064077', 'rewards_train/rejected': '0.0069921', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.057085', 'logps_train/rejected': '-97.644', 'logps_train/chosen': '-118.5', 'loss/train': '0.67653', 'examples_per_second': '31.654', 'grad_norm': '27.75', 'counters/examples': 221984, 'counters/updates': 6937}
train stats after 222016 examples: {'rewards_train/chosen': '0.1827', 'rewards_train/rejected': '0.044577', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13812', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-121.84', 'loss/train': '0.63467', 'examples_per_second': '30.558', 'grad_norm': '33.25', 'counters/examples': 222016, 'counters/updates': 6938}
train stats after 222048 examples: {'rewards_train/chosen': '0.067545', 'rewards_train/rejected': '0.040925', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02662', 'logps_train/rejected': '-124.62', 'logps_train/chosen': '-119.55', 'loss/train': '0.68536', 'examples_per_second': '32.725', 'grad_norm': '28.75', 'counters/examples': 222048, 'counters/updates': 6939}
train stats after 222080 examples: {'rewards_train/chosen': '0.24731', 'rewards_train/rejected': '0.054628', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19268', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-190.71', 'loss/train': '0.6105', 'examples_per_second': '32.217', 'grad_norm': '36', 'counters/examples': 222080, 'counters/updates': 6940}
train stats after 222112 examples: {'rewards_train/chosen': '0.16084', 'rewards_train/rejected': '0.089615', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.071229', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-141.9', 'loss/train': '0.66843', 'examples_per_second': '31.353', 'grad_norm': '28.5', 'counters/examples': 222112, 'counters/updates': 6941}
train stats after 222144 examples: {'rewards_train/chosen': '0.22587', 'rewards_train/rejected': '0.016734', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20913', 'logps_train/rejected': '-144.06', 'logps_train/chosen': '-202.05', 'loss/train': '0.62462', 'examples_per_second': '31.469', 'grad_norm': '39.25', 'counters/examples': 222144, 'counters/updates': 6942}
skipping logging after 222176 examples to avoid logging too frequently
train stats after 222208 examples: {'rewards_train/chosen': '0.13353', 'rewards_train/rejected': '0.024184', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10934', 'logps_train/rejected': '-135.66', 'logps_train/chosen': '-154.52', 'loss/train': '0.65462', 'examples_per_second': '30.464', 'grad_norm': '31.75', 'counters/examples': 222208, 'counters/updates': 6944}
train stats after 222240 examples: {'rewards_train/chosen': '0.20282', 'rewards_train/rejected': '0.051805', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15101', 'logps_train/rejected': '-115.9', 'logps_train/chosen': '-133.54', 'loss/train': '0.63045', 'examples_per_second': '32.403', 'grad_norm': '24.125', 'counters/examples': 222240, 'counters/updates': 6945}
train stats after 222272 examples: {'rewards_train/chosen': '0.19002', 'rewards_train/rejected': '0.066031', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12399', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-130.99', 'loss/train': '0.65325', 'examples_per_second': '30.675', 'grad_norm': '32.75', 'counters/examples': 222272, 'counters/updates': 6946}
train stats after 222304 examples: {'rewards_train/chosen': '0.19361', 'rewards_train/rejected': '0.02149', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17212', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-121.34', 'loss/train': '0.62784', 'examples_per_second': '29.939', 'grad_norm': '42.25', 'counters/examples': 222304, 'counters/updates': 6947}
train stats after 222336 examples: {'rewards_train/chosen': '0.089467', 'rewards_train/rejected': '0.044013', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.045455', 'logps_train/rejected': '-105.7', 'logps_train/chosen': '-142.12', 'loss/train': '0.67795', 'examples_per_second': '31.437', 'grad_norm': '32.5', 'counters/examples': 222336, 'counters/updates': 6948}
train stats after 222368 examples: {'rewards_train/chosen': '0.19614', 'rewards_train/rejected': '0.070049', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12609', 'logps_train/rejected': '-105.41', 'logps_train/chosen': '-121.08', 'loss/train': '0.64643', 'examples_per_second': '31.455', 'grad_norm': '26.25', 'counters/examples': 222368, 'counters/updates': 6949}
train stats after 222400 examples: {'rewards_train/chosen': '0.22113', 'rewards_train/rejected': '0.024605', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19652', 'logps_train/rejected': '-87.993', 'logps_train/chosen': '-165.44', 'loss/train': '0.61928', 'examples_per_second': '31.445', 'grad_norm': '24.625', 'counters/examples': 222400, 'counters/updates': 6950}
train stats after 222432 examples: {'rewards_train/chosen': '0.12133', 'rewards_train/rejected': '0.082918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038414', 'logps_train/rejected': '-119.68', 'logps_train/chosen': '-123.88', 'loss/train': '0.68828', 'examples_per_second': '30.379', 'grad_norm': '28.875', 'counters/examples': 222432, 'counters/updates': 6951}
train stats after 222464 examples: {'rewards_train/chosen': '0.12822', 'rewards_train/rejected': '0.17144', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.043222', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-169.5', 'loss/train': '0.73934', 'examples_per_second': '31.454', 'grad_norm': '52.75', 'counters/examples': 222464, 'counters/updates': 6952}
train stats after 222496 examples: {'rewards_train/chosen': '0.14684', 'rewards_train/rejected': '0.096993', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.04985', 'logps_train/rejected': '-123.77', 'logps_train/chosen': '-130.38', 'loss/train': '0.68991', 'examples_per_second': '31.359', 'grad_norm': '53', 'counters/examples': 222496, 'counters/updates': 6953}
train stats after 222528 examples: {'rewards_train/chosen': '0.30995', 'rewards_train/rejected': '0.063723', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.24623', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-170.41', 'loss/train': '0.63303', 'examples_per_second': '32.55', 'grad_norm': '28.75', 'counters/examples': 222528, 'counters/updates': 6954}
train stats after 222560 examples: {'rewards_train/chosen': '0.17206', 'rewards_train/rejected': '0.11808', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053972', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-137.56', 'loss/train': '0.67637', 'examples_per_second': '32.592', 'grad_norm': '34.75', 'counters/examples': 222560, 'counters/updates': 6955}
train stats after 222592 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '0.0051916', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12824', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-148.03', 'loss/train': '0.64096', 'examples_per_second': '30.532', 'grad_norm': '26', 'counters/examples': 222592, 'counters/updates': 6956}
train stats after 222624 examples: {'rewards_train/chosen': '0.17178', 'rewards_train/rejected': '0.14869', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02309', 'logps_train/rejected': '-182.34', 'logps_train/chosen': '-156.5', 'loss/train': '0.70141', 'examples_per_second': '31.208', 'grad_norm': '57.5', 'counters/examples': 222624, 'counters/updates': 6957}
skipping logging after 222656 examples to avoid logging too frequently
train stats after 222688 examples: {'rewards_train/chosen': '0.19483', 'rewards_train/rejected': '0.025298', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16953', 'logps_train/rejected': '-104.88', 'logps_train/chosen': '-162.91', 'loss/train': '0.63084', 'examples_per_second': '30.737', 'grad_norm': '37.25', 'counters/examples': 222688, 'counters/updates': 6959}
skipping logging after 222720 examples to avoid logging too frequently
train stats after 222752 examples: {'rewards_train/chosen': '0.11246', 'rewards_train/rejected': '0.056619', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055844', 'logps_train/rejected': '-119.75', 'logps_train/chosen': '-136.1', 'loss/train': '0.68018', 'examples_per_second': '31.21', 'grad_norm': '30.375', 'counters/examples': 222752, 'counters/updates': 6961}
train stats after 222784 examples: {'rewards_train/chosen': '0.21796', 'rewards_train/rejected': '0.025671', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19229', 'logps_train/rejected': '-124.87', 'logps_train/chosen': '-116.34', 'loss/train': '0.61368', 'examples_per_second': '32', 'grad_norm': '28.125', 'counters/examples': 222784, 'counters/updates': 6962}
train stats after 222816 examples: {'rewards_train/chosen': '0.22107', 'rewards_train/rejected': '0.10521', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11586', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-156.12', 'loss/train': '0.65934', 'examples_per_second': '31.588', 'grad_norm': '30.875', 'counters/examples': 222816, 'counters/updates': 6963}
train stats after 222848 examples: {'rewards_train/chosen': '0.10283', 'rewards_train/rejected': '0.1431', 'rewards_train/accuracies': '0.34375', 'rewards_train/margins': '-0.040277', 'logps_train/rejected': '-141.66', 'logps_train/chosen': '-148.3', 'loss/train': '0.72368', 'examples_per_second': '31.495', 'grad_norm': '30.875', 'counters/examples': 222848, 'counters/updates': 6964}
train stats after 222880 examples: {'rewards_train/chosen': '0.22804', 'rewards_train/rejected': '0.017747', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21029', 'logps_train/rejected': '-125.14', 'logps_train/chosen': '-196.5', 'loss/train': '0.62415', 'examples_per_second': '32.18', 'grad_norm': '34.25', 'counters/examples': 222880, 'counters/updates': 6965}
train stats after 222912 examples: {'rewards_train/chosen': '0.17879', 'rewards_train/rejected': '0.060075', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11871', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-167.46', 'loss/train': '0.65173', 'examples_per_second': '32.329', 'grad_norm': '33.75', 'counters/examples': 222912, 'counters/updates': 6966}
train stats after 222944 examples: {'rewards_train/chosen': '0.090647', 'rewards_train/rejected': '0.016267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.07438', 'logps_train/rejected': '-136.61', 'logps_train/chosen': '-148.94', 'loss/train': '0.67248', 'examples_per_second': '31.137', 'grad_norm': '29.625', 'counters/examples': 222944, 'counters/updates': 6967}
train stats after 222976 examples: {'rewards_train/chosen': '0.19903', 'rewards_train/rejected': '0.18512', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.013914', 'logps_train/rejected': '-147.38', 'logps_train/chosen': '-141.07', 'loss/train': '0.70658', 'examples_per_second': '30.317', 'grad_norm': '44.5', 'counters/examples': 222976, 'counters/updates': 6968}
train stats after 223008 examples: {'rewards_train/chosen': '0.12946', 'rewards_train/rejected': '0.043496', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085967', 'logps_train/rejected': '-156.83', 'logps_train/chosen': '-128.04', 'loss/train': '0.6697', 'examples_per_second': '33.086', 'grad_norm': '30.625', 'counters/examples': 223008, 'counters/updates': 6969}
train stats after 223040 examples: {'rewards_train/chosen': '0.10725', 'rewards_train/rejected': '0.066412', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040835', 'logps_train/rejected': '-146.33', 'logps_train/chosen': '-130.4', 'loss/train': '0.68446', 'examples_per_second': '31.513', 'grad_norm': '29.375', 'counters/examples': 223040, 'counters/updates': 6970}
train stats after 223072 examples: {'rewards_train/chosen': '0.09796', 'rewards_train/rejected': '0.091572', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0063884', 'logps_train/rejected': '-119.61', 'logps_train/chosen': '-153.06', 'loss/train': '0.71518', 'examples_per_second': '32.598', 'grad_norm': '42', 'counters/examples': 223072, 'counters/updates': 6971}
skipping logging after 223104 examples to avoid logging too frequently
train stats after 223136 examples: {'rewards_train/chosen': '0.12928', 'rewards_train/rejected': '0.078858', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.050424', 'logps_train/rejected': '-126.04', 'logps_train/chosen': '-143.98', 'loss/train': '0.68183', 'examples_per_second': '31.481', 'grad_norm': '28.75', 'counters/examples': 223136, 'counters/updates': 6973}
train stats after 223168 examples: {'rewards_train/chosen': '0.20006', 'rewards_train/rejected': '0.082515', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11754', 'logps_train/rejected': '-105.33', 'logps_train/chosen': '-147.87', 'loss/train': '0.6535', 'examples_per_second': '31.595', 'grad_norm': '26.25', 'counters/examples': 223168, 'counters/updates': 6974}
train stats after 223200 examples: {'rewards_train/chosen': '0.20292', 'rewards_train/rejected': '0.0080098', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19491', 'logps_train/rejected': '-115.64', 'logps_train/chosen': '-144.01', 'loss/train': '0.62405', 'examples_per_second': '23.5', 'grad_norm': '27.125', 'counters/examples': 223200, 'counters/updates': 6975}
train stats after 223232 examples: {'rewards_train/chosen': '0.095831', 'rewards_train/rejected': '-0.033579', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12941', 'logps_train/rejected': '-86.246', 'logps_train/chosen': '-121.78', 'loss/train': '0.64447', 'examples_per_second': '30.92', 'grad_norm': '22.375', 'counters/examples': 223232, 'counters/updates': 6976}
train stats after 223264 examples: {'rewards_train/chosen': '0.22346', 'rewards_train/rejected': '0.060735', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16272', 'logps_train/rejected': '-134.76', 'logps_train/chosen': '-119.69', 'loss/train': '0.64332', 'examples_per_second': '31.958', 'grad_norm': '33.25', 'counters/examples': 223264, 'counters/updates': 6977}
skipping logging after 223296 examples to avoid logging too frequently
train stats after 223328 examples: {'rewards_train/chosen': '0.16943', 'rewards_train/rejected': '0.039755', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12968', 'logps_train/rejected': '-134.82', 'logps_train/chosen': '-149.35', 'loss/train': '0.64337', 'examples_per_second': '30.132', 'grad_norm': '26.5', 'counters/examples': 223328, 'counters/updates': 6979}
train stats after 223360 examples: {'rewards_train/chosen': '0.25023', 'rewards_train/rejected': '0.1344', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11583', 'logps_train/rejected': '-141.81', 'logps_train/chosen': '-159.13', 'loss/train': '0.68003', 'examples_per_second': '32.325', 'grad_norm': '38.25', 'counters/examples': 223360, 'counters/updates': 6980}
skipping logging after 223392 examples to avoid logging too frequently
train stats after 223424 examples: {'rewards_train/chosen': '0.19201', 'rewards_train/rejected': '-0.014938', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20694', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-156.98', 'loss/train': '0.61343', 'examples_per_second': '30.027', 'grad_norm': '49.25', 'counters/examples': 223424, 'counters/updates': 6982}
train stats after 223456 examples: {'rewards_train/chosen': '0.30876', 'rewards_train/rejected': '0.23727', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.071498', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-143.1', 'loss/train': '0.68176', 'examples_per_second': '30.232', 'grad_norm': '54.75', 'counters/examples': 223456, 'counters/updates': 6983}
train stats after 223488 examples: {'rewards_train/chosen': '0.14259', 'rewards_train/rejected': '0.067742', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074844', 'logps_train/rejected': '-111.65', 'logps_train/chosen': '-132.19', 'loss/train': '0.6756', 'examples_per_second': '31.078', 'grad_norm': '30.875', 'counters/examples': 223488, 'counters/updates': 6984}
train stats after 223520 examples: {'rewards_train/chosen': '0.27896', 'rewards_train/rejected': '0.054059', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2249', 'logps_train/rejected': '-93.712', 'logps_train/chosen': '-148.03', 'loss/train': '0.60732', 'examples_per_second': '30.107', 'grad_norm': '70.5', 'counters/examples': 223520, 'counters/updates': 6985}
train stats after 223552 examples: {'rewards_train/chosen': '0.21558', 'rewards_train/rejected': '-0.04319', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25877', 'logps_train/rejected': '-121.16', 'logps_train/chosen': '-159.62', 'loss/train': '0.59736', 'examples_per_second': '30.708', 'grad_norm': '27.25', 'counters/examples': 223552, 'counters/updates': 6986}
train stats after 223584 examples: {'rewards_train/chosen': '0.18613', 'rewards_train/rejected': '0.10382', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082315', 'logps_train/rejected': '-142.62', 'logps_train/chosen': '-187.9', 'loss/train': '0.67604', 'examples_per_second': '31.482', 'grad_norm': '47.5', 'counters/examples': 223584, 'counters/updates': 6987}
train stats after 223616 examples: {'rewards_train/chosen': '0.24672', 'rewards_train/rejected': '0.14253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1042', 'logps_train/rejected': '-146.87', 'logps_train/chosen': '-172.5', 'loss/train': '0.65632', 'examples_per_second': '31.545', 'grad_norm': '27.125', 'counters/examples': 223616, 'counters/updates': 6988}
skipping logging after 223648 examples to avoid logging too frequently
train stats after 223680 examples: {'rewards_train/chosen': '0.18031', 'rewards_train/rejected': '0.036784', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14353', 'logps_train/rejected': '-126.94', 'logps_train/chosen': '-159', 'loss/train': '0.64025', 'examples_per_second': '32.608', 'grad_norm': '36.75', 'counters/examples': 223680, 'counters/updates': 6990}
train stats after 223712 examples: {'rewards_train/chosen': '0.18221', 'rewards_train/rejected': '0.070918', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11129', 'logps_train/rejected': '-119.57', 'logps_train/chosen': '-139.22', 'loss/train': '0.66217', 'examples_per_second': '31.808', 'grad_norm': '27.5', 'counters/examples': 223712, 'counters/updates': 6991}
skipping logging after 223744 examples to avoid logging too frequently
train stats after 223776 examples: {'rewards_train/chosen': '0.16663', 'rewards_train/rejected': '-0.025279', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19191', 'logps_train/rejected': '-125.13', 'logps_train/chosen': '-150.32', 'loss/train': '0.61338', 'examples_per_second': '30.904', 'grad_norm': '32.5', 'counters/examples': 223776, 'counters/updates': 6993}
skipping logging after 223808 examples to avoid logging too frequently
train stats after 223840 examples: {'rewards_train/chosen': '0.17524', 'rewards_train/rejected': '0.068079', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10716', 'logps_train/rejected': '-109.12', 'logps_train/chosen': '-136.89', 'loss/train': '0.65225', 'examples_per_second': '30.663', 'grad_norm': '25.75', 'counters/examples': 223840, 'counters/updates': 6995}
skipping logging after 223872 examples to avoid logging too frequently
train stats after 223904 examples: {'rewards_train/chosen': '0.12187', 'rewards_train/rejected': '0.05752', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.064349', 'logps_train/rejected': '-108.71', 'logps_train/chosen': '-123.1', 'loss/train': '0.67187', 'examples_per_second': '33.943', 'grad_norm': '35', 'counters/examples': 223904, 'counters/updates': 6997}
train stats after 223936 examples: {'rewards_train/chosen': '0.21981', 'rewards_train/rejected': '0.0097554', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21006', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-130.73', 'loss/train': '0.62265', 'examples_per_second': '32.17', 'grad_norm': '29.375', 'counters/examples': 223936, 'counters/updates': 6998}
train stats after 223968 examples: {'rewards_train/chosen': '0.16688', 'rewards_train/rejected': '0.07056', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096322', 'logps_train/rejected': '-150.3', 'logps_train/chosen': '-168.55', 'loss/train': '0.65801', 'examples_per_second': '31.433', 'grad_norm': '30', 'counters/examples': 223968, 'counters/updates': 6999}
train stats after 224000 examples: {'rewards_train/chosen': '0.21835', 'rewards_train/rejected': '-0.039032', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25738', 'logps_train/rejected': '-91.28', 'logps_train/chosen': '-135.52', 'loss/train': '0.58815', 'examples_per_second': '31.475', 'grad_norm': '30.375', 'counters/examples': 224000, 'counters/updates': 7000}
skipping logging after 224032 examples to avoid logging too frequently
train stats after 224064 examples: {'rewards_train/chosen': '0.15456', 'rewards_train/rejected': '0.035869', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1187', 'logps_train/rejected': '-134.7', 'logps_train/chosen': '-186.26', 'loss/train': '0.6597', 'examples_per_second': '30.28', 'grad_norm': '39', 'counters/examples': 224064, 'counters/updates': 7002}
train stats after 224096 examples: {'rewards_train/chosen': '0.088207', 'rewards_train/rejected': '0.0146', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073607', 'logps_train/rejected': '-132.78', 'logps_train/chosen': '-148.93', 'loss/train': '0.66887', 'examples_per_second': '32.074', 'grad_norm': '36', 'counters/examples': 224096, 'counters/updates': 7003}
train stats after 224128 examples: {'rewards_train/chosen': '0.1204', 'rewards_train/rejected': '0.077685', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042718', 'logps_train/rejected': '-120.04', 'logps_train/chosen': '-151.29', 'loss/train': '0.6884', 'examples_per_second': '30.354', 'grad_norm': '31.125', 'counters/examples': 224128, 'counters/updates': 7004}
train stats after 224160 examples: {'rewards_train/chosen': '0.24868', 'rewards_train/rejected': '0.056208', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19247', 'logps_train/rejected': '-112.36', 'logps_train/chosen': '-148.83', 'loss/train': '0.61577', 'examples_per_second': '31.848', 'grad_norm': '25.125', 'counters/examples': 224160, 'counters/updates': 7005}
train stats after 224192 examples: {'rewards_train/chosen': '0.18595', 'rewards_train/rejected': '0.11283', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.073126', 'logps_train/rejected': '-124.58', 'logps_train/chosen': '-131.37', 'loss/train': '0.67167', 'examples_per_second': '32.476', 'grad_norm': '30.25', 'counters/examples': 224192, 'counters/updates': 7006}
train stats after 224224 examples: {'rewards_train/chosen': '0.14571', 'rewards_train/rejected': '0.034197', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11151', 'logps_train/rejected': '-117.09', 'logps_train/chosen': '-148.64', 'loss/train': '0.65297', 'examples_per_second': '24.23', 'grad_norm': '34.25', 'counters/examples': 224224, 'counters/updates': 7007}
train stats after 224256 examples: {'rewards_train/chosen': '0.14804', 'rewards_train/rejected': '0.083821', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.064221', 'logps_train/rejected': '-142.77', 'logps_train/chosen': '-155.58', 'loss/train': '0.6795', 'examples_per_second': '31.554', 'grad_norm': '30.125', 'counters/examples': 224256, 'counters/updates': 7008}
train stats after 224288 examples: {'rewards_train/chosen': '0.18193', 'rewards_train/rejected': '0.0062898', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17564', 'logps_train/rejected': '-143.42', 'logps_train/chosen': '-152.87', 'loss/train': '0.62753', 'examples_per_second': '31.503', 'grad_norm': '34.75', 'counters/examples': 224288, 'counters/updates': 7009}
train stats after 224320 examples: {'rewards_train/chosen': '0.19877', 'rewards_train/rejected': '0.10806', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.090717', 'logps_train/rejected': '-131.25', 'logps_train/chosen': '-141.96', 'loss/train': '0.67076', 'examples_per_second': '24.261', 'grad_norm': '37.75', 'counters/examples': 224320, 'counters/updates': 7010}
train stats after 224352 examples: {'rewards_train/chosen': '0.16327', 'rewards_train/rejected': '0.03531', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12796', 'logps_train/rejected': '-136.25', 'logps_train/chosen': '-149.1', 'loss/train': '0.64819', 'examples_per_second': '31.231', 'grad_norm': '26.375', 'counters/examples': 224352, 'counters/updates': 7011}
train stats after 224384 examples: {'rewards_train/chosen': '0.14533', 'rewards_train/rejected': '-0.00044716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14578', 'logps_train/rejected': '-98.523', 'logps_train/chosen': '-127.61', 'loss/train': '0.63453', 'examples_per_second': '31.651', 'grad_norm': '27.125', 'counters/examples': 224384, 'counters/updates': 7012}
train stats after 224416 examples: {'rewards_train/chosen': '0.18958', 'rewards_train/rejected': '0.1612', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.028382', 'logps_train/rejected': '-147.14', 'logps_train/chosen': '-137.63', 'loss/train': '0.69601', 'examples_per_second': '30.692', 'grad_norm': '31.375', 'counters/examples': 224416, 'counters/updates': 7013}
train stats after 224448 examples: {'rewards_train/chosen': '0.25707', 'rewards_train/rejected': '0.078778', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1783', 'logps_train/rejected': '-136.94', 'logps_train/chosen': '-141.73', 'loss/train': '0.62853', 'examples_per_second': '32.228', 'grad_norm': '32', 'counters/examples': 224448, 'counters/updates': 7014}
train stats after 224480 examples: {'rewards_train/chosen': '0.096447', 'rewards_train/rejected': '0.056178', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040268', 'logps_train/rejected': '-150.27', 'logps_train/chosen': '-144.71', 'loss/train': '0.68548', 'examples_per_second': '32.304', 'grad_norm': '31', 'counters/examples': 224480, 'counters/updates': 7015}
train stats after 224512 examples: {'rewards_train/chosen': '0.21262', 'rewards_train/rejected': '0.084591', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12803', 'logps_train/rejected': '-153.66', 'logps_train/chosen': '-141.69', 'loss/train': '0.64914', 'examples_per_second': '31.468', 'grad_norm': '37.75', 'counters/examples': 224512, 'counters/updates': 7016}
train stats after 224544 examples: {'rewards_train/chosen': '0.19776', 'rewards_train/rejected': '0.027371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17039', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-159.01', 'loss/train': '0.63392', 'examples_per_second': '30.636', 'grad_norm': '34.75', 'counters/examples': 224544, 'counters/updates': 7017}
train stats after 224576 examples: {'rewards_train/chosen': '0.16048', 'rewards_train/rejected': '-0.044378', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20485', 'logps_train/rejected': '-98.211', 'logps_train/chosen': '-182.06', 'loss/train': '0.60923', 'examples_per_second': '31.401', 'grad_norm': '28.625', 'counters/examples': 224576, 'counters/updates': 7018}
train stats after 224608 examples: {'rewards_train/chosen': '0.19713', 'rewards_train/rejected': '0.066444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13069', 'logps_train/rejected': '-145.66', 'logps_train/chosen': '-143.14', 'loss/train': '0.64664', 'examples_per_second': '33.233', 'grad_norm': '28.375', 'counters/examples': 224608, 'counters/updates': 7019}
train stats after 224640 examples: {'rewards_train/chosen': '0.11925', 'rewards_train/rejected': '-0.0068657', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12612', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-157.48', 'loss/train': '0.64308', 'examples_per_second': '31.479', 'grad_norm': '35.5', 'counters/examples': 224640, 'counters/updates': 7020}
skipping logging after 224672 examples to avoid logging too frequently
train stats after 224704 examples: {'rewards_train/chosen': '0.12665', 'rewards_train/rejected': '0.004106', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12254', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-145.04', 'loss/train': '0.64552', 'examples_per_second': '31.462', 'grad_norm': '31.5', 'counters/examples': 224704, 'counters/updates': 7022}
train stats after 224736 examples: {'rewards_train/chosen': '0.23612', 'rewards_train/rejected': '0.12057', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11555', 'logps_train/rejected': '-162.72', 'logps_train/chosen': '-144.7', 'loss/train': '0.65795', 'examples_per_second': '31.504', 'grad_norm': '35.25', 'counters/examples': 224736, 'counters/updates': 7023}
skipping logging after 224768 examples to avoid logging too frequently
train stats after 224800 examples: {'rewards_train/chosen': '0.17755', 'rewards_train/rejected': '0.10037', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077177', 'logps_train/rejected': '-99.167', 'logps_train/chosen': '-104.63', 'loss/train': '0.66489', 'examples_per_second': '32.473', 'grad_norm': '27.625', 'counters/examples': 224800, 'counters/updates': 7025}
train stats after 224832 examples: {'rewards_train/chosen': '0.20385', 'rewards_train/rejected': '0.0075628', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19629', 'logps_train/rejected': '-136.05', 'logps_train/chosen': '-154.12', 'loss/train': '0.61343', 'examples_per_second': '30.817', 'grad_norm': '27.375', 'counters/examples': 224832, 'counters/updates': 7026}
skipping logging after 224864 examples to avoid logging too frequently
train stats after 224896 examples: {'rewards_train/chosen': '0.13611', 'rewards_train/rejected': '0.11343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.022684', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-125.89', 'loss/train': '0.69646', 'examples_per_second': '36.155', 'grad_norm': '30', 'counters/examples': 224896, 'counters/updates': 7028}
train stats after 224928 examples: {'rewards_train/chosen': '0.21854', 'rewards_train/rejected': '0.062936', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15561', 'logps_train/rejected': '-103.28', 'logps_train/chosen': '-118.12', 'loss/train': '0.63892', 'examples_per_second': '31.237', 'grad_norm': '28.5', 'counters/examples': 224928, 'counters/updates': 7029}
skipping logging after 224960 examples to avoid logging too frequently
train stats after 224992 examples: {'rewards_train/chosen': '0.056692', 'rewards_train/rejected': '-0.038048', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094739', 'logps_train/rejected': '-92.982', 'logps_train/chosen': '-122.91', 'loss/train': '0.654', 'examples_per_second': '34.046', 'grad_norm': '27.875', 'counters/examples': 224992, 'counters/updates': 7031}
skipping logging after 225024 examples to avoid logging too frequently
train stats after 225056 examples: {'rewards_train/chosen': '0.21866', 'rewards_train/rejected': '0.089937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12872', 'logps_train/rejected': '-93.974', 'logps_train/chosen': '-140.56', 'loss/train': '0.64291', 'examples_per_second': '32.518', 'grad_norm': '29.25', 'counters/examples': 225056, 'counters/updates': 7033}
train stats after 225088 examples: {'rewards_train/chosen': '0.24308', 'rewards_train/rejected': '-0.081294', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.32437', 'logps_train/rejected': '-81.217', 'logps_train/chosen': '-162.43', 'loss/train': '0.57331', 'examples_per_second': '31.171', 'grad_norm': '36', 'counters/examples': 225088, 'counters/updates': 7034}
train stats after 225120 examples: {'rewards_train/chosen': '0.19484', 'rewards_train/rejected': '0.051944', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1429', 'logps_train/rejected': '-138.82', 'logps_train/chosen': '-128.06', 'loss/train': '0.64794', 'examples_per_second': '30.499', 'grad_norm': '32.25', 'counters/examples': 225120, 'counters/updates': 7035}
skipping logging after 225152 examples to avoid logging too frequently
train stats after 225184 examples: {'rewards_train/chosen': '0.16854', 'rewards_train/rejected': '0.042701', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12584', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-123.69', 'loss/train': '0.64781', 'examples_per_second': '30.181', 'grad_norm': '27.125', 'counters/examples': 225184, 'counters/updates': 7037}
train stats after 225216 examples: {'rewards_train/chosen': '0.22543', 'rewards_train/rejected': '0.064267', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16116', 'logps_train/rejected': '-121.04', 'logps_train/chosen': '-153.22', 'loss/train': '0.63101', 'examples_per_second': '32.774', 'grad_norm': '28.625', 'counters/examples': 225216, 'counters/updates': 7038}
train stats after 225248 examples: {'rewards_train/chosen': '0.23084', 'rewards_train/rejected': '0.12158', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10926', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-150.39', 'loss/train': '0.66182', 'examples_per_second': '31.503', 'grad_norm': '25.375', 'counters/examples': 225248, 'counters/updates': 7039}
train stats after 225280 examples: {'rewards_train/chosen': '0.14603', 'rewards_train/rejected': '0.074152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071877', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-120.59', 'loss/train': '0.66338', 'examples_per_second': '31.512', 'grad_norm': '26.25', 'counters/examples': 225280, 'counters/updates': 7040}
skipping logging after 225312 examples to avoid logging too frequently
train stats after 225344 examples: {'rewards_train/chosen': '0.13964', 'rewards_train/rejected': '0.066715', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072921', 'logps_train/rejected': '-104.79', 'logps_train/chosen': '-98.085', 'loss/train': '0.66269', 'examples_per_second': '30.238', 'grad_norm': '25.25', 'counters/examples': 225344, 'counters/updates': 7042}
train stats after 225376 examples: {'rewards_train/chosen': '0.14778', 'rewards_train/rejected': '0.046305', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10147', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-143.36', 'loss/train': '0.66366', 'examples_per_second': '30.992', 'grad_norm': '31.875', 'counters/examples': 225376, 'counters/updates': 7043}
train stats after 225408 examples: {'rewards_train/chosen': '0.15076', 'rewards_train/rejected': '0.062903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087861', 'logps_train/rejected': '-157.13', 'logps_train/chosen': '-148.23', 'loss/train': '0.66445', 'examples_per_second': '29.982', 'grad_norm': '33.75', 'counters/examples': 225408, 'counters/updates': 7044}
train stats after 225440 examples: {'rewards_train/chosen': '0.20295', 'rewards_train/rejected': '0.025542', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17741', 'logps_train/rejected': '-134.49', 'logps_train/chosen': '-157.55', 'loss/train': '0.6309', 'examples_per_second': '30.008', 'grad_norm': '30.5', 'counters/examples': 225440, 'counters/updates': 7045}
train stats after 225472 examples: {'rewards_train/chosen': '0.14685', 'rewards_train/rejected': '0.053341', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093512', 'logps_train/rejected': '-95.144', 'logps_train/chosen': '-111.34', 'loss/train': '0.65515', 'examples_per_second': '31.745', 'grad_norm': '24.125', 'counters/examples': 225472, 'counters/updates': 7046}
train stats after 225504 examples: {'rewards_train/chosen': '0.24459', 'rewards_train/rejected': '-0.0061243', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25072', 'logps_train/rejected': '-174.25', 'logps_train/chosen': '-169.61', 'loss/train': '0.59228', 'examples_per_second': '30.868', 'grad_norm': '25.125', 'counters/examples': 225504, 'counters/updates': 7047}
train stats after 225536 examples: {'rewards_train/chosen': '0.19274', 'rewards_train/rejected': '0.12674', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.066003', 'logps_train/rejected': '-137.58', 'logps_train/chosen': '-151.78', 'loss/train': '0.67664', 'examples_per_second': '33.138', 'grad_norm': '37.5', 'counters/examples': 225536, 'counters/updates': 7048}
train stats after 225568 examples: {'rewards_train/chosen': '0.028129', 'rewards_train/rejected': '0.038091', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0099618', 'logps_train/rejected': '-109.86', 'logps_train/chosen': '-116.71', 'loss/train': '0.7129', 'examples_per_second': '30.717', 'grad_norm': '35.5', 'counters/examples': 225568, 'counters/updates': 7049}
train stats after 225600 examples: {'rewards_train/chosen': '0.11189', 'rewards_train/rejected': '0.23383', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.12195', 'logps_train/rejected': '-152.86', 'logps_train/chosen': '-149.46', 'loss/train': '0.79237', 'examples_per_second': '31.454', 'grad_norm': '65.5', 'counters/examples': 225600, 'counters/updates': 7050}
train stats after 225632 examples: {'rewards_train/chosen': '0.21156', 'rewards_train/rejected': '0.0068929', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20467', 'logps_train/rejected': '-106.29', 'logps_train/chosen': '-124.74', 'loss/train': '0.60935', 'examples_per_second': '31.431', 'grad_norm': '23.75', 'counters/examples': 225632, 'counters/updates': 7051}
train stats after 225664 examples: {'rewards_train/chosen': '0.17637', 'rewards_train/rejected': '0.0053893', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17099', 'logps_train/rejected': '-119.29', 'logps_train/chosen': '-134.82', 'loss/train': '0.62392', 'examples_per_second': '31.308', 'grad_norm': '27.875', 'counters/examples': 225664, 'counters/updates': 7052}
train stats after 225696 examples: {'rewards_train/chosen': '0.24994', 'rewards_train/rejected': '0.12951', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12044', 'logps_train/rejected': '-157.76', 'logps_train/chosen': '-120.69', 'loss/train': '0.65746', 'examples_per_second': '31.199', 'grad_norm': '49.5', 'counters/examples': 225696, 'counters/updates': 7053}
train stats after 225728 examples: {'rewards_train/chosen': '0.19446', 'rewards_train/rejected': '0.073246', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12122', 'logps_train/rejected': '-133.35', 'logps_train/chosen': '-178.03', 'loss/train': '0.65078', 'examples_per_second': '31.452', 'grad_norm': '86', 'counters/examples': 225728, 'counters/updates': 7054}
train stats after 225760 examples: {'rewards_train/chosen': '0.17498', 'rewards_train/rejected': '0.11914', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.055835', 'logps_train/rejected': '-108.96', 'logps_train/chosen': '-98.509', 'loss/train': '0.67884', 'examples_per_second': '31.59', 'grad_norm': '39.75', 'counters/examples': 225760, 'counters/updates': 7055}
train stats after 225792 examples: {'rewards_train/chosen': '0.11878', 'rewards_train/rejected': '-0.0087726', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12756', 'logps_train/rejected': '-95.925', 'logps_train/chosen': '-143.57', 'loss/train': '0.64008', 'examples_per_second': '30.287', 'grad_norm': '25.125', 'counters/examples': 225792, 'counters/updates': 7056}
train stats after 225824 examples: {'rewards_train/chosen': '0.14914', 'rewards_train/rejected': '0.068304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08084', 'logps_train/rejected': '-138.33', 'logps_train/chosen': '-156.71', 'loss/train': '0.6623', 'examples_per_second': '32.248', 'grad_norm': '28.5', 'counters/examples': 225824, 'counters/updates': 7057}
skipping logging after 225856 examples to avoid logging too frequently
train stats after 225888 examples: {'rewards_train/chosen': '0.22422', 'rewards_train/rejected': '0.06734', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15688', 'logps_train/rejected': '-122.55', 'logps_train/chosen': '-157.52', 'loss/train': '0.63327', 'examples_per_second': '30.388', 'grad_norm': '39', 'counters/examples': 225888, 'counters/updates': 7059}
train stats after 225920 examples: {'rewards_train/chosen': '0.12832', 'rewards_train/rejected': '0.046021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.082304', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-143.79', 'loss/train': '0.66113', 'examples_per_second': '29.927', 'grad_norm': '26.125', 'counters/examples': 225920, 'counters/updates': 7060}
train stats after 225952 examples: {'rewards_train/chosen': '0.086395', 'rewards_train/rejected': '0.061469', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024925', 'logps_train/rejected': '-134.95', 'logps_train/chosen': '-102.4', 'loss/train': '0.70012', 'examples_per_second': '32.095', 'grad_norm': '36', 'counters/examples': 225952, 'counters/updates': 7061}
train stats after 225984 examples: {'rewards_train/chosen': '0.19574', 'rewards_train/rejected': '0.031574', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16416', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-148.08', 'loss/train': '0.62547', 'examples_per_second': '31.367', 'grad_norm': '30.625', 'counters/examples': 225984, 'counters/updates': 7062}
train stats after 226016 examples: {'rewards_train/chosen': '0.23935', 'rewards_train/rejected': '0.048974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19037', 'logps_train/rejected': '-157.54', 'logps_train/chosen': '-168.56', 'loss/train': '0.6224', 'examples_per_second': '31.635', 'grad_norm': '35', 'counters/examples': 226016, 'counters/updates': 7063}
skipping logging after 226048 examples to avoid logging too frequently
train stats after 226080 examples: {'rewards_train/chosen': '0.17902', 'rewards_train/rejected': '0.036714', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14231', 'logps_train/rejected': '-99.235', 'logps_train/chosen': '-140.49', 'loss/train': '0.64638', 'examples_per_second': '30.92', 'grad_norm': '26.125', 'counters/examples': 226080, 'counters/updates': 7065}
skipping logging after 226112 examples to avoid logging too frequently
train stats after 226144 examples: {'rewards_train/chosen': '0.078934', 'rewards_train/rejected': '0.029481', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.049453', 'logps_train/rejected': '-125.7', 'logps_train/chosen': '-142.66', 'loss/train': '0.6837', 'examples_per_second': '33.722', 'grad_norm': '31.25', 'counters/examples': 226144, 'counters/updates': 7067}
train stats after 226176 examples: {'rewards_train/chosen': '0.12187', 'rewards_train/rejected': '0.093556', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028318', 'logps_train/rejected': '-156.91', 'logps_train/chosen': '-116.63', 'loss/train': '0.6921', 'examples_per_second': '31.021', 'grad_norm': '40.5', 'counters/examples': 226176, 'counters/updates': 7068}
train stats after 226208 examples: {'rewards_train/chosen': '0.20313', 'rewards_train/rejected': '0.079278', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12386', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-112.64', 'loss/train': '0.65664', 'examples_per_second': '32.105', 'grad_norm': '34', 'counters/examples': 226208, 'counters/updates': 7069}
train stats after 226240 examples: {'rewards_train/chosen': '0.24361', 'rewards_train/rejected': '0.10356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14004', 'logps_train/rejected': '-133.66', 'logps_train/chosen': '-179.35', 'loss/train': '0.64532', 'examples_per_second': '31.228', 'grad_norm': '33.25', 'counters/examples': 226240, 'counters/updates': 7070}
train stats after 226272 examples: {'rewards_train/chosen': '0.15351', 'rewards_train/rejected': '0.020593', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13292', 'logps_train/rejected': '-112.92', 'logps_train/chosen': '-110.25', 'loss/train': '0.64141', 'examples_per_second': '29.952', 'grad_norm': '28.625', 'counters/examples': 226272, 'counters/updates': 7071}
train stats after 226304 examples: {'rewards_train/chosen': '0.15517', 'rewards_train/rejected': '0.15653', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0013609', 'logps_train/rejected': '-144.86', 'logps_train/chosen': '-122.13', 'loss/train': '0.7006', 'examples_per_second': '32.633', 'grad_norm': '31.125', 'counters/examples': 226304, 'counters/updates': 7072}
train stats after 226336 examples: {'rewards_train/chosen': '0.20313', 'rewards_train/rejected': '0.091473', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.11166', 'logps_train/rejected': '-137.3', 'logps_train/chosen': '-123.48', 'loss/train': '0.6576', 'examples_per_second': '33.171', 'grad_norm': '31.625', 'counters/examples': 226336, 'counters/updates': 7073}
train stats after 226368 examples: {'rewards_train/chosen': '0.17192', 'rewards_train/rejected': '0.04453', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12739', 'logps_train/rejected': '-135.38', 'logps_train/chosen': '-156.67', 'loss/train': '0.64839', 'examples_per_second': '31.457', 'grad_norm': '28.25', 'counters/examples': 226368, 'counters/updates': 7074}
train stats after 226400 examples: {'rewards_train/chosen': '0.12017', 'rewards_train/rejected': '0.051935', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068233', 'logps_train/rejected': '-120.26', 'logps_train/chosen': '-126.8', 'loss/train': '0.67383', 'examples_per_second': '29.917', 'grad_norm': '27.5', 'counters/examples': 226400, 'counters/updates': 7075}
train stats after 226432 examples: {'rewards_train/chosen': '0.17705', 'rewards_train/rejected': '0.10141', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.075636', 'logps_train/rejected': '-155.25', 'logps_train/chosen': '-157.39', 'loss/train': '0.67532', 'examples_per_second': '31.205', 'grad_norm': '34.75', 'counters/examples': 226432, 'counters/updates': 7076}
train stats after 226464 examples: {'rewards_train/chosen': '0.13394', 'rewards_train/rejected': '0.026535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1074', 'logps_train/rejected': '-97.421', 'logps_train/chosen': '-116.96', 'loss/train': '0.64957', 'examples_per_second': '31.498', 'grad_norm': '33', 'counters/examples': 226464, 'counters/updates': 7077}
train stats after 226496 examples: {'rewards_train/chosen': '0.23051', 'rewards_train/rejected': '0.0042039', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2263', 'logps_train/rejected': '-129.9', 'logps_train/chosen': '-148.96', 'loss/train': '0.59879', 'examples_per_second': '30.781', 'grad_norm': '27.25', 'counters/examples': 226496, 'counters/updates': 7078}
train stats after 226528 examples: {'rewards_train/chosen': '0.24799', 'rewards_train/rejected': '0.1826', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065399', 'logps_train/rejected': '-117', 'logps_train/chosen': '-141.69', 'loss/train': '0.67699', 'examples_per_second': '30.469', 'grad_norm': '28.375', 'counters/examples': 226528, 'counters/updates': 7079}
train stats after 226560 examples: {'rewards_train/chosen': '0.10236', 'rewards_train/rejected': '0.069701', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.032657', 'logps_train/rejected': '-128.63', 'logps_train/chosen': '-142.15', 'loss/train': '0.68523', 'examples_per_second': '30.616', 'grad_norm': '26.125', 'counters/examples': 226560, 'counters/updates': 7080}
train stats after 226592 examples: {'rewards_train/chosen': '0.17428', 'rewards_train/rejected': '-0.016035', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19032', 'logps_train/rejected': '-166.25', 'logps_train/chosen': '-180.73', 'loss/train': '0.62417', 'examples_per_second': '31.478', 'grad_norm': '44.25', 'counters/examples': 226592, 'counters/updates': 7081}
skipping logging after 226624 examples to avoid logging too frequently
train stats after 226656 examples: {'rewards_train/chosen': '0.11363', 'rewards_train/rejected': '0.092163', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.021466', 'logps_train/rejected': '-128.72', 'logps_train/chosen': '-153.1', 'loss/train': '0.70424', 'examples_per_second': '31.386', 'grad_norm': '36', 'counters/examples': 226656, 'counters/updates': 7083}
train stats after 226688 examples: {'rewards_train/chosen': '0.1635', 'rewards_train/rejected': '0.14543', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.018074', 'logps_train/rejected': '-131.11', 'logps_train/chosen': '-146.52', 'loss/train': '0.69342', 'examples_per_second': '32.011', 'grad_norm': '34.75', 'counters/examples': 226688, 'counters/updates': 7084}
skipping logging after 226720 examples to avoid logging too frequently
train stats after 226752 examples: {'rewards_train/chosen': '0.16172', 'rewards_train/rejected': '0.099547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062176', 'logps_train/rejected': '-135.48', 'logps_train/chosen': '-138.42', 'loss/train': '0.67569', 'examples_per_second': '31.18', 'grad_norm': '37.75', 'counters/examples': 226752, 'counters/updates': 7086}
train stats after 226784 examples: {'rewards_train/chosen': '0.21063', 'rewards_train/rejected': '0.0012375', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20939', 'logps_train/rejected': '-119.39', 'logps_train/chosen': '-180.78', 'loss/train': '0.63762', 'examples_per_second': '32.602', 'grad_norm': '37.25', 'counters/examples': 226784, 'counters/updates': 7087}
train stats after 226816 examples: {'rewards_train/chosen': '0.18231', 'rewards_train/rejected': '0.021964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16034', 'logps_train/rejected': '-108.08', 'logps_train/chosen': '-143.35', 'loss/train': '0.63704', 'examples_per_second': '30.023', 'grad_norm': '25.75', 'counters/examples': 226816, 'counters/updates': 7088}
train stats after 226848 examples: {'rewards_train/chosen': '0.095742', 'rewards_train/rejected': '-0.016245', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11199', 'logps_train/rejected': '-124.13', 'logps_train/chosen': '-146.29', 'loss/train': '0.64794', 'examples_per_second': '32.047', 'grad_norm': '28', 'counters/examples': 226848, 'counters/updates': 7089}
train stats after 226880 examples: {'rewards_train/chosen': '0.26459', 'rewards_train/rejected': '0.027481', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23711', 'logps_train/rejected': '-146.03', 'logps_train/chosen': '-179.69', 'loss/train': '0.61875', 'examples_per_second': '29.907', 'grad_norm': '33.75', 'counters/examples': 226880, 'counters/updates': 7090}
train stats after 226912 examples: {'rewards_train/chosen': '0.11862', 'rewards_train/rejected': '0.091246', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.027376', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-141.57', 'loss/train': '0.69416', 'examples_per_second': '32.815', 'grad_norm': '28.125', 'counters/examples': 226912, 'counters/updates': 7091}
train stats after 226944 examples: {'rewards_train/chosen': '0.20637', 'rewards_train/rejected': '0.0038727', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2025', 'logps_train/rejected': '-130.97', 'logps_train/chosen': '-166.7', 'loss/train': '0.61666', 'examples_per_second': '30.137', 'grad_norm': '25.5', 'counters/examples': 226944, 'counters/updates': 7092}
train stats after 226976 examples: {'rewards_train/chosen': '0.14351', 'rewards_train/rejected': '0.031368', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11215', 'logps_train/rejected': '-128.01', 'logps_train/chosen': '-198.18', 'loss/train': '0.64773', 'examples_per_second': '31.487', 'grad_norm': '33.5', 'counters/examples': 226976, 'counters/updates': 7093}
skipping logging after 227008 examples to avoid logging too frequently
train stats after 227040 examples: {'rewards_train/chosen': '0.21029', 'rewards_train/rejected': '0.01997', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19032', 'logps_train/rejected': '-143.77', 'logps_train/chosen': '-189.89', 'loss/train': '0.62345', 'examples_per_second': '31.494', 'grad_norm': '42.25', 'counters/examples': 227040, 'counters/updates': 7095}
train stats after 227072 examples: {'rewards_train/chosen': '0.098661', 'rewards_train/rejected': '0.046013', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052648', 'logps_train/rejected': '-124.2', 'logps_train/chosen': '-150.54', 'loss/train': '0.67997', 'examples_per_second': '31.503', 'grad_norm': '26.125', 'counters/examples': 227072, 'counters/updates': 7096}
train stats after 227104 examples: {'rewards_train/chosen': '0.27839', 'rewards_train/rejected': '-0.00099209', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.27938', 'logps_train/rejected': '-124.74', 'logps_train/chosen': '-194.12', 'loss/train': '0.57391', 'examples_per_second': '30.033', 'grad_norm': '30.5', 'counters/examples': 227104, 'counters/updates': 7097}
train stats after 227136 examples: {'rewards_train/chosen': '0.13719', 'rewards_train/rejected': '0.095576', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041619', 'logps_train/rejected': '-138.62', 'logps_train/chosen': '-156.21', 'loss/train': '0.69556', 'examples_per_second': '31.478', 'grad_norm': '32.5', 'counters/examples': 227136, 'counters/updates': 7098}
train stats after 227168 examples: {'rewards_train/chosen': '0.24588', 'rewards_train/rejected': '0.1133', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13258', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-153.23', 'loss/train': '0.6498', 'examples_per_second': '31.908', 'grad_norm': '26.75', 'counters/examples': 227168, 'counters/updates': 7099}
train stats after 227200 examples: {'rewards_train/chosen': '0.13647', 'rewards_train/rejected': '0.15412', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.017647', 'logps_train/rejected': '-168.38', 'logps_train/chosen': '-165.4', 'loss/train': '0.72528', 'examples_per_second': '31.468', 'grad_norm': '49', 'counters/examples': 227200, 'counters/updates': 7100}
train stats after 227232 examples: {'rewards_train/chosen': '0.12411', 'rewards_train/rejected': '0.023829', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10028', 'logps_train/rejected': '-144.07', 'logps_train/chosen': '-164.75', 'loss/train': '0.65717', 'examples_per_second': '31.107', 'grad_norm': '71', 'counters/examples': 227232, 'counters/updates': 7101}
skipping logging after 227264 examples to avoid logging too frequently
train stats after 227296 examples: {'rewards_train/chosen': '0.15992', 'rewards_train/rejected': '0.0095869', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15033', 'logps_train/rejected': '-101.19', 'logps_train/chosen': '-169.72', 'loss/train': '0.63303', 'examples_per_second': '30.883', 'grad_norm': '28.5', 'counters/examples': 227296, 'counters/updates': 7103}
skipping logging after 227328 examples to avoid logging too frequently
train stats after 227360 examples: {'rewards_train/chosen': '0.11123', 'rewards_train/rejected': '0.044226', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067005', 'logps_train/rejected': '-143.26', 'logps_train/chosen': '-100.82', 'loss/train': '0.66797', 'examples_per_second': '30.541', 'grad_norm': '30.625', 'counters/examples': 227360, 'counters/updates': 7105}
skipping logging after 227392 examples to avoid logging too frequently
train stats after 227424 examples: {'rewards_train/chosen': '0.22114', 'rewards_train/rejected': '0.13608', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.085059', 'logps_train/rejected': '-141.46', 'logps_train/chosen': '-151.46', 'loss/train': '0.65861', 'examples_per_second': '34.147', 'grad_norm': '61.75', 'counters/examples': 227424, 'counters/updates': 7107}
train stats after 227456 examples: {'rewards_train/chosen': '0.24686', 'rewards_train/rejected': '0.020112', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.22675', 'logps_train/rejected': '-116.53', 'logps_train/chosen': '-167.96', 'loss/train': '0.59675', 'examples_per_second': '31.35', 'grad_norm': '29.5', 'counters/examples': 227456, 'counters/updates': 7108}
train stats after 227488 examples: {'rewards_train/chosen': '0.18162', 'rewards_train/rejected': '0.0066806', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17494', 'logps_train/rejected': '-120.12', 'logps_train/chosen': '-141.74', 'loss/train': '0.62568', 'examples_per_second': '31.489', 'grad_norm': '30.375', 'counters/examples': 227488, 'counters/updates': 7109}
train stats after 227520 examples: {'rewards_train/chosen': '0.20584', 'rewards_train/rejected': '0.003164', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20268', 'logps_train/rejected': '-103.55', 'logps_train/chosen': '-120.38', 'loss/train': '0.61667', 'examples_per_second': '31.262', 'grad_norm': '21.625', 'counters/examples': 227520, 'counters/updates': 7110}
train stats after 227552 examples: {'rewards_train/chosen': '0.11995', 'rewards_train/rejected': '0.10239', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.017562', 'logps_train/rejected': '-135', 'logps_train/chosen': '-151.04', 'loss/train': '0.69978', 'examples_per_second': '31.373', 'grad_norm': '38', 'counters/examples': 227552, 'counters/updates': 7111}
skipping logging after 227584 examples to avoid logging too frequently
train stats after 227616 examples: {'rewards_train/chosen': '0.19084', 'rewards_train/rejected': '0.045261', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14558', 'logps_train/rejected': '-129.96', 'logps_train/chosen': '-139.26', 'loss/train': '0.64778', 'examples_per_second': '35.841', 'grad_norm': '29.375', 'counters/examples': 227616, 'counters/updates': 7113}
train stats after 227648 examples: {'rewards_train/chosen': '0.16451', 'rewards_train/rejected': '-0.03946', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20397', 'logps_train/rejected': '-101.12', 'logps_train/chosen': '-125.12', 'loss/train': '0.6124', 'examples_per_second': '30.658', 'grad_norm': '24.875', 'counters/examples': 227648, 'counters/updates': 7114}
train stats after 227680 examples: {'rewards_train/chosen': '0.14397', 'rewards_train/rejected': '0.076278', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.067688', 'logps_train/rejected': '-117.21', 'logps_train/chosen': '-149', 'loss/train': '0.67982', 'examples_per_second': '32.615', 'grad_norm': '25.625', 'counters/examples': 227680, 'counters/updates': 7115}
train stats after 227712 examples: {'rewards_train/chosen': '0.11083', 'rewards_train/rejected': '0.13421', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.023382', 'logps_train/rejected': '-183.79', 'logps_train/chosen': '-117.96', 'loss/train': '0.71741', 'examples_per_second': '30.067', 'grad_norm': '40.75', 'counters/examples': 227712, 'counters/updates': 7116}
train stats after 227744 examples: {'rewards_train/chosen': '0.2059', 'rewards_train/rejected': '0.1187', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087204', 'logps_train/rejected': '-152.92', 'logps_train/chosen': '-157.39', 'loss/train': '0.67798', 'examples_per_second': '29.993', 'grad_norm': '40.25', 'counters/examples': 227744, 'counters/updates': 7117}
train stats after 227776 examples: {'rewards_train/chosen': '0.055593', 'rewards_train/rejected': '-0.019056', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074649', 'logps_train/rejected': '-126.56', 'logps_train/chosen': '-145.03', 'loss/train': '0.66552', 'examples_per_second': '29.9', 'grad_norm': '30.375', 'counters/examples': 227776, 'counters/updates': 7118}
train stats after 227808 examples: {'rewards_train/chosen': '0.19912', 'rewards_train/rejected': '0.081752', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.11737', 'logps_train/rejected': '-164.6', 'logps_train/chosen': '-156.6', 'loss/train': '0.65948', 'examples_per_second': '31.49', 'grad_norm': '44.75', 'counters/examples': 227808, 'counters/updates': 7119}
train stats after 227840 examples: {'rewards_train/chosen': '0.10415', 'rewards_train/rejected': '0.035191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06896', 'logps_train/rejected': '-148.39', 'logps_train/chosen': '-162.14', 'loss/train': '0.67301', 'examples_per_second': '31.449', 'grad_norm': '43.25', 'counters/examples': 227840, 'counters/updates': 7120}
skipping logging after 227872 examples to avoid logging too frequently
train stats after 227904 examples: {'rewards_train/chosen': '0.22798', 'rewards_train/rejected': '0.11999', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10798', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-165.58', 'loss/train': '0.65812', 'examples_per_second': '30.358', 'grad_norm': '31.5', 'counters/examples': 227904, 'counters/updates': 7122}
skipping logging after 227936 examples to avoid logging too frequently
train stats after 227968 examples: {'rewards_train/chosen': '0.18524', 'rewards_train/rejected': '-0.038371', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22361', 'logps_train/rejected': '-138.93', 'logps_train/chosen': '-154.2', 'loss/train': '0.6021', 'examples_per_second': '31.457', 'grad_norm': '28.5', 'counters/examples': 227968, 'counters/updates': 7124}
train stats after 228000 examples: {'rewards_train/chosen': '0.12964', 'rewards_train/rejected': '-0.059724', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18937', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-151.28', 'loss/train': '0.62301', 'examples_per_second': '31.454', 'grad_norm': '30.75', 'counters/examples': 228000, 'counters/updates': 7125}
Running evaluation after 228000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.08it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.98it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.93it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.90it/s]
eval after 228000: {'rewards_eval/chosen': '0.18077', 'rewards_eval/rejected': '0.071812', 'rewards_eval/accuracies': '0.60938', 'rewards_eval/margins': '0.10896', 'logps_eval/rejected': '-121.41', 'logps_eval/chosen': '-142.3', 'loss/eval': '0.66091'}
skipping save for non epoch
train stats after 228032 examples: {'rewards_train/chosen': '0.15978', 'rewards_train/rejected': '0.11553', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.044252', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-129.04', 'loss/train': '0.69341', 'examples_per_second': '32.891', 'grad_norm': '38', 'counters/examples': 228032, 'counters/updates': 7126}
train stats after 228064 examples: {'rewards_train/chosen': '0.25692', 'rewards_train/rejected': '-0.019945', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.27686', 'logps_train/rejected': '-111.32', 'logps_train/chosen': '-191.37', 'loss/train': '0.57609', 'examples_per_second': '31.265', 'grad_norm': '28.25', 'counters/examples': 228064, 'counters/updates': 7127}
train stats after 228096 examples: {'rewards_train/chosen': '0.16098', 'rewards_train/rejected': '0.16384', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0028684', 'logps_train/rejected': '-142.28', 'logps_train/chosen': '-161', 'loss/train': '0.70492', 'examples_per_second': '29.939', 'grad_norm': '38.25', 'counters/examples': 228096, 'counters/updates': 7128}
train stats after 228128 examples: {'rewards_train/chosen': '0.16818', 'rewards_train/rejected': '0.086762', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.081418', 'logps_train/rejected': '-188.66', 'logps_train/chosen': '-165.85', 'loss/train': '0.66793', 'examples_per_second': '29.857', 'grad_norm': '37.5', 'counters/examples': 228128, 'counters/updates': 7129}
train stats after 228160 examples: {'rewards_train/chosen': '0.12219', 'rewards_train/rejected': '0.082842', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.039353', 'logps_train/rejected': '-108.97', 'logps_train/chosen': '-99.556', 'loss/train': '0.68647', 'examples_per_second': '32.954', 'grad_norm': '29.875', 'counters/examples': 228160, 'counters/updates': 7130}
skipping logging after 228192 examples to avoid logging too frequently
train stats after 228224 examples: {'rewards_train/chosen': '0.11741', 'rewards_train/rejected': '0.0051324', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11228', 'logps_train/rejected': '-137.45', 'logps_train/chosen': '-146.7', 'loss/train': '0.64872', 'examples_per_second': '34.456', 'grad_norm': '26.375', 'counters/examples': 228224, 'counters/updates': 7132}
train stats after 228256 examples: {'rewards_train/chosen': '0.13832', 'rewards_train/rejected': '-0.00063463', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13895', 'logps_train/rejected': '-95.022', 'logps_train/chosen': '-137.82', 'loss/train': '0.64779', 'examples_per_second': '31.185', 'grad_norm': '26.625', 'counters/examples': 228256, 'counters/updates': 7133}
train stats after 228288 examples: {'rewards_train/chosen': '0.1703', 'rewards_train/rejected': '-0.0074985', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1778', 'logps_train/rejected': '-123.81', 'logps_train/chosen': '-163.27', 'loss/train': '0.62251', 'examples_per_second': '32.64', 'grad_norm': '31', 'counters/examples': 228288, 'counters/updates': 7134}
train stats after 228320 examples: {'rewards_train/chosen': '0.19775', 'rewards_train/rejected': '0.044859', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15289', 'logps_train/rejected': '-118.5', 'logps_train/chosen': '-142.22', 'loss/train': '0.62842', 'examples_per_second': '32.732', 'grad_norm': '32', 'counters/examples': 228320, 'counters/updates': 7135}
train stats after 228352 examples: {'rewards_train/chosen': '0.21347', 'rewards_train/rejected': '0.049985', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16349', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-168.12', 'loss/train': '0.63178', 'examples_per_second': '30.082', 'grad_norm': '26.375', 'counters/examples': 228352, 'counters/updates': 7136}
train stats after 228384 examples: {'rewards_train/chosen': '0.13573', 'rewards_train/rejected': '0.025913', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10982', 'logps_train/rejected': '-106.77', 'logps_train/chosen': '-138.71', 'loss/train': '0.65137', 'examples_per_second': '31.525', 'grad_norm': '30.25', 'counters/examples': 228384, 'counters/updates': 7137}
train stats after 228416 examples: {'rewards_train/chosen': '0.29673', 'rewards_train/rejected': '0.05945', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23728', 'logps_train/rejected': '-145.94', 'logps_train/chosen': '-201.31', 'loss/train': '0.6351', 'examples_per_second': '31.443', 'grad_norm': '29.5', 'counters/examples': 228416, 'counters/updates': 7138}
train stats after 228448 examples: {'rewards_train/chosen': '0.25009', 'rewards_train/rejected': '0.14309', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.107', 'logps_train/rejected': '-146.72', 'logps_train/chosen': '-167.56', 'loss/train': '0.66911', 'examples_per_second': '31.303', 'grad_norm': '42.5', 'counters/examples': 228448, 'counters/updates': 7139}
skipping logging after 228480 examples to avoid logging too frequently
train stats after 228512 examples: {'rewards_train/chosen': '0.10224', 'rewards_train/rejected': '0.068716', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.033523', 'logps_train/rejected': '-145.93', 'logps_train/chosen': '-183.1', 'loss/train': '0.6921', 'examples_per_second': '31.498', 'grad_norm': '34', 'counters/examples': 228512, 'counters/updates': 7141}
train stats after 228544 examples: {'rewards_train/chosen': '0.1853', 'rewards_train/rejected': '0.0079132', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17739', 'logps_train/rejected': '-116.52', 'logps_train/chosen': '-134.98', 'loss/train': '0.61867', 'examples_per_second': '29.654', 'grad_norm': '26.375', 'counters/examples': 228544, 'counters/updates': 7142}
skipping logging after 228576 examples to avoid logging too frequently
train stats after 228608 examples: {'rewards_train/chosen': '0.1078', 'rewards_train/rejected': '0.0028277', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10498', 'logps_train/rejected': '-103.42', 'logps_train/chosen': '-130.09', 'loss/train': '0.64909', 'examples_per_second': '31.99', 'grad_norm': '31.625', 'counters/examples': 228608, 'counters/updates': 7144}
train stats after 228640 examples: {'rewards_train/chosen': '0.15326', 'rewards_train/rejected': '0.045207', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10806', 'logps_train/rejected': '-109.04', 'logps_train/chosen': '-141.7', 'loss/train': '0.65412', 'examples_per_second': '31.438', 'grad_norm': '32.75', 'counters/examples': 228640, 'counters/updates': 7145}
skipping logging after 228672 examples to avoid logging too frequently
train stats after 228704 examples: {'rewards_train/chosen': '0.10977', 'rewards_train/rejected': '0.012174', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097594', 'logps_train/rejected': '-96.252', 'logps_train/chosen': '-141.52', 'loss/train': '0.66498', 'examples_per_second': '31.619', 'grad_norm': '25.75', 'counters/examples': 228704, 'counters/updates': 7147}
train stats after 228736 examples: {'rewards_train/chosen': '0.077285', 'rewards_train/rejected': '-0.014141', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091425', 'logps_train/rejected': '-129.34', 'logps_train/chosen': '-133.51', 'loss/train': '0.66035', 'examples_per_second': '31.776', 'grad_norm': '41.25', 'counters/examples': 228736, 'counters/updates': 7148}
train stats after 228768 examples: {'rewards_train/chosen': '0.20493', 'rewards_train/rejected': '0.062476', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14245', 'logps_train/rejected': '-133.45', 'logps_train/chosen': '-160.41', 'loss/train': '0.65753', 'examples_per_second': '24.128', 'grad_norm': '37.75', 'counters/examples': 228768, 'counters/updates': 7149}
train stats after 228800 examples: {'rewards_train/chosen': '0.16442', 'rewards_train/rejected': '0.15259', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011839', 'logps_train/rejected': '-144.12', 'logps_train/chosen': '-142.81', 'loss/train': '0.70105', 'examples_per_second': '30.561', 'grad_norm': '28.25', 'counters/examples': 228800, 'counters/updates': 7150}
skipping logging after 228832 examples to avoid logging too frequently
train stats after 228864 examples: {'rewards_train/chosen': '0.23', 'rewards_train/rejected': '0.18094', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.049059', 'logps_train/rejected': '-135.25', 'logps_train/chosen': '-155.88', 'loss/train': '0.68068', 'examples_per_second': '33.18', 'grad_norm': '38', 'counters/examples': 228864, 'counters/updates': 7152}
skipping logging after 228896 examples to avoid logging too frequently
train stats after 228928 examples: {'rewards_train/chosen': '0.19277', 'rewards_train/rejected': '0.10329', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089483', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-125.32', 'loss/train': '0.66422', 'examples_per_second': '32.383', 'grad_norm': '30.25', 'counters/examples': 228928, 'counters/updates': 7154}
skipping logging after 228960 examples to avoid logging too frequently
train stats after 228992 examples: {'rewards_train/chosen': '0.22612', 'rewards_train/rejected': '0.087977', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13815', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-143.06', 'loss/train': '0.63833', 'examples_per_second': '31.369', 'grad_norm': '27.25', 'counters/examples': 228992, 'counters/updates': 7156}
train stats after 229024 examples: {'rewards_train/chosen': '0.26807', 'rewards_train/rejected': '0.066174', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2019', 'logps_train/rejected': '-104.43', 'logps_train/chosen': '-154.97', 'loss/train': '0.61936', 'examples_per_second': '31.883', 'grad_norm': '25.25', 'counters/examples': 229024, 'counters/updates': 7157}
train stats after 229056 examples: {'rewards_train/chosen': '0.1425', 'rewards_train/rejected': '0.073809', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.068689', 'logps_train/rejected': '-126.3', 'logps_train/chosen': '-123.49', 'loss/train': '0.67163', 'examples_per_second': '31.443', 'grad_norm': '67', 'counters/examples': 229056, 'counters/updates': 7158}
train stats after 229088 examples: {'rewards_train/chosen': '0.16263', 'rewards_train/rejected': '0.16371', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0010862', 'logps_train/rejected': '-138.94', 'logps_train/chosen': '-137.92', 'loss/train': '0.71459', 'examples_per_second': '31.115', 'grad_norm': '37.75', 'counters/examples': 229088, 'counters/updates': 7159}
train stats after 229120 examples: {'rewards_train/chosen': '0.28249', 'rewards_train/rejected': '0.037581', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24491', 'logps_train/rejected': '-119.11', 'logps_train/chosen': '-176.81', 'loss/train': '0.59331', 'examples_per_second': '30.158', 'grad_norm': '38.25', 'counters/examples': 229120, 'counters/updates': 7160}
train stats after 229152 examples: {'rewards_train/chosen': '0.1778', 'rewards_train/rejected': '0.082073', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095727', 'logps_train/rejected': '-118.51', 'logps_train/chosen': '-140.88', 'loss/train': '0.65752', 'examples_per_second': '30.03', 'grad_norm': '29.375', 'counters/examples': 229152, 'counters/updates': 7161}
train stats after 229184 examples: {'rewards_train/chosen': '0.16941', 'rewards_train/rejected': '0.09647', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07294', 'logps_train/rejected': '-105.42', 'logps_train/chosen': '-122.87', 'loss/train': '0.66632', 'examples_per_second': '31.191', 'grad_norm': '26.75', 'counters/examples': 229184, 'counters/updates': 7162}
train stats after 229216 examples: {'rewards_train/chosen': '0.16561', 'rewards_train/rejected': '0.17022', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0046107', 'logps_train/rejected': '-141.68', 'logps_train/chosen': '-161.79', 'loss/train': '0.72088', 'examples_per_second': '31.132', 'grad_norm': '44.75', 'counters/examples': 229216, 'counters/updates': 7163}
train stats after 229248 examples: {'rewards_train/chosen': '0.18959', 'rewards_train/rejected': '0.079627', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10997', 'logps_train/rejected': '-181.67', 'logps_train/chosen': '-171.49', 'loss/train': '0.66163', 'examples_per_second': '30.197', 'grad_norm': '40.25', 'counters/examples': 229248, 'counters/updates': 7164}
train stats after 229280 examples: {'rewards_train/chosen': '0.20255', 'rewards_train/rejected': '0.12174', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080806', 'logps_train/rejected': '-167.4', 'logps_train/chosen': '-158.42', 'loss/train': '0.6653', 'examples_per_second': '33.192', 'grad_norm': '39.75', 'counters/examples': 229280, 'counters/updates': 7165}
train stats after 229312 examples: {'rewards_train/chosen': '0.17528', 'rewards_train/rejected': '0.23005', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '-0.054774', 'logps_train/rejected': '-129.7', 'logps_train/chosen': '-136.2', 'loss/train': '0.78748', 'examples_per_second': '31.433', 'grad_norm': '52.25', 'counters/examples': 229312, 'counters/updates': 7166}
skipping logging after 229344 examples to avoid logging too frequently
train stats after 229376 examples: {'rewards_train/chosen': '0.11455', 'rewards_train/rejected': '-0.045275', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15982', 'logps_train/rejected': '-126.53', 'logps_train/chosen': '-162.16', 'loss/train': '0.63272', 'examples_per_second': '30.055', 'grad_norm': '26.75', 'counters/examples': 229376, 'counters/updates': 7168}
train stats after 229408 examples: {'rewards_train/chosen': '0.10509', 'rewards_train/rejected': '0.074603', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.030483', 'logps_train/rejected': '-135.8', 'logps_train/chosen': '-166.3', 'loss/train': '0.69349', 'examples_per_second': '32.623', 'grad_norm': '30.625', 'counters/examples': 229408, 'counters/updates': 7169}
train stats after 229440 examples: {'rewards_train/chosen': '0.083533', 'rewards_train/rejected': '0.0059802', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.077553', 'logps_train/rejected': '-107.84', 'logps_train/chosen': '-149.18', 'loss/train': '0.66398', 'examples_per_second': '29.978', 'grad_norm': '30.5', 'counters/examples': 229440, 'counters/updates': 7170}
train stats after 229472 examples: {'rewards_train/chosen': '0.092424', 'rewards_train/rejected': '0.070458', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.021966', 'logps_train/rejected': '-110.29', 'logps_train/chosen': '-120.3', 'loss/train': '0.69618', 'examples_per_second': '31.727', 'grad_norm': '31.625', 'counters/examples': 229472, 'counters/updates': 7171}
train stats after 229504 examples: {'rewards_train/chosen': '0.15604', 'rewards_train/rejected': '0.059253', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.096786', 'logps_train/rejected': '-126.19', 'logps_train/chosen': '-172.45', 'loss/train': '0.66396', 'examples_per_second': '32.256', 'grad_norm': '33.25', 'counters/examples': 229504, 'counters/updates': 7172}
train stats after 229536 examples: {'rewards_train/chosen': '0.15052', 'rewards_train/rejected': '0.0058633', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14466', 'logps_train/rejected': '-133.26', 'logps_train/chosen': '-128.39', 'loss/train': '0.63628', 'examples_per_second': '31.878', 'grad_norm': '31.625', 'counters/examples': 229536, 'counters/updates': 7173}
train stats after 229568 examples: {'rewards_train/chosen': '0.19852', 'rewards_train/rejected': '0.072043', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12648', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-155.5', 'loss/train': '0.64296', 'examples_per_second': '31.048', 'grad_norm': '34.75', 'counters/examples': 229568, 'counters/updates': 7174}
train stats after 229600 examples: {'rewards_train/chosen': '0.11492', 'rewards_train/rejected': '0.12631', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.011392', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-120.23', 'loss/train': '0.71573', 'examples_per_second': '32.304', 'grad_norm': '41.5', 'counters/examples': 229600, 'counters/updates': 7175}
train stats after 229632 examples: {'rewards_train/chosen': '0.20203', 'rewards_train/rejected': '0.058011', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14402', 'logps_train/rejected': '-116.34', 'logps_train/chosen': '-144.86', 'loss/train': '0.63615', 'examples_per_second': '32.181', 'grad_norm': '25.625', 'counters/examples': 229632, 'counters/updates': 7176}
train stats after 229664 examples: {'rewards_train/chosen': '0.19909', 'rewards_train/rejected': '0.04237', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15672', 'logps_train/rejected': '-127.19', 'logps_train/chosen': '-171.07', 'loss/train': '0.63093', 'examples_per_second': '30.3', 'grad_norm': '28', 'counters/examples': 229664, 'counters/updates': 7177}
train stats after 229696 examples: {'rewards_train/chosen': '0.29744', 'rewards_train/rejected': '0.086264', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21118', 'logps_train/rejected': '-99.056', 'logps_train/chosen': '-170.55', 'loss/train': '0.60896', 'examples_per_second': '24.34', 'grad_norm': '26.375', 'counters/examples': 229696, 'counters/updates': 7178}
train stats after 229728 examples: {'rewards_train/chosen': '0.18862', 'rewards_train/rejected': '0.060228', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1284', 'logps_train/rejected': '-115.31', 'logps_train/chosen': '-162.29', 'loss/train': '0.6489', 'examples_per_second': '31.041', 'grad_norm': '31.25', 'counters/examples': 229728, 'counters/updates': 7179}
train stats after 229760 examples: {'rewards_train/chosen': '0.15583', 'rewards_train/rejected': '0.011867', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14397', 'logps_train/rejected': '-112.45', 'logps_train/chosen': '-167.56', 'loss/train': '0.63316', 'examples_per_second': '31.314', 'grad_norm': '30', 'counters/examples': 229760, 'counters/updates': 7180}
train stats after 229792 examples: {'rewards_train/chosen': '0.13888', 'rewards_train/rejected': '0.0038541', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13503', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-136.82', 'loss/train': '0.63982', 'examples_per_second': '24.348', 'grad_norm': '26.75', 'counters/examples': 229792, 'counters/updates': 7181}
train stats after 229824 examples: {'rewards_train/chosen': '0.26244', 'rewards_train/rejected': '0.0076473', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.25479', 'logps_train/rejected': '-93.827', 'logps_train/chosen': '-133.33', 'loss/train': '0.60608', 'examples_per_second': '32.347', 'grad_norm': '23.125', 'counters/examples': 229824, 'counters/updates': 7182}
skipping logging after 229856 examples to avoid logging too frequently
train stats after 229888 examples: {'rewards_train/chosen': '0.21428', 'rewards_train/rejected': '0.097677', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1166', 'logps_train/rejected': '-137.61', 'logps_train/chosen': '-170.71', 'loss/train': '0.67147', 'examples_per_second': '30.011', 'grad_norm': '39.75', 'counters/examples': 229888, 'counters/updates': 7184}
train stats after 229920 examples: {'rewards_train/chosen': '0.061527', 'rewards_train/rejected': '-0.057421', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11895', 'logps_train/rejected': '-125.83', 'logps_train/chosen': '-151.07', 'loss/train': '0.65041', 'examples_per_second': '30.497', 'grad_norm': '27', 'counters/examples': 229920, 'counters/updates': 7185}
train stats after 229952 examples: {'rewards_train/chosen': '0.15142', 'rewards_train/rejected': '0.040073', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11135', 'logps_train/rejected': '-97.405', 'logps_train/chosen': '-151.04', 'loss/train': '0.65068', 'examples_per_second': '31.551', 'grad_norm': '26.75', 'counters/examples': 229952, 'counters/updates': 7186}
skipping logging after 229984 examples to avoid logging too frequently
train stats after 230016 examples: {'rewards_train/chosen': '0.10822', 'rewards_train/rejected': '-0.047367', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15559', 'logps_train/rejected': '-97.107', 'logps_train/chosen': '-108.14', 'loss/train': '0.64072', 'examples_per_second': '31.45', 'grad_norm': '24.375', 'counters/examples': 230016, 'counters/updates': 7188}
train stats after 230048 examples: {'rewards_train/chosen': '0.14092', 'rewards_train/rejected': '0.1198', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.021121', 'logps_train/rejected': '-123.23', 'logps_train/chosen': '-164.08', 'loss/train': '0.6982', 'examples_per_second': '31.714', 'grad_norm': '41.25', 'counters/examples': 230048, 'counters/updates': 7189}
train stats after 230080 examples: {'rewards_train/chosen': '0.12826', 'rewards_train/rejected': '-0.0087259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13698', 'logps_train/rejected': '-100.8', 'logps_train/chosen': '-119.08', 'loss/train': '0.63516', 'examples_per_second': '30.837', 'grad_norm': '32.25', 'counters/examples': 230080, 'counters/updates': 7190}
skipping logging after 230112 examples to avoid logging too frequently
train stats after 230144 examples: {'rewards_train/chosen': '0.11248', 'rewards_train/rejected': '0.074436', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03804', 'logps_train/rejected': '-156.55', 'logps_train/chosen': '-164.82', 'loss/train': '0.6871', 'examples_per_second': '33.712', 'grad_norm': '35', 'counters/examples': 230144, 'counters/updates': 7192}
train stats after 230176 examples: {'rewards_train/chosen': '0.17921', 'rewards_train/rejected': '0.045162', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13405', 'logps_train/rejected': '-146.8', 'logps_train/chosen': '-157.66', 'loss/train': '0.64151', 'examples_per_second': '31.951', 'grad_norm': '31.25', 'counters/examples': 230176, 'counters/updates': 7193}
train stats after 230208 examples: {'rewards_train/chosen': '0.14009', 'rewards_train/rejected': '0.040976', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099118', 'logps_train/rejected': '-138.66', 'logps_train/chosen': '-148.31', 'loss/train': '0.65633', 'examples_per_second': '30.394', 'grad_norm': '41', 'counters/examples': 230208, 'counters/updates': 7194}
train stats after 230240 examples: {'rewards_train/chosen': '0.16549', 'rewards_train/rejected': '0.099254', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066235', 'logps_train/rejected': '-167.88', 'logps_train/chosen': '-137.15', 'loss/train': '0.67964', 'examples_per_second': '31.109', 'grad_norm': '67', 'counters/examples': 230240, 'counters/updates': 7195}
train stats after 230272 examples: {'rewards_train/chosen': '0.13277', 'rewards_train/rejected': '0.00029922', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13247', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-124.71', 'loss/train': '0.6347', 'examples_per_second': '30.722', 'grad_norm': '27.875', 'counters/examples': 230272, 'counters/updates': 7196}
train stats after 230304 examples: {'rewards_train/chosen': '0.19535', 'rewards_train/rejected': '0.013554', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1818', 'logps_train/rejected': '-114.13', 'logps_train/chosen': '-113.75', 'loss/train': '0.61804', 'examples_per_second': '32.399', 'grad_norm': '27.5', 'counters/examples': 230304, 'counters/updates': 7197}
train stats after 230336 examples: {'rewards_train/chosen': '0.11416', 'rewards_train/rejected': '0.039628', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074536', 'logps_train/rejected': '-120.77', 'logps_train/chosen': '-157.9', 'loss/train': '0.67025', 'examples_per_second': '30.931', 'grad_norm': '31.75', 'counters/examples': 230336, 'counters/updates': 7198}
train stats after 230368 examples: {'rewards_train/chosen': '0.16022', 'rewards_train/rejected': '0.089875', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070345', 'logps_train/rejected': '-145.89', 'logps_train/chosen': '-129.34', 'loss/train': '0.66817', 'examples_per_second': '30.853', 'grad_norm': '38.5', 'counters/examples': 230368, 'counters/updates': 7199}
train stats after 230400 examples: {'rewards_train/chosen': '0.13275', 'rewards_train/rejected': '0.18287', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.050125', 'logps_train/rejected': '-160.69', 'logps_train/chosen': '-126.7', 'loss/train': '0.7848', 'examples_per_second': '30.941', 'grad_norm': '105.5', 'counters/examples': 230400, 'counters/updates': 7200}
train stats after 230432 examples: {'rewards_train/chosen': '0.15374', 'rewards_train/rejected': '0.096046', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05769', 'logps_train/rejected': '-140.65', 'logps_train/chosen': '-159.12', 'loss/train': '0.68209', 'examples_per_second': '31.337', 'grad_norm': '33.25', 'counters/examples': 230432, 'counters/updates': 7201}
train stats after 230464 examples: {'rewards_train/chosen': '0.14925', 'rewards_train/rejected': '0.11437', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.034878', 'logps_train/rejected': '-178.86', 'logps_train/chosen': '-147.89', 'loss/train': '0.70807', 'examples_per_second': '31.477', 'grad_norm': '47', 'counters/examples': 230464, 'counters/updates': 7202}
train stats after 230496 examples: {'rewards_train/chosen': '0.1034', 'rewards_train/rejected': '0.058797', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044605', 'logps_train/rejected': '-101.03', 'logps_train/chosen': '-120.26', 'loss/train': '0.68256', 'examples_per_second': '31.05', 'grad_norm': '24.125', 'counters/examples': 230496, 'counters/updates': 7203}
train stats after 230528 examples: {'rewards_train/chosen': '0.18866', 'rewards_train/rejected': '0.0072734', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18139', 'logps_train/rejected': '-115.01', 'logps_train/chosen': '-170.22', 'loss/train': '0.61638', 'examples_per_second': '33.373', 'grad_norm': '34', 'counters/examples': 230528, 'counters/updates': 7204}
train stats after 230560 examples: {'rewards_train/chosen': '0.14188', 'rewards_train/rejected': '0.030184', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11169', 'logps_train/rejected': '-131.15', 'logps_train/chosen': '-117.53', 'loss/train': '0.64713', 'examples_per_second': '31.482', 'grad_norm': '25.5', 'counters/examples': 230560, 'counters/updates': 7205}
train stats after 230592 examples: {'rewards_train/chosen': '0.11482', 'rewards_train/rejected': '-0.0038756', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1187', 'logps_train/rejected': '-125.69', 'logps_train/chosen': '-132.53', 'loss/train': '0.64889', 'examples_per_second': '30.787', 'grad_norm': '30.5', 'counters/examples': 230592, 'counters/updates': 7206}
train stats after 230624 examples: {'rewards_train/chosen': '0.17132', 'rewards_train/rejected': '-0.0072843', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17861', 'logps_train/rejected': '-118.86', 'logps_train/chosen': '-116.44', 'loss/train': '0.62092', 'examples_per_second': '31.507', 'grad_norm': '28.25', 'counters/examples': 230624, 'counters/updates': 7207}
skipping logging after 230656 examples to avoid logging too frequently
train stats after 230688 examples: {'rewards_train/chosen': '0.13116', 'rewards_train/rejected': '0.037886', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093277', 'logps_train/rejected': '-103.76', 'logps_train/chosen': '-154.42', 'loss/train': '0.67483', 'examples_per_second': '31.046', 'grad_norm': '24.875', 'counters/examples': 230688, 'counters/updates': 7209}
train stats after 230720 examples: {'rewards_train/chosen': '0.2208', 'rewards_train/rejected': '0.072033', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14877', 'logps_train/rejected': '-120.68', 'logps_train/chosen': '-137.36', 'loss/train': '0.64059', 'examples_per_second': '30.614', 'grad_norm': '24.625', 'counters/examples': 230720, 'counters/updates': 7210}
train stats after 230752 examples: {'rewards_train/chosen': '0.14736', 'rewards_train/rejected': '0.044148', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10321', 'logps_train/rejected': '-154.65', 'logps_train/chosen': '-138.76', 'loss/train': '0.65638', 'examples_per_second': '31.519', 'grad_norm': '29.75', 'counters/examples': 230752, 'counters/updates': 7211}
skipping logging after 230784 examples to avoid logging too frequently
train stats after 230816 examples: {'rewards_train/chosen': '0.15271', 'rewards_train/rejected': '0.076116', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076596', 'logps_train/rejected': '-121.94', 'logps_train/chosen': '-160.34', 'loss/train': '0.66326', 'examples_per_second': '32.483', 'grad_norm': '27.375', 'counters/examples': 230816, 'counters/updates': 7213}
skipping logging after 230848 examples to avoid logging too frequently
train stats after 230880 examples: {'rewards_train/chosen': '0.12384', 'rewards_train/rejected': '-0.0009918', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12483', 'logps_train/rejected': '-110.27', 'logps_train/chosen': '-145.67', 'loss/train': '0.64621', 'examples_per_second': '29.94', 'grad_norm': '26.875', 'counters/examples': 230880, 'counters/updates': 7215}
train stats after 230912 examples: {'rewards_train/chosen': '0.18595', 'rewards_train/rejected': '0.097446', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.088507', 'logps_train/rejected': '-126.05', 'logps_train/chosen': '-135.04', 'loss/train': '0.66128', 'examples_per_second': '32.306', 'grad_norm': '34.5', 'counters/examples': 230912, 'counters/updates': 7216}
train stats after 230944 examples: {'rewards_train/chosen': '0.17051', 'rewards_train/rejected': '0.12016', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05035', 'logps_train/rejected': '-172.4', 'logps_train/chosen': '-162.1', 'loss/train': '0.68137', 'examples_per_second': '32.246', 'grad_norm': '40.75', 'counters/examples': 230944, 'counters/updates': 7217}
train stats after 230976 examples: {'rewards_train/chosen': '0.099304', 'rewards_train/rejected': '0.065564', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.03374', 'logps_train/rejected': '-127.52', 'logps_train/chosen': '-164.73', 'loss/train': '0.69325', 'examples_per_second': '31.481', 'grad_norm': '39.75', 'counters/examples': 230976, 'counters/updates': 7218}
train stats after 231008 examples: {'rewards_train/chosen': '0.19938', 'rewards_train/rejected': '0.10612', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09326', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-138.67', 'loss/train': '0.65737', 'examples_per_second': '31.514', 'grad_norm': '38.25', 'counters/examples': 231008, 'counters/updates': 7219}
train stats after 231040 examples: {'rewards_train/chosen': '0.12098', 'rewards_train/rejected': '0.032795', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088182', 'logps_train/rejected': '-137.06', 'logps_train/chosen': '-127.43', 'loss/train': '0.66231', 'examples_per_second': '31.379', 'grad_norm': '31.125', 'counters/examples': 231040, 'counters/updates': 7220}
skipping logging after 231072 examples to avoid logging too frequently
train stats after 231104 examples: {'rewards_train/chosen': '0.10934', 'rewards_train/rejected': '0.094334', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.015006', 'logps_train/rejected': '-118.45', 'logps_train/chosen': '-129.15', 'loss/train': '0.69652', 'examples_per_second': '31.728', 'grad_norm': '41', 'counters/examples': 231104, 'counters/updates': 7222}
skipping logging after 231136 examples to avoid logging too frequently
train stats after 231168 examples: {'rewards_train/chosen': '0.18341', 'rewards_train/rejected': '0.062268', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12114', 'logps_train/rejected': '-107.99', 'logps_train/chosen': '-152.59', 'loss/train': '0.65289', 'examples_per_second': '29.257', 'grad_norm': '30', 'counters/examples': 231168, 'counters/updates': 7224}
skipping logging after 231200 examples to avoid logging too frequently
train stats after 231232 examples: {'rewards_train/chosen': '0.15115', 'rewards_train/rejected': '0.021816', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12934', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-141.65', 'loss/train': '0.64701', 'examples_per_second': '30.138', 'grad_norm': '38.5', 'counters/examples': 231232, 'counters/updates': 7226}
train stats after 231264 examples: {'rewards_train/chosen': '0.21888', 'rewards_train/rejected': '0.046789', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1721', 'logps_train/rejected': '-109.4', 'logps_train/chosen': '-173.36', 'loss/train': '0.62343', 'examples_per_second': '31.53', 'grad_norm': '50', 'counters/examples': 231264, 'counters/updates': 7227}
train stats after 231296 examples: {'rewards_train/chosen': '0.13153', 'rewards_train/rejected': '-0.043644', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17517', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-156.61', 'loss/train': '0.62048', 'examples_per_second': '31.422', 'grad_norm': '42.25', 'counters/examples': 231296, 'counters/updates': 7228}
skipping logging after 231328 examples to avoid logging too frequently
train stats after 231360 examples: {'rewards_train/chosen': '0.1877', 'rewards_train/rejected': '0.12478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062912', 'logps_train/rejected': '-112.09', 'logps_train/chosen': '-143.85', 'loss/train': '0.66811', 'examples_per_second': '31.214', 'grad_norm': '38.5', 'counters/examples': 231360, 'counters/updates': 7230}
train stats after 231392 examples: {'rewards_train/chosen': '0.077466', 'rewards_train/rejected': '0.043956', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03351', 'logps_train/rejected': '-117.63', 'logps_train/chosen': '-128.77', 'loss/train': '0.68957', 'examples_per_second': '31.576', 'grad_norm': '27.75', 'counters/examples': 231392, 'counters/updates': 7231}
train stats after 231424 examples: {'rewards_train/chosen': '0.14278', 'rewards_train/rejected': '0.02379', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11899', 'logps_train/rejected': '-146.3', 'logps_train/chosen': '-120.03', 'loss/train': '0.64641', 'examples_per_second': '30.411', 'grad_norm': '38', 'counters/examples': 231424, 'counters/updates': 7232}
skipping logging after 231456 examples to avoid logging too frequently
train stats after 231488 examples: {'rewards_train/chosen': '0.20685', 'rewards_train/rejected': '0.1126', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.094252', 'logps_train/rejected': '-118.42', 'logps_train/chosen': '-156.73', 'loss/train': '0.66367', 'examples_per_second': '33.413', 'grad_norm': '38.25', 'counters/examples': 231488, 'counters/updates': 7234}
train stats after 231520 examples: {'rewards_train/chosen': '0.17458', 'rewards_train/rejected': '0.052549', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12203', 'logps_train/rejected': '-126.66', 'logps_train/chosen': '-143.99', 'loss/train': '0.646', 'examples_per_second': '31.403', 'grad_norm': '33', 'counters/examples': 231520, 'counters/updates': 7235}
skipping logging after 231552 examples to avoid logging too frequently
train stats after 231584 examples: {'rewards_train/chosen': '0.18412', 'rewards_train/rejected': '0.099347', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.08477', 'logps_train/rejected': '-154.34', 'logps_train/chosen': '-155.21', 'loss/train': '0.6615', 'examples_per_second': '30.597', 'grad_norm': '31.25', 'counters/examples': 231584, 'counters/updates': 7237}
train stats after 231616 examples: {'rewards_train/chosen': '0.27179', 'rewards_train/rejected': '-0.01319', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28498', 'logps_train/rejected': '-101.08', 'logps_train/chosen': '-134.25', 'loss/train': '0.58345', 'examples_per_second': '32.537', 'grad_norm': '21.375', 'counters/examples': 231616, 'counters/updates': 7238}
train stats after 231648 examples: {'rewards_train/chosen': '0.11998', 'rewards_train/rejected': '0.090181', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.029797', 'logps_train/rejected': '-121.12', 'logps_train/chosen': '-164.93', 'loss/train': '0.68933', 'examples_per_second': '32.45', 'grad_norm': '36', 'counters/examples': 231648, 'counters/updates': 7239}
train stats after 231680 examples: {'rewards_train/chosen': '0.28151', 'rewards_train/rejected': '0.078721', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20279', 'logps_train/rejected': '-122.29', 'logps_train/chosen': '-152.88', 'loss/train': '0.61654', 'examples_per_second': '29.714', 'grad_norm': '46', 'counters/examples': 231680, 'counters/updates': 7240}
skipping logging after 231712 examples to avoid logging too frequently
train stats after 231744 examples: {'rewards_train/chosen': '0.21704', 'rewards_train/rejected': '0.0065799', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21046', 'logps_train/rejected': '-99.643', 'logps_train/chosen': '-154.94', 'loss/train': '0.60861', 'examples_per_second': '31.469', 'grad_norm': '26', 'counters/examples': 231744, 'counters/updates': 7242}
train stats after 231776 examples: {'rewards_train/chosen': '0.10677', 'rewards_train/rejected': '0.0043504', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10242', 'logps_train/rejected': '-131.68', 'logps_train/chosen': '-131.01', 'loss/train': '0.65464', 'examples_per_second': '31.521', 'grad_norm': '30.625', 'counters/examples': 231776, 'counters/updates': 7243}
train stats after 231808 examples: {'rewards_train/chosen': '0.16082', 'rewards_train/rejected': '0.0052254', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1556', 'logps_train/rejected': '-128.25', 'logps_train/chosen': '-128.46', 'loss/train': '0.63458', 'examples_per_second': '29.976', 'grad_norm': '28.375', 'counters/examples': 231808, 'counters/updates': 7244}
train stats after 231840 examples: {'rewards_train/chosen': '0.014719', 'rewards_train/rejected': '0.076213', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.061494', 'logps_train/rejected': '-160.41', 'logps_train/chosen': '-154.13', 'loss/train': '0.73923', 'examples_per_second': '31.627', 'grad_norm': '36.75', 'counters/examples': 231840, 'counters/updates': 7245}
train stats after 231872 examples: {'rewards_train/chosen': '0.097784', 'rewards_train/rejected': '0.064336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033448', 'logps_train/rejected': '-132.32', 'logps_train/chosen': '-160.9', 'loss/train': '0.6915', 'examples_per_second': '30.132', 'grad_norm': '62', 'counters/examples': 231872, 'counters/updates': 7246}
skipping logging after 231904 examples to avoid logging too frequently
train stats after 231936 examples: {'rewards_train/chosen': '0.1987', 'rewards_train/rejected': '0.04136', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15735', 'logps_train/rejected': '-88.8', 'logps_train/chosen': '-143.8', 'loss/train': '0.62749', 'examples_per_second': '34.23', 'grad_norm': '33', 'counters/examples': 231936, 'counters/updates': 7248}
train stats after 231968 examples: {'rewards_train/chosen': '0.3041', 'rewards_train/rejected': '0.05968', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24442', 'logps_train/rejected': '-126.21', 'logps_train/chosen': '-148.31', 'loss/train': '0.62072', 'examples_per_second': '30.643', 'grad_norm': '25.5', 'counters/examples': 231968, 'counters/updates': 7249}
train stats after 232000 examples: {'rewards_train/chosen': '0.17205', 'rewards_train/rejected': '0.12237', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049684', 'logps_train/rejected': '-160.87', 'logps_train/chosen': '-125.75', 'loss/train': '0.68204', 'examples_per_second': '31.445', 'grad_norm': '29.75', 'counters/examples': 232000, 'counters/updates': 7250}
skipping logging after 232032 examples to avoid logging too frequently
train stats after 232064 examples: {'rewards_train/chosen': '0.16013', 'rewards_train/rejected': '0.002834', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1573', 'logps_train/rejected': '-106.15', 'logps_train/chosen': '-134.2', 'loss/train': '0.63401', 'examples_per_second': '31.839', 'grad_norm': '29.625', 'counters/examples': 232064, 'counters/updates': 7252}
train stats after 232096 examples: {'rewards_train/chosen': '0.10673', 'rewards_train/rejected': '0.031227', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075498', 'logps_train/rejected': '-115.61', 'logps_train/chosen': '-158.99', 'loss/train': '0.67079', 'examples_per_second': '32.712', 'grad_norm': '46.75', 'counters/examples': 232096, 'counters/updates': 7253}
train stats after 232128 examples: {'rewards_train/chosen': '0.24068', 'rewards_train/rejected': '0.019198', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22148', 'logps_train/rejected': '-95.602', 'logps_train/chosen': '-136.92', 'loss/train': '0.62371', 'examples_per_second': '31.557', 'grad_norm': '25.875', 'counters/examples': 232128, 'counters/updates': 7254}
train stats after 232160 examples: {'rewards_train/chosen': '0.22377', 'rewards_train/rejected': '0.12805', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095724', 'logps_train/rejected': '-102.01', 'logps_train/chosen': '-116.13', 'loss/train': '0.66261', 'examples_per_second': '31.138', 'grad_norm': '26.375', 'counters/examples': 232160, 'counters/updates': 7255}
train stats after 232192 examples: {'rewards_train/chosen': '0.15404', 'rewards_train/rejected': '-0.0068675', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.16091', 'logps_train/rejected': '-121.64', 'logps_train/chosen': '-122.9', 'loss/train': '0.63507', 'examples_per_second': '30.435', 'grad_norm': '34.5', 'counters/examples': 232192, 'counters/updates': 7256}
skipping logging after 232224 examples to avoid logging too frequently
train stats after 232256 examples: {'rewards_train/chosen': '0.10802', 'rewards_train/rejected': '0.029597', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.078425', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-137.47', 'loss/train': '0.67352', 'examples_per_second': '31.855', 'grad_norm': '25.75', 'counters/examples': 232256, 'counters/updates': 7258}
train stats after 232288 examples: {'rewards_train/chosen': '0.15807', 'rewards_train/rejected': '0.071489', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086585', 'logps_train/rejected': '-141.28', 'logps_train/chosen': '-124.48', 'loss/train': '0.66171', 'examples_per_second': '31.282', 'grad_norm': '27.375', 'counters/examples': 232288, 'counters/updates': 7259}
train stats after 232320 examples: {'rewards_train/chosen': '0.19381', 'rewards_train/rejected': '0.095592', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098217', 'logps_train/rejected': '-160.98', 'logps_train/chosen': '-165.78', 'loss/train': '0.68799', 'examples_per_second': '30.326', 'grad_norm': '92', 'counters/examples': 232320, 'counters/updates': 7260}
skipping logging after 232352 examples to avoid logging too frequently
train stats after 232384 examples: {'rewards_train/chosen': '0.094416', 'rewards_train/rejected': '0.06543', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.028986', 'logps_train/rejected': '-105.36', 'logps_train/chosen': '-164.06', 'loss/train': '0.68746', 'examples_per_second': '30.278', 'grad_norm': '32.75', 'counters/examples': 232384, 'counters/updates': 7262}
train stats after 232416 examples: {'rewards_train/chosen': '0.073498', 'rewards_train/rejected': '0.04849', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.025008', 'logps_train/rejected': '-122.73', 'logps_train/chosen': '-143.45', 'loss/train': '0.69504', 'examples_per_second': '31.463', 'grad_norm': '33', 'counters/examples': 232416, 'counters/updates': 7263}
train stats after 232448 examples: {'rewards_train/chosen': '0.17874', 'rewards_train/rejected': '-0.0081093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18685', 'logps_train/rejected': '-135.11', 'logps_train/chosen': '-154.06', 'loss/train': '0.62774', 'examples_per_second': '31.987', 'grad_norm': '47.75', 'counters/examples': 232448, 'counters/updates': 7264}
train stats after 232480 examples: {'rewards_train/chosen': '0.2294', 'rewards_train/rejected': '0.12937', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10003', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-148.89', 'loss/train': '0.65326', 'examples_per_second': '31.981', 'grad_norm': '31', 'counters/examples': 232480, 'counters/updates': 7265}
train stats after 232512 examples: {'rewards_train/chosen': '0.21683', 'rewards_train/rejected': '0.11604', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10079', 'logps_train/rejected': '-144.96', 'logps_train/chosen': '-169.13', 'loss/train': '0.66693', 'examples_per_second': '30.07', 'grad_norm': '37.5', 'counters/examples': 232512, 'counters/updates': 7266}
train stats after 232544 examples: {'rewards_train/chosen': '0.13995', 'rewards_train/rejected': '-0.055291', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19524', 'logps_train/rejected': '-146.46', 'logps_train/chosen': '-140.29', 'loss/train': '0.62507', 'examples_per_second': '29.817', 'grad_norm': '36.5', 'counters/examples': 232544, 'counters/updates': 7267}
train stats after 232576 examples: {'rewards_train/chosen': '0.067084', 'rewards_train/rejected': '-0.070798', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13788', 'logps_train/rejected': '-98.328', 'logps_train/chosen': '-132', 'loss/train': '0.63456', 'examples_per_second': '32.074', 'grad_norm': '22.5', 'counters/examples': 232576, 'counters/updates': 7268}
train stats after 232608 examples: {'rewards_train/chosen': '0.11982', 'rewards_train/rejected': '-0.037256', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15708', 'logps_train/rejected': '-120.23', 'logps_train/chosen': '-140.01', 'loss/train': '0.63288', 'examples_per_second': '31.298', 'grad_norm': '33.5', 'counters/examples': 232608, 'counters/updates': 7269}
train stats after 232640 examples: {'rewards_train/chosen': '0.14891', 'rewards_train/rejected': '0.1242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.024705', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-138.26', 'loss/train': '0.68758', 'examples_per_second': '32.695', 'grad_norm': '29.625', 'counters/examples': 232640, 'counters/updates': 7270}
train stats after 232672 examples: {'rewards_train/chosen': '0.13925', 'rewards_train/rejected': '0.0036557', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1356', 'logps_train/rejected': '-136.17', 'logps_train/chosen': '-161.56', 'loss/train': '0.64142', 'examples_per_second': '30.294', 'grad_norm': '44.75', 'counters/examples': 232672, 'counters/updates': 7271}
train stats after 232704 examples: {'rewards_train/chosen': '0.19567', 'rewards_train/rejected': '0.099499', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096174', 'logps_train/rejected': '-130.79', 'logps_train/chosen': '-161.78', 'loss/train': '0.66437', 'examples_per_second': '32.908', 'grad_norm': '38', 'counters/examples': 232704, 'counters/updates': 7272}
train stats after 232736 examples: {'rewards_train/chosen': '0.15958', 'rewards_train/rejected': '0.13197', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.027603', 'logps_train/rejected': '-196.2', 'logps_train/chosen': '-170.7', 'loss/train': '0.70162', 'examples_per_second': '31.323', 'grad_norm': '67', 'counters/examples': 232736, 'counters/updates': 7273}
train stats after 232768 examples: {'rewards_train/chosen': '0.11283', 'rewards_train/rejected': '0.083926', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.028907', 'logps_train/rejected': '-120.1', 'logps_train/chosen': '-128.93', 'loss/train': '0.69411', 'examples_per_second': '32.535', 'grad_norm': '31.25', 'counters/examples': 232768, 'counters/updates': 7274}
train stats after 232800 examples: {'rewards_train/chosen': '0.23623', 'rewards_train/rejected': '0.10379', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13244', 'logps_train/rejected': '-94.232', 'logps_train/chosen': '-116.37', 'loss/train': '0.65765', 'examples_per_second': '31.338', 'grad_norm': '31.625', 'counters/examples': 232800, 'counters/updates': 7275}
train stats after 232832 examples: {'rewards_train/chosen': '0.052484', 'rewards_train/rejected': '0.015524', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036961', 'logps_train/rejected': '-143.94', 'logps_train/chosen': '-183.56', 'loss/train': '0.68684', 'examples_per_second': '30.099', 'grad_norm': '35.5', 'counters/examples': 232832, 'counters/updates': 7276}
skipping logging after 232864 examples to avoid logging too frequently
train stats after 232896 examples: {'rewards_train/chosen': '0.23375', 'rewards_train/rejected': '-0.0060161', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23977', 'logps_train/rejected': '-123.24', 'logps_train/chosen': '-141.11', 'loss/train': '0.60306', 'examples_per_second': '31.448', 'grad_norm': '24.625', 'counters/examples': 232896, 'counters/updates': 7278}
train stats after 232928 examples: {'rewards_train/chosen': '0.18094', 'rewards_train/rejected': '0.08003', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10091', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-144.86', 'loss/train': '0.65985', 'examples_per_second': '31.471', 'grad_norm': '28.875', 'counters/examples': 232928, 'counters/updates': 7279}
train stats after 232960 examples: {'rewards_train/chosen': '0.099643', 'rewards_train/rejected': '0.037396', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062247', 'logps_train/rejected': '-114.8', 'logps_train/chosen': '-132.61', 'loss/train': '0.67561', 'examples_per_second': '32.73', 'grad_norm': '26.5', 'counters/examples': 232960, 'counters/updates': 7280}
train stats after 232992 examples: {'rewards_train/chosen': '0.17254', 'rewards_train/rejected': '0.0029989', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16955', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-141.45', 'loss/train': '0.62413', 'examples_per_second': '31.445', 'grad_norm': '31.375', 'counters/examples': 232992, 'counters/updates': 7281}
train stats after 233024 examples: {'rewards_train/chosen': '0.17794', 'rewards_train/rejected': '-0.071787', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.24973', 'logps_train/rejected': '-119.24', 'logps_train/chosen': '-144.61', 'loss/train': '0.59204', 'examples_per_second': '31.448', 'grad_norm': '28.375', 'counters/examples': 233024, 'counters/updates': 7282}
train stats after 233056 examples: {'rewards_train/chosen': '0.092547', 'rewards_train/rejected': '0.095295', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0027479', 'logps_train/rejected': '-135.72', 'logps_train/chosen': '-153.35', 'loss/train': '0.71481', 'examples_per_second': '31.789', 'grad_norm': '32.25', 'counters/examples': 233056, 'counters/updates': 7283}
train stats after 233088 examples: {'rewards_train/chosen': '0.21052', 'rewards_train/rejected': '0.06465', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14587', 'logps_train/rejected': '-127.14', 'logps_train/chosen': '-144.29', 'loss/train': '0.63974', 'examples_per_second': '31.807', 'grad_norm': '28.625', 'counters/examples': 233088, 'counters/updates': 7284}
train stats after 233120 examples: {'rewards_train/chosen': '0.13705', 'rewards_train/rejected': '0.081399', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.055648', 'logps_train/rejected': '-126.49', 'logps_train/chosen': '-106.13', 'loss/train': '0.67364', 'examples_per_second': '31.745', 'grad_norm': '38.5', 'counters/examples': 233120, 'counters/updates': 7285}
train stats after 233152 examples: {'rewards_train/chosen': '0.13742', 'rewards_train/rejected': '-0.026025', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16344', 'logps_train/rejected': '-105.71', 'logps_train/chosen': '-143.5', 'loss/train': '0.62815', 'examples_per_second': '30.045', 'grad_norm': '29.125', 'counters/examples': 233152, 'counters/updates': 7286}
train stats after 233184 examples: {'rewards_train/chosen': '0.057466', 'rewards_train/rejected': '0.05453', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.0029365', 'logps_train/rejected': '-137.65', 'logps_train/chosen': '-137.45', 'loss/train': '0.70317', 'examples_per_second': '31.647', 'grad_norm': '26', 'counters/examples': 233184, 'counters/updates': 7287}
train stats after 233216 examples: {'rewards_train/chosen': '0.25475', 'rewards_train/rejected': '0.042693', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21206', 'logps_train/rejected': '-134.55', 'logps_train/chosen': '-176.51', 'loss/train': '0.61891', 'examples_per_second': '31.919', 'grad_norm': '27', 'counters/examples': 233216, 'counters/updates': 7288}
train stats after 233248 examples: {'rewards_train/chosen': '0.092482', 'rewards_train/rejected': '0.052684', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.039798', 'logps_train/rejected': '-102.73', 'logps_train/chosen': '-118.09', 'loss/train': '0.67942', 'examples_per_second': '31.452', 'grad_norm': '25.375', 'counters/examples': 233248, 'counters/updates': 7289}
train stats after 233280 examples: {'rewards_train/chosen': '0.21683', 'rewards_train/rejected': '0.11794', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.098889', 'logps_train/rejected': '-116.74', 'logps_train/chosen': '-157.83', 'loss/train': '0.66145', 'examples_per_second': '31.572', 'grad_norm': '36.75', 'counters/examples': 233280, 'counters/updates': 7290}
train stats after 233312 examples: {'rewards_train/chosen': '0.20078', 'rewards_train/rejected': '0.07846', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12232', 'logps_train/rejected': '-122.11', 'logps_train/chosen': '-123.6', 'loss/train': '0.64919', 'examples_per_second': '30.951', 'grad_norm': '29.375', 'counters/examples': 233312, 'counters/updates': 7291}
train stats after 233344 examples: {'rewards_train/chosen': '0.17066', 'rewards_train/rejected': '0.13563', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035031', 'logps_train/rejected': '-162.27', 'logps_train/chosen': '-139.2', 'loss/train': '0.69261', 'examples_per_second': '31.495', 'grad_norm': '60', 'counters/examples': 233344, 'counters/updates': 7292}
train stats after 233376 examples: {'rewards_train/chosen': '0.13347', 'rewards_train/rejected': '0.09924', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.034226', 'logps_train/rejected': '-119.51', 'logps_train/chosen': '-156.22', 'loss/train': '0.68532', 'examples_per_second': '31.444', 'grad_norm': '41.25', 'counters/examples': 233376, 'counters/updates': 7293}
train stats after 233408 examples: {'rewards_train/chosen': '0.27481', 'rewards_train/rejected': '0.076158', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19865', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-151.35', 'loss/train': '0.62396', 'examples_per_second': '31.29', 'grad_norm': '24.875', 'counters/examples': 233408, 'counters/updates': 7294}
train stats after 233440 examples: {'rewards_train/chosen': '0.21323', 'rewards_train/rejected': '0.074643', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13858', 'logps_train/rejected': '-145.72', 'logps_train/chosen': '-158.85', 'loss/train': '0.63997', 'examples_per_second': '30.573', 'grad_norm': '31', 'counters/examples': 233440, 'counters/updates': 7295}
train stats after 233472 examples: {'rewards_train/chosen': '0.1509', 'rewards_train/rejected': '0.054621', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096283', 'logps_train/rejected': '-84.146', 'logps_train/chosen': '-135.44', 'loss/train': '0.65612', 'examples_per_second': '32.355', 'grad_norm': '25.125', 'counters/examples': 233472, 'counters/updates': 7296}
train stats after 233504 examples: {'rewards_train/chosen': '0.25076', 'rewards_train/rejected': '0.09457', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15619', 'logps_train/rejected': '-157.06', 'logps_train/chosen': '-127.81', 'loss/train': '0.67078', 'examples_per_second': '31.474', 'grad_norm': '40.25', 'counters/examples': 233504, 'counters/updates': 7297}
skipping logging after 233536 examples to avoid logging too frequently
train stats after 233568 examples: {'rewards_train/chosen': '0.10475', 'rewards_train/rejected': '0.029617', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075137', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-121.74', 'loss/train': '0.66904', 'examples_per_second': '33.677', 'grad_norm': '27', 'counters/examples': 233568, 'counters/updates': 7299}
train stats after 233600 examples: {'rewards_train/chosen': '0.1971', 'rewards_train/rejected': '0.058607', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13849', 'logps_train/rejected': '-150.57', 'logps_train/chosen': '-142.82', 'loss/train': '0.64605', 'examples_per_second': '32.307', 'grad_norm': '35.5', 'counters/examples': 233600, 'counters/updates': 7300}
train stats after 233632 examples: {'rewards_train/chosen': '0.22043', 'rewards_train/rejected': '-0.010616', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23105', 'logps_train/rejected': '-124.54', 'logps_train/chosen': '-127.34', 'loss/train': '0.59378', 'examples_per_second': '30.205', 'grad_norm': '29', 'counters/examples': 233632, 'counters/updates': 7301}
train stats after 233664 examples: {'rewards_train/chosen': '0.1418', 'rewards_train/rejected': '-0.093141', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.23494', 'logps_train/rejected': '-127.42', 'logps_train/chosen': '-141.86', 'loss/train': '0.59584', 'examples_per_second': '31.674', 'grad_norm': '35.5', 'counters/examples': 233664, 'counters/updates': 7302}
train stats after 233696 examples: {'rewards_train/chosen': '0.13179', 'rewards_train/rejected': '-0.025392', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15718', 'logps_train/rejected': '-137.56', 'logps_train/chosen': '-140.15', 'loss/train': '0.63148', 'examples_per_second': '29.976', 'grad_norm': '29', 'counters/examples': 233696, 'counters/updates': 7303}
train stats after 233728 examples: {'rewards_train/chosen': '0.089174', 'rewards_train/rejected': '0.11689', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.027713', 'logps_train/rejected': '-168.12', 'logps_train/chosen': '-168.94', 'loss/train': '0.71568', 'examples_per_second': '31.459', 'grad_norm': '39.5', 'counters/examples': 233728, 'counters/updates': 7304}
skipping logging after 233760 examples to avoid logging too frequently
train stats after 233792 examples: {'rewards_train/chosen': '0.15333', 'rewards_train/rejected': '0.015739', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13759', 'logps_train/rejected': '-119.62', 'logps_train/chosen': '-157.11', 'loss/train': '0.6388', 'examples_per_second': '31.167', 'grad_norm': '25.625', 'counters/examples': 233792, 'counters/updates': 7306}
train stats after 233824 examples: {'rewards_train/chosen': '0.19255', 'rewards_train/rejected': '0.052478', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14007', 'logps_train/rejected': '-153.26', 'logps_train/chosen': '-160.44', 'loss/train': '0.63584', 'examples_per_second': '30.703', 'grad_norm': '29.375', 'counters/examples': 233824, 'counters/updates': 7307}
train stats after 233856 examples: {'rewards_train/chosen': '0.19418', 'rewards_train/rejected': '0.021068', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17311', 'logps_train/rejected': '-112.76', 'logps_train/chosen': '-144.79', 'loss/train': '0.62308', 'examples_per_second': '30.933', 'grad_norm': '33.25', 'counters/examples': 233856, 'counters/updates': 7308}
train stats after 233888 examples: {'rewards_train/chosen': '0.19559', 'rewards_train/rejected': '0.013852', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18173', 'logps_train/rejected': '-112.86', 'logps_train/chosen': '-141.32', 'loss/train': '0.61827', 'examples_per_second': '32.686', 'grad_norm': '25.125', 'counters/examples': 233888, 'counters/updates': 7309}
train stats after 233920 examples: {'rewards_train/chosen': '0.22566', 'rewards_train/rejected': '0.094944', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13072', 'logps_train/rejected': '-167.88', 'logps_train/chosen': '-189.29', 'loss/train': '0.65641', 'examples_per_second': '32.879', 'grad_norm': '55', 'counters/examples': 233920, 'counters/updates': 7310}
train stats after 233952 examples: {'rewards_train/chosen': '0.30506', 'rewards_train/rejected': '0.11346', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1916', 'logps_train/rejected': '-150.62', 'logps_train/chosen': '-155.65', 'loss/train': '0.62995', 'examples_per_second': '31.335', 'grad_norm': '39.75', 'counters/examples': 233952, 'counters/updates': 7311}
train stats after 233984 examples: {'rewards_train/chosen': '0.1286', 'rewards_train/rejected': '-0.0098383', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13844', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-153.54', 'loss/train': '0.63673', 'examples_per_second': '31.919', 'grad_norm': '37.75', 'counters/examples': 233984, 'counters/updates': 7312}
train stats after 234016 examples: {'rewards_train/chosen': '0.2427', 'rewards_train/rejected': '0.088147', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15455', 'logps_train/rejected': '-141.8', 'logps_train/chosen': '-182.9', 'loss/train': '0.63308', 'examples_per_second': '30.992', 'grad_norm': '40', 'counters/examples': 234016, 'counters/updates': 7313}
train stats after 234048 examples: {'rewards_train/chosen': '0.087562', 'rewards_train/rejected': '-0.024908', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11247', 'logps_train/rejected': '-165.21', 'logps_train/chosen': '-156.82', 'loss/train': '0.66256', 'examples_per_second': '31.61', 'grad_norm': '81.5', 'counters/examples': 234048, 'counters/updates': 7314}
train stats after 234080 examples: {'rewards_train/chosen': '0.26119', 'rewards_train/rejected': '0.0072625', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.25392', 'logps_train/rejected': '-151.7', 'logps_train/chosen': '-180.32', 'loss/train': '0.60826', 'examples_per_second': '30.047', 'grad_norm': '29.625', 'counters/examples': 234080, 'counters/updates': 7315}
train stats after 234112 examples: {'rewards_train/chosen': '0.22455', 'rewards_train/rejected': '0.0072896', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21726', 'logps_train/rejected': '-127.34', 'logps_train/chosen': '-170.46', 'loss/train': '0.60997', 'examples_per_second': '30.439', 'grad_norm': '28.125', 'counters/examples': 234112, 'counters/updates': 7316}
train stats after 234144 examples: {'rewards_train/chosen': '0.16012', 'rewards_train/rejected': '-0.017047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17716', 'logps_train/rejected': '-102.39', 'logps_train/chosen': '-155.65', 'loss/train': '0.62427', 'examples_per_second': '31.128', 'grad_norm': '23.75', 'counters/examples': 234144, 'counters/updates': 7317}
train stats after 234176 examples: {'rewards_train/chosen': '0.19669', 'rewards_train/rejected': '0.046396', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15029', 'logps_train/rejected': '-137.18', 'logps_train/chosen': '-152.69', 'loss/train': '0.63193', 'examples_per_second': '31.891', 'grad_norm': '31', 'counters/examples': 234176, 'counters/updates': 7318}
train stats after 234208 examples: {'rewards_train/chosen': '0.10304', 'rewards_train/rejected': '0.015535', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087505', 'logps_train/rejected': '-124.54', 'logps_train/chosen': '-124.64', 'loss/train': '0.66205', 'examples_per_second': '30.587', 'grad_norm': '27', 'counters/examples': 234208, 'counters/updates': 7319}
skipping logging after 234240 examples to avoid logging too frequently
train stats after 234272 examples: {'rewards_train/chosen': '0.14851', 'rewards_train/rejected': '0.14586', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.0026528', 'logps_train/rejected': '-124.34', 'logps_train/chosen': '-115.19', 'loss/train': '0.70999', 'examples_per_second': '31.451', 'grad_norm': '42.25', 'counters/examples': 234272, 'counters/updates': 7321}
train stats after 234304 examples: {'rewards_train/chosen': '0.050648', 'rewards_train/rejected': '0.090492', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.039844', 'logps_train/rejected': '-109.42', 'logps_train/chosen': '-120.47', 'loss/train': '0.73138', 'examples_per_second': '32.659', 'grad_norm': '70.5', 'counters/examples': 234304, 'counters/updates': 7322}
train stats after 234336 examples: {'rewards_train/chosen': '0.19448', 'rewards_train/rejected': '-0.028465', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22294', 'logps_train/rejected': '-103', 'logps_train/chosen': '-157.57', 'loss/train': '0.60251', 'examples_per_second': '25.077', 'grad_norm': '23.875', 'counters/examples': 234336, 'counters/updates': 7323}
train stats after 234368 examples: {'rewards_train/chosen': '0.15983', 'rewards_train/rejected': '0.13225', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.02758', 'logps_train/rejected': '-115.33', 'logps_train/chosen': '-160.32', 'loss/train': '0.68789', 'examples_per_second': '30.471', 'grad_norm': '30.375', 'counters/examples': 234368, 'counters/updates': 7324}
skipping logging after 234400 examples to avoid logging too frequently
train stats after 234432 examples: {'rewards_train/chosen': '0.12559', 'rewards_train/rejected': '-0.0082538', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13384', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-142.98', 'loss/train': '0.63983', 'examples_per_second': '36.042', 'grad_norm': '37', 'counters/examples': 234432, 'counters/updates': 7326}
train stats after 234464 examples: {'rewards_train/chosen': '0.21769', 'rewards_train/rejected': '0.13219', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085497', 'logps_train/rejected': '-147.52', 'logps_train/chosen': '-134.02', 'loss/train': '0.67414', 'examples_per_second': '31.327', 'grad_norm': '40', 'counters/examples': 234464, 'counters/updates': 7327}
train stats after 234496 examples: {'rewards_train/chosen': '0.20584', 'rewards_train/rejected': '0.071139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1347', 'logps_train/rejected': '-141.83', 'logps_train/chosen': '-171.19', 'loss/train': '0.64548', 'examples_per_second': '31.361', 'grad_norm': '37', 'counters/examples': 234496, 'counters/updates': 7328}
train stats after 234528 examples: {'rewards_train/chosen': '0.188', 'rewards_train/rejected': '0.020657', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16735', 'logps_train/rejected': '-170.04', 'logps_train/chosen': '-166.54', 'loss/train': '0.63095', 'examples_per_second': '30.119', 'grad_norm': '30.875', 'counters/examples': 234528, 'counters/updates': 7329}
train stats after 234560 examples: {'rewards_train/chosen': '0.043394', 'rewards_train/rejected': '0.13117', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.08778', 'logps_train/rejected': '-158.54', 'logps_train/chosen': '-143.71', 'loss/train': '0.75348', 'examples_per_second': '31.486', 'grad_norm': '46.25', 'counters/examples': 234560, 'counters/updates': 7330}
train stats after 234592 examples: {'rewards_train/chosen': '0.19283', 'rewards_train/rejected': '0.13583', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.056996', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-154.7', 'loss/train': '0.68717', 'examples_per_second': '30.155', 'grad_norm': '34.75', 'counters/examples': 234592, 'counters/updates': 7331}
train stats after 234624 examples: {'rewards_train/chosen': '0.12884', 'rewards_train/rejected': '0.0053932', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12345', 'logps_train/rejected': '-143.48', 'logps_train/chosen': '-136.89', 'loss/train': '0.64975', 'examples_per_second': '31.473', 'grad_norm': '29.125', 'counters/examples': 234624, 'counters/updates': 7332}
train stats after 234656 examples: {'rewards_train/chosen': '0.06929', 'rewards_train/rejected': '0.016611', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.05268', 'logps_train/rejected': '-136.67', 'logps_train/chosen': '-157.89', 'loss/train': '0.67725', 'examples_per_second': '30.771', 'grad_norm': '26.625', 'counters/examples': 234656, 'counters/updates': 7333}
train stats after 234688 examples: {'rewards_train/chosen': '0.20308', 'rewards_train/rejected': '0.14089', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062187', 'logps_train/rejected': '-128.66', 'logps_train/chosen': '-182.02', 'loss/train': '0.681', 'examples_per_second': '31.097', 'grad_norm': '32.25', 'counters/examples': 234688, 'counters/updates': 7334}
train stats after 234720 examples: {'rewards_train/chosen': '0.049794', 'rewards_train/rejected': '0.017418', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032376', 'logps_train/rejected': '-115.14', 'logps_train/chosen': '-137.85', 'loss/train': '0.70135', 'examples_per_second': '30.363', 'grad_norm': '42', 'counters/examples': 234720, 'counters/updates': 7335}
skipping logging after 234752 examples to avoid logging too frequently
train stats after 234784 examples: {'rewards_train/chosen': '0.22235', 'rewards_train/rejected': '0.07281', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14954', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-168.28', 'loss/train': '0.63664', 'examples_per_second': '30.971', 'grad_norm': '27.875', 'counters/examples': 234784, 'counters/updates': 7337}
train stats after 234816 examples: {'rewards_train/chosen': '0.079007', 'rewards_train/rejected': '0.080054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.0010469', 'logps_train/rejected': '-130.11', 'logps_train/chosen': '-147.19', 'loss/train': '0.71001', 'examples_per_second': '30.074', 'grad_norm': '38.5', 'counters/examples': 234816, 'counters/updates': 7338}
train stats after 234848 examples: {'rewards_train/chosen': '0.084426', 'rewards_train/rejected': '-0.035647', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12007', 'logps_train/rejected': '-102.65', 'logps_train/chosen': '-120.84', 'loss/train': '0.6486', 'examples_per_second': '31.476', 'grad_norm': '28.5', 'counters/examples': 234848, 'counters/updates': 7339}
train stats after 234880 examples: {'rewards_train/chosen': '0.15245', 'rewards_train/rejected': '0.096836', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055619', 'logps_train/rejected': '-142.22', 'logps_train/chosen': '-157.49', 'loss/train': '0.68676', 'examples_per_second': '30.493', 'grad_norm': '35.5', 'counters/examples': 234880, 'counters/updates': 7340}
train stats after 234912 examples: {'rewards_train/chosen': '0.15347', 'rewards_train/rejected': '-0.081104', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23458', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-153.37', 'loss/train': '0.60933', 'examples_per_second': '31.278', 'grad_norm': '28', 'counters/examples': 234912, 'counters/updates': 7341}
train stats after 234944 examples: {'rewards_train/chosen': '0.21763', 'rewards_train/rejected': '0.056104', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16153', 'logps_train/rejected': '-144.87', 'logps_train/chosen': '-157.23', 'loss/train': '0.62659', 'examples_per_second': '31.459', 'grad_norm': '29.5', 'counters/examples': 234944, 'counters/updates': 7342}
train stats after 234976 examples: {'rewards_train/chosen': '0.21111', 'rewards_train/rejected': '0.084085', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12702', 'logps_train/rejected': '-102.61', 'logps_train/chosen': '-130.53', 'loss/train': '0.65292', 'examples_per_second': '31.433', 'grad_norm': '27.75', 'counters/examples': 234976, 'counters/updates': 7343}
train stats after 235008 examples: {'rewards_train/chosen': '0.15059', 'rewards_train/rejected': '0.059384', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091203', 'logps_train/rejected': '-124.3', 'logps_train/chosen': '-139.45', 'loss/train': '0.66217', 'examples_per_second': '32.289', 'grad_norm': '32.25', 'counters/examples': 235008, 'counters/updates': 7344}
train stats after 235040 examples: {'rewards_train/chosen': '0.12143', 'rewards_train/rejected': '0.097674', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.02376', 'logps_train/rejected': '-124.12', 'logps_train/chosen': '-148.34', 'loss/train': '0.70714', 'examples_per_second': '32.408', 'grad_norm': '44.25', 'counters/examples': 235040, 'counters/updates': 7345}
skipping logging after 235072 examples to avoid logging too frequently
train stats after 235104 examples: {'rewards_train/chosen': '0.12757', 'rewards_train/rejected': '-0.054154', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18172', 'logps_train/rejected': '-111.54', 'logps_train/chosen': '-137.66', 'loss/train': '0.61747', 'examples_per_second': '31.172', 'grad_norm': '22.875', 'counters/examples': 235104, 'counters/updates': 7347}
skipping logging after 235136 examples to avoid logging too frequently
train stats after 235168 examples: {'rewards_train/chosen': '0.28459', 'rewards_train/rejected': '0.15006', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.13454', 'logps_train/rejected': '-129.23', 'logps_train/chosen': '-172.78', 'loss/train': '0.65449', 'examples_per_second': '23.311', 'grad_norm': '33', 'counters/examples': 235168, 'counters/updates': 7349}
train stats after 235200 examples: {'rewards_train/chosen': '0.2402', 'rewards_train/rejected': '0.099772', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14042', 'logps_train/rejected': '-126.87', 'logps_train/chosen': '-139.88', 'loss/train': '0.648', 'examples_per_second': '31.503', 'grad_norm': '33.25', 'counters/examples': 235200, 'counters/updates': 7350}
train stats after 235232 examples: {'rewards_train/chosen': '0.26505', 'rewards_train/rejected': '0.097683', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16737', 'logps_train/rejected': '-151.39', 'logps_train/chosen': '-180.74', 'loss/train': '0.6397', 'examples_per_second': '31.927', 'grad_norm': '38', 'counters/examples': 235232, 'counters/updates': 7351}
train stats after 235264 examples: {'rewards_train/chosen': '0.11338', 'rewards_train/rejected': '0.054089', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059289', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-123.9', 'loss/train': '0.67187', 'examples_per_second': '23.518', 'grad_norm': '29.5', 'counters/examples': 235264, 'counters/updates': 7352}
skipping logging after 235296 examples to avoid logging too frequently
train stats after 235328 examples: {'rewards_train/chosen': '0.070583', 'rewards_train/rejected': '-0.030429', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10101', 'logps_train/rejected': '-118.77', 'logps_train/chosen': '-166.31', 'loss/train': '0.65446', 'examples_per_second': '30.472', 'grad_norm': '38.5', 'counters/examples': 235328, 'counters/updates': 7354}
train stats after 235360 examples: {'rewards_train/chosen': '0.15347', 'rewards_train/rejected': '0.053192', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10028', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-108.54', 'loss/train': '0.65094', 'examples_per_second': '32.675', 'grad_norm': '26.375', 'counters/examples': 235360, 'counters/updates': 7355}
train stats after 235392 examples: {'rewards_train/chosen': '0.11906', 'rewards_train/rejected': '0.009598', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10946', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-107.62', 'loss/train': '0.6552', 'examples_per_second': '31.292', 'grad_norm': '32', 'counters/examples': 235392, 'counters/updates': 7356}
train stats after 235424 examples: {'rewards_train/chosen': '0.13682', 'rewards_train/rejected': '0.041463', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095361', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-147.8', 'loss/train': '0.65664', 'examples_per_second': '32.075', 'grad_norm': '27.75', 'counters/examples': 235424, 'counters/updates': 7357}
train stats after 235456 examples: {'rewards_train/chosen': '0.23228', 'rewards_train/rejected': '0.058219', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17406', 'logps_train/rejected': '-129.91', 'logps_train/chosen': '-149.11', 'loss/train': '0.62371', 'examples_per_second': '31.458', 'grad_norm': '29.625', 'counters/examples': 235456, 'counters/updates': 7358}
train stats after 235488 examples: {'rewards_train/chosen': '0.17412', 'rewards_train/rejected': '0.091893', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.082228', 'logps_train/rejected': '-131.82', 'logps_train/chosen': '-154.85', 'loss/train': '0.66308', 'examples_per_second': '31.181', 'grad_norm': '28.5', 'counters/examples': 235488, 'counters/updates': 7359}
train stats after 235520 examples: {'rewards_train/chosen': '0.15367', 'rewards_train/rejected': '0.097229', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.056437', 'logps_train/rejected': '-138.44', 'logps_train/chosen': '-141.89', 'loss/train': '0.67328', 'examples_per_second': '31.397', 'grad_norm': '27.75', 'counters/examples': 235520, 'counters/updates': 7360}
train stats after 235552 examples: {'rewards_train/chosen': '0.09891', 'rewards_train/rejected': '0.047328', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051583', 'logps_train/rejected': '-93.878', 'logps_train/chosen': '-156.42', 'loss/train': '0.67871', 'examples_per_second': '30.298', 'grad_norm': '37', 'counters/examples': 235552, 'counters/updates': 7361}
skipping logging after 235584 examples to avoid logging too frequently
train stats after 235616 examples: {'rewards_train/chosen': '0.22939', 'rewards_train/rejected': '0.042643', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18675', 'logps_train/rejected': '-145.85', 'logps_train/chosen': '-141.62', 'loss/train': '0.62058', 'examples_per_second': '30.833', 'grad_norm': '27.75', 'counters/examples': 235616, 'counters/updates': 7363}
train stats after 235648 examples: {'rewards_train/chosen': '0.089805', 'rewards_train/rejected': '0.038856', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.050949', 'logps_train/rejected': '-109.59', 'logps_train/chosen': '-117.74', 'loss/train': '0.68107', 'examples_per_second': '31.547', 'grad_norm': '24.375', 'counters/examples': 235648, 'counters/updates': 7364}
skipping logging after 235680 examples to avoid logging too frequently
train stats after 235712 examples: {'rewards_train/chosen': '0.16245', 'rewards_train/rejected': '0.076959', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.08549', 'logps_train/rejected': '-159.35', 'logps_train/chosen': '-172.85', 'loss/train': '0.69359', 'examples_per_second': '30.422', 'grad_norm': '36.75', 'counters/examples': 235712, 'counters/updates': 7366}
train stats after 235744 examples: {'rewards_train/chosen': '0.19233', 'rewards_train/rejected': '0.094207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098128', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-127.12', 'loss/train': '0.65889', 'examples_per_second': '32.415', 'grad_norm': '30', 'counters/examples': 235744, 'counters/updates': 7367}
skipping logging after 235776 examples to avoid logging too frequently
train stats after 235808 examples: {'rewards_train/chosen': '0.16416', 'rewards_train/rejected': '0.010623', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15353', 'logps_train/rejected': '-96.662', 'logps_train/chosen': '-135.91', 'loss/train': '0.63633', 'examples_per_second': '30.75', 'grad_norm': '28.875', 'counters/examples': 235808, 'counters/updates': 7369}
train stats after 235840 examples: {'rewards_train/chosen': '0.081034', 'rewards_train/rejected': '0.074177', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.0068566', 'logps_train/rejected': '-122.65', 'logps_train/chosen': '-163.88', 'loss/train': '0.69923', 'examples_per_second': '32.132', 'grad_norm': '28', 'counters/examples': 235840, 'counters/updates': 7370}
train stats after 235872 examples: {'rewards_train/chosen': '0.22475', 'rewards_train/rejected': '0.02729', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19746', 'logps_train/rejected': '-147.6', 'logps_train/chosen': '-156.33', 'loss/train': '0.61101', 'examples_per_second': '31.84', 'grad_norm': '25.625', 'counters/examples': 235872, 'counters/updates': 7371}
skipping logging after 235904 examples to avoid logging too frequently
train stats after 235936 examples: {'rewards_train/chosen': '0.22157', 'rewards_train/rejected': '0.10668', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11488', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-166.97', 'loss/train': '0.65082', 'examples_per_second': '32.968', 'grad_norm': '40.25', 'counters/examples': 235936, 'counters/updates': 7373}
skipping logging after 235968 examples to avoid logging too frequently
train stats after 236000 examples: {'rewards_train/chosen': '0.084875', 'rewards_train/rejected': '0.032538', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.052337', 'logps_train/rejected': '-91.104', 'logps_train/chosen': '-112.31', 'loss/train': '0.68005', 'examples_per_second': '30.366', 'grad_norm': '27', 'counters/examples': 236000, 'counters/updates': 7375}
skipping logging after 236032 examples to avoid logging too frequently
train stats after 236064 examples: {'rewards_train/chosen': '0.17264', 'rewards_train/rejected': '0.033328', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13931', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-149.44', 'loss/train': '0.64138', 'examples_per_second': '30.979', 'grad_norm': '26.75', 'counters/examples': 236064, 'counters/updates': 7377}
train stats after 236096 examples: {'rewards_train/chosen': '0.31005', 'rewards_train/rejected': '0.028743', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.28131', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-133.35', 'loss/train': '0.62803', 'examples_per_second': '32.304', 'grad_norm': '26.125', 'counters/examples': 236096, 'counters/updates': 7378}
train stats after 236128 examples: {'rewards_train/chosen': '0.17867', 'rewards_train/rejected': '0.046877', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13179', 'logps_train/rejected': '-115.97', 'logps_train/chosen': '-199.17', 'loss/train': '0.63786', 'examples_per_second': '31.504', 'grad_norm': '27.625', 'counters/examples': 236128, 'counters/updates': 7379}
skipping logging after 236160 examples to avoid logging too frequently
train stats after 236192 examples: {'rewards_train/chosen': '0.18243', 'rewards_train/rejected': '0.018452', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.16398', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-162.93', 'loss/train': '0.62205', 'examples_per_second': '31.291', 'grad_norm': '31.5', 'counters/examples': 236192, 'counters/updates': 7381}
skipping logging after 236224 examples to avoid logging too frequently
train stats after 236256 examples: {'rewards_train/chosen': '0.20808', 'rewards_train/rejected': '0.1608', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047273', 'logps_train/rejected': '-149.94', 'logps_train/chosen': '-131.2', 'loss/train': '0.68125', 'examples_per_second': '29.775', 'grad_norm': '39.25', 'counters/examples': 236256, 'counters/updates': 7383}
train stats after 236288 examples: {'rewards_train/chosen': '0.10712', 'rewards_train/rejected': '-0.01552', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12264', 'logps_train/rejected': '-113.61', 'logps_train/chosen': '-130.87', 'loss/train': '0.648', 'examples_per_second': '30.383', 'grad_norm': '38.5', 'counters/examples': 236288, 'counters/updates': 7384}
train stats after 236320 examples: {'rewards_train/chosen': '0.13318', 'rewards_train/rejected': '0.042975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0902', 'logps_train/rejected': '-112.85', 'logps_train/chosen': '-146.35', 'loss/train': '0.65705', 'examples_per_second': '30.474', 'grad_norm': '26.375', 'counters/examples': 236320, 'counters/updates': 7385}
train stats after 236352 examples: {'rewards_train/chosen': '0.054216', 'rewards_train/rejected': '0.08414', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.029923', 'logps_train/rejected': '-127.39', 'logps_train/chosen': '-124.85', 'loss/train': '0.71981', 'examples_per_second': '32.577', 'grad_norm': '31.5', 'counters/examples': 236352, 'counters/updates': 7386}
train stats after 236384 examples: {'rewards_train/chosen': '0.1101', 'rewards_train/rejected': '-0.023076', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13317', 'logps_train/rejected': '-152.84', 'logps_train/chosen': '-148.68', 'loss/train': '0.64017', 'examples_per_second': '31.473', 'grad_norm': '37.75', 'counters/examples': 236384, 'counters/updates': 7387}
train stats after 236416 examples: {'rewards_train/chosen': '0.20165', 'rewards_train/rejected': '0.099234', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10242', 'logps_train/rejected': '-142.68', 'logps_train/chosen': '-127.63', 'loss/train': '0.65714', 'examples_per_second': '31.657', 'grad_norm': '30.5', 'counters/examples': 236416, 'counters/updates': 7388}
train stats after 236448 examples: {'rewards_train/chosen': '0.22438', 'rewards_train/rejected': '0.14365', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080735', 'logps_train/rejected': '-146.74', 'logps_train/chosen': '-163.9', 'loss/train': '0.67034', 'examples_per_second': '31.448', 'grad_norm': '35.25', 'counters/examples': 236448, 'counters/updates': 7389}
train stats after 236480 examples: {'rewards_train/chosen': '0.20726', 'rewards_train/rejected': '0.040328', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16694', 'logps_train/rejected': '-151.83', 'logps_train/chosen': '-159.82', 'loss/train': '0.62941', 'examples_per_second': '31.309', 'grad_norm': '29.625', 'counters/examples': 236480, 'counters/updates': 7390}
train stats after 236512 examples: {'rewards_train/chosen': '0.15795', 'rewards_train/rejected': '0.088072', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.069879', 'logps_train/rejected': '-97.732', 'logps_train/chosen': '-157.26', 'loss/train': '0.67949', 'examples_per_second': '29.965', 'grad_norm': '34', 'counters/examples': 236512, 'counters/updates': 7391}
train stats after 236544 examples: {'rewards_train/chosen': '0.15866', 'rewards_train/rejected': '0.11873', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.039926', 'logps_train/rejected': '-153.84', 'logps_train/chosen': '-168.56', 'loss/train': '0.70043', 'examples_per_second': '30.702', 'grad_norm': '45.5', 'counters/examples': 236544, 'counters/updates': 7392}
train stats after 236576 examples: {'rewards_train/chosen': '0.10251', 'rewards_train/rejected': '0.046051', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.056454', 'logps_train/rejected': '-114.42', 'logps_train/chosen': '-162.84', 'loss/train': '0.68809', 'examples_per_second': '30.319', 'grad_norm': '30.875', 'counters/examples': 236576, 'counters/updates': 7393}
train stats after 236608 examples: {'rewards_train/chosen': '0.2891', 'rewards_train/rejected': '0.053438', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23567', 'logps_train/rejected': '-155.98', 'logps_train/chosen': '-157.36', 'loss/train': '0.59715', 'examples_per_second': '31.477', 'grad_norm': '28.875', 'counters/examples': 236608, 'counters/updates': 7394}
train stats after 236640 examples: {'rewards_train/chosen': '0.18406', 'rewards_train/rejected': '-0.0056806', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18974', 'logps_train/rejected': '-134.17', 'logps_train/chosen': '-134.6', 'loss/train': '0.6337', 'examples_per_second': '32.276', 'grad_norm': '31', 'counters/examples': 236640, 'counters/updates': 7395}
skipping logging after 236672 examples to avoid logging too frequently
train stats after 236704 examples: {'rewards_train/chosen': '0.086118', 'rewards_train/rejected': '0.039632', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046486', 'logps_train/rejected': '-120.21', 'logps_train/chosen': '-126.78', 'loss/train': '0.68164', 'examples_per_second': '31.448', 'grad_norm': '31', 'counters/examples': 236704, 'counters/updates': 7397}
skipping logging after 236736 examples to avoid logging too frequently
train stats after 236768 examples: {'rewards_train/chosen': '0.18384', 'rewards_train/rejected': '0.036146', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14769', 'logps_train/rejected': '-107.33', 'logps_train/chosen': '-148.12', 'loss/train': '0.63193', 'examples_per_second': '31.436', 'grad_norm': '30', 'counters/examples': 236768, 'counters/updates': 7399}
skipping logging after 236800 examples to avoid logging too frequently
train stats after 236832 examples: {'rewards_train/chosen': '0.13487', 'rewards_train/rejected': '0.083593', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051274', 'logps_train/rejected': '-122.41', 'logps_train/chosen': '-141.42', 'loss/train': '0.67288', 'examples_per_second': '31.578', 'grad_norm': '33.75', 'counters/examples': 236832, 'counters/updates': 7401}
train stats after 236864 examples: {'rewards_train/chosen': '0.17618', 'rewards_train/rejected': '0.074535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10164', 'logps_train/rejected': '-87.515', 'logps_train/chosen': '-151.68', 'loss/train': '0.66125', 'examples_per_second': '32.339', 'grad_norm': '28.125', 'counters/examples': 236864, 'counters/updates': 7402}
skipping logging after 236896 examples to avoid logging too frequently
train stats after 236928 examples: {'rewards_train/chosen': '0.11379', 'rewards_train/rejected': '0.073421', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.040373', 'logps_train/rejected': '-128.09', 'logps_train/chosen': '-138.26', 'loss/train': '0.68677', 'examples_per_second': '29.964', 'grad_norm': '34.75', 'counters/examples': 236928, 'counters/updates': 7404}
train stats after 236960 examples: {'rewards_train/chosen': '0.10007', 'rewards_train/rejected': '-0.027873', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12794', 'logps_train/rejected': '-114.02', 'logps_train/chosen': '-141.93', 'loss/train': '0.65159', 'examples_per_second': '30.361', 'grad_norm': '36.25', 'counters/examples': 236960, 'counters/updates': 7405}
train stats after 236992 examples: {'rewards_train/chosen': '0.10412', 'rewards_train/rejected': '0.085923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018201', 'logps_train/rejected': '-128.82', 'logps_train/chosen': '-140.84', 'loss/train': '0.69206', 'examples_per_second': '32.704', 'grad_norm': '36.75', 'counters/examples': 236992, 'counters/updates': 7406}
train stats after 237024 examples: {'rewards_train/chosen': '0.076221', 'rewards_train/rejected': '0.021177', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.055044', 'logps_train/rejected': '-136.92', 'logps_train/chosen': '-182.98', 'loss/train': '0.67906', 'examples_per_second': '31.37', 'grad_norm': '34', 'counters/examples': 237024, 'counters/updates': 7407}
train stats after 237056 examples: {'rewards_train/chosen': '0.23628', 'rewards_train/rejected': '0.028172', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2081', 'logps_train/rejected': '-143.89', 'logps_train/chosen': '-150.58', 'loss/train': '0.62119', 'examples_per_second': '32.033', 'grad_norm': '27.625', 'counters/examples': 237056, 'counters/updates': 7408}
train stats after 237088 examples: {'rewards_train/chosen': '0.12644', 'rewards_train/rejected': '0.0082014', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11823', 'logps_train/rejected': '-114.48', 'logps_train/chosen': '-146.27', 'loss/train': '0.64848', 'examples_per_second': '32.439', 'grad_norm': '23.75', 'counters/examples': 237088, 'counters/updates': 7409}
train stats after 237120 examples: {'rewards_train/chosen': '0.11802', 'rewards_train/rejected': '0.1165', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0015146', 'logps_train/rejected': '-119.61', 'logps_train/chosen': '-154.15', 'loss/train': '0.70186', 'examples_per_second': '33.125', 'grad_norm': '38.25', 'counters/examples': 237120, 'counters/updates': 7410}
train stats after 237152 examples: {'rewards_train/chosen': '0.12454', 'rewards_train/rejected': '0.017673', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10686', 'logps_train/rejected': '-126.1', 'logps_train/chosen': '-124.55', 'loss/train': '0.65029', 'examples_per_second': '31.239', 'grad_norm': '36', 'counters/examples': 237152, 'counters/updates': 7411}
train stats after 237184 examples: {'rewards_train/chosen': '0.19047', 'rewards_train/rejected': '0.060503', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12997', 'logps_train/rejected': '-134.66', 'logps_train/chosen': '-134.72', 'loss/train': '0.64404', 'examples_per_second': '30.426', 'grad_norm': '26.625', 'counters/examples': 237184, 'counters/updates': 7412}
train stats after 237216 examples: {'rewards_train/chosen': '0.13385', 'rewards_train/rejected': '-0.030747', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1646', 'logps_train/rejected': '-119.6', 'logps_train/chosen': '-124.91', 'loss/train': '0.62975', 'examples_per_second': '31.43', 'grad_norm': '26.125', 'counters/examples': 237216, 'counters/updates': 7413}
train stats after 237248 examples: {'rewards_train/chosen': '0.12372', 'rewards_train/rejected': '0.021999', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10172', 'logps_train/rejected': '-153.36', 'logps_train/chosen': '-146.98', 'loss/train': '0.6543', 'examples_per_second': '33.329', 'grad_norm': '39', 'counters/examples': 237248, 'counters/updates': 7414}
train stats after 237280 examples: {'rewards_train/chosen': '0.11581', 'rewards_train/rejected': '0.037857', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077953', 'logps_train/rejected': '-94.833', 'logps_train/chosen': '-83.744', 'loss/train': '0.66944', 'examples_per_second': '30.555', 'grad_norm': '26', 'counters/examples': 237280, 'counters/updates': 7415}
train stats after 237312 examples: {'rewards_train/chosen': '0.13814', 'rewards_train/rejected': '-0.0068314', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14497', 'logps_train/rejected': '-134.72', 'logps_train/chosen': '-162.35', 'loss/train': '0.63831', 'examples_per_second': '31.912', 'grad_norm': '32', 'counters/examples': 237312, 'counters/updates': 7416}
train stats after 237344 examples: {'rewards_train/chosen': '0.1994', 'rewards_train/rejected': '0.096167', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10323', 'logps_train/rejected': '-98.221', 'logps_train/chosen': '-141.07', 'loss/train': '0.65474', 'examples_per_second': '31.384', 'grad_norm': '29.5', 'counters/examples': 237344, 'counters/updates': 7417}
skipping logging after 237376 examples to avoid logging too frequently
train stats after 237408 examples: {'rewards_train/chosen': '0.19631', 'rewards_train/rejected': '0.099646', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.096663', 'logps_train/rejected': '-110.17', 'logps_train/chosen': '-138.7', 'loss/train': '0.66258', 'examples_per_second': '29.891', 'grad_norm': '33.75', 'counters/examples': 237408, 'counters/updates': 7419}
train stats after 237440 examples: {'rewards_train/chosen': '0.16874', 'rewards_train/rejected': '0.046288', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12245', 'logps_train/rejected': '-115.48', 'logps_train/chosen': '-152.69', 'loss/train': '0.65013', 'examples_per_second': '31.469', 'grad_norm': '34', 'counters/examples': 237440, 'counters/updates': 7420}
train stats after 237472 examples: {'rewards_train/chosen': '0.15201', 'rewards_train/rejected': '0.0092295', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14279', 'logps_train/rejected': '-136.22', 'logps_train/chosen': '-176.17', 'loss/train': '0.63513', 'examples_per_second': '30.48', 'grad_norm': '30.375', 'counters/examples': 237472, 'counters/updates': 7421}
train stats after 237504 examples: {'rewards_train/chosen': '0.077839', 'rewards_train/rejected': '0.05961', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018229', 'logps_train/rejected': '-98.689', 'logps_train/chosen': '-113.38', 'loss/train': '0.69637', 'examples_per_second': '32.393', 'grad_norm': '37.5', 'counters/examples': 237504, 'counters/updates': 7422}
train stats after 237536 examples: {'rewards_train/chosen': '0.16321', 'rewards_train/rejected': '0.043589', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11962', 'logps_train/rejected': '-106.07', 'logps_train/chosen': '-147.93', 'loss/train': '0.64511', 'examples_per_second': '31.132', 'grad_norm': '27.25', 'counters/examples': 237536, 'counters/updates': 7423}
train stats after 237568 examples: {'rewards_train/chosen': '0.18635', 'rewards_train/rejected': '0.075399', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11096', 'logps_train/rejected': '-110.76', 'logps_train/chosen': '-125.61', 'loss/train': '0.65537', 'examples_per_second': '30.943', 'grad_norm': '28.875', 'counters/examples': 237568, 'counters/updates': 7424}
train stats after 237600 examples: {'rewards_train/chosen': '0.14227', 'rewards_train/rejected': '0.0032231', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13905', 'logps_train/rejected': '-111.21', 'logps_train/chosen': '-144.89', 'loss/train': '0.64815', 'examples_per_second': '29.916', 'grad_norm': '33.5', 'counters/examples': 237600, 'counters/updates': 7425}
train stats after 237632 examples: {'rewards_train/chosen': '0.14586', 'rewards_train/rejected': '0.056671', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089188', 'logps_train/rejected': '-116.15', 'logps_train/chosen': '-156.72', 'loss/train': '0.65757', 'examples_per_second': '30.107', 'grad_norm': '26.375', 'counters/examples': 237632, 'counters/updates': 7426}
skipping logging after 237664 examples to avoid logging too frequently
train stats after 237696 examples: {'rewards_train/chosen': '0.16949', 'rewards_train/rejected': '0.099792', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069696', 'logps_train/rejected': '-125.5', 'logps_train/chosen': '-114.66', 'loss/train': '0.67328', 'examples_per_second': '31.328', 'grad_norm': '28.375', 'counters/examples': 237696, 'counters/updates': 7428}
train stats after 237728 examples: {'rewards_train/chosen': '0.25165', 'rewards_train/rejected': '0.029292', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22236', 'logps_train/rejected': '-120.49', 'logps_train/chosen': '-105.9', 'loss/train': '0.61149', 'examples_per_second': '31.536', 'grad_norm': '28.75', 'counters/examples': 237728, 'counters/updates': 7429}
skipping logging after 237760 examples to avoid logging too frequently
train stats after 237792 examples: {'rewards_train/chosen': '0.17135', 'rewards_train/rejected': '-0.001181', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17254', 'logps_train/rejected': '-147.45', 'logps_train/chosen': '-147.24', 'loss/train': '0.63613', 'examples_per_second': '31.204', 'grad_norm': '35.75', 'counters/examples': 237792, 'counters/updates': 7431}
train stats after 237824 examples: {'rewards_train/chosen': '0.25193', 'rewards_train/rejected': '0.13915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11277', 'logps_train/rejected': '-132.01', 'logps_train/chosen': '-144.15', 'loss/train': '0.64699', 'examples_per_second': '31.185', 'grad_norm': '31.125', 'counters/examples': 237824, 'counters/updates': 7432}
train stats after 237856 examples: {'rewards_train/chosen': '0.17352', 'rewards_train/rejected': '0.027211', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1463', 'logps_train/rejected': '-147.74', 'logps_train/chosen': '-156.58', 'loss/train': '0.6408', 'examples_per_second': '31.541', 'grad_norm': '30.25', 'counters/examples': 237856, 'counters/updates': 7433}
train stats after 237888 examples: {'rewards_train/chosen': '0.096835', 'rewards_train/rejected': '0.079761', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.017074', 'logps_train/rejected': '-142.83', 'logps_train/chosen': '-162.99', 'loss/train': '0.70733', 'examples_per_second': '31.263', 'grad_norm': '40.25', 'counters/examples': 237888, 'counters/updates': 7434}
train stats after 237920 examples: {'rewards_train/chosen': '0.223', 'rewards_train/rejected': '0.073195', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1498', 'logps_train/rejected': '-121.49', 'logps_train/chosen': '-163.67', 'loss/train': '0.63862', 'examples_per_second': '31.489', 'grad_norm': '27.375', 'counters/examples': 237920, 'counters/updates': 7435}
skipping logging after 237952 examples to avoid logging too frequently
train stats after 237984 examples: {'rewards_train/chosen': '0.15365', 'rewards_train/rejected': '0.14142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.012227', 'logps_train/rejected': '-157.59', 'logps_train/chosen': '-128.72', 'loss/train': '0.71129', 'examples_per_second': '33.211', 'grad_norm': '53', 'counters/examples': 237984, 'counters/updates': 7437}
skipping logging after 238016 examples to avoid logging too frequently
train stats after 238048 examples: {'rewards_train/chosen': '0.21049', 'rewards_train/rejected': '0.053242', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15725', 'logps_train/rejected': '-100.52', 'logps_train/chosen': '-104.81', 'loss/train': '0.63203', 'examples_per_second': '33.873', 'grad_norm': '22.25', 'counters/examples': 238048, 'counters/updates': 7439}
train stats after 238080 examples: {'rewards_train/chosen': '0.13259', 'rewards_train/rejected': '0.068751', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063843', 'logps_train/rejected': '-101.17', 'logps_train/chosen': '-150.27', 'loss/train': '0.68425', 'examples_per_second': '33.233', 'grad_norm': '83.5', 'counters/examples': 238080, 'counters/updates': 7440}
train stats after 238112 examples: {'rewards_train/chosen': '0.10234', 'rewards_train/rejected': '0.056609', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.045729', 'logps_train/rejected': '-131.88', 'logps_train/chosen': '-142.22', 'loss/train': '0.67903', 'examples_per_second': '32.391', 'grad_norm': '27.375', 'counters/examples': 238112, 'counters/updates': 7441}
train stats after 238144 examples: {'rewards_train/chosen': '0.12967', 'rewards_train/rejected': '-0.00080465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13048', 'logps_train/rejected': '-137.88', 'logps_train/chosen': '-147.17', 'loss/train': '0.64907', 'examples_per_second': '30.064', 'grad_norm': '36.25', 'counters/examples': 238144, 'counters/updates': 7442}
train stats after 238176 examples: {'rewards_train/chosen': '0.17407', 'rewards_train/rejected': '0.08203', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092039', 'logps_train/rejected': '-111', 'logps_train/chosen': '-156.21', 'loss/train': '0.65659', 'examples_per_second': '31.539', 'grad_norm': '27.75', 'counters/examples': 238176, 'counters/updates': 7443}
train stats after 238208 examples: {'rewards_train/chosen': '0.16923', 'rewards_train/rejected': '0.068287', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10094', 'logps_train/rejected': '-124.99', 'logps_train/chosen': '-127.31', 'loss/train': '0.65848', 'examples_per_second': '30.71', 'grad_norm': '28', 'counters/examples': 238208, 'counters/updates': 7444}
train stats after 238240 examples: {'rewards_train/chosen': '0.19495', 'rewards_train/rejected': '0.0072485', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18771', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-187.62', 'loss/train': '0.61543', 'examples_per_second': '30.326', 'grad_norm': '32.25', 'counters/examples': 238240, 'counters/updates': 7445}
train stats after 238272 examples: {'rewards_train/chosen': '0.089058', 'rewards_train/rejected': '0.021695', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067362', 'logps_train/rejected': '-103.44', 'logps_train/chosen': '-155.77', 'loss/train': '0.67497', 'examples_per_second': '30.843', 'grad_norm': '26.125', 'counters/examples': 238272, 'counters/updates': 7446}
train stats after 238304 examples: {'rewards_train/chosen': '0.15258', 'rewards_train/rejected': '0.016677', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1359', 'logps_train/rejected': '-127.17', 'logps_train/chosen': '-150.52', 'loss/train': '0.63825', 'examples_per_second': '31.526', 'grad_norm': '26.5', 'counters/examples': 238304, 'counters/updates': 7447}
train stats after 238336 examples: {'rewards_train/chosen': '0.19636', 'rewards_train/rejected': '0.065542', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13082', 'logps_train/rejected': '-130.61', 'logps_train/chosen': '-172.78', 'loss/train': '0.64882', 'examples_per_second': '31.993', 'grad_norm': '30', 'counters/examples': 238336, 'counters/updates': 7448}
train stats after 238368 examples: {'rewards_train/chosen': '0.18286', 'rewards_train/rejected': '0.032339', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15052', 'logps_train/rejected': '-154.58', 'logps_train/chosen': '-165.94', 'loss/train': '0.63223', 'examples_per_second': '32.036', 'grad_norm': '32', 'counters/examples': 238368, 'counters/updates': 7449}
train stats after 238400 examples: {'rewards_train/chosen': '0.14494', 'rewards_train/rejected': '0.082655', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062282', 'logps_train/rejected': '-139.73', 'logps_train/chosen': '-107.06', 'loss/train': '0.67295', 'examples_per_second': '32.041', 'grad_norm': '24.375', 'counters/examples': 238400, 'counters/updates': 7450}
train stats after 238432 examples: {'rewards_train/chosen': '0.21991', 'rewards_train/rejected': '0.060045', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15986', 'logps_train/rejected': '-134.74', 'logps_train/chosen': '-160.18', 'loss/train': '0.62632', 'examples_per_second': '30.597', 'grad_norm': '30.25', 'counters/examples': 238432, 'counters/updates': 7451}
train stats after 238464 examples: {'rewards_train/chosen': '0.16557', 'rewards_train/rejected': '0.088563', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.077007', 'logps_train/rejected': '-121.1', 'logps_train/chosen': '-125.1', 'loss/train': '0.67099', 'examples_per_second': '32.581', 'grad_norm': '37.75', 'counters/examples': 238464, 'counters/updates': 7452}
train stats after 238496 examples: {'rewards_train/chosen': '0.15398', 'rewards_train/rejected': '0.059521', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094456', 'logps_train/rejected': '-153.93', 'logps_train/chosen': '-126.38', 'loss/train': '0.68396', 'examples_per_second': '31.524', 'grad_norm': '60.25', 'counters/examples': 238496, 'counters/updates': 7453}
train stats after 238528 examples: {'rewards_train/chosen': '0.14114', 'rewards_train/rejected': '0.045533', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095607', 'logps_train/rejected': '-125.76', 'logps_train/chosen': '-125.48', 'loss/train': '0.66693', 'examples_per_second': '30.697', 'grad_norm': '25.5', 'counters/examples': 238528, 'counters/updates': 7454}
train stats after 238560 examples: {'rewards_train/chosen': '0.064356', 'rewards_train/rejected': '-0.024915', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089272', 'logps_train/rejected': '-134.51', 'logps_train/chosen': '-152.88', 'loss/train': '0.65453', 'examples_per_second': '31.241', 'grad_norm': '26.625', 'counters/examples': 238560, 'counters/updates': 7455}
train stats after 238592 examples: {'rewards_train/chosen': '0.10974', 'rewards_train/rejected': '0.014814', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.09493', 'logps_train/rejected': '-119.03', 'logps_train/chosen': '-133.72', 'loss/train': '0.65839', 'examples_per_second': '32.24', 'grad_norm': '25.25', 'counters/examples': 238592, 'counters/updates': 7456}
train stats after 238624 examples: {'rewards_train/chosen': '0.13717', 'rewards_train/rejected': '0.06011', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077063', 'logps_train/rejected': '-114.1', 'logps_train/chosen': '-151.24', 'loss/train': '0.67409', 'examples_per_second': '31.821', 'grad_norm': '50.75', 'counters/examples': 238624, 'counters/updates': 7457}
train stats after 238656 examples: {'rewards_train/chosen': '0.19428', 'rewards_train/rejected': '0.078343', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11594', 'logps_train/rejected': '-131.13', 'logps_train/chosen': '-159.79', 'loss/train': '0.65732', 'examples_per_second': '30.801', 'grad_norm': '55.25', 'counters/examples': 238656, 'counters/updates': 7458}
train stats after 238688 examples: {'rewards_train/chosen': '0.13284', 'rewards_train/rejected': '0.041148', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091694', 'logps_train/rejected': '-123.85', 'logps_train/chosen': '-123.16', 'loss/train': '0.66077', 'examples_per_second': '31.697', 'grad_norm': '32.25', 'counters/examples': 238688, 'counters/updates': 7459}
train stats after 238720 examples: {'rewards_train/chosen': '0.11877', 'rewards_train/rejected': '0.030165', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.088604', 'logps_train/rejected': '-162.04', 'logps_train/chosen': '-136.34', 'loss/train': '0.66404', 'examples_per_second': '31.363', 'grad_norm': '38', 'counters/examples': 238720, 'counters/updates': 7460}
train stats after 238752 examples: {'rewards_train/chosen': '0.15651', 'rewards_train/rejected': '-0.0086039', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16511', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-131.8', 'loss/train': '0.62634', 'examples_per_second': '31.415', 'grad_norm': '31.5', 'counters/examples': 238752, 'counters/updates': 7461}
train stats after 238784 examples: {'rewards_train/chosen': '0.20594', 'rewards_train/rejected': '-0.059374', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.26532', 'logps_train/rejected': '-127.02', 'logps_train/chosen': '-156.54', 'loss/train': '0.59989', 'examples_per_second': '30.91', 'grad_norm': '29.75', 'counters/examples': 238784, 'counters/updates': 7462}
train stats after 238816 examples: {'rewards_train/chosen': '0.19974', 'rewards_train/rejected': '0.10871', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091029', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-150.7', 'loss/train': '0.67841', 'examples_per_second': '31.398', 'grad_norm': '40.75', 'counters/examples': 238816, 'counters/updates': 7463}
train stats after 238848 examples: {'rewards_train/chosen': '0.039196', 'rewards_train/rejected': '0.037253', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0019433', 'logps_train/rejected': '-143.25', 'logps_train/chosen': '-165.29', 'loss/train': '0.70646', 'examples_per_second': '30.433', 'grad_norm': '35.5', 'counters/examples': 238848, 'counters/updates': 7464}
train stats after 238880 examples: {'rewards_train/chosen': '0.088665', 'rewards_train/rejected': '0.018959', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.069706', 'logps_train/rejected': '-137.47', 'logps_train/chosen': '-110.96', 'loss/train': '0.66758', 'examples_per_second': '31.489', 'grad_norm': '30.375', 'counters/examples': 238880, 'counters/updates': 7465}
skipping logging after 238912 examples to avoid logging too frequently
train stats after 238944 examples: {'rewards_train/chosen': '0.088249', 'rewards_train/rejected': '-0.033269', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12152', 'logps_train/rejected': '-102.56', 'logps_train/chosen': '-118.4', 'loss/train': '0.64712', 'examples_per_second': '30.675', 'grad_norm': '25.5', 'counters/examples': 238944, 'counters/updates': 7467}
train stats after 238976 examples: {'rewards_train/chosen': '0.25635', 'rewards_train/rejected': '0.074449', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1819', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-194.62', 'loss/train': '0.62372', 'examples_per_second': '31.513', 'grad_norm': '41', 'counters/examples': 238976, 'counters/updates': 7468}
train stats after 239008 examples: {'rewards_train/chosen': '0.085434', 'rewards_train/rejected': '0.042435', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.042998', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-155.74', 'loss/train': '0.6857', 'examples_per_second': '31.863', 'grad_norm': '32.5', 'counters/examples': 239008, 'counters/updates': 7469}
train stats after 239040 examples: {'rewards_train/chosen': '0.23765', 'rewards_train/rejected': '0.017862', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21979', 'logps_train/rejected': '-72.257', 'logps_train/chosen': '-119.44', 'loss/train': '0.60297', 'examples_per_second': '31.68', 'grad_norm': '26.625', 'counters/examples': 239040, 'counters/updates': 7470}
train stats after 239072 examples: {'rewards_train/chosen': '0.2653', 'rewards_train/rejected': '0.032643', 'rewards_train/accuracies': '0.9375', 'rewards_train/margins': '0.23265', 'logps_train/rejected': '-120.76', 'logps_train/chosen': '-166', 'loss/train': '0.59213', 'examples_per_second': '31.519', 'grad_norm': '32.25', 'counters/examples': 239072, 'counters/updates': 7471}
train stats after 239104 examples: {'rewards_train/chosen': '0.18258', 'rewards_train/rejected': '-0.0012617', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18384', 'logps_train/rejected': '-107.38', 'logps_train/chosen': '-148.36', 'loss/train': '0.62239', 'examples_per_second': '30.455', 'grad_norm': '34.5', 'counters/examples': 239104, 'counters/updates': 7472}
train stats after 239136 examples: {'rewards_train/chosen': '0.2606', 'rewards_train/rejected': '0.14869', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11191', 'logps_train/rejected': '-155.38', 'logps_train/chosen': '-154.48', 'loss/train': '0.68877', 'examples_per_second': '30.029', 'grad_norm': '51.75', 'counters/examples': 239136, 'counters/updates': 7473}
train stats after 239168 examples: {'rewards_train/chosen': '0.13722', 'rewards_train/rejected': '0.021302', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11591', 'logps_train/rejected': '-106.88', 'logps_train/chosen': '-151.62', 'loss/train': '0.66112', 'examples_per_second': '32.53', 'grad_norm': '29.125', 'counters/examples': 239168, 'counters/updates': 7474}
train stats after 239200 examples: {'rewards_train/chosen': '0.087544', 'rewards_train/rejected': '-0.0074812', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095025', 'logps_train/rejected': '-132.28', 'logps_train/chosen': '-143.31', 'loss/train': '0.65925', 'examples_per_second': '30.452', 'grad_norm': '27.75', 'counters/examples': 239200, 'counters/updates': 7475}
train stats after 239232 examples: {'rewards_train/chosen': '0.25349', 'rewards_train/rejected': '0.049971', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20352', 'logps_train/rejected': '-179.68', 'logps_train/chosen': '-175.04', 'loss/train': '0.66377', 'examples_per_second': '31.52', 'grad_norm': '45', 'counters/examples': 239232, 'counters/updates': 7476}
train stats after 239264 examples: {'rewards_train/chosen': '0.12862', 'rewards_train/rejected': '0.039258', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.089363', 'logps_train/rejected': '-115.5', 'logps_train/chosen': '-158.6', 'loss/train': '0.66582', 'examples_per_second': '29.9', 'grad_norm': '34.5', 'counters/examples': 239264, 'counters/updates': 7477}
train stats after 239296 examples: {'rewards_train/chosen': '0.1872', 'rewards_train/rejected': '0.10721', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079986', 'logps_train/rejected': '-135.32', 'logps_train/chosen': '-143.02', 'loss/train': '0.68914', 'examples_per_second': '32.925', 'grad_norm': '37.75', 'counters/examples': 239296, 'counters/updates': 7478}
train stats after 239328 examples: {'rewards_train/chosen': '0.15486', 'rewards_train/rejected': '0.069598', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085265', 'logps_train/rejected': '-102.67', 'logps_train/chosen': '-140.69', 'loss/train': '0.66674', 'examples_per_second': '30.907', 'grad_norm': '31.75', 'counters/examples': 239328, 'counters/updates': 7479}
train stats after 239360 examples: {'rewards_train/chosen': '0.25308', 'rewards_train/rejected': '0.062133', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19094', 'logps_train/rejected': '-159.14', 'logps_train/chosen': '-163.61', 'loss/train': '0.6151', 'examples_per_second': '31.824', 'grad_norm': '27.375', 'counters/examples': 239360, 'counters/updates': 7480}
train stats after 239392 examples: {'rewards_train/chosen': '0.11456', 'rewards_train/rejected': '0.07848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.036083', 'logps_train/rejected': '-136.64', 'logps_train/chosen': '-178.27', 'loss/train': '0.69163', 'examples_per_second': '31.364', 'grad_norm': '32.25', 'counters/examples': 239392, 'counters/updates': 7481}
skipping logging after 239424 examples to avoid logging too frequently
train stats after 239456 examples: {'rewards_train/chosen': '0.14078', 'rewards_train/rejected': '0.083359', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.057419', 'logps_train/rejected': '-144.95', 'logps_train/chosen': '-142.25', 'loss/train': '0.67989', 'examples_per_second': '30.036', 'grad_norm': '32.25', 'counters/examples': 239456, 'counters/updates': 7483}
train stats after 239488 examples: {'rewards_train/chosen': '0.13093', 'rewards_train/rejected': '-0.0073028', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13823', 'logps_train/rejected': '-102.36', 'logps_train/chosen': '-137.36', 'loss/train': '0.64443', 'examples_per_second': '31.334', 'grad_norm': '29.375', 'counters/examples': 239488, 'counters/updates': 7484}
train stats after 239520 examples: {'rewards_train/chosen': '0.16497', 'rewards_train/rejected': '0.025791', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13918', 'logps_train/rejected': '-137.45', 'logps_train/chosen': '-155.1', 'loss/train': '0.63847', 'examples_per_second': '31.478', 'grad_norm': '30.375', 'counters/examples': 239520, 'counters/updates': 7485}
train stats after 239552 examples: {'rewards_train/chosen': '0.21091', 'rewards_train/rejected': '0.0092753', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20163', 'logps_train/rejected': '-139.67', 'logps_train/chosen': '-135.77', 'loss/train': '0.61312', 'examples_per_second': '31.323', 'grad_norm': '29.875', 'counters/examples': 239552, 'counters/updates': 7486}
train stats after 239584 examples: {'rewards_train/chosen': '0.1248', 'rewards_train/rejected': '0.019002', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1058', 'logps_train/rejected': '-149.61', 'logps_train/chosen': '-186.82', 'loss/train': '0.65753', 'examples_per_second': '30.404', 'grad_norm': '29.375', 'counters/examples': 239584, 'counters/updates': 7487}
train stats after 239616 examples: {'rewards_train/chosen': '0.086425', 'rewards_train/rejected': '0.043748', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042677', 'logps_train/rejected': '-131.74', 'logps_train/chosen': '-141.07', 'loss/train': '0.68426', 'examples_per_second': '31.393', 'grad_norm': '29', 'counters/examples': 239616, 'counters/updates': 7488}
train stats after 239648 examples: {'rewards_train/chosen': '0.099029', 'rewards_train/rejected': '0.028054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070975', 'logps_train/rejected': '-110.3', 'logps_train/chosen': '-140.06', 'loss/train': '0.66947', 'examples_per_second': '30.951', 'grad_norm': '33.75', 'counters/examples': 239648, 'counters/updates': 7489}
skipping logging after 239680 examples to avoid logging too frequently
train stats after 239712 examples: {'rewards_train/chosen': '0.16943', 'rewards_train/rejected': '0.086876', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082553', 'logps_train/rejected': '-104.28', 'logps_train/chosen': '-162.87', 'loss/train': '0.66875', 'examples_per_second': '31.011', 'grad_norm': '30.25', 'counters/examples': 239712, 'counters/updates': 7491}
train stats after 239744 examples: {'rewards_train/chosen': '0.19063', 'rewards_train/rejected': '0.075388', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11524', 'logps_train/rejected': '-112.94', 'logps_train/chosen': '-128.08', 'loss/train': '0.65182', 'examples_per_second': '32.835', 'grad_norm': '26.375', 'counters/examples': 239744, 'counters/updates': 7492}
train stats after 239776 examples: {'rewards_train/chosen': '0.16513', 'rewards_train/rejected': '0.092837', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.072294', 'logps_train/rejected': '-153.82', 'logps_train/chosen': '-122.66', 'loss/train': '0.66666', 'examples_per_second': '31.576', 'grad_norm': '32.75', 'counters/examples': 239776, 'counters/updates': 7493}
skipping logging after 239808 examples to avoid logging too frequently
train stats after 239840 examples: {'rewards_train/chosen': '0.14382', 'rewards_train/rejected': '0.069033', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.074788', 'logps_train/rejected': '-113.06', 'logps_train/chosen': '-139.81', 'loss/train': '0.66738', 'examples_per_second': '35.496', 'grad_norm': '32', 'counters/examples': 239840, 'counters/updates': 7495}
train stats after 239872 examples: {'rewards_train/chosen': '0.19444', 'rewards_train/rejected': '0.035834', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15861', 'logps_train/rejected': '-140.84', 'logps_train/chosen': '-146.66', 'loss/train': '0.62802', 'examples_per_second': '31.426', 'grad_norm': '33.75', 'counters/examples': 239872, 'counters/updates': 7496}
train stats after 239904 examples: {'rewards_train/chosen': '0.10118', 'rewards_train/rejected': '0.047688', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.053496', 'logps_train/rejected': '-139.47', 'logps_train/chosen': '-128.15', 'loss/train': '0.67875', 'examples_per_second': '25.385', 'grad_norm': '32.25', 'counters/examples': 239904, 'counters/updates': 7497}
train stats after 239936 examples: {'rewards_train/chosen': '0.14509', 'rewards_train/rejected': '0.039239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10585', 'logps_train/rejected': '-116.79', 'logps_train/chosen': '-152.39', 'loss/train': '0.66267', 'examples_per_second': '31.48', 'grad_norm': '35.75', 'counters/examples': 239936, 'counters/updates': 7498}
train stats after 239968 examples: {'rewards_train/chosen': '0.094447', 'rewards_train/rejected': '-0.0033979', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097845', 'logps_train/rejected': '-113.24', 'logps_train/chosen': '-115.77', 'loss/train': '0.65259', 'examples_per_second': '31.985', 'grad_norm': '29.875', 'counters/examples': 239968, 'counters/updates': 7499}
train stats after 240000 examples: {'rewards_train/chosen': '0.14268', 'rewards_train/rejected': '0.096288', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046387', 'logps_train/rejected': '-111.93', 'logps_train/chosen': '-125.76', 'loss/train': '0.68149', 'examples_per_second': '32.568', 'grad_norm': '25.625', 'counters/examples': 240000, 'counters/updates': 7500}
Running evaluation after 240000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.84it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.85it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 240000: {'rewards_eval/chosen': '0.19129', 'rewards_eval/rejected': '0.058808', 'rewards_eval/accuracies': '0.61719', 'rewards_eval/margins': '0.13248', 'logps_eval/rejected': '-121.54', 'logps_eval/chosen': '-142.19', 'loss/eval': '0.64891'}
skipping save for non epoch
train stats after 240032 examples: {'rewards_train/chosen': '0.20137', 'rewards_train/rejected': '0.053383', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14799', 'logps_train/rejected': '-150.23', 'logps_train/chosen': '-155.99', 'loss/train': '0.6382', 'examples_per_second': '30.585', 'grad_norm': '53.75', 'counters/examples': 240032, 'counters/updates': 7501}
train stats after 240064 examples: {'rewards_train/chosen': '0.12022', 'rewards_train/rejected': '-0.052909', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17313', 'logps_train/rejected': '-99.549', 'logps_train/chosen': '-118.41', 'loss/train': '0.61848', 'examples_per_second': '30.609', 'grad_norm': '22.5', 'counters/examples': 240064, 'counters/updates': 7502}
train stats after 240096 examples: {'rewards_train/chosen': '0.047431', 'rewards_train/rejected': '0.016389', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031042', 'logps_train/rejected': '-111.07', 'logps_train/chosen': '-123.26', 'loss/train': '0.7004', 'examples_per_second': '30.811', 'grad_norm': '29.875', 'counters/examples': 240096, 'counters/updates': 7503}
train stats after 240128 examples: {'rewards_train/chosen': '0.25517', 'rewards_train/rejected': '0.089378', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16579', 'logps_train/rejected': '-124.56', 'logps_train/chosen': '-173.51', 'loss/train': '0.63299', 'examples_per_second': '30.941', 'grad_norm': '28.75', 'counters/examples': 240128, 'counters/updates': 7504}
train stats after 240160 examples: {'rewards_train/chosen': '0.20046', 'rewards_train/rejected': '0.024196', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17627', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-156.78', 'loss/train': '0.61929', 'examples_per_second': '30.834', 'grad_norm': '27.625', 'counters/examples': 240160, 'counters/updates': 7505}
train stats after 240192 examples: {'rewards_train/chosen': '0.20742', 'rewards_train/rejected': '0.041027', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16639', 'logps_train/rejected': '-130', 'logps_train/chosen': '-139.85', 'loss/train': '0.6276', 'examples_per_second': '30.867', 'grad_norm': '28.5', 'counters/examples': 240192, 'counters/updates': 7506}
train stats after 240224 examples: {'rewards_train/chosen': '0.15707', 'rewards_train/rejected': '0.045057', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11201', 'logps_train/rejected': '-130.34', 'logps_train/chosen': '-141.04', 'loss/train': '0.6552', 'examples_per_second': '31.244', 'grad_norm': '27.25', 'counters/examples': 240224, 'counters/updates': 7507}
skipping logging after 240256 examples to avoid logging too frequently
train stats after 240288 examples: {'rewards_train/chosen': '0.15279', 'rewards_train/rejected': '0.014562', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13823', 'logps_train/rejected': '-140.63', 'logps_train/chosen': '-134.16', 'loss/train': '0.63254', 'examples_per_second': '30.5', 'grad_norm': '26.625', 'counters/examples': 240288, 'counters/updates': 7509}
train stats after 240320 examples: {'rewards_train/chosen': '0.10975', 'rewards_train/rejected': '0.016962', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092789', 'logps_train/rejected': '-156.93', 'logps_train/chosen': '-157.25', 'loss/train': '0.65893', 'examples_per_second': '30.852', 'grad_norm': '37.5', 'counters/examples': 240320, 'counters/updates': 7510}
train stats after 240352 examples: {'rewards_train/chosen': '0.15566', 'rewards_train/rejected': '0.039988', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11567', 'logps_train/rejected': '-108.79', 'logps_train/chosen': '-131.61', 'loss/train': '0.65357', 'examples_per_second': '32.899', 'grad_norm': '29.875', 'counters/examples': 240352, 'counters/updates': 7511}
skipping logging after 240384 examples to avoid logging too frequently
train stats after 240416 examples: {'rewards_train/chosen': '0.11714', 'rewards_train/rejected': '0.050727', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066416', 'logps_train/rejected': '-109.49', 'logps_train/chosen': '-106.05', 'loss/train': '0.66972', 'examples_per_second': '30.852', 'grad_norm': '24.375', 'counters/examples': 240416, 'counters/updates': 7513}
train stats after 240448 examples: {'rewards_train/chosen': '0.14605', 'rewards_train/rejected': '-0.073153', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21921', 'logps_train/rejected': '-116.71', 'logps_train/chosen': '-150.78', 'loss/train': '0.60264', 'examples_per_second': '31.375', 'grad_norm': '31.875', 'counters/examples': 240448, 'counters/updates': 7514}
train stats after 240480 examples: {'rewards_train/chosen': '0.15594', 'rewards_train/rejected': '0.15883', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0028948', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-134.15', 'loss/train': '0.7034', 'examples_per_second': '30.301', 'grad_norm': '34.75', 'counters/examples': 240480, 'counters/updates': 7515}
train stats after 240512 examples: {'rewards_train/chosen': '0.14287', 'rewards_train/rejected': '0.02444', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11843', 'logps_train/rejected': '-113.37', 'logps_train/chosen': '-157.12', 'loss/train': '0.65023', 'examples_per_second': '30.5', 'grad_norm': '33.5', 'counters/examples': 240512, 'counters/updates': 7516}
skipping logging after 240544 examples to avoid logging too frequently
train stats after 240576 examples: {'rewards_train/chosen': '0.059184', 'rewards_train/rejected': '0.017449', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041736', 'logps_train/rejected': '-122.85', 'logps_train/chosen': '-152.75', 'loss/train': '0.68407', 'examples_per_second': '30.51', 'grad_norm': '38', 'counters/examples': 240576, 'counters/updates': 7518}
train stats after 240608 examples: {'rewards_train/chosen': '0.19839', 'rewards_train/rejected': '0.086554', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11184', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-133.14', 'loss/train': '0.64878', 'examples_per_second': '32.92', 'grad_norm': '28.125', 'counters/examples': 240608, 'counters/updates': 7519}
train stats after 240640 examples: {'rewards_train/chosen': '0.13723', 'rewards_train/rejected': '0.083907', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.053326', 'logps_train/rejected': '-143.68', 'logps_train/chosen': '-133.41', 'loss/train': '0.67363', 'examples_per_second': '24.47', 'grad_norm': '27.75', 'counters/examples': 240640, 'counters/updates': 7520}
skipping logging after 240672 examples to avoid logging too frequently
train stats after 240704 examples: {'rewards_train/chosen': '0.23969', 'rewards_train/rejected': '0.22581', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013873', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-132.23', 'loss/train': '0.71247', 'examples_per_second': '35.354', 'grad_norm': '36.5', 'counters/examples': 240704, 'counters/updates': 7522}
train stats after 240736 examples: {'rewards_train/chosen': '0.1889', 'rewards_train/rejected': '0.06335', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12555', 'logps_train/rejected': '-135.03', 'logps_train/chosen': '-172.06', 'loss/train': '0.64403', 'examples_per_second': '24.194', 'grad_norm': '39.75', 'counters/examples': 240736, 'counters/updates': 7523}
train stats after 240768 examples: {'rewards_train/chosen': '0.18909', 'rewards_train/rejected': '0.07243', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11666', 'logps_train/rejected': '-118.69', 'logps_train/chosen': '-130.55', 'loss/train': '0.65387', 'examples_per_second': '30.878', 'grad_norm': '32', 'counters/examples': 240768, 'counters/updates': 7524}
train stats after 240800 examples: {'rewards_train/chosen': '0.055217', 'rewards_train/rejected': '0.047574', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0076434', 'logps_train/rejected': '-130.24', 'logps_train/chosen': '-114.96', 'loss/train': '0.69975', 'examples_per_second': '33.169', 'grad_norm': '26.5', 'counters/examples': 240800, 'counters/updates': 7525}
train stats after 240832 examples: {'rewards_train/chosen': '0.08629', 'rewards_train/rejected': '0.073332', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012958', 'logps_train/rejected': '-128.16', 'logps_train/chosen': '-179.42', 'loss/train': '0.70161', 'examples_per_second': '30.6', 'grad_norm': '55.5', 'counters/examples': 240832, 'counters/updates': 7526}
train stats after 240864 examples: {'rewards_train/chosen': '0.20985', 'rewards_train/rejected': '0.1175', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092351', 'logps_train/rejected': '-102.65', 'logps_train/chosen': '-140.51', 'loss/train': '0.65948', 'examples_per_second': '32.305', 'grad_norm': '30.625', 'counters/examples': 240864, 'counters/updates': 7527}
train stats after 240896 examples: {'rewards_train/chosen': '0.22833', 'rewards_train/rejected': '0.10612', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1222', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-151.06', 'loss/train': '0.65658', 'examples_per_second': '30.841', 'grad_norm': '37.5', 'counters/examples': 240896, 'counters/updates': 7528}
train stats after 240928 examples: {'rewards_train/chosen': '0.19565', 'rewards_train/rejected': '0.063895', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13175', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-122.95', 'loss/train': '0.63954', 'examples_per_second': '31.87', 'grad_norm': '28.125', 'counters/examples': 240928, 'counters/updates': 7529}
skipping logging after 240960 examples to avoid logging too frequently
train stats after 240992 examples: {'rewards_train/chosen': '0.20688', 'rewards_train/rejected': '-0.00054182', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20742', 'logps_train/rejected': '-97.193', 'logps_train/chosen': '-135.67', 'loss/train': '0.62002', 'examples_per_second': '33.344', 'grad_norm': '26.375', 'counters/examples': 240992, 'counters/updates': 7531}
train stats after 241024 examples: {'rewards_train/chosen': '0.32988', 'rewards_train/rejected': '0.11584', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21404', 'logps_train/rejected': '-112.18', 'logps_train/chosen': '-141.7', 'loss/train': '0.61694', 'examples_per_second': '30.939', 'grad_norm': '29.875', 'counters/examples': 241024, 'counters/updates': 7532}
skipping logging after 241056 examples to avoid logging too frequently
train stats after 241088 examples: {'rewards_train/chosen': '0.20581', 'rewards_train/rejected': '0.097114', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1087', 'logps_train/rejected': '-140.64', 'logps_train/chosen': '-140.78', 'loss/train': '0.65461', 'examples_per_second': '29.856', 'grad_norm': '38.25', 'counters/examples': 241088, 'counters/updates': 7534}
train stats after 241120 examples: {'rewards_train/chosen': '0.17458', 'rewards_train/rejected': '0.050202', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12438', 'logps_train/rejected': '-152.98', 'logps_train/chosen': '-131.67', 'loss/train': '0.64255', 'examples_per_second': '31.354', 'grad_norm': '30.5', 'counters/examples': 241120, 'counters/updates': 7535}
train stats after 241152 examples: {'rewards_train/chosen': '0.28792', 'rewards_train/rejected': '0.075594', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21233', 'logps_train/rejected': '-120.14', 'logps_train/chosen': '-144.27', 'loss/train': '0.61526', 'examples_per_second': '30.541', 'grad_norm': '32.5', 'counters/examples': 241152, 'counters/updates': 7536}
train stats after 241184 examples: {'rewards_train/chosen': '0.25814', 'rewards_train/rejected': '0.16125', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096893', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-140.85', 'loss/train': '0.65372', 'examples_per_second': '30.864', 'grad_norm': '34.25', 'counters/examples': 241184, 'counters/updates': 7537}
train stats after 241216 examples: {'rewards_train/chosen': '0.17198', 'rewards_train/rejected': '0.076765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095218', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-114.53', 'loss/train': '0.66716', 'examples_per_second': '30.37', 'grad_norm': '30.125', 'counters/examples': 241216, 'counters/updates': 7538}
skipping logging after 241248 examples to avoid logging too frequently
train stats after 241280 examples: {'rewards_train/chosen': '0.27022', 'rewards_train/rejected': '0.11153', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15869', 'logps_train/rejected': '-114.72', 'logps_train/chosen': '-134.22', 'loss/train': '0.63282', 'examples_per_second': '32.991', 'grad_norm': '26.625', 'counters/examples': 241280, 'counters/updates': 7540}
train stats after 241312 examples: {'rewards_train/chosen': '0.090281', 'rewards_train/rejected': '-0.025127', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11541', 'logps_train/rejected': '-125.25', 'logps_train/chosen': '-137.28', 'loss/train': '0.65805', 'examples_per_second': '31.659', 'grad_norm': '35.5', 'counters/examples': 241312, 'counters/updates': 7541}
train stats after 241344 examples: {'rewards_train/chosen': '0.1857', 'rewards_train/rejected': '0.086067', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099635', 'logps_train/rejected': '-165.31', 'logps_train/chosen': '-153.3', 'loss/train': '0.65836', 'examples_per_second': '31.451', 'grad_norm': '45', 'counters/examples': 241344, 'counters/updates': 7542}
train stats after 241376 examples: {'rewards_train/chosen': '0.14834', 'rewards_train/rejected': '0.028943', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1194', 'logps_train/rejected': '-138.53', 'logps_train/chosen': '-173.28', 'loss/train': '0.6487', 'examples_per_second': '31.515', 'grad_norm': '34.5', 'counters/examples': 241376, 'counters/updates': 7543}
train stats after 241408 examples: {'rewards_train/chosen': '0.064191', 'rewards_train/rejected': '0.062001', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.00219', 'logps_train/rejected': '-142.85', 'logps_train/chosen': '-152.22', 'loss/train': '0.70364', 'examples_per_second': '31.002', 'grad_norm': '29.25', 'counters/examples': 241408, 'counters/updates': 7544}
train stats after 241440 examples: {'rewards_train/chosen': '0.32089', 'rewards_train/rejected': '0.06095', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25994', 'logps_train/rejected': '-122.16', 'logps_train/chosen': '-180.66', 'loss/train': '0.59828', 'examples_per_second': '31.456', 'grad_norm': '30', 'counters/examples': 241440, 'counters/updates': 7545}
train stats after 241472 examples: {'rewards_train/chosen': '0.20504', 'rewards_train/rejected': '0.09485', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11019', 'logps_train/rejected': '-120.16', 'logps_train/chosen': '-140.81', 'loss/train': '0.65704', 'examples_per_second': '32.388', 'grad_norm': '28.875', 'counters/examples': 241472, 'counters/updates': 7546}
train stats after 241504 examples: {'rewards_train/chosen': '0.13609', 'rewards_train/rejected': '-0.035466', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17156', 'logps_train/rejected': '-132.37', 'logps_train/chosen': '-185.62', 'loss/train': '0.65714', 'examples_per_second': '31.44', 'grad_norm': '43.25', 'counters/examples': 241504, 'counters/updates': 7547}
train stats after 241536 examples: {'rewards_train/chosen': '0.16278', 'rewards_train/rejected': '0.025895', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13688', 'logps_train/rejected': '-128.48', 'logps_train/chosen': '-158.89', 'loss/train': '0.63875', 'examples_per_second': '31.267', 'grad_norm': '39.5', 'counters/examples': 241536, 'counters/updates': 7548}
train stats after 241568 examples: {'rewards_train/chosen': '0.17465', 'rewards_train/rejected': '0.024633', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15002', 'logps_train/rejected': '-119.93', 'logps_train/chosen': '-168.86', 'loss/train': '0.63214', 'examples_per_second': '29.915', 'grad_norm': '33.25', 'counters/examples': 241568, 'counters/updates': 7549}
train stats after 241600 examples: {'rewards_train/chosen': '0.19991', 'rewards_train/rejected': '0.061327', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13858', 'logps_train/rejected': '-110.73', 'logps_train/chosen': '-138.79', 'loss/train': '0.64284', 'examples_per_second': '31.496', 'grad_norm': '27', 'counters/examples': 241600, 'counters/updates': 7550}
train stats after 241632 examples: {'rewards_train/chosen': '0.13821', 'rewards_train/rejected': '0.049306', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.088907', 'logps_train/rejected': '-124.6', 'logps_train/chosen': '-135.35', 'loss/train': '0.66153', 'examples_per_second': '32.384', 'grad_norm': '30.125', 'counters/examples': 241632, 'counters/updates': 7551}
train stats after 241664 examples: {'rewards_train/chosen': '0.14914', 'rewards_train/rejected': '0.045141', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.104', 'logps_train/rejected': '-102.09', 'logps_train/chosen': '-172.72', 'loss/train': '0.65762', 'examples_per_second': '31.763', 'grad_norm': '27.875', 'counters/examples': 241664, 'counters/updates': 7552}
train stats after 241696 examples: {'rewards_train/chosen': '0.18596', 'rewards_train/rejected': '-0.0035892', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18955', 'logps_train/rejected': '-88.481', 'logps_train/chosen': '-120.01', 'loss/train': '0.61011', 'examples_per_second': '32.212', 'grad_norm': '26', 'counters/examples': 241696, 'counters/updates': 7553}
train stats after 241728 examples: {'rewards_train/chosen': '0.30926', 'rewards_train/rejected': '0.10556', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2037', 'logps_train/rejected': '-156.13', 'logps_train/chosen': '-159.77', 'loss/train': '0.62331', 'examples_per_second': '31.248', 'grad_norm': '38.5', 'counters/examples': 241728, 'counters/updates': 7554}
train stats after 241760 examples: {'rewards_train/chosen': '0.050572', 'rewards_train/rejected': '0.056665', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.006093', 'logps_train/rejected': '-148.84', 'logps_train/chosen': '-171.6', 'loss/train': '0.72392', 'examples_per_second': '31.161', 'grad_norm': '69', 'counters/examples': 241760, 'counters/updates': 7555}
train stats after 241792 examples: {'rewards_train/chosen': '0.27376', 'rewards_train/rejected': '0.1518', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12195', 'logps_train/rejected': '-136.94', 'logps_train/chosen': '-144.1', 'loss/train': '0.6495', 'examples_per_second': '30.627', 'grad_norm': '29.5', 'counters/examples': 241792, 'counters/updates': 7556}
train stats after 241824 examples: {'rewards_train/chosen': '0.10341', 'rewards_train/rejected': '0.052034', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051375', 'logps_train/rejected': '-143.48', 'logps_train/chosen': '-181.36', 'loss/train': '0.68089', 'examples_per_second': '31.507', 'grad_norm': '28.75', 'counters/examples': 241824, 'counters/updates': 7557}
train stats after 241856 examples: {'rewards_train/chosen': '0.060199', 'rewards_train/rejected': '0.031268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.028931', 'logps_train/rejected': '-142.01', 'logps_train/chosen': '-125.88', 'loss/train': '0.68799', 'examples_per_second': '31.482', 'grad_norm': '28.625', 'counters/examples': 241856, 'counters/updates': 7558}
skipping logging after 241888 examples to avoid logging too frequently
train stats after 241920 examples: {'rewards_train/chosen': '0.17854', 'rewards_train/rejected': '0.094757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083783', 'logps_train/rejected': '-136.12', 'logps_train/chosen': '-144.42', 'loss/train': '0.66445', 'examples_per_second': '31.225', 'grad_norm': '34', 'counters/examples': 241920, 'counters/updates': 7560}
train stats after 241952 examples: {'rewards_train/chosen': '0.29609', 'rewards_train/rejected': '0.13969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1564', 'logps_train/rejected': '-169.64', 'logps_train/chosen': '-173.47', 'loss/train': '0.64535', 'examples_per_second': '31.35', 'grad_norm': '34.75', 'counters/examples': 241952, 'counters/updates': 7561}
train stats after 241984 examples: {'rewards_train/chosen': '0.15471', 'rewards_train/rejected': '0.070604', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.084108', 'logps_train/rejected': '-156.55', 'logps_train/chosen': '-158.84', 'loss/train': '0.67698', 'examples_per_second': '31.403', 'grad_norm': '45.75', 'counters/examples': 241984, 'counters/updates': 7562}
train stats after 242016 examples: {'rewards_train/chosen': '0.15982', 'rewards_train/rejected': '0.10031', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.059519', 'logps_train/rejected': '-106.93', 'logps_train/chosen': '-125.51', 'loss/train': '0.67425', 'examples_per_second': '30.412', 'grad_norm': '46.75', 'counters/examples': 242016, 'counters/updates': 7563}
train stats after 242048 examples: {'rewards_train/chosen': '0.15246', 'rewards_train/rejected': '0.038649', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11381', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-150.88', 'loss/train': '0.65002', 'examples_per_second': '30.497', 'grad_norm': '29', 'counters/examples': 242048, 'counters/updates': 7564}
train stats after 242080 examples: {'rewards_train/chosen': '0.21563', 'rewards_train/rejected': '0.046744', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16888', 'logps_train/rejected': '-158.32', 'logps_train/chosen': '-170.27', 'loss/train': '0.63089', 'examples_per_second': '31.478', 'grad_norm': '29.625', 'counters/examples': 242080, 'counters/updates': 7565}
train stats after 242112 examples: {'rewards_train/chosen': '0.11157', 'rewards_train/rejected': '0.045803', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06577', 'logps_train/rejected': '-130.65', 'logps_train/chosen': '-124.76', 'loss/train': '0.66772', 'examples_per_second': '31.055', 'grad_norm': '30.375', 'counters/examples': 242112, 'counters/updates': 7566}
train stats after 242144 examples: {'rewards_train/chosen': '0.15325', 'rewards_train/rejected': '0.13467', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.018581', 'logps_train/rejected': '-158.84', 'logps_train/chosen': '-155.04', 'loss/train': '0.70906', 'examples_per_second': '31.706', 'grad_norm': '45.5', 'counters/examples': 242144, 'counters/updates': 7567}
skipping logging after 242176 examples to avoid logging too frequently
train stats after 242208 examples: {'rewards_train/chosen': '0.22716', 'rewards_train/rejected': '0.1171', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11006', 'logps_train/rejected': '-142.37', 'logps_train/chosen': '-149.49', 'loss/train': '0.65682', 'examples_per_second': '33.341', 'grad_norm': '41.5', 'counters/examples': 242208, 'counters/updates': 7569}
skipping logging after 242240 examples to avoid logging too frequently
train stats after 242272 examples: {'rewards_train/chosen': '0.2701', 'rewards_train/rejected': '0.12919', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14091', 'logps_train/rejected': '-149.07', 'logps_train/chosen': '-178.22', 'loss/train': '0.66253', 'examples_per_second': '29.897', 'grad_norm': '38.5', 'counters/examples': 242272, 'counters/updates': 7571}
train stats after 242304 examples: {'rewards_train/chosen': '0.27604', 'rewards_train/rejected': '0.093433', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18261', 'logps_train/rejected': '-144.92', 'logps_train/chosen': '-172.39', 'loss/train': '0.62613', 'examples_per_second': '29.987', 'grad_norm': '43.25', 'counters/examples': 242304, 'counters/updates': 7572}
train stats after 242336 examples: {'rewards_train/chosen': '0.23353', 'rewards_train/rejected': '0.090801', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14273', 'logps_train/rejected': '-113.7', 'logps_train/chosen': '-151.55', 'loss/train': '0.64516', 'examples_per_second': '30.575', 'grad_norm': '40.75', 'counters/examples': 242336, 'counters/updates': 7573}
train stats after 242368 examples: {'rewards_train/chosen': '0.10973', 'rewards_train/rejected': '0.011619', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.098111', 'logps_train/rejected': '-107.73', 'logps_train/chosen': '-139.81', 'loss/train': '0.65647', 'examples_per_second': '32.228', 'grad_norm': '33.75', 'counters/examples': 242368, 'counters/updates': 7574}
train stats after 242400 examples: {'rewards_train/chosen': '0.16877', 'rewards_train/rejected': '-0.01975', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18852', 'logps_train/rejected': '-131.57', 'logps_train/chosen': '-153.94', 'loss/train': '0.61854', 'examples_per_second': '31.517', 'grad_norm': '28.5', 'counters/examples': 242400, 'counters/updates': 7575}
train stats after 242432 examples: {'rewards_train/chosen': '0.13656', 'rewards_train/rejected': '0.066367', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.07019', 'logps_train/rejected': '-131.58', 'logps_train/chosen': '-101.7', 'loss/train': '0.68001', 'examples_per_second': '31.352', 'grad_norm': '28.375', 'counters/examples': 242432, 'counters/updates': 7576}
train stats after 242464 examples: {'rewards_train/chosen': '0.19767', 'rewards_train/rejected': '0.095193', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10247', 'logps_train/rejected': '-128.61', 'logps_train/chosen': '-155.02', 'loss/train': '0.65786', 'examples_per_second': '30.162', 'grad_norm': '35.75', 'counters/examples': 242464, 'counters/updates': 7577}
train stats after 242496 examples: {'rewards_train/chosen': '0.075236', 'rewards_train/rejected': '0.0061843', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069052', 'logps_train/rejected': '-132.06', 'logps_train/chosen': '-141.01', 'loss/train': '0.67252', 'examples_per_second': '30.717', 'grad_norm': '31', 'counters/examples': 242496, 'counters/updates': 7578}
train stats after 242528 examples: {'rewards_train/chosen': '0.15912', 'rewards_train/rejected': '-0.021305', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18042', 'logps_train/rejected': '-113.62', 'logps_train/chosen': '-144.05', 'loss/train': '0.62373', 'examples_per_second': '31.49', 'grad_norm': '24.875', 'counters/examples': 242528, 'counters/updates': 7579}
skipping logging after 242560 examples to avoid logging too frequently
train stats after 242592 examples: {'rewards_train/chosen': '0.16859', 'rewards_train/rejected': '0.035773', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13282', 'logps_train/rejected': '-108.98', 'logps_train/chosen': '-136.38', 'loss/train': '0.64166', 'examples_per_second': '30.053', 'grad_norm': '31.375', 'counters/examples': 242592, 'counters/updates': 7581}
train stats after 242624 examples: {'rewards_train/chosen': '0.15926', 'rewards_train/rejected': '0.078176', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.081082', 'logps_train/rejected': '-137.34', 'logps_train/chosen': '-131.51', 'loss/train': '0.67593', 'examples_per_second': '30.864', 'grad_norm': '34.5', 'counters/examples': 242624, 'counters/updates': 7582}
train stats after 242656 examples: {'rewards_train/chosen': '0.21765', 'rewards_train/rejected': '0.032957', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1847', 'logps_train/rejected': '-103.87', 'logps_train/chosen': '-122.02', 'loss/train': '0.6187', 'examples_per_second': '29.912', 'grad_norm': '29.125', 'counters/examples': 242656, 'counters/updates': 7583}
train stats after 242688 examples: {'rewards_train/chosen': '0.098607', 'rewards_train/rejected': '0.0076831', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.090924', 'logps_train/rejected': '-104.72', 'logps_train/chosen': '-128.61', 'loss/train': '0.66077', 'examples_per_second': '31.261', 'grad_norm': '38.75', 'counters/examples': 242688, 'counters/updates': 7584}
train stats after 242720 examples: {'rewards_train/chosen': '0.12975', 'rewards_train/rejected': '0.044904', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.084843', 'logps_train/rejected': '-95.747', 'logps_train/chosen': '-113.46', 'loss/train': '0.66194', 'examples_per_second': '30.689', 'grad_norm': '24.125', 'counters/examples': 242720, 'counters/updates': 7585}
train stats after 242752 examples: {'rewards_train/chosen': '0.045583', 'rewards_train/rejected': '-0.053835', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099418', 'logps_train/rejected': '-147.47', 'logps_train/chosen': '-118.1', 'loss/train': '0.66432', 'examples_per_second': '32.542', 'grad_norm': '30.25', 'counters/examples': 242752, 'counters/updates': 7586}
train stats after 242784 examples: {'rewards_train/chosen': '0.13196', 'rewards_train/rejected': '0.067281', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.06468', 'logps_train/rejected': '-105.88', 'logps_train/chosen': '-136.3', 'loss/train': '0.67199', 'examples_per_second': '32.83', 'grad_norm': '26.125', 'counters/examples': 242784, 'counters/updates': 7587}
skipping logging after 242816 examples to avoid logging too frequently
train stats after 242848 examples: {'rewards_train/chosen': '0.19382', 'rewards_train/rejected': '0.20583', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.012013', 'logps_train/rejected': '-148.2', 'logps_train/chosen': '-158.74', 'loss/train': '0.71705', 'examples_per_second': '31.507', 'grad_norm': '33.75', 'counters/examples': 242848, 'counters/updates': 7589}
train stats after 242880 examples: {'rewards_train/chosen': '0.12261', 'rewards_train/rejected': '0.039727', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.082881', 'logps_train/rejected': '-137.08', 'logps_train/chosen': '-128.27', 'loss/train': '0.68389', 'examples_per_second': '31.497', 'grad_norm': '33.5', 'counters/examples': 242880, 'counters/updates': 7590}
train stats after 242912 examples: {'rewards_train/chosen': '0.19892', 'rewards_train/rejected': '0.048906', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15002', 'logps_train/rejected': '-131.37', 'logps_train/chosen': '-163.17', 'loss/train': '0.63442', 'examples_per_second': '31.454', 'grad_norm': '38.75', 'counters/examples': 242912, 'counters/updates': 7591}
train stats after 242944 examples: {'rewards_train/chosen': '0.11577', 'rewards_train/rejected': '-0.02302', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13879', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-135.83', 'loss/train': '0.64051', 'examples_per_second': '31.517', 'grad_norm': '31.125', 'counters/examples': 242944, 'counters/updates': 7592}
train stats after 242976 examples: {'rewards_train/chosen': '0.26543', 'rewards_train/rejected': '0.11622', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14921', 'logps_train/rejected': '-138.64', 'logps_train/chosen': '-147.99', 'loss/train': '0.63929', 'examples_per_second': '31.369', 'grad_norm': '45.75', 'counters/examples': 242976, 'counters/updates': 7593}
train stats after 243008 examples: {'rewards_train/chosen': '0.20806', 'rewards_train/rejected': '0.026636', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18143', 'logps_train/rejected': '-121.9', 'logps_train/chosen': '-137.64', 'loss/train': '0.6183', 'examples_per_second': '31.496', 'grad_norm': '24.375', 'counters/examples': 243008, 'counters/updates': 7594}
skipping logging after 243040 examples to avoid logging too frequently
train stats after 243072 examples: {'rewards_train/chosen': '0.085307', 'rewards_train/rejected': '0.05195', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.033357', 'logps_train/rejected': '-116.11', 'logps_train/chosen': '-152.25', 'loss/train': '0.68536', 'examples_per_second': '33.999', 'grad_norm': '28.875', 'counters/examples': 243072, 'counters/updates': 7596}
train stats after 243104 examples: {'rewards_train/chosen': '0.13583', 'rewards_train/rejected': '0.069221', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066607', 'logps_train/rejected': '-124.4', 'logps_train/chosen': '-156.85', 'loss/train': '0.6822', 'examples_per_second': '31.52', 'grad_norm': '32.75', 'counters/examples': 243104, 'counters/updates': 7597}
train stats after 243136 examples: {'rewards_train/chosen': '0.18724', 'rewards_train/rejected': '-0.033184', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22043', 'logps_train/rejected': '-133.56', 'logps_train/chosen': '-151.76', 'loss/train': '0.62059', 'examples_per_second': '31.052', 'grad_norm': '33.75', 'counters/examples': 243136, 'counters/updates': 7598}
train stats after 243168 examples: {'rewards_train/chosen': '0.11712', 'rewards_train/rejected': '0.053253', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.063865', 'logps_train/rejected': '-142.13', 'logps_train/chosen': '-157.55', 'loss/train': '0.67356', 'examples_per_second': '32.159', 'grad_norm': '29', 'counters/examples': 243168, 'counters/updates': 7599}
train stats after 243200 examples: {'rewards_train/chosen': '0.18048', 'rewards_train/rejected': '0.032793', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.14769', 'logps_train/rejected': '-123.37', 'logps_train/chosen': '-106.25', 'loss/train': '0.63338', 'examples_per_second': '30.099', 'grad_norm': '24.75', 'counters/examples': 243200, 'counters/updates': 7600}
train stats after 243232 examples: {'rewards_train/chosen': '0.067746', 'rewards_train/rejected': '0.048691', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019055', 'logps_train/rejected': '-121.76', 'logps_train/chosen': '-148.44', 'loss/train': '0.71003', 'examples_per_second': '31.095', 'grad_norm': '40', 'counters/examples': 243232, 'counters/updates': 7601}
train stats after 243264 examples: {'rewards_train/chosen': '0.19151', 'rewards_train/rejected': '-0.03564', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22715', 'logps_train/rejected': '-126.22', 'logps_train/chosen': '-126.75', 'loss/train': '0.60167', 'examples_per_second': '32.782', 'grad_norm': '27.375', 'counters/examples': 243264, 'counters/updates': 7602}
train stats after 243296 examples: {'rewards_train/chosen': '0.11348', 'rewards_train/rejected': '0.083886', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.029593', 'logps_train/rejected': '-127.87', 'logps_train/chosen': '-152.06', 'loss/train': '0.6897', 'examples_per_second': '31.252', 'grad_norm': '29', 'counters/examples': 243296, 'counters/updates': 7603}
train stats after 243328 examples: {'rewards_train/chosen': '0.17262', 'rewards_train/rejected': '0.14974', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.022878', 'logps_train/rejected': '-104.41', 'logps_train/chosen': '-140.66', 'loss/train': '0.69349', 'examples_per_second': '31.588', 'grad_norm': '27.5', 'counters/examples': 243328, 'counters/updates': 7604}
train stats after 243360 examples: {'rewards_train/chosen': '0.16422', 'rewards_train/rejected': '0.061387', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10283', 'logps_train/rejected': '-129.69', 'logps_train/chosen': '-141.78', 'loss/train': '0.65504', 'examples_per_second': '31.292', 'grad_norm': '30.25', 'counters/examples': 243360, 'counters/updates': 7605}
train stats after 243392 examples: {'rewards_train/chosen': '0.24993', 'rewards_train/rejected': '0.08796', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16197', 'logps_train/rejected': '-140.08', 'logps_train/chosen': '-189.02', 'loss/train': '0.63228', 'examples_per_second': '33.16', 'grad_norm': '28.75', 'counters/examples': 243392, 'counters/updates': 7606}
train stats after 243424 examples: {'rewards_train/chosen': '0.10042', 'rewards_train/rejected': '0.043748', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.056676', 'logps_train/rejected': '-134.03', 'logps_train/chosen': '-128.16', 'loss/train': '0.68637', 'examples_per_second': '31.012', 'grad_norm': '30', 'counters/examples': 243424, 'counters/updates': 7607}
train stats after 243456 examples: {'rewards_train/chosen': '0.19438', 'rewards_train/rejected': '0.055535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13885', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-157.4', 'loss/train': '0.648', 'examples_per_second': '31.514', 'grad_norm': '36.25', 'counters/examples': 243456, 'counters/updates': 7608}
train stats after 243488 examples: {'rewards_train/chosen': '0.11898', 'rewards_train/rejected': '0.050557', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.068424', 'logps_train/rejected': '-107.09', 'logps_train/chosen': '-116.72', 'loss/train': '0.67398', 'examples_per_second': '31.623', 'grad_norm': '28.75', 'counters/examples': 243488, 'counters/updates': 7609}
skipping logging after 243520 examples to avoid logging too frequently
train stats after 243552 examples: {'rewards_train/chosen': '0.14033', 'rewards_train/rejected': '0.10488', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.035451', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-165.67', 'loss/train': '0.69399', 'examples_per_second': '31.025', 'grad_norm': '38', 'counters/examples': 243552, 'counters/updates': 7611}
train stats after 243584 examples: {'rewards_train/chosen': '0.16772', 'rewards_train/rejected': '-0.022081', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1898', 'logps_train/rejected': '-105.03', 'logps_train/chosen': '-128.19', 'loss/train': '0.61309', 'examples_per_second': '31.52', 'grad_norm': '22.375', 'counters/examples': 243584, 'counters/updates': 7612}
train stats after 243616 examples: {'rewards_train/chosen': '0.10799', 'rewards_train/rejected': '0.039657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068337', 'logps_train/rejected': '-119.82', 'logps_train/chosen': '-147.85', 'loss/train': '0.67133', 'examples_per_second': '29.942', 'grad_norm': '28.25', 'counters/examples': 243616, 'counters/updates': 7613}
train stats after 243648 examples: {'rewards_train/chosen': '0.1576', 'rewards_train/rejected': '0.041516', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11609', 'logps_train/rejected': '-131.04', 'logps_train/chosen': '-154.57', 'loss/train': '0.65136', 'examples_per_second': '30.335', 'grad_norm': '41.5', 'counters/examples': 243648, 'counters/updates': 7614}
train stats after 243680 examples: {'rewards_train/chosen': '0.15309', 'rewards_train/rejected': '0.0093301', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14376', 'logps_train/rejected': '-106.44', 'logps_train/chosen': '-145.39', 'loss/train': '0.63498', 'examples_per_second': '31.448', 'grad_norm': '28.75', 'counters/examples': 243680, 'counters/updates': 7615}
train stats after 243712 examples: {'rewards_train/chosen': '0.15993', 'rewards_train/rejected': '0.017439', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14249', 'logps_train/rejected': '-93.797', 'logps_train/chosen': '-127.17', 'loss/train': '0.65058', 'examples_per_second': '32.462', 'grad_norm': '25.625', 'counters/examples': 243712, 'counters/updates': 7616}
train stats after 243744 examples: {'rewards_train/chosen': '0.17432', 'rewards_train/rejected': '0.074186', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10013', 'logps_train/rejected': '-134.12', 'logps_train/chosen': '-156.21', 'loss/train': '0.65946', 'examples_per_second': '30.04', 'grad_norm': '44.75', 'counters/examples': 243744, 'counters/updates': 7617}
train stats after 243776 examples: {'rewards_train/chosen': '0.074349', 'rewards_train/rejected': '0.024408', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.049941', 'logps_train/rejected': '-98.814', 'logps_train/chosen': '-102.2', 'loss/train': '0.67563', 'examples_per_second': '31.503', 'grad_norm': '26.25', 'counters/examples': 243776, 'counters/updates': 7618}
train stats after 243808 examples: {'rewards_train/chosen': '0.11314', 'rewards_train/rejected': '0.060062', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.053076', 'logps_train/rejected': '-118.21', 'logps_train/chosen': '-138.03', 'loss/train': '0.67873', 'examples_per_second': '32.932', 'grad_norm': '28.75', 'counters/examples': 243808, 'counters/updates': 7619}
train stats after 243840 examples: {'rewards_train/chosen': '0.1562', 'rewards_train/rejected': '0.027739', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12846', 'logps_train/rejected': '-149.88', 'logps_train/chosen': '-137.56', 'loss/train': '0.64088', 'examples_per_second': '31.455', 'grad_norm': '30.125', 'counters/examples': 243840, 'counters/updates': 7620}
train stats after 243872 examples: {'rewards_train/chosen': '0.057442', 'rewards_train/rejected': '0.15585', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.098412', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-137.26', 'loss/train': '0.76084', 'examples_per_second': '31.509', 'grad_norm': '35.5', 'counters/examples': 243872, 'counters/updates': 7621}
train stats after 243904 examples: {'rewards_train/chosen': '0.17142', 'rewards_train/rejected': '-0.018583', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19', 'logps_train/rejected': '-136.63', 'logps_train/chosen': '-144.38', 'loss/train': '0.61513', 'examples_per_second': '31.461', 'grad_norm': '26.875', 'counters/examples': 243904, 'counters/updates': 7622}
train stats after 243936 examples: {'rewards_train/chosen': '0.1459', 'rewards_train/rejected': '-0.021546', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16744', 'logps_train/rejected': '-115.47', 'logps_train/chosen': '-130.85', 'loss/train': '0.62593', 'examples_per_second': '32.779', 'grad_norm': '24', 'counters/examples': 243936, 'counters/updates': 7623}
train stats after 243968 examples: {'rewards_train/chosen': '0.20045', 'rewards_train/rejected': '-0.031374', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23182', 'logps_train/rejected': '-125.29', 'logps_train/chosen': '-175.95', 'loss/train': '0.59718', 'examples_per_second': '30.521', 'grad_norm': '28.875', 'counters/examples': 243968, 'counters/updates': 7624}
train stats after 244000 examples: {'rewards_train/chosen': '0.062814', 'rewards_train/rejected': '-1.1961e-05', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.062826', 'logps_train/rejected': '-113.2', 'logps_train/chosen': '-151.02', 'loss/train': '0.68734', 'examples_per_second': '31.451', 'grad_norm': '36.25', 'counters/examples': 244000, 'counters/updates': 7625}
train stats after 244032 examples: {'rewards_train/chosen': '0.24162', 'rewards_train/rejected': '0.089326', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1523', 'logps_train/rejected': '-137.97', 'logps_train/chosen': '-151.36', 'loss/train': '0.64748', 'examples_per_second': '30.255', 'grad_norm': '33', 'counters/examples': 244032, 'counters/updates': 7626}
skipping logging after 244064 examples to avoid logging too frequently
train stats after 244096 examples: {'rewards_train/chosen': '0.18112', 'rewards_train/rejected': '0.15378', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027344', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-148.37', 'loss/train': '0.6894', 'examples_per_second': '30.874', 'grad_norm': '36.5', 'counters/examples': 244096, 'counters/updates': 7628}
train stats after 244128 examples: {'rewards_train/chosen': '0.062234', 'rewards_train/rejected': '-0.030444', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092679', 'logps_train/rejected': '-132.77', 'logps_train/chosen': '-137.63', 'loss/train': '0.66005', 'examples_per_second': '31.552', 'grad_norm': '39.5', 'counters/examples': 244128, 'counters/updates': 7629}
train stats after 244160 examples: {'rewards_train/chosen': '0.12113', 'rewards_train/rejected': '0.0085724', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11256', 'logps_train/rejected': '-106.63', 'logps_train/chosen': '-124.06', 'loss/train': '0.64761', 'examples_per_second': '32.151', 'grad_norm': '23.5', 'counters/examples': 244160, 'counters/updates': 7630}
train stats after 244192 examples: {'rewards_train/chosen': '0.24647', 'rewards_train/rejected': '0.11222', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13424', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-127.8', 'loss/train': '0.64293', 'examples_per_second': '30.685', 'grad_norm': '38.25', 'counters/examples': 244192, 'counters/updates': 7631}
train stats after 244224 examples: {'rewards_train/chosen': '0.10814', 'rewards_train/rejected': '-0.098693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20683', 'logps_train/rejected': '-137.69', 'logps_train/chosen': '-150.26', 'loss/train': '0.63234', 'examples_per_second': '30.514', 'grad_norm': '34.5', 'counters/examples': 244224, 'counters/updates': 7632}
skipping logging after 244256 examples to avoid logging too frequently
train stats after 244288 examples: {'rewards_train/chosen': '0.1707', 'rewards_train/rejected': '0.048056', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12265', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-138.32', 'loss/train': '0.64057', 'examples_per_second': '32.852', 'grad_norm': '25.875', 'counters/examples': 244288, 'counters/updates': 7634}
skipping logging after 244320 examples to avoid logging too frequently
train stats after 244352 examples: {'rewards_train/chosen': '0.14692', 'rewards_train/rejected': '0.012002', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13492', 'logps_train/rejected': '-115.02', 'logps_train/chosen': '-120.72', 'loss/train': '0.65711', 'examples_per_second': '33.589', 'grad_norm': '42.25', 'counters/examples': 244352, 'counters/updates': 7636}
skipping logging after 244384 examples to avoid logging too frequently
train stats after 244416 examples: {'rewards_train/chosen': '0.21031', 'rewards_train/rejected': '0.082716', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1276', 'logps_train/rejected': '-120.32', 'logps_train/chosen': '-148.75', 'loss/train': '0.64668', 'examples_per_second': '30.568', 'grad_norm': '41.25', 'counters/examples': 244416, 'counters/updates': 7638}
train stats after 244448 examples: {'rewards_train/chosen': '0.11452', 'rewards_train/rejected': '0.017759', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.096756', 'logps_train/rejected': '-111.19', 'logps_train/chosen': '-126.49', 'loss/train': '0.6549', 'examples_per_second': '30.019', 'grad_norm': '36.25', 'counters/examples': 244448, 'counters/updates': 7639}
train stats after 244480 examples: {'rewards_train/chosen': '0.11202', 'rewards_train/rejected': '-0.060702', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17272', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-152.39', 'loss/train': '0.62707', 'examples_per_second': '31.583', 'grad_norm': '28', 'counters/examples': 244480, 'counters/updates': 7640}
train stats after 244512 examples: {'rewards_train/chosen': '0.17785', 'rewards_train/rejected': '0.014525', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16332', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-159.38', 'loss/train': '0.63418', 'examples_per_second': '31.909', 'grad_norm': '33.75', 'counters/examples': 244512, 'counters/updates': 7641}
train stats after 244544 examples: {'rewards_train/chosen': '0.1951', 'rewards_train/rejected': '0.045473', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14963', 'logps_train/rejected': '-110.21', 'logps_train/chosen': '-156.42', 'loss/train': '0.63203', 'examples_per_second': '31.473', 'grad_norm': '38.5', 'counters/examples': 244544, 'counters/updates': 7642}
train stats after 244576 examples: {'rewards_train/chosen': '0.14786', 'rewards_train/rejected': '0.08154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066319', 'logps_train/rejected': '-124.94', 'logps_train/chosen': '-134', 'loss/train': '0.67669', 'examples_per_second': '32.724', 'grad_norm': '29.125', 'counters/examples': 244576, 'counters/updates': 7643}
train stats after 244608 examples: {'rewards_train/chosen': '0.15789', 'rewards_train/rejected': '0.058621', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099267', 'logps_train/rejected': '-142.98', 'logps_train/chosen': '-125.92', 'loss/train': '0.66135', 'examples_per_second': '31.291', 'grad_norm': '38.75', 'counters/examples': 244608, 'counters/updates': 7644}
train stats after 244640 examples: {'rewards_train/chosen': '0.22766', 'rewards_train/rejected': '0.036949', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19072', 'logps_train/rejected': '-133.96', 'logps_train/chosen': '-146.15', 'loss/train': '0.61869', 'examples_per_second': '31.115', 'grad_norm': '32.25', 'counters/examples': 244640, 'counters/updates': 7645}
train stats after 244672 examples: {'rewards_train/chosen': '0.232', 'rewards_train/rejected': '0.10588', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12612', 'logps_train/rejected': '-125.87', 'logps_train/chosen': '-126.96', 'loss/train': '0.64617', 'examples_per_second': '32.975', 'grad_norm': '32.75', 'counters/examples': 244672, 'counters/updates': 7646}
train stats after 244704 examples: {'rewards_train/chosen': '0.12035', 'rewards_train/rejected': '0.022739', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.097611', 'logps_train/rejected': '-118.26', 'logps_train/chosen': '-108.28', 'loss/train': '0.65747', 'examples_per_second': '33.296', 'grad_norm': '25', 'counters/examples': 244704, 'counters/updates': 7647}
train stats after 244736 examples: {'rewards_train/chosen': '0.13179', 'rewards_train/rejected': '0.023362', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10843', 'logps_train/rejected': '-112.88', 'logps_train/chosen': '-124.76', 'loss/train': '0.65627', 'examples_per_second': '31.334', 'grad_norm': '27.625', 'counters/examples': 244736, 'counters/updates': 7648}
train stats after 244768 examples: {'rewards_train/chosen': '0.15814', 'rewards_train/rejected': '-0.08422', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.24236', 'logps_train/rejected': '-135.28', 'logps_train/chosen': '-129.56', 'loss/train': '0.60194', 'examples_per_second': '30.866', 'grad_norm': '30.125', 'counters/examples': 244768, 'counters/updates': 7649}
train stats after 244800 examples: {'rewards_train/chosen': '0.19854', 'rewards_train/rejected': '0.028539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17001', 'logps_train/rejected': '-112.54', 'logps_train/chosen': '-126.22', 'loss/train': '0.62894', 'examples_per_second': '32.032', 'grad_norm': '67.5', 'counters/examples': 244800, 'counters/updates': 7650}
train stats after 244832 examples: {'rewards_train/chosen': '0.026235', 'rewards_train/rejected': '-0.046113', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.072348', 'logps_train/rejected': '-158.24', 'logps_train/chosen': '-140.9', 'loss/train': '0.67587', 'examples_per_second': '31.505', 'grad_norm': '31.125', 'counters/examples': 244832, 'counters/updates': 7651}
train stats after 244864 examples: {'rewards_train/chosen': '0.12888', 'rewards_train/rejected': '-0.0087384', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13762', 'logps_train/rejected': '-127.18', 'logps_train/chosen': '-139.74', 'loss/train': '0.6434', 'examples_per_second': '33.222', 'grad_norm': '31.25', 'counters/examples': 244864, 'counters/updates': 7652}
train stats after 244896 examples: {'rewards_train/chosen': '0.17641', 'rewards_train/rejected': '0.092211', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084201', 'logps_train/rejected': '-119.81', 'logps_train/chosen': '-167.18', 'loss/train': '0.67184', 'examples_per_second': '30.491', 'grad_norm': '40.5', 'counters/examples': 244896, 'counters/updates': 7653}
train stats after 244928 examples: {'rewards_train/chosen': '0.14966', 'rewards_train/rejected': '0.048762', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1009', 'logps_train/rejected': '-115.22', 'logps_train/chosen': '-120.28', 'loss/train': '0.65874', 'examples_per_second': '31.25', 'grad_norm': '27.5', 'counters/examples': 244928, 'counters/updates': 7654}
train stats after 244960 examples: {'rewards_train/chosen': '0.14107', 'rewards_train/rejected': '0.064719', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.076352', 'logps_train/rejected': '-144.38', 'logps_train/chosen': '-154.43', 'loss/train': '0.6787', 'examples_per_second': '31.444', 'grad_norm': '71', 'counters/examples': 244960, 'counters/updates': 7655}
train stats after 244992 examples: {'rewards_train/chosen': '0.067101', 'rewards_train/rejected': '-0.027647', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.094748', 'logps_train/rejected': '-94.242', 'logps_train/chosen': '-145.33', 'loss/train': '0.66571', 'examples_per_second': '31.567', 'grad_norm': '28', 'counters/examples': 244992, 'counters/updates': 7656}
train stats after 245024 examples: {'rewards_train/chosen': '0.05351', 'rewards_train/rejected': '0.041815', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.011694', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-150.85', 'loss/train': '0.69832', 'examples_per_second': '32.143', 'grad_norm': '28.625', 'counters/examples': 245024, 'counters/updates': 7657}
train stats after 245056 examples: {'rewards_train/chosen': '0.064733', 'rewards_train/rejected': '0.0057684', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058965', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-145.75', 'loss/train': '0.67869', 'examples_per_second': '33.013', 'grad_norm': '28.5', 'counters/examples': 245056, 'counters/updates': 7658}
train stats after 245088 examples: {'rewards_train/chosen': '0.16914', 'rewards_train/rejected': '0.018141', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.151', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-153.36', 'loss/train': '0.64928', 'examples_per_second': '31.24', 'grad_norm': '38.25', 'counters/examples': 245088, 'counters/updates': 7659}
train stats after 245120 examples: {'rewards_train/chosen': '0.15859', 'rewards_train/rejected': '0.066619', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.091973', 'logps_train/rejected': '-136.35', 'logps_train/chosen': '-162.38', 'loss/train': '0.65778', 'examples_per_second': '30.856', 'grad_norm': '37.5', 'counters/examples': 245120, 'counters/updates': 7660}
train stats after 245152 examples: {'rewards_train/chosen': '0.19152', 'rewards_train/rejected': '0.015144', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17637', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-149.87', 'loss/train': '0.62433', 'examples_per_second': '31.138', 'grad_norm': '27.25', 'counters/examples': 245152, 'counters/updates': 7661}
train stats after 245184 examples: {'rewards_train/chosen': '0.12915', 'rewards_train/rejected': '0.10242', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.026734', 'logps_train/rejected': '-117.16', 'logps_train/chosen': '-121.02', 'loss/train': '0.68911', 'examples_per_second': '30.997', 'grad_norm': '24.25', 'counters/examples': 245184, 'counters/updates': 7662}
skipping logging after 245216 examples to avoid logging too frequently
train stats after 245248 examples: {'rewards_train/chosen': '0.12376', 'rewards_train/rejected': '0.024303', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099456', 'logps_train/rejected': '-147.2', 'logps_train/chosen': '-147.68', 'loss/train': '0.66045', 'examples_per_second': '30.521', 'grad_norm': '31.75', 'counters/examples': 245248, 'counters/updates': 7664}
train stats after 245280 examples: {'rewards_train/chosen': '0.20132', 'rewards_train/rejected': '0.14722', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054093', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-141.15', 'loss/train': '0.68992', 'examples_per_second': '31.301', 'grad_norm': '35', 'counters/examples': 245280, 'counters/updates': 7665}
train stats after 245312 examples: {'rewards_train/chosen': '0.213', 'rewards_train/rejected': '0.020508', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1925', 'logps_train/rejected': '-117.61', 'logps_train/chosen': '-163.9', 'loss/train': '0.61134', 'examples_per_second': '30.614', 'grad_norm': '36', 'counters/examples': 245312, 'counters/updates': 7666}
train stats after 245344 examples: {'rewards_train/chosen': '0.19141', 'rewards_train/rejected': '0.049388', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14203', 'logps_train/rejected': '-112.74', 'logps_train/chosen': '-169.92', 'loss/train': '0.63476', 'examples_per_second': '31.52', 'grad_norm': '27.875', 'counters/examples': 245344, 'counters/updates': 7667}
train stats after 245376 examples: {'rewards_train/chosen': '0.14943', 'rewards_train/rejected': '0.068182', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081248', 'logps_train/rejected': '-136.2', 'logps_train/chosen': '-137.12', 'loss/train': '0.6636', 'examples_per_second': '31.532', 'grad_norm': '27.625', 'counters/examples': 245376, 'counters/updates': 7668}
train stats after 245408 examples: {'rewards_train/chosen': '0.073738', 'rewards_train/rejected': '-0.047937', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12168', 'logps_train/rejected': '-98.843', 'logps_train/chosen': '-129.41', 'loss/train': '0.64392', 'examples_per_second': '31.545', 'grad_norm': '25.375', 'counters/examples': 245408, 'counters/updates': 7669}
train stats after 245440 examples: {'rewards_train/chosen': '0.11499', 'rewards_train/rejected': '-0.017055', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13204', 'logps_train/rejected': '-128.68', 'logps_train/chosen': '-131.49', 'loss/train': '0.64558', 'examples_per_second': '32.876', 'grad_norm': '28.875', 'counters/examples': 245440, 'counters/updates': 7670}
train stats after 245472 examples: {'rewards_train/chosen': '0.10226', 'rewards_train/rejected': '-0.03873', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14099', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-142.98', 'loss/train': '0.63871', 'examples_per_second': '31.528', 'grad_norm': '26.75', 'counters/examples': 245472, 'counters/updates': 7671}
train stats after 245504 examples: {'rewards_train/chosen': '0.10723', 'rewards_train/rejected': '0.016885', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.090343', 'logps_train/rejected': '-127.94', 'logps_train/chosen': '-128.51', 'loss/train': '0.66152', 'examples_per_second': '30.991', 'grad_norm': '24.625', 'counters/examples': 245504, 'counters/updates': 7672}
train stats after 245536 examples: {'rewards_train/chosen': '0.22113', 'rewards_train/rejected': '0.11302', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10811', 'logps_train/rejected': '-147.36', 'logps_train/chosen': '-154.22', 'loss/train': '0.67414', 'examples_per_second': '31.014', 'grad_norm': '42.5', 'counters/examples': 245536, 'counters/updates': 7673}
train stats after 245568 examples: {'rewards_train/chosen': '0.1218', 'rewards_train/rejected': '0.13659', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014786', 'logps_train/rejected': '-158.44', 'logps_train/chosen': '-172.17', 'loss/train': '0.71048', 'examples_per_second': '31.413', 'grad_norm': '38.5', 'counters/examples': 245568, 'counters/updates': 7674}
train stats after 245600 examples: {'rewards_train/chosen': '0.12725', 'rewards_train/rejected': '0.0035113', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12373', 'logps_train/rejected': '-102', 'logps_train/chosen': '-125.37', 'loss/train': '0.64559', 'examples_per_second': '32.104', 'grad_norm': '34.5', 'counters/examples': 245600, 'counters/updates': 7675}
train stats after 245632 examples: {'rewards_train/chosen': '0.16466', 'rewards_train/rejected': '0.16803', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.0033701', 'logps_train/rejected': '-107', 'logps_train/chosen': '-136.32', 'loss/train': '0.70537', 'examples_per_second': '31.508', 'grad_norm': '31.375', 'counters/examples': 245632, 'counters/updates': 7676}
train stats after 245664 examples: {'rewards_train/chosen': '0.33922', 'rewards_train/rejected': '0.2296', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10962', 'logps_train/rejected': '-156.98', 'logps_train/chosen': '-168.24', 'loss/train': '0.65619', 'examples_per_second': '31.549', 'grad_norm': '40', 'counters/examples': 245664, 'counters/updates': 7677}
train stats after 245696 examples: {'rewards_train/chosen': '0.15121', 'rewards_train/rejected': '-0.025787', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.177', 'logps_train/rejected': '-127.46', 'logps_train/chosen': '-153.82', 'loss/train': '0.62641', 'examples_per_second': '31.539', 'grad_norm': '25.75', 'counters/examples': 245696, 'counters/updates': 7678}
train stats after 245728 examples: {'rewards_train/chosen': '0.1814', 'rewards_train/rejected': '0.10114', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08026', 'logps_train/rejected': '-115.83', 'logps_train/chosen': '-165.16', 'loss/train': '0.66958', 'examples_per_second': '31.495', 'grad_norm': '43.75', 'counters/examples': 245728, 'counters/updates': 7679}
train stats after 245760 examples: {'rewards_train/chosen': '0.11988', 'rewards_train/rejected': '-0.03201', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15189', 'logps_train/rejected': '-110.4', 'logps_train/chosen': '-143.71', 'loss/train': '0.63389', 'examples_per_second': '31.993', 'grad_norm': '24', 'counters/examples': 245760, 'counters/updates': 7680}
train stats after 245792 examples: {'rewards_train/chosen': '0.17052', 'rewards_train/rejected': '0.11', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060523', 'logps_train/rejected': '-144.93', 'logps_train/chosen': '-153.41', 'loss/train': '0.68363', 'examples_per_second': '32.292', 'grad_norm': '36.25', 'counters/examples': 245792, 'counters/updates': 7681}
train stats after 245824 examples: {'rewards_train/chosen': '0.092327', 'rewards_train/rejected': '0.11499', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.022664', 'logps_train/rejected': '-130.48', 'logps_train/chosen': '-123.36', 'loss/train': '0.71598', 'examples_per_second': '32.68', 'grad_norm': '74', 'counters/examples': 245824, 'counters/updates': 7682}
train stats after 245856 examples: {'rewards_train/chosen': '0.09081', 'rewards_train/rejected': '0.0010369', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089773', 'logps_train/rejected': '-89.372', 'logps_train/chosen': '-144.39', 'loss/train': '0.67334', 'examples_per_second': '31.473', 'grad_norm': '35', 'counters/examples': 245856, 'counters/updates': 7683}
skipping logging after 245888 examples to avoid logging too frequently
train stats after 245920 examples: {'rewards_train/chosen': '0.07483', 'rewards_train/rejected': '-0.02939', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10422', 'logps_train/rejected': '-98.294', 'logps_train/chosen': '-117.94', 'loss/train': '0.65444', 'examples_per_second': '35.258', 'grad_norm': '30.125', 'counters/examples': 245920, 'counters/updates': 7685}
train stats after 245952 examples: {'rewards_train/chosen': '0.14547', 'rewards_train/rejected': '0.051957', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.093515', 'logps_train/rejected': '-141.29', 'logps_train/chosen': '-151.8', 'loss/train': '0.66578', 'examples_per_second': '31.494', 'grad_norm': '45.25', 'counters/examples': 245952, 'counters/updates': 7686}
train stats after 245984 examples: {'rewards_train/chosen': '0.21441', 'rewards_train/rejected': '0.15637', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.058047', 'logps_train/rejected': '-139.89', 'logps_train/chosen': '-129.38', 'loss/train': '0.67591', 'examples_per_second': '31.542', 'grad_norm': '29', 'counters/examples': 245984, 'counters/updates': 7687}
train stats after 246016 examples: {'rewards_train/chosen': '0.18626', 'rewards_train/rejected': '0.10358', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.082677', 'logps_train/rejected': '-147.75', 'logps_train/chosen': '-132.13', 'loss/train': '0.66753', 'examples_per_second': '32.405', 'grad_norm': '42.75', 'counters/examples': 246016, 'counters/updates': 7688}
skipping logging after 246048 examples to avoid logging too frequently
train stats after 246080 examples: {'rewards_train/chosen': '0.2378', 'rewards_train/rejected': '0.20527', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032536', 'logps_train/rejected': '-161.93', 'logps_train/chosen': '-146.76', 'loss/train': '0.69248', 'examples_per_second': '30.103', 'grad_norm': '48', 'counters/examples': 246080, 'counters/updates': 7690}
train stats after 246112 examples: {'rewards_train/chosen': '0.18444', 'rewards_train/rejected': '0.0065919', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17785', 'logps_train/rejected': '-92.916', 'logps_train/chosen': '-139.93', 'loss/train': '0.61902', 'examples_per_second': '24.263', 'grad_norm': '32', 'counters/examples': 246112, 'counters/updates': 7691}
train stats after 246144 examples: {'rewards_train/chosen': '0.19747', 'rewards_train/rejected': '0.049157', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14831', 'logps_train/rejected': '-101.26', 'logps_train/chosen': '-131.1', 'loss/train': '0.64006', 'examples_per_second': '30.921', 'grad_norm': '30.875', 'counters/examples': 246144, 'counters/updates': 7692}
skipping logging after 246176 examples to avoid logging too frequently
train stats after 246208 examples: {'rewards_train/chosen': '0.16913', 'rewards_train/rejected': '-0.00082339', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16996', 'logps_train/rejected': '-115.84', 'logps_train/chosen': '-138.46', 'loss/train': '0.63377', 'examples_per_second': '24.61', 'grad_norm': '25.5', 'counters/examples': 246208, 'counters/updates': 7694}
train stats after 246240 examples: {'rewards_train/chosen': '0.13851', 'rewards_train/rejected': '0.02292', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11559', 'logps_train/rejected': '-135.06', 'logps_train/chosen': '-109.82', 'loss/train': '0.65836', 'examples_per_second': '32.431', 'grad_norm': '30.75', 'counters/examples': 246240, 'counters/updates': 7695}
train stats after 246272 examples: {'rewards_train/chosen': '0.22181', 'rewards_train/rejected': '-0.0051105', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22692', 'logps_train/rejected': '-90.363', 'logps_train/chosen': '-136.77', 'loss/train': '0.6126', 'examples_per_second': '32.243', 'grad_norm': '23.875', 'counters/examples': 246272, 'counters/updates': 7696}
train stats after 246304 examples: {'rewards_train/chosen': '0.24582', 'rewards_train/rejected': '0.15142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094399', 'logps_train/rejected': '-147.6', 'logps_train/chosen': '-184.87', 'loss/train': '0.67826', 'examples_per_second': '30.679', 'grad_norm': '35.75', 'counters/examples': 246304, 'counters/updates': 7697}
train stats after 246336 examples: {'rewards_train/chosen': '0.23235', 'rewards_train/rejected': '0.035534', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.19682', 'logps_train/rejected': '-113.83', 'logps_train/chosen': '-155.79', 'loss/train': '0.62432', 'examples_per_second': '31.091', 'grad_norm': '36.25', 'counters/examples': 246336, 'counters/updates': 7698}
train stats after 246368 examples: {'rewards_train/chosen': '0.069712', 'rewards_train/rejected': '0.027956', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.041756', 'logps_train/rejected': '-148.67', 'logps_train/chosen': '-111.38', 'loss/train': '0.688', 'examples_per_second': '31.491', 'grad_norm': '27.875', 'counters/examples': 246368, 'counters/updates': 7699}
train stats after 246400 examples: {'rewards_train/chosen': '0.18569', 'rewards_train/rejected': '0.0033889', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18231', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-119.1', 'loss/train': '0.61847', 'examples_per_second': '32.991', 'grad_norm': '24.75', 'counters/examples': 246400, 'counters/updates': 7700}
train stats after 246432 examples: {'rewards_train/chosen': '0.13382', 'rewards_train/rejected': '-0.027205', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16103', 'logps_train/rejected': '-128.57', 'logps_train/chosen': '-168.19', 'loss/train': '0.63908', 'examples_per_second': '30.408', 'grad_norm': '33.5', 'counters/examples': 246432, 'counters/updates': 7701}
skipping logging after 246464 examples to avoid logging too frequently
train stats after 246496 examples: {'rewards_train/chosen': '0.20221', 'rewards_train/rejected': '0.095586', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10662', 'logps_train/rejected': '-105.18', 'logps_train/chosen': '-120.64', 'loss/train': '0.65535', 'examples_per_second': '31.443', 'grad_norm': '23.25', 'counters/examples': 246496, 'counters/updates': 7703}
train stats after 246528 examples: {'rewards_train/chosen': '0.13142', 'rewards_train/rejected': '0.12097', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.010455', 'logps_train/rejected': '-144.85', 'logps_train/chosen': '-130.78', 'loss/train': '0.70496', 'examples_per_second': '31.367', 'grad_norm': '35.5', 'counters/examples': 246528, 'counters/updates': 7704}
train stats after 246560 examples: {'rewards_train/chosen': '0.12733', 'rewards_train/rejected': '0.039978', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08735', 'logps_train/rejected': '-107.29', 'logps_train/chosen': '-102.33', 'loss/train': '0.66096', 'examples_per_second': '30.219', 'grad_norm': '22.25', 'counters/examples': 246560, 'counters/updates': 7705}
train stats after 246592 examples: {'rewards_train/chosen': '0.22138', 'rewards_train/rejected': '0.081482', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1399', 'logps_train/rejected': '-113.9', 'logps_train/chosen': '-125.85', 'loss/train': '0.64242', 'examples_per_second': '31.163', 'grad_norm': '28.875', 'counters/examples': 246592, 'counters/updates': 7706}
train stats after 246624 examples: {'rewards_train/chosen': '0.16069', 'rewards_train/rejected': '-0.0066546', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16734', 'logps_train/rejected': '-101.97', 'logps_train/chosen': '-149.97', 'loss/train': '0.62681', 'examples_per_second': '30.179', 'grad_norm': '37.25', 'counters/examples': 246624, 'counters/updates': 7707}
train stats after 246656 examples: {'rewards_train/chosen': '0.19359', 'rewards_train/rejected': '0.1867', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0068846', 'logps_train/rejected': '-145.04', 'logps_train/chosen': '-132.88', 'loss/train': '0.70235', 'examples_per_second': '30.39', 'grad_norm': '27.25', 'counters/examples': 246656, 'counters/updates': 7708}
train stats after 246688 examples: {'rewards_train/chosen': '0.1242', 'rewards_train/rejected': '0.050256', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073944', 'logps_train/rejected': '-109.56', 'logps_train/chosen': '-110.77', 'loss/train': '0.66959', 'examples_per_second': '31.782', 'grad_norm': '26.75', 'counters/examples': 246688, 'counters/updates': 7709}
skipping logging after 246720 examples to avoid logging too frequently
train stats after 246752 examples: {'rewards_train/chosen': '0.11186', 'rewards_train/rejected': '0.079941', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.031921', 'logps_train/rejected': '-142.33', 'logps_train/chosen': '-167.17', 'loss/train': '0.69114', 'examples_per_second': '31.406', 'grad_norm': '33', 'counters/examples': 246752, 'counters/updates': 7711}
train stats after 246784 examples: {'rewards_train/chosen': '0.10871', 'rewards_train/rejected': '0.062654', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.046054', 'logps_train/rejected': '-114.19', 'logps_train/chosen': '-132.85', 'loss/train': '0.68636', 'examples_per_second': '24.024', 'grad_norm': '26', 'counters/examples': 246784, 'counters/updates': 7712}
train stats after 246816 examples: {'rewards_train/chosen': '0.15596', 'rewards_train/rejected': '0.059519', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096439', 'logps_train/rejected': '-112.38', 'logps_train/chosen': '-113.95', 'loss/train': '0.67945', 'examples_per_second': '32.165', 'grad_norm': '35.25', 'counters/examples': 246816, 'counters/updates': 7713}
train stats after 246848 examples: {'rewards_train/chosen': '0.17717', 'rewards_train/rejected': '0.10708', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070091', 'logps_train/rejected': '-131.44', 'logps_train/chosen': '-133.03', 'loss/train': '0.66948', 'examples_per_second': '31.487', 'grad_norm': '39', 'counters/examples': 246848, 'counters/updates': 7714}
train stats after 246880 examples: {'rewards_train/chosen': '0.16044', 'rewards_train/rejected': '0.10552', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054918', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-145.58', 'loss/train': '0.67841', 'examples_per_second': '31.404', 'grad_norm': '30.375', 'counters/examples': 246880, 'counters/updates': 7715}
skipping logging after 246912 examples to avoid logging too frequently
train stats after 246944 examples: {'rewards_train/chosen': '0.056707', 'rewards_train/rejected': '0.039971', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.016736', 'logps_train/rejected': '-112.4', 'logps_train/chosen': '-139.77', 'loss/train': '0.6925', 'examples_per_second': '31.318', 'grad_norm': '25.375', 'counters/examples': 246944, 'counters/updates': 7717}
train stats after 246976 examples: {'rewards_train/chosen': '0.17432', 'rewards_train/rejected': '0.11431', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.06001', 'logps_train/rejected': '-143.31', 'logps_train/chosen': '-180.73', 'loss/train': '0.68297', 'examples_per_second': '29.906', 'grad_norm': '41.25', 'counters/examples': 246976, 'counters/updates': 7718}
train stats after 247008 examples: {'rewards_train/chosen': '0.15912', 'rewards_train/rejected': '0.085917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073202', 'logps_train/rejected': '-155.42', 'logps_train/chosen': '-146.99', 'loss/train': '0.68077', 'examples_per_second': '31.228', 'grad_norm': '48.25', 'counters/examples': 247008, 'counters/updates': 7719}
skipping logging after 247040 examples to avoid logging too frequently
train stats after 247072 examples: {'rewards_train/chosen': '0.1017', 'rewards_train/rejected': '0.06399', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.037709', 'logps_train/rejected': '-141.13', 'logps_train/chosen': '-107.75', 'loss/train': '0.68765', 'examples_per_second': '30.436', 'grad_norm': '30.375', 'counters/examples': 247072, 'counters/updates': 7721}
train stats after 247104 examples: {'rewards_train/chosen': '0.3041', 'rewards_train/rejected': '0.12661', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17749', 'logps_train/rejected': '-157.87', 'logps_train/chosen': '-180.43', 'loss/train': '0.62384', 'examples_per_second': '29.989', 'grad_norm': '42.25', 'counters/examples': 247104, 'counters/updates': 7722}
train stats after 247136 examples: {'rewards_train/chosen': '0.046376', 'rewards_train/rejected': '0.013107', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03327', 'logps_train/rejected': '-143.16', 'logps_train/chosen': '-116.25', 'loss/train': '0.69337', 'examples_per_second': '31.474', 'grad_norm': '41.5', 'counters/examples': 247136, 'counters/updates': 7723}
train stats after 247168 examples: {'rewards_train/chosen': '0.099936', 'rewards_train/rejected': '0.036282', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.063655', 'logps_train/rejected': '-117.53', 'logps_train/chosen': '-173.58', 'loss/train': '0.67161', 'examples_per_second': '33.07', 'grad_norm': '30.75', 'counters/examples': 247168, 'counters/updates': 7724}
train stats after 247200 examples: {'rewards_train/chosen': '0.068844', 'rewards_train/rejected': '0.11215', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.043301', 'logps_train/rejected': '-161.07', 'logps_train/chosen': '-158.43', 'loss/train': '0.72988', 'examples_per_second': '30.731', 'grad_norm': '33.75', 'counters/examples': 247200, 'counters/updates': 7725}
train stats after 247232 examples: {'rewards_train/chosen': '0.13279', 'rewards_train/rejected': '0.016294', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11649', 'logps_train/rejected': '-111.19', 'logps_train/chosen': '-157.3', 'loss/train': '0.65202', 'examples_per_second': '31.446', 'grad_norm': '27.875', 'counters/examples': 247232, 'counters/updates': 7726}
skipping logging after 247264 examples to avoid logging too frequently
train stats after 247296 examples: {'rewards_train/chosen': '0.13693', 'rewards_train/rejected': '0.10366', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.033271', 'logps_train/rejected': '-163.55', 'logps_train/chosen': '-142.51', 'loss/train': '0.68718', 'examples_per_second': '30.31', 'grad_norm': '36.25', 'counters/examples': 247296, 'counters/updates': 7728}
train stats after 247328 examples: {'rewards_train/chosen': '0.20164', 'rewards_train/rejected': '0.113', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088635', 'logps_train/rejected': '-152.26', 'logps_train/chosen': '-164.02', 'loss/train': '0.66244', 'examples_per_second': '29.976', 'grad_norm': '36.75', 'counters/examples': 247328, 'counters/updates': 7729}
skipping logging after 247360 examples to avoid logging too frequently
train stats after 247392 examples: {'rewards_train/chosen': '0.10757', 'rewards_train/rejected': '0.057189', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050381', 'logps_train/rejected': '-123.72', 'logps_train/chosen': '-175.9', 'loss/train': '0.67904', 'examples_per_second': '31.323', 'grad_norm': '46', 'counters/examples': 247392, 'counters/updates': 7731}
train stats after 247424 examples: {'rewards_train/chosen': '0.22126', 'rewards_train/rejected': '-0.017564', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23882', 'logps_train/rejected': '-110.46', 'logps_train/chosen': '-125.13', 'loss/train': '0.59457', 'examples_per_second': '31.841', 'grad_norm': '27.75', 'counters/examples': 247424, 'counters/updates': 7732}
train stats after 247456 examples: {'rewards_train/chosen': '0.17233', 'rewards_train/rejected': '0.096929', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.075397', 'logps_train/rejected': '-136.42', 'logps_train/chosen': '-125.26', 'loss/train': '0.6728', 'examples_per_second': '30.483', 'grad_norm': '44', 'counters/examples': 247456, 'counters/updates': 7733}
train stats after 247488 examples: {'rewards_train/chosen': '0.14138', 'rewards_train/rejected': '0.030072', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11131', 'logps_train/rejected': '-114.88', 'logps_train/chosen': '-136.07', 'loss/train': '0.66545', 'examples_per_second': '31.834', 'grad_norm': '43.5', 'counters/examples': 247488, 'counters/updates': 7734}
skipping logging after 247520 examples to avoid logging too frequently
train stats after 247552 examples: {'rewards_train/chosen': '0.25916', 'rewards_train/rejected': '0.027441', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.23172', 'logps_train/rejected': '-107.61', 'logps_train/chosen': '-156.34', 'loss/train': '0.60046', 'examples_per_second': '32.427', 'grad_norm': '32.5', 'counters/examples': 247552, 'counters/updates': 7736}
train stats after 247584 examples: {'rewards_train/chosen': '0.23059', 'rewards_train/rejected': '0.0051961', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.22539', 'logps_train/rejected': '-136.91', 'logps_train/chosen': '-133.71', 'loss/train': '0.6005', 'examples_per_second': '32.893', 'grad_norm': '27.25', 'counters/examples': 247584, 'counters/updates': 7737}
train stats after 247616 examples: {'rewards_train/chosen': '0.14659', 'rewards_train/rejected': '0.066822', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.079766', 'logps_train/rejected': '-127.12', 'logps_train/chosen': '-132.48', 'loss/train': '0.67367', 'examples_per_second': '31.117', 'grad_norm': '29.5', 'counters/examples': 247616, 'counters/updates': 7738}
skipping logging after 247648 examples to avoid logging too frequently
train stats after 247680 examples: {'rewards_train/chosen': '0.10404', 'rewards_train/rejected': '-0.030556', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13459', 'logps_train/rejected': '-100.59', 'logps_train/chosen': '-126.14', 'loss/train': '0.64331', 'examples_per_second': '32.248', 'grad_norm': '28.625', 'counters/examples': 247680, 'counters/updates': 7740}
train stats after 247712 examples: {'rewards_train/chosen': '0.14681', 'rewards_train/rejected': '0.047395', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099415', 'logps_train/rejected': '-109.09', 'logps_train/chosen': '-172.4', 'loss/train': '0.66141', 'examples_per_second': '31.065', 'grad_norm': '40', 'counters/examples': 247712, 'counters/updates': 7741}
train stats after 247744 examples: {'rewards_train/chosen': '0.16004', 'rewards_train/rejected': '0.14506', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.01498', 'logps_train/rejected': '-123.64', 'logps_train/chosen': '-120.17', 'loss/train': '0.69674', 'examples_per_second': '32.773', 'grad_norm': '33.5', 'counters/examples': 247744, 'counters/updates': 7742}
train stats after 247776 examples: {'rewards_train/chosen': '0.12723', 'rewards_train/rejected': '0.055152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.072075', 'logps_train/rejected': '-123.82', 'logps_train/chosen': '-115.48', 'loss/train': '0.67045', 'examples_per_second': '33.168', 'grad_norm': '25.75', 'counters/examples': 247776, 'counters/updates': 7743}
train stats after 247808 examples: {'rewards_train/chosen': '0.16143', 'rewards_train/rejected': '-0.021108', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18253', 'logps_train/rejected': '-89.024', 'logps_train/chosen': '-117.69', 'loss/train': '0.62623', 'examples_per_second': '31.38', 'grad_norm': '27.875', 'counters/examples': 247808, 'counters/updates': 7744}
train stats after 247840 examples: {'rewards_train/chosen': '0.11446', 'rewards_train/rejected': '0.058101', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.056355', 'logps_train/rejected': '-96.171', 'logps_train/chosen': '-116.49', 'loss/train': '0.67744', 'examples_per_second': '30.446', 'grad_norm': '25.375', 'counters/examples': 247840, 'counters/updates': 7745}
train stats after 247872 examples: {'rewards_train/chosen': '0.1778', 'rewards_train/rejected': '0.091716', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086081', 'logps_train/rejected': '-111.28', 'logps_train/chosen': '-116.16', 'loss/train': '0.65967', 'examples_per_second': '30.516', 'grad_norm': '27.25', 'counters/examples': 247872, 'counters/updates': 7746}
train stats after 247904 examples: {'rewards_train/chosen': '0.17836', 'rewards_train/rejected': '0.024145', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15422', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-137.2', 'loss/train': '0.63528', 'examples_per_second': '31.518', 'grad_norm': '28', 'counters/examples': 247904, 'counters/updates': 7747}
train stats after 247936 examples: {'rewards_train/chosen': '0.14218', 'rewards_train/rejected': '0.083706', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.058475', 'logps_train/rejected': '-143.67', 'logps_train/chosen': '-156.64', 'loss/train': '0.67867', 'examples_per_second': '32.965', 'grad_norm': '29.875', 'counters/examples': 247936, 'counters/updates': 7748}
train stats after 247968 examples: {'rewards_train/chosen': '0.25771', 'rewards_train/rejected': '0.11068', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14703', 'logps_train/rejected': '-125.46', 'logps_train/chosen': '-143.76', 'loss/train': '0.64495', 'examples_per_second': '30.497', 'grad_norm': '38.5', 'counters/examples': 247968, 'counters/updates': 7749}
train stats after 248000 examples: {'rewards_train/chosen': '0.16014', 'rewards_train/rejected': '0.071043', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089098', 'logps_train/rejected': '-131.49', 'logps_train/chosen': '-117.57', 'loss/train': '0.67139', 'examples_per_second': '31.159', 'grad_norm': '41.25', 'counters/examples': 248000, 'counters/updates': 7750}
train stats after 248032 examples: {'rewards_train/chosen': '0.16052', 'rewards_train/rejected': '-0.030195', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19071', 'logps_train/rejected': '-125.65', 'logps_train/chosen': '-178.46', 'loss/train': '0.61871', 'examples_per_second': '31.074', 'grad_norm': '29.5', 'counters/examples': 248032, 'counters/updates': 7751}
train stats after 248064 examples: {'rewards_train/chosen': '0.15494', 'rewards_train/rejected': '0.063047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.091893', 'logps_train/rejected': '-117.82', 'logps_train/chosen': '-144.89', 'loss/train': '0.65793', 'examples_per_second': '31.476', 'grad_norm': '27.375', 'counters/examples': 248064, 'counters/updates': 7752}
train stats after 248096 examples: {'rewards_train/chosen': '0.14376', 'rewards_train/rejected': '0.033186', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11057', 'logps_train/rejected': '-120.38', 'logps_train/chosen': '-127.04', 'loss/train': '0.64561', 'examples_per_second': '30.274', 'grad_norm': '28.5', 'counters/examples': 248096, 'counters/updates': 7753}
train stats after 248128 examples: {'rewards_train/chosen': '0.15558', 'rewards_train/rejected': '-0.00029988', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15588', 'logps_train/rejected': '-142.25', 'logps_train/chosen': '-180.55', 'loss/train': '0.63469', 'examples_per_second': '31.625', 'grad_norm': '31.625', 'counters/examples': 248128, 'counters/updates': 7754}
train stats after 248160 examples: {'rewards_train/chosen': '0.12287', 'rewards_train/rejected': '0.024952', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097916', 'logps_train/rejected': '-119.58', 'logps_train/chosen': '-127.83', 'loss/train': '0.6646', 'examples_per_second': '30.847', 'grad_norm': '25.625', 'counters/examples': 248160, 'counters/updates': 7755}
train stats after 248192 examples: {'rewards_train/chosen': '0.12134', 'rewards_train/rejected': '0.018685', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10266', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-128.65', 'loss/train': '0.65366', 'examples_per_second': '31.447', 'grad_norm': '39.75', 'counters/examples': 248192, 'counters/updates': 7756}
train stats after 248224 examples: {'rewards_train/chosen': '0.16974', 'rewards_train/rejected': '0.024942', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1448', 'logps_train/rejected': '-100.22', 'logps_train/chosen': '-161.84', 'loss/train': '0.6325', 'examples_per_second': '32.859', 'grad_norm': '25.375', 'counters/examples': 248224, 'counters/updates': 7757}
skipping logging after 248256 examples to avoid logging too frequently
train stats after 248288 examples: {'rewards_train/chosen': '0.20818', 'rewards_train/rejected': '0.054144', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15404', 'logps_train/rejected': '-155.47', 'logps_train/chosen': '-135.37', 'loss/train': '0.64256', 'examples_per_second': '31.423', 'grad_norm': '33.75', 'counters/examples': 248288, 'counters/updates': 7759}
train stats after 248320 examples: {'rewards_train/chosen': '0.24776', 'rewards_train/rejected': '0.010189', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.23757', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-128.29', 'loss/train': '0.60115', 'examples_per_second': '30.446', 'grad_norm': '24.875', 'counters/examples': 248320, 'counters/updates': 7760}
train stats after 248352 examples: {'rewards_train/chosen': '0.15563', 'rewards_train/rejected': '-0.025696', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18133', 'logps_train/rejected': '-120.08', 'logps_train/chosen': '-164.59', 'loss/train': '0.6185', 'examples_per_second': '33.159', 'grad_norm': '27.125', 'counters/examples': 248352, 'counters/updates': 7761}
skipping logging after 248384 examples to avoid logging too frequently
train stats after 248416 examples: {'rewards_train/chosen': '0.11678', 'rewards_train/rejected': '0.082041', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.034735', 'logps_train/rejected': '-135.9', 'logps_train/chosen': '-132.77', 'loss/train': '0.68804', 'examples_per_second': '29.945', 'grad_norm': '32.75', 'counters/examples': 248416, 'counters/updates': 7763}
skipping logging after 248448 examples to avoid logging too frequently
train stats after 248480 examples: {'rewards_train/chosen': '0.21071', 'rewards_train/rejected': '0.032866', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17785', 'logps_train/rejected': '-100.25', 'logps_train/chosen': '-137.99', 'loss/train': '0.62061', 'examples_per_second': '31.495', 'grad_norm': '26.625', 'counters/examples': 248480, 'counters/updates': 7765}
train stats after 248512 examples: {'rewards_train/chosen': '0.15138', 'rewards_train/rejected': '0.026114', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12527', 'logps_train/rejected': '-142.43', 'logps_train/chosen': '-160.51', 'loss/train': '0.64266', 'examples_per_second': '31.001', 'grad_norm': '30.625', 'counters/examples': 248512, 'counters/updates': 7766}
train stats after 248544 examples: {'rewards_train/chosen': '0.2531', 'rewards_train/rejected': '0.14501', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10809', 'logps_train/rejected': '-132.24', 'logps_train/chosen': '-161.06', 'loss/train': '0.65501', 'examples_per_second': '31.335', 'grad_norm': '31', 'counters/examples': 248544, 'counters/updates': 7767}
train stats after 248576 examples: {'rewards_train/chosen': '0.08697', 'rewards_train/rejected': '0.11909', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.032118', 'logps_train/rejected': '-160.37', 'logps_train/chosen': '-158.13', 'loss/train': '0.72668', 'examples_per_second': '30.018', 'grad_norm': '63.25', 'counters/examples': 248576, 'counters/updates': 7768}
train stats after 248608 examples: {'rewards_train/chosen': '0.1725', 'rewards_train/rejected': '0.18776', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.015257', 'logps_train/rejected': '-167.24', 'logps_train/chosen': '-139.58', 'loss/train': '0.73715', 'examples_per_second': '31.325', 'grad_norm': '41.75', 'counters/examples': 248608, 'counters/updates': 7769}
train stats after 248640 examples: {'rewards_train/chosen': '0.17213', 'rewards_train/rejected': '0.071768', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10036', 'logps_train/rejected': '-120.91', 'logps_train/chosen': '-129.56', 'loss/train': '0.65944', 'examples_per_second': '31.214', 'grad_norm': '25.875', 'counters/examples': 248640, 'counters/updates': 7770}
skipping logging after 248672 examples to avoid logging too frequently
train stats after 248704 examples: {'rewards_train/chosen': '0.16819', 'rewards_train/rejected': '0.1547', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.013492', 'logps_train/rejected': '-122.09', 'logps_train/chosen': '-126.51', 'loss/train': '0.70828', 'examples_per_second': '33', 'grad_norm': '39.25', 'counters/examples': 248704, 'counters/updates': 7772}
train stats after 248736 examples: {'rewards_train/chosen': '0.20212', 'rewards_train/rejected': '0.18232', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.019795', 'logps_train/rejected': '-203.49', 'logps_train/chosen': '-178.69', 'loss/train': '0.70637', 'examples_per_second': '31.497', 'grad_norm': '42.25', 'counters/examples': 248736, 'counters/updates': 7773}
train stats after 248768 examples: {'rewards_train/chosen': '0.17687', 'rewards_train/rejected': '0.09901', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077862', 'logps_train/rejected': '-137.81', 'logps_train/chosen': '-135.71', 'loss/train': '0.66751', 'examples_per_second': '31.32', 'grad_norm': '28.875', 'counters/examples': 248768, 'counters/updates': 7774}
skipping logging after 248800 examples to avoid logging too frequently
train stats after 248832 examples: {'rewards_train/chosen': '0.2689', 'rewards_train/rejected': '0.098626', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17028', 'logps_train/rejected': '-136.56', 'logps_train/chosen': '-157.45', 'loss/train': '0.63368', 'examples_per_second': '29.94', 'grad_norm': '41', 'counters/examples': 248832, 'counters/updates': 7776}
train stats after 248864 examples: {'rewards_train/chosen': '0.17115', 'rewards_train/rejected': '0.094172', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.076978', 'logps_train/rejected': '-138.37', 'logps_train/chosen': '-145.55', 'loss/train': '0.66606', 'examples_per_second': '29.968', 'grad_norm': '30.625', 'counters/examples': 248864, 'counters/updates': 7777}
train stats after 248896 examples: {'rewards_train/chosen': '0.14796', 'rewards_train/rejected': '0.10874', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.039214', 'logps_train/rejected': '-116.59', 'logps_train/chosen': '-166.1', 'loss/train': '0.68617', 'examples_per_second': '32.893', 'grad_norm': '41.75', 'counters/examples': 248896, 'counters/updates': 7778}
skipping logging after 248928 examples to avoid logging too frequently
train stats after 248960 examples: {'rewards_train/chosen': '0.12072', 'rewards_train/rejected': '0.12941', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0086818', 'logps_train/rejected': '-123.1', 'logps_train/chosen': '-142.01', 'loss/train': '0.71049', 'examples_per_second': '31.483', 'grad_norm': '47', 'counters/examples': 248960, 'counters/updates': 7780}
train stats after 248992 examples: {'rewards_train/chosen': '0.16467', 'rewards_train/rejected': '-0.0075016', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.17217', 'logps_train/rejected': '-115.32', 'logps_train/chosen': '-218.38', 'loss/train': '0.62458', 'examples_per_second': '32.798', 'grad_norm': '40.75', 'counters/examples': 248992, 'counters/updates': 7781}
skipping logging after 249024 examples to avoid logging too frequently
train stats after 249056 examples: {'rewards_train/chosen': '0.18031', 'rewards_train/rejected': '0.095254', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085057', 'logps_train/rejected': '-95.195', 'logps_train/chosen': '-118.19', 'loss/train': '0.66508', 'examples_per_second': '30.174', 'grad_norm': '26.5', 'counters/examples': 249056, 'counters/updates': 7783}
train stats after 249088 examples: {'rewards_train/chosen': '0.21846', 'rewards_train/rejected': '0.051729', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16674', 'logps_train/rejected': '-127', 'logps_train/chosen': '-159.72', 'loss/train': '0.63624', 'examples_per_second': '31.172', 'grad_norm': '27.125', 'counters/examples': 249088, 'counters/updates': 7784}
train stats after 249120 examples: {'rewards_train/chosen': '0.15286', 'rewards_train/rejected': '0.083814', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.069048', 'logps_train/rejected': '-144.65', 'logps_train/chosen': '-160.27', 'loss/train': '0.68957', 'examples_per_second': '31.432', 'grad_norm': '33.75', 'counters/examples': 249120, 'counters/updates': 7785}
skipping logging after 249152 examples to avoid logging too frequently
train stats after 249184 examples: {'rewards_train/chosen': '0.11815', 'rewards_train/rejected': '0.085814', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.032334', 'logps_train/rejected': '-135.71', 'logps_train/chosen': '-115.89', 'loss/train': '0.69108', 'examples_per_second': '30.682', 'grad_norm': '29.625', 'counters/examples': 249184, 'counters/updates': 7787}
train stats after 249216 examples: {'rewards_train/chosen': '0.20396', 'rewards_train/rejected': '0.10988', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.09408', 'logps_train/rejected': '-109.92', 'logps_train/chosen': '-175.37', 'loss/train': '0.65819', 'examples_per_second': '30.361', 'grad_norm': '44.25', 'counters/examples': 249216, 'counters/updates': 7788}
skipping logging after 249248 examples to avoid logging too frequently
train stats after 249280 examples: {'rewards_train/chosen': '0.21248', 'rewards_train/rejected': '0.10803', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10445', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-159.57', 'loss/train': '0.66272', 'examples_per_second': '29.854', 'grad_norm': '38.5', 'counters/examples': 249280, 'counters/updates': 7790}
train stats after 249312 examples: {'rewards_train/chosen': '0.11239', 'rewards_train/rejected': '0.014316', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.09807', 'logps_train/rejected': '-120.93', 'logps_train/chosen': '-94.16', 'loss/train': '0.65846', 'examples_per_second': '30.712', 'grad_norm': '21.375', 'counters/examples': 249312, 'counters/updates': 7791}
train stats after 249344 examples: {'rewards_train/chosen': '0.22227', 'rewards_train/rejected': '0.055921', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16635', 'logps_train/rejected': '-103.74', 'logps_train/chosen': '-148.28', 'loss/train': '0.6254', 'examples_per_second': '30.191', 'grad_norm': '24.125', 'counters/examples': 249344, 'counters/updates': 7792}
train stats after 249376 examples: {'rewards_train/chosen': '0.1767', 'rewards_train/rejected': '0.093282', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.083415', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-146.79', 'loss/train': '0.6848', 'examples_per_second': '31.174', 'grad_norm': '43.25', 'counters/examples': 249376, 'counters/updates': 7793}
train stats after 249408 examples: {'rewards_train/chosen': '0.17163', 'rewards_train/rejected': '0.063945', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10768', 'logps_train/rejected': '-103.9', 'logps_train/chosen': '-167.74', 'loss/train': '0.65429', 'examples_per_second': '32.019', 'grad_norm': '37.25', 'counters/examples': 249408, 'counters/updates': 7794}
train stats after 249440 examples: {'rewards_train/chosen': '0.16709', 'rewards_train/rejected': '0.065639', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10145', 'logps_train/rejected': '-163.52', 'logps_train/chosen': '-181.2', 'loss/train': '0.66943', 'examples_per_second': '31.198', 'grad_norm': '35', 'counters/examples': 249440, 'counters/updates': 7795}
skipping logging after 249472 examples to avoid logging too frequently
train stats after 249504 examples: {'rewards_train/chosen': '0.057709', 'rewards_train/rejected': '0.01986', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.03785', 'logps_train/rejected': '-89.923', 'logps_train/chosen': '-112.64', 'loss/train': '0.68679', 'examples_per_second': '40.145', 'grad_norm': '22.75', 'counters/examples': 249504, 'counters/updates': 7797}
train stats after 249536 examples: {'rewards_train/chosen': '0.14695', 'rewards_train/rejected': '0.060478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.086476', 'logps_train/rejected': '-103.66', 'logps_train/chosen': '-110.46', 'loss/train': '0.65604', 'examples_per_second': '30.928', 'grad_norm': '23.75', 'counters/examples': 249536, 'counters/updates': 7798}
train stats after 249568 examples: {'rewards_train/chosen': '0.13395', 'rewards_train/rejected': '0.0046017', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12935', 'logps_train/rejected': '-117.5', 'logps_train/chosen': '-129.49', 'loss/train': '0.64675', 'examples_per_second': '31.477', 'grad_norm': '27.875', 'counters/examples': 249568, 'counters/updates': 7799}
train stats after 249600 examples: {'rewards_train/chosen': '0.25496', 'rewards_train/rejected': '0.053139', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20182', 'logps_train/rejected': '-141.51', 'logps_train/chosen': '-151.13', 'loss/train': '0.62026', 'examples_per_second': '33.189', 'grad_norm': '32.25', 'counters/examples': 249600, 'counters/updates': 7800}
train stats after 249632 examples: {'rewards_train/chosen': '0.24323', 'rewards_train/rejected': '0.17798', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.065255', 'logps_train/rejected': '-160.08', 'logps_train/chosen': '-214.99', 'loss/train': '0.68155', 'examples_per_second': '31.532', 'grad_norm': '87.5', 'counters/examples': 249632, 'counters/updates': 7801}
train stats after 249664 examples: {'rewards_train/chosen': '0.2634', 'rewards_train/rejected': '0.080982', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18242', 'logps_train/rejected': '-177.53', 'logps_train/chosen': '-140.38', 'loss/train': '0.61916', 'examples_per_second': '31.49', 'grad_norm': '29.875', 'counters/examples': 249664, 'counters/updates': 7802}
train stats after 249696 examples: {'rewards_train/chosen': '0.21261', 'rewards_train/rejected': '0.089081', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12353', 'logps_train/rejected': '-117.2', 'logps_train/chosen': '-146.86', 'loss/train': '0.652', 'examples_per_second': '32.316', 'grad_norm': '31.5', 'counters/examples': 249696, 'counters/updates': 7803}
train stats after 249728 examples: {'rewards_train/chosen': '0.19366', 'rewards_train/rejected': '0.0097695', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18389', 'logps_train/rejected': '-97.58', 'logps_train/chosen': '-133.6', 'loss/train': '0.61891', 'examples_per_second': '30.412', 'grad_norm': '26.25', 'counters/examples': 249728, 'counters/updates': 7804}
skipping logging after 249760 examples to avoid logging too frequently
train stats after 249792 examples: {'rewards_train/chosen': '0.21338', 'rewards_train/rejected': '0.11254', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10084', 'logps_train/rejected': '-108.51', 'logps_train/chosen': '-112.77', 'loss/train': '0.65719', 'examples_per_second': '30.546', 'grad_norm': '29.375', 'counters/examples': 249792, 'counters/updates': 7806}
train stats after 249824 examples: {'rewards_train/chosen': '0.16876', 'rewards_train/rejected': '0.056131', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11263', 'logps_train/rejected': '-152.8', 'logps_train/chosen': '-154.57', 'loss/train': '0.65003', 'examples_per_second': '31.432', 'grad_norm': '33.25', 'counters/examples': 249824, 'counters/updates': 7807}
train stats after 249856 examples: {'rewards_train/chosen': '0.16829', 'rewards_train/rejected': '0.086753', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081542', 'logps_train/rejected': '-122.32', 'logps_train/chosen': '-129.63', 'loss/train': '0.67249', 'examples_per_second': '32.748', 'grad_norm': '35.75', 'counters/examples': 249856, 'counters/updates': 7808}
train stats after 249888 examples: {'rewards_train/chosen': '0.073759', 'rewards_train/rejected': '0.062929', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.01083', 'logps_train/rejected': '-93.588', 'logps_train/chosen': '-113.29', 'loss/train': '0.71049', 'examples_per_second': '31.473', 'grad_norm': '34', 'counters/examples': 249888, 'counters/updates': 7809}
train stats after 249920 examples: {'rewards_train/chosen': '0.13542', 'rewards_train/rejected': '0.0080687', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12735', 'logps_train/rejected': '-118.35', 'logps_train/chosen': '-145.28', 'loss/train': '0.64222', 'examples_per_second': '30.485', 'grad_norm': '26.75', 'counters/examples': 249920, 'counters/updates': 7810}
train stats after 249952 examples: {'rewards_train/chosen': '0.16285', 'rewards_train/rejected': '0.04949', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11336', 'logps_train/rejected': '-99.93', 'logps_train/chosen': '-125.07', 'loss/train': '0.65265', 'examples_per_second': '31.355', 'grad_norm': '30', 'counters/examples': 249952, 'counters/updates': 7811}
train stats after 249984 examples: {'rewards_train/chosen': '0.15137', 'rewards_train/rejected': '0.14492', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.006448', 'logps_train/rejected': '-149.1', 'logps_train/chosen': '-154.95', 'loss/train': '0.71478', 'examples_per_second': '29.951', 'grad_norm': '33.5', 'counters/examples': 249984, 'counters/updates': 7812}
skipping logging after 250016 examples to avoid logging too frequently
train stats after 250048 examples: {'rewards_train/chosen': '0.24513', 'rewards_train/rejected': '0.066003', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17913', 'logps_train/rejected': '-137.4', 'logps_train/chosen': '-149.44', 'loss/train': '0.61911', 'examples_per_second': '30.989', 'grad_norm': '30.25', 'counters/examples': 250048, 'counters/updates': 7814}
train stats after 250080 examples: {'rewards_train/chosen': '0.073', 'rewards_train/rejected': '-0.014643', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087644', 'logps_train/rejected': '-116.76', 'logps_train/chosen': '-116.89', 'loss/train': '0.66155', 'examples_per_second': '32.609', 'grad_norm': '25.375', 'counters/examples': 250080, 'counters/updates': 7815}
skipping logging after 250112 examples to avoid logging too frequently
train stats after 250144 examples: {'rewards_train/chosen': '0.22588', 'rewards_train/rejected': '0.10558', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1203', 'logps_train/rejected': '-134.08', 'logps_train/chosen': '-169.35', 'loss/train': '0.64585', 'examples_per_second': '32.755', 'grad_norm': '29.625', 'counters/examples': 250144, 'counters/updates': 7817}
train stats after 250176 examples: {'rewards_train/chosen': '0.24445', 'rewards_train/rejected': '0.10199', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14246', 'logps_train/rejected': '-128.84', 'logps_train/chosen': '-139.12', 'loss/train': '0.63826', 'examples_per_second': '29.936', 'grad_norm': '29.125', 'counters/examples': 250176, 'counters/updates': 7818}
skipping logging after 250208 examples to avoid logging too frequently
train stats after 250240 examples: {'rewards_train/chosen': '0.11069', 'rewards_train/rejected': '0.032835', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077855', 'logps_train/rejected': '-98.142', 'logps_train/chosen': '-137.91', 'loss/train': '0.66827', 'examples_per_second': '33.322', 'grad_norm': '42.75', 'counters/examples': 250240, 'counters/updates': 7820}
train stats after 250272 examples: {'rewards_train/chosen': '0.15412', 'rewards_train/rejected': '-0.072784', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2269', 'logps_train/rejected': '-101.89', 'logps_train/chosen': '-157.07', 'loss/train': '0.60087', 'examples_per_second': '32.159', 'grad_norm': '34.25', 'counters/examples': 250272, 'counters/updates': 7821}
train stats after 250304 examples: {'rewards_train/chosen': '0.11024', 'rewards_train/rejected': '0.019626', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090615', 'logps_train/rejected': '-110.91', 'logps_train/chosen': '-117.95', 'loss/train': '0.66421', 'examples_per_second': '32.616', 'grad_norm': '27', 'counters/examples': 250304, 'counters/updates': 7822}
train stats after 250336 examples: {'rewards_train/chosen': '0.12923', 'rewards_train/rejected': '-0.016313', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14554', 'logps_train/rejected': '-106.38', 'logps_train/chosen': '-141.24', 'loss/train': '0.6376', 'examples_per_second': '31.15', 'grad_norm': '36.5', 'counters/examples': 250336, 'counters/updates': 7823}
train stats after 250368 examples: {'rewards_train/chosen': '0.10613', 'rewards_train/rejected': '0.037643', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068488', 'logps_train/rejected': '-120.47', 'logps_train/chosen': '-126.96', 'loss/train': '0.67305', 'examples_per_second': '31.497', 'grad_norm': '32.75', 'counters/examples': 250368, 'counters/updates': 7824}
skipping logging after 250400 examples to avoid logging too frequently
train stats after 250432 examples: {'rewards_train/chosen': '0.17664', 'rewards_train/rejected': '0.14503', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.031612', 'logps_train/rejected': '-188.13', 'logps_train/chosen': '-151.09', 'loss/train': '0.69605', 'examples_per_second': '31.57', 'grad_norm': '33.25', 'counters/examples': 250432, 'counters/updates': 7826}
train stats after 250464 examples: {'rewards_train/chosen': '0.21077', 'rewards_train/rejected': '0.16282', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.047952', 'logps_train/rejected': '-175.55', 'logps_train/chosen': '-156.83', 'loss/train': '0.69267', 'examples_per_second': '29.964', 'grad_norm': '34.5', 'counters/examples': 250464, 'counters/updates': 7827}
skipping logging after 250496 examples to avoid logging too frequently
train stats after 250528 examples: {'rewards_train/chosen': '0.12605', 'rewards_train/rejected': '0.056667', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069388', 'logps_train/rejected': '-105.18', 'logps_train/chosen': '-127.16', 'loss/train': '0.67121', 'examples_per_second': '31.241', 'grad_norm': '23.625', 'counters/examples': 250528, 'counters/updates': 7829}
train stats after 250560 examples: {'rewards_train/chosen': '0.1289', 'rewards_train/rejected': '-0.038429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16733', 'logps_train/rejected': '-106.41', 'logps_train/chosen': '-112.5', 'loss/train': '0.62932', 'examples_per_second': '32.588', 'grad_norm': '26.75', 'counters/examples': 250560, 'counters/updates': 7830}
train stats after 250592 examples: {'rewards_train/chosen': '0.065096', 'rewards_train/rejected': '0.00023322', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.064863', 'logps_train/rejected': '-133.9', 'logps_train/chosen': '-131.25', 'loss/train': '0.67055', 'examples_per_second': '30.877', 'grad_norm': '31.125', 'counters/examples': 250592, 'counters/updates': 7831}
train stats after 250624 examples: {'rewards_train/chosen': '0.17529', 'rewards_train/rejected': '0.089881', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.085407', 'logps_train/rejected': '-129.17', 'logps_train/chosen': '-143.21', 'loss/train': '0.65882', 'examples_per_second': '31.475', 'grad_norm': '31.625', 'counters/examples': 250624, 'counters/updates': 7832}
train stats after 250656 examples: {'rewards_train/chosen': '0.19193', 'rewards_train/rejected': '-0.010519', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20245', 'logps_train/rejected': '-107.42', 'logps_train/chosen': '-139.19', 'loss/train': '0.61054', 'examples_per_second': '32.76', 'grad_norm': '25', 'counters/examples': 250656, 'counters/updates': 7833}
train stats after 250688 examples: {'rewards_train/chosen': '0.2263', 'rewards_train/rejected': '0.019744', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20656', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-176.77', 'loss/train': '0.60707', 'examples_per_second': '31.735', 'grad_norm': '34', 'counters/examples': 250688, 'counters/updates': 7834}
train stats after 250720 examples: {'rewards_train/chosen': '0.27871', 'rewards_train/rejected': '0.071321', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20738', 'logps_train/rejected': '-126.89', 'logps_train/chosen': '-161.5', 'loss/train': '0.6372', 'examples_per_second': '31.018', 'grad_norm': '32.5', 'counters/examples': 250720, 'counters/updates': 7835}
train stats after 250752 examples: {'rewards_train/chosen': '0.16741', 'rewards_train/rejected': '0.024432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14297', 'logps_train/rejected': '-110.74', 'logps_train/chosen': '-115.16', 'loss/train': '0.63249', 'examples_per_second': '31.784', 'grad_norm': '25.125', 'counters/examples': 250752, 'counters/updates': 7836}
train stats after 250784 examples: {'rewards_train/chosen': '0.25911', 'rewards_train/rejected': '0.050462', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20865', 'logps_train/rejected': '-115.37', 'logps_train/chosen': '-128.25', 'loss/train': '0.60228', 'examples_per_second': '31.519', 'grad_norm': '21.75', 'counters/examples': 250784, 'counters/updates': 7837}
train stats after 250816 examples: {'rewards_train/chosen': '0.32364', 'rewards_train/rejected': '0.13694', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1867', 'logps_train/rejected': '-132.74', 'logps_train/chosen': '-144.88', 'loss/train': '0.63369', 'examples_per_second': '30.757', 'grad_norm': '30.25', 'counters/examples': 250816, 'counters/updates': 7838}
train stats after 250848 examples: {'rewards_train/chosen': '0.2278', 'rewards_train/rejected': '0.0033845', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22441', 'logps_train/rejected': '-131.76', 'logps_train/chosen': '-132.41', 'loss/train': '0.62318', 'examples_per_second': '30.49', 'grad_norm': '24.75', 'counters/examples': 250848, 'counters/updates': 7839}
skipping logging after 250880 examples to avoid logging too frequently
train stats after 250912 examples: {'rewards_train/chosen': '0.077555', 'rewards_train/rejected': '0.063967', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.013588', 'logps_train/rejected': '-155.29', 'logps_train/chosen': '-161.36', 'loss/train': '0.70038', 'examples_per_second': '29.957', 'grad_norm': '35.5', 'counters/examples': 250912, 'counters/updates': 7841}
train stats after 250944 examples: {'rewards_train/chosen': '0.11739', 'rewards_train/rejected': '0.052351', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.065038', 'logps_train/rejected': '-97.118', 'logps_train/chosen': '-113.42', 'loss/train': '0.6767', 'examples_per_second': '30.276', 'grad_norm': '41.5', 'counters/examples': 250944, 'counters/updates': 7842}
train stats after 250976 examples: {'rewards_train/chosen': '0.24033', 'rewards_train/rejected': '0.061712', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17861', 'logps_train/rejected': '-108.83', 'logps_train/chosen': '-145.56', 'loss/train': '0.62595', 'examples_per_second': '31.726', 'grad_norm': '28.625', 'counters/examples': 250976, 'counters/updates': 7843}
train stats after 251008 examples: {'rewards_train/chosen': '0.14767', 'rewards_train/rejected': '0.10801', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039655', 'logps_train/rejected': '-157.64', 'logps_train/chosen': '-152.58', 'loss/train': '0.69685', 'examples_per_second': '31.548', 'grad_norm': '42.5', 'counters/examples': 251008, 'counters/updates': 7844}
skipping logging after 251040 examples to avoid logging too frequently
train stats after 251072 examples: {'rewards_train/chosen': '0.062045', 'rewards_train/rejected': '-0.042201', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10425', 'logps_train/rejected': '-97.901', 'logps_train/chosen': '-109.63', 'loss/train': '0.64985', 'examples_per_second': '34.739', 'grad_norm': '28.875', 'counters/examples': 251072, 'counters/updates': 7846}
train stats after 251104 examples: {'rewards_train/chosen': '0.18294', 'rewards_train/rejected': '0.060547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12239', 'logps_train/rejected': '-154.16', 'logps_train/chosen': '-131.68', 'loss/train': '0.65953', 'examples_per_second': '33.146', 'grad_norm': '40.5', 'counters/examples': 251104, 'counters/updates': 7847}
train stats after 251136 examples: {'rewards_train/chosen': '0.2382', 'rewards_train/rejected': '0.14147', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.09673', 'logps_train/rejected': '-129.35', 'logps_train/chosen': '-156.8', 'loss/train': '0.65687', 'examples_per_second': '31.499', 'grad_norm': '30.5', 'counters/examples': 251136, 'counters/updates': 7848}
train stats after 251168 examples: {'rewards_train/chosen': '0.16414', 'rewards_train/rejected': '0.087257', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076881', 'logps_train/rejected': '-116.33', 'logps_train/chosen': '-114.95', 'loss/train': '0.66585', 'examples_per_second': '31.706', 'grad_norm': '25.625', 'counters/examples': 251168, 'counters/updates': 7849}
skipping logging after 251200 examples to avoid logging too frequently
train stats after 251232 examples: {'rewards_train/chosen': '0.17631', 'rewards_train/rejected': '0.059047', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11726', 'logps_train/rejected': '-116.3', 'logps_train/chosen': '-159.97', 'loss/train': '0.6502', 'examples_per_second': '33.132', 'grad_norm': '33.25', 'counters/examples': 251232, 'counters/updates': 7851}
train stats after 251264 examples: {'rewards_train/chosen': '0.15606', 'rewards_train/rejected': '0.041799', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11426', 'logps_train/rejected': '-114.37', 'logps_train/chosen': '-151.04', 'loss/train': '0.6516', 'examples_per_second': '31.448', 'grad_norm': '24.75', 'counters/examples': 251264, 'counters/updates': 7852}
train stats after 251296 examples: {'rewards_train/chosen': '0.13204', 'rewards_train/rejected': '-0.053816', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18586', 'logps_train/rejected': '-125.17', 'logps_train/chosen': '-128.02', 'loss/train': '0.61876', 'examples_per_second': '31.294', 'grad_norm': '31.125', 'counters/examples': 251296, 'counters/updates': 7853}
train stats after 251328 examples: {'rewards_train/chosen': '0.2719', 'rewards_train/rejected': '0.050453', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22144', 'logps_train/rejected': '-129.41', 'logps_train/chosen': '-157.01', 'loss/train': '0.60029', 'examples_per_second': '31.542', 'grad_norm': '30', 'counters/examples': 251328, 'counters/updates': 7854}
train stats after 251360 examples: {'rewards_train/chosen': '0.19701', 'rewards_train/rejected': '0.038627', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15839', 'logps_train/rejected': '-122.34', 'logps_train/chosen': '-126.84', 'loss/train': '0.64189', 'examples_per_second': '31.673', 'grad_norm': '27', 'counters/examples': 251360, 'counters/updates': 7855}
train stats after 251392 examples: {'rewards_train/chosen': '0.13801', 'rewards_train/rejected': '0.069722', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.068289', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-127.75', 'loss/train': '0.66834', 'examples_per_second': '31.521', 'grad_norm': '43', 'counters/examples': 251392, 'counters/updates': 7856}
skipping logging after 251424 examples to avoid logging too frequently
train stats after 251456 examples: {'rewards_train/chosen': '0.20965', 'rewards_train/rejected': '0.10228', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10737', 'logps_train/rejected': '-157.93', 'logps_train/chosen': '-146.19', 'loss/train': '0.65629', 'examples_per_second': '30.759', 'grad_norm': '33', 'counters/examples': 251456, 'counters/updates': 7858}
train stats after 251488 examples: {'rewards_train/chosen': '0.16931', 'rewards_train/rejected': '0.068142', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10116', 'logps_train/rejected': '-112.25', 'logps_train/chosen': '-130.32', 'loss/train': '0.65933', 'examples_per_second': '32.023', 'grad_norm': '27.625', 'counters/examples': 251488, 'counters/updates': 7859}
train stats after 251520 examples: {'rewards_train/chosen': '0.16212', 'rewards_train/rejected': '-0.086987', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24911', 'logps_train/rejected': '-133.76', 'logps_train/chosen': '-134.58', 'loss/train': '0.58991', 'examples_per_second': '30.639', 'grad_norm': '28.125', 'counters/examples': 251520, 'counters/updates': 7860}
train stats after 251552 examples: {'rewards_train/chosen': '0.19253', 'rewards_train/rejected': '0.13822', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.054313', 'logps_train/rejected': '-154.61', 'logps_train/chosen': '-141.45', 'loss/train': '0.68804', 'examples_per_second': '31.245', 'grad_norm': '32.5', 'counters/examples': 251552, 'counters/updates': 7861}
train stats after 251584 examples: {'rewards_train/chosen': '0.14603', 'rewards_train/rejected': '0.1001', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.045936', 'logps_train/rejected': '-149.54', 'logps_train/chosen': '-196.98', 'loss/train': '0.68453', 'examples_per_second': '23.949', 'grad_norm': '42.75', 'counters/examples': 251584, 'counters/updates': 7862}
train stats after 251616 examples: {'rewards_train/chosen': '0.20383', 'rewards_train/rejected': '0.078736', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12509', 'logps_train/rejected': '-125.07', 'logps_train/chosen': '-170.64', 'loss/train': '0.64706', 'examples_per_second': '31.538', 'grad_norm': '30.5', 'counters/examples': 251616, 'counters/updates': 7863}
train stats after 251648 examples: {'rewards_train/chosen': '0.10031', 'rewards_train/rejected': '0.018374', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081934', 'logps_train/rejected': '-123.56', 'logps_train/chosen': '-143.96', 'loss/train': '0.66693', 'examples_per_second': '31.436', 'grad_norm': '27.5', 'counters/examples': 251648, 'counters/updates': 7864}
train stats after 251680 examples: {'rewards_train/chosen': '0.1673', 'rewards_train/rejected': '0.02968', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13762', 'logps_train/rejected': '-102.16', 'logps_train/chosen': '-138.61', 'loss/train': '0.64419', 'examples_per_second': '24.265', 'grad_norm': '25.375', 'counters/examples': 251680, 'counters/updates': 7865}
train stats after 251712 examples: {'rewards_train/chosen': '0.14642', 'rewards_train/rejected': '0.062818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083602', 'logps_train/rejected': '-125.31', 'logps_train/chosen': '-162.7', 'loss/train': '0.66674', 'examples_per_second': '30.052', 'grad_norm': '34.5', 'counters/examples': 251712, 'counters/updates': 7866}
train stats after 251744 examples: {'rewards_train/chosen': '0.10355', 'rewards_train/rejected': '0.16288', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059327', 'logps_train/rejected': '-173.45', 'logps_train/chosen': '-163.15', 'loss/train': '0.75161', 'examples_per_second': '31.481', 'grad_norm': '48.5', 'counters/examples': 251744, 'counters/updates': 7867}
train stats after 251776 examples: {'rewards_train/chosen': '0.25451', 'rewards_train/rejected': '0.15371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1008', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-173.99', 'loss/train': '0.66253', 'examples_per_second': '31.516', 'grad_norm': '39.75', 'counters/examples': 251776, 'counters/updates': 7868}
skipping logging after 251808 examples to avoid logging too frequently
train stats after 251840 examples: {'rewards_train/chosen': '0.098518', 'rewards_train/rejected': '-0.030905', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12942', 'logps_train/rejected': '-103.42', 'logps_train/chosen': '-148.56', 'loss/train': '0.64612', 'examples_per_second': '31.551', 'grad_norm': '27.625', 'counters/examples': 251840, 'counters/updates': 7870}
skipping logging after 251872 examples to avoid logging too frequently
train stats after 251904 examples: {'rewards_train/chosen': '0.2399', 'rewards_train/rejected': '0.094607', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14529', 'logps_train/rejected': '-132.2', 'logps_train/chosen': '-136.64', 'loss/train': '0.63386', 'examples_per_second': '31.341', 'grad_norm': '36.5', 'counters/examples': 251904, 'counters/updates': 7872}
train stats after 251936 examples: {'rewards_train/chosen': '0.34454', 'rewards_train/rejected': '0.029157', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.31538', 'logps_train/rejected': '-131.09', 'logps_train/chosen': '-158.36', 'loss/train': '0.60921', 'examples_per_second': '31.568', 'grad_norm': '27.375', 'counters/examples': 251936, 'counters/updates': 7873}
skipping logging after 251968 examples to avoid logging too frequently
train stats after 252000 examples: {'rewards_train/chosen': '0.1576', 'rewards_train/rejected': '0.046456', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11114', 'logps_train/rejected': '-160.59', 'logps_train/chosen': '-153.98', 'loss/train': '0.65541', 'examples_per_second': '30.406', 'grad_norm': '42.75', 'counters/examples': 252000, 'counters/updates': 7875}
Running evaluation after 252000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.25it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.00it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.90it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.83it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 252000: {'rewards_eval/chosen': '0.17899', 'rewards_eval/rejected': '0.074619', 'rewards_eval/accuracies': '0.59766', 'rewards_eval/margins': '0.10437', 'logps_eval/rejected': '-121.39', 'logps_eval/chosen': '-142.32', 'loss/eval': '0.65903'}
skipping save for non epoch
train stats after 252032 examples: {'rewards_train/chosen': '0.13341', 'rewards_train/rejected': '0.059891', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073516', 'logps_train/rejected': '-147.34', 'logps_train/chosen': '-142.75', 'loss/train': '0.66733', 'examples_per_second': '32.624', 'grad_norm': '26.5', 'counters/examples': 252032, 'counters/updates': 7876}
train stats after 252064 examples: {'rewards_train/chosen': '0.20248', 'rewards_train/rejected': '0.16245', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040026', 'logps_train/rejected': '-123.7', 'logps_train/chosen': '-131.99', 'loss/train': '0.68257', 'examples_per_second': '31.446', 'grad_norm': '28.875', 'counters/examples': 252064, 'counters/updates': 7877}
train stats after 252096 examples: {'rewards_train/chosen': '0.14673', 'rewards_train/rejected': '0.09203', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.054703', 'logps_train/rejected': '-135.6', 'logps_train/chosen': '-155.42', 'loss/train': '0.70014', 'examples_per_second': '30.519', 'grad_norm': '59', 'counters/examples': 252096, 'counters/updates': 7878}
train stats after 252128 examples: {'rewards_train/chosen': '0.25261', 'rewards_train/rejected': '0.16454', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.088068', 'logps_train/rejected': '-145.88', 'logps_train/chosen': '-152.63', 'loss/train': '0.67214', 'examples_per_second': '30.888', 'grad_norm': '30', 'counters/examples': 252128, 'counters/updates': 7879}
train stats after 252160 examples: {'rewards_train/chosen': '0.15663', 'rewards_train/rejected': '0.017638', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13899', 'logps_train/rejected': '-110.7', 'logps_train/chosen': '-133.67', 'loss/train': '0.63863', 'examples_per_second': '31.533', 'grad_norm': '28.125', 'counters/examples': 252160, 'counters/updates': 7880}
skipping logging after 252192 examples to avoid logging too frequently
train stats after 252224 examples: {'rewards_train/chosen': '0.17163', 'rewards_train/rejected': '0.011521', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16011', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-135.05', 'loss/train': '0.64509', 'examples_per_second': '30.686', 'grad_norm': '39.25', 'counters/examples': 252224, 'counters/updates': 7882}
skipping logging after 252256 examples to avoid logging too frequently
train stats after 252288 examples: {'rewards_train/chosen': '0.13951', 'rewards_train/rejected': '0.031576', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10793', 'logps_train/rejected': '-132.83', 'logps_train/chosen': '-123.37', 'loss/train': '0.6572', 'examples_per_second': '32.001', 'grad_norm': '32', 'counters/examples': 252288, 'counters/updates': 7884}
train stats after 252320 examples: {'rewards_train/chosen': '0.10422', 'rewards_train/rejected': '-0.011494', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11571', 'logps_train/rejected': '-108.08', 'logps_train/chosen': '-116.69', 'loss/train': '0.64343', 'examples_per_second': '30.894', 'grad_norm': '24.375', 'counters/examples': 252320, 'counters/updates': 7885}
train stats after 252352 examples: {'rewards_train/chosen': '0.14022', 'rewards_train/rejected': '0.0048514', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13537', 'logps_train/rejected': '-140', 'logps_train/chosen': '-157.16', 'loss/train': '0.6344', 'examples_per_second': '29.987', 'grad_norm': '30.875', 'counters/examples': 252352, 'counters/updates': 7886}
train stats after 252384 examples: {'rewards_train/chosen': '0.15457', 'rewards_train/rejected': '0.092718', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.061847', 'logps_train/rejected': '-129.3', 'logps_train/chosen': '-161.56', 'loss/train': '0.67502', 'examples_per_second': '25.285', 'grad_norm': '31.125', 'counters/examples': 252384, 'counters/updates': 7887}
train stats after 252416 examples: {'rewards_train/chosen': '0.13227', 'rewards_train/rejected': '0.024878', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.10739', 'logps_train/rejected': '-133.5', 'logps_train/chosen': '-166.32', 'loss/train': '0.66041', 'examples_per_second': '31.632', 'grad_norm': '52.25', 'counters/examples': 252416, 'counters/updates': 7888}
train stats after 252448 examples: {'rewards_train/chosen': '0.25846', 'rewards_train/rejected': '-0.018873', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.27733', 'logps_train/rejected': '-100.94', 'logps_train/chosen': '-170.46', 'loss/train': '0.58382', 'examples_per_second': '30.328', 'grad_norm': '30.75', 'counters/examples': 252448, 'counters/updates': 7889}
skipping logging after 252480 examples to avoid logging too frequently
train stats after 252512 examples: {'rewards_train/chosen': '0.10853', 'rewards_train/rejected': '0.022184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.086345', 'logps_train/rejected': '-99.584', 'logps_train/chosen': '-130.37', 'loss/train': '0.66824', 'examples_per_second': '30.197', 'grad_norm': '27', 'counters/examples': 252512, 'counters/updates': 7891}
train stats after 252544 examples: {'rewards_train/chosen': '0.16922', 'rewards_train/rejected': '0.056425', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1128', 'logps_train/rejected': '-113.41', 'logps_train/chosen': '-137.83', 'loss/train': '0.65374', 'examples_per_second': '30.533', 'grad_norm': '28', 'counters/examples': 252544, 'counters/updates': 7892}
train stats after 252576 examples: {'rewards_train/chosen': '0.26333', 'rewards_train/rejected': '0.040454', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.22287', 'logps_train/rejected': '-103.79', 'logps_train/chosen': '-114.85', 'loss/train': '0.60499', 'examples_per_second': '29.992', 'grad_norm': '34.25', 'counters/examples': 252576, 'counters/updates': 7893}
train stats after 252608 examples: {'rewards_train/chosen': '0.22527', 'rewards_train/rejected': '0.098706', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12657', 'logps_train/rejected': '-148.63', 'logps_train/chosen': '-174.99', 'loss/train': '0.6584', 'examples_per_second': '31.519', 'grad_norm': '49', 'counters/examples': 252608, 'counters/updates': 7894}
train stats after 252640 examples: {'rewards_train/chosen': '0.29267', 'rewards_train/rejected': '0.066979', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.22569', 'logps_train/rejected': '-118.23', 'logps_train/chosen': '-166.59', 'loss/train': '0.64653', 'examples_per_second': '31.303', 'grad_norm': '25.625', 'counters/examples': 252640, 'counters/updates': 7895}
train stats after 252672 examples: {'rewards_train/chosen': '0.17377', 'rewards_train/rejected': '0.065058', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10871', 'logps_train/rejected': '-83.348', 'logps_train/chosen': '-130.02', 'loss/train': '0.65635', 'examples_per_second': '31.481', 'grad_norm': '36.75', 'counters/examples': 252672, 'counters/updates': 7896}
skipping logging after 252704 examples to avoid logging too frequently
train stats after 252736 examples: {'rewards_train/chosen': '0.22324', 'rewards_train/rejected': '0.11387', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10937', 'logps_train/rejected': '-141.14', 'logps_train/chosen': '-184.15', 'loss/train': '0.65923', 'examples_per_second': '33.683', 'grad_norm': '32.75', 'counters/examples': 252736, 'counters/updates': 7898}
train stats after 252768 examples: {'rewards_train/chosen': '0.25097', 'rewards_train/rejected': '0.16793', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083034', 'logps_train/rejected': '-133.54', 'logps_train/chosen': '-133.17', 'loss/train': '0.67711', 'examples_per_second': '30.671', 'grad_norm': '65', 'counters/examples': 252768, 'counters/updates': 7899}
train stats after 252800 examples: {'rewards_train/chosen': '0.21685', 'rewards_train/rejected': '0.031407', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.18544', 'logps_train/rejected': '-142.94', 'logps_train/chosen': '-146.37', 'loss/train': '0.61795', 'examples_per_second': '30.386', 'grad_norm': '27.125', 'counters/examples': 252800, 'counters/updates': 7900}
train stats after 252832 examples: {'rewards_train/chosen': '0.14617', 'rewards_train/rejected': '-0.05249', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19866', 'logps_train/rejected': '-138.2', 'logps_train/chosen': '-163.83', 'loss/train': '0.61673', 'examples_per_second': '31.184', 'grad_norm': '36', 'counters/examples': 252832, 'counters/updates': 7901}
train stats after 252864 examples: {'rewards_train/chosen': '0.13106', 'rewards_train/rejected': '-0.030739', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1618', 'logps_train/rejected': '-100.26', 'logps_train/chosen': '-162.98', 'loss/train': '0.6288', 'examples_per_second': '32.378', 'grad_norm': '25', 'counters/examples': 252864, 'counters/updates': 7902}
train stats after 252896 examples: {'rewards_train/chosen': '0.1378', 'rewards_train/rejected': '0.053186', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.084617', 'logps_train/rejected': '-125.6', 'logps_train/chosen': '-134.55', 'loss/train': '0.65863', 'examples_per_second': '30.485', 'grad_norm': '41', 'counters/examples': 252896, 'counters/updates': 7903}
train stats after 252928 examples: {'rewards_train/chosen': '0.25591', 'rewards_train/rejected': '0.18104', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.074873', 'logps_train/rejected': '-114.31', 'logps_train/chosen': '-117.56', 'loss/train': '0.68957', 'examples_per_second': '33.156', 'grad_norm': '86', 'counters/examples': 252928, 'counters/updates': 7904}
skipping logging after 252960 examples to avoid logging too frequently
train stats after 252992 examples: {'rewards_train/chosen': '0.21734', 'rewards_train/rejected': '-0.036336', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.25368', 'logps_train/rejected': '-136.19', 'logps_train/chosen': '-168.89', 'loss/train': '0.59559', 'examples_per_second': '31.465', 'grad_norm': '27.75', 'counters/examples': 252992, 'counters/updates': 7906}
train stats after 253024 examples: {'rewards_train/chosen': '0.20527', 'rewards_train/rejected': '0.014784', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19048', 'logps_train/rejected': '-101.41', 'logps_train/chosen': '-152.74', 'loss/train': '0.61562', 'examples_per_second': '32.3', 'grad_norm': '28.375', 'counters/examples': 253024, 'counters/updates': 7907}
train stats after 253056 examples: {'rewards_train/chosen': '0.19186', 'rewards_train/rejected': '0.062407', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12945', 'logps_train/rejected': '-149.56', 'logps_train/chosen': '-140.26', 'loss/train': '0.66064', 'examples_per_second': '30.424', 'grad_norm': '46', 'counters/examples': 253056, 'counters/updates': 7908}
skipping logging after 253088 examples to avoid logging too frequently
train stats after 253120 examples: {'rewards_train/chosen': '0.21902', 'rewards_train/rejected': '-0.024961', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24398', 'logps_train/rejected': '-146.95', 'logps_train/chosen': '-133.52', 'loss/train': '0.58934', 'examples_per_second': '34.824', 'grad_norm': '28.375', 'counters/examples': 253120, 'counters/updates': 7910}
train stats after 253152 examples: {'rewards_train/chosen': '0.15176', 'rewards_train/rejected': '0.10103', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.050729', 'logps_train/rejected': '-124.38', 'logps_train/chosen': '-135.92', 'loss/train': '0.69257', 'examples_per_second': '31.297', 'grad_norm': '36.25', 'counters/examples': 253152, 'counters/updates': 7911}
skipping logging after 253184 examples to avoid logging too frequently
train stats after 253216 examples: {'rewards_train/chosen': '0.12692', 'rewards_train/rejected': '0.088004', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038916', 'logps_train/rejected': '-99.763', 'logps_train/chosen': '-141.17', 'loss/train': '0.68654', 'examples_per_second': '30.965', 'grad_norm': '24.375', 'counters/examples': 253216, 'counters/updates': 7913}
train stats after 253248 examples: {'rewards_train/chosen': '0.066061', 'rewards_train/rejected': '0.090383', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.024322', 'logps_train/rejected': '-107.11', 'logps_train/chosen': '-108.45', 'loss/train': '0.72199', 'examples_per_second': '32.349', 'grad_norm': '26.375', 'counters/examples': 253248, 'counters/updates': 7914}
train stats after 253280 examples: {'rewards_train/chosen': '0.15362', 'rewards_train/rejected': '0.052529', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10109', 'logps_train/rejected': '-155.82', 'logps_train/chosen': '-179.74', 'loss/train': '0.66233', 'examples_per_second': '31.277', 'grad_norm': '29', 'counters/examples': 253280, 'counters/updates': 7915}
train stats after 253312 examples: {'rewards_train/chosen': '0.16158', 'rewards_train/rejected': '0.042025', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11955', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-148.53', 'loss/train': '0.64999', 'examples_per_second': '30.771', 'grad_norm': '31.375', 'counters/examples': 253312, 'counters/updates': 7916}
skipping logging after 253344 examples to avoid logging too frequently
train stats after 253376 examples: {'rewards_train/chosen': '0.12999', 'rewards_train/rejected': '0.058768', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071218', 'logps_train/rejected': '-131.21', 'logps_train/chosen': '-117.82', 'loss/train': '0.66813', 'examples_per_second': '31.428', 'grad_norm': '31.875', 'counters/examples': 253376, 'counters/updates': 7918}
skipping logging after 253408 examples to avoid logging too frequently
train stats after 253440 examples: {'rewards_train/chosen': '0.10217', 'rewards_train/rejected': '0.10037', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0017974', 'logps_train/rejected': '-131.86', 'logps_train/chosen': '-126.64', 'loss/train': '0.70733', 'examples_per_second': '35.869', 'grad_norm': '36.25', 'counters/examples': 253440, 'counters/updates': 7920}
train stats after 253472 examples: {'rewards_train/chosen': '0.16365', 'rewards_train/rejected': '0.071288', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.092366', 'logps_train/rejected': '-125.62', 'logps_train/chosen': '-167.2', 'loss/train': '0.66252', 'examples_per_second': '31.451', 'grad_norm': '36', 'counters/examples': 253472, 'counters/updates': 7921}
train stats after 253504 examples: {'rewards_train/chosen': '0.1294', 'rewards_train/rejected': '-0.021334', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15074', 'logps_train/rejected': '-87.735', 'logps_train/chosen': '-149.46', 'loss/train': '0.62743', 'examples_per_second': '32.208', 'grad_norm': '23.75', 'counters/examples': 253504, 'counters/updates': 7922}
train stats after 253536 examples: {'rewards_train/chosen': '0.12364', 'rewards_train/rejected': '0.039641', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.083994', 'logps_train/rejected': '-144.51', 'logps_train/chosen': '-159.06', 'loss/train': '0.66494', 'examples_per_second': '31.572', 'grad_norm': '31.75', 'counters/examples': 253536, 'counters/updates': 7923}
train stats after 253568 examples: {'rewards_train/chosen': '0.20238', 'rewards_train/rejected': '0.0088787', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19351', 'logps_train/rejected': '-133.69', 'logps_train/chosen': '-147.98', 'loss/train': '0.61695', 'examples_per_second': '30.954', 'grad_norm': '31', 'counters/examples': 253568, 'counters/updates': 7924}
train stats after 253600 examples: {'rewards_train/chosen': '0.17474', 'rewards_train/rejected': '0.09086', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08388', 'logps_train/rejected': '-129.26', 'logps_train/chosen': '-117.81', 'loss/train': '0.66926', 'examples_per_second': '31.053', 'grad_norm': '36.75', 'counters/examples': 253600, 'counters/updates': 7925}
train stats after 253632 examples: {'rewards_train/chosen': '0.1584', 'rewards_train/rejected': '0.051392', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.107', 'logps_train/rejected': '-107.41', 'logps_train/chosen': '-125.54', 'loss/train': '0.65705', 'examples_per_second': '32.958', 'grad_norm': '25', 'counters/examples': 253632, 'counters/updates': 7926}
train stats after 253664 examples: {'rewards_train/chosen': '0.2283', 'rewards_train/rejected': '-0.023322', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25163', 'logps_train/rejected': '-106.14', 'logps_train/chosen': '-156.22', 'loss/train': '0.59332', 'examples_per_second': '30.605', 'grad_norm': '28.375', 'counters/examples': 253664, 'counters/updates': 7927}
train stats after 253696 examples: {'rewards_train/chosen': '0.094724', 'rewards_train/rejected': '0.02322', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071504', 'logps_train/rejected': '-115.13', 'logps_train/chosen': '-122.48', 'loss/train': '0.66819', 'examples_per_second': '30.53', 'grad_norm': '31.125', 'counters/examples': 253696, 'counters/updates': 7928}
train stats after 253728 examples: {'rewards_train/chosen': '0.17595', 'rewards_train/rejected': '0.088944', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087002', 'logps_train/rejected': '-158.56', 'logps_train/chosen': '-168', 'loss/train': '0.6621', 'examples_per_second': '31.434', 'grad_norm': '36', 'counters/examples': 253728, 'counters/updates': 7929}
train stats after 253760 examples: {'rewards_train/chosen': '0.20143', 'rewards_train/rejected': '-0.0022178', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20365', 'logps_train/rejected': '-102.05', 'logps_train/chosen': '-126.18', 'loss/train': '0.61428', 'examples_per_second': '29.883', 'grad_norm': '34.25', 'counters/examples': 253760, 'counters/updates': 7930}
train stats after 253792 examples: {'rewards_train/chosen': '0.18517', 'rewards_train/rejected': '0.20474', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.019572', 'logps_train/rejected': '-138.8', 'logps_train/chosen': '-160.67', 'loss/train': '0.74693', 'examples_per_second': '31.266', 'grad_norm': '47', 'counters/examples': 253792, 'counters/updates': 7931}
skipping logging after 253824 examples to avoid logging too frequently
train stats after 253856 examples: {'rewards_train/chosen': '0.16707', 'rewards_train/rejected': '0.14799', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.019074', 'logps_train/rejected': '-117.33', 'logps_train/chosen': '-150.85', 'loss/train': '0.69356', 'examples_per_second': '31.525', 'grad_norm': '36', 'counters/examples': 253856, 'counters/updates': 7933}
train stats after 253888 examples: {'rewards_train/chosen': '0.14413', 'rewards_train/rejected': '0.086576', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.057557', 'logps_train/rejected': '-165.26', 'logps_train/chosen': '-156.7', 'loss/train': '0.68762', 'examples_per_second': '33.104', 'grad_norm': '31.875', 'counters/examples': 253888, 'counters/updates': 7934}
train stats after 253920 examples: {'rewards_train/chosen': '0.11958', 'rewards_train/rejected': '0.077647', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.041931', 'logps_train/rejected': '-153.3', 'logps_train/chosen': '-186.88', 'loss/train': '0.68835', 'examples_per_second': '31.48', 'grad_norm': '39.75', 'counters/examples': 253920, 'counters/updates': 7935}
train stats after 253952 examples: {'rewards_train/chosen': '0.18184', 'rewards_train/rejected': '0.05785', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12399', 'logps_train/rejected': '-113.85', 'logps_train/chosen': '-177.27', 'loss/train': '0.65193', 'examples_per_second': '31.692', 'grad_norm': '31.375', 'counters/examples': 253952, 'counters/updates': 7936}
train stats after 253984 examples: {'rewards_train/chosen': '0.15414', 'rewards_train/rejected': '0.035592', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11855', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-129.65', 'loss/train': '0.65194', 'examples_per_second': '31.259', 'grad_norm': '27.625', 'counters/examples': 253984, 'counters/updates': 7937}
skipping logging after 254016 examples to avoid logging too frequently
train stats after 254048 examples: {'rewards_train/chosen': '0.14808', 'rewards_train/rejected': '-0.084262', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23234', 'logps_train/rejected': '-110.8', 'logps_train/chosen': '-119.84', 'loss/train': '0.60383', 'examples_per_second': '35.985', 'grad_norm': '23.75', 'counters/examples': 254048, 'counters/updates': 7939}
train stats after 254080 examples: {'rewards_train/chosen': '0.14779', 'rewards_train/rejected': '0.084845', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.062948', 'logps_train/rejected': '-106.19', 'logps_train/chosen': '-128.86', 'loss/train': '0.67879', 'examples_per_second': '31.475', 'grad_norm': '28.125', 'counters/examples': 254080, 'counters/updates': 7940}
train stats after 254112 examples: {'rewards_train/chosen': '0.21549', 'rewards_train/rejected': '0.092452', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12304', 'logps_train/rejected': '-98.396', 'logps_train/chosen': '-135.01', 'loss/train': '0.64725', 'examples_per_second': '31.966', 'grad_norm': '29.375', 'counters/examples': 254112, 'counters/updates': 7941}
skipping logging after 254144 examples to avoid logging too frequently
train stats after 254176 examples: {'rewards_train/chosen': '0.23818', 'rewards_train/rejected': '0.16112', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077067', 'logps_train/rejected': '-134.27', 'logps_train/chosen': '-138.6', 'loss/train': '0.67677', 'examples_per_second': '30.653', 'grad_norm': '49', 'counters/examples': 254176, 'counters/updates': 7943}
train stats after 254208 examples: {'rewards_train/chosen': '0.25572', 'rewards_train/rejected': '0.053515', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.2022', 'logps_train/rejected': '-120.83', 'logps_train/chosen': '-154.44', 'loss/train': '0.61085', 'examples_per_second': '31.268', 'grad_norm': '32.25', 'counters/examples': 254208, 'counters/updates': 7944}
train stats after 254240 examples: {'rewards_train/chosen': '0.078569', 'rewards_train/rejected': '0.022897', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055672', 'logps_train/rejected': '-108.22', 'logps_train/chosen': '-125.75', 'loss/train': '0.69125', 'examples_per_second': '31.385', 'grad_norm': '28.375', 'counters/examples': 254240, 'counters/updates': 7945}
skipping logging after 254272 examples to avoid logging too frequently
train stats after 254304 examples: {'rewards_train/chosen': '0.16966', 'rewards_train/rejected': '0.057349', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11231', 'logps_train/rejected': '-137.71', 'logps_train/chosen': '-136.28', 'loss/train': '0.64991', 'examples_per_second': '30.355', 'grad_norm': '30', 'counters/examples': 254304, 'counters/updates': 7947}
train stats after 254336 examples: {'rewards_train/chosen': '0.1553', 'rewards_train/rejected': '0.058947', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.096354', 'logps_train/rejected': '-165.72', 'logps_train/chosen': '-124.76', 'loss/train': '0.66218', 'examples_per_second': '32.28', 'grad_norm': '27.5', 'counters/examples': 254336, 'counters/updates': 7948}
train stats after 254368 examples: {'rewards_train/chosen': '0.13885', 'rewards_train/rejected': '0.0027328', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13612', 'logps_train/rejected': '-109.85', 'logps_train/chosen': '-144.92', 'loss/train': '0.64209', 'examples_per_second': '30.072', 'grad_norm': '30.25', 'counters/examples': 254368, 'counters/updates': 7949}
train stats after 254400 examples: {'rewards_train/chosen': '0.10422', 'rewards_train/rejected': '0.072121', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.032103', 'logps_train/rejected': '-96.215', 'logps_train/chosen': '-161.78', 'loss/train': '0.68481', 'examples_per_second': '32.871', 'grad_norm': '30.375', 'counters/examples': 254400, 'counters/updates': 7950}
train stats after 254432 examples: {'rewards_train/chosen': '0.16293', 'rewards_train/rejected': '0.024475', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13845', 'logps_train/rejected': '-131.72', 'logps_train/chosen': '-137.48', 'loss/train': '0.63255', 'examples_per_second': '31.546', 'grad_norm': '35', 'counters/examples': 254432, 'counters/updates': 7951}
train stats after 254464 examples: {'rewards_train/chosen': '0.12674', 'rewards_train/rejected': '0.02647', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10028', 'logps_train/rejected': '-126.54', 'logps_train/chosen': '-149.48', 'loss/train': '0.65593', 'examples_per_second': '30.035', 'grad_norm': '26.625', 'counters/examples': 254464, 'counters/updates': 7952}
skipping logging after 254496 examples to avoid logging too frequently
train stats after 254528 examples: {'rewards_train/chosen': '0.15615', 'rewards_train/rejected': '0.12551', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.030639', 'logps_train/rejected': '-150.28', 'logps_train/chosen': '-147.42', 'loss/train': '0.72782', 'examples_per_second': '31.463', 'grad_norm': '38.25', 'counters/examples': 254528, 'counters/updates': 7954}
train stats after 254560 examples: {'rewards_train/chosen': '0.11713', 'rewards_train/rejected': '0.056945', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.060188', 'logps_train/rejected': '-103.75', 'logps_train/chosen': '-143.19', 'loss/train': '0.6718', 'examples_per_second': '30.454', 'grad_norm': '30.25', 'counters/examples': 254560, 'counters/updates': 7955}
train stats after 254592 examples: {'rewards_train/chosen': '0.20812', 'rewards_train/rejected': '0.10614', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10199', 'logps_train/rejected': '-132.3', 'logps_train/chosen': '-125.35', 'loss/train': '0.66399', 'examples_per_second': '31.55', 'grad_norm': '43.75', 'counters/examples': 254592, 'counters/updates': 7956}
train stats after 254624 examples: {'rewards_train/chosen': '0.16397', 'rewards_train/rejected': '0.13362', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.030346', 'logps_train/rejected': '-108.55', 'logps_train/chosen': '-124.7', 'loss/train': '0.68924', 'examples_per_second': '31.793', 'grad_norm': '38.75', 'counters/examples': 254624, 'counters/updates': 7957}
skipping logging after 254656 examples to avoid logging too frequently
train stats after 254688 examples: {'rewards_train/chosen': '0.11985', 'rewards_train/rejected': '0.13271', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.012854', 'logps_train/rejected': '-139.92', 'logps_train/chosen': '-164.21', 'loss/train': '0.72375', 'examples_per_second': '31.559', 'grad_norm': '48.5', 'counters/examples': 254688, 'counters/updates': 7959}
skipping logging after 254720 examples to avoid logging too frequently
train stats after 254752 examples: {'rewards_train/chosen': '0.11859', 'rewards_train/rejected': '0.0076914', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11089', 'logps_train/rejected': '-124.1', 'logps_train/chosen': '-144.14', 'loss/train': '0.65052', 'examples_per_second': '30.381', 'grad_norm': '29.375', 'counters/examples': 254752, 'counters/updates': 7961}
train stats after 254784 examples: {'rewards_train/chosen': '0.089473', 'rewards_train/rejected': '0.093218', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.003745', 'logps_train/rejected': '-155.74', 'logps_train/chosen': '-140.83', 'loss/train': '0.70502', 'examples_per_second': '31.495', 'grad_norm': '32.25', 'counters/examples': 254784, 'counters/updates': 7962}
train stats after 254816 examples: {'rewards_train/chosen': '0.1351', 'rewards_train/rejected': '0.061112', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.073989', 'logps_train/rejected': '-164.53', 'logps_train/chosen': '-143.74', 'loss/train': '0.67117', 'examples_per_second': '30.205', 'grad_norm': '100', 'counters/examples': 254816, 'counters/updates': 7963}
skipping logging after 254848 examples to avoid logging too frequently
train stats after 254880 examples: {'rewards_train/chosen': '0.16697', 'rewards_train/rejected': '0.12915', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.037817', 'logps_train/rejected': '-178.9', 'logps_train/chosen': '-163.41', 'loss/train': '0.68633', 'examples_per_second': '31.453', 'grad_norm': '39', 'counters/examples': 254880, 'counters/updates': 7965}
skipping logging after 254912 examples to avoid logging too frequently
train stats after 254944 examples: {'rewards_train/chosen': '0.15312', 'rewards_train/rejected': '0.014424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13869', 'logps_train/rejected': '-131.29', 'logps_train/chosen': '-142.36', 'loss/train': '0.64086', 'examples_per_second': '34.588', 'grad_norm': '55.75', 'counters/examples': 254944, 'counters/updates': 7967}
train stats after 254976 examples: {'rewards_train/chosen': '0.12805', 'rewards_train/rejected': '0.033705', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094345', 'logps_train/rejected': '-122.92', 'logps_train/chosen': '-115.81', 'loss/train': '0.67218', 'examples_per_second': '31.153', 'grad_norm': '25.5', 'counters/examples': 254976, 'counters/updates': 7968}
train stats after 255008 examples: {'rewards_train/chosen': '0.18173', 'rewards_train/rejected': '0.021406', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16033', 'logps_train/rejected': '-112.72', 'logps_train/chosen': '-148.67', 'loss/train': '0.63795', 'examples_per_second': '33.176', 'grad_norm': '37.5', 'counters/examples': 255008, 'counters/updates': 7969}
train stats after 255040 examples: {'rewards_train/chosen': '0.34133', 'rewards_train/rejected': '0.089904', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.25142', 'logps_train/rejected': '-134.69', 'logps_train/chosen': '-172.71', 'loss/train': '0.60588', 'examples_per_second': '30.706', 'grad_norm': '26', 'counters/examples': 255040, 'counters/updates': 7970}
train stats after 255072 examples: {'rewards_train/chosen': '0.19364', 'rewards_train/rejected': '0.065885', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12775', 'logps_train/rejected': '-130.94', 'logps_train/chosen': '-167.66', 'loss/train': '0.64806', 'examples_per_second': '31.565', 'grad_norm': '31.5', 'counters/examples': 255072, 'counters/updates': 7971}
train stats after 255104 examples: {'rewards_train/chosen': '0.12718', 'rewards_train/rejected': '0.0015508', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12563', 'logps_train/rejected': '-123.41', 'logps_train/chosen': '-155.73', 'loss/train': '0.64854', 'examples_per_second': '30.291', 'grad_norm': '27.625', 'counters/examples': 255104, 'counters/updates': 7972}
train stats after 255136 examples: {'rewards_train/chosen': '0.089683', 'rewards_train/rejected': '-0.010908', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10059', 'logps_train/rejected': '-140.12', 'logps_train/chosen': '-143.65', 'loss/train': '0.65689', 'examples_per_second': '32.217', 'grad_norm': '34.5', 'counters/examples': 255136, 'counters/updates': 7973}
train stats after 255168 examples: {'rewards_train/chosen': '0.073624', 'rewards_train/rejected': '0.00203', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.071594', 'logps_train/rejected': '-121.53', 'logps_train/chosen': '-169.35', 'loss/train': '0.67291', 'examples_per_second': '30.682', 'grad_norm': '34.75', 'counters/examples': 255168, 'counters/updates': 7974}
train stats after 255200 examples: {'rewards_train/chosen': '0.16031', 'rewards_train/rejected': '0.069389', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.090922', 'logps_train/rejected': '-136.6', 'logps_train/chosen': '-137.49', 'loss/train': '0.66871', 'examples_per_second': '30.439', 'grad_norm': '29.875', 'counters/examples': 255200, 'counters/updates': 7975}
skipping logging after 255232 examples to avoid logging too frequently
train stats after 255264 examples: {'rewards_train/chosen': '0.23557', 'rewards_train/rejected': '-0.020382', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25596', 'logps_train/rejected': '-132.39', 'logps_train/chosen': '-159.96', 'loss/train': '0.58678', 'examples_per_second': '33.452', 'grad_norm': '28', 'counters/examples': 255264, 'counters/updates': 7977}
train stats after 255296 examples: {'rewards_train/chosen': '0.21391', 'rewards_train/rejected': '-0.0018416', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21576', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-124.03', 'loss/train': '0.61601', 'examples_per_second': '30.855', 'grad_norm': '27.5', 'counters/examples': 255296, 'counters/updates': 7978}
train stats after 255328 examples: {'rewards_train/chosen': '0.13365', 'rewards_train/rejected': '-0.050321', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18397', 'logps_train/rejected': '-110.36', 'logps_train/chosen': '-114.2', 'loss/train': '0.61963', 'examples_per_second': '31.826', 'grad_norm': '27.75', 'counters/examples': 255328, 'counters/updates': 7979}
train stats after 255360 examples: {'rewards_train/chosen': '0.12297', 'rewards_train/rejected': '-0.026112', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14908', 'logps_train/rejected': '-112.03', 'logps_train/chosen': '-153.62', 'loss/train': '0.63218', 'examples_per_second': '31.657', 'grad_norm': '27.375', 'counters/examples': 255360, 'counters/updates': 7980}
train stats after 255392 examples: {'rewards_train/chosen': '0.10048', 'rewards_train/rejected': '-0.01523', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11571', 'logps_train/rejected': '-148.53', 'logps_train/chosen': '-141.89', 'loss/train': '0.64863', 'examples_per_second': '32.936', 'grad_norm': '26.625', 'counters/examples': 255392, 'counters/updates': 7981}
skipping logging after 255424 examples to avoid logging too frequently
train stats after 255456 examples: {'rewards_train/chosen': '0.096485', 'rewards_train/rejected': '0.11527', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.018781', 'logps_train/rejected': '-133.1', 'logps_train/chosen': '-132.5', 'loss/train': '0.7133', 'examples_per_second': '30.001', 'grad_norm': '31.125', 'counters/examples': 255456, 'counters/updates': 7983}
train stats after 255488 examples: {'rewards_train/chosen': '0.10591', 'rewards_train/rejected': '0.054713', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051202', 'logps_train/rejected': '-149.33', 'logps_train/chosen': '-171.66', 'loss/train': '0.68819', 'examples_per_second': '31.506', 'grad_norm': '39.25', 'counters/examples': 255488, 'counters/updates': 7984}
train stats after 255520 examples: {'rewards_train/chosen': '0.13572', 'rewards_train/rejected': '0.050773', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084946', 'logps_train/rejected': '-123.53', 'logps_train/chosen': '-107.23', 'loss/train': '0.65867', 'examples_per_second': '30.255', 'grad_norm': '25.5', 'counters/examples': 255520, 'counters/updates': 7985}
train stats after 255552 examples: {'rewards_train/chosen': '0.15559', 'rewards_train/rejected': '0.12248', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.033111', 'logps_train/rejected': '-147.94', 'logps_train/chosen': '-148.14', 'loss/train': '0.69776', 'examples_per_second': '32.483', 'grad_norm': '38.25', 'counters/examples': 255552, 'counters/updates': 7986}
train stats after 255584 examples: {'rewards_train/chosen': '0.22722', 'rewards_train/rejected': '0.029269', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19795', 'logps_train/rejected': '-134.62', 'logps_train/chosen': '-151.14', 'loss/train': '0.60897', 'examples_per_second': '30.747', 'grad_norm': '31.25', 'counters/examples': 255584, 'counters/updates': 7987}
train stats after 255616 examples: {'rewards_train/chosen': '0.20532', 'rewards_train/rejected': '0.062051', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14327', 'logps_train/rejected': '-128.08', 'logps_train/chosen': '-173.91', 'loss/train': '0.6366', 'examples_per_second': '31.427', 'grad_norm': '37.5', 'counters/examples': 255616, 'counters/updates': 7988}
train stats after 255648 examples: {'rewards_train/chosen': '0.13782', 'rewards_train/rejected': '0.080936', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056888', 'logps_train/rejected': '-98.876', 'logps_train/chosen': '-97.281', 'loss/train': '0.6728', 'examples_per_second': '32.228', 'grad_norm': '37.25', 'counters/examples': 255648, 'counters/updates': 7989}
train stats after 255680 examples: {'rewards_train/chosen': '0.14767', 'rewards_train/rejected': '0.093345', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.054325', 'logps_train/rejected': '-190.9', 'logps_train/chosen': '-179.6', 'loss/train': '0.6832', 'examples_per_second': '31.451', 'grad_norm': '30.125', 'counters/examples': 255680, 'counters/updates': 7990}
train stats after 255712 examples: {'rewards_train/chosen': '0.1491', 'rewards_train/rejected': '0.12073', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.028374', 'logps_train/rejected': '-110.72', 'logps_train/chosen': '-135.23', 'loss/train': '0.69097', 'examples_per_second': '31.055', 'grad_norm': '34.25', 'counters/examples': 255712, 'counters/updates': 7991}
train stats after 255744 examples: {'rewards_train/chosen': '0.13379', 'rewards_train/rejected': '0.0073693', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12642', 'logps_train/rejected': '-144.53', 'logps_train/chosen': '-143.92', 'loss/train': '0.64972', 'examples_per_second': '31.505', 'grad_norm': '29.875', 'counters/examples': 255744, 'counters/updates': 7992}
skipping logging after 255776 examples to avoid logging too frequently
train stats after 255808 examples: {'rewards_train/chosen': '0.17568', 'rewards_train/rejected': '0.09264', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083036', 'logps_train/rejected': '-145.35', 'logps_train/chosen': '-119.61', 'loss/train': '0.67804', 'examples_per_second': '33.181', 'grad_norm': '54.75', 'counters/examples': 255808, 'counters/updates': 7994}
train stats after 255840 examples: {'rewards_train/chosen': '0.236', 'rewards_train/rejected': '0.069829', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16617', 'logps_train/rejected': '-137.62', 'logps_train/chosen': '-123.02', 'loss/train': '0.63884', 'examples_per_second': '29.957', 'grad_norm': '33', 'counters/examples': 255840, 'counters/updates': 7995}
train stats after 255872 examples: {'rewards_train/chosen': '0.12119', 'rewards_train/rejected': '-0.015827', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13702', 'logps_train/rejected': '-102.67', 'logps_train/chosen': '-133.45', 'loss/train': '0.63532', 'examples_per_second': '29.867', 'grad_norm': '24.875', 'counters/examples': 255872, 'counters/updates': 7996}
train stats after 255904 examples: {'rewards_train/chosen': '0.29287', 'rewards_train/rejected': '0.15634', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13653', 'logps_train/rejected': '-153.04', 'logps_train/chosen': '-158.46', 'loss/train': '0.64782', 'examples_per_second': '31.462', 'grad_norm': '47.25', 'counters/examples': 255904, 'counters/updates': 7997}
train stats after 255936 examples: {'rewards_train/chosen': '0.16421', 'rewards_train/rejected': '-5.5451e-05', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16426', 'logps_train/rejected': '-140.47', 'logps_train/chosen': '-158.27', 'loss/train': '0.62408', 'examples_per_second': '32.713', 'grad_norm': '30', 'counters/examples': 255936, 'counters/updates': 7998}
train stats after 255968 examples: {'rewards_train/chosen': '0.19744', 'rewards_train/rejected': '0.099819', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097625', 'logps_train/rejected': '-133.31', 'logps_train/chosen': '-159.67', 'loss/train': '0.6732', 'examples_per_second': '31.254', 'grad_norm': '38', 'counters/examples': 255968, 'counters/updates': 7999}
train stats after 256000 examples: {'rewards_train/chosen': '0.21225', 'rewards_train/rejected': '0.034002', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17825', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-130.87', 'loss/train': '0.63848', 'examples_per_second': '31.466', 'grad_norm': '36.75', 'counters/examples': 256000, 'counters/updates': 8000}
skipping logging after 256032 examples to avoid logging too frequently
train stats after 256064 examples: {'rewards_train/chosen': '0.2266', 'rewards_train/rejected': '0.0025631', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22404', 'logps_train/rejected': '-119.84', 'logps_train/chosen': '-155.84', 'loss/train': '0.5991', 'examples_per_second': '31.52', 'grad_norm': '24.25', 'counters/examples': 256064, 'counters/updates': 8002}
skipping logging after 256096 examples to avoid logging too frequently
train stats after 256128 examples: {'rewards_train/chosen': '0.088891', 'rewards_train/rejected': '0.04804', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.040851', 'logps_train/rejected': '-116.77', 'logps_train/chosen': '-124.04', 'loss/train': '0.68396', 'examples_per_second': '31.348', 'grad_norm': '40', 'counters/examples': 256128, 'counters/updates': 8004}
train stats after 256160 examples: {'rewards_train/chosen': '0.2118', 'rewards_train/rejected': '0.02646', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18534', 'logps_train/rejected': '-141.28', 'logps_train/chosen': '-147.6', 'loss/train': '0.61287', 'examples_per_second': '30.94', 'grad_norm': '29.125', 'counters/examples': 256160, 'counters/updates': 8005}
train stats after 256192 examples: {'rewards_train/chosen': '0.12418', 'rewards_train/rejected': '0.019486', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10469', 'logps_train/rejected': '-105.77', 'logps_train/chosen': '-121.5', 'loss/train': '0.65094', 'examples_per_second': '31.472', 'grad_norm': '29.5', 'counters/examples': 256192, 'counters/updates': 8006}
skipping logging after 256224 examples to avoid logging too frequently
train stats after 256256 examples: {'rewards_train/chosen': '0.2113', 'rewards_train/rejected': '0.061439', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14986', 'logps_train/rejected': '-119.44', 'logps_train/chosen': '-160.25', 'loss/train': '0.64147', 'examples_per_second': '30.872', 'grad_norm': '30.375', 'counters/examples': 256256, 'counters/updates': 8008}
train stats after 256288 examples: {'rewards_train/chosen': '0.19992', 'rewards_train/rejected': '0.12431', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.075609', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-150', 'loss/train': '0.66596', 'examples_per_second': '30.376', 'grad_norm': '30.375', 'counters/examples': 256288, 'counters/updates': 8009}
train stats after 256320 examples: {'rewards_train/chosen': '0.19019', 'rewards_train/rejected': '0.080299', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10989', 'logps_train/rejected': '-147.98', 'logps_train/chosen': '-163.04', 'loss/train': '0.64919', 'examples_per_second': '31.409', 'grad_norm': '47', 'counters/examples': 256320, 'counters/updates': 8010}
train stats after 256352 examples: {'rewards_train/chosen': '0.13217', 'rewards_train/rejected': '-0.0091214', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14129', 'logps_train/rejected': '-109.17', 'logps_train/chosen': '-122.86', 'loss/train': '0.63957', 'examples_per_second': '30.21', 'grad_norm': '23.125', 'counters/examples': 256352, 'counters/updates': 8011}
skipping logging after 256384 examples to avoid logging too frequently
train stats after 256416 examples: {'rewards_train/chosen': '0.14524', 'rewards_train/rejected': '0.039196', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10604', 'logps_train/rejected': '-118.02', 'logps_train/chosen': '-120.93', 'loss/train': '0.65448', 'examples_per_second': '32.365', 'grad_norm': '32.25', 'counters/examples': 256416, 'counters/updates': 8013}
train stats after 256448 examples: {'rewards_train/chosen': '0.21973', 'rewards_train/rejected': '0.022429', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1973', 'logps_train/rejected': '-174.08', 'logps_train/chosen': '-183.63', 'loss/train': '0.62442', 'examples_per_second': '29.987', 'grad_norm': '32.5', 'counters/examples': 256448, 'counters/updates': 8014}
skipping logging after 256480 examples to avoid logging too frequently
train stats after 256512 examples: {'rewards_train/chosen': '0.18975', 'rewards_train/rejected': '0.011839', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17791', 'logps_train/rejected': '-99.104', 'logps_train/chosen': '-148.18', 'loss/train': '0.62265', 'examples_per_second': '30.506', 'grad_norm': '24.375', 'counters/examples': 256512, 'counters/updates': 8016}
skipping logging after 256544 examples to avoid logging too frequently
train stats after 256576 examples: {'rewards_train/chosen': '0.13826', 'rewards_train/rejected': '0.15284', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '-0.014578', 'logps_train/rejected': '-152.46', 'logps_train/chosen': '-125.77', 'loss/train': '0.70801', 'examples_per_second': '30.593', 'grad_norm': '41.5', 'counters/examples': 256576, 'counters/updates': 8018}
train stats after 256608 examples: {'rewards_train/chosen': '0.14536', 'rewards_train/rejected': '0.10731', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038047', 'logps_train/rejected': '-154.5', 'logps_train/chosen': '-204.36', 'loss/train': '0.69385', 'examples_per_second': '31.503', 'grad_norm': '47.5', 'counters/examples': 256608, 'counters/updates': 8019}
train stats after 256640 examples: {'rewards_train/chosen': '0.14501', 'rewards_train/rejected': '0.019461', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12555', 'logps_train/rejected': '-101.96', 'logps_train/chosen': '-110', 'loss/train': '0.64279', 'examples_per_second': '29.932', 'grad_norm': '31.5', 'counters/examples': 256640, 'counters/updates': 8020}
train stats after 256672 examples: {'rewards_train/chosen': '0.18132', 'rewards_train/rejected': '0.029698', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15162', 'logps_train/rejected': '-126.41', 'logps_train/chosen': '-161.69', 'loss/train': '0.64067', 'examples_per_second': '32.157', 'grad_norm': '29.625', 'counters/examples': 256672, 'counters/updates': 8021}
train stats after 256704 examples: {'rewards_train/chosen': '0.16987', 'rewards_train/rejected': '0.085898', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.083969', 'logps_train/rejected': '-136.23', 'logps_train/chosen': '-139.15', 'loss/train': '0.66979', 'examples_per_second': '32.091', 'grad_norm': '30.875', 'counters/examples': 256704, 'counters/updates': 8022}
train stats after 256736 examples: {'rewards_train/chosen': '0.083168', 'rewards_train/rejected': '0.079365', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0038031', 'logps_train/rejected': '-144.13', 'logps_train/chosen': '-142.54', 'loss/train': '0.70407', 'examples_per_second': '31.191', 'grad_norm': '37.5', 'counters/examples': 256736, 'counters/updates': 8023}
train stats after 256768 examples: {'rewards_train/chosen': '0.18663', 'rewards_train/rejected': '0.11848', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068148', 'logps_train/rejected': '-139.95', 'logps_train/chosen': '-170.19', 'loss/train': '0.67791', 'examples_per_second': '31.538', 'grad_norm': '35.5', 'counters/examples': 256768, 'counters/updates': 8024}
train stats after 256800 examples: {'rewards_train/chosen': '0.16639', 'rewards_train/rejected': '0.0064248', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15997', 'logps_train/rejected': '-101.71', 'logps_train/chosen': '-124.6', 'loss/train': '0.62852', 'examples_per_second': '32.116', 'grad_norm': '24.75', 'counters/examples': 256800, 'counters/updates': 8025}
train stats after 256832 examples: {'rewards_train/chosen': '0.054329', 'rewards_train/rejected': '0.014918', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.039411', 'logps_train/rejected': '-111.88', 'logps_train/chosen': '-167.83', 'loss/train': '0.69243', 'examples_per_second': '31.485', 'grad_norm': '32.75', 'counters/examples': 256832, 'counters/updates': 8026}
skipping logging after 256864 examples to avoid logging too frequently
train stats after 256896 examples: {'rewards_train/chosen': '0.11284', 'rewards_train/rejected': '0.016263', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.096573', 'logps_train/rejected': '-117.87', 'logps_train/chosen': '-104.33', 'loss/train': '0.65576', 'examples_per_second': '31.505', 'grad_norm': '25.75', 'counters/examples': 256896, 'counters/updates': 8028}
skipping logging after 256928 examples to avoid logging too frequently
train stats after 256960 examples: {'rewards_train/chosen': '0.14739', 'rewards_train/rejected': '0.040621', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10677', 'logps_train/rejected': '-152.55', 'logps_train/chosen': '-178.29', 'loss/train': '0.65632', 'examples_per_second': '29.877', 'grad_norm': '37.25', 'counters/examples': 256960, 'counters/updates': 8030}
skipping logging after 256992 examples to avoid logging too frequently
train stats after 257024 examples: {'rewards_train/chosen': '0.19035', 'rewards_train/rejected': '0.056427', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13392', 'logps_train/rejected': '-139.55', 'logps_train/chosen': '-148.61', 'loss/train': '0.63885', 'examples_per_second': '29.839', 'grad_norm': '27.625', 'counters/examples': 257024, 'counters/updates': 8032}
train stats after 257056 examples: {'rewards_train/chosen': '0.13343', 'rewards_train/rejected': '-0.037632', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17106', 'logps_train/rejected': '-117.42', 'logps_train/chosen': '-151.05', 'loss/train': '0.62604', 'examples_per_second': '23.571', 'grad_norm': '27.875', 'counters/examples': 257056, 'counters/updates': 8033}
train stats after 257088 examples: {'rewards_train/chosen': '0.12878', 'rewards_train/rejected': '0.063773', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065008', 'logps_train/rejected': '-110.99', 'logps_train/chosen': '-151.93', 'loss/train': '0.67534', 'examples_per_second': '30.026', 'grad_norm': '42.75', 'counters/examples': 257088, 'counters/updates': 8034}
train stats after 257120 examples: {'rewards_train/chosen': '0.25199', 'rewards_train/rejected': '0.033482', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21851', 'logps_train/rejected': '-170.36', 'logps_train/chosen': '-143.45', 'loss/train': '0.60642', 'examples_per_second': '31.51', 'grad_norm': '31.625', 'counters/examples': 257120, 'counters/updates': 8035}
train stats after 257152 examples: {'rewards_train/chosen': '0.23596', 'rewards_train/rejected': '0.12102', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11494', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-152.61', 'loss/train': '0.65839', 'examples_per_second': '24.366', 'grad_norm': '30.875', 'counters/examples': 257152, 'counters/updates': 8036}
train stats after 257184 examples: {'rewards_train/chosen': '0.14279', 'rewards_train/rejected': '0.17618', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.033398', 'logps_train/rejected': '-169.08', 'logps_train/chosen': '-171.89', 'loss/train': '0.72187', 'examples_per_second': '30.045', 'grad_norm': '40.75', 'counters/examples': 257184, 'counters/updates': 8037}
train stats after 257216 examples: {'rewards_train/chosen': '0.17344', 'rewards_train/rejected': '-0.0048249', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17827', 'logps_train/rejected': '-109.99', 'logps_train/chosen': '-124.68', 'loss/train': '0.61998', 'examples_per_second': '30.172', 'grad_norm': '33.75', 'counters/examples': 257216, 'counters/updates': 8038}
train stats after 257248 examples: {'rewards_train/chosen': '0.17431', 'rewards_train/rejected': '0.070964', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10335', 'logps_train/rejected': '-131.87', 'logps_train/chosen': '-140.4', 'loss/train': '0.65653', 'examples_per_second': '29.802', 'grad_norm': '35.25', 'counters/examples': 257248, 'counters/updates': 8039}
train stats after 257280 examples: {'rewards_train/chosen': '0.21542', 'rewards_train/rejected': '0.036682', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17874', 'logps_train/rejected': '-154.39', 'logps_train/chosen': '-170.96', 'loss/train': '0.62316', 'examples_per_second': '30.17', 'grad_norm': '32', 'counters/examples': 257280, 'counters/updates': 8040}
skipping logging after 257312 examples to avoid logging too frequently
train stats after 257344 examples: {'rewards_train/chosen': '0.19567', 'rewards_train/rejected': '-0.0064558', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20213', 'logps_train/rejected': '-109.85', 'logps_train/chosen': '-127.18', 'loss/train': '0.60692', 'examples_per_second': '31.097', 'grad_norm': '23.125', 'counters/examples': 257344, 'counters/updates': 8042}
train stats after 257376 examples: {'rewards_train/chosen': '0.20472', 'rewards_train/rejected': '0.10456', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10016', 'logps_train/rejected': '-150.99', 'logps_train/chosen': '-138.15', 'loss/train': '0.67571', 'examples_per_second': '32.592', 'grad_norm': '51.75', 'counters/examples': 257376, 'counters/updates': 8043}
train stats after 257408 examples: {'rewards_train/chosen': '0.13929', 'rewards_train/rejected': '0.052927', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.086365', 'logps_train/rejected': '-120.15', 'logps_train/chosen': '-123.17', 'loss/train': '0.66843', 'examples_per_second': '31.183', 'grad_norm': '33.75', 'counters/examples': 257408, 'counters/updates': 8044}
train stats after 257440 examples: {'rewards_train/chosen': '0.16167', 'rewards_train/rejected': '0.15367', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0079995', 'logps_train/rejected': '-152.14', 'logps_train/chosen': '-167.76', 'loss/train': '0.70515', 'examples_per_second': '33.124', 'grad_norm': '39', 'counters/examples': 257440, 'counters/updates': 8045}
train stats after 257472 examples: {'rewards_train/chosen': '0.19904', 'rewards_train/rejected': '0.13608', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.062961', 'logps_train/rejected': '-117.48', 'logps_train/chosen': '-163.11', 'loss/train': '0.68022', 'examples_per_second': '31.302', 'grad_norm': '36.75', 'counters/examples': 257472, 'counters/updates': 8046}
train stats after 257504 examples: {'rewards_train/chosen': '0.071219', 'rewards_train/rejected': '0.0083992', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.06282', 'logps_train/rejected': '-128.55', 'logps_train/chosen': '-123.03', 'loss/train': '0.68345', 'examples_per_second': '32.83', 'grad_norm': '39.5', 'counters/examples': 257504, 'counters/updates': 8047}
train stats after 257536 examples: {'rewards_train/chosen': '0.12782', 'rewards_train/rejected': '0.078824', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.048992', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-149.15', 'loss/train': '0.69048', 'examples_per_second': '30.949', 'grad_norm': '41.5', 'counters/examples': 257536, 'counters/updates': 8048}
train stats after 257568 examples: {'rewards_train/chosen': '0.12257', 'rewards_train/rejected': '0.012006', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11057', 'logps_train/rejected': '-134.67', 'logps_train/chosen': '-140.92', 'loss/train': '0.65416', 'examples_per_second': '32.156', 'grad_norm': '29.5', 'counters/examples': 257568, 'counters/updates': 8049}
train stats after 257600 examples: {'rewards_train/chosen': '0.096116', 'rewards_train/rejected': '0.095324', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.00079253', 'logps_train/rejected': '-106.56', 'logps_train/chosen': '-132.09', 'loss/train': '0.70271', 'examples_per_second': '32.034', 'grad_norm': '37', 'counters/examples': 257600, 'counters/updates': 8050}
train stats after 257632 examples: {'rewards_train/chosen': '0.16427', 'rewards_train/rejected': '0.07336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.090909', 'logps_train/rejected': '-118', 'logps_train/chosen': '-144.92', 'loss/train': '0.66189', 'examples_per_second': '31.479', 'grad_norm': '29.375', 'counters/examples': 257632, 'counters/updates': 8051}
train stats after 257664 examples: {'rewards_train/chosen': '0.23235', 'rewards_train/rejected': '0.023397', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20895', 'logps_train/rejected': '-144.61', 'logps_train/chosen': '-171.68', 'loss/train': '0.61726', 'examples_per_second': '30.508', 'grad_norm': '38', 'counters/examples': 257664, 'counters/updates': 8052}
train stats after 257696 examples: {'rewards_train/chosen': '0.12586', 'rewards_train/rejected': '0.028867', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.096991', 'logps_train/rejected': '-186.01', 'logps_train/chosen': '-140.01', 'loss/train': '0.65984', 'examples_per_second': '30.901', 'grad_norm': '32.75', 'counters/examples': 257696, 'counters/updates': 8053}
train stats after 257728 examples: {'rewards_train/chosen': '0.18207', 'rewards_train/rejected': '0.044233', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13784', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-136.4', 'loss/train': '0.65159', 'examples_per_second': '31.466', 'grad_norm': '74', 'counters/examples': 257728, 'counters/updates': 8054}
train stats after 257760 examples: {'rewards_train/chosen': '0.14094', 'rewards_train/rejected': '0.011529', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12941', 'logps_train/rejected': '-136.99', 'logps_train/chosen': '-164.98', 'loss/train': '0.64489', 'examples_per_second': '32.393', 'grad_norm': '30.5', 'counters/examples': 257760, 'counters/updates': 8055}
train stats after 257792 examples: {'rewards_train/chosen': '0.21252', 'rewards_train/rejected': '0.14949', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.063037', 'logps_train/rejected': '-127.61', 'logps_train/chosen': '-137.37', 'loss/train': '0.68101', 'examples_per_second': '32.359', 'grad_norm': '34.5', 'counters/examples': 257792, 'counters/updates': 8056}
train stats after 257824 examples: {'rewards_train/chosen': '0.19874', 'rewards_train/rejected': '0.074111', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12462', 'logps_train/rejected': '-127.21', 'logps_train/chosen': '-125.73', 'loss/train': '0.65127', 'examples_per_second': '31.808', 'grad_norm': '25.5', 'counters/examples': 257824, 'counters/updates': 8057}
train stats after 257856 examples: {'rewards_train/chosen': '0.25686', 'rewards_train/rejected': '-0.023831', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.28069', 'logps_train/rejected': '-147.16', 'logps_train/chosen': '-145.58', 'loss/train': '0.59038', 'examples_per_second': '30.445', 'grad_norm': '34', 'counters/examples': 257856, 'counters/updates': 8058}
train stats after 257888 examples: {'rewards_train/chosen': '0.18918', 'rewards_train/rejected': '0.15722', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.031957', 'logps_train/rejected': '-130.39', 'logps_train/chosen': '-122.82', 'loss/train': '0.6938', 'examples_per_second': '32.007', 'grad_norm': '27.875', 'counters/examples': 257888, 'counters/updates': 8059}
skipping logging after 257920 examples to avoid logging too frequently
train stats after 257952 examples: {'rewards_train/chosen': '0.044333', 'rewards_train/rejected': '0.033663', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.01067', 'logps_train/rejected': '-129.99', 'logps_train/chosen': '-130.53', 'loss/train': '0.70001', 'examples_per_second': '24.395', 'grad_norm': '32.25', 'counters/examples': 257952, 'counters/updates': 8061}
train stats after 257984 examples: {'rewards_train/chosen': '0.10824', 'rewards_train/rejected': '0.01428', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093957', 'logps_train/rejected': '-129.27', 'logps_train/chosen': '-150.59', 'loss/train': '0.65991', 'examples_per_second': '32.297', 'grad_norm': '41', 'counters/examples': 257984, 'counters/updates': 8062}
train stats after 258016 examples: {'rewards_train/chosen': '0.19012', 'rewards_train/rejected': '0.094184', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095933', 'logps_train/rejected': '-119.48', 'logps_train/chosen': '-136.73', 'loss/train': '0.65939', 'examples_per_second': '32.144', 'grad_norm': '30.875', 'counters/examples': 258016, 'counters/updates': 8063}
train stats after 258048 examples: {'rewards_train/chosen': '0.1016', 'rewards_train/rejected': '0.049782', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.051819', 'logps_train/rejected': '-104.73', 'logps_train/chosen': '-163.6', 'loss/train': '0.68805', 'examples_per_second': '31.486', 'grad_norm': '32.75', 'counters/examples': 258048, 'counters/updates': 8064}
train stats after 258080 examples: {'rewards_train/chosen': '0.2043', 'rewards_train/rejected': '0.066453', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13785', 'logps_train/rejected': '-143.97', 'logps_train/chosen': '-165.61', 'loss/train': '0.64817', 'examples_per_second': '29.993', 'grad_norm': '27.75', 'counters/examples': 258080, 'counters/updates': 8065}
train stats after 258112 examples: {'rewards_train/chosen': '0.17492', 'rewards_train/rejected': '0.093445', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.081477', 'logps_train/rejected': '-177.97', 'logps_train/chosen': '-159.94', 'loss/train': '0.67751', 'examples_per_second': '30.008', 'grad_norm': '35', 'counters/examples': 258112, 'counters/updates': 8066}
skipping logging after 258144 examples to avoid logging too frequently
train stats after 258176 examples: {'rewards_train/chosen': '0.13314', 'rewards_train/rejected': '-0.024638', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15777', 'logps_train/rejected': '-104.63', 'logps_train/chosen': '-168.84', 'loss/train': '0.62605', 'examples_per_second': '30.081', 'grad_norm': '53.25', 'counters/examples': 258176, 'counters/updates': 8068}
skipping logging after 258208 examples to avoid logging too frequently
train stats after 258240 examples: {'rewards_train/chosen': '0.14859', 'rewards_train/rejected': '0.086536', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062057', 'logps_train/rejected': '-136.69', 'logps_train/chosen': '-162.65', 'loss/train': '0.68022', 'examples_per_second': '31.303', 'grad_norm': '29', 'counters/examples': 258240, 'counters/updates': 8070}
train stats after 258272 examples: {'rewards_train/chosen': '0.043385', 'rewards_train/rejected': '0.045159', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '-0.0017741', 'logps_train/rejected': '-114.86', 'logps_train/chosen': '-121.71', 'loss/train': '0.70139', 'examples_per_second': '30.751', 'grad_norm': '25.5', 'counters/examples': 258272, 'counters/updates': 8071}
train stats after 258304 examples: {'rewards_train/chosen': '0.096521', 'rewards_train/rejected': '0.05438', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.042141', 'logps_train/rejected': '-121.36', 'logps_train/chosen': '-140.22', 'loss/train': '0.67858', 'examples_per_second': '32.437', 'grad_norm': '29.375', 'counters/examples': 258304, 'counters/updates': 8072}
train stats after 258336 examples: {'rewards_train/chosen': '0.23017', 'rewards_train/rejected': '0.016893', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21328', 'logps_train/rejected': '-137.1', 'logps_train/chosen': '-153.54', 'loss/train': '0.60886', 'examples_per_second': '31.539', 'grad_norm': '31.875', 'counters/examples': 258336, 'counters/updates': 8073}
train stats after 258368 examples: {'rewards_train/chosen': '0.16795', 'rewards_train/rejected': '0.090582', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077364', 'logps_train/rejected': '-121.37', 'logps_train/chosen': '-121.92', 'loss/train': '0.66309', 'examples_per_second': '31.347', 'grad_norm': '28', 'counters/examples': 258368, 'counters/updates': 8074}
train stats after 258400 examples: {'rewards_train/chosen': '0.15525', 'rewards_train/rejected': '0.066137', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089114', 'logps_train/rejected': '-154.19', 'logps_train/chosen': '-157.67', 'loss/train': '0.66094', 'examples_per_second': '31.368', 'grad_norm': '28.5', 'counters/examples': 258400, 'counters/updates': 8075}
train stats after 258432 examples: {'rewards_train/chosen': '0.24451', 'rewards_train/rejected': '0.055486', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.18902', 'logps_train/rejected': '-110.15', 'logps_train/chosen': '-147.91', 'loss/train': '0.61151', 'examples_per_second': '31.295', 'grad_norm': '31', 'counters/examples': 258432, 'counters/updates': 8076}
train stats after 258464 examples: {'rewards_train/chosen': '0.1555', 'rewards_train/rejected': '-0.0073939', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1629', 'logps_train/rejected': '-124.27', 'logps_train/chosen': '-160.89', 'loss/train': '0.6254', 'examples_per_second': '29.951', 'grad_norm': '31.75', 'counters/examples': 258464, 'counters/updates': 8077}
train stats after 258496 examples: {'rewards_train/chosen': '0.29087', 'rewards_train/rejected': '0.1345', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15637', 'logps_train/rejected': '-131.47', 'logps_train/chosen': '-158.13', 'loss/train': '0.63082', 'examples_per_second': '32.333', 'grad_norm': '27.75', 'counters/examples': 258496, 'counters/updates': 8078}
train stats after 258528 examples: {'rewards_train/chosen': '0.16207', 'rewards_train/rejected': '0.018963', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14311', 'logps_train/rejected': '-149.66', 'logps_train/chosen': '-168.46', 'loss/train': '0.63633', 'examples_per_second': '29.91', 'grad_norm': '31.75', 'counters/examples': 258528, 'counters/updates': 8079}
skipping logging after 258560 examples to avoid logging too frequently
train stats after 258592 examples: {'rewards_train/chosen': '0.11512', 'rewards_train/rejected': '0.076043', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.03908', 'logps_train/rejected': '-124.67', 'logps_train/chosen': '-147.66', 'loss/train': '0.68468', 'examples_per_second': '31.977', 'grad_norm': '26.5', 'counters/examples': 258592, 'counters/updates': 8081}
train stats after 258624 examples: {'rewards_train/chosen': '0.14755', 'rewards_train/rejected': '0.10903', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.038527', 'logps_train/rejected': '-161.89', 'logps_train/chosen': '-177.15', 'loss/train': '0.68724', 'examples_per_second': '31.236', 'grad_norm': '35.5', 'counters/examples': 258624, 'counters/updates': 8082}
train stats after 258656 examples: {'rewards_train/chosen': '0.27286', 'rewards_train/rejected': '0.14424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12862', 'logps_train/rejected': '-138.28', 'logps_train/chosen': '-176.19', 'loss/train': '0.64776', 'examples_per_second': '31.455', 'grad_norm': '30.5', 'counters/examples': 258656, 'counters/updates': 8083}
skipping logging after 258688 examples to avoid logging too frequently
train stats after 258720 examples: {'rewards_train/chosen': '0.18392', 'rewards_train/rejected': '0.042801', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14112', 'logps_train/rejected': '-149.51', 'logps_train/chosen': '-165.65', 'loss/train': '0.65409', 'examples_per_second': '31.483', 'grad_norm': '49.25', 'counters/examples': 258720, 'counters/updates': 8085}
train stats after 258752 examples: {'rewards_train/chosen': '0.31763', 'rewards_train/rejected': '0.17902', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13861', 'logps_train/rejected': '-149.33', 'logps_train/chosen': '-162.17', 'loss/train': '0.64469', 'examples_per_second': '33.097', 'grad_norm': '50.75', 'counters/examples': 258752, 'counters/updates': 8086}
skipping logging after 258784 examples to avoid logging too frequently
train stats after 258816 examples: {'rewards_train/chosen': '0.26949', 'rewards_train/rejected': '0.090311', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17918', 'logps_train/rejected': '-142.47', 'logps_train/chosen': '-172.72', 'loss/train': '0.63042', 'examples_per_second': '30.137', 'grad_norm': '43.5', 'counters/examples': 258816, 'counters/updates': 8088}
train stats after 258848 examples: {'rewards_train/chosen': '0.21114', 'rewards_train/rejected': '0.10098', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11015', 'logps_train/rejected': '-126.4', 'logps_train/chosen': '-156.2', 'loss/train': '0.64919', 'examples_per_second': '31.313', 'grad_norm': '39.75', 'counters/examples': 258848, 'counters/updates': 8089}
train stats after 258880 examples: {'rewards_train/chosen': '0.19784', 'rewards_train/rejected': '0.12478', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073063', 'logps_train/rejected': '-164.62', 'logps_train/chosen': '-153.99', 'loss/train': '0.67186', 'examples_per_second': '30.704', 'grad_norm': '35.75', 'counters/examples': 258880, 'counters/updates': 8090}
train stats after 258912 examples: {'rewards_train/chosen': '0.24104', 'rewards_train/rejected': '0.14207', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.098968', 'logps_train/rejected': '-172.28', 'logps_train/chosen': '-142.37', 'loss/train': '0.66551', 'examples_per_second': '30.331', 'grad_norm': '44.5', 'counters/examples': 258912, 'counters/updates': 8091}
train stats after 258944 examples: {'rewards_train/chosen': '0.1745', 'rewards_train/rejected': '0.10672', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067777', 'logps_train/rejected': '-127.59', 'logps_train/chosen': '-148.98', 'loss/train': '0.67944', 'examples_per_second': '31.471', 'grad_norm': '38.75', 'counters/examples': 258944, 'counters/updates': 8092}
train stats after 258976 examples: {'rewards_train/chosen': '0.2122', 'rewards_train/rejected': '0.052248', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15995', 'logps_train/rejected': '-104.48', 'logps_train/chosen': '-161.19', 'loss/train': '0.62989', 'examples_per_second': '32.419', 'grad_norm': '29.5', 'counters/examples': 258976, 'counters/updates': 8093}
train stats after 259008 examples: {'rewards_train/chosen': '0.21501', 'rewards_train/rejected': '0.14183', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073187', 'logps_train/rejected': '-125.75', 'logps_train/chosen': '-162.73', 'loss/train': '0.68181', 'examples_per_second': '31.501', 'grad_norm': '54.25', 'counters/examples': 259008, 'counters/updates': 8094}
train stats after 259040 examples: {'rewards_train/chosen': '0.28481', 'rewards_train/rejected': '0.070866', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21394', 'logps_train/rejected': '-121.05', 'logps_train/chosen': '-189.6', 'loss/train': '0.61652', 'examples_per_second': '31.257', 'grad_norm': '31', 'counters/examples': 259040, 'counters/updates': 8095}
train stats after 259072 examples: {'rewards_train/chosen': '0.19263', 'rewards_train/rejected': '0.066587', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12604', 'logps_train/rejected': '-134.28', 'logps_train/chosen': '-154.55', 'loss/train': '0.66191', 'examples_per_second': '30.59', 'grad_norm': '34.5', 'counters/examples': 259072, 'counters/updates': 8096}
train stats after 259104 examples: {'rewards_train/chosen': '0.23299', 'rewards_train/rejected': '0.05093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18206', 'logps_train/rejected': '-130.64', 'logps_train/chosen': '-165.51', 'loss/train': '0.63484', 'examples_per_second': '31.469', 'grad_norm': '31.75', 'counters/examples': 259104, 'counters/updates': 8097}
train stats after 259136 examples: {'rewards_train/chosen': '0.20629', 'rewards_train/rejected': '0.064946', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14135', 'logps_train/rejected': '-97.113', 'logps_train/chosen': '-130.98', 'loss/train': '0.63249', 'examples_per_second': '31.837', 'grad_norm': '30.25', 'counters/examples': 259136, 'counters/updates': 8098}
train stats after 259168 examples: {'rewards_train/chosen': '0.10233', 'rewards_train/rejected': '-0.025158', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.12749', 'logps_train/rejected': '-96.806', 'logps_train/chosen': '-122.07', 'loss/train': '0.63854', 'examples_per_second': '30.477', 'grad_norm': '27.625', 'counters/examples': 259168, 'counters/updates': 8099}
train stats after 259200 examples: {'rewards_train/chosen': '0.10594', 'rewards_train/rejected': '-0.11571', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.22165', 'logps_train/rejected': '-159.09', 'logps_train/chosen': '-156.95', 'loss/train': '0.61575', 'examples_per_second': '31.497', 'grad_norm': '27.375', 'counters/examples': 259200, 'counters/updates': 8100}
train stats after 259232 examples: {'rewards_train/chosen': '0.29611', 'rewards_train/rejected': '0.11785', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17826', 'logps_train/rejected': '-143.83', 'logps_train/chosen': '-151.72', 'loss/train': '0.62944', 'examples_per_second': '30.519', 'grad_norm': '36', 'counters/examples': 259232, 'counters/updates': 8101}
train stats after 259264 examples: {'rewards_train/chosen': '0.11486', 'rewards_train/rejected': '0.012746', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10211', 'logps_train/rejected': '-137.16', 'logps_train/chosen': '-141.35', 'loss/train': '0.65549', 'examples_per_second': '30.119', 'grad_norm': '45.5', 'counters/examples': 259264, 'counters/updates': 8102}
train stats after 259296 examples: {'rewards_train/chosen': '0.19067', 'rewards_train/rejected': '0.069701', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12097', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-117.8', 'loss/train': '0.65062', 'examples_per_second': '31.253', 'grad_norm': '25.125', 'counters/examples': 259296, 'counters/updates': 8103}
train stats after 259328 examples: {'rewards_train/chosen': '0.17349', 'rewards_train/rejected': '0.085969', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08752', 'logps_train/rejected': '-127.13', 'logps_train/chosen': '-141.16', 'loss/train': '0.66452', 'examples_per_second': '32.528', 'grad_norm': '30.25', 'counters/examples': 259328, 'counters/updates': 8104}
skipping logging after 259360 examples to avoid logging too frequently
train stats after 259392 examples: {'rewards_train/chosen': '0.19482', 'rewards_train/rejected': '0.12445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.070376', 'logps_train/rejected': '-103.44', 'logps_train/chosen': '-129.89', 'loss/train': '0.67546', 'examples_per_second': '32.027', 'grad_norm': '33', 'counters/examples': 259392, 'counters/updates': 8106}
train stats after 259424 examples: {'rewards_train/chosen': '0.071872', 'rewards_train/rejected': '0.035631', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036241', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-146.72', 'loss/train': '0.68306', 'examples_per_second': '31.526', 'grad_norm': '29.75', 'counters/examples': 259424, 'counters/updates': 8107}
train stats after 259456 examples: {'rewards_train/chosen': '0.1047', 'rewards_train/rejected': '0.0048334', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.099869', 'logps_train/rejected': '-127.44', 'logps_train/chosen': '-163.78', 'loss/train': '0.65627', 'examples_per_second': '31.481', 'grad_norm': '29.75', 'counters/examples': 259456, 'counters/updates': 8108}
train stats after 259488 examples: {'rewards_train/chosen': '0.17371', 'rewards_train/rejected': '0.024346', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14936', 'logps_train/rejected': '-109.07', 'logps_train/chosen': '-133.95', 'loss/train': '0.63488', 'examples_per_second': '30.601', 'grad_norm': '44.75', 'counters/examples': 259488, 'counters/updates': 8109}
train stats after 259520 examples: {'rewards_train/chosen': '0.16857', 'rewards_train/rejected': '0.056293', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11228', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-160.67', 'loss/train': '0.66028', 'examples_per_second': '30.311', 'grad_norm': '27.625', 'counters/examples': 259520, 'counters/updates': 8110}
train stats after 259552 examples: {'rewards_train/chosen': '0.23743', 'rewards_train/rejected': '0.018944', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21849', 'logps_train/rejected': '-103.43', 'logps_train/chosen': '-119.19', 'loss/train': '0.60352', 'examples_per_second': '30.305', 'grad_norm': '22.625', 'counters/examples': 259552, 'counters/updates': 8111}
train stats after 259584 examples: {'rewards_train/chosen': '0.19915', 'rewards_train/rejected': '0.013303', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18584', 'logps_train/rejected': '-131.84', 'logps_train/chosen': '-141.87', 'loss/train': '0.61872', 'examples_per_second': '31.46', 'grad_norm': '25.75', 'counters/examples': 259584, 'counters/updates': 8112}
train stats after 259616 examples: {'rewards_train/chosen': '0.17525', 'rewards_train/rejected': '0.04754', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12771', 'logps_train/rejected': '-145.94', 'logps_train/chosen': '-146.85', 'loss/train': '0.64589', 'examples_per_second': '30.462', 'grad_norm': '34.75', 'counters/examples': 259616, 'counters/updates': 8113}
skipping logging after 259648 examples to avoid logging too frequently
train stats after 259680 examples: {'rewards_train/chosen': '0.22673', 'rewards_train/rejected': '0.010932', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2158', 'logps_train/rejected': '-113.23', 'logps_train/chosen': '-208.17', 'loss/train': '0.61003', 'examples_per_second': '31.511', 'grad_norm': '33.5', 'counters/examples': 259680, 'counters/updates': 8115}
train stats after 259712 examples: {'rewards_train/chosen': '0.22324', 'rewards_train/rejected': '0.0091305', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21411', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-173.69', 'loss/train': '0.60593', 'examples_per_second': '31.368', 'grad_norm': '30.5', 'counters/examples': 259712, 'counters/updates': 8116}
train stats after 259744 examples: {'rewards_train/chosen': '0.23655', 'rewards_train/rejected': '0.042215', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19433', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-152.37', 'loss/train': '0.63357', 'examples_per_second': '32.478', 'grad_norm': '33.5', 'counters/examples': 259744, 'counters/updates': 8117}
train stats after 259776 examples: {'rewards_train/chosen': '0.15943', 'rewards_train/rejected': '0.011005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14843', 'logps_train/rejected': '-92.72', 'logps_train/chosen': '-168.97', 'loss/train': '0.6438', 'examples_per_second': '31.007', 'grad_norm': '48', 'counters/examples': 259776, 'counters/updates': 8118}
train stats after 259808 examples: {'rewards_train/chosen': '0.16087', 'rewards_train/rejected': '0.13277', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.028101', 'logps_train/rejected': '-140.9', 'logps_train/chosen': '-139.98', 'loss/train': '0.69887', 'examples_per_second': '31.604', 'grad_norm': '36.5', 'counters/examples': 259808, 'counters/updates': 8119}
skipping logging after 259840 examples to avoid logging too frequently
train stats after 259872 examples: {'rewards_train/chosen': '0.1696', 'rewards_train/rejected': '0.030338', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13927', 'logps_train/rejected': '-138.57', 'logps_train/chosen': '-125.69', 'loss/train': '0.64904', 'examples_per_second': '34.047', 'grad_norm': '32.25', 'counters/examples': 259872, 'counters/updates': 8121}
train stats after 259904 examples: {'rewards_train/chosen': '0.13945', 'rewards_train/rejected': '0.081445', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058004', 'logps_train/rejected': '-104.78', 'logps_train/chosen': '-138.72', 'loss/train': '0.68405', 'examples_per_second': '32.024', 'grad_norm': '35.25', 'counters/examples': 259904, 'counters/updates': 8122}
train stats after 259936 examples: {'rewards_train/chosen': '0.26388', 'rewards_train/rejected': '0.10356', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.16033', 'logps_train/rejected': '-123.22', 'logps_train/chosen': '-197.8', 'loss/train': '0.64393', 'examples_per_second': '33.163', 'grad_norm': '43.75', 'counters/examples': 259936, 'counters/updates': 8123}
train stats after 259968 examples: {'rewards_train/chosen': '0.16325', 'rewards_train/rejected': '-0.017191', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18044', 'logps_train/rejected': '-101.83', 'logps_train/chosen': '-111.1', 'loss/train': '0.61991', 'examples_per_second': '31.766', 'grad_norm': '25.125', 'counters/examples': 259968, 'counters/updates': 8124}
train stats after 260000 examples: {'rewards_train/chosen': '0.15533', 'rewards_train/rejected': '-0.0031196', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15845', 'logps_train/rejected': '-152.66', 'logps_train/chosen': '-133.47', 'loss/train': '0.63346', 'examples_per_second': '31.018', 'grad_norm': '37.25', 'counters/examples': 260000, 'counters/updates': 8125}
train stats after 260032 examples: {'rewards_train/chosen': '0.090888', 'rewards_train/rejected': '0.010175', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.080713', 'logps_train/rejected': '-124.46', 'logps_train/chosen': '-120.59', 'loss/train': '0.66283', 'examples_per_second': '31.52', 'grad_norm': '25.75', 'counters/examples': 260032, 'counters/updates': 8126}
skipping logging after 260064 examples to avoid logging too frequently
train stats after 260096 examples: {'rewards_train/chosen': '0.19606', 'rewards_train/rejected': '0.063881', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13218', 'logps_train/rejected': '-133.22', 'logps_train/chosen': '-143.55', 'loss/train': '0.64529', 'examples_per_second': '31.532', 'grad_norm': '29.375', 'counters/examples': 260096, 'counters/updates': 8128}
train stats after 260128 examples: {'rewards_train/chosen': '0.24374', 'rewards_train/rejected': '0.10671', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13703', 'logps_train/rejected': '-123.58', 'logps_train/chosen': '-157.3', 'loss/train': '0.64148', 'examples_per_second': '30.551', 'grad_norm': '28.125', 'counters/examples': 260128, 'counters/updates': 8129}
train stats after 260160 examples: {'rewards_train/chosen': '0.18165', 'rewards_train/rejected': '0.084524', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.097129', 'logps_train/rejected': '-134.44', 'logps_train/chosen': '-137.49', 'loss/train': '0.65883', 'examples_per_second': '31.526', 'grad_norm': '30.75', 'counters/examples': 260160, 'counters/updates': 8130}
train stats after 260192 examples: {'rewards_train/chosen': '0.18036', 'rewards_train/rejected': '0.10314', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.077216', 'logps_train/rejected': '-152.62', 'logps_train/chosen': '-130.86', 'loss/train': '0.67785', 'examples_per_second': '30.21', 'grad_norm': '38.5', 'counters/examples': 260192, 'counters/updates': 8131}
train stats after 260224 examples: {'rewards_train/chosen': '0.24529', 'rewards_train/rejected': '0.16926', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.07603', 'logps_train/rejected': '-113.93', 'logps_train/chosen': '-151.76', 'loss/train': '0.68128', 'examples_per_second': '31.426', 'grad_norm': '36.25', 'counters/examples': 260224, 'counters/updates': 8132}
train stats after 260256 examples: {'rewards_train/chosen': '0.17379', 'rewards_train/rejected': '0.11738', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.056412', 'logps_train/rejected': '-127.1', 'logps_train/chosen': '-131.58', 'loss/train': '0.69169', 'examples_per_second': '31.182', 'grad_norm': '28.5', 'counters/examples': 260256, 'counters/updates': 8133}
train stats after 260288 examples: {'rewards_train/chosen': '0.22111', 'rewards_train/rejected': '0.022004', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.19911', 'logps_train/rejected': '-114.25', 'logps_train/chosen': '-152.37', 'loss/train': '0.62781', 'examples_per_second': '32.6', 'grad_norm': '32.75', 'counters/examples': 260288, 'counters/updates': 8134}
skipping logging after 260320 examples to avoid logging too frequently
train stats after 260352 examples: {'rewards_train/chosen': '0.19001', 'rewards_train/rejected': '0.043916', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1461', 'logps_train/rejected': '-141.69', 'logps_train/chosen': '-156.22', 'loss/train': '0.63491', 'examples_per_second': '33.256', 'grad_norm': '28.75', 'counters/examples': 260352, 'counters/updates': 8136}
train stats after 260384 examples: {'rewards_train/chosen': '0.13822', 'rewards_train/rejected': '0.071261', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066961', 'logps_train/rejected': '-158.68', 'logps_train/chosen': '-160.88', 'loss/train': '0.67508', 'examples_per_second': '32.368', 'grad_norm': '32.25', 'counters/examples': 260384, 'counters/updates': 8137}
train stats after 260416 examples: {'rewards_train/chosen': '0.18367', 'rewards_train/rejected': '0.14714', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036529', 'logps_train/rejected': '-116.81', 'logps_train/chosen': '-138.15', 'loss/train': '0.68929', 'examples_per_second': '32.103', 'grad_norm': '34.25', 'counters/examples': 260416, 'counters/updates': 8138}
train stats after 260448 examples: {'rewards_train/chosen': '0.19245', 'rewards_train/rejected': '0.050742', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.14171', 'logps_train/rejected': '-127.08', 'logps_train/chosen': '-140.9', 'loss/train': '0.63473', 'examples_per_second': '31.581', 'grad_norm': '27.375', 'counters/examples': 260448, 'counters/updates': 8139}
train stats after 260480 examples: {'rewards_train/chosen': '0.22945', 'rewards_train/rejected': '0.09075', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1387', 'logps_train/rejected': '-129.4', 'logps_train/chosen': '-163.52', 'loss/train': '0.66128', 'examples_per_second': '30', 'grad_norm': '25.625', 'counters/examples': 260480, 'counters/updates': 8140}
train stats after 260512 examples: {'rewards_train/chosen': '0.11573', 'rewards_train/rejected': '0.036332', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.079396', 'logps_train/rejected': '-107.59', 'logps_train/chosen': '-120.41', 'loss/train': '0.67117', 'examples_per_second': '32.541', 'grad_norm': '30.375', 'counters/examples': 260512, 'counters/updates': 8141}
train stats after 260544 examples: {'rewards_train/chosen': '0.12985', 'rewards_train/rejected': '0.075335', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.054511', 'logps_train/rejected': '-130.66', 'logps_train/chosen': '-118.68', 'loss/train': '0.67611', 'examples_per_second': '30.851', 'grad_norm': '30.875', 'counters/examples': 260544, 'counters/updates': 8142}
train stats after 260576 examples: {'rewards_train/chosen': '0.20246', 'rewards_train/rejected': '0.10977', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.09269', 'logps_train/rejected': '-148.78', 'logps_train/chosen': '-141.35', 'loss/train': '0.66538', 'examples_per_second': '31.537', 'grad_norm': '28', 'counters/examples': 260576, 'counters/updates': 8143}
train stats after 260608 examples: {'rewards_train/chosen': '0.16355', 'rewards_train/rejected': '0.11554', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.048016', 'logps_train/rejected': '-123.61', 'logps_train/chosen': '-173.17', 'loss/train': '0.67494', 'examples_per_second': '31.024', 'grad_norm': '30.5', 'counters/examples': 260608, 'counters/updates': 8144}
train stats after 260640 examples: {'rewards_train/chosen': '0.22077', 'rewards_train/rejected': '0.1242', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096563', 'logps_train/rejected': '-150.45', 'logps_train/chosen': '-186.41', 'loss/train': '0.66971', 'examples_per_second': '30.814', 'grad_norm': '45', 'counters/examples': 260640, 'counters/updates': 8145}
train stats after 260672 examples: {'rewards_train/chosen': '0.095613', 'rewards_train/rejected': '0.033372', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.062242', 'logps_train/rejected': '-166.32', 'logps_train/chosen': '-149.04', 'loss/train': '0.67683', 'examples_per_second': '31.231', 'grad_norm': '57.25', 'counters/examples': 260672, 'counters/updates': 8146}
skipping logging after 260704 examples to avoid logging too frequently
train stats after 260736 examples: {'rewards_train/chosen': '0.13427', 'rewards_train/rejected': '0.10382', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.030449', 'logps_train/rejected': '-137.12', 'logps_train/chosen': '-150.15', 'loss/train': '0.69094', 'examples_per_second': '34.825', 'grad_norm': '26.875', 'counters/examples': 260736, 'counters/updates': 8148}
train stats after 260768 examples: {'rewards_train/chosen': '0.1184', 'rewards_train/rejected': '-0.031662', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15006', 'logps_train/rejected': '-98.907', 'logps_train/chosen': '-147.62', 'loss/train': '0.63735', 'examples_per_second': '30.726', 'grad_norm': '26.875', 'counters/examples': 260768, 'counters/updates': 8149}
skipping logging after 260800 examples to avoid logging too frequently
train stats after 260832 examples: {'rewards_train/chosen': '0.20513', 'rewards_train/rejected': '0.091168', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11396', 'logps_train/rejected': '-121.57', 'logps_train/chosen': '-148.1', 'loss/train': '0.65804', 'examples_per_second': '31.103', 'grad_norm': '31.25', 'counters/examples': 260832, 'counters/updates': 8151}
skipping logging after 260864 examples to avoid logging too frequently
train stats after 260896 examples: {'rewards_train/chosen': '0.19863', 'rewards_train/rejected': '0.00609', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19254', 'logps_train/rejected': '-120.25', 'logps_train/chosen': '-121.45', 'loss/train': '0.62678', 'examples_per_second': '33.168', 'grad_norm': '30.875', 'counters/examples': 260896, 'counters/updates': 8153}
train stats after 260928 examples: {'rewards_train/chosen': '0.15299', 'rewards_train/rejected': '0.0096663', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14332', 'logps_train/rejected': '-134.58', 'logps_train/chosen': '-149.36', 'loss/train': '0.63607', 'examples_per_second': '31.367', 'grad_norm': '26.5', 'counters/examples': 260928, 'counters/updates': 8154}
train stats after 260960 examples: {'rewards_train/chosen': '0.19264', 'rewards_train/rejected': '0.11263', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080011', 'logps_train/rejected': '-97.575', 'logps_train/chosen': '-110.31', 'loss/train': '0.6644', 'examples_per_second': '31.602', 'grad_norm': '25', 'counters/examples': 260960, 'counters/updates': 8155}
skipping logging after 260992 examples to avoid logging too frequently
train stats after 261024 examples: {'rewards_train/chosen': '0.22746', 'rewards_train/rejected': '0.11321', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11425', 'logps_train/rejected': '-125.73', 'logps_train/chosen': '-122.83', 'loss/train': '0.6603', 'examples_per_second': '30.457', 'grad_norm': '32.75', 'counters/examples': 261024, 'counters/updates': 8157}
train stats after 261056 examples: {'rewards_train/chosen': '0.10475', 'rewards_train/rejected': '0.086426', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.018325', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-147.52', 'loss/train': '0.70218', 'examples_per_second': '30.779', 'grad_norm': '34.75', 'counters/examples': 261056, 'counters/updates': 8158}
train stats after 261088 examples: {'rewards_train/chosen': '0.26675', 'rewards_train/rejected': '0.020097', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24666', 'logps_train/rejected': '-113.25', 'logps_train/chosen': '-161.84', 'loss/train': '0.60964', 'examples_per_second': '32.517', 'grad_norm': '31.375', 'counters/examples': 261088, 'counters/updates': 8159}
train stats after 261120 examples: {'rewards_train/chosen': '0.14963', 'rewards_train/rejected': '0.059994', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.089637', 'logps_train/rejected': '-97.836', 'logps_train/chosen': '-123.27', 'loss/train': '0.65688', 'examples_per_second': '30.511', 'grad_norm': '27.25', 'counters/examples': 261120, 'counters/updates': 8160}
skipping logging after 261152 examples to avoid logging too frequently
train stats after 261184 examples: {'rewards_train/chosen': '0.19182', 'rewards_train/rejected': '0.03194', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15988', 'logps_train/rejected': '-126.93', 'logps_train/chosen': '-142.66', 'loss/train': '0.63641', 'examples_per_second': '31.552', 'grad_norm': '30.25', 'counters/examples': 261184, 'counters/updates': 8162}
train stats after 261216 examples: {'rewards_train/chosen': '0.2136', 'rewards_train/rejected': '0.11434', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.099252', 'logps_train/rejected': '-129.08', 'logps_train/chosen': '-162.89', 'loss/train': '0.67719', 'examples_per_second': '30.265', 'grad_norm': '35.5', 'counters/examples': 261216, 'counters/updates': 8163}
train stats after 261248 examples: {'rewards_train/chosen': '0.19574', 'rewards_train/rejected': '0.097998', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.097738', 'logps_train/rejected': '-109.06', 'logps_train/chosen': '-133.32', 'loss/train': '0.66907', 'examples_per_second': '30.88', 'grad_norm': '31.5', 'counters/examples': 261248, 'counters/updates': 8164}
train stats after 261280 examples: {'rewards_train/chosen': '0.1397', 'rewards_train/rejected': '0.031157', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10854', 'logps_train/rejected': '-87.356', 'logps_train/chosen': '-138.28', 'loss/train': '0.65534', 'examples_per_second': '30.738', 'grad_norm': '27.375', 'counters/examples': 261280, 'counters/updates': 8165}
train stats after 261312 examples: {'rewards_train/chosen': '0.14225', 'rewards_train/rejected': '0.0016423', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14061', 'logps_train/rejected': '-126.68', 'logps_train/chosen': '-136.12', 'loss/train': '0.63912', 'examples_per_second': '31.489', 'grad_norm': '31.625', 'counters/examples': 261312, 'counters/updates': 8166}
train stats after 261344 examples: {'rewards_train/chosen': '0.2015', 'rewards_train/rejected': '0.15645', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045052', 'logps_train/rejected': '-175.51', 'logps_train/chosen': '-182.31', 'loss/train': '0.6967', 'examples_per_second': '32.705', 'grad_norm': '39.75', 'counters/examples': 261344, 'counters/updates': 8167}
train stats after 261376 examples: {'rewards_train/chosen': '0.14093', 'rewards_train/rejected': '0.086805', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054125', 'logps_train/rejected': '-125.8', 'logps_train/chosen': '-114.83', 'loss/train': '0.69264', 'examples_per_second': '29.71', 'grad_norm': '68', 'counters/examples': 261376, 'counters/updates': 8168}
skipping logging after 261408 examples to avoid logging too frequently
train stats after 261440 examples: {'rewards_train/chosen': '0.45093', 'rewards_train/rejected': '0.15077', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.30016', 'logps_train/rejected': '-140.26', 'logps_train/chosen': '-170.82', 'loss/train': '0.59499', 'examples_per_second': '30.77', 'grad_norm': '35', 'counters/examples': 261440, 'counters/updates': 8170}
train stats after 261472 examples: {'rewards_train/chosen': '0.1659', 'rewards_train/rejected': '-0.017437', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18333', 'logps_train/rejected': '-102.44', 'logps_train/chosen': '-120.7', 'loss/train': '0.62169', 'examples_per_second': '31.479', 'grad_norm': '25.125', 'counters/examples': 261472, 'counters/updates': 8171}
train stats after 261504 examples: {'rewards_train/chosen': '0.25366', 'rewards_train/rejected': '0.0078799', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.24578', 'logps_train/rejected': '-124.25', 'logps_train/chosen': '-134.71', 'loss/train': '0.59471', 'examples_per_second': '33.068', 'grad_norm': '28.875', 'counters/examples': 261504, 'counters/updates': 8172}
train stats after 261536 examples: {'rewards_train/chosen': '0.24452', 'rewards_train/rejected': '0.086536', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15799', 'logps_train/rejected': '-146.22', 'logps_train/chosen': '-147.06', 'loss/train': '0.63706', 'examples_per_second': '31.457', 'grad_norm': '31.75', 'counters/examples': 261536, 'counters/updates': 8173}
train stats after 261568 examples: {'rewards_train/chosen': '0.13847', 'rewards_train/rejected': '0.024714', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11376', 'logps_train/rejected': '-108.59', 'logps_train/chosen': '-143.78', 'loss/train': '0.65211', 'examples_per_second': '32.024', 'grad_norm': '45.25', 'counters/examples': 261568, 'counters/updates': 8174}
train stats after 261600 examples: {'rewards_train/chosen': '0.15401', 'rewards_train/rejected': '-0.004389', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1584', 'logps_train/rejected': '-104.37', 'logps_train/chosen': '-109.67', 'loss/train': '0.63812', 'examples_per_second': '31.213', 'grad_norm': '24.625', 'counters/examples': 261600, 'counters/updates': 8175}
train stats after 261632 examples: {'rewards_train/chosen': '0.19732', 'rewards_train/rejected': '-0.0021578', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19948', 'logps_train/rejected': '-147.51', 'logps_train/chosen': '-155.02', 'loss/train': '0.63293', 'examples_per_second': '30.067', 'grad_norm': '40.75', 'counters/examples': 261632, 'counters/updates': 8176}
train stats after 261664 examples: {'rewards_train/chosen': '0.1906', 'rewards_train/rejected': '0.048506', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14209', 'logps_train/rejected': '-145.9', 'logps_train/chosen': '-136.28', 'loss/train': '0.63285', 'examples_per_second': '33.229', 'grad_norm': '25.75', 'counters/examples': 261664, 'counters/updates': 8177}
train stats after 261696 examples: {'rewards_train/chosen': '0.10962', 'rewards_train/rejected': '0.0039684', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10566', 'logps_train/rejected': '-101.05', 'logps_train/chosen': '-109.51', 'loss/train': '0.65997', 'examples_per_second': '31.683', 'grad_norm': '34.75', 'counters/examples': 261696, 'counters/updates': 8178}
skipping logging after 261728 examples to avoid logging too frequently
train stats after 261760 examples: {'rewards_train/chosen': '0.10791', 'rewards_train/rejected': '-0.011593', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11951', 'logps_train/rejected': '-93.418', 'logps_train/chosen': '-146.05', 'loss/train': '0.64498', 'examples_per_second': '33.759', 'grad_norm': '25.875', 'counters/examples': 261760, 'counters/updates': 8180}
train stats after 261792 examples: {'rewards_train/chosen': '0.12728', 'rewards_train/rejected': '0.0601', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067175', 'logps_train/rejected': '-118.77', 'logps_train/chosen': '-134.19', 'loss/train': '0.66759', 'examples_per_second': '31.272', 'grad_norm': '28.125', 'counters/examples': 261792, 'counters/updates': 8181}
train stats after 261824 examples: {'rewards_train/chosen': '0.17683', 'rewards_train/rejected': '-0.065714', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24254', 'logps_train/rejected': '-137.33', 'logps_train/chosen': '-143.03', 'loss/train': '0.59465', 'examples_per_second': '32.328', 'grad_norm': '26.625', 'counters/examples': 261824, 'counters/updates': 8182}
train stats after 261856 examples: {'rewards_train/chosen': '0.34439', 'rewards_train/rejected': '0.12613', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21826', 'logps_train/rejected': '-124.37', 'logps_train/chosen': '-148.4', 'loss/train': '0.61266', 'examples_per_second': '31.601', 'grad_norm': '26', 'counters/examples': 261856, 'counters/updates': 8183}
train stats after 261888 examples: {'rewards_train/chosen': '0.27009', 'rewards_train/rejected': '0.097609', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17248', 'logps_train/rejected': '-113.48', 'logps_train/chosen': '-165.36', 'loss/train': '0.6419', 'examples_per_second': '31.489', 'grad_norm': '26.875', 'counters/examples': 261888, 'counters/updates': 8184}
train stats after 261920 examples: {'rewards_train/chosen': '0.1472', 'rewards_train/rejected': '0.039256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10794', 'logps_train/rejected': '-158.31', 'logps_train/chosen': '-152.23', 'loss/train': '0.65375', 'examples_per_second': '30.748', 'grad_norm': '35.5', 'counters/examples': 261920, 'counters/updates': 8185}
train stats after 261952 examples: {'rewards_train/chosen': '0.16531', 'rewards_train/rejected': '0.021424', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14388', 'logps_train/rejected': '-110.6', 'logps_train/chosen': '-130.76', 'loss/train': '0.64079', 'examples_per_second': '32.864', 'grad_norm': '26.875', 'counters/examples': 261952, 'counters/updates': 8186}
skipping logging after 261984 examples to avoid logging too frequently
train stats after 262016 examples: {'rewards_train/chosen': '0.20224', 'rewards_train/rejected': '-0.055223', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.25746', 'logps_train/rejected': '-97.712', 'logps_train/chosen': '-145.56', 'loss/train': '0.59812', 'examples_per_second': '32.095', 'grad_norm': '27.125', 'counters/examples': 262016, 'counters/updates': 8188}
train stats after 262048 examples: {'rewards_train/chosen': '0.13418', 'rewards_train/rejected': '0.080313', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.053869', 'logps_train/rejected': '-165.37', 'logps_train/chosen': '-169.16', 'loss/train': '0.677', 'examples_per_second': '30.039', 'grad_norm': '53.5', 'counters/examples': 262048, 'counters/updates': 8189}
train stats after 262080 examples: {'rewards_train/chosen': '0.18868', 'rewards_train/rejected': '0.037172', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15151', 'logps_train/rejected': '-104.69', 'logps_train/chosen': '-134.58', 'loss/train': '0.64406', 'examples_per_second': '31.559', 'grad_norm': '25.125', 'counters/examples': 262080, 'counters/updates': 8190}
train stats after 262112 examples: {'rewards_train/chosen': '0.18928', 'rewards_train/rejected': '0.1443', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.044979', 'logps_train/rejected': '-162.74', 'logps_train/chosen': '-165.22', 'loss/train': '0.68765', 'examples_per_second': '30.545', 'grad_norm': '62.5', 'counters/examples': 262112, 'counters/updates': 8191}
train stats after 262144 examples: {'rewards_train/chosen': '0.24336', 'rewards_train/rejected': '0.089749', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15361', 'logps_train/rejected': '-133.55', 'logps_train/chosen': '-153.91', 'loss/train': '0.64474', 'examples_per_second': '31.81', 'grad_norm': '40', 'counters/examples': 262144, 'counters/updates': 8192}
train stats after 262176 examples: {'rewards_train/chosen': '0.20166', 'rewards_train/rejected': '-0.039423', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.24108', 'logps_train/rejected': '-113.97', 'logps_train/chosen': '-134.27', 'loss/train': '0.59737', 'examples_per_second': '30.532', 'grad_norm': '31.375', 'counters/examples': 262176, 'counters/updates': 8193}
train stats after 262208 examples: {'rewards_train/chosen': '0.21167', 'rewards_train/rejected': '0.24916', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.037495', 'logps_train/rejected': '-143.47', 'logps_train/chosen': '-132.79', 'loss/train': '0.73495', 'examples_per_second': '32.39', 'grad_norm': '46', 'counters/examples': 262208, 'counters/updates': 8194}
train stats after 262240 examples: {'rewards_train/chosen': '-0.0064837', 'rewards_train/rejected': '0.016419', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.022902', 'logps_train/rejected': '-153.08', 'logps_train/chosen': '-159.35', 'loss/train': '0.7319', 'examples_per_second': '31.367', 'grad_norm': '40.25', 'counters/examples': 262240, 'counters/updates': 8195}
train stats after 262272 examples: {'rewards_train/chosen': '0.14544', 'rewards_train/rejected': '0.084501', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.060938', 'logps_train/rejected': '-106.99', 'logps_train/chosen': '-131.82', 'loss/train': '0.67713', 'examples_per_second': '31.619', 'grad_norm': '28', 'counters/examples': 262272, 'counters/updates': 8196}
train stats after 262304 examples: {'rewards_train/chosen': '0.13914', 'rewards_train/rejected': '-0.015535', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.15467', 'logps_train/rejected': '-99.418', 'logps_train/chosen': '-142.82', 'loss/train': '0.62957', 'examples_per_second': '31.525', 'grad_norm': '22.875', 'counters/examples': 262304, 'counters/updates': 8197}
train stats after 262336 examples: {'rewards_train/chosen': '0.18829', 'rewards_train/rejected': '0.14728', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.041011', 'logps_train/rejected': '-145.56', 'logps_train/chosen': '-151.86', 'loss/train': '0.6924', 'examples_per_second': '31.389', 'grad_norm': '62.25', 'counters/examples': 262336, 'counters/updates': 8198}
train stats after 262368 examples: {'rewards_train/chosen': '0.22331', 'rewards_train/rejected': '0.094535', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12877', 'logps_train/rejected': '-137.72', 'logps_train/chosen': '-144.74', 'loss/train': '0.6496', 'examples_per_second': '31.469', 'grad_norm': '35.75', 'counters/examples': 262368, 'counters/updates': 8199}
train stats after 262400 examples: {'rewards_train/chosen': '0.058112', 'rewards_train/rejected': '0.03807', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.020042', 'logps_train/rejected': '-142.48', 'logps_train/chosen': '-103', 'loss/train': '0.69718', 'examples_per_second': '31.527', 'grad_norm': '24.75', 'counters/examples': 262400, 'counters/updates': 8200}
train stats after 262432 examples: {'rewards_train/chosen': '0.17664', 'rewards_train/rejected': '0.10167', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.074966', 'logps_train/rejected': '-127.65', 'logps_train/chosen': '-112.7', 'loss/train': '0.67325', 'examples_per_second': '30.139', 'grad_norm': '23.75', 'counters/examples': 262432, 'counters/updates': 8201}
skipping logging after 262464 examples to avoid logging too frequently
train stats after 262496 examples: {'rewards_train/chosen': '0.23783', 'rewards_train/rejected': '0.15903', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.078798', 'logps_train/rejected': '-140.88', 'logps_train/chosen': '-161.79', 'loss/train': '0.67991', 'examples_per_second': '31.68', 'grad_norm': '32.75', 'counters/examples': 262496, 'counters/updates': 8203}
train stats after 262528 examples: {'rewards_train/chosen': '0.18063', 'rewards_train/rejected': '0.045773', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13485', 'logps_train/rejected': '-136.73', 'logps_train/chosen': '-150.48', 'loss/train': '0.64064', 'examples_per_second': '23.448', 'grad_norm': '35', 'counters/examples': 262528, 'counters/updates': 8204}
train stats after 262560 examples: {'rewards_train/chosen': '0.14695', 'rewards_train/rejected': '0.002959', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.14399', 'logps_train/rejected': '-144.72', 'logps_train/chosen': '-142.45', 'loss/train': '0.65257', 'examples_per_second': '31.423', 'grad_norm': '25.125', 'counters/examples': 262560, 'counters/updates': 8205}
train stats after 262592 examples: {'rewards_train/chosen': '0.097397', 'rewards_train/rejected': '-0.0018124', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099209', 'logps_train/rejected': '-117.52', 'logps_train/chosen': '-141.83', 'loss/train': '0.65516', 'examples_per_second': '31.536', 'grad_norm': '27', 'counters/examples': 262592, 'counters/updates': 8206}
train stats after 262624 examples: {'rewards_train/chosen': '0.2439', 'rewards_train/rejected': '0.10149', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14242', 'logps_train/rejected': '-115.75', 'logps_train/chosen': '-157.28', 'loss/train': '0.64389', 'examples_per_second': '26.767', 'grad_norm': '36.25', 'counters/examples': 262624, 'counters/updates': 8207}
train stats after 262656 examples: {'rewards_train/chosen': '0.21044', 'rewards_train/rejected': '0.14084', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.069599', 'logps_train/rejected': '-141.18', 'logps_train/chosen': '-137.82', 'loss/train': '0.68029', 'examples_per_second': '30.49', 'grad_norm': '31', 'counters/examples': 262656, 'counters/updates': 8208}
train stats after 262688 examples: {'rewards_train/chosen': '0.35479', 'rewards_train/rejected': '0.17509', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1797', 'logps_train/rejected': '-149.37', 'logps_train/chosen': '-177.46', 'loss/train': '0.62422', 'examples_per_second': '33.016', 'grad_norm': '38', 'counters/examples': 262688, 'counters/updates': 8209}
skipping logging after 262720 examples to avoid logging too frequently
train stats after 262752 examples: {'rewards_train/chosen': '0.068328', 'rewards_train/rejected': '0.071165', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '-0.0028368', 'logps_train/rejected': '-160.17', 'logps_train/chosen': '-129.5', 'loss/train': '0.72369', 'examples_per_second': '31.613', 'grad_norm': '38.5', 'counters/examples': 262752, 'counters/updates': 8211}
train stats after 262784 examples: {'rewards_train/chosen': '0.19367', 'rewards_train/rejected': '0.028435', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16524', 'logps_train/rejected': '-151.71', 'logps_train/chosen': '-145.64', 'loss/train': '0.63234', 'examples_per_second': '31.592', 'grad_norm': '28.875', 'counters/examples': 262784, 'counters/updates': 8212}
train stats after 262816 examples: {'rewards_train/chosen': '0.13469', 'rewards_train/rejected': '0.044028', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.090665', 'logps_train/rejected': '-134.45', 'logps_train/chosen': '-149.63', 'loss/train': '0.66242', 'examples_per_second': '31.197', 'grad_norm': '28.75', 'counters/examples': 262816, 'counters/updates': 8213}
train stats after 262848 examples: {'rewards_train/chosen': '0.15619', 'rewards_train/rejected': '0.11638', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.039808', 'logps_train/rejected': '-158.54', 'logps_train/chosen': '-185.39', 'loss/train': '0.68219', 'examples_per_second': '29.917', 'grad_norm': '32.5', 'counters/examples': 262848, 'counters/updates': 8214}
skipping logging after 262880 examples to avoid logging too frequently
train stats after 262912 examples: {'rewards_train/chosen': '0.2368', 'rewards_train/rejected': '0.091047', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14575', 'logps_train/rejected': '-136.59', 'logps_train/chosen': '-161.67', 'loss/train': '0.63972', 'examples_per_second': '34.17', 'grad_norm': '34.25', 'counters/examples': 262912, 'counters/updates': 8216}
skipping logging after 262944 examples to avoid logging too frequently
train stats after 262976 examples: {'rewards_train/chosen': '0.16663', 'rewards_train/rejected': '0.025136', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1415', 'logps_train/rejected': '-160.85', 'logps_train/chosen': '-154.75', 'loss/train': '0.64098', 'examples_per_second': '31.325', 'grad_norm': '41.25', 'counters/examples': 262976, 'counters/updates': 8218}
train stats after 263008 examples: {'rewards_train/chosen': '0.30689', 'rewards_train/rejected': '0.13838', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16851', 'logps_train/rejected': '-141.98', 'logps_train/chosen': '-131.17', 'loss/train': '0.646', 'examples_per_second': '30.369', 'grad_norm': '30.375', 'counters/examples': 263008, 'counters/updates': 8219}
skipping logging after 263040 examples to avoid logging too frequently
train stats after 263072 examples: {'rewards_train/chosen': '0.18597', 'rewards_train/rejected': '0.099973', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.086001', 'logps_train/rejected': '-144.61', 'logps_train/chosen': '-148.92', 'loss/train': '0.66305', 'examples_per_second': '30.857', 'grad_norm': '37.25', 'counters/examples': 263072, 'counters/updates': 8221}
skipping logging after 263104 examples to avoid logging too frequently
train stats after 263136 examples: {'rewards_train/chosen': '0.23151', 'rewards_train/rejected': '0.11696', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11455', 'logps_train/rejected': '-156.64', 'logps_train/chosen': '-163.62', 'loss/train': '0.64689', 'examples_per_second': '31.508', 'grad_norm': '34.5', 'counters/examples': 263136, 'counters/updates': 8223}
train stats after 263168 examples: {'rewards_train/chosen': '0.21887', 'rewards_train/rejected': '0.079497', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13937', 'logps_train/rejected': '-141.78', 'logps_train/chosen': '-161.5', 'loss/train': '0.64402', 'examples_per_second': '32.529', 'grad_norm': '32.75', 'counters/examples': 263168, 'counters/updates': 8224}
train stats after 263200 examples: {'rewards_train/chosen': '0.17859', 'rewards_train/rejected': '-0.00072285', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17932', 'logps_train/rejected': '-108.44', 'logps_train/chosen': '-143.32', 'loss/train': '0.62162', 'examples_per_second': '30.947', 'grad_norm': '26.875', 'counters/examples': 263200, 'counters/updates': 8225}
train stats after 263232 examples: {'rewards_train/chosen': '0.15344', 'rewards_train/rejected': '0.038094', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11534', 'logps_train/rejected': '-116.78', 'logps_train/chosen': '-172.88', 'loss/train': '0.65098', 'examples_per_second': '32.671', 'grad_norm': '35.25', 'counters/examples': 263232, 'counters/updates': 8226}
train stats after 263264 examples: {'rewards_train/chosen': '0.28665', 'rewards_train/rejected': '0.12355', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1631', 'logps_train/rejected': '-174.55', 'logps_train/chosen': '-192.79', 'loss/train': '0.63302', 'examples_per_second': '31.333', 'grad_norm': '34', 'counters/examples': 263264, 'counters/updates': 8227}
train stats after 263296 examples: {'rewards_train/chosen': '0.24667', 'rewards_train/rejected': '0.04353', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20314', 'logps_train/rejected': '-146.23', 'logps_train/chosen': '-185.95', 'loss/train': '0.61341', 'examples_per_second': '30.633', 'grad_norm': '32.75', 'counters/examples': 263296, 'counters/updates': 8228}
train stats after 263328 examples: {'rewards_train/chosen': '0.27723', 'rewards_train/rejected': '0.061512', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.21572', 'logps_train/rejected': '-123.34', 'logps_train/chosen': '-161.13', 'loss/train': '0.60505', 'examples_per_second': '32.668', 'grad_norm': '35', 'counters/examples': 263328, 'counters/updates': 8229}
train stats after 263360 examples: {'rewards_train/chosen': '0.15486', 'rewards_train/rejected': '0.047728', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10713', 'logps_train/rejected': '-121.99', 'logps_train/chosen': '-173.08', 'loss/train': '0.65611', 'examples_per_second': '31.733', 'grad_norm': '39.25', 'counters/examples': 263360, 'counters/updates': 8230}
train stats after 263392 examples: {'rewards_train/chosen': '0.23981', 'rewards_train/rejected': '0.053407', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1864', 'logps_train/rejected': '-103.8', 'logps_train/chosen': '-151.86', 'loss/train': '0.62274', 'examples_per_second': '31.493', 'grad_norm': '33.25', 'counters/examples': 263392, 'counters/updates': 8231}
train stats after 263424 examples: {'rewards_train/chosen': '0.14683', 'rewards_train/rejected': '-0.012506', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15934', 'logps_train/rejected': '-98.403', 'logps_train/chosen': '-154.84', 'loss/train': '0.62358', 'examples_per_second': '31.507', 'grad_norm': '30', 'counters/examples': 263424, 'counters/updates': 8232}
train stats after 263456 examples: {'rewards_train/chosen': '0.094217', 'rewards_train/rejected': '0.0031811', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.091036', 'logps_train/rejected': '-101.53', 'logps_train/chosen': '-121.62', 'loss/train': '0.66073', 'examples_per_second': '31.522', 'grad_norm': '38', 'counters/examples': 263456, 'counters/updates': 8233}
train stats after 263488 examples: {'rewards_train/chosen': '0.22069', 'rewards_train/rejected': '0.014041', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20665', 'logps_train/rejected': '-164.03', 'logps_train/chosen': '-142.21', 'loss/train': '0.62952', 'examples_per_second': '31.398', 'grad_norm': '43.5', 'counters/examples': 263488, 'counters/updates': 8234}
train stats after 263520 examples: {'rewards_train/chosen': '0.20289', 'rewards_train/rejected': '0.08555', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11734', 'logps_train/rejected': '-150.51', 'logps_train/chosen': '-172.2', 'loss/train': '0.64537', 'examples_per_second': '23.85', 'grad_norm': '39.25', 'counters/examples': 263520, 'counters/updates': 8235}
train stats after 263552 examples: {'rewards_train/chosen': '0.13042', 'rewards_train/rejected': '0.066617', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063803', 'logps_train/rejected': '-142.51', 'logps_train/chosen': '-148.26', 'loss/train': '0.67677', 'examples_per_second': '29.975', 'grad_norm': '29.625', 'counters/examples': 263552, 'counters/updates': 8236}
train stats after 263584 examples: {'rewards_train/chosen': '0.19033', 'rewards_train/rejected': '0.042356', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14797', 'logps_train/rejected': '-104.21', 'logps_train/chosen': '-124.32', 'loss/train': '0.64818', 'examples_per_second': '31.13', 'grad_norm': '23.375', 'counters/examples': 263584, 'counters/updates': 8237}
skipping logging after 263616 examples to avoid logging too frequently
train stats after 263648 examples: {'rewards_train/chosen': '0.15263', 'rewards_train/rejected': '-0.003506', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15614', 'logps_train/rejected': '-137.75', 'logps_train/chosen': '-125.94', 'loss/train': '0.62841', 'examples_per_second': '35.593', 'grad_norm': '26.5', 'counters/examples': 263648, 'counters/updates': 8239}
train stats after 263680 examples: {'rewards_train/chosen': '0.13446', 'rewards_train/rejected': '0.055553', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078908', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-118.41', 'loss/train': '0.66799', 'examples_per_second': '32.09', 'grad_norm': '25.125', 'counters/examples': 263680, 'counters/updates': 8240}
train stats after 263712 examples: {'rewards_train/chosen': '0.19257', 'rewards_train/rejected': '0.06071', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13186', 'logps_train/rejected': '-153.54', 'logps_train/chosen': '-152.58', 'loss/train': '0.65028', 'examples_per_second': '31.845', 'grad_norm': '40.75', 'counters/examples': 263712, 'counters/updates': 8241}
train stats after 263744 examples: {'rewards_train/chosen': '0.16673', 'rewards_train/rejected': '0.026833', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1399', 'logps_train/rejected': '-135.09', 'logps_train/chosen': '-117.07', 'loss/train': '0.63273', 'examples_per_second': '30.352', 'grad_norm': '26.125', 'counters/examples': 263744, 'counters/updates': 8242}
train stats after 263776 examples: {'rewards_train/chosen': '0.22095', 'rewards_train/rejected': '0.063707', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15724', 'logps_train/rejected': '-122.97', 'logps_train/chosen': '-110.55', 'loss/train': '0.6347', 'examples_per_second': '31.476', 'grad_norm': '24.75', 'counters/examples': 263776, 'counters/updates': 8243}
train stats after 263808 examples: {'rewards_train/chosen': '0.19793', 'rewards_train/rejected': '0.085875', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11206', 'logps_train/rejected': '-155.4', 'logps_train/chosen': '-190.46', 'loss/train': '0.65189', 'examples_per_second': '31.637', 'grad_norm': '28.625', 'counters/examples': 263808, 'counters/updates': 8244}
train stats after 263840 examples: {'rewards_train/chosen': '0.26704', 'rewards_train/rejected': '0.095322', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.17172', 'logps_train/rejected': '-158.58', 'logps_train/chosen': '-168.17', 'loss/train': '0.63352', 'examples_per_second': '29.996', 'grad_norm': '29.25', 'counters/examples': 263840, 'counters/updates': 8245}
skipping logging after 263872 examples to avoid logging too frequently
train stats after 263904 examples: {'rewards_train/chosen': '0.17641', 'rewards_train/rejected': '0.026826', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14959', 'logps_train/rejected': '-95.095', 'logps_train/chosen': '-141.88', 'loss/train': '0.6311', 'examples_per_second': '31.462', 'grad_norm': '28.125', 'counters/examples': 263904, 'counters/updates': 8247}
train stats after 263936 examples: {'rewards_train/chosen': '0.24221', 'rewards_train/rejected': '0.03955', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20265', 'logps_train/rejected': '-113.31', 'logps_train/chosen': '-179.22', 'loss/train': '0.62071', 'examples_per_second': '29.94', 'grad_norm': '37.75', 'counters/examples': 263936, 'counters/updates': 8248}
train stats after 263968 examples: {'rewards_train/chosen': '0.15815', 'rewards_train/rejected': '0.055155', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10299', 'logps_train/rejected': '-115.8', 'logps_train/chosen': '-146.22', 'loss/train': '0.65062', 'examples_per_second': '30.923', 'grad_norm': '35', 'counters/examples': 263968, 'counters/updates': 8249}
train stats after 264000 examples: {'rewards_train/chosen': '0.15726', 'rewards_train/rejected': '0.090154', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.067103', 'logps_train/rejected': '-130.91', 'logps_train/chosen': '-178.62', 'loss/train': '0.66982', 'examples_per_second': '30.796', 'grad_norm': '35.75', 'counters/examples': 264000, 'counters/updates': 8250}
Running evaluation after 264000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.24it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.84it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.94it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.93it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.99it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.91it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.92it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 264000: {'rewards_eval/chosen': '0.17955', 'rewards_eval/rejected': '0.068384', 'rewards_eval/accuracies': '0.57422', 'rewards_eval/margins': '0.11116', 'logps_eval/rejected': '-121.45', 'logps_eval/chosen': '-142.31', 'loss/eval': '0.66087'}
skipping save for non epoch
train stats after 264032 examples: {'rewards_train/chosen': '0.1407', 'rewards_train/rejected': '0.00059566', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1401', 'logps_train/rejected': '-133.29', 'logps_train/chosen': '-148.6', 'loss/train': '0.64885', 'examples_per_second': '36.132', 'grad_norm': '31', 'counters/examples': 264032, 'counters/updates': 8251}
train stats after 264064 examples: {'rewards_train/chosen': '0.095496', 'rewards_train/rejected': '0.095962', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.0004657', 'logps_train/rejected': '-114.63', 'logps_train/chosen': '-129.5', 'loss/train': '0.70269', 'examples_per_second': '32.382', 'grad_norm': '32.75', 'counters/examples': 264064, 'counters/updates': 8252}
train stats after 264096 examples: {'rewards_train/chosen': '0.10445', 'rewards_train/rejected': '0.026334', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07812', 'logps_train/rejected': '-99.386', 'logps_train/chosen': '-147.44', 'loss/train': '0.66836', 'examples_per_second': '32.912', 'grad_norm': '32', 'counters/examples': 264096, 'counters/updates': 8253}
train stats after 264128 examples: {'rewards_train/chosen': '0.10117', 'rewards_train/rejected': '-0.019095', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12027', 'logps_train/rejected': '-139.02', 'logps_train/chosen': '-169.35', 'loss/train': '0.64979', 'examples_per_second': '31.465', 'grad_norm': '43.75', 'counters/examples': 264128, 'counters/updates': 8254}
train stats after 264160 examples: {'rewards_train/chosen': '0.22781', 'rewards_train/rejected': '0.061405', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16641', 'logps_train/rejected': '-94.494', 'logps_train/chosen': '-146.28', 'loss/train': '0.63348', 'examples_per_second': '31.649', 'grad_norm': '30', 'counters/examples': 264160, 'counters/updates': 8255}
train stats after 264192 examples: {'rewards_train/chosen': '0.11454', 'rewards_train/rejected': '0.073649', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040888', 'logps_train/rejected': '-122', 'logps_train/chosen': '-147.75', 'loss/train': '0.6945', 'examples_per_second': '31.848', 'grad_norm': '37.5', 'counters/examples': 264192, 'counters/updates': 8256}
train stats after 264224 examples: {'rewards_train/chosen': '0.21088', 'rewards_train/rejected': '0.031078', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1798', 'logps_train/rejected': '-96.641', 'logps_train/chosen': '-121.98', 'loss/train': '0.62877', 'examples_per_second': '30.779', 'grad_norm': '24.75', 'counters/examples': 264224, 'counters/updates': 8257}
train stats after 264256 examples: {'rewards_train/chosen': '0.10104', 'rewards_train/rejected': '-0.0037757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10482', 'logps_train/rejected': '-135.3', 'logps_train/chosen': '-114.97', 'loss/train': '0.66015', 'examples_per_second': '30.378', 'grad_norm': '26.875', 'counters/examples': 264256, 'counters/updates': 8258}
skipping logging after 264288 examples to avoid logging too frequently
train stats after 264320 examples: {'rewards_train/chosen': '0.084534', 'rewards_train/rejected': '0.067247', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.017287', 'logps_train/rejected': '-108.6', 'logps_train/chosen': '-158.63', 'loss/train': '0.70007', 'examples_per_second': '31.488', 'grad_norm': '31.625', 'counters/examples': 264320, 'counters/updates': 8260}
train stats after 264352 examples: {'rewards_train/chosen': '0.13071', 'rewards_train/rejected': '0.12585', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0048599', 'logps_train/rejected': '-113.71', 'logps_train/chosen': '-114.27', 'loss/train': '0.69611', 'examples_per_second': '31.19', 'grad_norm': '33.5', 'counters/examples': 264352, 'counters/updates': 8261}
train stats after 264384 examples: {'rewards_train/chosen': '0.12077', 'rewards_train/rejected': '0.056617', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '0.064157', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-151.64', 'loss/train': '0.68617', 'examples_per_second': '31.514', 'grad_norm': '34.75', 'counters/examples': 264384, 'counters/updates': 8262}
train stats after 264416 examples: {'rewards_train/chosen': '0.1596', 'rewards_train/rejected': '0.012509', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1471', 'logps_train/rejected': '-154.59', 'logps_train/chosen': '-114.02', 'loss/train': '0.64066', 'examples_per_second': '31.245', 'grad_norm': '36', 'counters/examples': 264416, 'counters/updates': 8263}
train stats after 264448 examples: {'rewards_train/chosen': '0.24217', 'rewards_train/rejected': '0.14988', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092287', 'logps_train/rejected': '-167.86', 'logps_train/chosen': '-167.95', 'loss/train': '0.66048', 'examples_per_second': '29.831', 'grad_norm': '34.5', 'counters/examples': 264448, 'counters/updates': 8264}
train stats after 264480 examples: {'rewards_train/chosen': '0.21356', 'rewards_train/rejected': '0.051586', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16197', 'logps_train/rejected': '-139.46', 'logps_train/chosen': '-131.52', 'loss/train': '0.62951', 'examples_per_second': '32.551', 'grad_norm': '56.75', 'counters/examples': 264480, 'counters/updates': 8265}
train stats after 264512 examples: {'rewards_train/chosen': '0.19822', 'rewards_train/rejected': '0.0060856', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19214', 'logps_train/rejected': '-117.59', 'logps_train/chosen': '-143.03', 'loss/train': '0.61816', 'examples_per_second': '31.514', 'grad_norm': '38.75', 'counters/examples': 264512, 'counters/updates': 8266}
train stats after 264544 examples: {'rewards_train/chosen': '0.16948', 'rewards_train/rejected': '-0.012732', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18221', 'logps_train/rejected': '-115.81', 'logps_train/chosen': '-144.51', 'loss/train': '0.62287', 'examples_per_second': '30.694', 'grad_norm': '29.875', 'counters/examples': 264544, 'counters/updates': 8267}
train stats after 264576 examples: {'rewards_train/chosen': '0.1733', 'rewards_train/rejected': '0.14694', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.026365', 'logps_train/rejected': '-132.45', 'logps_train/chosen': '-135.78', 'loss/train': '0.68816', 'examples_per_second': '30.117', 'grad_norm': '28.25', 'counters/examples': 264576, 'counters/updates': 8268}
train stats after 264608 examples: {'rewards_train/chosen': '0.21714', 'rewards_train/rejected': '0.10289', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11424', 'logps_train/rejected': '-109.84', 'logps_train/chosen': '-170.23', 'loss/train': '0.66189', 'examples_per_second': '30.098', 'grad_norm': '38.5', 'counters/examples': 264608, 'counters/updates': 8269}
train stats after 264640 examples: {'rewards_train/chosen': '0.12041', 'rewards_train/rejected': '0.017481', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10293', 'logps_train/rejected': '-123.94', 'logps_train/chosen': '-150.26', 'loss/train': '0.66956', 'examples_per_second': '33.005', 'grad_norm': '37.25', 'counters/examples': 264640, 'counters/updates': 8270}
train stats after 264672 examples: {'rewards_train/chosen': '0.17015', 'rewards_train/rejected': '0.049209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12094', 'logps_train/rejected': '-114.23', 'logps_train/chosen': '-114.31', 'loss/train': '0.651', 'examples_per_second': '32.914', 'grad_norm': '26.5', 'counters/examples': 264672, 'counters/updates': 8271}
train stats after 264704 examples: {'rewards_train/chosen': '0.23323', 'rewards_train/rejected': '0.079629', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1536', 'logps_train/rejected': '-115.79', 'logps_train/chosen': '-147.48', 'loss/train': '0.63267', 'examples_per_second': '32.056', 'grad_norm': '31', 'counters/examples': 264704, 'counters/updates': 8272}
skipping logging after 264736 examples to avoid logging too frequently
train stats after 264768 examples: {'rewards_train/chosen': '0.15717', 'rewards_train/rejected': '0.026656', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13051', 'logps_train/rejected': '-117.64', 'logps_train/chosen': '-172.05', 'loss/train': '0.64613', 'examples_per_second': '34.755', 'grad_norm': '30.25', 'counters/examples': 264768, 'counters/updates': 8274}
skipping logging after 264800 examples to avoid logging too frequently
train stats after 264832 examples: {'rewards_train/chosen': '0.15036', 'rewards_train/rejected': '0.043496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10687', 'logps_train/rejected': '-120.9', 'logps_train/chosen': '-123.48', 'loss/train': '0.65429', 'examples_per_second': '31.625', 'grad_norm': '26.75', 'counters/examples': 264832, 'counters/updates': 8276}
train stats after 264864 examples: {'rewards_train/chosen': '0.091635', 'rewards_train/rejected': '0.016307', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.075328', 'logps_train/rejected': '-131.75', 'logps_train/chosen': '-138.85', 'loss/train': '0.66654', 'examples_per_second': '30.494', 'grad_norm': '35.5', 'counters/examples': 264864, 'counters/updates': 8277}
train stats after 264896 examples: {'rewards_train/chosen': '0.17703', 'rewards_train/rejected': '0.10364', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.073398', 'logps_train/rejected': '-121.26', 'logps_train/chosen': '-145.06', 'loss/train': '0.67363', 'examples_per_second': '31.794', 'grad_norm': '26', 'counters/examples': 264896, 'counters/updates': 8278}
train stats after 264928 examples: {'rewards_train/chosen': '0.087228', 'rewards_train/rejected': '0.078585', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0086435', 'logps_train/rejected': '-139.6', 'logps_train/chosen': '-142.66', 'loss/train': '0.70509', 'examples_per_second': '30.488', 'grad_norm': '41.25', 'counters/examples': 264928, 'counters/updates': 8279}
train stats after 264960 examples: {'rewards_train/chosen': '0.24471', 'rewards_train/rejected': '0.12819', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11652', 'logps_train/rejected': '-134.35', 'logps_train/chosen': '-152.66', 'loss/train': '0.65421', 'examples_per_second': '30.452', 'grad_norm': '36.25', 'counters/examples': 264960, 'counters/updates': 8280}
train stats after 264992 examples: {'rewards_train/chosen': '0.24207', 'rewards_train/rejected': '0.034798', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20727', 'logps_train/rejected': '-147.47', 'logps_train/chosen': '-162.27', 'loss/train': '0.61358', 'examples_per_second': '32.261', 'grad_norm': '40', 'counters/examples': 264992, 'counters/updates': 8281}
train stats after 265024 examples: {'rewards_train/chosen': '0.19156', 'rewards_train/rejected': '-0.028302', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.21987', 'logps_train/rejected': '-121.98', 'logps_train/chosen': '-133.23', 'loss/train': '0.60184', 'examples_per_second': '31.624', 'grad_norm': '25.875', 'counters/examples': 265024, 'counters/updates': 8282}
train stats after 265056 examples: {'rewards_train/chosen': '0.23269', 'rewards_train/rejected': '0.094191', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1385', 'logps_train/rejected': '-152.35', 'logps_train/chosen': '-165.63', 'loss/train': '0.63592', 'examples_per_second': '29.902', 'grad_norm': '29.625', 'counters/examples': 265056, 'counters/updates': 8283}
train stats after 265088 examples: {'rewards_train/chosen': '0.19205', 'rewards_train/rejected': '0.10668', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08537', 'logps_train/rejected': '-119.29', 'logps_train/chosen': '-130.34', 'loss/train': '0.66416', 'examples_per_second': '32.535', 'grad_norm': '30.75', 'counters/examples': 265088, 'counters/updates': 8284}
train stats after 265120 examples: {'rewards_train/chosen': '0.1071', 'rewards_train/rejected': '0.12535', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.018254', 'logps_train/rejected': '-143.09', 'logps_train/chosen': '-150.69', 'loss/train': '0.7127', 'examples_per_second': '31.33', 'grad_norm': '38.5', 'counters/examples': 265120, 'counters/updates': 8285}
train stats after 265152 examples: {'rewards_train/chosen': '0.14167', 'rewards_train/rejected': '0.091268', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.050398', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-129.78', 'loss/train': '0.69642', 'examples_per_second': '30.889', 'grad_norm': '33.25', 'counters/examples': 265152, 'counters/updates': 8286}
train stats after 265184 examples: {'rewards_train/chosen': '0.11637', 'rewards_train/rejected': '-0.02669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14306', 'logps_train/rejected': '-140.24', 'logps_train/chosen': '-118.75', 'loss/train': '0.64571', 'examples_per_second': '32.315', 'grad_norm': '27.375', 'counters/examples': 265184, 'counters/updates': 8287}
skipping logging after 265216 examples to avoid logging too frequently
train stats after 265248 examples: {'rewards_train/chosen': '0.21419', 'rewards_train/rejected': '0.038553', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17564', 'logps_train/rejected': '-98.014', 'logps_train/chosen': '-163.8', 'loss/train': '0.63845', 'examples_per_second': '30.022', 'grad_norm': '35.25', 'counters/examples': 265248, 'counters/updates': 8289}
train stats after 265280 examples: {'rewards_train/chosen': '0.13148', 'rewards_train/rejected': '0.029262', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10222', 'logps_train/rejected': '-125.78', 'logps_train/chosen': '-160.71', 'loss/train': '0.65412', 'examples_per_second': '32.145', 'grad_norm': '32', 'counters/examples': 265280, 'counters/updates': 8290}
train stats after 265312 examples: {'rewards_train/chosen': '0.15201', 'rewards_train/rejected': '0.040082', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11193', 'logps_train/rejected': '-144.04', 'logps_train/chosen': '-169.14', 'loss/train': '0.66599', 'examples_per_second': '31.382', 'grad_norm': '36.75', 'counters/examples': 265312, 'counters/updates': 8291}
train stats after 265344 examples: {'rewards_train/chosen': '0.23587', 'rewards_train/rejected': '0.025107', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21076', 'logps_train/rejected': '-122.76', 'logps_train/chosen': '-129.52', 'loss/train': '0.61189', 'examples_per_second': '33.021', 'grad_norm': '27.5', 'counters/examples': 265344, 'counters/updates': 8292}
train stats after 265376 examples: {'rewards_train/chosen': '0.11379', 'rewards_train/rejected': '0.025384', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.088402', 'logps_train/rejected': '-105.71', 'logps_train/chosen': '-134.85', 'loss/train': '0.65822', 'examples_per_second': '31.485', 'grad_norm': '25.75', 'counters/examples': 265376, 'counters/updates': 8293}
train stats after 265408 examples: {'rewards_train/chosen': '0.18', 'rewards_train/rejected': '0.11163', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.068375', 'logps_train/rejected': '-146.02', 'logps_train/chosen': '-140.4', 'loss/train': '0.67409', 'examples_per_second': '31.25', 'grad_norm': '37.75', 'counters/examples': 265408, 'counters/updates': 8294}
skipping logging after 265440 examples to avoid logging too frequently
train stats after 265472 examples: {'rewards_train/chosen': '0.085096', 'rewards_train/rejected': '-0.027474', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11257', 'logps_train/rejected': '-103.49', 'logps_train/chosen': '-106.27', 'loss/train': '0.6563', 'examples_per_second': '34.349', 'grad_norm': '25.125', 'counters/examples': 265472, 'counters/updates': 8296}
train stats after 265504 examples: {'rewards_train/chosen': '0.1992', 'rewards_train/rejected': '0.13544', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063759', 'logps_train/rejected': '-126.84', 'logps_train/chosen': '-173.79', 'loss/train': '0.67841', 'examples_per_second': '29.948', 'grad_norm': '37', 'counters/examples': 265504, 'counters/updates': 8297}
train stats after 265536 examples: {'rewards_train/chosen': '0.13976', 'rewards_train/rejected': '-0.00023818', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14', 'logps_train/rejected': '-106.06', 'logps_train/chosen': '-145.8', 'loss/train': '0.63847', 'examples_per_second': '31.791', 'grad_norm': '28.25', 'counters/examples': 265536, 'counters/updates': 8298}
skipping logging after 265568 examples to avoid logging too frequently
train stats after 265600 examples: {'rewards_train/chosen': '0.17311', 'rewards_train/rejected': '-0.042649', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21576', 'logps_train/rejected': '-121.47', 'logps_train/chosen': '-160.7', 'loss/train': '0.61733', 'examples_per_second': '33.658', 'grad_norm': '33.75', 'counters/examples': 265600, 'counters/updates': 8300}
train stats after 265632 examples: {'rewards_train/chosen': '0.1236', 'rewards_train/rejected': '0.01835', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10525', 'logps_train/rejected': '-96.126', 'logps_train/chosen': '-122.79', 'loss/train': '0.65678', 'examples_per_second': '32.412', 'grad_norm': '23.5', 'counters/examples': 265632, 'counters/updates': 8301}
train stats after 265664 examples: {'rewards_train/chosen': '0.17236', 'rewards_train/rejected': '0.12038', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.051976', 'logps_train/rejected': '-155.26', 'logps_train/chosen': '-146.31', 'loss/train': '0.68642', 'examples_per_second': '30.348', 'grad_norm': '41.25', 'counters/examples': 265664, 'counters/updates': 8302}
train stats after 265696 examples: {'rewards_train/chosen': '0.35483', 'rewards_train/rejected': '0.099412', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.25541', 'logps_train/rejected': '-153.95', 'logps_train/chosen': '-181.32', 'loss/train': '0.59195', 'examples_per_second': '29.928', 'grad_norm': '42.5', 'counters/examples': 265696, 'counters/updates': 8303}
train stats after 265728 examples: {'rewards_train/chosen': '0.1682', 'rewards_train/rejected': '0.034646', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13356', 'logps_train/rejected': '-136.34', 'logps_train/chosen': '-133.77', 'loss/train': '0.64376', 'examples_per_second': '32.393', 'grad_norm': '25.625', 'counters/examples': 265728, 'counters/updates': 8304}
train stats after 265760 examples: {'rewards_train/chosen': '0.18167', 'rewards_train/rejected': '0.17369', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0079849', 'logps_train/rejected': '-113.66', 'logps_train/chosen': '-133.27', 'loss/train': '0.71941', 'examples_per_second': '31.499', 'grad_norm': '36', 'counters/examples': 265760, 'counters/updates': 8305}
train stats after 265792 examples: {'rewards_train/chosen': '0.13027', 'rewards_train/rejected': '0.041097', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.089168', 'logps_train/rejected': '-128.28', 'logps_train/chosen': '-186.08', 'loss/train': '0.67086', 'examples_per_second': '31.598', 'grad_norm': '38.75', 'counters/examples': 265792, 'counters/updates': 8306}
train stats after 265824 examples: {'rewards_train/chosen': '0.22353', 'rewards_train/rejected': '-0.053708', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.27724', 'logps_train/rejected': '-104.83', 'logps_train/chosen': '-176.45', 'loss/train': '0.59223', 'examples_per_second': '32.666', 'grad_norm': '29.75', 'counters/examples': 265824, 'counters/updates': 8307}
train stats after 265856 examples: {'rewards_train/chosen': '0.14157', 'rewards_train/rejected': '0.13558', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.005998', 'logps_train/rejected': '-114.73', 'logps_train/chosen': '-139', 'loss/train': '0.7018', 'examples_per_second': '31.504', 'grad_norm': '37.25', 'counters/examples': 265856, 'counters/updates': 8308}
train stats after 265888 examples: {'rewards_train/chosen': '0.24261', 'rewards_train/rejected': '0.13554', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10706', 'logps_train/rejected': '-143', 'logps_train/chosen': '-133.57', 'loss/train': '0.66152', 'examples_per_second': '30.604', 'grad_norm': '31.375', 'counters/examples': 265888, 'counters/updates': 8309}
train stats after 265920 examples: {'rewards_train/chosen': '0.23338', 'rewards_train/rejected': '0.049925', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18345', 'logps_train/rejected': '-155.32', 'logps_train/chosen': '-165.01', 'loss/train': '0.63705', 'examples_per_second': '31.837', 'grad_norm': '45.75', 'counters/examples': 265920, 'counters/updates': 8310}
train stats after 265952 examples: {'rewards_train/chosen': '0.27429', 'rewards_train/rejected': '0.072215', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20208', 'logps_train/rejected': '-141.71', 'logps_train/chosen': '-171.91', 'loss/train': '0.61128', 'examples_per_second': '32.037', 'grad_norm': '38.75', 'counters/examples': 265952, 'counters/updates': 8311}
train stats after 265984 examples: {'rewards_train/chosen': '0.23623', 'rewards_train/rejected': '-0.020108', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25634', 'logps_train/rejected': '-115.79', 'logps_train/chosen': '-146.74', 'loss/train': '0.5896', 'examples_per_second': '31.489', 'grad_norm': '31.75', 'counters/examples': 265984, 'counters/updates': 8312}
train stats after 266016 examples: {'rewards_train/chosen': '0.14998', 'rewards_train/rejected': '0.054045', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095934', 'logps_train/rejected': '-114.89', 'logps_train/chosen': '-144.8', 'loss/train': '0.65888', 'examples_per_second': '30.171', 'grad_norm': '28.625', 'counters/examples': 266016, 'counters/updates': 8313}
train stats after 266048 examples: {'rewards_train/chosen': '0.18222', 'rewards_train/rejected': '-0.0047685', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18699', 'logps_train/rejected': '-107.67', 'logps_train/chosen': '-193.71', 'loss/train': '0.61972', 'examples_per_second': '30.486', 'grad_norm': '27.375', 'counters/examples': 266048, 'counters/updates': 8314}
train stats after 266080 examples: {'rewards_train/chosen': '0.1692', 'rewards_train/rejected': '0.078892', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.090308', 'logps_train/rejected': '-124.9', 'logps_train/chosen': '-165.19', 'loss/train': '0.66788', 'examples_per_second': '31.197', 'grad_norm': '37.25', 'counters/examples': 266080, 'counters/updates': 8315}
train stats after 266112 examples: {'rewards_train/chosen': '0.18132', 'rewards_train/rejected': '0.048474', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.13284', 'logps_train/rejected': '-130.47', 'logps_train/chosen': '-128.31', 'loss/train': '0.64497', 'examples_per_second': '31.856', 'grad_norm': '30.875', 'counters/examples': 266112, 'counters/updates': 8316}
train stats after 266144 examples: {'rewards_train/chosen': '0.15562', 'rewards_train/rejected': '0.1614', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.005772', 'logps_train/rejected': '-141.37', 'logps_train/chosen': '-161.57', 'loss/train': '0.71255', 'examples_per_second': '30.146', 'grad_norm': '34.25', 'counters/examples': 266144, 'counters/updates': 8317}
train stats after 266176 examples: {'rewards_train/chosen': '0.10539', 'rewards_train/rejected': '-0.048666', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15406', 'logps_train/rejected': '-102.34', 'logps_train/chosen': '-124.96', 'loss/train': '0.62958', 'examples_per_second': '30.652', 'grad_norm': '25.625', 'counters/examples': 266176, 'counters/updates': 8318}
train stats after 266208 examples: {'rewards_train/chosen': '0.13012', 'rewards_train/rejected': '0.12357', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0065459', 'logps_train/rejected': '-125.49', 'logps_train/chosen': '-130.29', 'loss/train': '0.69646', 'examples_per_second': '31.497', 'grad_norm': '30.375', 'counters/examples': 266208, 'counters/updates': 8319}
train stats after 266240 examples: {'rewards_train/chosen': '0.251', 'rewards_train/rejected': '0.094884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15612', 'logps_train/rejected': '-102.03', 'logps_train/chosen': '-142.91', 'loss/train': '0.63408', 'examples_per_second': '31.213', 'grad_norm': '25.875', 'counters/examples': 266240, 'counters/updates': 8320}
train stats after 266272 examples: {'rewards_train/chosen': '0.32688', 'rewards_train/rejected': '0.055509', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.27137', 'logps_train/rejected': '-154.5', 'logps_train/chosen': '-174.91', 'loss/train': '0.61862', 'examples_per_second': '31.246', 'grad_norm': '48.25', 'counters/examples': 266272, 'counters/updates': 8321}
skipping logging after 266304 examples to avoid logging too frequently
train stats after 266336 examples: {'rewards_train/chosen': '0.28065', 'rewards_train/rejected': '0.039942', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.24071', 'logps_train/rejected': '-122.61', 'logps_train/chosen': '-160.07', 'loss/train': '0.60446', 'examples_per_second': '36.019', 'grad_norm': '29.75', 'counters/examples': 266336, 'counters/updates': 8323}
train stats after 266368 examples: {'rewards_train/chosen': '0.088127', 'rewards_train/rejected': '-0.054992', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.14312', 'logps_train/rejected': '-121.17', 'logps_train/chosen': '-171.66', 'loss/train': '0.63665', 'examples_per_second': '31.476', 'grad_norm': '30.125', 'counters/examples': 266368, 'counters/updates': 8324}
skipping logging after 266400 examples to avoid logging too frequently
train stats after 266432 examples: {'rewards_train/chosen': '0.073945', 'rewards_train/rejected': '0.016555', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.05739', 'logps_train/rejected': '-110.73', 'logps_train/chosen': '-102.16', 'loss/train': '0.68186', 'examples_per_second': '33.638', 'grad_norm': '25.75', 'counters/examples': 266432, 'counters/updates': 8326}
skipping logging after 266464 examples to avoid logging too frequently
train stats after 266496 examples: {'rewards_train/chosen': '0.21293', 'rewards_train/rejected': '0.090189', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12274', 'logps_train/rejected': '-122.47', 'logps_train/chosen': '-161.24', 'loss/train': '0.65458', 'examples_per_second': '30.93', 'grad_norm': '50.5', 'counters/examples': 266496, 'counters/updates': 8328}
train stats after 266528 examples: {'rewards_train/chosen': '0.14241', 'rewards_train/rejected': '0.008134', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13428', 'logps_train/rejected': '-102.2', 'logps_train/chosen': '-141.74', 'loss/train': '0.63706', 'examples_per_second': '29.948', 'grad_norm': '25.25', 'counters/examples': 266528, 'counters/updates': 8329}
skipping logging after 266560 examples to avoid logging too frequently
train stats after 266592 examples: {'rewards_train/chosen': '0.077729', 'rewards_train/rejected': '0.0076201', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.070109', 'logps_train/rejected': '-164.05', 'logps_train/chosen': '-190.33', 'loss/train': '0.68476', 'examples_per_second': '31.428', 'grad_norm': '32.25', 'counters/examples': 266592, 'counters/updates': 8331}
train stats after 266624 examples: {'rewards_train/chosen': '0.10948', 'rewards_train/rejected': '0.099178', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.010305', 'logps_train/rejected': '-132.1', 'logps_train/chosen': '-149.05', 'loss/train': '0.71132', 'examples_per_second': '31.482', 'grad_norm': '32.25', 'counters/examples': 266624, 'counters/updates': 8332}
train stats after 266656 examples: {'rewards_train/chosen': '0.14262', 'rewards_train/rejected': '0.01956', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12306', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-137.95', 'loss/train': '0.65777', 'examples_per_second': '30.856', 'grad_norm': '38.25', 'counters/examples': 266656, 'counters/updates': 8333}
train stats after 266688 examples: {'rewards_train/chosen': '0.16821', 'rewards_train/rejected': '0.039325', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12889', 'logps_train/rejected': '-131.26', 'logps_train/chosen': '-128.65', 'loss/train': '0.64228', 'examples_per_second': '30.142', 'grad_norm': '37.25', 'counters/examples': 266688, 'counters/updates': 8334}
train stats after 266720 examples: {'rewards_train/chosen': '0.15002', 'rewards_train/rejected': '0.057148', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.092869', 'logps_train/rejected': '-153.96', 'logps_train/chosen': '-136.06', 'loss/train': '0.66216', 'examples_per_second': '30.494', 'grad_norm': '27.875', 'counters/examples': 266720, 'counters/updates': 8335}
train stats after 266752 examples: {'rewards_train/chosen': '0.18952', 'rewards_train/rejected': '0.052848', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13667', 'logps_train/rejected': '-132.95', 'logps_train/chosen': '-171.31', 'loss/train': '0.64907', 'examples_per_second': '31.619', 'grad_norm': '54.75', 'counters/examples': 266752, 'counters/updates': 8336}
train stats after 266784 examples: {'rewards_train/chosen': '0.082722', 'rewards_train/rejected': '0.060433', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.022289', 'logps_train/rejected': '-116.03', 'logps_train/chosen': '-159.74', 'loss/train': '0.693', 'examples_per_second': '32.473', 'grad_norm': '28.25', 'counters/examples': 266784, 'counters/updates': 8337}
train stats after 266816 examples: {'rewards_train/chosen': '0.17026', 'rewards_train/rejected': '0.036176', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13409', 'logps_train/rejected': '-103.01', 'logps_train/chosen': '-138.63', 'loss/train': '0.64269', 'examples_per_second': '29.94', 'grad_norm': '28.875', 'counters/examples': 266816, 'counters/updates': 8338}
train stats after 266848 examples: {'rewards_train/chosen': '0.16128', 'rewards_train/rejected': '0.050907', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11037', 'logps_train/rejected': '-117.17', 'logps_train/chosen': '-144.93', 'loss/train': '0.65874', 'examples_per_second': '31.455', 'grad_norm': '41.75', 'counters/examples': 266848, 'counters/updates': 8339}
train stats after 266880 examples: {'rewards_train/chosen': '0.22001', 'rewards_train/rejected': '0.05717', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16284', 'logps_train/rejected': '-142.91', 'logps_train/chosen': '-163.34', 'loss/train': '0.6367', 'examples_per_second': '30.086', 'grad_norm': '35.25', 'counters/examples': 266880, 'counters/updates': 8340}
train stats after 266912 examples: {'rewards_train/chosen': '0.26377', 'rewards_train/rejected': '0.032202', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23157', 'logps_train/rejected': '-141.26', 'logps_train/chosen': '-178.67', 'loss/train': '0.60316', 'examples_per_second': '30.014', 'grad_norm': '26.5', 'counters/examples': 266912, 'counters/updates': 8341}
train stats after 266944 examples: {'rewards_train/chosen': '0.16536', 'rewards_train/rejected': '0.055459', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.1099', 'logps_train/rejected': '-181.37', 'logps_train/chosen': '-150.74', 'loss/train': '0.65719', 'examples_per_second': '31.002', 'grad_norm': '35.25', 'counters/examples': 266944, 'counters/updates': 8342}
train stats after 266976 examples: {'rewards_train/chosen': '0.21837', 'rewards_train/rejected': '0.047541', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17083', 'logps_train/rejected': '-129.1', 'logps_train/chosen': '-121.56', 'loss/train': '0.63717', 'examples_per_second': '30.914', 'grad_norm': '26.5', 'counters/examples': 266976, 'counters/updates': 8343}
train stats after 267008 examples: {'rewards_train/chosen': '0.15741', 'rewards_train/rejected': '0.074672', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.082739', 'logps_train/rejected': '-102.53', 'logps_train/chosen': '-143.24', 'loss/train': '0.66716', 'examples_per_second': '30.184', 'grad_norm': '27.25', 'counters/examples': 267008, 'counters/updates': 8344}
train stats after 267040 examples: {'rewards_train/chosen': '0.16766', 'rewards_train/rejected': '-0.01011', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17777', 'logps_train/rejected': '-88.8', 'logps_train/chosen': '-112.19', 'loss/train': '0.62203', 'examples_per_second': '31.368', 'grad_norm': '20.75', 'counters/examples': 267040, 'counters/updates': 8345}
train stats after 267072 examples: {'rewards_train/chosen': '0.21516', 'rewards_train/rejected': '0.00066837', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.21449', 'logps_train/rejected': '-139.03', 'logps_train/chosen': '-160.04', 'loss/train': '0.61166', 'examples_per_second': '31.481', 'grad_norm': '27.25', 'counters/examples': 267072, 'counters/updates': 8346}
train stats after 267104 examples: {'rewards_train/chosen': '0.25728', 'rewards_train/rejected': '0.16256', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.094722', 'logps_train/rejected': '-141.77', 'logps_train/chosen': '-136.82', 'loss/train': '0.65897', 'examples_per_second': '30.372', 'grad_norm': '31.75', 'counters/examples': 267104, 'counters/updates': 8347}
train stats after 267136 examples: {'rewards_train/chosen': '0.25938', 'rewards_train/rejected': '0.033634', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.22575', 'logps_train/rejected': '-127.01', 'logps_train/chosen': '-188.04', 'loss/train': '0.6009', 'examples_per_second': '29.988', 'grad_norm': '48.5', 'counters/examples': 267136, 'counters/updates': 8348}
train stats after 267168 examples: {'rewards_train/chosen': '0.1237', 'rewards_train/rejected': '-0.019443', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14315', 'logps_train/rejected': '-96.471', 'logps_train/chosen': '-128.05', 'loss/train': '0.63606', 'examples_per_second': '31.998', 'grad_norm': '23', 'counters/examples': 267168, 'counters/updates': 8349}
train stats after 267200 examples: {'rewards_train/chosen': '0.284', 'rewards_train/rejected': '0.095049', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18895', 'logps_train/rejected': '-144.84', 'logps_train/chosen': '-159.21', 'loss/train': '0.61804', 'examples_per_second': '31.444', 'grad_norm': '40.75', 'counters/examples': 267200, 'counters/updates': 8350}
skipping logging after 267232 examples to avoid logging too frequently
train stats after 267264 examples: {'rewards_train/chosen': '0.16818', 'rewards_train/rejected': '0.1031', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.065075', 'logps_train/rejected': '-135.23', 'logps_train/chosen': '-141.23', 'loss/train': '0.68353', 'examples_per_second': '30.975', 'grad_norm': '43.25', 'counters/examples': 267264, 'counters/updates': 8352}
skipping logging after 267296 examples to avoid logging too frequently
train stats after 267328 examples: {'rewards_train/chosen': '0.081307', 'rewards_train/rejected': '0.0094634', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.071843', 'logps_train/rejected': '-114.78', 'logps_train/chosen': '-125.64', 'loss/train': '0.6753', 'examples_per_second': '31.5', 'grad_norm': '31.875', 'counters/examples': 267328, 'counters/updates': 8354}
train stats after 267360 examples: {'rewards_train/chosen': '0.22186', 'rewards_train/rejected': '0.0083456', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21352', 'logps_train/rejected': '-96.658', 'logps_train/chosen': '-153.88', 'loss/train': '0.60586', 'examples_per_second': '30.387', 'grad_norm': '23.125', 'counters/examples': 267360, 'counters/updates': 8355}
train stats after 267392 examples: {'rewards_train/chosen': '0.15187', 'rewards_train/rejected': '0.11928', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.03259', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-152.82', 'loss/train': '0.71281', 'examples_per_second': '31.417', 'grad_norm': '42.25', 'counters/examples': 267392, 'counters/updates': 8356}
train stats after 267424 examples: {'rewards_train/chosen': '0.079428', 'rewards_train/rejected': '-0.094415', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17384', 'logps_train/rejected': '-132.42', 'logps_train/chosen': '-129.21', 'loss/train': '0.62261', 'examples_per_second': '32.603', 'grad_norm': '26.625', 'counters/examples': 267424, 'counters/updates': 8357}
train stats after 267456 examples: {'rewards_train/chosen': '0.32699', 'rewards_train/rejected': '0.051548', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27544', 'logps_train/rejected': '-144.8', 'logps_train/chosen': '-195.95', 'loss/train': '0.59083', 'examples_per_second': '31.44', 'grad_norm': '33.25', 'counters/examples': 267456, 'counters/updates': 8358}
train stats after 267488 examples: {'rewards_train/chosen': '0.1716', 'rewards_train/rejected': '0.071105', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10049', 'logps_train/rejected': '-113.6', 'logps_train/chosen': '-140.01', 'loss/train': '0.6767', 'examples_per_second': '31.149', 'grad_norm': '27.75', 'counters/examples': 267488, 'counters/updates': 8359}
train stats after 267520 examples: {'rewards_train/chosen': '0.26091', 'rewards_train/rejected': '0.028521', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.23239', 'logps_train/rejected': '-127.89', 'logps_train/chosen': '-140.44', 'loss/train': '0.5972', 'examples_per_second': '31.712', 'grad_norm': '27.5', 'counters/examples': 267520, 'counters/updates': 8360}
train stats after 267552 examples: {'rewards_train/chosen': '0.29141', 'rewards_train/rejected': '0.059872', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.23154', 'logps_train/rejected': '-126.09', 'logps_train/chosen': '-152.4', 'loss/train': '0.60588', 'examples_per_second': '31.485', 'grad_norm': '26.5', 'counters/examples': 267552, 'counters/updates': 8361}
train stats after 267584 examples: {'rewards_train/chosen': '0.091413', 'rewards_train/rejected': '0.031777', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.059637', 'logps_train/rejected': '-117.49', 'logps_train/chosen': '-151.16', 'loss/train': '0.6832', 'examples_per_second': '33.127', 'grad_norm': '29.625', 'counters/examples': 267584, 'counters/updates': 8362}
skipping logging after 267616 examples to avoid logging too frequently
train stats after 267648 examples: {'rewards_train/chosen': '0.15096', 'rewards_train/rejected': '0.019841', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13112', 'logps_train/rejected': '-108.13', 'logps_train/chosen': '-126.7', 'loss/train': '0.64454', 'examples_per_second': '30.657', 'grad_norm': '43.25', 'counters/examples': 267648, 'counters/updates': 8364}
train stats after 267680 examples: {'rewards_train/chosen': '0.36784', 'rewards_train/rejected': '0.095089', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.27275', 'logps_train/rejected': '-143.35', 'logps_train/chosen': '-199.22', 'loss/train': '0.59166', 'examples_per_second': '29.816', 'grad_norm': '27.375', 'counters/examples': 267680, 'counters/updates': 8365}
train stats after 267712 examples: {'rewards_train/chosen': '0.045049', 'rewards_train/rejected': '0.11186', 'rewards_train/accuracies': '0.3125', 'rewards_train/margins': '-0.066807', 'logps_train/rejected': '-110.52', 'logps_train/chosen': '-99.074', 'loss/train': '0.73364', 'examples_per_second': '31.531', 'grad_norm': '48.25', 'counters/examples': 267712, 'counters/updates': 8366}
skipping logging after 267744 examples to avoid logging too frequently
train stats after 267776 examples: {'rewards_train/chosen': '0.20803', 'rewards_train/rejected': '-0.03187', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2399', 'logps_train/rejected': '-109.69', 'logps_train/chosen': '-149.49', 'loss/train': '0.5895', 'examples_per_second': '31.486', 'grad_norm': '36.5', 'counters/examples': 267776, 'counters/updates': 8368}
train stats after 267808 examples: {'rewards_train/chosen': '0.22388', 'rewards_train/rejected': '0.035056', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18883', 'logps_train/rejected': '-96.277', 'logps_train/chosen': '-139.14', 'loss/train': '0.61333', 'examples_per_second': '30.568', 'grad_norm': '30.125', 'counters/examples': 267808, 'counters/updates': 8369}
train stats after 267840 examples: {'rewards_train/chosen': '0.1761', 'rewards_train/rejected': '0.078927', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.097172', 'logps_train/rejected': '-94.966', 'logps_train/chosen': '-128.05', 'loss/train': '0.66576', 'examples_per_second': '30.876', 'grad_norm': '25.125', 'counters/examples': 267840, 'counters/updates': 8370}
skipping logging after 267872 examples to avoid logging too frequently
train stats after 267904 examples: {'rewards_train/chosen': '0.20532', 'rewards_train/rejected': '0.013785', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.19154', 'logps_train/rejected': '-101.6', 'logps_train/chosen': '-137.22', 'loss/train': '0.6108', 'examples_per_second': '32.406', 'grad_norm': '26.125', 'counters/examples': 267904, 'counters/updates': 8372}
train stats after 267936 examples: {'rewards_train/chosen': '0.15291', 'rewards_train/rejected': '0.038181', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11473', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-133.49', 'loss/train': '0.64967', 'examples_per_second': '30.523', 'grad_norm': '26.875', 'counters/examples': 267936, 'counters/updates': 8373}
train stats after 267968 examples: {'rewards_train/chosen': '0.2345', 'rewards_train/rejected': '0.073788', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16071', 'logps_train/rejected': '-135.42', 'logps_train/chosen': '-159.55', 'loss/train': '0.6283', 'examples_per_second': '30.263', 'grad_norm': '26', 'counters/examples': 267968, 'counters/updates': 8374}
train stats after 268000 examples: {'rewards_train/chosen': '0.19748', 'rewards_train/rejected': '0.13117', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.066306', 'logps_train/rejected': '-140.56', 'logps_train/chosen': '-162.45', 'loss/train': '0.68083', 'examples_per_second': '24.288', 'grad_norm': '46', 'counters/examples': 268000, 'counters/updates': 8375}
train stats after 268032 examples: {'rewards_train/chosen': '0.17246', 'rewards_train/rejected': '-0.0036256', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17609', 'logps_train/rejected': '-145.34', 'logps_train/chosen': '-137.9', 'loss/train': '0.62325', 'examples_per_second': '30.566', 'grad_norm': '25.375', 'counters/examples': 268032, 'counters/updates': 8376}
train stats after 268064 examples: {'rewards_train/chosen': '0.23837', 'rewards_train/rejected': '-0.084911', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.32328', 'logps_train/rejected': '-105.15', 'logps_train/chosen': '-125.08', 'loss/train': '0.56419', 'examples_per_second': '30.461', 'grad_norm': '23', 'counters/examples': 268064, 'counters/updates': 8377}
train stats after 268096 examples: {'rewards_train/chosen': '0.32326', 'rewards_train/rejected': '0.13162', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.19164', 'logps_train/rejected': '-131', 'logps_train/chosen': '-188.02', 'loss/train': '0.62599', 'examples_per_second': '24.293', 'grad_norm': '29.875', 'counters/examples': 268096, 'counters/updates': 8378}
train stats after 268128 examples: {'rewards_train/chosen': '0.16226', 'rewards_train/rejected': '0.12559', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.036677', 'logps_train/rejected': '-127.54', 'logps_train/chosen': '-132.98', 'loss/train': '0.69158', 'examples_per_second': '31.498', 'grad_norm': '28.625', 'counters/examples': 268128, 'counters/updates': 8379}
skipping logging after 268160 examples to avoid logging too frequently
train stats after 268192 examples: {'rewards_train/chosen': '0.17507', 'rewards_train/rejected': '0.039993', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13508', 'logps_train/rejected': '-108.73', 'logps_train/chosen': '-156.29', 'loss/train': '0.64532', 'examples_per_second': '32.398', 'grad_norm': '35.5', 'counters/examples': 268192, 'counters/updates': 8381}
train stats after 268224 examples: {'rewards_train/chosen': '0.10795', 'rewards_train/rejected': '0.10226', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0056958', 'logps_train/rejected': '-102.68', 'logps_train/chosen': '-99.73', 'loss/train': '0.70672', 'examples_per_second': '31.48', 'grad_norm': '30.25', 'counters/examples': 268224, 'counters/updates': 8382}
train stats after 268256 examples: {'rewards_train/chosen': '0.26751', 'rewards_train/rejected': '0.16751', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1', 'logps_train/rejected': '-123.62', 'logps_train/chosen': '-145.57', 'loss/train': '0.67639', 'examples_per_second': '30.505', 'grad_norm': '36.25', 'counters/examples': 268256, 'counters/updates': 8383}
train stats after 268288 examples: {'rewards_train/chosen': '0.19522', 'rewards_train/rejected': '0.10933', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.085894', 'logps_train/rejected': '-129.77', 'logps_train/chosen': '-167.16', 'loss/train': '0.66707', 'examples_per_second': '30.866', 'grad_norm': '32.75', 'counters/examples': 268288, 'counters/updates': 8384}
train stats after 268320 examples: {'rewards_train/chosen': '0.28317', 'rewards_train/rejected': '0.075081', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20809', 'logps_train/rejected': '-135.41', 'logps_train/chosen': '-185.72', 'loss/train': '0.6126', 'examples_per_second': '30.319', 'grad_norm': '26.75', 'counters/examples': 268320, 'counters/updates': 8385}
train stats after 268352 examples: {'rewards_train/chosen': '0.15286', 'rewards_train/rejected': '-0.012988', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16585', 'logps_train/rejected': '-189.81', 'logps_train/chosen': '-123.82', 'loss/train': '0.62763', 'examples_per_second': '31.359', 'grad_norm': '26.5', 'counters/examples': 268352, 'counters/updates': 8386}
train stats after 268384 examples: {'rewards_train/chosen': '0.079377', 'rewards_train/rejected': '0.042945', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.036433', 'logps_train/rejected': '-166.94', 'logps_train/chosen': '-159.76', 'loss/train': '0.68558', 'examples_per_second': '31.425', 'grad_norm': '34.5', 'counters/examples': 268384, 'counters/updates': 8387}
train stats after 268416 examples: {'rewards_train/chosen': '0.29644', 'rewards_train/rejected': '0.10231', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.19413', 'logps_train/rejected': '-162.41', 'logps_train/chosen': '-192.18', 'loss/train': '0.63396', 'examples_per_second': '31.459', 'grad_norm': '27.875', 'counters/examples': 268416, 'counters/updates': 8388}
skipping logging after 268448 examples to avoid logging too frequently
train stats after 268480 examples: {'rewards_train/chosen': '0.088714', 'rewards_train/rejected': '0.029506', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.059208', 'logps_train/rejected': '-120.95', 'logps_train/chosen': '-133.99', 'loss/train': '0.66983', 'examples_per_second': '31.367', 'grad_norm': '29', 'counters/examples': 268480, 'counters/updates': 8390}
skipping logging after 268512 examples to avoid logging too frequently
train stats after 268544 examples: {'rewards_train/chosen': '0.10297', 'rewards_train/rejected': '0.093638', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0093307', 'logps_train/rejected': '-116.28', 'logps_train/chosen': '-125.15', 'loss/train': '0.70678', 'examples_per_second': '33.744', 'grad_norm': '31.5', 'counters/examples': 268544, 'counters/updates': 8392}
train stats after 268576 examples: {'rewards_train/chosen': '0.1614', 'rewards_train/rejected': '0.056719', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10468', 'logps_train/rejected': '-133.48', 'logps_train/chosen': '-131.55', 'loss/train': '0.65535', 'examples_per_second': '31.272', 'grad_norm': '30.5', 'counters/examples': 268576, 'counters/updates': 8393}
skipping logging after 268608 examples to avoid logging too frequently
train stats after 268640 examples: {'rewards_train/chosen': '0.11655', 'rewards_train/rejected': '0.030335', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08622', 'logps_train/rejected': '-138.76', 'logps_train/chosen': '-176.37', 'loss/train': '0.67203', 'examples_per_second': '29.844', 'grad_norm': '29.625', 'counters/examples': 268640, 'counters/updates': 8395}
train stats after 268672 examples: {'rewards_train/chosen': '0.31429', 'rewards_train/rejected': '0.012633', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.30166', 'logps_train/rejected': '-137.19', 'logps_train/chosen': '-167.55', 'loss/train': '0.57341', 'examples_per_second': '31.331', 'grad_norm': '27.625', 'counters/examples': 268672, 'counters/updates': 8396}
train stats after 268704 examples: {'rewards_train/chosen': '0.17191', 'rewards_train/rejected': '-0.022348', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.19425', 'logps_train/rejected': '-126.13', 'logps_train/chosen': '-125.04', 'loss/train': '0.60808', 'examples_per_second': '31.117', 'grad_norm': '37.25', 'counters/examples': 268704, 'counters/updates': 8397}
train stats after 268736 examples: {'rewards_train/chosen': '0.29794', 'rewards_train/rejected': '0.052604', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.24534', 'logps_train/rejected': '-134.63', 'logps_train/chosen': '-141.35', 'loss/train': '0.60508', 'examples_per_second': '31.488', 'grad_norm': '25.5', 'counters/examples': 268736, 'counters/updates': 8398}
train stats after 268768 examples: {'rewards_train/chosen': '0.088482', 'rewards_train/rejected': '-0.11371', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20219', 'logps_train/rejected': '-147.98', 'logps_train/chosen': '-149.43', 'loss/train': '0.61314', 'examples_per_second': '31.088', 'grad_norm': '31.5', 'counters/examples': 268768, 'counters/updates': 8399}
skipping logging after 268800 examples to avoid logging too frequently
train stats after 268832 examples: {'rewards_train/chosen': '0.19056', 'rewards_train/rejected': '-0.0088266', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.19939', 'logps_train/rejected': '-139.76', 'logps_train/chosen': '-140.52', 'loss/train': '0.60953', 'examples_per_second': '31.431', 'grad_norm': '40.75', 'counters/examples': 268832, 'counters/updates': 8401}
train stats after 268864 examples: {'rewards_train/chosen': '0.262', 'rewards_train/rejected': '0.081778', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18023', 'logps_train/rejected': '-151.92', 'logps_train/chosen': '-175.08', 'loss/train': '0.6293', 'examples_per_second': '30.851', 'grad_norm': '26.125', 'counters/examples': 268864, 'counters/updates': 8402}
skipping logging after 268896 examples to avoid logging too frequently
train stats after 268928 examples: {'rewards_train/chosen': '0.17156', 'rewards_train/rejected': '0.15518', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.016388', 'logps_train/rejected': '-144.15', 'logps_train/chosen': '-110.5', 'loss/train': '0.70328', 'examples_per_second': '31.395', 'grad_norm': '38.25', 'counters/examples': 268928, 'counters/updates': 8404}
skipping logging after 268960 examples to avoid logging too frequently
train stats after 268992 examples: {'rewards_train/chosen': '0.25315', 'rewards_train/rejected': '0.093585', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15957', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-110.54', 'loss/train': '0.6422', 'examples_per_second': '33.616', 'grad_norm': '24.5', 'counters/examples': 268992, 'counters/updates': 8406}
train stats after 269024 examples: {'rewards_train/chosen': '0.11028', 'rewards_train/rejected': '0.063557', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.046723', 'logps_train/rejected': '-155.7', 'logps_train/chosen': '-129.74', 'loss/train': '0.68198', 'examples_per_second': '30.33', 'grad_norm': '31.75', 'counters/examples': 269024, 'counters/updates': 8407}
skipping logging after 269056 examples to avoid logging too frequently
train stats after 269088 examples: {'rewards_train/chosen': '0.12013', 'rewards_train/rejected': '-0.014935', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13507', 'logps_train/rejected': '-121.01', 'logps_train/chosen': '-176.16', 'loss/train': '0.64985', 'examples_per_second': '23.6', 'grad_norm': '27.75', 'counters/examples': 269088, 'counters/updates': 8409}
train stats after 269120 examples: {'rewards_train/chosen': '0.19628', 'rewards_train/rejected': '0.08342', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11286', 'logps_train/rejected': '-126.33', 'logps_train/chosen': '-125.15', 'loss/train': '0.6486', 'examples_per_second': '31.406', 'grad_norm': '38.25', 'counters/examples': 269120, 'counters/updates': 8410}
train stats after 269152 examples: {'rewards_train/chosen': '0.12745', 'rewards_train/rejected': '0.15748', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.030035', 'logps_train/rejected': '-144.23', 'logps_train/chosen': '-110.04', 'loss/train': '0.74516', 'examples_per_second': '32.531', 'grad_norm': '32', 'counters/examples': 269152, 'counters/updates': 8411}
skipping logging after 269184 examples to avoid logging too frequently
train stats after 269216 examples: {'rewards_train/chosen': '0.25194', 'rewards_train/rejected': '-0.032439', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.28438', 'logps_train/rejected': '-125.21', 'logps_train/chosen': '-164.12', 'loss/train': '0.58361', 'examples_per_second': '31.87', 'grad_norm': '27.125', 'counters/examples': 269216, 'counters/updates': 8413}
train stats after 269248 examples: {'rewards_train/chosen': '0.14131', 'rewards_train/rejected': '0.013772', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12754', 'logps_train/rejected': '-122.02', 'logps_train/chosen': '-154.17', 'loss/train': '0.65068', 'examples_per_second': '31.357', 'grad_norm': '30.875', 'counters/examples': 269248, 'counters/updates': 8414}
train stats after 269280 examples: {'rewards_train/chosen': '0.17766', 'rewards_train/rejected': '0.075688', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10197', 'logps_train/rejected': '-164.75', 'logps_train/chosen': '-166.36', 'loss/train': '0.65213', 'examples_per_second': '31.287', 'grad_norm': '32.75', 'counters/examples': 269280, 'counters/updates': 8415}
train stats after 269312 examples: {'rewards_train/chosen': '0.20501', 'rewards_train/rejected': '0.00061295', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2044', 'logps_train/rejected': '-96.588', 'logps_train/chosen': '-160.71', 'loss/train': '0.61726', 'examples_per_second': '32.137', 'grad_norm': '28.5', 'counters/examples': 269312, 'counters/updates': 8416}
train stats after 269344 examples: {'rewards_train/chosen': '0.24412', 'rewards_train/rejected': '0.13301', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11111', 'logps_train/rejected': '-129.98', 'logps_train/chosen': '-143.75', 'loss/train': '0.66599', 'examples_per_second': '31.448', 'grad_norm': '38.25', 'counters/examples': 269344, 'counters/updates': 8417}
train stats after 269376 examples: {'rewards_train/chosen': '0.22905', 'rewards_train/rejected': '0.057371', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17167', 'logps_train/rejected': '-108.36', 'logps_train/chosen': '-113.11', 'loss/train': '0.62735', 'examples_per_second': '32.295', 'grad_norm': '29.375', 'counters/examples': 269376, 'counters/updates': 8418}
skipping logging after 269408 examples to avoid logging too frequently
train stats after 269440 examples: {'rewards_train/chosen': '0.17087', 'rewards_train/rejected': '0.077564', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093304', 'logps_train/rejected': '-141.96', 'logps_train/chosen': '-132.2', 'loss/train': '0.67331', 'examples_per_second': '30.001', 'grad_norm': '28.125', 'counters/examples': 269440, 'counters/updates': 8420}
train stats after 269472 examples: {'rewards_train/chosen': '0.17629', 'rewards_train/rejected': '0.069417', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10687', 'logps_train/rejected': '-163.06', 'logps_train/chosen': '-129.39', 'loss/train': '0.65123', 'examples_per_second': '30.513', 'grad_norm': '31.75', 'counters/examples': 269472, 'counters/updates': 8421}
train stats after 269504 examples: {'rewards_train/chosen': '0.12427', 'rewards_train/rejected': '0.15796', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.033686', 'logps_train/rejected': '-156.25', 'logps_train/chosen': '-147.06', 'loss/train': '0.72279', 'examples_per_second': '29.73', 'grad_norm': '30.625', 'counters/examples': 269504, 'counters/updates': 8422}
train stats after 269536 examples: {'rewards_train/chosen': '0.22264', 'rewards_train/rejected': '0.067612', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15503', 'logps_train/rejected': '-141.99', 'logps_train/chosen': '-194.97', 'loss/train': '0.64107', 'examples_per_second': '32.006', 'grad_norm': '27.125', 'counters/examples': 269536, 'counters/updates': 8423}
train stats after 269568 examples: {'rewards_train/chosen': '0.25554', 'rewards_train/rejected': '0.055475', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.20007', 'logps_train/rejected': '-109.9', 'logps_train/chosen': '-160.25', 'loss/train': '0.62856', 'examples_per_second': '29.795', 'grad_norm': '26.375', 'counters/examples': 269568, 'counters/updates': 8424}
train stats after 269600 examples: {'rewards_train/chosen': '0.077956', 'rewards_train/rejected': '0.079796', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.00184', 'logps_train/rejected': '-120.75', 'logps_train/chosen': '-140.6', 'loss/train': '0.70114', 'examples_per_second': '31.114', 'grad_norm': '31.875', 'counters/examples': 269600, 'counters/updates': 8425}
skipping logging after 269632 examples to avoid logging too frequently
train stats after 269664 examples: {'rewards_train/chosen': '0.098297', 'rewards_train/rejected': '0.001486', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096811', 'logps_train/rejected': '-135.25', 'logps_train/chosen': '-148.48', 'loss/train': '0.65713', 'examples_per_second': '31.446', 'grad_norm': '31.5', 'counters/examples': 269664, 'counters/updates': 8427}
train stats after 269696 examples: {'rewards_train/chosen': '0.17351', 'rewards_train/rejected': '0.11225', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.061259', 'logps_train/rejected': '-141.05', 'logps_train/chosen': '-143.32', 'loss/train': '0.68465', 'examples_per_second': '31.381', 'grad_norm': '38.25', 'counters/examples': 269696, 'counters/updates': 8428}
train stats after 269728 examples: {'rewards_train/chosen': '0.13', 'rewards_train/rejected': '0.031277', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.09872', 'logps_train/rejected': '-97.604', 'logps_train/chosen': '-115.25', 'loss/train': '0.66166', 'examples_per_second': '31.558', 'grad_norm': '30.875', 'counters/examples': 269728, 'counters/updates': 8429}
train stats after 269760 examples: {'rewards_train/chosen': '0.24213', 'rewards_train/rejected': '0.012785', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.22935', 'logps_train/rejected': '-121.03', 'logps_train/chosen': '-147.18', 'loss/train': '0.61472', 'examples_per_second': '30.121', 'grad_norm': '27.75', 'counters/examples': 269760, 'counters/updates': 8430}
train stats after 269792 examples: {'rewards_train/chosen': '0.15352', 'rewards_train/rejected': '-0.012972', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1665', 'logps_train/rejected': '-127.64', 'logps_train/chosen': '-131.12', 'loss/train': '0.63496', 'examples_per_second': '32.545', 'grad_norm': '28.625', 'counters/examples': 269792, 'counters/updates': 8431}
skipping logging after 269824 examples to avoid logging too frequently
train stats after 269856 examples: {'rewards_train/chosen': '0.088099', 'rewards_train/rejected': '-0.017389', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10549', 'logps_train/rejected': '-130.98', 'logps_train/chosen': '-147.94', 'loss/train': '0.65704', 'examples_per_second': '31.374', 'grad_norm': '60.5', 'counters/examples': 269856, 'counters/updates': 8433}
train stats after 269888 examples: {'rewards_train/chosen': '0.15262', 'rewards_train/rejected': '0.047316', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1053', 'logps_train/rejected': '-98.857', 'logps_train/chosen': '-144.53', 'loss/train': '0.65764', 'examples_per_second': '30.691', 'grad_norm': '26.625', 'counters/examples': 269888, 'counters/updates': 8434}
train stats after 269920 examples: {'rewards_train/chosen': '0.18918', 'rewards_train/rejected': '0.055997', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13318', 'logps_train/rejected': '-134.88', 'logps_train/chosen': '-113.59', 'loss/train': '0.63735', 'examples_per_second': '30.784', 'grad_norm': '27.5', 'counters/examples': 269920, 'counters/updates': 8435}
train stats after 269952 examples: {'rewards_train/chosen': '0.35628', 'rewards_train/rejected': '0.10628', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25', 'logps_train/rejected': '-153.99', 'logps_train/chosen': '-149.46', 'loss/train': '0.59364', 'examples_per_second': '30.287', 'grad_norm': '27.375', 'counters/examples': 269952, 'counters/updates': 8436}
train stats after 269984 examples: {'rewards_train/chosen': '0.20604', 'rewards_train/rejected': '0.00030838', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20573', 'logps_train/rejected': '-108.39', 'logps_train/chosen': '-173.54', 'loss/train': '0.61325', 'examples_per_second': '32.715', 'grad_norm': '26.375', 'counters/examples': 269984, 'counters/updates': 8437}
train stats after 270016 examples: {'rewards_train/chosen': '0.1846', 'rewards_train/rejected': '0.068397', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1162', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-171.82', 'loss/train': '0.65577', 'examples_per_second': '32.634', 'grad_norm': '37.5', 'counters/examples': 270016, 'counters/updates': 8438}
skipping logging after 270048 examples to avoid logging too frequently
train stats after 270080 examples: {'rewards_train/chosen': '0.28953', 'rewards_train/rejected': '-0.013415', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.30295', 'logps_train/rejected': '-100.86', 'logps_train/chosen': '-164.8', 'loss/train': '0.58868', 'examples_per_second': '30.978', 'grad_norm': '27.25', 'counters/examples': 270080, 'counters/updates': 8440}
train stats after 270112 examples: {'rewards_train/chosen': '0.24941', 'rewards_train/rejected': '0.071633', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17778', 'logps_train/rejected': '-123.89', 'logps_train/chosen': '-158.97', 'loss/train': '0.62128', 'examples_per_second': '30.805', 'grad_norm': '29.625', 'counters/examples': 270112, 'counters/updates': 8441}
train stats after 270144 examples: {'rewards_train/chosen': '0.19175', 'rewards_train/rejected': '0.096132', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.095615', 'logps_train/rejected': '-126.73', 'logps_train/chosen': '-117.33', 'loss/train': '0.68168', 'examples_per_second': '30.549', 'grad_norm': '31.375', 'counters/examples': 270144, 'counters/updates': 8442}
train stats after 270176 examples: {'rewards_train/chosen': '0.18084', 'rewards_train/rejected': '0.003798', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17704', 'logps_train/rejected': '-122.28', 'logps_train/chosen': '-174.41', 'loss/train': '0.62477', 'examples_per_second': '31.237', 'grad_norm': '54.75', 'counters/examples': 270176, 'counters/updates': 8443}
train stats after 270208 examples: {'rewards_train/chosen': '0.21534', 'rewards_train/rejected': '0.037035', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1783', 'logps_train/rejected': '-123.6', 'logps_train/chosen': '-144.26', 'loss/train': '0.63407', 'examples_per_second': '31.942', 'grad_norm': '32.5', 'counters/examples': 270208, 'counters/updates': 8444}
train stats after 270240 examples: {'rewards_train/chosen': '0.28118', 'rewards_train/rejected': '0.079239', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20194', 'logps_train/rejected': '-127.62', 'logps_train/chosen': '-174.32', 'loss/train': '0.62193', 'examples_per_second': '31.603', 'grad_norm': '49.25', 'counters/examples': 270240, 'counters/updates': 8445}
train stats after 270272 examples: {'rewards_train/chosen': '0.17084', 'rewards_train/rejected': '0.048505', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12233', 'logps_train/rejected': '-118.58', 'logps_train/chosen': '-151.28', 'loss/train': '0.6463', 'examples_per_second': '32.3', 'grad_norm': '25.75', 'counters/examples': 270272, 'counters/updates': 8446}
train stats after 270304 examples: {'rewards_train/chosen': '0.19549', 'rewards_train/rejected': '0.077742', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11775', 'logps_train/rejected': '-136.49', 'logps_train/chosen': '-173.48', 'loss/train': '0.64482', 'examples_per_second': '31.559', 'grad_norm': '27.75', 'counters/examples': 270304, 'counters/updates': 8447}
train stats after 270336 examples: {'rewards_train/chosen': '0.16149', 'rewards_train/rejected': '0.06669', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094804', 'logps_train/rejected': '-122.1', 'logps_train/chosen': '-138.62', 'loss/train': '0.66418', 'examples_per_second': '32.378', 'grad_norm': '28.875', 'counters/examples': 270336, 'counters/updates': 8448}
train stats after 270368 examples: {'rewards_train/chosen': '0.18096', 'rewards_train/rejected': '0.012657', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1683', 'logps_train/rejected': '-96.536', 'logps_train/chosen': '-136.05', 'loss/train': '0.63803', 'examples_per_second': '32.02', 'grad_norm': '30.125', 'counters/examples': 270368, 'counters/updates': 8449}
skipping logging after 270400 examples to avoid logging too frequently
train stats after 270432 examples: {'rewards_train/chosen': '0.21539', 'rewards_train/rejected': '0.068971', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14642', 'logps_train/rejected': '-113.03', 'logps_train/chosen': '-138.18', 'loss/train': '0.63975', 'examples_per_second': '32.213', 'grad_norm': '35', 'counters/examples': 270432, 'counters/updates': 8451}
train stats after 270464 examples: {'rewards_train/chosen': '0.24274', 'rewards_train/rejected': '0.055758', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18698', 'logps_train/rejected': '-151.58', 'logps_train/chosen': '-161.37', 'loss/train': '0.61969', 'examples_per_second': '30.335', 'grad_norm': '34.25', 'counters/examples': 270464, 'counters/updates': 8452}
skipping logging after 270496 examples to avoid logging too frequently
train stats after 270528 examples: {'rewards_train/chosen': '0.13486', 'rewards_train/rejected': '0.048547', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.08631', 'logps_train/rejected': '-159.12', 'logps_train/chosen': '-149.44', 'loss/train': '0.65981', 'examples_per_second': '29.925', 'grad_norm': '30.75', 'counters/examples': 270528, 'counters/updates': 8454}
skipping logging after 270560 examples to avoid logging too frequently
train stats after 270592 examples: {'rewards_train/chosen': '0.30766', 'rewards_train/rejected': '0.11017', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1975', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-166.19', 'loss/train': '0.64372', 'examples_per_second': '31.32', 'grad_norm': '34.75', 'counters/examples': 270592, 'counters/updates': 8456}
train stats after 270624 examples: {'rewards_train/chosen': '0.17474', 'rewards_train/rejected': '0.11075', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.063991', 'logps_train/rejected': '-158.16', 'logps_train/chosen': '-129.48', 'loss/train': '0.69078', 'examples_per_second': '31.895', 'grad_norm': '43.5', 'counters/examples': 270624, 'counters/updates': 8457}
train stats after 270656 examples: {'rewards_train/chosen': '0.2562', 'rewards_train/rejected': '0.055832', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20036', 'logps_train/rejected': '-137.21', 'logps_train/chosen': '-164.18', 'loss/train': '0.62898', 'examples_per_second': '30.473', 'grad_norm': '33.25', 'counters/examples': 270656, 'counters/updates': 8458}
train stats after 270688 examples: {'rewards_train/chosen': '0.22619', 'rewards_train/rejected': '0.091838', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13435', 'logps_train/rejected': '-150.5', 'logps_train/chosen': '-148.26', 'loss/train': '0.66211', 'examples_per_second': '31.331', 'grad_norm': '61.75', 'counters/examples': 270688, 'counters/updates': 8459}
train stats after 270720 examples: {'rewards_train/chosen': '0.21109', 'rewards_train/rejected': '-0.055795', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.26688', 'logps_train/rejected': '-101.41', 'logps_train/chosen': '-121.86', 'loss/train': '0.59433', 'examples_per_second': '31.186', 'grad_norm': '29.375', 'counters/examples': 270720, 'counters/updates': 8460}
train stats after 270752 examples: {'rewards_train/chosen': '0.085123', 'rewards_train/rejected': '0.072311', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.012812', 'logps_train/rejected': '-119.89', 'logps_train/chosen': '-117.31', 'loss/train': '0.69971', 'examples_per_second': '30.237', 'grad_norm': '25.75', 'counters/examples': 270752, 'counters/updates': 8461}
train stats after 270784 examples: {'rewards_train/chosen': '0.09158', 'rewards_train/rejected': '0.08303', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.0085498', 'logps_train/rejected': '-127.15', 'logps_train/chosen': '-119.5', 'loss/train': '0.702', 'examples_per_second': '30.829', 'grad_norm': '29.5', 'counters/examples': 270784, 'counters/updates': 8462}
skipping logging after 270816 examples to avoid logging too frequently
train stats after 270848 examples: {'rewards_train/chosen': '0.16534', 'rewards_train/rejected': '0.049718', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11563', 'logps_train/rejected': '-116.01', 'logps_train/chosen': '-115.17', 'loss/train': '0.64993', 'examples_per_second': '34.701', 'grad_norm': '24.875', 'counters/examples': 270848, 'counters/updates': 8464}
train stats after 270880 examples: {'rewards_train/chosen': '0.12635', 'rewards_train/rejected': '0.07752', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.048827', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-134.54', 'loss/train': '0.68487', 'examples_per_second': '32.18', 'grad_norm': '33', 'counters/examples': 270880, 'counters/updates': 8465}
skipping logging after 270912 examples to avoid logging too frequently
train stats after 270944 examples: {'rewards_train/chosen': '0.23379', 'rewards_train/rejected': '0.03557', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19822', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-122.29', 'loss/train': '0.60977', 'examples_per_second': '31.316', 'grad_norm': '24.5', 'counters/examples': 270944, 'counters/updates': 8467}
skipping logging after 270976 examples to avoid logging too frequently
train stats after 271008 examples: {'rewards_train/chosen': '0.20204', 'rewards_train/rejected': '0.095008', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10704', 'logps_train/rejected': '-126.26', 'logps_train/chosen': '-112.24', 'loss/train': '0.65511', 'examples_per_second': '30.486', 'grad_norm': '33.5', 'counters/examples': 271008, 'counters/updates': 8469}
skipping logging after 271040 examples to avoid logging too frequently
train stats after 271072 examples: {'rewards_train/chosen': '0.20226', 'rewards_train/rejected': '0.025333', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17693', 'logps_train/rejected': '-129.09', 'logps_train/chosen': '-148.39', 'loss/train': '0.61935', 'examples_per_second': '36.024', 'grad_norm': '28', 'counters/examples': 271072, 'counters/updates': 8471}
train stats after 271104 examples: {'rewards_train/chosen': '0.1928', 'rewards_train/rejected': '0.081465', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11134', 'logps_train/rejected': '-121.42', 'logps_train/chosen': '-143.06', 'loss/train': '0.65559', 'examples_per_second': '31.127', 'grad_norm': '25.5', 'counters/examples': 271104, 'counters/updates': 8472}
train stats after 271136 examples: {'rewards_train/chosen': '0.11378', 'rewards_train/rejected': '-0.018919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1327', 'logps_train/rejected': '-103.3', 'logps_train/chosen': '-104.74', 'loss/train': '0.63818', 'examples_per_second': '29.978', 'grad_norm': '26.375', 'counters/examples': 271136, 'counters/updates': 8473}
train stats after 271168 examples: {'rewards_train/chosen': '0.21313', 'rewards_train/rejected': '0.040338', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17279', 'logps_train/rejected': '-121.73', 'logps_train/chosen': '-121.45', 'loss/train': '0.62384', 'examples_per_second': '31.404', 'grad_norm': '29.375', 'counters/examples': 271168, 'counters/updates': 8474}
train stats after 271200 examples: {'rewards_train/chosen': '0.19143', 'rewards_train/rejected': '0.025128', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1663', 'logps_train/rejected': '-118.36', 'logps_train/chosen': '-131.52', 'loss/train': '0.62294', 'examples_per_second': '30.448', 'grad_norm': '24.25', 'counters/examples': 271200, 'counters/updates': 8475}
train stats after 271232 examples: {'rewards_train/chosen': '0.1628', 'rewards_train/rejected': '0.031878', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13093', 'logps_train/rejected': '-150.65', 'logps_train/chosen': '-141.48', 'loss/train': '0.64902', 'examples_per_second': '31.456', 'grad_norm': '29.125', 'counters/examples': 271232, 'counters/updates': 8476}
train stats after 271264 examples: {'rewards_train/chosen': '0.24314', 'rewards_train/rejected': '-0.0063585', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.2495', 'logps_train/rejected': '-126.7', 'logps_train/chosen': '-160.84', 'loss/train': '0.59679', 'examples_per_second': '30.629', 'grad_norm': '46', 'counters/examples': 271264, 'counters/updates': 8477}
train stats after 271296 examples: {'rewards_train/chosen': '0.098658', 'rewards_train/rejected': '0.066902', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.031756', 'logps_train/rejected': '-136.04', 'logps_train/chosen': '-146.26', 'loss/train': '0.69136', 'examples_per_second': '31.61', 'grad_norm': '32', 'counters/examples': 271296, 'counters/updates': 8478}
train stats after 271328 examples: {'rewards_train/chosen': '0.21655', 'rewards_train/rejected': '0.091688', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12487', 'logps_train/rejected': '-137.27', 'logps_train/chosen': '-151.25', 'loss/train': '0.65239', 'examples_per_second': '31.341', 'grad_norm': '35.25', 'counters/examples': 271328, 'counters/updates': 8479}
train stats after 271360 examples: {'rewards_train/chosen': '0.26396', 'rewards_train/rejected': '0.088273', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17569', 'logps_train/rejected': '-132.99', 'logps_train/chosen': '-155.61', 'loss/train': '0.62156', 'examples_per_second': '31.6', 'grad_norm': '32', 'counters/examples': 271360, 'counters/updates': 8480}
train stats after 271392 examples: {'rewards_train/chosen': '0.29817', 'rewards_train/rejected': '0.12704', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.17113', 'logps_train/rejected': '-107.43', 'logps_train/chosen': '-148.71', 'loss/train': '0.64155', 'examples_per_second': '30.363', 'grad_norm': '30.5', 'counters/examples': 271392, 'counters/updates': 8481}
train stats after 271424 examples: {'rewards_train/chosen': '0.13832', 'rewards_train/rejected': '0.015285', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12303', 'logps_train/rejected': '-125.04', 'logps_train/chosen': '-110.59', 'loss/train': '0.65771', 'examples_per_second': '30.813', 'grad_norm': '24.375', 'counters/examples': 271424, 'counters/updates': 8482}
train stats after 271456 examples: {'rewards_train/chosen': '0.26477', 'rewards_train/rejected': '0.1048', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15997', 'logps_train/rejected': '-130.62', 'logps_train/chosen': '-185.85', 'loss/train': '0.64087', 'examples_per_second': '31.492', 'grad_norm': '47.25', 'counters/examples': 271456, 'counters/updates': 8483}
skipping logging after 271488 examples to avoid logging too frequently
train stats after 271520 examples: {'rewards_train/chosen': '0.10093', 'rewards_train/rejected': '0.088438', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '0.012493', 'logps_train/rejected': '-162.32', 'logps_train/chosen': '-118.89', 'loss/train': '0.70245', 'examples_per_second': '30.472', 'grad_norm': '40', 'counters/examples': 271520, 'counters/updates': 8485}
skipping logging after 271552 examples to avoid logging too frequently
train stats after 271584 examples: {'rewards_train/chosen': '0.14423', 'rewards_train/rejected': '-0.029337', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17356', 'logps_train/rejected': '-131.81', 'logps_train/chosen': '-171.55', 'loss/train': '0.62183', 'examples_per_second': '30.392', 'grad_norm': '29.75', 'counters/examples': 271584, 'counters/updates': 8487}
skipping logging after 271616 examples to avoid logging too frequently
train stats after 271648 examples: {'rewards_train/chosen': '0.24221', 'rewards_train/rejected': '0.085776', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15644', 'logps_train/rejected': '-135.21', 'logps_train/chosen': '-152.83', 'loss/train': '0.63529', 'examples_per_second': '31.521', 'grad_norm': '39.25', 'counters/examples': 271648, 'counters/updates': 8489}
train stats after 271680 examples: {'rewards_train/chosen': '0.13', 'rewards_train/rejected': '0.077357', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052647', 'logps_train/rejected': '-123.5', 'logps_train/chosen': '-123.39', 'loss/train': '0.67773', 'examples_per_second': '30.613', 'grad_norm': '25.875', 'counters/examples': 271680, 'counters/updates': 8490}
train stats after 271712 examples: {'rewards_train/chosen': '0.11649', 'rewards_train/rejected': '-0.034624', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15111', 'logps_train/rejected': '-95.26', 'logps_train/chosen': '-123.61', 'loss/train': '0.63522', 'examples_per_second': '32.583', 'grad_norm': '25.125', 'counters/examples': 271712, 'counters/updates': 8491}
train stats after 271744 examples: {'rewards_train/chosen': '0.19048', 'rewards_train/rejected': '0.066613', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12387', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-150.52', 'loss/train': '0.6474', 'examples_per_second': '31.019', 'grad_norm': '27.875', 'counters/examples': 271744, 'counters/updates': 8492}
train stats after 271776 examples: {'rewards_train/chosen': '0.22958', 'rewards_train/rejected': '0.046778', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1828', 'logps_train/rejected': '-181.19', 'logps_train/chosen': '-197.76', 'loss/train': '0.6358', 'examples_per_second': '30.486', 'grad_norm': '35', 'counters/examples': 271776, 'counters/updates': 8493}
train stats after 271808 examples: {'rewards_train/chosen': '0.23986', 'rewards_train/rejected': '0.15391', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.085955', 'logps_train/rejected': '-119.36', 'logps_train/chosen': '-137.18', 'loss/train': '0.66554', 'examples_per_second': '31.322', 'grad_norm': '31.625', 'counters/examples': 271808, 'counters/updates': 8494}
train stats after 271840 examples: {'rewards_train/chosen': '0.21312', 'rewards_train/rejected': '0.092426', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12069', 'logps_train/rejected': '-123.52', 'logps_train/chosen': '-138.2', 'loss/train': '0.64549', 'examples_per_second': '31.441', 'grad_norm': '36.75', 'counters/examples': 271840, 'counters/updates': 8495}
train stats after 271872 examples: {'rewards_train/chosen': '0.29641', 'rewards_train/rejected': '0.10572', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19069', 'logps_train/rejected': '-120.73', 'logps_train/chosen': '-164.6', 'loss/train': '0.61824', 'examples_per_second': '30.968', 'grad_norm': '42', 'counters/examples': 271872, 'counters/updates': 8496}
train stats after 271904 examples: {'rewards_train/chosen': '0.071466', 'rewards_train/rejected': '0.02459', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.046876', 'logps_train/rejected': '-131.13', 'logps_train/chosen': '-113.44', 'loss/train': '0.67766', 'examples_per_second': '31.797', 'grad_norm': '27', 'counters/examples': 271904, 'counters/updates': 8497}
skipping logging after 271936 examples to avoid logging too frequently
train stats after 271968 examples: {'rewards_train/chosen': '0.17198', 'rewards_train/rejected': '0.045969', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.12601', 'logps_train/rejected': '-123.16', 'logps_train/chosen': '-141.59', 'loss/train': '0.6649', 'examples_per_second': '32.974', 'grad_norm': '28.125', 'counters/examples': 271968, 'counters/updates': 8499}
train stats after 272000 examples: {'rewards_train/chosen': '0.067765', 'rewards_train/rejected': '0.044583', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.023182', 'logps_train/rejected': '-118.87', 'logps_train/chosen': '-111.72', 'loss/train': '0.695', 'examples_per_second': '31.486', 'grad_norm': '28.25', 'counters/examples': 272000, 'counters/updates': 8500}
train stats after 272032 examples: {'rewards_train/chosen': '0.14305', 'rewards_train/rejected': '-0.022576', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.16563', 'logps_train/rejected': '-129.92', 'logps_train/chosen': '-144.57', 'loss/train': '0.63393', 'examples_per_second': '32.372', 'grad_norm': '26.75', 'counters/examples': 272032, 'counters/updates': 8501}
train stats after 272064 examples: {'rewards_train/chosen': '0.07652', 'rewards_train/rejected': '-0.042557', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11908', 'logps_train/rejected': '-100.01', 'logps_train/chosen': '-114.91', 'loss/train': '0.65202', 'examples_per_second': '32.779', 'grad_norm': '24.875', 'counters/examples': 272064, 'counters/updates': 8502}
train stats after 272096 examples: {'rewards_train/chosen': '0.15331', 'rewards_train/rejected': '0.0047417', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14857', 'logps_train/rejected': '-154.5', 'logps_train/chosen': '-189.83', 'loss/train': '0.637', 'examples_per_second': '30.046', 'grad_norm': '47.75', 'counters/examples': 272096, 'counters/updates': 8503}
skipping logging after 272128 examples to avoid logging too frequently
train stats after 272160 examples: {'rewards_train/chosen': '0.11922', 'rewards_train/rejected': '0.027245', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.091973', 'logps_train/rejected': '-152.72', 'logps_train/chosen': '-156.45', 'loss/train': '0.66771', 'examples_per_second': '30.613', 'grad_norm': '30.125', 'counters/examples': 272160, 'counters/updates': 8505}
train stats after 272192 examples: {'rewards_train/chosen': '0.21809', 'rewards_train/rejected': '0.079312', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13878', 'logps_train/rejected': '-127.37', 'logps_train/chosen': '-139.43', 'loss/train': '0.64247', 'examples_per_second': '31.485', 'grad_norm': '35.25', 'counters/examples': 272192, 'counters/updates': 8506}
skipping logging after 272224 examples to avoid logging too frequently
train stats after 272256 examples: {'rewards_train/chosen': '0.10413', 'rewards_train/rejected': '0.10576', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.0016311', 'logps_train/rejected': '-141.75', 'logps_train/chosen': '-163.27', 'loss/train': '0.71804', 'examples_per_second': '31.541', 'grad_norm': '39.75', 'counters/examples': 272256, 'counters/updates': 8508}
train stats after 272288 examples: {'rewards_train/chosen': '0.17315', 'rewards_train/rejected': '0.022908', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15024', 'logps_train/rejected': '-83.19', 'logps_train/chosen': '-106.33', 'loss/train': '0.63727', 'examples_per_second': '30.384', 'grad_norm': '25.25', 'counters/examples': 272288, 'counters/updates': 8509}
train stats after 272320 examples: {'rewards_train/chosen': '0.059506', 'rewards_train/rejected': '0.091677', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.03217', 'logps_train/rejected': '-118.12', 'logps_train/chosen': '-132.67', 'loss/train': '0.73785', 'examples_per_second': '31.445', 'grad_norm': '38.5', 'counters/examples': 272320, 'counters/updates': 8510}
train stats after 272352 examples: {'rewards_train/chosen': '0.17537', 'rewards_train/rejected': '0.022722', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15265', 'logps_train/rejected': '-111.69', 'logps_train/chosen': '-151.58', 'loss/train': '0.64832', 'examples_per_second': '31.538', 'grad_norm': '25.5', 'counters/examples': 272352, 'counters/updates': 8511}
train stats after 272384 examples: {'rewards_train/chosen': '0.24175', 'rewards_train/rejected': '0.041751', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.2', 'logps_train/rejected': '-154.54', 'logps_train/chosen': '-172.68', 'loss/train': '0.61644', 'examples_per_second': '31.322', 'grad_norm': '48.25', 'counters/examples': 272384, 'counters/updates': 8512}
train stats after 272416 examples: {'rewards_train/chosen': '0.18331', 'rewards_train/rejected': '0.029535', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15377', 'logps_train/rejected': '-120.18', 'logps_train/chosen': '-151.27', 'loss/train': '0.63966', 'examples_per_second': '31.362', 'grad_norm': '38', 'counters/examples': 272416, 'counters/updates': 8513}
train stats after 272448 examples: {'rewards_train/chosen': '0.2496', 'rewards_train/rejected': '0.063048', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18656', 'logps_train/rejected': '-128.96', 'logps_train/chosen': '-203.36', 'loss/train': '0.62899', 'examples_per_second': '29.973', 'grad_norm': '41', 'counters/examples': 272448, 'counters/updates': 8514}
train stats after 272480 examples: {'rewards_train/chosen': '0.18278', 'rewards_train/rejected': '0.088054', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094726', 'logps_train/rejected': '-124.7', 'logps_train/chosen': '-145.44', 'loss/train': '0.66874', 'examples_per_second': '32.292', 'grad_norm': '33.75', 'counters/examples': 272480, 'counters/updates': 8515}
train stats after 272512 examples: {'rewards_train/chosen': '0.11221', 'rewards_train/rejected': '0.048907', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.063302', 'logps_train/rejected': '-112.61', 'logps_train/chosen': '-113.38', 'loss/train': '0.67354', 'examples_per_second': '30.002', 'grad_norm': '43.25', 'counters/examples': 272512, 'counters/updates': 8516}
train stats after 272544 examples: {'rewards_train/chosen': '0.15191', 'rewards_train/rejected': '0.083124', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.068789', 'logps_train/rejected': '-144.62', 'logps_train/chosen': '-159.49', 'loss/train': '0.67064', 'examples_per_second': '31.262', 'grad_norm': '35.75', 'counters/examples': 272544, 'counters/updates': 8517}
skipping logging after 272576 examples to avoid logging too frequently
train stats after 272608 examples: {'rewards_train/chosen': '0.1818', 'rewards_train/rejected': '-0.00083343', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18263', 'logps_train/rejected': '-129.5', 'logps_train/chosen': '-116.51', 'loss/train': '0.62841', 'examples_per_second': '32.912', 'grad_norm': '36.25', 'counters/examples': 272608, 'counters/updates': 8519}
skipping logging after 272640 examples to avoid logging too frequently
train stats after 272672 examples: {'rewards_train/chosen': '0.16789', 'rewards_train/rejected': '0.040108', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12779', 'logps_train/rejected': '-113.17', 'logps_train/chosen': '-153.76', 'loss/train': '0.64716', 'examples_per_second': '36.862', 'grad_norm': '37.5', 'counters/examples': 272672, 'counters/updates': 8521}
train stats after 272704 examples: {'rewards_train/chosen': '0.13888', 'rewards_train/rejected': '0.076316', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062562', 'logps_train/rejected': '-127.88', 'logps_train/chosen': '-135.57', 'loss/train': '0.6825', 'examples_per_second': '32.171', 'grad_norm': '36.75', 'counters/examples': 272704, 'counters/updates': 8522}
skipping logging after 272736 examples to avoid logging too frequently
train stats after 272768 examples: {'rewards_train/chosen': '0.078282', 'rewards_train/rejected': '-0.038611', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11689', 'logps_train/rejected': '-84.44', 'logps_train/chosen': '-145.94', 'loss/train': '0.64339', 'examples_per_second': '31.13', 'grad_norm': '28.375', 'counters/examples': 272768, 'counters/updates': 8524}
train stats after 272800 examples: {'rewards_train/chosen': '0.20232', 'rewards_train/rejected': '-0.019694', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.22202', 'logps_train/rejected': '-112.62', 'logps_train/chosen': '-143.09', 'loss/train': '0.60252', 'examples_per_second': '31.493', 'grad_norm': '40.75', 'counters/examples': 272800, 'counters/updates': 8525}
train stats after 272832 examples: {'rewards_train/chosen': '0.22031', 'rewards_train/rejected': '0.18631', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.034001', 'logps_train/rejected': '-141.3', 'logps_train/chosen': '-131.7', 'loss/train': '0.70285', 'examples_per_second': '31.503', 'grad_norm': '32.75', 'counters/examples': 272832, 'counters/updates': 8526}
train stats after 272864 examples: {'rewards_train/chosen': '0.15412', 'rewards_train/rejected': '0.040074', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11404', 'logps_train/rejected': '-108.81', 'logps_train/chosen': '-146.59', 'loss/train': '0.64936', 'examples_per_second': '30.022', 'grad_norm': '34.75', 'counters/examples': 272864, 'counters/updates': 8527}
train stats after 272896 examples: {'rewards_train/chosen': '0.16556', 'rewards_train/rejected': '0.00052782', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16503', 'logps_train/rejected': '-97.356', 'logps_train/chosen': '-109.31', 'loss/train': '0.62701', 'examples_per_second': '32.744', 'grad_norm': '25.25', 'counters/examples': 272896, 'counters/updates': 8528}
train stats after 272928 examples: {'rewards_train/chosen': '0.18185', 'rewards_train/rejected': '-0.023002', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.20485', 'logps_train/rejected': '-121.78', 'logps_train/chosen': '-143.45', 'loss/train': '0.61529', 'examples_per_second': '31.847', 'grad_norm': '27.125', 'counters/examples': 272928, 'counters/updates': 8529}
skipping logging after 272960 examples to avoid logging too frequently
train stats after 272992 examples: {'rewards_train/chosen': '0.13519', 'rewards_train/rejected': '0.034155', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10103', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-102.31', 'loss/train': '0.65626', 'examples_per_second': '31.687', 'grad_norm': '27.25', 'counters/examples': 272992, 'counters/updates': 8531}
skipping logging after 273024 examples to avoid logging too frequently
train stats after 273056 examples: {'rewards_train/chosen': '0.15329', 'rewards_train/rejected': '0.051252', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10204', 'logps_train/rejected': '-109.13', 'logps_train/chosen': '-139.96', 'loss/train': '0.65214', 'examples_per_second': '30.289', 'grad_norm': '33', 'counters/examples': 273056, 'counters/updates': 8533}
train stats after 273088 examples: {'rewards_train/chosen': '0.26438', 'rewards_train/rejected': '-0.065456', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.32984', 'logps_train/rejected': '-116.24', 'logps_train/chosen': '-183.44', 'loss/train': '0.56653', 'examples_per_second': '32.655', 'grad_norm': '30.125', 'counters/examples': 273088, 'counters/updates': 8534}
train stats after 273120 examples: {'rewards_train/chosen': '0.17336', 'rewards_train/rejected': '0.042821', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13054', 'logps_train/rejected': '-120.54', 'logps_train/chosen': '-138.04', 'loss/train': '0.63806', 'examples_per_second': '32.453', 'grad_norm': '35.75', 'counters/examples': 273120, 'counters/updates': 8535}
train stats after 273152 examples: {'rewards_train/chosen': '0.26069', 'rewards_train/rejected': '0.051419', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20928', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-152.32', 'loss/train': '0.61526', 'examples_per_second': '31.958', 'grad_norm': '28.125', 'counters/examples': 273152, 'counters/updates': 8536}
skipping logging after 273184 examples to avoid logging too frequently
train stats after 273216 examples: {'rewards_train/chosen': '0.17097', 'rewards_train/rejected': '0.020897', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15007', 'logps_train/rejected': '-135.96', 'logps_train/chosen': '-145.36', 'loss/train': '0.63623', 'examples_per_second': '32.128', 'grad_norm': '28.625', 'counters/examples': 273216, 'counters/updates': 8538}
train stats after 273248 examples: {'rewards_train/chosen': '0.13177', 'rewards_train/rejected': '-0.0067926', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13856', 'logps_train/rejected': '-118.34', 'logps_train/chosen': '-101.71', 'loss/train': '0.63883', 'examples_per_second': '30.898', 'grad_norm': '26.25', 'counters/examples': 273248, 'counters/updates': 8539}
train stats after 273280 examples: {'rewards_train/chosen': '0.21981', 'rewards_train/rejected': '0.11259', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10723', 'logps_train/rejected': '-112.81', 'logps_train/chosen': '-145.43', 'loss/train': '0.65733', 'examples_per_second': '31.945', 'grad_norm': '28.625', 'counters/examples': 273280, 'counters/updates': 8540}
train stats after 273312 examples: {'rewards_train/chosen': '0.15718', 'rewards_train/rejected': '0.067774', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.089408', 'logps_train/rejected': '-124.53', 'logps_train/chosen': '-156.79', 'loss/train': '0.66113', 'examples_per_second': '31.252', 'grad_norm': '44', 'counters/examples': 273312, 'counters/updates': 8541}
train stats after 273344 examples: {'rewards_train/chosen': '0.31198', 'rewards_train/rejected': '0.26772', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.04426', 'logps_train/rejected': '-150.31', 'logps_train/chosen': '-157.42', 'loss/train': '0.7017', 'examples_per_second': '31.536', 'grad_norm': '59', 'counters/examples': 273344, 'counters/updates': 8542}
train stats after 273376 examples: {'rewards_train/chosen': '0.099852', 'rewards_train/rejected': '-0.030461', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.13031', 'logps_train/rejected': '-91.39', 'logps_train/chosen': '-94.173', 'loss/train': '0.64466', 'examples_per_second': '31.54', 'grad_norm': '21.5', 'counters/examples': 273376, 'counters/updates': 8543}
train stats after 273408 examples: {'rewards_train/chosen': '0.1555', 'rewards_train/rejected': '0.16199', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.006487', 'logps_train/rejected': '-124.81', 'logps_train/chosen': '-135.89', 'loss/train': '0.70841', 'examples_per_second': '30.412', 'grad_norm': '28.375', 'counters/examples': 273408, 'counters/updates': 8544}
skipping logging after 273440 examples to avoid logging too frequently
train stats after 273472 examples: {'rewards_train/chosen': '0.13831', 'rewards_train/rejected': '0.043785', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.094524', 'logps_train/rejected': '-125.63', 'logps_train/chosen': '-163.35', 'loss/train': '0.6647', 'examples_per_second': '26.47', 'grad_norm': '36', 'counters/examples': 273472, 'counters/updates': 8546}
train stats after 273504 examples: {'rewards_train/chosen': '0.1327', 'rewards_train/rejected': '0.12578', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.006915', 'logps_train/rejected': '-101.31', 'logps_train/chosen': '-131.95', 'loss/train': '0.70257', 'examples_per_second': '30.813', 'grad_norm': '30.125', 'counters/examples': 273504, 'counters/updates': 8547}
train stats after 273536 examples: {'rewards_train/chosen': '0.14735', 'rewards_train/rejected': '0.049', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.098353', 'logps_train/rejected': '-160.79', 'logps_train/chosen': '-139.42', 'loss/train': '0.66426', 'examples_per_second': '32.461', 'grad_norm': '35.75', 'counters/examples': 273536, 'counters/updates': 8548}
train stats after 273568 examples: {'rewards_train/chosen': '0.22927', 'rewards_train/rejected': '0.01724', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21203', 'logps_train/rejected': '-118.65', 'logps_train/chosen': '-183.59', 'loss/train': '0.6', 'examples_per_second': '25.78', 'grad_norm': '27.875', 'counters/examples': 273568, 'counters/updates': 8549}
train stats after 273600 examples: {'rewards_train/chosen': '0.18699', 'rewards_train/rejected': '0.041353', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14564', 'logps_train/rejected': '-111.73', 'logps_train/chosen': '-129.26', 'loss/train': '0.63422', 'examples_per_second': '31.401', 'grad_norm': '26.5', 'counters/examples': 273600, 'counters/updates': 8550}
train stats after 273632 examples: {'rewards_train/chosen': '0.16283', 'rewards_train/rejected': '0.077817', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085008', 'logps_train/rejected': '-156.33', 'logps_train/chosen': '-122.73', 'loss/train': '0.67796', 'examples_per_second': '31.127', 'grad_norm': '33.75', 'counters/examples': 273632, 'counters/updates': 8551}
train stats after 273664 examples: {'rewards_train/chosen': '0.13885', 'rewards_train/rejected': '-0.038976', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17783', 'logps_train/rejected': '-118.94', 'logps_train/chosen': '-124.45', 'loss/train': '0.6189', 'examples_per_second': '31.936', 'grad_norm': '24.5', 'counters/examples': 273664, 'counters/updates': 8552}
train stats after 273696 examples: {'rewards_train/chosen': '0.17709', 'rewards_train/rejected': '0.04179', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1353', 'logps_train/rejected': '-118.56', 'logps_train/chosen': '-133.74', 'loss/train': '0.64469', 'examples_per_second': '30.991', 'grad_norm': '24.375', 'counters/examples': 273696, 'counters/updates': 8553}
skipping logging after 273728 examples to avoid logging too frequently
train stats after 273760 examples: {'rewards_train/chosen': '0.1403', 'rewards_train/rejected': '-0.01797', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15827', 'logps_train/rejected': '-115.88', 'logps_train/chosen': '-126.31', 'loss/train': '0.6381', 'examples_per_second': '32.379', 'grad_norm': '25.875', 'counters/examples': 273760, 'counters/updates': 8555}
train stats after 273792 examples: {'rewards_train/chosen': '0.18911', 'rewards_train/rejected': '0.016033', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17308', 'logps_train/rejected': '-151.68', 'logps_train/chosen': '-148.17', 'loss/train': '0.62992', 'examples_per_second': '30.639', 'grad_norm': '32.75', 'counters/examples': 273792, 'counters/updates': 8556}
skipping logging after 273824 examples to avoid logging too frequently
train stats after 273856 examples: {'rewards_train/chosen': '0.18851', 'rewards_train/rejected': '0.12641', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.062101', 'logps_train/rejected': '-123.03', 'logps_train/chosen': '-173.38', 'loss/train': '0.69022', 'examples_per_second': '38.951', 'grad_norm': '40.25', 'counters/examples': 273856, 'counters/updates': 8558}
train stats after 273888 examples: {'rewards_train/chosen': '0.12742', 'rewards_train/rejected': '0.090778', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.036642', 'logps_train/rejected': '-130.74', 'logps_train/chosen': '-132.67', 'loss/train': '0.6907', 'examples_per_second': '30.042', 'grad_norm': '29.875', 'counters/examples': 273888, 'counters/updates': 8559}
train stats after 273920 examples: {'rewards_train/chosen': '0.17087', 'rewards_train/rejected': '0.048971', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.1219', 'logps_train/rejected': '-149.45', 'logps_train/chosen': '-144.31', 'loss/train': '0.65416', 'examples_per_second': '31.456', 'grad_norm': '28.375', 'counters/examples': 273920, 'counters/updates': 8560}
train stats after 273952 examples: {'rewards_train/chosen': '0.18551', 'rewards_train/rejected': '0.06239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12312', 'logps_train/rejected': '-122.12', 'logps_train/chosen': '-137.17', 'loss/train': '0.64296', 'examples_per_second': '31.49', 'grad_norm': '28', 'counters/examples': 273952, 'counters/updates': 8561}
train stats after 273984 examples: {'rewards_train/chosen': '0.16689', 'rewards_train/rejected': '0.11832', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.048565', 'logps_train/rejected': '-137.28', 'logps_train/chosen': '-119.63', 'loss/train': '0.68741', 'examples_per_second': '30.814', 'grad_norm': '45.75', 'counters/examples': 273984, 'counters/updates': 8562}
train stats after 274016 examples: {'rewards_train/chosen': '0.19139', 'rewards_train/rejected': '0.010446', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18094', 'logps_train/rejected': '-116.14', 'logps_train/chosen': '-128.59', 'loss/train': '0.62429', 'examples_per_second': '30.123', 'grad_norm': '32.5', 'counters/examples': 274016, 'counters/updates': 8563}
skipping logging after 274048 examples to avoid logging too frequently
train stats after 274080 examples: {'rewards_train/chosen': '0.16764', 'rewards_train/rejected': '0.018343', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.1493', 'logps_train/rejected': '-111.95', 'logps_train/chosen': '-146.93', 'loss/train': '0.63332', 'examples_per_second': '33.068', 'grad_norm': '28', 'counters/examples': 274080, 'counters/updates': 8565}
train stats after 274112 examples: {'rewards_train/chosen': '0.12666', 'rewards_train/rejected': '0.032948', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093715', 'logps_train/rejected': '-121.43', 'logps_train/chosen': '-161.54', 'loss/train': '0.6653', 'examples_per_second': '29.867', 'grad_norm': '35.25', 'counters/examples': 274112, 'counters/updates': 8566}
train stats after 274144 examples: {'rewards_train/chosen': '0.22633', 'rewards_train/rejected': '0.12847', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.097856', 'logps_train/rejected': '-170.6', 'logps_train/chosen': '-162.98', 'loss/train': '0.66217', 'examples_per_second': '31.342', 'grad_norm': '33', 'counters/examples': 274144, 'counters/updates': 8567}
train stats after 274176 examples: {'rewards_train/chosen': '0.17731', 'rewards_train/rejected': '0.076121', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10119', 'logps_train/rejected': '-125.15', 'logps_train/chosen': '-147.78', 'loss/train': '0.66163', 'examples_per_second': '32.943', 'grad_norm': '32.75', 'counters/examples': 274176, 'counters/updates': 8568}
train stats after 274208 examples: {'rewards_train/chosen': '0.21961', 'rewards_train/rejected': '0.074798', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14481', 'logps_train/rejected': '-112.39', 'logps_train/chosen': '-132.68', 'loss/train': '0.63167', 'examples_per_second': '33.207', 'grad_norm': '25.25', 'counters/examples': 274208, 'counters/updates': 8569}
train stats after 274240 examples: {'rewards_train/chosen': '0.23536', 'rewards_train/rejected': '0.053904', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18146', 'logps_train/rejected': '-137.9', 'logps_train/chosen': '-128.84', 'loss/train': '0.61624', 'examples_per_second': '31.485', 'grad_norm': '35.75', 'counters/examples': 274240, 'counters/updates': 8570}
train stats after 274272 examples: {'rewards_train/chosen': '0.15836', 'rewards_train/rejected': '-0.058096', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21645', 'logps_train/rejected': '-134.56', 'logps_train/chosen': '-156.51', 'loss/train': '0.61217', 'examples_per_second': '31.276', 'grad_norm': '27.25', 'counters/examples': 274272, 'counters/updates': 8571}
train stats after 274304 examples: {'rewards_train/chosen': '0.18212', 'rewards_train/rejected': '0.073449', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10867', 'logps_train/rejected': '-125.02', 'logps_train/chosen': '-115.89', 'loss/train': '0.65039', 'examples_per_second': '33.021', 'grad_norm': '42.5', 'counters/examples': 274304, 'counters/updates': 8572}
train stats after 274336 examples: {'rewards_train/chosen': '0.090326', 'rewards_train/rejected': '0.01023', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.080095', 'logps_train/rejected': '-111.79', 'logps_train/chosen': '-138.68', 'loss/train': '0.66321', 'examples_per_second': '31.818', 'grad_norm': '44.25', 'counters/examples': 274336, 'counters/updates': 8573}
train stats after 274368 examples: {'rewards_train/chosen': '0.076123', 'rewards_train/rejected': '-0.01249', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.088613', 'logps_train/rejected': '-126.57', 'logps_train/chosen': '-133.69', 'loss/train': '0.66649', 'examples_per_second': '31.418', 'grad_norm': '25.875', 'counters/examples': 274368, 'counters/updates': 8574}
skipping logging after 274400 examples to avoid logging too frequently
train stats after 274432 examples: {'rewards_train/chosen': '0.055109', 'rewards_train/rejected': '0.10073', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.045622', 'logps_train/rejected': '-105.6', 'logps_train/chosen': '-125.55', 'loss/train': '0.73498', 'examples_per_second': '31.434', 'grad_norm': '39', 'counters/examples': 274432, 'counters/updates': 8576}
train stats after 274464 examples: {'rewards_train/chosen': '0.2188', 'rewards_train/rejected': '0.07608', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14272', 'logps_train/rejected': '-133.18', 'logps_train/chosen': '-146.56', 'loss/train': '0.65563', 'examples_per_second': '31.522', 'grad_norm': '44.75', 'counters/examples': 274464, 'counters/updates': 8577}
train stats after 274496 examples: {'rewards_train/chosen': '0.1462', 'rewards_train/rejected': '0.05415', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092052', 'logps_train/rejected': '-113.47', 'logps_train/chosen': '-124.21', 'loss/train': '0.66903', 'examples_per_second': '31.536', 'grad_norm': '33.25', 'counters/examples': 274496, 'counters/updates': 8578}
train stats after 274528 examples: {'rewards_train/chosen': '0.17745', 'rewards_train/rejected': '0.021501', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.15595', 'logps_train/rejected': '-138.44', 'logps_train/chosen': '-145', 'loss/train': '0.64793', 'examples_per_second': '30.546', 'grad_norm': '31.25', 'counters/examples': 274528, 'counters/updates': 8579}
train stats after 274560 examples: {'rewards_train/chosen': '0.1489', 'rewards_train/rejected': '0.07802', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.070884', 'logps_train/rejected': '-163.17', 'logps_train/chosen': '-159.45', 'loss/train': '0.67478', 'examples_per_second': '31.238', 'grad_norm': '31.25', 'counters/examples': 274560, 'counters/updates': 8580}
train stats after 274592 examples: {'rewards_train/chosen': '0.24476', 'rewards_train/rejected': '0.1972', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.047558', 'logps_train/rejected': '-157.01', 'logps_train/chosen': '-145.57', 'loss/train': '0.69027', 'examples_per_second': '33.08', 'grad_norm': '38.25', 'counters/examples': 274592, 'counters/updates': 8581}
skipping logging after 274624 examples to avoid logging too frequently
train stats after 274656 examples: {'rewards_train/chosen': '0.1036', 'rewards_train/rejected': '0.043581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060023', 'logps_train/rejected': '-160.07', 'logps_train/chosen': '-173.56', 'loss/train': '0.68003', 'examples_per_second': '25.624', 'grad_norm': '30.125', 'counters/examples': 274656, 'counters/updates': 8583}
train stats after 274688 examples: {'rewards_train/chosen': '0.1685', 'rewards_train/rejected': '0.12349', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.045009', 'logps_train/rejected': '-137.68', 'logps_train/chosen': '-142.16', 'loss/train': '0.68658', 'examples_per_second': '31.491', 'grad_norm': '40', 'counters/examples': 274688, 'counters/updates': 8584}
train stats after 274720 examples: {'rewards_train/chosen': '0.089001', 'rewards_train/rejected': '-0.017444', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10644', 'logps_train/rejected': '-165.19', 'logps_train/chosen': '-136.24', 'loss/train': '0.65017', 'examples_per_second': '31.074', 'grad_norm': '27.875', 'counters/examples': 274720, 'counters/updates': 8585}
train stats after 274752 examples: {'rewards_train/chosen': '0.10645', 'rewards_train/rejected': '-0.0052507', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1117', 'logps_train/rejected': '-99.154', 'logps_train/chosen': '-160.86', 'loss/train': '0.64615', 'examples_per_second': '31.389', 'grad_norm': '27', 'counters/examples': 274752, 'counters/updates': 8586}
train stats after 274784 examples: {'rewards_train/chosen': '0.19107', 'rewards_train/rejected': '0.086988', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10409', 'logps_train/rejected': '-119.23', 'logps_train/chosen': '-149.18', 'loss/train': '0.64917', 'examples_per_second': '30.626', 'grad_norm': '28.625', 'counters/examples': 274784, 'counters/updates': 8587}
train stats after 274816 examples: {'rewards_train/chosen': '0.16855', 'rewards_train/rejected': '0.036356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1322', 'logps_train/rejected': '-147.09', 'logps_train/chosen': '-145.51', 'loss/train': '0.64631', 'examples_per_second': '31.34', 'grad_norm': '36.25', 'counters/examples': 274816, 'counters/updates': 8588}
skipping logging after 274848 examples to avoid logging too frequently
train stats after 274880 examples: {'rewards_train/chosen': '0.13954', 'rewards_train/rejected': '0.14537', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.0058309', 'logps_train/rejected': '-147.5', 'logps_train/chosen': '-143.11', 'loss/train': '0.70584', 'examples_per_second': '31.519', 'grad_norm': '34.5', 'counters/examples': 274880, 'counters/updates': 8590}
train stats after 274912 examples: {'rewards_train/chosen': '0.13029', 'rewards_train/rejected': '-0.0085329', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13882', 'logps_train/rejected': '-138.62', 'logps_train/chosen': '-158.29', 'loss/train': '0.64194', 'examples_per_second': '31.498', 'grad_norm': '31.5', 'counters/examples': 274912, 'counters/updates': 8591}
train stats after 274944 examples: {'rewards_train/chosen': '0.1213', 'rewards_train/rejected': '0.0050387', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11626', 'logps_train/rejected': '-130.17', 'logps_train/chosen': '-108.8', 'loss/train': '0.65469', 'examples_per_second': '29.982', 'grad_norm': '23.5', 'counters/examples': 274944, 'counters/updates': 8592}
skipping logging after 274976 examples to avoid logging too frequently
train stats after 275008 examples: {'rewards_train/chosen': '0.23105', 'rewards_train/rejected': '0.066683', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16437', 'logps_train/rejected': '-118.06', 'logps_train/chosen': '-135.98', 'loss/train': '0.64386', 'examples_per_second': '30.457', 'grad_norm': '32.25', 'counters/examples': 275008, 'counters/updates': 8594}
skipping logging after 275040 examples to avoid logging too frequently
train stats after 275072 examples: {'rewards_train/chosen': '0.24551', 'rewards_train/rejected': '0.10546', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14005', 'logps_train/rejected': '-136.74', 'logps_train/chosen': '-138.93', 'loss/train': '0.6431', 'examples_per_second': '30.405', 'grad_norm': '30.875', 'counters/examples': 275072, 'counters/updates': 8596}
train stats after 275104 examples: {'rewards_train/chosen': '0.12395', 'rewards_train/rejected': '0.15067', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.026715', 'logps_train/rejected': '-151.93', 'logps_train/chosen': '-183.36', 'loss/train': '0.72694', 'examples_per_second': '31.374', 'grad_norm': '37.5', 'counters/examples': 275104, 'counters/updates': 8597}
train stats after 275136 examples: {'rewards_train/chosen': '0.25312', 'rewards_train/rejected': '0.097182', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15593', 'logps_train/rejected': '-106.9', 'logps_train/chosen': '-140.44', 'loss/train': '0.64133', 'examples_per_second': '31.27', 'grad_norm': '35.5', 'counters/examples': 275136, 'counters/updates': 8598}
train stats after 275168 examples: {'rewards_train/chosen': '0.14568', 'rewards_train/rejected': '0.084818', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.060865', 'logps_train/rejected': '-121.52', 'logps_train/chosen': '-115.82', 'loss/train': '0.67395', 'examples_per_second': '32.591', 'grad_norm': '35.75', 'counters/examples': 275168, 'counters/updates': 8599}
skipping logging after 275200 examples to avoid logging too frequently
train stats after 275232 examples: {'rewards_train/chosen': '0.13471', 'rewards_train/rejected': '0.040765', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.093941', 'logps_train/rejected': '-114.09', 'logps_train/chosen': '-137.93', 'loss/train': '0.66296', 'examples_per_second': '31.149', 'grad_norm': '25.125', 'counters/examples': 275232, 'counters/updates': 8601}
train stats after 275264 examples: {'rewards_train/chosen': '0.20038', 'rewards_train/rejected': '0.070772', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1296', 'logps_train/rejected': '-105.27', 'logps_train/chosen': '-128.6', 'loss/train': '0.64559', 'examples_per_second': '31.026', 'grad_norm': '26.5', 'counters/examples': 275264, 'counters/updates': 8602}
train stats after 275296 examples: {'rewards_train/chosen': '0.27339', 'rewards_train/rejected': '-0.014818', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.28821', 'logps_train/rejected': '-126.83', 'logps_train/chosen': '-141.37', 'loss/train': '0.58128', 'examples_per_second': '31.221', 'grad_norm': '28.625', 'counters/examples': 275296, 'counters/updates': 8603}
train stats after 275328 examples: {'rewards_train/chosen': '0.11436', 'rewards_train/rejected': '0.03112', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.083239', 'logps_train/rejected': '-131.08', 'logps_train/chosen': '-101.35', 'loss/train': '0.68201', 'examples_per_second': '30.886', 'grad_norm': '37', 'counters/examples': 275328, 'counters/updates': 8604}
train stats after 275360 examples: {'rewards_train/chosen': '0.13315', 'rewards_train/rejected': '0.077827', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.055327', 'logps_train/rejected': '-142.09', 'logps_train/chosen': '-169.14', 'loss/train': '0.67495', 'examples_per_second': '31.504', 'grad_norm': '32.5', 'counters/examples': 275360, 'counters/updates': 8605}
train stats after 275392 examples: {'rewards_train/chosen': '0.19784', 'rewards_train/rejected': '0.012676', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18516', 'logps_train/rejected': '-125.06', 'logps_train/chosen': '-137.62', 'loss/train': '0.6176', 'examples_per_second': '32.263', 'grad_norm': '35.25', 'counters/examples': 275392, 'counters/updates': 8606}
train stats after 275424 examples: {'rewards_train/chosen': '0.15208', 'rewards_train/rejected': '0.070748', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.081332', 'logps_train/rejected': '-116.65', 'logps_train/chosen': '-138.89', 'loss/train': '0.66844', 'examples_per_second': '31.942', 'grad_norm': '34', 'counters/examples': 275424, 'counters/updates': 8607}
train stats after 275456 examples: {'rewards_train/chosen': '0.21852', 'rewards_train/rejected': '0.11967', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.098855', 'logps_train/rejected': '-145.2', 'logps_train/chosen': '-139.93', 'loss/train': '0.66408', 'examples_per_second': '31.477', 'grad_norm': '35.75', 'counters/examples': 275456, 'counters/updates': 8608}
train stats after 275488 examples: {'rewards_train/chosen': '0.2096', 'rewards_train/rejected': '0.17093', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.038672', 'logps_train/rejected': '-117.01', 'logps_train/chosen': '-134.4', 'loss/train': '0.69862', 'examples_per_second': '29.662', 'grad_norm': '48', 'counters/examples': 275488, 'counters/updates': 8609}
train stats after 275520 examples: {'rewards_train/chosen': '0.21028', 'rewards_train/rejected': '0.04478', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.1655', 'logps_train/rejected': '-114.44', 'logps_train/chosen': '-163.51', 'loss/train': '0.62774', 'examples_per_second': '30.95', 'grad_norm': '26.25', 'counters/examples': 275520, 'counters/updates': 8610}
skipping logging after 275552 examples to avoid logging too frequently
train stats after 275584 examples: {'rewards_train/chosen': '0.24362', 'rewards_train/rejected': '0.085487', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15814', 'logps_train/rejected': '-144.74', 'logps_train/chosen': '-163.51', 'loss/train': '0.63241', 'examples_per_second': '32.883', 'grad_norm': '31', 'counters/examples': 275584, 'counters/updates': 8612}
train stats after 275616 examples: {'rewards_train/chosen': '0.14778', 'rewards_train/rejected': '0.0048562', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14292', 'logps_train/rejected': '-107.23', 'logps_train/chosen': '-151.93', 'loss/train': '0.63765', 'examples_per_second': '31.47', 'grad_norm': '24.875', 'counters/examples': 275616, 'counters/updates': 8613}
train stats after 275648 examples: {'rewards_train/chosen': '0.11647', 'rewards_train/rejected': '0.031273', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.085196', 'logps_train/rejected': '-128.1', 'logps_train/chosen': '-101.68', 'loss/train': '0.66073', 'examples_per_second': '31.85', 'grad_norm': '26.75', 'counters/examples': 275648, 'counters/updates': 8614}
train stats after 275680 examples: {'rewards_train/chosen': '0.2074', 'rewards_train/rejected': '0.050275', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15713', 'logps_train/rejected': '-86.177', 'logps_train/chosen': '-134.62', 'loss/train': '0.63012', 'examples_per_second': '32.969', 'grad_norm': '23.125', 'counters/examples': 275680, 'counters/updates': 8615}
train stats after 275712 examples: {'rewards_train/chosen': '0.21187', 'rewards_train/rejected': '0.073706', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13816', 'logps_train/rejected': '-131.45', 'logps_train/chosen': '-125.37', 'loss/train': '0.6526', 'examples_per_second': '31.168', 'grad_norm': '34.25', 'counters/examples': 275712, 'counters/updates': 8616}
train stats after 275744 examples: {'rewards_train/chosen': '0.24469', 'rewards_train/rejected': '0.10252', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14218', 'logps_train/rejected': '-149.6', 'logps_train/chosen': '-179.63', 'loss/train': '0.64543', 'examples_per_second': '32.962', 'grad_norm': '31.125', 'counters/examples': 275744, 'counters/updates': 8617}
train stats after 275776 examples: {'rewards_train/chosen': '0.20118', 'rewards_train/rejected': '-0.010499', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21168', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-152.25', 'loss/train': '0.60912', 'examples_per_second': '30.507', 'grad_norm': '34.5', 'counters/examples': 275776, 'counters/updates': 8618}
train stats after 275808 examples: {'rewards_train/chosen': '0.25593', 'rewards_train/rejected': '0.093995', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16193', 'logps_train/rejected': '-132.6', 'logps_train/chosen': '-149.35', 'loss/train': '0.63283', 'examples_per_second': '31.502', 'grad_norm': '25.25', 'counters/examples': 275808, 'counters/updates': 8619}
train stats after 275840 examples: {'rewards_train/chosen': '0.072963', 'rewards_train/rejected': '0.041588', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.031375', 'logps_train/rejected': '-161.23', 'logps_train/chosen': '-129.68', 'loss/train': '0.69174', 'examples_per_second': '31.128', 'grad_norm': '86', 'counters/examples': 275840, 'counters/updates': 8620}
train stats after 275872 examples: {'rewards_train/chosen': '0.25942', 'rewards_train/rejected': '0.090599', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16882', 'logps_train/rejected': '-144.73', 'logps_train/chosen': '-172.07', 'loss/train': '0.63393', 'examples_per_second': '30.897', 'grad_norm': '29.875', 'counters/examples': 275872, 'counters/updates': 8621}
train stats after 275904 examples: {'rewards_train/chosen': '0.10114', 'rewards_train/rejected': '0.034884', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.066252', 'logps_train/rejected': '-101.36', 'logps_train/chosen': '-86.239', 'loss/train': '0.67377', 'examples_per_second': '31.819', 'grad_norm': '24.125', 'counters/examples': 275904, 'counters/updates': 8622}
skipping logging after 275936 examples to avoid logging too frequently
train stats after 275968 examples: {'rewards_train/chosen': '0.17496', 'rewards_train/rejected': '0.10365', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.071307', 'logps_train/rejected': '-133.46', 'logps_train/chosen': '-155.73', 'loss/train': '0.67899', 'examples_per_second': '31.459', 'grad_norm': '113.5', 'counters/examples': 275968, 'counters/updates': 8624}
train stats after 276000 examples: {'rewards_train/chosen': '0.26572', 'rewards_train/rejected': '0.093636', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17208', 'logps_train/rejected': '-134.21', 'logps_train/chosen': '-117.27', 'loss/train': '0.62851', 'examples_per_second': '32.235', 'grad_norm': '29.125', 'counters/examples': 276000, 'counters/updates': 8625}
Running evaluation after 276000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.26it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.95it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.86it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  4.01it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.92it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.88it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
eval after 276000: {'rewards_eval/chosen': '0.19983', 'rewards_eval/rejected': '0.064287', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.13554', 'logps_eval/rejected': '-121.49', 'logps_eval/chosen': '-142.11', 'loss/eval': '0.64675'}
skipping save for non epoch
train stats after 276032 examples: {'rewards_train/chosen': '0.12961', 'rewards_train/rejected': '-0.057792', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1874', 'logps_train/rejected': '-121.92', 'logps_train/chosen': '-143.58', 'loss/train': '0.61433', 'examples_per_second': '33.289', 'grad_norm': '31.5', 'counters/examples': 276032, 'counters/updates': 8626}
skipping logging after 276064 examples to avoid logging too frequently
train stats after 276096 examples: {'rewards_train/chosen': '0.082066', 'rewards_train/rejected': '0.14183', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.059761', 'logps_train/rejected': '-151.95', 'logps_train/chosen': '-146.87', 'loss/train': '0.73618', 'examples_per_second': '34.552', 'grad_norm': '32.75', 'counters/examples': 276096, 'counters/updates': 8628}
train stats after 276128 examples: {'rewards_train/chosen': '0.13969', 'rewards_train/rejected': '0.03209', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1076', 'logps_train/rejected': '-111.44', 'logps_train/chosen': '-133.96', 'loss/train': '0.65263', 'examples_per_second': '30.407', 'grad_norm': '28.375', 'counters/examples': 276128, 'counters/updates': 8629}
train stats after 276160 examples: {'rewards_train/chosen': '0.13698', 'rewards_train/rejected': '0.035431', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10155', 'logps_train/rejected': '-104.58', 'logps_train/chosen': '-131.26', 'loss/train': '0.65753', 'examples_per_second': '31.501', 'grad_norm': '36', 'counters/examples': 276160, 'counters/updates': 8630}
skipping logging after 276192 examples to avoid logging too frequently
train stats after 276224 examples: {'rewards_train/chosen': '0.12005', 'rewards_train/rejected': '0.20182', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.081769', 'logps_train/rejected': '-148.49', 'logps_train/chosen': '-128.71', 'loss/train': '0.75083', 'examples_per_second': '30.931', 'grad_norm': '43.5', 'counters/examples': 276224, 'counters/updates': 8632}
train stats after 276256 examples: {'rewards_train/chosen': '0.071155', 'rewards_train/rejected': '0.057007', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.014147', 'logps_train/rejected': '-142.43', 'logps_train/chosen': '-197.85', 'loss/train': '0.74734', 'examples_per_second': '30.981', 'grad_norm': '215', 'counters/examples': 276256, 'counters/updates': 8633}
train stats after 276288 examples: {'rewards_train/chosen': '0.18952', 'rewards_train/rejected': '0.117', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.072517', 'logps_train/rejected': '-135.24', 'logps_train/chosen': '-153.01', 'loss/train': '0.67861', 'examples_per_second': '31.939', 'grad_norm': '33', 'counters/examples': 276288, 'counters/updates': 8634}
train stats after 276320 examples: {'rewards_train/chosen': '0.10543', 'rewards_train/rejected': '0.010428', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.095007', 'logps_train/rejected': '-133.34', 'logps_train/chosen': '-160.02', 'loss/train': '0.65687', 'examples_per_second': '30.676', 'grad_norm': '29', 'counters/examples': 276320, 'counters/updates': 8635}
train stats after 276352 examples: {'rewards_train/chosen': '0.1679', 'rewards_train/rejected': '0.089375', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.078529', 'logps_train/rejected': '-159.67', 'logps_train/chosen': '-157.6', 'loss/train': '0.67736', 'examples_per_second': '31.099', 'grad_norm': '30.375', 'counters/examples': 276352, 'counters/updates': 8636}
train stats after 276384 examples: {'rewards_train/chosen': '0.21743', 'rewards_train/rejected': '0.088448', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12898', 'logps_train/rejected': '-101.94', 'logps_train/chosen': '-120.74', 'loss/train': '0.6448', 'examples_per_second': '30.554', 'grad_norm': '33', 'counters/examples': 276384, 'counters/updates': 8637}
skipping logging after 276416 examples to avoid logging too frequently
train stats after 276448 examples: {'rewards_train/chosen': '0.30345', 'rewards_train/rejected': '0.15054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15291', 'logps_train/rejected': '-156.5', 'logps_train/chosen': '-168.89', 'loss/train': '0.64467', 'examples_per_second': '31.379', 'grad_norm': '33.25', 'counters/examples': 276448, 'counters/updates': 8639}
skipping logging after 276480 examples to avoid logging too frequently
train stats after 276512 examples: {'rewards_train/chosen': '0.19952', 'rewards_train/rejected': '-0.014531', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21405', 'logps_train/rejected': '-123.14', 'logps_train/chosen': '-128.55', 'loss/train': '0.60782', 'examples_per_second': '33.633', 'grad_norm': '23.875', 'counters/examples': 276512, 'counters/updates': 8641}
train stats after 276544 examples: {'rewards_train/chosen': '0.075934', 'rewards_train/rejected': '0.03326', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.042674', 'logps_train/rejected': '-112.68', 'logps_train/chosen': '-118.41', 'loss/train': '0.68859', 'examples_per_second': '32.391', 'grad_norm': '27.5', 'counters/examples': 276544, 'counters/updates': 8642}
train stats after 276576 examples: {'rewards_train/chosen': '0.24769', 'rewards_train/rejected': '-0.062747', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.31043', 'logps_train/rejected': '-114.21', 'logps_train/chosen': '-167.8', 'loss/train': '0.57401', 'examples_per_second': '31.408', 'grad_norm': '33', 'counters/examples': 276576, 'counters/updates': 8643}
train stats after 276608 examples: {'rewards_train/chosen': '0.21017', 'rewards_train/rejected': '0.003081', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20709', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-156.44', 'loss/train': '0.60792', 'examples_per_second': '31.817', 'grad_norm': '34', 'counters/examples': 276608, 'counters/updates': 8644}
train stats after 276640 examples: {'rewards_train/chosen': '0.1744', 'rewards_train/rejected': '0.075053', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.099346', 'logps_train/rejected': '-106.64', 'logps_train/chosen': '-140.88', 'loss/train': '0.6561', 'examples_per_second': '29.93', 'grad_norm': '25.625', 'counters/examples': 276640, 'counters/updates': 8645}
train stats after 276672 examples: {'rewards_train/chosen': '0.18087', 'rewards_train/rejected': '0.067515', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11335', 'logps_train/rejected': '-163.95', 'logps_train/chosen': '-135.63', 'loss/train': '0.64893', 'examples_per_second': '31.43', 'grad_norm': '48.5', 'counters/examples': 276672, 'counters/updates': 8646}
train stats after 276704 examples: {'rewards_train/chosen': '0.11272', 'rewards_train/rejected': '0.02529', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.087426', 'logps_train/rejected': '-113.14', 'logps_train/chosen': '-144.24', 'loss/train': '0.68476', 'examples_per_second': '29.671', 'grad_norm': '27', 'counters/examples': 276704, 'counters/updates': 8647}
skipping logging after 276736 examples to avoid logging too frequently
train stats after 276768 examples: {'rewards_train/chosen': '0.2022', 'rewards_train/rejected': '0.024766', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17743', 'logps_train/rejected': '-110.85', 'logps_train/chosen': '-128.82', 'loss/train': '0.6207', 'examples_per_second': '33.172', 'grad_norm': '23.875', 'counters/examples': 276768, 'counters/updates': 8649}
train stats after 276800 examples: {'rewards_train/chosen': '0.076808', 'rewards_train/rejected': '-0.073529', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15034', 'logps_train/rejected': '-138.16', 'logps_train/chosen': '-166.51', 'loss/train': '0.63596', 'examples_per_second': '29.965', 'grad_norm': '29.875', 'counters/examples': 276800, 'counters/updates': 8650}
train stats after 276832 examples: {'rewards_train/chosen': '0.149', 'rewards_train/rejected': '0.0059356', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14307', 'logps_train/rejected': '-110.95', 'logps_train/chosen': '-97.606', 'loss/train': '0.65157', 'examples_per_second': '31.822', 'grad_norm': '21.375', 'counters/examples': 276832, 'counters/updates': 8651}
train stats after 276864 examples: {'rewards_train/chosen': '0.099853', 'rewards_train/rejected': '0.024394', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.075459', 'logps_train/rejected': '-99.919', 'logps_train/chosen': '-133.99', 'loss/train': '0.66749', 'examples_per_second': '31.491', 'grad_norm': '43.5', 'counters/examples': 276864, 'counters/updates': 8652}
train stats after 276896 examples: {'rewards_train/chosen': '0.1019', 'rewards_train/rejected': '0.021283', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.080618', 'logps_train/rejected': '-162.05', 'logps_train/chosen': '-121.65', 'loss/train': '0.6743', 'examples_per_second': '31.472', 'grad_norm': '35.5', 'counters/examples': 276896, 'counters/updates': 8653}
train stats after 276928 examples: {'rewards_train/chosen': '0.1832', 'rewards_train/rejected': '0.039618', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14358', 'logps_train/rejected': '-138.11', 'logps_train/chosen': '-159.5', 'loss/train': '0.63979', 'examples_per_second': '32.358', 'grad_norm': '29.375', 'counters/examples': 276928, 'counters/updates': 8654}
train stats after 276960 examples: {'rewards_train/chosen': '0.17158', 'rewards_train/rejected': '0.087521', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084057', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-110.27', 'loss/train': '0.6696', 'examples_per_second': '31.97', 'grad_norm': '28.625', 'counters/examples': 276960, 'counters/updates': 8655}
train stats after 276992 examples: {'rewards_train/chosen': '0.2021', 'rewards_train/rejected': '0.045462', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15664', 'logps_train/rejected': '-98.792', 'logps_train/chosen': '-141.53', 'loss/train': '0.63479', 'examples_per_second': '31.498', 'grad_norm': '25', 'counters/examples': 276992, 'counters/updates': 8656}
skipping logging after 277024 examples to avoid logging too frequently
train stats after 277056 examples: {'rewards_train/chosen': '0.17332', 'rewards_train/rejected': '0.10314', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.070181', 'logps_train/rejected': '-126.16', 'logps_train/chosen': '-151.37', 'loss/train': '0.6704', 'examples_per_second': '32.742', 'grad_norm': '34.5', 'counters/examples': 277056, 'counters/updates': 8658}
train stats after 277088 examples: {'rewards_train/chosen': '0.14472', 'rewards_train/rejected': '-0.02149', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16621', 'logps_train/rejected': '-112.83', 'logps_train/chosen': '-125.32', 'loss/train': '0.62915', 'examples_per_second': '32.22', 'grad_norm': '27.875', 'counters/examples': 277088, 'counters/updates': 8659}
train stats after 277120 examples: {'rewards_train/chosen': '0.19071', 'rewards_train/rejected': '0.039045', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15166', 'logps_train/rejected': '-186.49', 'logps_train/chosen': '-149.43', 'loss/train': '0.66382', 'examples_per_second': '31.519', 'grad_norm': '60.25', 'counters/examples': 277120, 'counters/updates': 8660}
train stats after 277152 examples: {'rewards_train/chosen': '0.12792', 'rewards_train/rejected': '-0.033539', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16146', 'logps_train/rejected': '-116.72', 'logps_train/chosen': '-135.34', 'loss/train': '0.62997', 'examples_per_second': '32.948', 'grad_norm': '32.5', 'counters/examples': 277152, 'counters/updates': 8661}
skipping logging after 277184 examples to avoid logging too frequently
train stats after 277216 examples: {'rewards_train/chosen': '0.059807', 'rewards_train/rejected': '0.074493', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.014687', 'logps_train/rejected': '-126.53', 'logps_train/chosen': '-126.25', 'loss/train': '0.72276', 'examples_per_second': '34.132', 'grad_norm': '40', 'counters/examples': 277216, 'counters/updates': 8663}
skipping logging after 277248 examples to avoid logging too frequently
train stats after 277280 examples: {'rewards_train/chosen': '0.18952', 'rewards_train/rejected': '0.22908', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.039555', 'logps_train/rejected': '-152.68', 'logps_train/chosen': '-173.66', 'loss/train': '0.76429', 'examples_per_second': '31.52', 'grad_norm': '96', 'counters/examples': 277280, 'counters/updates': 8665}
train stats after 277312 examples: {'rewards_train/chosen': '0.070848', 'rewards_train/rejected': '0.009757', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061091', 'logps_train/rejected': '-146.02', 'logps_train/chosen': '-128.7', 'loss/train': '0.67005', 'examples_per_second': '30.587', 'grad_norm': '37', 'counters/examples': 277312, 'counters/updates': 8666}
train stats after 277344 examples: {'rewards_train/chosen': '0.15091', 'rewards_train/rejected': '0.11521', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0357', 'logps_train/rejected': '-140.34', 'logps_train/chosen': '-143.25', 'loss/train': '0.69935', 'examples_per_second': '31.202', 'grad_norm': '44.75', 'counters/examples': 277344, 'counters/updates': 8667}
train stats after 277376 examples: {'rewards_train/chosen': '0.23168', 'rewards_train/rejected': '0.053864', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17782', 'logps_train/rejected': '-117.47', 'logps_train/chosen': '-137.19', 'loss/train': '0.62227', 'examples_per_second': '31.625', 'grad_norm': '30.125', 'counters/examples': 277376, 'counters/updates': 8668}
train stats after 277408 examples: {'rewards_train/chosen': '0.1485', 'rewards_train/rejected': '0.052131', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.096368', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-168.48', 'loss/train': '0.65847', 'examples_per_second': '31.698', 'grad_norm': '28.75', 'counters/examples': 277408, 'counters/updates': 8669}
train stats after 277440 examples: {'rewards_train/chosen': '0.18179', 'rewards_train/rejected': '0.15876', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.02303', 'logps_train/rejected': '-150.26', 'logps_train/chosen': '-145.51', 'loss/train': '0.70029', 'examples_per_second': '32.164', 'grad_norm': '35.25', 'counters/examples': 277440, 'counters/updates': 8670}
train stats after 277472 examples: {'rewards_train/chosen': '0.19709', 'rewards_train/rejected': '0.11837', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078722', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-127.94', 'loss/train': '0.66554', 'examples_per_second': '30.642', 'grad_norm': '30.25', 'counters/examples': 277472, 'counters/updates': 8671}
train stats after 277504 examples: {'rewards_train/chosen': '0.16917', 'rewards_train/rejected': '0.037856', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13132', 'logps_train/rejected': '-158.95', 'logps_train/chosen': '-148.8', 'loss/train': '0.65352', 'examples_per_second': '30.383', 'grad_norm': '35.5', 'counters/examples': 277504, 'counters/updates': 8672}
train stats after 277536 examples: {'rewards_train/chosen': '0.13094', 'rewards_train/rejected': '0.04227', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088668', 'logps_train/rejected': '-123.48', 'logps_train/chosen': '-103.52', 'loss/train': '0.6581', 'examples_per_second': '32.288', 'grad_norm': '23.5', 'counters/examples': 277536, 'counters/updates': 8673}
train stats after 277568 examples: {'rewards_train/chosen': '0.27517', 'rewards_train/rejected': '0.11474', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16042', 'logps_train/rejected': '-149.65', 'logps_train/chosen': '-132.76', 'loss/train': '0.63', 'examples_per_second': '30', 'grad_norm': '29.375', 'counters/examples': 277568, 'counters/updates': 8674}
train stats after 277600 examples: {'rewards_train/chosen': '0.2965', 'rewards_train/rejected': '0.13401', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16248', 'logps_train/rejected': '-126.28', 'logps_train/chosen': '-132.03', 'loss/train': '0.63231', 'examples_per_second': '31.481', 'grad_norm': '26.125', 'counters/examples': 277600, 'counters/updates': 8675}
skipping logging after 277632 examples to avoid logging too frequently
train stats after 277664 examples: {'rewards_train/chosen': '0.18936', 'rewards_train/rejected': '0.075992', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11337', 'logps_train/rejected': '-125.25', 'logps_train/chosen': '-112.91', 'loss/train': '0.65453', 'examples_per_second': '30.5', 'grad_norm': '27.5', 'counters/examples': 277664, 'counters/updates': 8677}
train stats after 277696 examples: {'rewards_train/chosen': '0.17881', 'rewards_train/rejected': '0.039971', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13884', 'logps_train/rejected': '-117.51', 'logps_train/chosen': '-134.12', 'loss/train': '0.6407', 'examples_per_second': '32.781', 'grad_norm': '28.375', 'counters/examples': 277696, 'counters/updates': 8678}
skipping logging after 277728 examples to avoid logging too frequently
train stats after 277760 examples: {'rewards_train/chosen': '0.024801', 'rewards_train/rejected': '0.0050231', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.019778', 'logps_train/rejected': '-126.55', 'logps_train/chosen': '-172.67', 'loss/train': '0.69521', 'examples_per_second': '33.764', 'grad_norm': '43.5', 'counters/examples': 277760, 'counters/updates': 8680}
train stats after 277792 examples: {'rewards_train/chosen': '0.18972', 'rewards_train/rejected': '0.00045229', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.18927', 'logps_train/rejected': '-167.01', 'logps_train/chosen': '-166.65', 'loss/train': '0.61506', 'examples_per_second': '31.508', 'grad_norm': '30.625', 'counters/examples': 277792, 'counters/updates': 8681}
train stats after 277824 examples: {'rewards_train/chosen': '0.17694', 'rewards_train/rejected': '-0.0024457', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17939', 'logps_train/rejected': '-118.95', 'logps_train/chosen': '-140.06', 'loss/train': '0.62818', 'examples_per_second': '31.148', 'grad_norm': '29.75', 'counters/examples': 277824, 'counters/updates': 8682}
train stats after 277856 examples: {'rewards_train/chosen': '0.17084', 'rewards_train/rejected': '0.04405', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12679', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-161.14', 'loss/train': '0.64542', 'examples_per_second': '31.296', 'grad_norm': '36.5', 'counters/examples': 277856, 'counters/updates': 8683}
train stats after 277888 examples: {'rewards_train/chosen': '0.18451', 'rewards_train/rejected': '0.070917', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11359', 'logps_train/rejected': '-135.97', 'logps_train/chosen': '-142.58', 'loss/train': '0.64662', 'examples_per_second': '30.727', 'grad_norm': '32.75', 'counters/examples': 277888, 'counters/updates': 8684}
train stats after 277920 examples: {'rewards_train/chosen': '0.18015', 'rewards_train/rejected': '0.073571', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10658', 'logps_train/rejected': '-113.49', 'logps_train/chosen': '-160.33', 'loss/train': '0.65492', 'examples_per_second': '30.834', 'grad_norm': '30.25', 'counters/examples': 277920, 'counters/updates': 8685}
train stats after 277952 examples: {'rewards_train/chosen': '0.21656', 'rewards_train/rejected': '0.037102', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17946', 'logps_train/rejected': '-88.081', 'logps_train/chosen': '-115.96', 'loss/train': '0.62946', 'examples_per_second': '31.495', 'grad_norm': '24.125', 'counters/examples': 277952, 'counters/updates': 8686}
train stats after 277984 examples: {'rewards_train/chosen': '0.11423', 'rewards_train/rejected': '0.0089602', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.10527', 'logps_train/rejected': '-97.19', 'logps_train/chosen': '-134.3', 'loss/train': '0.6502', 'examples_per_second': '30.404', 'grad_norm': '26.25', 'counters/examples': 277984, 'counters/updates': 8687}
train stats after 278016 examples: {'rewards_train/chosen': '0.22264', 'rewards_train/rejected': '0.057007', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16563', 'logps_train/rejected': '-147.31', 'logps_train/chosen': '-157.07', 'loss/train': '0.64577', 'examples_per_second': '31.523', 'grad_norm': '68', 'counters/examples': 278016, 'counters/updates': 8688}
train stats after 278048 examples: {'rewards_train/chosen': '0.065449', 'rewards_train/rejected': '0.052114', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013335', 'logps_train/rejected': '-106.37', 'logps_train/chosen': '-130.87', 'loss/train': '0.69626', 'examples_per_second': '31.503', 'grad_norm': '31.375', 'counters/examples': 278048, 'counters/updates': 8689}
skipping logging after 278080 examples to avoid logging too frequently
train stats after 278112 examples: {'rewards_train/chosen': '0.30242', 'rewards_train/rejected': '-0.025823', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.32825', 'logps_train/rejected': '-115.12', 'logps_train/chosen': '-164.87', 'loss/train': '0.57175', 'examples_per_second': '31.729', 'grad_norm': '26.875', 'counters/examples': 278112, 'counters/updates': 8691}
train stats after 278144 examples: {'rewards_train/chosen': '0.12323', 'rewards_train/rejected': '-0.024091', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.14732', 'logps_train/rejected': '-123.96', 'logps_train/chosen': '-138.33', 'loss/train': '0.64111', 'examples_per_second': '31.514', 'grad_norm': '31.125', 'counters/examples': 278144, 'counters/updates': 8692}
train stats after 278176 examples: {'rewards_train/chosen': '0.20084', 'rewards_train/rejected': '0.12387', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.07697', 'logps_train/rejected': '-127.55', 'logps_train/chosen': '-130.27', 'loss/train': '0.66481', 'examples_per_second': '32.616', 'grad_norm': '32.5', 'counters/examples': 278176, 'counters/updates': 8693}
train stats after 278208 examples: {'rewards_train/chosen': '0.25663', 'rewards_train/rejected': '0.039065', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21756', 'logps_train/rejected': '-153.56', 'logps_train/chosen': '-171.88', 'loss/train': '0.6272', 'examples_per_second': '30.109', 'grad_norm': '57.75', 'counters/examples': 278208, 'counters/updates': 8694}
train stats after 278240 examples: {'rewards_train/chosen': '0.16401', 'rewards_train/rejected': '0.14781', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.016199', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-144.23', 'loss/train': '0.70442', 'examples_per_second': '31.042', 'grad_norm': '33', 'counters/examples': 278240, 'counters/updates': 8695}
train stats after 278272 examples: {'rewards_train/chosen': '0.15393', 'rewards_train/rejected': '0.10235', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.051577', 'logps_train/rejected': '-137.23', 'logps_train/chosen': '-131.74', 'loss/train': '0.67895', 'examples_per_second': '31.485', 'grad_norm': '30.625', 'counters/examples': 278272, 'counters/updates': 8696}
train stats after 278304 examples: {'rewards_train/chosen': '0.20265', 'rewards_train/rejected': '0.069021', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13362', 'logps_train/rejected': '-96.697', 'logps_train/chosen': '-157.87', 'loss/train': '0.64186', 'examples_per_second': '30.876', 'grad_norm': '27', 'counters/examples': 278304, 'counters/updates': 8697}
train stats after 278336 examples: {'rewards_train/chosen': '0.056635', 'rewards_train/rejected': '0.0050022', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.051632', 'logps_train/rejected': '-107.17', 'logps_train/chosen': '-146.56', 'loss/train': '0.67944', 'examples_per_second': '31.831', 'grad_norm': '25', 'counters/examples': 278336, 'counters/updates': 8698}
train stats after 278368 examples: {'rewards_train/chosen': '0.12244', 'rewards_train/rejected': '0.085788', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.036654', 'logps_train/rejected': '-135.78', 'logps_train/chosen': '-149.52', 'loss/train': '0.68677', 'examples_per_second': '31.439', 'grad_norm': '36.75', 'counters/examples': 278368, 'counters/updates': 8699}
train stats after 278400 examples: {'rewards_train/chosen': '0.18152', 'rewards_train/rejected': '0.078304', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10322', 'logps_train/rejected': '-147.15', 'logps_train/chosen': '-154.91', 'loss/train': '0.6693', 'examples_per_second': '31.266', 'grad_norm': '33.25', 'counters/examples': 278400, 'counters/updates': 8700}
train stats after 278432 examples: {'rewards_train/chosen': '0.16305', 'rewards_train/rejected': '-0.035444', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1985', 'logps_train/rejected': '-110.44', 'logps_train/chosen': '-146.5', 'loss/train': '0.62623', 'examples_per_second': '31.789', 'grad_norm': '27', 'counters/examples': 278432, 'counters/updates': 8701}
train stats after 278464 examples: {'rewards_train/chosen': '0.034597', 'rewards_train/rejected': '-0.053134', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.087731', 'logps_train/rejected': '-126.75', 'logps_train/chosen': '-164.22', 'loss/train': '0.66149', 'examples_per_second': '30.554', 'grad_norm': '30.375', 'counters/examples': 278464, 'counters/updates': 8702}
train stats after 278496 examples: {'rewards_train/chosen': '0.21156', 'rewards_train/rejected': '0.0049258', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.20663', 'logps_train/rejected': '-120.59', 'logps_train/chosen': '-151.31', 'loss/train': '0.61678', 'examples_per_second': '31.993', 'grad_norm': '28.25', 'counters/examples': 278496, 'counters/updates': 8703}
train stats after 278528 examples: {'rewards_train/chosen': '0.1883', 'rewards_train/rejected': '0.15899', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.029311', 'logps_train/rejected': '-137.59', 'logps_train/chosen': '-151.47', 'loss/train': '0.69921', 'examples_per_second': '30.451', 'grad_norm': '36', 'counters/examples': 278528, 'counters/updates': 8704}
train stats after 278560 examples: {'rewards_train/chosen': '0.14309', 'rewards_train/rejected': '-0.019288', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16238', 'logps_train/rejected': '-110.97', 'logps_train/chosen': '-125.31', 'loss/train': '0.62648', 'examples_per_second': '30.773', 'grad_norm': '28', 'counters/examples': 278560, 'counters/updates': 8705}
train stats after 278592 examples: {'rewards_train/chosen': '0.16292', 'rewards_train/rejected': '-0.0062027', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16912', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-132.56', 'loss/train': '0.6253', 'examples_per_second': '30.836', 'grad_norm': '31.375', 'counters/examples': 278592, 'counters/updates': 8706}
train stats after 278624 examples: {'rewards_train/chosen': '0.15816', 'rewards_train/rejected': '0.0604', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.097762', 'logps_train/rejected': '-126.08', 'logps_train/chosen': '-130.66', 'loss/train': '0.67013', 'examples_per_second': '32.602', 'grad_norm': '32.25', 'counters/examples': 278624, 'counters/updates': 8707}
train stats after 278656 examples: {'rewards_train/chosen': '0.3577', 'rewards_train/rejected': '0.087889', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.26981', 'logps_train/rejected': '-116.74', 'logps_train/chosen': '-216.24', 'loss/train': '0.63108', 'examples_per_second': '31.383', 'grad_norm': '42.5', 'counters/examples': 278656, 'counters/updates': 8708}
train stats after 278688 examples: {'rewards_train/chosen': '0.19979', 'rewards_train/rejected': '0.11112', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.088675', 'logps_train/rejected': '-142.54', 'logps_train/chosen': '-188.21', 'loss/train': '0.66952', 'examples_per_second': '30.097', 'grad_norm': '50.5', 'counters/examples': 278688, 'counters/updates': 8709}
skipping logging after 278720 examples to avoid logging too frequently
train stats after 278752 examples: {'rewards_train/chosen': '0.11469', 'rewards_train/rejected': '-0.0034134', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1181', 'logps_train/rejected': '-131.8', 'logps_train/chosen': '-136.83', 'loss/train': '0.65073', 'examples_per_second': '31.485', 'grad_norm': '33.75', 'counters/examples': 278752, 'counters/updates': 8711}
train stats after 278784 examples: {'rewards_train/chosen': '0.16747', 'rewards_train/rejected': '0.043232', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12424', 'logps_train/rejected': '-129.64', 'logps_train/chosen': '-138.47', 'loss/train': '0.64939', 'examples_per_second': '30.41', 'grad_norm': '28.875', 'counters/examples': 278784, 'counters/updates': 8712}
train stats after 278816 examples: {'rewards_train/chosen': '0.11246', 'rewards_train/rejected': '-0.066963', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.17942', 'logps_train/rejected': '-104.58', 'logps_train/chosen': '-117.68', 'loss/train': '0.62029', 'examples_per_second': '31.501', 'grad_norm': '23.25', 'counters/examples': 278816, 'counters/updates': 8713}
train stats after 278848 examples: {'rewards_train/chosen': '0.17459', 'rewards_train/rejected': '0.075191', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.099395', 'logps_train/rejected': '-146.21', 'logps_train/chosen': '-146.64', 'loss/train': '0.65956', 'examples_per_second': '31.466', 'grad_norm': '38.5', 'counters/examples': 278848, 'counters/updates': 8714}
train stats after 278880 examples: {'rewards_train/chosen': '0.13458', 'rewards_train/rejected': '0.055845', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.078732', 'logps_train/rejected': '-118.88', 'logps_train/chosen': '-129.77', 'loss/train': '0.67379', 'examples_per_second': '33.005', 'grad_norm': '37.75', 'counters/examples': 278880, 'counters/updates': 8715}
train stats after 278912 examples: {'rewards_train/chosen': '0.12495', 'rewards_train/rejected': '0.036324', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.08863', 'logps_train/rejected': '-113.64', 'logps_train/chosen': '-136.01', 'loss/train': '0.67239', 'examples_per_second': '32.343', 'grad_norm': '37.5', 'counters/examples': 278912, 'counters/updates': 8716}
train stats after 278944 examples: {'rewards_train/chosen': '0.09626', 'rewards_train/rejected': '0.1056', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '-0.0093371', 'logps_train/rejected': '-156.87', 'logps_train/chosen': '-162.68', 'loss/train': '0.7118', 'examples_per_second': '23.111', 'grad_norm': '32.5', 'counters/examples': 278944, 'counters/updates': 8717}
skipping logging after 278976 examples to avoid logging too frequently
train stats after 279008 examples: {'rewards_train/chosen': '0.25793', 'rewards_train/rejected': '0.060485', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19745', 'logps_train/rejected': '-133.68', 'logps_train/chosen': '-133.55', 'loss/train': '0.61956', 'examples_per_second': '31.592', 'grad_norm': '33.5', 'counters/examples': 279008, 'counters/updates': 8719}
train stats after 279040 examples: {'rewards_train/chosen': '0.21049', 'rewards_train/rejected': '0.0089822', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.20151', 'logps_train/rejected': '-111', 'logps_train/chosen': '-138.2', 'loss/train': '0.61427', 'examples_per_second': '24.124', 'grad_norm': '25.125', 'counters/examples': 279040, 'counters/updates': 8720}
skipping logging after 279072 examples to avoid logging too frequently
train stats after 279104 examples: {'rewards_train/chosen': '0.15687', 'rewards_train/rejected': '0.040163', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11671', 'logps_train/rejected': '-156.78', 'logps_train/chosen': '-157.19', 'loss/train': '0.65733', 'examples_per_second': '32.101', 'grad_norm': '27.875', 'counters/examples': 279104, 'counters/updates': 8722}
train stats after 279136 examples: {'rewards_train/chosen': '0.15888', 'rewards_train/rejected': '0.072173', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.086705', 'logps_train/rejected': '-167.48', 'logps_train/chosen': '-167.24', 'loss/train': '0.66889', 'examples_per_second': '31.291', 'grad_norm': '43', 'counters/examples': 279136, 'counters/updates': 8723}
train stats after 279168 examples: {'rewards_train/chosen': '0.15457', 'rewards_train/rejected': '0.092787', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.06178', 'logps_train/rejected': '-127.47', 'logps_train/chosen': '-158.01', 'loss/train': '0.67777', 'examples_per_second': '32.227', 'grad_norm': '31.75', 'counters/examples': 279168, 'counters/updates': 8724}
train stats after 279200 examples: {'rewards_train/chosen': '0.17152', 'rewards_train/rejected': '0.034432', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.13709', 'logps_train/rejected': '-131.95', 'logps_train/chosen': '-149.13', 'loss/train': '0.63815', 'examples_per_second': '29.814', 'grad_norm': '28.875', 'counters/examples': 279200, 'counters/updates': 8725}
train stats after 279232 examples: {'rewards_train/chosen': '0.16742', 'rewards_train/rejected': '0.015502', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15192', 'logps_train/rejected': '-122.01', 'logps_train/chosen': '-162.01', 'loss/train': '0.63491', 'examples_per_second': '31.422', 'grad_norm': '78.5', 'counters/examples': 279232, 'counters/updates': 8726}
train stats after 279264 examples: {'rewards_train/chosen': '0.19798', 'rewards_train/rejected': '0.0022694', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19571', 'logps_train/rejected': '-122.81', 'logps_train/chosen': '-120.88', 'loss/train': '0.6138', 'examples_per_second': '30.915', 'grad_norm': '26.5', 'counters/examples': 279264, 'counters/updates': 8727}
train stats after 279296 examples: {'rewards_train/chosen': '0.053862', 'rewards_train/rejected': '0.043928', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0099337', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-138.12', 'loss/train': '0.69944', 'examples_per_second': '29.985', 'grad_norm': '28', 'counters/examples': 279296, 'counters/updates': 8728}
train stats after 279328 examples: {'rewards_train/chosen': '0.12507', 'rewards_train/rejected': '0.019725', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10535', 'logps_train/rejected': '-95.42', 'logps_train/chosen': '-115.69', 'loss/train': '0.67726', 'examples_per_second': '32.989', 'grad_norm': '41.25', 'counters/examples': 279328, 'counters/updates': 8729}
train stats after 279360 examples: {'rewards_train/chosen': '0.17754', 'rewards_train/rejected': '0.14244', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.035097', 'logps_train/rejected': '-175.08', 'logps_train/chosen': '-170.55', 'loss/train': '0.69363', 'examples_per_second': '31.415', 'grad_norm': '34.75', 'counters/examples': 279360, 'counters/updates': 8730}
train stats after 279392 examples: {'rewards_train/chosen': '0.17254', 'rewards_train/rejected': '-0.009694', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.18224', 'logps_train/rejected': '-143.98', 'logps_train/chosen': '-161.87', 'loss/train': '0.63315', 'examples_per_second': '30.884', 'grad_norm': '34', 'counters/examples': 279392, 'counters/updates': 8731}
skipping logging after 279424 examples to avoid logging too frequently
train stats after 279456 examples: {'rewards_train/chosen': '0.1205', 'rewards_train/rejected': '0.04336', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.077144', 'logps_train/rejected': '-113.19', 'logps_train/chosen': '-134.17', 'loss/train': '0.6662', 'examples_per_second': '30.108', 'grad_norm': '27', 'counters/examples': 279456, 'counters/updates': 8733}
train stats after 279488 examples: {'rewards_train/chosen': '0.25436', 'rewards_train/rejected': '0.097932', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15642', 'logps_train/rejected': '-143.59', 'logps_train/chosen': '-115.77', 'loss/train': '0.65553', 'examples_per_second': '31.514', 'grad_norm': '30.125', 'counters/examples': 279488, 'counters/updates': 8734}
train stats after 279520 examples: {'rewards_train/chosen': '0.14924', 'rewards_train/rejected': '0.0085272', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14072', 'logps_train/rejected': '-117.3', 'logps_train/chosen': '-119.64', 'loss/train': '0.64042', 'examples_per_second': '31.758', 'grad_norm': '34.75', 'counters/examples': 279520, 'counters/updates': 8735}
train stats after 279552 examples: {'rewards_train/chosen': '0.19508', 'rewards_train/rejected': '0.0051126', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18997', 'logps_train/rejected': '-122.37', 'logps_train/chosen': '-141.68', 'loss/train': '0.62051', 'examples_per_second': '32.021', 'grad_norm': '25', 'counters/examples': 279552, 'counters/updates': 8736}
train stats after 279584 examples: {'rewards_train/chosen': '0.12312', 'rewards_train/rejected': '0.084385', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.038732', 'logps_train/rejected': '-143.05', 'logps_train/chosen': '-171.09', 'loss/train': '0.68944', 'examples_per_second': '31.479', 'grad_norm': '37.75', 'counters/examples': 279584, 'counters/updates': 8737}
train stats after 279616 examples: {'rewards_train/chosen': '0.13605', 'rewards_train/rejected': '0.12551', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.010545', 'logps_train/rejected': '-145.22', 'logps_train/chosen': '-188.85', 'loss/train': '0.70355', 'examples_per_second': '31.721', 'grad_norm': '35.75', 'counters/examples': 279616, 'counters/updates': 8738}
train stats after 279648 examples: {'rewards_train/chosen': '0.20458', 'rewards_train/rejected': '0.0026272', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.20195', 'logps_train/rejected': '-148.26', 'logps_train/chosen': '-160.29', 'loss/train': '0.62196', 'examples_per_second': '30.298', 'grad_norm': '27.25', 'counters/examples': 279648, 'counters/updates': 8739}
train stats after 279680 examples: {'rewards_train/chosen': '0.12047', 'rewards_train/rejected': '0.10191', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.018559', 'logps_train/rejected': '-147.23', 'logps_train/chosen': '-133.04', 'loss/train': '0.69799', 'examples_per_second': '31.185', 'grad_norm': '38.75', 'counters/examples': 279680, 'counters/updates': 8740}
train stats after 279712 examples: {'rewards_train/chosen': '0.24829', 'rewards_train/rejected': '0.0024373', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.24585', 'logps_train/rejected': '-108.9', 'logps_train/chosen': '-137.79', 'loss/train': '0.60054', 'examples_per_second': '31.288', 'grad_norm': '37.5', 'counters/examples': 279712, 'counters/updates': 8741}
train stats after 279744 examples: {'rewards_train/chosen': '0.10231', 'rewards_train/rejected': '0.0699', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.032407', 'logps_train/rejected': '-129.74', 'logps_train/chosen': '-193.15', 'loss/train': '0.68891', 'examples_per_second': '31.549', 'grad_norm': '29.25', 'counters/examples': 279744, 'counters/updates': 8742}
skipping logging after 279776 examples to avoid logging too frequently
train stats after 279808 examples: {'rewards_train/chosen': '0.14307', 'rewards_train/rejected': '-0.047744', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.19082', 'logps_train/rejected': '-126.97', 'logps_train/chosen': '-121.26', 'loss/train': '0.6207', 'examples_per_second': '30.139', 'grad_norm': '22.25', 'counters/examples': 279808, 'counters/updates': 8744}
skipping logging after 279840 examples to avoid logging too frequently
train stats after 279872 examples: {'rewards_train/chosen': '0.11709', 'rewards_train/rejected': '0.010636', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10645', 'logps_train/rejected': '-91.924', 'logps_train/chosen': '-105.75', 'loss/train': '0.65326', 'examples_per_second': '31.179', 'grad_norm': '27.875', 'counters/examples': 279872, 'counters/updates': 8746}
skipping logging after 279904 examples to avoid logging too frequently
train stats after 279936 examples: {'rewards_train/chosen': '0.17603', 'rewards_train/rejected': '0.14283', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.033208', 'logps_train/rejected': '-101.3', 'logps_train/chosen': '-133.36', 'loss/train': '0.68582', 'examples_per_second': '31.293', 'grad_norm': '45.75', 'counters/examples': 279936, 'counters/updates': 8748}
train stats after 279968 examples: {'rewards_train/chosen': '0.21184', 'rewards_train/rejected': '0.10853', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10331', 'logps_train/rejected': '-143.4', 'logps_train/chosen': '-169.53', 'loss/train': '0.65863', 'examples_per_second': '31.477', 'grad_norm': '32.75', 'counters/examples': 279968, 'counters/updates': 8749}
train stats after 280000 examples: {'rewards_train/chosen': '0.24468', 'rewards_train/rejected': '0.1016', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.14307', 'logps_train/rejected': '-118.74', 'logps_train/chosen': '-136.15', 'loss/train': '0.64633', 'examples_per_second': '30.619', 'grad_norm': '32.5', 'counters/examples': 280000, 'counters/updates': 8750}
train stats after 280032 examples: {'rewards_train/chosen': '0.18648', 'rewards_train/rejected': '0.082854', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.10363', 'logps_train/rejected': '-134.34', 'logps_train/chosen': '-111.06', 'loss/train': '0.65484', 'examples_per_second': '30.993', 'grad_norm': '31.875', 'counters/examples': 280032, 'counters/updates': 8751}
train stats after 280064 examples: {'rewards_train/chosen': '0.068934', 'rewards_train/rejected': '0.065686', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.0032475', 'logps_train/rejected': '-138.67', 'logps_train/chosen': '-137.96', 'loss/train': '0.70149', 'examples_per_second': '32.209', 'grad_norm': '38', 'counters/examples': 280064, 'counters/updates': 8752}
train stats after 280096 examples: {'rewards_train/chosen': '0.15014', 'rewards_train/rejected': '0.045574', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10456', 'logps_train/rejected': '-133.65', 'logps_train/chosen': '-124.55', 'loss/train': '0.65583', 'examples_per_second': '31.455', 'grad_norm': '43', 'counters/examples': 280096, 'counters/updates': 8753}
train stats after 280128 examples: {'rewards_train/chosen': '0.15973', 'rewards_train/rejected': '0.034656', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12507', 'logps_train/rejected': '-155.63', 'logps_train/chosen': '-111.11', 'loss/train': '0.6398', 'examples_per_second': '31.356', 'grad_norm': '26.5', 'counters/examples': 280128, 'counters/updates': 8754}
train stats after 280160 examples: {'rewards_train/chosen': '0.22049', 'rewards_train/rejected': '0.036753', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18374', 'logps_train/rejected': '-93.899', 'logps_train/chosen': '-128.31', 'loss/train': '0.62259', 'examples_per_second': '32.419', 'grad_norm': '27.25', 'counters/examples': 280160, 'counters/updates': 8755}
train stats after 280192 examples: {'rewards_train/chosen': '0.13716', 'rewards_train/rejected': '0.021694', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11546', 'logps_train/rejected': '-148.08', 'logps_train/chosen': '-164.14', 'loss/train': '0.65266', 'examples_per_second': '30.053', 'grad_norm': '31.875', 'counters/examples': 280192, 'counters/updates': 8756}
train stats after 280224 examples: {'rewards_train/chosen': '0.10551', 'rewards_train/rejected': '0.038578', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.066934', 'logps_train/rejected': '-119.22', 'logps_train/chosen': '-126.28', 'loss/train': '0.67505', 'examples_per_second': '24.696', 'grad_norm': '26.25', 'counters/examples': 280224, 'counters/updates': 8757}
train stats after 280256 examples: {'rewards_train/chosen': '0.18932', 'rewards_train/rejected': '0.0046044', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18472', 'logps_train/rejected': '-135.77', 'logps_train/chosen': '-164.8', 'loss/train': '0.62277', 'examples_per_second': '32.099', 'grad_norm': '27.75', 'counters/examples': 280256, 'counters/updates': 8758}
train stats after 280288 examples: {'rewards_train/chosen': '0.27052', 'rewards_train/rejected': '0.082943', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18757', 'logps_train/rejected': '-148.7', 'logps_train/chosen': '-195.17', 'loss/train': '0.62812', 'examples_per_second': '30.585', 'grad_norm': '28.625', 'counters/examples': 280288, 'counters/updates': 8759}
train stats after 280320 examples: {'rewards_train/chosen': '0.20075', 'rewards_train/rejected': '0.16417', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.03658', 'logps_train/rejected': '-168.1', 'logps_train/chosen': '-178.93', 'loss/train': '0.71035', 'examples_per_second': '30.81', 'grad_norm': '31.25', 'counters/examples': 280320, 'counters/updates': 8760}
train stats after 280352 examples: {'rewards_train/chosen': '0.15464', 'rewards_train/rejected': '0.20501', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '-0.050361', 'logps_train/rejected': '-151.36', 'logps_train/chosen': '-150.59', 'loss/train': '0.73257', 'examples_per_second': '30.706', 'grad_norm': '40.5', 'counters/examples': 280352, 'counters/updates': 8761}
train stats after 280384 examples: {'rewards_train/chosen': '0.13345', 'rewards_train/rejected': '0.037054', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.096396', 'logps_train/rejected': '-103.23', 'logps_train/chosen': '-132.55', 'loss/train': '0.65854', 'examples_per_second': '30.397', 'grad_norm': '28.25', 'counters/examples': 280384, 'counters/updates': 8762}
train stats after 280416 examples: {'rewards_train/chosen': '0.20431', 'rewards_train/rejected': '0.11892', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.085384', 'logps_train/rejected': '-166.2', 'logps_train/chosen': '-158.1', 'loss/train': '0.66518', 'examples_per_second': '32.6', 'grad_norm': '34', 'counters/examples': 280416, 'counters/updates': 8763}
skipping logging after 280448 examples to avoid logging too frequently
train stats after 280480 examples: {'rewards_train/chosen': '0.15586', 'rewards_train/rejected': '0.10002', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.055838', 'logps_train/rejected': '-105.78', 'logps_train/chosen': '-114.78', 'loss/train': '0.67397', 'examples_per_second': '35.852', 'grad_norm': '25.625', 'counters/examples': 280480, 'counters/updates': 8765}
skipping logging after 280512 examples to avoid logging too frequently
train stats after 280544 examples: {'rewards_train/chosen': '0.27635', 'rewards_train/rejected': '0.062923', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.21342', 'logps_train/rejected': '-134.05', 'logps_train/chosen': '-172.83', 'loss/train': '0.61715', 'examples_per_second': '32.248', 'grad_norm': '38', 'counters/examples': 280544, 'counters/updates': 8767}
train stats after 280576 examples: {'rewards_train/chosen': '0.21456', 'rewards_train/rejected': '0.11421', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.10034', 'logps_train/rejected': '-150.29', 'logps_train/chosen': '-154.47', 'loss/train': '0.66424', 'examples_per_second': '29.955', 'grad_norm': '66', 'counters/examples': 280576, 'counters/updates': 8768}
train stats after 280608 examples: {'rewards_train/chosen': '0.19193', 'rewards_train/rejected': '0.055865', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13606', 'logps_train/rejected': '-147', 'logps_train/chosen': '-160.35', 'loss/train': '0.64111', 'examples_per_second': '31.412', 'grad_norm': '37', 'counters/examples': 280608, 'counters/updates': 8769}
train stats after 280640 examples: {'rewards_train/chosen': '0.14106', 'rewards_train/rejected': '0.0099202', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13114', 'logps_train/rejected': '-150.78', 'logps_train/chosen': '-183.3', 'loss/train': '0.65096', 'examples_per_second': '30.65', 'grad_norm': '35', 'counters/examples': 280640, 'counters/updates': 8770}
train stats after 280672 examples: {'rewards_train/chosen': '0.077795', 'rewards_train/rejected': '0.066968', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.010827', 'logps_train/rejected': '-158.62', 'logps_train/chosen': '-180.08', 'loss/train': '0.71536', 'examples_per_second': '30.035', 'grad_norm': '40', 'counters/examples': 280672, 'counters/updates': 8771}
train stats after 280704 examples: {'rewards_train/chosen': '0.15272', 'rewards_train/rejected': '0.042531', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.11019', 'logps_train/rejected': '-153.7', 'logps_train/chosen': '-146.19', 'loss/train': '0.6592', 'examples_per_second': '32.857', 'grad_norm': '32.75', 'counters/examples': 280704, 'counters/updates': 8772}
train stats after 280736 examples: {'rewards_train/chosen': '0.25596', 'rewards_train/rejected': '0.092569', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16339', 'logps_train/rejected': '-145.45', 'logps_train/chosen': '-152.02', 'loss/train': '0.62632', 'examples_per_second': '32.302', 'grad_norm': '42.75', 'counters/examples': 280736, 'counters/updates': 8773}
train stats after 280768 examples: {'rewards_train/chosen': '0.14425', 'rewards_train/rejected': '0.042053', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1022', 'logps_train/rejected': '-104.07', 'logps_train/chosen': '-124.65', 'loss/train': '0.65881', 'examples_per_second': '32.376', 'grad_norm': '28.875', 'counters/examples': 280768, 'counters/updates': 8774}
skipping logging after 280800 examples to avoid logging too frequently
train stats after 280832 examples: {'rewards_train/chosen': '0.09764', 'rewards_train/rejected': '-0.014725', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.11237', 'logps_train/rejected': '-97.888', 'logps_train/chosen': '-120.04', 'loss/train': '0.65489', 'examples_per_second': '30.946', 'grad_norm': '26.625', 'counters/examples': 280832, 'counters/updates': 8776}
train stats after 280864 examples: {'rewards_train/chosen': '0.17161', 'rewards_train/rejected': '0.072371', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.099236', 'logps_train/rejected': '-135.81', 'logps_train/chosen': '-99.608', 'loss/train': '0.66413', 'examples_per_second': '31.981', 'grad_norm': '28.375', 'counters/examples': 280864, 'counters/updates': 8777}
train stats after 280896 examples: {'rewards_train/chosen': '0.23449', 'rewards_train/rejected': '0.017176', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.21731', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-165.53', 'loss/train': '0.60918', 'examples_per_second': '31.42', 'grad_norm': '32.75', 'counters/examples': 280896, 'counters/updates': 8778}
train stats after 280928 examples: {'rewards_train/chosen': '0.12826', 'rewards_train/rejected': '0.068467', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.059791', 'logps_train/rejected': '-106.22', 'logps_train/chosen': '-110.44', 'loss/train': '0.67597', 'examples_per_second': '30.67', 'grad_norm': '29.125', 'counters/examples': 280928, 'counters/updates': 8779}
skipping logging after 280960 examples to avoid logging too frequently
train stats after 280992 examples: {'rewards_train/chosen': '0.12776', 'rewards_train/rejected': '-0.054811', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18257', 'logps_train/rejected': '-124.86', 'logps_train/chosen': '-140.08', 'loss/train': '0.62388', 'examples_per_second': '30.041', 'grad_norm': '29.25', 'counters/examples': 280992, 'counters/updates': 8781}
train stats after 281024 examples: {'rewards_train/chosen': '0.18448', 'rewards_train/rejected': '0.067914', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11657', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-127.75', 'loss/train': '0.65012', 'examples_per_second': '30.676', 'grad_norm': '36.25', 'counters/examples': 281024, 'counters/updates': 8782}
train stats after 281056 examples: {'rewards_train/chosen': '0.32095', 'rewards_train/rejected': '0.12915', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.19181', 'logps_train/rejected': '-119.64', 'logps_train/chosen': '-156.56', 'loss/train': '0.61041', 'examples_per_second': '32.711', 'grad_norm': '31', 'counters/examples': 281056, 'counters/updates': 8783}
train stats after 281088 examples: {'rewards_train/chosen': '0.11252', 'rewards_train/rejected': '0.10796', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.0045531', 'logps_train/rejected': '-131.63', 'logps_train/chosen': '-187.62', 'loss/train': '0.71721', 'examples_per_second': '31.445', 'grad_norm': '69.5', 'counters/examples': 281088, 'counters/updates': 8784}
train stats after 281120 examples: {'rewards_train/chosen': '0.13419', 'rewards_train/rejected': '0.026919', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.10727', 'logps_train/rejected': '-119.07', 'logps_train/chosen': '-151.11', 'loss/train': '0.6525', 'examples_per_second': '31.044', 'grad_norm': '36.25', 'counters/examples': 281120, 'counters/updates': 8785}
train stats after 281152 examples: {'rewards_train/chosen': '0.18274', 'rewards_train/rejected': '0.11319', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.069553', 'logps_train/rejected': '-124.93', 'logps_train/chosen': '-128.93', 'loss/train': '0.673', 'examples_per_second': '30.738', 'grad_norm': '45.25', 'counters/examples': 281152, 'counters/updates': 8786}
train stats after 281184 examples: {'rewards_train/chosen': '0.12016', 'rewards_train/rejected': '0.01893', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.10123', 'logps_train/rejected': '-138.72', 'logps_train/chosen': '-163.23', 'loss/train': '0.65492', 'examples_per_second': '30.684', 'grad_norm': '28.375', 'counters/examples': 281184, 'counters/updates': 8787}
train stats after 281216 examples: {'rewards_train/chosen': '0.29025', 'rewards_train/rejected': '0.018005', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.27224', 'logps_train/rejected': '-139.11', 'logps_train/chosen': '-143.17', 'loss/train': '0.59217', 'examples_per_second': '29.917', 'grad_norm': '28.125', 'counters/examples': 281216, 'counters/updates': 8788}
skipping logging after 281248 examples to avoid logging too frequently
train stats after 281280 examples: {'rewards_train/chosen': '0.16924', 'rewards_train/rejected': '0.0034474', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16579', 'logps_train/rejected': '-126.58', 'logps_train/chosen': '-151.31', 'loss/train': '0.62743', 'examples_per_second': '33.823', 'grad_norm': '27.375', 'counters/examples': 281280, 'counters/updates': 8790}
train stats after 281312 examples: {'rewards_train/chosen': '0.17274', 'rewards_train/rejected': '0.066157', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.10658', 'logps_train/rejected': '-160.95', 'logps_train/chosen': '-164.74', 'loss/train': '0.65115', 'examples_per_second': '32.634', 'grad_norm': '32', 'counters/examples': 281312, 'counters/updates': 8791}
train stats after 281344 examples: {'rewards_train/chosen': '0.20241', 'rewards_train/rejected': '0.068642', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13377', 'logps_train/rejected': '-136.02', 'logps_train/chosen': '-159.37', 'loss/train': '0.6402', 'examples_per_second': '30.056', 'grad_norm': '36.75', 'counters/examples': 281344, 'counters/updates': 8792}
train stats after 281376 examples: {'rewards_train/chosen': '0.15167', 'rewards_train/rejected': '0.060775', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0909', 'logps_train/rejected': '-139.33', 'logps_train/chosen': '-191', 'loss/train': '0.66708', 'examples_per_second': '30.13', 'grad_norm': '57.75', 'counters/examples': 281376, 'counters/updates': 8793}
skipping logging after 281408 examples to avoid logging too frequently
train stats after 281440 examples: {'rewards_train/chosen': '0.13792', 'rewards_train/rejected': '0.0029803', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13494', 'logps_train/rejected': '-122.53', 'logps_train/chosen': '-135.56', 'loss/train': '0.64214', 'examples_per_second': '35.613', 'grad_norm': '24.875', 'counters/examples': 281440, 'counters/updates': 8795}
train stats after 281472 examples: {'rewards_train/chosen': '0.21681', 'rewards_train/rejected': '-0.0024661', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.21928', 'logps_train/rejected': '-173.72', 'logps_train/chosen': '-155.31', 'loss/train': '0.60781', 'examples_per_second': '30.191', 'grad_norm': '31.5', 'counters/examples': 281472, 'counters/updates': 8796}
train stats after 281504 examples: {'rewards_train/chosen': '0.092319', 'rewards_train/rejected': '-0.073504', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16582', 'logps_train/rejected': '-118.57', 'logps_train/chosen': '-143.82', 'loss/train': '0.62872', 'examples_per_second': '31.413', 'grad_norm': '26.625', 'counters/examples': 281504, 'counters/updates': 8797}
skipping logging after 281536 examples to avoid logging too frequently
train stats after 281568 examples: {'rewards_train/chosen': '0.11065', 'rewards_train/rejected': '0.097592', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.013059', 'logps_train/rejected': '-132.53', 'logps_train/chosen': '-162.03', 'loss/train': '0.70056', 'examples_per_second': '32.499', 'grad_norm': '28.375', 'counters/examples': 281568, 'counters/updates': 8799}
train stats after 281600 examples: {'rewards_train/chosen': '0.11191', 'rewards_train/rejected': '-0.011701', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12361', 'logps_train/rejected': '-130.81', 'logps_train/chosen': '-142.26', 'loss/train': '0.64357', 'examples_per_second': '32.076', 'grad_norm': '35.75', 'counters/examples': 281600, 'counters/updates': 8800}
train stats after 281632 examples: {'rewards_train/chosen': '0.20029', 'rewards_train/rejected': '0.072037', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.12825', 'logps_train/rejected': '-166.3', 'logps_train/chosen': '-207.19', 'loss/train': '0.64573', 'examples_per_second': '32.201', 'grad_norm': '46.5', 'counters/examples': 281632, 'counters/updates': 8801}
skipping logging after 281664 examples to avoid logging too frequently
train stats after 281696 examples: {'rewards_train/chosen': '0.16983', 'rewards_train/rejected': '0.037941', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13189', 'logps_train/rejected': '-115.64', 'logps_train/chosen': '-149.87', 'loss/train': '0.64075', 'examples_per_second': '31.576', 'grad_norm': '52.5', 'counters/examples': 281696, 'counters/updates': 8803}
train stats after 281728 examples: {'rewards_train/chosen': '0.21926', 'rewards_train/rejected': '0.048724', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.17054', 'logps_train/rejected': '-123.3', 'logps_train/chosen': '-148.9', 'loss/train': '0.62994', 'examples_per_second': '30.147', 'grad_norm': '30.25', 'counters/examples': 281728, 'counters/updates': 8804}
train stats after 281760 examples: {'rewards_train/chosen': '0.11571', 'rewards_train/rejected': '0.099615', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.016095', 'logps_train/rejected': '-123.15', 'logps_train/chosen': '-112.78', 'loss/train': '0.70073', 'examples_per_second': '30.492', 'grad_norm': '39.75', 'counters/examples': 281760, 'counters/updates': 8805}
train stats after 281792 examples: {'rewards_train/chosen': '0.048781', 'rewards_train/rejected': '-0.012755', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.061537', 'logps_train/rejected': '-122.84', 'logps_train/chosen': '-155.29', 'loss/train': '0.68317', 'examples_per_second': '31.549', 'grad_norm': '36.5', 'counters/examples': 281792, 'counters/updates': 8806}
train stats after 281824 examples: {'rewards_train/chosen': '0.28045', 'rewards_train/rejected': '0.1369', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.14355', 'logps_train/rejected': '-119.1', 'logps_train/chosen': '-153.83', 'loss/train': '0.64139', 'examples_per_second': '31.88', 'grad_norm': '30.25', 'counters/examples': 281824, 'counters/updates': 8807}
train stats after 281856 examples: {'rewards_train/chosen': '0.23008', 'rewards_train/rejected': '0.080026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15006', 'logps_train/rejected': '-146.62', 'logps_train/chosen': '-164.26', 'loss/train': '0.63482', 'examples_per_second': '31.34', 'grad_norm': '31.625', 'counters/examples': 281856, 'counters/updates': 8808}
train stats after 281888 examples: {'rewards_train/chosen': '0.19338', 'rewards_train/rejected': '0.031252', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16212', 'logps_train/rejected': '-118.99', 'logps_train/chosen': '-146.4', 'loss/train': '0.64103', 'examples_per_second': '32.001', 'grad_norm': '28.25', 'counters/examples': 281888, 'counters/updates': 8809}
train stats after 281920 examples: {'rewards_train/chosen': '0.27472', 'rewards_train/rejected': '0.097997', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.17673', 'logps_train/rejected': '-136.57', 'logps_train/chosen': '-162.11', 'loss/train': '0.63172', 'examples_per_second': '31.512', 'grad_norm': '30.5', 'counters/examples': 281920, 'counters/updates': 8810}
train stats after 281952 examples: {'rewards_train/chosen': '0.19345', 'rewards_train/rejected': '0.036147', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1573', 'logps_train/rejected': '-127.35', 'logps_train/chosen': '-112.71', 'loss/train': '0.63454', 'examples_per_second': '32.034', 'grad_norm': '23.625', 'counters/examples': 281952, 'counters/updates': 8811}
train stats after 281984 examples: {'rewards_train/chosen': '0.10044', 'rewards_train/rejected': '0.062923', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.037517', 'logps_train/rejected': '-110.42', 'logps_train/chosen': '-137.95', 'loss/train': '0.68662', 'examples_per_second': '30.503', 'grad_norm': '29.125', 'counters/examples': 281984, 'counters/updates': 8812}
skipping logging after 282016 examples to avoid logging too frequently
train stats after 282048 examples: {'rewards_train/chosen': '0.21585', 'rewards_train/rejected': '0.084139', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13171', 'logps_train/rejected': '-110', 'logps_train/chosen': '-146.77', 'loss/train': '0.63974', 'examples_per_second': '30.188', 'grad_norm': '29.25', 'counters/examples': 282048, 'counters/updates': 8814}
train stats after 282080 examples: {'rewards_train/chosen': '0.20214', 'rewards_train/rejected': '0.0017408', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.2004', 'logps_train/rejected': '-122.82', 'logps_train/chosen': '-135.61', 'loss/train': '0.60912', 'examples_per_second': '31.507', 'grad_norm': '26.25', 'counters/examples': 282080, 'counters/updates': 8815}
train stats after 282112 examples: {'rewards_train/chosen': '0.31445', 'rewards_train/rejected': '0.012884', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.30157', 'logps_train/rejected': '-111.3', 'logps_train/chosen': '-157.03', 'loss/train': '0.5813', 'examples_per_second': '30.52', 'grad_norm': '28.375', 'counters/examples': 282112, 'counters/updates': 8816}
train stats after 282144 examples: {'rewards_train/chosen': '0.27457', 'rewards_train/rejected': '-0.071191', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.34576', 'logps_train/rejected': '-116.05', 'logps_train/chosen': '-166.1', 'loss/train': '0.55597', 'examples_per_second': '31.446', 'grad_norm': '25.375', 'counters/examples': 282144, 'counters/updates': 8817}
train stats after 282176 examples: {'rewards_train/chosen': '0.28403', 'rewards_train/rejected': '0.023997', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26003', 'logps_train/rejected': '-132.05', 'logps_train/chosen': '-150.89', 'loss/train': '0.59174', 'examples_per_second': '31.529', 'grad_norm': '33', 'counters/examples': 282176, 'counters/updates': 8818}
train stats after 282208 examples: {'rewards_train/chosen': '0.17476', 'rewards_train/rejected': '0.012431', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16233', 'logps_train/rejected': '-120.51', 'logps_train/chosen': '-103.85', 'loss/train': '0.63552', 'examples_per_second': '31.315', 'grad_norm': '24.75', 'counters/examples': 282208, 'counters/updates': 8819}
skipping logging after 282240 examples to avoid logging too frequently
train stats after 282272 examples: {'rewards_train/chosen': '0.14824', 'rewards_train/rejected': '-0.010076', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15831', 'logps_train/rejected': '-142.28', 'logps_train/chosen': '-140.42', 'loss/train': '0.62533', 'examples_per_second': '31.525', 'grad_norm': '26.875', 'counters/examples': 282272, 'counters/updates': 8821}
skipping logging after 282304 examples to avoid logging too frequently
train stats after 282336 examples: {'rewards_train/chosen': '0.2759', 'rewards_train/rejected': '0.11663', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.15927', 'logps_train/rejected': '-124.44', 'logps_train/chosen': '-148.84', 'loss/train': '0.64083', 'examples_per_second': '31.648', 'grad_norm': '30.125', 'counters/examples': 282336, 'counters/updates': 8823}
train stats after 282368 examples: {'rewards_train/chosen': '0.12635', 'rewards_train/rejected': '0.034195', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.092152', 'logps_train/rejected': '-155.05', 'logps_train/chosen': '-133.69', 'loss/train': '0.66198', 'examples_per_second': '32.901', 'grad_norm': '28.625', 'counters/examples': 282368, 'counters/updates': 8824}
train stats after 282400 examples: {'rewards_train/chosen': '0.12486', 'rewards_train/rejected': '0.069941', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.054924', 'logps_train/rejected': '-124.91', 'logps_train/chosen': '-120.64', 'loss/train': '0.67643', 'examples_per_second': '30.368', 'grad_norm': '38.25', 'counters/examples': 282400, 'counters/updates': 8825}
train stats after 282432 examples: {'rewards_train/chosen': '0.12306', 'rewards_train/rejected': '0.095596', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.027462', 'logps_train/rejected': '-130.44', 'logps_train/chosen': '-156.31', 'loss/train': '0.69821', 'examples_per_second': '32.421', 'grad_norm': '41', 'counters/examples': 282432, 'counters/updates': 8826}
train stats after 282464 examples: {'rewards_train/chosen': '0.15566', 'rewards_train/rejected': '-0.0005504', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15621', 'logps_train/rejected': '-117.14', 'logps_train/chosen': '-147.43', 'loss/train': '0.63222', 'examples_per_second': '30.639', 'grad_norm': '25.875', 'counters/examples': 282464, 'counters/updates': 8827}
train stats after 282496 examples: {'rewards_train/chosen': '0.14454', 'rewards_train/rejected': '0.092498', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.052047', 'logps_train/rejected': '-136.77', 'logps_train/chosen': '-171.85', 'loss/train': '0.69084', 'examples_per_second': '30.532', 'grad_norm': '41.25', 'counters/examples': 282496, 'counters/updates': 8828}
skipping logging after 282528 examples to avoid logging too frequently
train stats after 282560 examples: {'rewards_train/chosen': '0.16381', 'rewards_train/rejected': '-0.018311', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18212', 'logps_train/rejected': '-144.08', 'logps_train/chosen': '-174.95', 'loss/train': '0.62327', 'examples_per_second': '31.025', 'grad_norm': '40', 'counters/examples': 282560, 'counters/updates': 8830}
train stats after 282592 examples: {'rewards_train/chosen': '0.19011', 'rewards_train/rejected': '-0.01741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.20752', 'logps_train/rejected': '-148.03', 'logps_train/chosen': '-179.55', 'loss/train': '0.61464', 'examples_per_second': '30.316', 'grad_norm': '35.5', 'counters/examples': 282592, 'counters/updates': 8831}
train stats after 282624 examples: {'rewards_train/chosen': '0.28649', 'rewards_train/rejected': '0.31752', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.031031', 'logps_train/rejected': '-176.92', 'logps_train/chosen': '-156.58', 'loss/train': '0.72644', 'examples_per_second': '30.451', 'grad_norm': '57.5', 'counters/examples': 282624, 'counters/updates': 8832}
train stats after 282656 examples: {'rewards_train/chosen': '0.19651', 'rewards_train/rejected': '-0.063394', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.25991', 'logps_train/rejected': '-121.3', 'logps_train/chosen': '-165.29', 'loss/train': '0.58725', 'examples_per_second': '31.522', 'grad_norm': '32.5', 'counters/examples': 282656, 'counters/updates': 8833}
train stats after 282688 examples: {'rewards_train/chosen': '0.17601', 'rewards_train/rejected': '-0.045262', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22127', 'logps_train/rejected': '-110.57', 'logps_train/chosen': '-130.67', 'loss/train': '0.60784', 'examples_per_second': '29.826', 'grad_norm': '25.375', 'counters/examples': 282688, 'counters/updates': 8834}
skipping logging after 282720 examples to avoid logging too frequently
train stats after 282752 examples: {'rewards_train/chosen': '0.26448', 'rewards_train/rejected': '0.042601', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.22188', 'logps_train/rejected': '-170.24', 'logps_train/chosen': '-164.34', 'loss/train': '0.607', 'examples_per_second': '30.477', 'grad_norm': '38', 'counters/examples': 282752, 'counters/updates': 8836}
skipping logging after 282784 examples to avoid logging too frequently
train stats after 282816 examples: {'rewards_train/chosen': '0.1867', 'rewards_train/rejected': '0.029213', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15749', 'logps_train/rejected': '-116.32', 'logps_train/chosen': '-167.7', 'loss/train': '0.63153', 'examples_per_second': '31.763', 'grad_norm': '35.75', 'counters/examples': 282816, 'counters/updates': 8838}
train stats after 282848 examples: {'rewards_train/chosen': '0.1874', 'rewards_train/rejected': '0.05741', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12999', 'logps_train/rejected': '-130.52', 'logps_train/chosen': '-152.37', 'loss/train': '0.6404', 'examples_per_second': '32.429', 'grad_norm': '31.25', 'counters/examples': 282848, 'counters/updates': 8839}
train stats after 282880 examples: {'rewards_train/chosen': '0.15544', 'rewards_train/rejected': '0.032298', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12315', 'logps_train/rejected': '-135.46', 'logps_train/chosen': '-188.74', 'loss/train': '0.64645', 'examples_per_second': '31.545', 'grad_norm': '33', 'counters/examples': 282880, 'counters/updates': 8840}
train stats after 282912 examples: {'rewards_train/chosen': '0.19984', 'rewards_train/rejected': '0.030483', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16936', 'logps_train/rejected': '-133.16', 'logps_train/chosen': '-163.41', 'loss/train': '0.63113', 'examples_per_second': '30.851', 'grad_norm': '42', 'counters/examples': 282912, 'counters/updates': 8841}
train stats after 282944 examples: {'rewards_train/chosen': '0.14289', 'rewards_train/rejected': '0.024585', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1183', 'logps_train/rejected': '-114.05', 'logps_train/chosen': '-128.9', 'loss/train': '0.64679', 'examples_per_second': '29.904', 'grad_norm': '28.5', 'counters/examples': 282944, 'counters/updates': 8842}
train stats after 282976 examples: {'rewards_train/chosen': '0.22839', 'rewards_train/rejected': '0.045692', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1827', 'logps_train/rejected': '-125.82', 'logps_train/chosen': '-148.3', 'loss/train': '0.61861', 'examples_per_second': '32.464', 'grad_norm': '23.75', 'counters/examples': 282976, 'counters/updates': 8843}
train stats after 283008 examples: {'rewards_train/chosen': '0.18269', 'rewards_train/rejected': '0.096247', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.086443', 'logps_train/rejected': '-131.67', 'logps_train/chosen': '-140.34', 'loss/train': '0.67738', 'examples_per_second': '32.799', 'grad_norm': '44.75', 'counters/examples': 283008, 'counters/updates': 8844}
train stats after 283040 examples: {'rewards_train/chosen': '0.11241', 'rewards_train/rejected': '0.031723', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.080683', 'logps_train/rejected': '-115.94', 'logps_train/chosen': '-131.11', 'loss/train': '0.66871', 'examples_per_second': '32.402', 'grad_norm': '31.25', 'counters/examples': 283040, 'counters/updates': 8845}
train stats after 283072 examples: {'rewards_train/chosen': '0.06855', 'rewards_train/rejected': '-0.03695', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1055', 'logps_train/rejected': '-83.269', 'logps_train/chosen': '-101.38', 'loss/train': '0.64911', 'examples_per_second': '30.557', 'grad_norm': '21.375', 'counters/examples': 283072, 'counters/updates': 8846}
train stats after 283104 examples: {'rewards_train/chosen': '0.17204', 'rewards_train/rejected': '0.111', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.061043', 'logps_train/rejected': '-158.18', 'logps_train/chosen': '-136.08', 'loss/train': '0.67643', 'examples_per_second': '31.457', 'grad_norm': '29.75', 'counters/examples': 283104, 'counters/updates': 8847}
train stats after 283136 examples: {'rewards_train/chosen': '0.11215', 'rewards_train/rejected': '-0.074532', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18668', 'logps_train/rejected': '-114.69', 'logps_train/chosen': '-156.31', 'loss/train': '0.62317', 'examples_per_second': '30.483', 'grad_norm': '31', 'counters/examples': 283136, 'counters/updates': 8848}
train stats after 283168 examples: {'rewards_train/chosen': '0.19914', 'rewards_train/rejected': '0.065904', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13324', 'logps_train/rejected': '-141.12', 'logps_train/chosen': '-143.87', 'loss/train': '0.6467', 'examples_per_second': '31.624', 'grad_norm': '30.75', 'counters/examples': 283168, 'counters/updates': 8849}
train stats after 283200 examples: {'rewards_train/chosen': '0.045538', 'rewards_train/rejected': '-0.026478', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.072016', 'logps_train/rejected': '-102.45', 'logps_train/chosen': '-158.68', 'loss/train': '0.67985', 'examples_per_second': '31.288', 'grad_norm': '32.5', 'counters/examples': 283200, 'counters/updates': 8850}
train stats after 283232 examples: {'rewards_train/chosen': '0.11275', 'rewards_train/rejected': '0.034929', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.077822', 'logps_train/rejected': '-134.9', 'logps_train/chosen': '-159.58', 'loss/train': '0.66193', 'examples_per_second': '31.955', 'grad_norm': '39.5', 'counters/examples': 283232, 'counters/updates': 8851}
train stats after 283264 examples: {'rewards_train/chosen': '0.18227', 'rewards_train/rejected': '0.093261', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.089004', 'logps_train/rejected': '-155.15', 'logps_train/chosen': '-154.58', 'loss/train': '0.6744', 'examples_per_second': '30.915', 'grad_norm': '37', 'counters/examples': 283264, 'counters/updates': 8852}
train stats after 283296 examples: {'rewards_train/chosen': '0.1365', 'rewards_train/rejected': '0.0091535', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.12735', 'logps_train/rejected': '-117.26', 'logps_train/chosen': '-144.63', 'loss/train': '0.64451', 'examples_per_second': '29.856', 'grad_norm': '28.375', 'counters/examples': 283296, 'counters/updates': 8853}
train stats after 283328 examples: {'rewards_train/chosen': '0.23502', 'rewards_train/rejected': '0.038187', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19683', 'logps_train/rejected': '-103.06', 'logps_train/chosen': '-125.49', 'loss/train': '0.61419', 'examples_per_second': '30.904', 'grad_norm': '29.375', 'counters/examples': 283328, 'counters/updates': 8854}
train stats after 283360 examples: {'rewards_train/chosen': '0.12538', 'rewards_train/rejected': '0.18641', 'rewards_train/accuracies': '0.375', 'rewards_train/margins': '-0.061025', 'logps_train/rejected': '-136.08', 'logps_train/chosen': '-142.61', 'loss/train': '0.7549', 'examples_per_second': '30.482', 'grad_norm': '40.75', 'counters/examples': 283360, 'counters/updates': 8855}
skipping logging after 283392 examples to avoid logging too frequently
train stats after 283424 examples: {'rewards_train/chosen': '0.31512', 'rewards_train/rejected': '0.14803', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.16709', 'logps_train/rejected': '-138.56', 'logps_train/chosen': '-145.88', 'loss/train': '0.64111', 'examples_per_second': '30.629', 'grad_norm': '35.25', 'counters/examples': 283424, 'counters/updates': 8857}
skipping logging after 283456 examples to avoid logging too frequently
train stats after 283488 examples: {'rewards_train/chosen': '0.099798', 'rewards_train/rejected': '-0.054974', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15477', 'logps_train/rejected': '-88.538', 'logps_train/chosen': '-126.61', 'loss/train': '0.63354', 'examples_per_second': '29.863', 'grad_norm': '30.125', 'counters/examples': 283488, 'counters/updates': 8859}
train stats after 283520 examples: {'rewards_train/chosen': '0.19481', 'rewards_train/rejected': '0.042711', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1521', 'logps_train/rejected': '-165.24', 'logps_train/chosen': '-180', 'loss/train': '0.62952', 'examples_per_second': '31.181', 'grad_norm': '39', 'counters/examples': 283520, 'counters/updates': 8860}
train stats after 283552 examples: {'rewards_train/chosen': '0.22315', 'rewards_train/rejected': '0.086746', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.1364', 'logps_train/rejected': '-143.62', 'logps_train/chosen': '-138.28', 'loss/train': '0.64507', 'examples_per_second': '31.476', 'grad_norm': '24.5', 'counters/examples': 283552, 'counters/updates': 8861}
train stats after 283584 examples: {'rewards_train/chosen': '0.21526', 'rewards_train/rejected': '0.12344', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.091829', 'logps_train/rejected': '-143.71', 'logps_train/chosen': '-155.52', 'loss/train': '0.65528', 'examples_per_second': '32.182', 'grad_norm': '39.5', 'counters/examples': 283584, 'counters/updates': 8862}
train stats after 283616 examples: {'rewards_train/chosen': '0.19125', 'rewards_train/rejected': '0.049472', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.14178', 'logps_train/rejected': '-121.19', 'logps_train/chosen': '-145.63', 'loss/train': '0.63406', 'examples_per_second': '32.185', 'grad_norm': '56.5', 'counters/examples': 283616, 'counters/updates': 8863}
train stats after 283648 examples: {'rewards_train/chosen': '0.1931', 'rewards_train/rejected': '0.053095', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.14', 'logps_train/rejected': '-96.422', 'logps_train/chosen': '-139.77', 'loss/train': '0.64445', 'examples_per_second': '32.51', 'grad_norm': '24.375', 'counters/examples': 283648, 'counters/updates': 8864}
train stats after 283680 examples: {'rewards_train/chosen': '0.19786', 'rewards_train/rejected': '0.15774', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.040114', 'logps_train/rejected': '-114.08', 'logps_train/chosen': '-128.17', 'loss/train': '0.68668', 'examples_per_second': '30.252', 'grad_norm': '33.25', 'counters/examples': 283680, 'counters/updates': 8865}
train stats after 283712 examples: {'rewards_train/chosen': '0.23974', 'rewards_train/rejected': '0.056069', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.18367', 'logps_train/rejected': '-110.76', 'logps_train/chosen': '-141.58', 'loss/train': '0.62489', 'examples_per_second': '31.393', 'grad_norm': '27', 'counters/examples': 283712, 'counters/updates': 8866}
skipping logging after 283744 examples to avoid logging too frequently
train stats after 283776 examples: {'rewards_train/chosen': '0.10899', 'rewards_train/rejected': '0.055894', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.053092', 'logps_train/rejected': '-108.46', 'logps_train/chosen': '-114.17', 'loss/train': '0.68926', 'examples_per_second': '31.092', 'grad_norm': '32.5', 'counters/examples': 283776, 'counters/updates': 8868}
train stats after 283808 examples: {'rewards_train/chosen': '0.22435', 'rewards_train/rejected': '0.036807', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.18754', 'logps_train/rejected': '-133.06', 'logps_train/chosen': '-144.06', 'loss/train': '0.61845', 'examples_per_second': '31.097', 'grad_norm': '30.875', 'counters/examples': 283808, 'counters/updates': 8869}
train stats after 283840 examples: {'rewards_train/chosen': '0.29245', 'rewards_train/rejected': '0.0018054', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.29065', 'logps_train/rejected': '-127.7', 'logps_train/chosen': '-158.84', 'loss/train': '0.58522', 'examples_per_second': '30.138', 'grad_norm': '36', 'counters/examples': 283840, 'counters/updates': 8870}
train stats after 283872 examples: {'rewards_train/chosen': '0.11104', 'rewards_train/rejected': '-0.0063103', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11735', 'logps_train/rejected': '-84.295', 'logps_train/chosen': '-123.79', 'loss/train': '0.65557', 'examples_per_second': '31.503', 'grad_norm': '58.75', 'counters/examples': 283872, 'counters/updates': 8871}
train stats after 283904 examples: {'rewards_train/chosen': '0.25288', 'rewards_train/rejected': '0.098483', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1544', 'logps_train/rejected': '-135.61', 'logps_train/chosen': '-163.9', 'loss/train': '0.63343', 'examples_per_second': '31.383', 'grad_norm': '31.625', 'counters/examples': 283904, 'counters/updates': 8872}
skipping logging after 283936 examples to avoid logging too frequently
train stats after 283968 examples: {'rewards_train/chosen': '0.13511', 'rewards_train/rejected': '0.0063062', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1288', 'logps_train/rejected': '-107.65', 'logps_train/chosen': '-156.65', 'loss/train': '0.65977', 'examples_per_second': '30.699', 'grad_norm': '29.875', 'counters/examples': 283968, 'counters/updates': 8874}
train stats after 284000 examples: {'rewards_train/chosen': '0.12396', 'rewards_train/rejected': '-0.015912', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13987', 'logps_train/rejected': '-108.06', 'logps_train/chosen': '-138.59', 'loss/train': '0.64681', 'examples_per_second': '31.192', 'grad_norm': '26.625', 'counters/examples': 284000, 'counters/updates': 8875}
skipping logging after 284032 examples to avoid logging too frequently
train stats after 284064 examples: {'rewards_train/chosen': '0.067827', 'rewards_train/rejected': '0.056513', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.011314', 'logps_train/rejected': '-131.43', 'logps_train/chosen': '-137.16', 'loss/train': '0.70504', 'examples_per_second': '31.459', 'grad_norm': '36.25', 'counters/examples': 284064, 'counters/updates': 8877}
train stats after 284096 examples: {'rewards_train/chosen': '0.19843', 'rewards_train/rejected': '0.11053', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.087896', 'logps_train/rejected': '-163.28', 'logps_train/chosen': '-148.93', 'loss/train': '0.6669', 'examples_per_second': '32.134', 'grad_norm': '39', 'counters/examples': 284096, 'counters/updates': 8878}
train stats after 284128 examples: {'rewards_train/chosen': '0.11914', 'rewards_train/rejected': '0.093524', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.025619', 'logps_train/rejected': '-126.46', 'logps_train/chosen': '-128.68', 'loss/train': '0.70608', 'examples_per_second': '31.603', 'grad_norm': '44.25', 'counters/examples': 284128, 'counters/updates': 8879}
train stats after 284160 examples: {'rewards_train/chosen': '0.26346', 'rewards_train/rejected': '0.1289', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.13456', 'logps_train/rejected': '-134.13', 'logps_train/chosen': '-158.15', 'loss/train': '0.65836', 'examples_per_second': '31.726', 'grad_norm': '33.25', 'counters/examples': 284160, 'counters/updates': 8880}
train stats after 284192 examples: {'rewards_train/chosen': '0.15621', 'rewards_train/rejected': '0.026699', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12951', 'logps_train/rejected': '-125.58', 'logps_train/chosen': '-134.22', 'loss/train': '0.64411', 'examples_per_second': '30.959', 'grad_norm': '29.5', 'counters/examples': 284192, 'counters/updates': 8881}
train stats after 284224 examples: {'rewards_train/chosen': '0.10392', 'rewards_train/rejected': '0.051463', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.052459', 'logps_train/rejected': '-165.66', 'logps_train/chosen': '-122.29', 'loss/train': '0.67749', 'examples_per_second': '31.45', 'grad_norm': '28.875', 'counters/examples': 284224, 'counters/updates': 8882}
train stats after 284256 examples: {'rewards_train/chosen': '0.16662', 'rewards_train/rejected': '0.038663', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.12796', 'logps_train/rejected': '-86.439', 'logps_train/chosen': '-109.4', 'loss/train': '0.64758', 'examples_per_second': '30.017', 'grad_norm': '26.75', 'counters/examples': 284256, 'counters/updates': 8883}
train stats after 284288 examples: {'rewards_train/chosen': '0.13565', 'rewards_train/rejected': '0.071816', 'rewards_train/accuracies': '0.4375', 'rewards_train/margins': '0.063831', 'logps_train/rejected': '-114.66', 'logps_train/chosen': '-172.75', 'loss/train': '0.67393', 'examples_per_second': '30.594', 'grad_norm': '39.5', 'counters/examples': 284288, 'counters/updates': 8884}
train stats after 284320 examples: {'rewards_train/chosen': '0.2571', 'rewards_train/rejected': '0.12834', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12876', 'logps_train/rejected': '-111.61', 'logps_train/chosen': '-124.17', 'loss/train': '0.64133', 'examples_per_second': '31.533', 'grad_norm': '33.25', 'counters/examples': 284320, 'counters/updates': 8885}
train stats after 284352 examples: {'rewards_train/chosen': '0.1662', 'rewards_train/rejected': '0.12326', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.042938', 'logps_train/rejected': '-147', 'logps_train/chosen': '-129.24', 'loss/train': '0.68058', 'examples_per_second': '30.686', 'grad_norm': '27.125', 'counters/examples': 284352, 'counters/updates': 8886}
train stats after 284384 examples: {'rewards_train/chosen': '0.18946', 'rewards_train/rejected': '0.11341', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.076045', 'logps_train/rejected': '-109.71', 'logps_train/chosen': '-171.7', 'loss/train': '0.66456', 'examples_per_second': '30.724', 'grad_norm': '25.375', 'counters/examples': 284384, 'counters/updates': 8887}
train stats after 284416 examples: {'rewards_train/chosen': '0.2116', 'rewards_train/rejected': '0.058675', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.15293', 'logps_train/rejected': '-149.4', 'logps_train/chosen': '-119.82', 'loss/train': '0.63008', 'examples_per_second': '23.32', 'grad_norm': '34.5', 'counters/examples': 284416, 'counters/updates': 8888}
train stats after 284448 examples: {'rewards_train/chosen': '0.1701', 'rewards_train/rejected': '0.046688', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12341', 'logps_train/rejected': '-166.87', 'logps_train/chosen': '-152.95', 'loss/train': '0.65122', 'examples_per_second': '31.496', 'grad_norm': '32.75', 'counters/examples': 284448, 'counters/updates': 8889}
skipping logging after 284480 examples to avoid logging too frequently
train stats after 284512 examples: {'rewards_train/chosen': '0.15231', 'rewards_train/rejected': '0.056728', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.095582', 'logps_train/rejected': '-99.933', 'logps_train/chosen': '-104', 'loss/train': '0.66289', 'examples_per_second': '23.783', 'grad_norm': '35.25', 'counters/examples': 284512, 'counters/updates': 8891}
train stats after 284544 examples: {'rewards_train/chosen': '0.13292', 'rewards_train/rejected': '0.15102', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.018094', 'logps_train/rejected': '-124.31', 'logps_train/chosen': '-136.82', 'loss/train': '0.72494', 'examples_per_second': '30.749', 'grad_norm': '40.5', 'counters/examples': 284544, 'counters/updates': 8892}
train stats after 284576 examples: {'rewards_train/chosen': '0.1726', 'rewards_train/rejected': '0.01123', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16137', 'logps_train/rejected': '-94.45', 'logps_train/chosen': '-138.19', 'loss/train': '0.63181', 'examples_per_second': '31.137', 'grad_norm': '24.25', 'counters/examples': 284576, 'counters/updates': 8893}
train stats after 284608 examples: {'rewards_train/chosen': '0.15848', 'rewards_train/rejected': '-0.014026', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.17251', 'logps_train/rejected': '-151.32', 'logps_train/chosen': '-172.73', 'loss/train': '0.62773', 'examples_per_second': '31.488', 'grad_norm': '31.5', 'counters/examples': 284608, 'counters/updates': 8894}
train stats after 284640 examples: {'rewards_train/chosen': '0.12629', 'rewards_train/rejected': '0.037222', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.089064', 'logps_train/rejected': '-137.05', 'logps_train/chosen': '-127.64', 'loss/train': '0.65961', 'examples_per_second': '31.526', 'grad_norm': '29', 'counters/examples': 284640, 'counters/updates': 8895}
skipping logging after 284672 examples to avoid logging too frequently
train stats after 284704 examples: {'rewards_train/chosen': '0.14056', 'rewards_train/rejected': '0.083233', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.057328', 'logps_train/rejected': '-129.52', 'logps_train/chosen': '-151.95', 'loss/train': '0.6771', 'examples_per_second': '31.358', 'grad_norm': '32.5', 'counters/examples': 284704, 'counters/updates': 8897}
skipping logging after 284736 examples to avoid logging too frequently
train stats after 284768 examples: {'rewards_train/chosen': '0.10481', 'rewards_train/rejected': '-0.030088', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1349', 'logps_train/rejected': '-111.36', 'logps_train/chosen': '-107.88', 'loss/train': '0.63999', 'examples_per_second': '31.536', 'grad_norm': '28.625', 'counters/examples': 284768, 'counters/updates': 8899}
train stats after 284800 examples: {'rewards_train/chosen': '0.14615', 'rewards_train/rejected': '0.071848', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.074304', 'logps_train/rejected': '-158.69', 'logps_train/chosen': '-133.07', 'loss/train': '0.68058', 'examples_per_second': '30.912', 'grad_norm': '40.5', 'counters/examples': 284800, 'counters/updates': 8900}
train stats after 284832 examples: {'rewards_train/chosen': '0.19463', 'rewards_train/rejected': '0.17847', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.016162', 'logps_train/rejected': '-137.55', 'logps_train/chosen': '-149.62', 'loss/train': '0.70991', 'examples_per_second': '31.89', 'grad_norm': '49.5', 'counters/examples': 284832, 'counters/updates': 8901}
train stats after 284864 examples: {'rewards_train/chosen': '0.062307', 'rewards_train/rejected': '0.012744', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.049562', 'logps_train/rejected': '-124.79', 'logps_train/chosen': '-146.18', 'loss/train': '0.67688', 'examples_per_second': '31.246', 'grad_norm': '29.375', 'counters/examples': 284864, 'counters/updates': 8902}
train stats after 284896 examples: {'rewards_train/chosen': '0.18174', 'rewards_train/rejected': '0.078184', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.10355', 'logps_train/rejected': '-123.76', 'logps_train/chosen': '-127.54', 'loss/train': '0.66181', 'examples_per_second': '32.865', 'grad_norm': '30.5', 'counters/examples': 284896, 'counters/updates': 8903}
train stats after 284928 examples: {'rewards_train/chosen': '0.19667', 'rewards_train/rejected': '0.068594', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.12808', 'logps_train/rejected': '-116.82', 'logps_train/chosen': '-145.83', 'loss/train': '0.6583', 'examples_per_second': '31.519', 'grad_norm': '37.75', 'counters/examples': 284928, 'counters/updates': 8904}
train stats after 284960 examples: {'rewards_train/chosen': '0.16842', 'rewards_train/rejected': '0.047239', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12118', 'logps_train/rejected': '-102.42', 'logps_train/chosen': '-117.89', 'loss/train': '0.65193', 'examples_per_second': '31.21', 'grad_norm': '33', 'counters/examples': 284960, 'counters/updates': 8905}
train stats after 284992 examples: {'rewards_train/chosen': '0.19514', 'rewards_train/rejected': '0.013768', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18137', 'logps_train/rejected': '-122.2', 'logps_train/chosen': '-141.42', 'loss/train': '0.6161', 'examples_per_second': '31.802', 'grad_norm': '23', 'counters/examples': 284992, 'counters/updates': 8906}
train stats after 285024 examples: {'rewards_train/chosen': '0.12005', 'rewards_train/rejected': '0.027992', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.092057', 'logps_train/rejected': '-103.85', 'logps_train/chosen': '-127.45', 'loss/train': '0.65911', 'examples_per_second': '30.837', 'grad_norm': '23.25', 'counters/examples': 285024, 'counters/updates': 8907}
train stats after 285056 examples: {'rewards_train/chosen': '0.13606', 'rewards_train/rejected': '-0.0041383', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.1402', 'logps_train/rejected': '-114.49', 'logps_train/chosen': '-169.07', 'loss/train': '0.6401', 'examples_per_second': '31.495', 'grad_norm': '28.625', 'counters/examples': 285056, 'counters/updates': 8908}
train stats after 285088 examples: {'rewards_train/chosen': '0.17203', 'rewards_train/rejected': '-0.06416', 'rewards_train/accuracies': '0.875', 'rewards_train/margins': '0.23619', 'logps_train/rejected': '-134.52', 'logps_train/chosen': '-168.77', 'loss/train': '0.59797', 'examples_per_second': '31.502', 'grad_norm': '27.375', 'counters/examples': 285088, 'counters/updates': 8909}
train stats after 285120 examples: {'rewards_train/chosen': '0.16024', 'rewards_train/rejected': '-0.01732', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17756', 'logps_train/rejected': '-101.46', 'logps_train/chosen': '-124.15', 'loss/train': '0.63288', 'examples_per_second': '29.988', 'grad_norm': '26.125', 'counters/examples': 285120, 'counters/updates': 8910}
train stats after 285152 examples: {'rewards_train/chosen': '0.16047', 'rewards_train/rejected': '-0.00051472', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.16098', 'logps_train/rejected': '-110.92', 'logps_train/chosen': '-146.12', 'loss/train': '0.63019', 'examples_per_second': '30.294', 'grad_norm': '24.375', 'counters/examples': 285152, 'counters/updates': 8911}
train stats after 285184 examples: {'rewards_train/chosen': '0.16468', 'rewards_train/rejected': '0.0072408', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.15744', 'logps_train/rejected': '-120.55', 'logps_train/chosen': '-138.84', 'loss/train': '0.62896', 'examples_per_second': '30.936', 'grad_norm': '35', 'counters/examples': 285184, 'counters/updates': 8912}
train stats after 285216 examples: {'rewards_train/chosen': '0.30737', 'rewards_train/rejected': '0.046594', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.26077', 'logps_train/rejected': '-129.66', 'logps_train/chosen': '-138.14', 'loss/train': '0.58818', 'examples_per_second': '31.57', 'grad_norm': '28.625', 'counters/examples': 285216, 'counters/updates': 8913}
train stats after 285248 examples: {'rewards_train/chosen': '0.10566', 'rewards_train/rejected': '0.11314', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '-0.007485', 'logps_train/rejected': '-106.33', 'logps_train/chosen': '-121.49', 'loss/train': '0.705', 'examples_per_second': '30.292', 'grad_norm': '30.875', 'counters/examples': 285248, 'counters/updates': 8914}
skipping logging after 285280 examples to avoid logging too frequently
train stats after 285312 examples: {'rewards_train/chosen': '0.16925', 'rewards_train/rejected': '0.041994', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12726', 'logps_train/rejected': '-143.73', 'logps_train/chosen': '-158.69', 'loss/train': '0.64011', 'examples_per_second': '31.334', 'grad_norm': '39.25', 'counters/examples': 285312, 'counters/updates': 8916}
train stats after 285344 examples: {'rewards_train/chosen': '0.15987', 'rewards_train/rejected': '-0.0030158', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16288', 'logps_train/rejected': '-114.97', 'logps_train/chosen': '-153.37', 'loss/train': '0.63903', 'examples_per_second': '32.924', 'grad_norm': '36.75', 'counters/examples': 285344, 'counters/updates': 8917}
train stats after 285376 examples: {'rewards_train/chosen': '0.13602', 'rewards_train/rejected': '-0.011115', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.14714', 'logps_train/rejected': '-149.9', 'logps_train/chosen': '-159.4', 'loss/train': '0.63887', 'examples_per_second': '31.464', 'grad_norm': '32.25', 'counters/examples': 285376, 'counters/updates': 8918}
train stats after 285408 examples: {'rewards_train/chosen': '0.21024', 'rewards_train/rejected': '-0.007085', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.21733', 'logps_train/rejected': '-103.82', 'logps_train/chosen': '-135.91', 'loss/train': '0.61064', 'examples_per_second': '32.235', 'grad_norm': '24.25', 'counters/examples': 285408, 'counters/updates': 8919}
train stats after 285440 examples: {'rewards_train/chosen': '0.23178', 'rewards_train/rejected': '0.11144', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.12034', 'logps_train/rejected': '-145.91', 'logps_train/chosen': '-165.29', 'loss/train': '0.64997', 'examples_per_second': '31.454', 'grad_norm': '28.75', 'counters/examples': 285440, 'counters/updates': 8920}
skipping logging after 285472 examples to avoid logging too frequently
train stats after 285504 examples: {'rewards_train/chosen': '0.07474', 'rewards_train/rejected': '0.051609', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.023131', 'logps_train/rejected': '-138.25', 'logps_train/chosen': '-108.23', 'loss/train': '0.6895', 'examples_per_second': '33.094', 'grad_norm': '26', 'counters/examples': 285504, 'counters/updates': 8922}
train stats after 285536 examples: {'rewards_train/chosen': '0.14741', 'rewards_train/rejected': '-0.055882', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.20329', 'logps_train/rejected': '-157.9', 'logps_train/chosen': '-165.32', 'loss/train': '0.62531', 'examples_per_second': '30.293', 'grad_norm': '29.125', 'counters/examples': 285536, 'counters/updates': 8923}
train stats after 285568 examples: {'rewards_train/chosen': '0.1408', 'rewards_train/rejected': '0.058075', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.082722', 'logps_train/rejected': '-166.31', 'logps_train/chosen': '-157.04', 'loss/train': '0.6645', 'examples_per_second': '32.116', 'grad_norm': '29', 'counters/examples': 285568, 'counters/updates': 8924}
train stats after 285600 examples: {'rewards_train/chosen': '0.098754', 'rewards_train/rejected': '0.10152', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0027627', 'logps_train/rejected': '-97.173', 'logps_train/chosen': '-133.9', 'loss/train': '0.70014', 'examples_per_second': '32.206', 'grad_norm': '30.875', 'counters/examples': 285600, 'counters/updates': 8925}
train stats after 285632 examples: {'rewards_train/chosen': '0.1585', 'rewards_train/rejected': '0.081385', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.077116', 'logps_train/rejected': '-137.79', 'logps_train/chosen': '-169.39', 'loss/train': '0.6665', 'examples_per_second': '31.466', 'grad_norm': '39', 'counters/examples': 285632, 'counters/updates': 8926}
skipping logging after 285664 examples to avoid logging too frequently
train stats after 285696 examples: {'rewards_train/chosen': '0.18317', 'rewards_train/rejected': '0.098883', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.084282', 'logps_train/rejected': '-145.01', 'logps_train/chosen': '-177.57', 'loss/train': '0.66807', 'examples_per_second': '33.011', 'grad_norm': '51.25', 'counters/examples': 285696, 'counters/updates': 8928}
train stats after 285728 examples: {'rewards_train/chosen': '0.21374', 'rewards_train/rejected': '0.096391', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11735', 'logps_train/rejected': '-106.84', 'logps_train/chosen': '-151.28', 'loss/train': '0.64519', 'examples_per_second': '31.485', 'grad_norm': '34', 'counters/examples': 285728, 'counters/updates': 8929}
train stats after 285760 examples: {'rewards_train/chosen': '0.10172', 'rewards_train/rejected': '0.10316', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '-0.0014345', 'logps_train/rejected': '-121.21', 'logps_train/chosen': '-152.52', 'loss/train': '0.70305', 'examples_per_second': '31.468', 'grad_norm': '27.375', 'counters/examples': 285760, 'counters/updates': 8930}
train stats after 285792 examples: {'rewards_train/chosen': '0.1855', 'rewards_train/rejected': '0.050857', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13464', 'logps_train/rejected': '-137.66', 'logps_train/chosen': '-131.01', 'loss/train': '0.64384', 'examples_per_second': '24.598', 'grad_norm': '38.75', 'counters/examples': 285792, 'counters/updates': 8931}
train stats after 285824 examples: {'rewards_train/chosen': '0.23562', 'rewards_train/rejected': '0.0015565', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.23406', 'logps_train/rejected': '-96.937', 'logps_train/chosen': '-163.54', 'loss/train': '0.5986', 'examples_per_second': '31.927', 'grad_norm': '29.375', 'counters/examples': 285824, 'counters/updates': 8932}
train stats after 285856 examples: {'rewards_train/chosen': '0.19409', 'rewards_train/rejected': '0.047652', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14643', 'logps_train/rejected': '-126.31', 'logps_train/chosen': '-148.8', 'loss/train': '0.63966', 'examples_per_second': '31.33', 'grad_norm': '23.875', 'counters/examples': 285856, 'counters/updates': 8933}
train stats after 285888 examples: {'rewards_train/chosen': '0.20974', 'rewards_train/rejected': '0.045488', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.16425', 'logps_train/rejected': '-148.38', 'logps_train/chosen': '-132.59', 'loss/train': '0.63521', 'examples_per_second': '30.464', 'grad_norm': '34.5', 'counters/examples': 285888, 'counters/updates': 8934}
train stats after 285920 examples: {'rewards_train/chosen': '0.15691', 'rewards_train/rejected': '-0.0010805', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.15799', 'logps_train/rejected': '-98.186', 'logps_train/chosen': '-149.35', 'loss/train': '0.63975', 'examples_per_second': '31.279', 'grad_norm': '26.75', 'counters/examples': 285920, 'counters/updates': 8935}
train stats after 285952 examples: {'rewards_train/chosen': '0.16432', 'rewards_train/rejected': '0.082904', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.08142', 'logps_train/rejected': '-166.87', 'logps_train/chosen': '-164.82', 'loss/train': '0.67303', 'examples_per_second': '32.479', 'grad_norm': '50', 'counters/examples': 285952, 'counters/updates': 8936}
skipping logging after 285984 examples to avoid logging too frequently
train stats after 286016 examples: {'rewards_train/chosen': '0.086293', 'rewards_train/rejected': '0.019026', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.067267', 'logps_train/rejected': '-102.86', 'logps_train/chosen': '-95.125', 'loss/train': '0.67543', 'examples_per_second': '30.435', 'grad_norm': '27', 'counters/examples': 286016, 'counters/updates': 8938}
train stats after 286048 examples: {'rewards_train/chosen': '0.093182', 'rewards_train/rejected': '-0.019346', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.11253', 'logps_train/rejected': '-106.96', 'logps_train/chosen': '-143.15', 'loss/train': '0.64595', 'examples_per_second': '32.442', 'grad_norm': '25.375', 'counters/examples': 286048, 'counters/updates': 8939}
train stats after 286080 examples: {'rewards_train/chosen': '0.24831', 'rewards_train/rejected': '0.14045', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.10786', 'logps_train/rejected': '-140.28', 'logps_train/chosen': '-130.29', 'loss/train': '0.65984', 'examples_per_second': '31.379', 'grad_norm': '37', 'counters/examples': 286080, 'counters/updates': 8940}
train stats after 286112 examples: {'rewards_train/chosen': '0.16816', 'rewards_train/rejected': '0.037581', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.13058', 'logps_train/rejected': '-152.04', 'logps_train/chosen': '-161.21', 'loss/train': '0.65986', 'examples_per_second': '31.507', 'grad_norm': '28', 'counters/examples': 286112, 'counters/updates': 8941}
train stats after 286144 examples: {'rewards_train/chosen': '0.17115', 'rewards_train/rejected': '0.052597', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11855', 'logps_train/rejected': '-108.1', 'logps_train/chosen': '-126.31', 'loss/train': '0.64957', 'examples_per_second': '30.347', 'grad_norm': '57', 'counters/examples': 286144, 'counters/updates': 8942}
train stats after 286176 examples: {'rewards_train/chosen': '0.16234', 'rewards_train/rejected': '0.031496', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.13085', 'logps_train/rejected': '-148.54', 'logps_train/chosen': '-118.27', 'loss/train': '0.64538', 'examples_per_second': '31.304', 'grad_norm': '37.75', 'counters/examples': 286176, 'counters/updates': 8943}
skipping logging after 286208 examples to avoid logging too frequently
train stats after 286240 examples: {'rewards_train/chosen': '0.17959', 'rewards_train/rejected': '0.069174', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.11042', 'logps_train/rejected': '-122.95', 'logps_train/chosen': '-127.28', 'loss/train': '0.65242', 'examples_per_second': '31.113', 'grad_norm': '28', 'counters/examples': 286240, 'counters/updates': 8945}
train stats after 286272 examples: {'rewards_train/chosen': '0.23838', 'rewards_train/rejected': '0.10268', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.1357', 'logps_train/rejected': '-114.58', 'logps_train/chosen': '-142.43', 'loss/train': '0.65338', 'examples_per_second': '31.34', 'grad_norm': '25.875', 'counters/examples': 286272, 'counters/updates': 8946}
train stats after 286304 examples: {'rewards_train/chosen': '0.10824', 'rewards_train/rejected': '0.081557', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.026687', 'logps_train/rejected': '-159.38', 'logps_train/chosen': '-148.14', 'loss/train': '0.69813', 'examples_per_second': '30.855', 'grad_norm': '87.5', 'counters/examples': 286304, 'counters/updates': 8947}
train stats after 286336 examples: {'rewards_train/chosen': '0.22104', 'rewards_train/rejected': '0.065505', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15553', 'logps_train/rejected': '-112.05', 'logps_train/chosen': '-129.72', 'loss/train': '0.64439', 'examples_per_second': '32.255', 'grad_norm': '71', 'counters/examples': 286336, 'counters/updates': 8948}
train stats after 286368 examples: {'rewards_train/chosen': '0.19964', 'rewards_train/rejected': '-0.012979', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.21262', 'logps_train/rejected': '-101.92', 'logps_train/chosen': '-116.19', 'loss/train': '0.60559', 'examples_per_second': '31.656', 'grad_norm': '23.25', 'counters/examples': 286368, 'counters/updates': 8949}
train stats after 286400 examples: {'rewards_train/chosen': '0.11572', 'rewards_train/rejected': '0.0051254', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.1106', 'logps_train/rejected': '-104.92', 'logps_train/chosen': '-136.64', 'loss/train': '0.65826', 'examples_per_second': '31.347', 'grad_norm': '30.625', 'counters/examples': 286400, 'counters/updates': 8950}
train stats after 286432 examples: {'rewards_train/chosen': '0.21425', 'rewards_train/rejected': '0.036025', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.17823', 'logps_train/rejected': '-165.31', 'logps_train/chosen': '-164.48', 'loss/train': '0.63527', 'examples_per_second': '31.15', 'grad_norm': '41.5', 'counters/examples': 286432, 'counters/updates': 8951}
skipping logging after 286464 examples to avoid logging too frequently
train stats after 286496 examples: {'rewards_train/chosen': '0.24547', 'rewards_train/rejected': '0.044568', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.20091', 'logps_train/rejected': '-89.302', 'logps_train/chosen': '-125.14', 'loss/train': '0.60821', 'examples_per_second': '30.824', 'grad_norm': '22.125', 'counters/examples': 286496, 'counters/updates': 8953}
train stats after 286528 examples: {'rewards_train/chosen': '0.14349', 'rewards_train/rejected': '0.008597', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.13489', 'logps_train/rejected': '-100.61', 'logps_train/chosen': '-126.17', 'loss/train': '0.6459', 'examples_per_second': '31.923', 'grad_norm': '34', 'counters/examples': 286528, 'counters/updates': 8954}
train stats after 286560 examples: {'rewards_train/chosen': '0.15549', 'rewards_train/rejected': '0.061196', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.094293', 'logps_train/rejected': '-128.34', 'logps_train/chosen': '-180.83', 'loss/train': '0.66859', 'examples_per_second': '32.166', 'grad_norm': '33.25', 'counters/examples': 286560, 'counters/updates': 8955}
train stats after 286592 examples: {'rewards_train/chosen': '0.10257', 'rewards_train/rejected': '0.063972', 'rewards_train/accuracies': '0.46875', 'rewards_train/margins': '0.038598', 'logps_train/rejected': '-138.44', 'logps_train/chosen': '-140.49', 'loss/train': '0.68692', 'examples_per_second': '31.265', 'grad_norm': '29.25', 'counters/examples': 286592, 'counters/updates': 8956}
train stats after 286624 examples: {'rewards_train/chosen': '0.15891', 'rewards_train/rejected': '-0.0080362', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.16694', 'logps_train/rejected': '-124.96', 'logps_train/chosen': '-115.9', 'loss/train': '0.62122', 'examples_per_second': '31.368', 'grad_norm': '29.375', 'counters/examples': 286624, 'counters/updates': 8957}
train stats after 286656 examples: {'rewards_train/chosen': '0.14536', 'rewards_train/rejected': '0.099799', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.04556', 'logps_train/rejected': '-141.29', 'logps_train/chosen': '-150.07', 'loss/train': '0.68243', 'examples_per_second': '31.396', 'grad_norm': '29.75', 'counters/examples': 286656, 'counters/updates': 8958}
train stats after 286688 examples: {'rewards_train/chosen': '0.17759', 'rewards_train/rejected': '0.046252', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.13134', 'logps_train/rejected': '-117.66', 'logps_train/chosen': '-139.61', 'loss/train': '0.64027', 'examples_per_second': '31.528', 'grad_norm': '42.25', 'counters/examples': 286688, 'counters/updates': 8959}
skipping logging after 286720 examples to avoid logging too frequently
train stats after 286752 examples: {'rewards_train/chosen': '0.15519', 'rewards_train/rejected': '0.04083', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.11436', 'logps_train/rejected': '-145.29', 'logps_train/chosen': '-165.87', 'loss/train': '0.65014', 'examples_per_second': '32.597', 'grad_norm': '32.75', 'counters/examples': 286752, 'counters/updates': 8961}
train stats after 286784 examples: {'rewards_train/chosen': '0.20661', 'rewards_train/rejected': '0.023288', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.18332', 'logps_train/rejected': '-172.95', 'logps_train/chosen': '-178.88', 'loss/train': '0.62605', 'examples_per_second': '32.909', 'grad_norm': '32.5', 'counters/examples': 286784, 'counters/updates': 8962}
train stats after 286816 examples: {'rewards_train/chosen': '0.16838', 'rewards_train/rejected': '0.056992', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.11139', 'logps_train/rejected': '-111.92', 'logps_train/chosen': '-118.78', 'loss/train': '0.64692', 'examples_per_second': '31.935', 'grad_norm': '29', 'counters/examples': 286816, 'counters/updates': 8963}
train stats after 286848 examples: {'rewards_train/chosen': '0.12386', 'rewards_train/rejected': '0.11535', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.0085078', 'logps_train/rejected': '-102.3', 'logps_train/chosen': '-119.94', 'loss/train': '0.70895', 'examples_per_second': '30.795', 'grad_norm': '79', 'counters/examples': 286848, 'counters/updates': 8964}
train stats after 286880 examples: {'rewards_train/chosen': '0.12006', 'rewards_train/rejected': '0.056635', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.06342', 'logps_train/rejected': '-119.08', 'logps_train/chosen': '-154.94', 'loss/train': '0.67269', 'examples_per_second': '31.177', 'grad_norm': '27.75', 'counters/examples': 286880, 'counters/updates': 8965}
train stats after 286912 examples: {'rewards_train/chosen': '0.12629', 'rewards_train/rejected': '-0.023735', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15003', 'logps_train/rejected': '-125.61', 'logps_train/chosen': '-182.29', 'loss/train': '0.63318', 'examples_per_second': '31.442', 'grad_norm': '30.625', 'counters/examples': 286912, 'counters/updates': 8966}
train stats after 286944 examples: {'rewards_train/chosen': '0.12841', 'rewards_train/rejected': '-0.03015', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.15856', 'logps_train/rejected': '-85.433', 'logps_train/chosen': '-136.04', 'loss/train': '0.63209', 'examples_per_second': '31.072', 'grad_norm': '25.25', 'counters/examples': 286944, 'counters/updates': 8967}
train stats after 286976 examples: {'rewards_train/chosen': '0.23307', 'rewards_train/rejected': '0.10154', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13153', 'logps_train/rejected': '-144.68', 'logps_train/chosen': '-147.84', 'loss/train': '0.6518', 'examples_per_second': '29.928', 'grad_norm': '32.25', 'counters/examples': 286976, 'counters/updates': 8968}
train stats after 287008 examples: {'rewards_train/chosen': '0.17971', 'rewards_train/rejected': '0.18289', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '-0.00318', 'logps_train/rejected': '-123.65', 'logps_train/chosen': '-131.05', 'loss/train': '0.71221', 'examples_per_second': '32.386', 'grad_norm': '44', 'counters/examples': 287008, 'counters/updates': 8969}
skipping logging after 287040 examples to avoid logging too frequently
train stats after 287072 examples: {'rewards_train/chosen': '0.15914', 'rewards_train/rejected': '0.066714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.092421', 'logps_train/rejected': '-130.38', 'logps_train/chosen': '-174.35', 'loss/train': '0.66062', 'examples_per_second': '31.348', 'grad_norm': '31.5', 'counters/examples': 287072, 'counters/updates': 8971}
train stats after 287104 examples: {'rewards_train/chosen': '0.045595', 'rewards_train/rejected': '-0.023527', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.069122', 'logps_train/rejected': '-118.76', 'logps_train/chosen': '-124.95', 'loss/train': '0.66726', 'examples_per_second': '31.416', 'grad_norm': '26.875', 'counters/examples': 287104, 'counters/updates': 8972}
skipping logging after 287136 examples to avoid logging too frequently
train stats after 287168 examples: {'rewards_train/chosen': '0.14098', 'rewards_train/rejected': '0.059285', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.081694', 'logps_train/rejected': '-112.26', 'logps_train/chosen': '-106.46', 'loss/train': '0.6645', 'examples_per_second': '31.534', 'grad_norm': '29.5', 'counters/examples': 287168, 'counters/updates': 8974}
train stats after 287200 examples: {'rewards_train/chosen': '0.30656', 'rewards_train/rejected': '0.063056', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.2435', 'logps_train/rejected': '-140.69', 'logps_train/chosen': '-170.92', 'loss/train': '0.59199', 'examples_per_second': '31.587', 'grad_norm': '39.25', 'counters/examples': 287200, 'counters/updates': 8975}
skipping logging after 287232 examples to avoid logging too frequently
train stats after 287264 examples: {'rewards_train/chosen': '0.071517', 'rewards_train/rejected': '-0.044185', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.1157', 'logps_train/rejected': '-140.22', 'logps_train/chosen': '-127.15', 'loss/train': '0.64784', 'examples_per_second': '31.524', 'grad_norm': '37.5', 'counters/examples': 287264, 'counters/updates': 8977}
train stats after 287296 examples: {'rewards_train/chosen': '0.27019', 'rewards_train/rejected': '0.086433', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18376', 'logps_train/rejected': '-130.26', 'logps_train/chosen': '-180.21', 'loss/train': '0.62551', 'examples_per_second': '29.407', 'grad_norm': '33.75', 'counters/examples': 287296, 'counters/updates': 8978}
train stats after 287328 examples: {'rewards_train/chosen': '0.18948', 'rewards_train/rejected': '-0.082512', 'rewards_train/accuracies': '0.84375', 'rewards_train/margins': '0.27199', 'logps_train/rejected': '-179.38', 'logps_train/chosen': '-158.28', 'loss/train': '0.59134', 'examples_per_second': '31.489', 'grad_norm': '29', 'counters/examples': 287328, 'counters/updates': 8979}
train stats after 287360 examples: {'rewards_train/chosen': '0.18084', 'rewards_train/rejected': '0.059575', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.12126', 'logps_train/rejected': '-149.78', 'logps_train/chosen': '-146.07', 'loss/train': '0.65657', 'examples_per_second': '31.6', 'grad_norm': '36.25', 'counters/examples': 287360, 'counters/updates': 8980}
train stats after 287392 examples: {'rewards_train/chosen': '0.19977', 'rewards_train/rejected': '0.028274', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.1715', 'logps_train/rejected': '-106.36', 'logps_train/chosen': '-149.42', 'loss/train': '0.62608', 'examples_per_second': '30.136', 'grad_norm': '34.25', 'counters/examples': 287392, 'counters/updates': 8981}
train stats after 287424 examples: {'rewards_train/chosen': '0.12301', 'rewards_train/rejected': '-0.016309', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13932', 'logps_train/rejected': '-126.61', 'logps_train/chosen': '-135.33', 'loss/train': '0.63813', 'examples_per_second': '32.935', 'grad_norm': '32.75', 'counters/examples': 287424, 'counters/updates': 8982}
skipping logging after 287456 examples to avoid logging too frequently
train stats after 287488 examples: {'rewards_train/chosen': '0.27083', 'rewards_train/rejected': '0.2123', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.058522', 'logps_train/rejected': '-163.8', 'logps_train/chosen': '-143.22', 'loss/train': '0.69615', 'examples_per_second': '31.256', 'grad_norm': '73.5', 'counters/examples': 287488, 'counters/updates': 8984}
train stats after 287520 examples: {'rewards_train/chosen': '0.32276', 'rewards_train/rejected': '0.16435', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.15842', 'logps_train/rejected': '-157.43', 'logps_train/chosen': '-168.27', 'loss/train': '0.64001', 'examples_per_second': '32.202', 'grad_norm': '38.5', 'counters/examples': 287520, 'counters/updates': 8985}
train stats after 287552 examples: {'rewards_train/chosen': '0.22451', 'rewards_train/rejected': '0.14773', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.076777', 'logps_train/rejected': '-145.4', 'logps_train/chosen': '-155.4', 'loss/train': '0.67451', 'examples_per_second': '31.61', 'grad_norm': '45', 'counters/examples': 287552, 'counters/updates': 8986}
train stats after 287584 examples: {'rewards_train/chosen': '0.16786', 'rewards_train/rejected': '0.016121', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.15174', 'logps_train/rejected': '-88.558', 'logps_train/chosen': '-126.28', 'loss/train': '0.63101', 'examples_per_second': '32.736', 'grad_norm': '25.625', 'counters/examples': 287584, 'counters/updates': 8987}
train stats after 287616 examples: {'rewards_train/chosen': '0.12512', 'rewards_train/rejected': '0.058224', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.066901', 'logps_train/rejected': '-124.29', 'logps_train/chosen': '-140.01', 'loss/train': '0.66951', 'examples_per_second': '31.792', 'grad_norm': '30.375', 'counters/examples': 287616, 'counters/updates': 8988}
train stats after 287648 examples: {'rewards_train/chosen': '0.12027', 'rewards_train/rejected': '0.060457', 'rewards_train/accuracies': '0.40625', 'rewards_train/margins': '0.059814', 'logps_train/rejected': '-144.77', 'logps_train/chosen': '-144.31', 'loss/train': '0.6728', 'examples_per_second': '31.123', 'grad_norm': '31.625', 'counters/examples': 287648, 'counters/updates': 8989}
train stats after 287680 examples: {'rewards_train/chosen': '0.15562', 'rewards_train/rejected': '0.08241', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.073212', 'logps_train/rejected': '-149.02', 'logps_train/chosen': '-149.46', 'loss/train': '0.67621', 'examples_per_second': '29.968', 'grad_norm': '49.5', 'counters/examples': 287680, 'counters/updates': 8990}
train stats after 287712 examples: {'rewards_train/chosen': '0.11282', 'rewards_train/rejected': '0.044724', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.068096', 'logps_train/rejected': '-131.53', 'logps_train/chosen': '-163.13', 'loss/train': '0.69224', 'examples_per_second': '31.457', 'grad_norm': '40', 'counters/examples': 287712, 'counters/updates': 8991}
skipping logging after 287744 examples to avoid logging too frequently
train stats after 287776 examples: {'rewards_train/chosen': '0.18492', 'rewards_train/rejected': '0.093714', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.091208', 'logps_train/rejected': '-131.39', 'logps_train/chosen': '-154.09', 'loss/train': '0.66088', 'examples_per_second': '31.363', 'grad_norm': '31.25', 'counters/examples': 287776, 'counters/updates': 8993}
skipping logging after 287808 examples to avoid logging too frequently
train stats after 287840 examples: {'rewards_train/chosen': '0.14392', 'rewards_train/rejected': '0.02772', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1162', 'logps_train/rejected': '-120.91', 'logps_train/chosen': '-118.53', 'loss/train': '0.64943', 'examples_per_second': '37.689', 'grad_norm': '24.125', 'counters/examples': 287840, 'counters/updates': 8995}
train stats after 287872 examples: {'rewards_train/chosen': '0.24531', 'rewards_train/rejected': '0.065241', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.18007', 'logps_train/rejected': '-117.15', 'logps_train/chosen': '-150.19', 'loss/train': '0.63262', 'examples_per_second': '31.948', 'grad_norm': '26.75', 'counters/examples': 287872, 'counters/updates': 8996}
train stats after 287904 examples: {'rewards_train/chosen': '0.20029', 'rewards_train/rejected': '0.0058393', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.19445', 'logps_train/rejected': '-111.18', 'logps_train/chosen': '-167.98', 'loss/train': '0.61269', 'examples_per_second': '30.455', 'grad_norm': '29.5', 'counters/examples': 287904, 'counters/updates': 8997}
train stats after 287936 examples: {'rewards_train/chosen': '0.13016', 'rewards_train/rejected': '-0.010844', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.141', 'logps_train/rejected': '-120.74', 'logps_train/chosen': '-122.29', 'loss/train': '0.63645', 'examples_per_second': '32.823', 'grad_norm': '43', 'counters/examples': 287936, 'counters/updates': 8998}
train stats after 287968 examples: {'rewards_train/chosen': '0.20356', 'rewards_train/rejected': '0.035711', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.16785', 'logps_train/rejected': '-158.35', 'logps_train/chosen': '-172.76', 'loss/train': '0.62737', 'examples_per_second': '31.2', 'grad_norm': '33.75', 'counters/examples': 287968, 'counters/updates': 8999}
train stats after 288000 examples: {'rewards_train/chosen': '0.17039', 'rewards_train/rejected': '0.0047286', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.16566', 'logps_train/rejected': '-133.52', 'logps_train/chosen': '-127.11', 'loss/train': '0.62779', 'examples_per_second': '31.737', 'grad_norm': '26.625', 'counters/examples': 288000, 'counters/updates': 9000}
Running evaluation after 288000 train examples
Computing eval metrics:   0%|          | 0/16 [00:00<?, ?it/s]Computing eval metrics:   6%|â–‹         | 1/16 [00:00<00:03,  4.23it/s]Computing eval metrics:  12%|â–ˆâ–Ž        | 2/16 [00:00<00:03,  3.85it/s]Computing eval metrics:  19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  3.95it/s]Computing eval metrics:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:01<00:03,  3.94it/s]Computing eval metrics:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:02,  3.85it/s]Computing eval metrics:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  4.09it/s]Computing eval metrics:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  4.00it/s]Computing eval metrics:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.95it/s]Computing eval metrics:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:01,  3.99it/s]Computing eval metrics:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:02<00:01,  3.91it/s]Computing eval metrics:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.92it/s]Computing eval metrics:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.93it/s]Computing eval metrics:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:03<00:00,  3.84it/s]Computing eval metrics:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:03<00:00,  3.89it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.81it/s]Computing eval metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.91it/s]
3 initializing distributed
Creating trainer on process 3 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 3
Loading HH static dataset (train split) from Huggingface...
done
2 initializing distributed
Creating trainer on process 2 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 2
Loading HH static dataset (train split) from Huggingface...
done
7 initializing distributed
Creating trainer on process 7 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 7
Loading HH static dataset (train split) from Huggingface...
done
6 initializing distributed
Creating trainer on process 6 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 6
Loading HH static dataset (train split) from Huggingface...
done
1 initializing distributed
Creating trainer on process 1 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 1
Loading HH static dataset (train split) from Huggingface...
done
5 initializing distributed
Creating trainer on process 5 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 5
Loading HH static dataset (train split) from Huggingface...
done
4 initializing distributed
Creating trainer on process 4 with world size 8
Loading HH static dataset (test split) from Huggingface...
done
Loaded model on rank 4
Loading HH static dataset (train split) from Huggingface...
done
eval after 288000: {'rewards_eval/chosen': '0.19504', 'rewards_eval/rejected': '0.066384', 'rewards_eval/accuracies': '0.60547', 'rewards_eval/margins': '0.12865', 'logps_eval/rejected': '-121.47', 'logps_eval/chosen': '-142.16', 'loss/eval': '0.65113'}
skipping save for non epoch
train stats after 288032 examples: {'rewards_train/chosen': '0.22729', 'rewards_train/rejected': '0.081864', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.14542', 'logps_train/rejected': '-131.92', 'logps_train/chosen': '-163.86', 'loss/train': '0.63909', 'examples_per_second': '31.652', 'grad_norm': '31.625', 'counters/examples': 288032, 'counters/updates': 9001}
train stats after 288064 examples: {'rewards_train/chosen': '0.18588', 'rewards_train/rejected': '0.012066', 'rewards_train/accuracies': '0.78125', 'rewards_train/margins': '0.17382', 'logps_train/rejected': '-118.32', 'logps_train/chosen': '-134.94', 'loss/train': '0.62049', 'examples_per_second': '29.778', 'grad_norm': '57.5', 'counters/examples': 288064, 'counters/updates': 9002}
train stats after 288096 examples: {'rewards_train/chosen': '0.12711', 'rewards_train/rejected': '-0.035366', 'rewards_train/accuracies': '0.6875', 'rewards_train/margins': '0.16247', 'logps_train/rejected': '-103.51', 'logps_train/chosen': '-148', 'loss/train': '0.62714', 'examples_per_second': '32.147', 'grad_norm': '24.5', 'counters/examples': 288096, 'counters/updates': 9003}
train stats after 288128 examples: {'rewards_train/chosen': '0.21731', 'rewards_train/rejected': '0.06545', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.15186', 'logps_train/rejected': '-127.05', 'logps_train/chosen': '-175.88', 'loss/train': '0.63205', 'examples_per_second': '32.34', 'grad_norm': '39', 'counters/examples': 288128, 'counters/updates': 9004}
skipping logging after 288160 examples to avoid logging too frequently
train stats after 288192 examples: {'rewards_train/chosen': '0.17886', 'rewards_train/rejected': '0.065361', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.1135', 'logps_train/rejected': '-146.04', 'logps_train/chosen': '-179.24', 'loss/train': '0.67727', 'examples_per_second': '32.375', 'grad_norm': '43.25', 'counters/examples': 288192, 'counters/updates': 9006}
train stats after 288224 examples: {'rewards_train/chosen': '0.20376', 'rewards_train/rejected': '0.088467', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.11529', 'logps_train/rejected': '-133.61', 'logps_train/chosen': '-169.24', 'loss/train': '0.65752', 'examples_per_second': '32.328', 'grad_norm': '30', 'counters/examples': 288224, 'counters/updates': 9007}
skipping logging after 288256 examples to avoid logging too frequently
train stats after 288288 examples: {'rewards_train/chosen': '0.16748', 'rewards_train/rejected': '0.038371', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12911', 'logps_train/rejected': '-135.62', 'logps_train/chosen': '-158.74', 'loss/train': '0.65204', 'examples_per_second': '30.834', 'grad_norm': '35.5', 'counters/examples': 288288, 'counters/updates': 9009}
train stats after 288320 examples: {'rewards_train/chosen': '0.099307', 'rewards_train/rejected': '0.053266', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.046041', 'logps_train/rejected': '-127.86', 'logps_train/chosen': '-158.3', 'loss/train': '0.68608', 'examples_per_second': '30.506', 'grad_norm': '32.75', 'counters/examples': 288320, 'counters/updates': 9010}
train stats after 288352 examples: {'rewards_train/chosen': '0.19524', 'rewards_train/rejected': '0.084398', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.11084', 'logps_train/rejected': '-128.58', 'logps_train/chosen': '-137.81', 'loss/train': '0.67304', 'examples_per_second': '30.022', 'grad_norm': '45.5', 'counters/examples': 288352, 'counters/updates': 9011}
skipping logging after 288384 examples to avoid logging too frequently
train stats after 288416 examples: {'rewards_train/chosen': '0.22909', 'rewards_train/rejected': '0.023489', 'rewards_train/accuracies': '0.75', 'rewards_train/margins': '0.2056', 'logps_train/rejected': '-141.34', 'logps_train/chosen': '-181.75', 'loss/train': '0.6067', 'examples_per_second': '29.93', 'grad_norm': '39.25', 'counters/examples': 288416, 'counters/updates': 9013}
train stats after 288448 examples: {'rewards_train/chosen': '0.1841', 'rewards_train/rejected': '0.10509', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.079005', 'logps_train/rejected': '-117.68', 'logps_train/chosen': '-144.63', 'loss/train': '0.66859', 'examples_per_second': '32.642', 'grad_norm': '38.5', 'counters/examples': 288448, 'counters/updates': 9014}
train stats after 288480 examples: {'rewards_train/chosen': '0.043622', 'rewards_train/rejected': '-0.016703', 'rewards_train/accuracies': '0.5', 'rewards_train/margins': '0.060324', 'logps_train/rejected': '-115.6', 'logps_train/chosen': '-156.98', 'loss/train': '0.68044', 'examples_per_second': '30.133', 'grad_norm': '31', 'counters/examples': 288480, 'counters/updates': 9015}
train stats after 288512 examples: {'rewards_train/chosen': '0.16863', 'rewards_train/rejected': '0.039874', 'rewards_train/accuracies': '0.625', 'rewards_train/margins': '0.12875', 'logps_train/rejected': '-123.11', 'logps_train/chosen': '-132.04', 'loss/train': '0.64784', 'examples_per_second': '31.874', 'grad_norm': '26', 'counters/examples': 288512, 'counters/updates': 9016}
train stats after 288544 examples: {'rewards_train/chosen': '0.13468', 'rewards_train/rejected': '0.038802', 'rewards_train/accuracies': '0.5625', 'rewards_train/margins': '0.095877', 'logps_train/rejected': '-112.53', 'logps_train/chosen': '-152.34', 'loss/train': '0.65851', 'examples_per_second': '31.337', 'grad_norm': '28.375', 'counters/examples': 288544, 'counters/updates': 9017}
train stats after 288576 examples: {'rewards_train/chosen': '0.17347', 'rewards_train/rejected': '0.036291', 'rewards_train/accuracies': '0.65625', 'rewards_train/margins': '0.13718', 'logps_train/rejected': '-150.33', 'logps_train/chosen': '-166.6', 'loss/train': '0.64644', 'examples_per_second': '31.403', 'grad_norm': '34.5', 'counters/examples': 288576, 'counters/updates': 9018}
skipping logging after 288608 examples to avoid logging too frequently
train stats after 288640 examples: {'rewards_train/chosen': '0.27336', 'rewards_train/rejected': '0.014475', 'rewards_train/accuracies': '0.71875', 'rewards_train/margins': '0.25889', 'logps_train/rejected': '-144.59', 'logps_train/chosen': '-172.58', 'loss/train': '0.59073', 'examples_per_second': '32.167', 'grad_norm': '31.375', 'counters/examples': 288640, 'counters/updates': 9020}
train stats after 288672 examples: {'rewards_train/chosen': '0.17068', 'rewards_train/rejected': '0.076853', 'rewards_train/accuracies': '0.53125', 'rewards_train/margins': '0.093827', 'logps_train/rejected': '-113.89', 'logps_train/chosen': '-150.71', 'loss/train': '0.6607', 'examples_per_second': '31.515', 'grad_norm': '30', 'counters/examples': 288672, 'counters/updates': 9021}
train stats after 288704 examples: {'rewards_train/chosen': '0.14091', 'rewards_train/rejected': '-0.041834', 'rewards_train/accuracies': '0.8125', 'rewards_train/margins': '0.18275', 'logps_train/rejected': '-137.16', 'logps_train/chosen': '-151.51', 'loss/train': '0.62089', 'examples_per_second': '31.286', 'grad_norm': '26.875', 'counters/examples': 288704, 'counters/updates': 9022}
skipping logging after 288736 examples to avoid logging too frequently
train stats after 288768 examples: {'rewards_train/chosen': '0.11886', 'rewards_train/rejected': '0.01152', 'rewards_train/accuracies': '0.59375', 'rewards_train/margins': '0.10734', 'logps_train/rejected': '-124.76', 'logps_train/chosen': '-159.68', 'loss/train': '0.65753', 'examples_per_second': '31.46', 'grad_norm': '28.75', 'counters/examples': 288768, 'counters/updates': 9024}
Finished generating 3 epochs on train split
writing checkpoint to .cache/laura/pythia2.8b_sfted2_dpo3_seed0_2024-03-19_00-55-54_494349/LATEST/policy.pt...
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: - 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:        counters/examples â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         counters/updates â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      examples_per_second â–…â–‚â–ƒâ–ƒâ–„â–„â–…â–„â–‚â–ˆâ–‚â–‚â–‚â–†â–ƒâ–„â–„â–„â–…â–â–ƒâ–…â–‚â–…â–…â–â–‚â–‚â–„â–ƒâ–…â–„â–ƒâ–„â–†â–…â–â–‚â–„â–„
wandb:                grad_norm â–‚â–ƒâ–ƒâ–„â–ƒâ–ˆâ–â–‚â–ƒâ–â–‚â–‚â–…â–„â–‚â–‚â–„â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚
wandb:        logps_eval/chosen â–â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆ
wandb:      logps_eval/rejected â–â–ƒâ–„â–…â–„â–„â–…â–…â–†â–†â–†â–…â–†â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡
wandb:       logps_train/chosen â–†â–‚â–‡â–…â–ƒâ–ƒâ–ˆâ–†â–â–‡â–‡â–„â–…â–…â–ˆâ–„â–†â–‚â–‚â–†â–†â–‡â–†â–…â–‡â–„â–…â–ƒâ–‡â–†â–„â–…â–â–†â–‚â–‡â–„â–‡â–…â–…
wandb:     logps_train/rejected â–…â–…â–„â–…â–„â–†â–‡â–„â–‚â–ˆâ–ƒâ–„â–â–ƒâ–„â–ˆâ–„â–‚â–‡â–…â–‡â–†â–‡â–„â–…â–ƒâ–ƒâ–…â–†â–ƒâ–ƒâ–…â–…â–ƒâ–†â–…â–ƒâ–…â–…â–…
wandb:                loss/eval â–ˆâ–ˆâ–‡â–‡â–„â–…â–…â–„â–†â–„â–„â–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–â–‚
wandb:               loss/train â–†â–…â–„â–„â–„â–ˆâ–ƒâ–„â–‚â–‚â–…â–ƒâ–…â–…â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–„â–„â–„â–‚â–„â–…â–‚â–ƒ
wandb:  rewards_eval/accuracies â–â–‚â–ƒâ–ƒâ–…â–…â–†â–…â–†â–†â–†â–ˆâ–ˆâ–†â–‡â–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–…â–‡â–‡
wandb:      rewards_eval/chosen â–â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆ
wandb:     rewards_eval/margins â–â–â–â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–…â–…â–†â–†â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–ˆâ–ˆ
wandb:    rewards_eval/rejected â–â–ƒâ–„â–…â–„â–„â–…â–…â–†â–†â–†â–…â–†â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡
wandb: rewards_train/accuracies â–â–…â–‚â–„â–…â–‚â–„â–‚â–‡â–†â–â–…â–…â–‚â–…â–‡â–ƒâ–‡â–„â–…â–…â–‡â–‚â–‡â–…â–…â–†â–‡â–‚â–…â–ˆâ–†â–‚â–„â–…â–†â–…â–…â–†â–…
wandb:     rewards_train/chosen â–‚â–â–‚â–„â–ƒâ–„â–„â–ƒâ–„â–…â–ƒâ–…â–„â–„â–„â–„â–…â–†â–†â–…â–„â–ˆâ–ƒâ–†â–†â–„â–†â–†â–…â–†â–…â–‡â–‡â–ƒâ–†â–†â–…â–„â–…â–…
wandb:    rewards_train/margins â–‚â–„â–„â–†â–„â–â–…â–„â–†â–†â–ƒâ–†â–ƒâ–ƒâ–…â–†â–„â–…â–†â–…â–…â–…â–…â–ˆâ–„â–…â–†â–†â–†â–‡â–†â–ˆâ–…â–„â–„â–†â–…â–„â–‡â–…
wandb:   rewards_train/rejected â–„â–â–‚â–ƒâ–‚â–ˆâ–„â–„â–‚â–ƒâ–„â–ƒâ–†â–†â–ƒâ–â–…â–…â–„â–„â–‚â–ˆâ–ƒâ–‚â–‡â–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ƒâ–†â–„â–…â–…â–‚â–„
wandb: 
wandb: Run summary:
wandb:        counters/examples 288768
wandb:         counters/updates 9024
wandb:      examples_per_second 31.45985
wandb:                grad_norm 28.75
wandb:        logps_eval/chosen -142.15511
wandb:      logps_eval/rejected -121.46909
wandb:       logps_train/chosen -159.67776
wandb:     logps_train/rejected -124.75545
wandb:                loss/eval 0.65113
wandb:               loss/train 0.65753
wandb:  rewards_eval/accuracies 0.60547
wandb:      rewards_eval/chosen 0.19504
wandb:     rewards_eval/margins 0.12865
wandb:    rewards_eval/rejected 0.06638
wandb: rewards_train/accuracies 0.59375
wandb:     rewards_train/chosen 0.11886
wandb:    rewards_train/margins 0.10734
wandb:   rewards_train/rejected 0.01152
wandb: 
wandb: ðŸš€ View run pythia2.8b_sfted2_dpo3_seed0 at: https://wandb.ai/lauraomahony999/pythia-dpo/runs/7t93d6jx
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: .cache/laura/wandb/run-20240319_005749-7t93d6jx/logs
